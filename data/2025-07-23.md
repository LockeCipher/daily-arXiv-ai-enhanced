<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 17]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Dream, Lift, Animate: From Single Images to Animatable Gaussian Avatars](https://arxiv.org/abs/2507.15979)
*Marcel C. Bühler,Ye Yuan,Xueting Li,Yangyi Huang,Koki Nagano,Umar Iqbal*

Main category: cs.GR

TL;DR: DLA框架通过视频扩散模型生成多视角、3D高斯提升和姿态感知UV空间映射，从单张图像重建可动画的3D人体化身，实现实时渲染和直观编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以从单张图像重建既保持精细视觉细节又支持动画的3D人体化身，需要一种能够桥接非结构化3D表示与高保真度动画就绪化身之间差距的方法。

Method: 提出DLA框架：1）使用视频扩散模型从单张图像生成合理的多视角图像；2）将多视角提升为非结构化3D高斯；3）使用基于Transformer的编码器建模全局空间关系，将高斯投影到与参数化身体模型UV空间对齐的结构化潜在表示；4）将潜在代码解码为可通过身体驱动变形进行动画的UV空间高斯。

Result: 在ActorsHQ和4D-Dress数据集上，DLA在感知质量和光度测量精度方面均优于现有最先进方法，能够实现实时渲染和直观编辑，无需后处理。

Conclusion: 通过将视频扩散模型的生成优势与姿态感知UV空间高斯映射相结合，DLA成功桥接了非结构化3D表示与高保真度动画就绪化身之间的差距，通过将高斯锚定到UV流形确保动画过程中的一致性并保持精细视觉细节。

Abstract: We introduce Dream, Lift, Animate (DLA), a novel framework that reconstructs animatable 3D human avatars from a single image. This is achieved by leveraging multi-view generation, 3D Gaussian lifting, and pose-aware UV-space mapping of 3D Gaussians. Given an image, we first dream plausible multi-views using a video diffusion model, capturing rich geometric and appearance details. These views are then lifted into unstructured 3D Gaussians. To enable animation, we propose a transformer-based encoder that models global spatial relationships and projects these Gaussians into a structured latent representation aligned with the UV space of a parametric body model. This latent code is decoded into UV-space Gaussians that can be animated via body-driven deformation and rendered conditioned on pose and viewpoint. By anchoring Gaussians to the UV manifold, our method ensures consistency during animation while preserving fine visual details. DLA enables real-time rendering and intuitive editing without requiring post-processing. Our method outperforms state-of-the-art approaches on ActorsHQ and 4D-Dress datasets in both perceptual quality and photometric accuracy. By combining the generative strengths of video diffusion models with a pose-aware UV-space Gaussian mapping, DLA bridges the gap between unstructured 3D representations and high-fidelity, animation-ready avatars.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [PAT++: a cautionary tale about generative visual augmentation for Object Re-identification](https://arxiv.org/abs/2507.15888)
*Leonardo Santiago Benitez Pereira,Arathy Jeevan*

Main category: cs.CV

TL;DR: 本文研究了生成式数据增强在目标重识别任务中的有效性，提出了PAT++管道，结合扩散自蒸馏和部分感知变换器，但实验结果显示生成图像会导致性能下降，挑战了生成模型在细粒度识别任务中的适用性假设。


<details>
  <summary>Details</summary>
Motivation: 虽然生成式数据增强在多个视觉任务中表现出色，但在需要保持细粒度视觉细节的目标重识别任务中，其影响尚未得到充分探索。作者希望评估身份保持图像生成在目标重识别中的有效性。

Method: 提出了名为PAT++的新型管道，将扩散自蒸馏（Diffusion Self-Distillation）融入到已建立的部分感知变换器（Part-Aware Transformer）中。使用Urban Elements ReID Challenge数据集进行广泛实验，将生成的图像用于模型训练和查询扩展。

Result: 实验结果显示性能持续下降，主要由领域偏移和无法保留身份定义特征所驱动。生成的图像在目标重识别任务中并未带来预期的性能提升。

Conclusion: 研究结果挑战了生成模型在细粒度识别任务中可转移性的假设，揭示了当前视觉增强方法在身份保持应用中的关键局限性。这表明生成式数据增强可能不适用于所有视觉任务，特别是那些需要精确保持细粒度特征的任务。

Abstract: Generative data augmentation has demonstrated gains in several vision tasks, but its impact on object re-identification - where preserving fine-grained visual details is essential - remains largely unexplored. In this work, we assess the effectiveness of identity-preserving image generation for object re-identification. Our novel pipeline, named PAT++, incorporates Diffusion Self-Distillation into the well-established Part-Aware Transformer. Using the Urban Elements ReID Challenge dataset, we conduct extensive experiments with generated images used for both model training and query expansion. Our results show consistent performance degradation, driven by domain shifts and failure to retain identity-defining features. These findings challenge assumptions about the transferability of generative models to fine-grained recognition tasks and expose key limitations in current approaches to visual augmentation for identity-preserving applications.

</details>


### [3] [Improving Personalized Image Generation through Social Context Feedback](https://arxiv.org/abs/2507.16095)
*Parul Gupta,Abhinav Dhall,Thanh-Toan Do*

Main category: cs.CV

TL;DR: 本文提出了一种基于反馈的个性化图像生成方法，通过集成姿态、人物-物体交互、面部识别和视线估计检测器来改进扩散模型，解决了复杂活动生成不当、身份保持不佳和视线模式不自然等问题


<details>
  <summary>Details</summary>
Motivation: 现有的个性化图像生成方法存在三个主要限制：1）复杂活动（如"男人推摩托车"）生成不当，人体姿态错误；2）参考人物身份无法保持；3）生成的人物视线模式不自然或与场景描述不一致

Method: 提出基于反馈的微调方法来改进现有个性化生成方法，使用最先进的姿态检测器、人物-物体交互检测器、人脸识别器和人眼视线估计器来优化扩散模型。根据信号是低级（如人体姿态）还是高级（如视线点），提出基于时间步的不同反馈模块集成策略

Result: 在三个基准数据集上，生成的图像在交互质量、面部身份保持和整体图像质量方面都有显著改善

Conclusion: 通过集成多种专门的检测器作为反馈信号，并采用时间步相关的反馈策略，能够有效解决个性化图像生成中的关键问题，显著提升生成图像的质量和准确性

Abstract: Personalized image generation, where reference images of one or more subjects are used to generate their image according to a scene description, has gathered significant interest in the community. However, such generated images suffer from three major limitations -- complex activities, such as $<$man, pushing, motorcycle$>$ are not generated properly with incorrect human poses, reference human identities are not preserved, and generated human gaze patterns are unnatural/inconsistent with the scene description. In this work, we propose to overcome these shortcomings through feedback-based fine-tuning of existing personalized generation methods, wherein, state-of-art detectors of pose, human-object-interaction, human facial recognition and human gaze-point estimation are used to refine the diffusion model. We also propose timestep-based inculcation of different feedback modules, depending upon whether the signal is low-level (such as human pose), or high-level (such as gaze point). The images generated in this manner show an improvement in the generated interactions, facial identities and image quality over three benchmark datasets.

</details>


### [4] [PUSA V1.0: Surpassing Wan-I2V with $500 Training Cost by Vectorized Timestep Adaptation](https://arxiv.org/abs/2507.16116)
*Yaofang Liu,Yumeng Ren,Aitor Artola,Yuxuan Hu,Xiaodong Cun,Xiaotong Zhao,Alan Zhao,Raymond H. Chan,Suiyun Zhang,Rui Liu,Dandan Tu,Jean-Michel Morel*

Main category: cs.CV

TL;DR: 本文提出了Pusa，一个使用向量化时间步适应(VTA)的视频扩散模型，实现了高效的图像到视频生成，以极低的训练成本(仅需1/200的费用和1/2500的数据量)超越了现有最先进模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在时间建模方面存在根本性限制，特别是传统标量时间步变量导致的帧演化刚性同步问题。虽然有任务特定适应和自回归模型试图解决这些挑战，但它们仍受到计算效率低下、灾难性遗忘或适用性狭窄的限制。

Method: 提出Pusa范式，采用向量化时间步适应(VTA)技术，在统一的视频扩散框架内实现细粒度时间控制。VTA是一种非破坏性适应方法，完全保留基础模型的能力。通过对最先进的Wan2.1-T2V-14B模型进行VTA微调来实现。

Result: 在图像到视频生成任务上取得了VBench-I2V总分87.32%的成绩(超过Wan-I2V-14B的86.86%)，同时训练成本仅为原来的1/200($500 vs. ≥$100,000)，数据集大小仅为1/2500(4K vs. ≥10M样本)。此外还实现了零样本多任务能力，如起始-结束帧和视频扩展等功能。

Conclusion: 该工作建立了一个可扩展、高效且多功能的下一代视频合成范式，为研究和工业界民主化高保真视频生成。机制分析表明该方法在保留基础模型生成先验的同时精确注入时间动态，避免了向量化时间步固有的组合爆炸问题。

Abstract: The rapid advancement of video diffusion models has been hindered by fundamental limitations in temporal modeling, particularly the rigid synchronization of frame evolution imposed by conventional scalar timestep variables. While task-specific adaptations and autoregressive models have sought to address these challenges, they remain constrained by computational inefficiency, catastrophic forgetting, or narrow applicability. In this work, we present Pusa, a groundbreaking paradigm that leverages vectorized timestep adaptation (VTA) to enable fine-grained temporal control within a unified video diffusion framework. Besides, VTA is a non-destructive adaptation, which means it fully preserves the capabilities of the base model. By finetuning the SOTA Wan2.1-T2V-14B model with VTA, we achieve unprecedented efficiency -- surpassing the performance of Wan-I2V-14B with $\leq$ 1/200 of the training cost (\$500 vs. $\geq$ \$100,000) and $\leq$ 1/2500 of the dataset size (4K vs. $\geq$ 10M samples). Pusa not only sets a new standard for image-to-video (I2V) generation, achieving a VBench-I2V total score of 87.32\% (vs. 86.86\% of Wan-I2V-14B), but also unlocks many zero-shot multi-task capabilities such as start-end frames and video extension -- all without task-specific training. Meanwhile, Pusa can still perform text-to-video generation. Mechanistic analyses reveal that our approach preserves the foundation model's generative priors while surgically injecting temporal dynamics, avoiding the combinatorial explosion inherent to vectorized timesteps. This work establishes a scalable, efficient, and versatile paradigm for next-generation video synthesis, democratizing high-fidelity video generation for research and industry alike. Code is open-sourced at https://github.com/Yaofang-Liu/Pusa-VidGen

</details>


### [5] [LongSplat: Online Generalizable 3D Gaussian Splatting from Long Sequence Images](https://arxiv.org/abs/2507.16144)
*Guichen Huang,Ruoyu Wang,Xiangjun Gao,Che Sun,Yuwei Wu,Shenghua Gao,Yunde Jia*

Main category: cs.CV

TL;DR: 提出了LongSplat，一个针对长序列图像输入的在线实时3D高斯重建框架，通过流式更新机制和高斯图像表示(GIR)实现高效的增量更新和冗余压缩，在保持实时性能的同时减少44%的高斯数量。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯喷射方法在在线长序列场景中应用受限，要么依赖缓慢的逐场景优化，要么无法提供高效的增量更新，阻碍了连续性能表现。需要一种能够处理长序列输入的实时3D重建方法。

Method: 提出流式更新机制，逐步整合当前视图观测并选择性压缩冗余的历史高斯；设计高斯图像表示(GIR)，将3D高斯参数编码为结构化的类图像2D格式；利用现有图像压缩方法指导生成更紧凑、更高质量的3D高斯。

Result: 在实时新视图合成中实现了最先进的效率-质量权衡，提供实时重建能力，同时相比现有的逐像素高斯预测方法减少了44%的高斯数量，有效控制了内存和计算成本。

Conclusion: LongSplat成功解决了3D高斯喷射在长序列场景中的应用限制，通过创新的流式更新机制和高斯图像表示，实现了高效的在线3D重建，为实时新视图合成提供了优秀的效率-质量平衡解决方案。

Abstract: 3D Gaussian Splatting achieves high-fidelity novel view synthesis, but its application to online long-sequence scenarios is still limited. Existing methods either rely on slow per-scene optimization or fail to provide efficient incremental updates, hindering continuous performance. In this paper, we propose LongSplat, an online real-time 3D Gaussian reconstruction framework designed for long-sequence image input. The core idea is a streaming update mechanism that incrementally integrates current-view observations while selectively compressing redundant historical Gaussians. Crucial to this mechanism is our Gaussian-Image Representation (GIR), a representation that encodes 3D Gaussian parameters into a structured, image-like 2D format. GIR simultaneously enables efficient fusion of current-view and historical Gaussians and identity-aware redundancy compression. These functions enable online reconstruction and adapt the model to long sequences without overwhelming memory or computational costs. Furthermore, we leverage an existing image compression method to guide the generation of more compact and higher-quality 3D Gaussians. Extensive evaluations demonstrate that LongSplat achieves state-of-the-art efficiency-quality trade-offs in real-time novel view synthesis, delivering real-time reconstruction while reducing Gaussian counts by 44\% compared to existing per-pixel Gaussian prediction methods.

</details>


### [6] [LSSGen: Leveraging Latent Space Scaling in Flow and Diffusion for Efficient Text to Image Generation](https://arxiv.org/abs/2507.16154)
*Jyun-Ze Tang,Chih-Fan Hsu,Jeng-Lin Li,Ming-Ching Chang,Wei-Chao Chen*

Main category: cs.CV

TL;DR: 提出了潜在空间缩放生成(LSSGen)框架，通过在潜在空间而非像素空间进行分辨率缩放来改进文本到图像生成的效率和质量，避免了传统上下采样方法引入的伪影和失真问题。


<details>
  <summary>Details</summary>
Motivation: 传统的文本到图像生成模型虽然效果出色，但为了加速合成通常在低分辨率下进行早期去噪，然后在像素空间进行下采样和上采样。这种方法会引入伪影和失真，当上采样图像重新编码到潜在空间时会导致最终图像质量下降。

Method: 提出潜在空间缩放生成(LSSGen)框架，使用轻量级潜在上采样器直接在潜在空间执行分辨率缩放，而无需修改Transformer或U-Net架构。该方法避免了像素空间缩放的问题，支持灵活的多分辨率生成。

Result: 在生成1024²图像时达到相似速度的情况下，LSSGen相比传统缩放方法在TOPIQ评分上实现了高达246%的改进。综合评估显示在文本-图像对齐和感知质量方面都显著优于传统缩放方法。

Conclusion: LSSGen框架通过在潜在空间直接进行分辨率缩放，成功解决了传统像素空间缩放方法的伪影和失真问题，在保持生成效率的同时显著提升了视觉质量，为文本到图像生成提供了更优的解决方案。

Abstract: Flow matching and diffusion models have shown impressive results in text-to-image generation, producing photorealistic images through an iterative denoising process. A common strategy to speed up synthesis is to perform early denoising at lower resolutions. However, traditional methods that downscale and upscale in pixel space often introduce artifacts and distortions. These issues arise when the upscaled images are re-encoded into the latent space, leading to degraded final image quality. To address this, we propose {\bf Latent Space Scaling Generation (LSSGen)}, a framework that performs resolution scaling directly in the latent space using a lightweight latent upsampler. Without altering the Transformer or U-Net architecture, LSSGen improves both efficiency and visual quality while supporting flexible multi-resolution generation. Our comprehensive evaluation covering text-image alignment and perceptual quality shows that LSSGen significantly outperforms conventional scaling approaches. When generating $1024^2$ images at similar speeds, it achieves up to 246\% TOPIQ score improvement.

</details>


### [7] [Explicit Context Reasoning with Supervision for Visual Tracking](https://arxiv.org/abs/2507.16191)
*Fansheng Zeng,Bineng Zhong,Haiying Xia,Yufei Tan,Xiantao Hu,Liangtao Shi,Shuxiang Song*

Main category: cs.CV

TL;DR: RSTrack是一个视觉跟踪算法，通过显式建模和监督上下文推理来增强跨帧建模中的时间一致性，在多个基准数据集上达到最先进性能并保持实时运行速度。


<details>
  <summary>Details</summary>
Motivation: 主流跟踪算法通常仅通过堆叠历史信息来关联上下文，而没有显式监督关联过程，这使得难以有效建模目标的演化动态，导致上下文关联发散问题。

Method: 提出RSTrack算法，包含三个核心机制：1）上下文推理机制：构建目标状态推理管道，将无约束的上下文关联转换为基于历史目标状态预测当前表示的时间推理过程；2）前向监督策略：利用真实目标特征作为锚点约束推理管道，引导预测输出趋向真实目标分布；3）高效状态建模：采用压缩-重构机制提取目标核心特征，去除跨帧冗余信息。

Result: 在多个基准数据集上达到最先进性能，同时保持实时运行速度。三个机制协同工作有效缓解了传统时间建模中的上下文关联发散问题。

Conclusion: RSTrack通过显式建模和监督上下文推理，有效解决了视觉跟踪中的时间一致性问题，在性能和效率方面都取得了显著改进，为视觉跟踪领域提供了新的解决方案。

Abstract: Contextual reasoning with constraints is crucial for enhancing temporal consistency in cross-frame modeling for visual tracking. However, mainstream tracking algorithms typically associate context by merely stacking historical information without explicitly supervising the association process, making it difficult to effectively model the target's evolving dynamics. To alleviate this problem, we propose RSTrack, which explicitly models and supervises context reasoning via three core mechanisms. \textit{1) Context Reasoning Mechanism}: Constructs a target state reasoning pipeline, converting unconstrained contextual associations into a temporal reasoning process that predicts the current representation based on historical target states, thereby enhancing temporal consistency. \textit{2) Forward Supervision Strategy}: Utilizes true target features as anchors to constrain the reasoning pipeline, guiding the predicted output toward the true target distribution and suppressing drift in the context reasoning process. \textit{3) Efficient State Modeling}: Employs a compression-reconstruction mechanism to extract the core features of the target, removing redundant information across frames and preventing ineffective contextual associations. These three mechanisms collaborate to effectively alleviate the issue of contextual association divergence in traditional temporal modeling. Experimental results show that RSTrack achieves state-of-the-art performance on multiple benchmark datasets while maintaining real-time running speeds. Our code is available at https://github.com/GXNU-ZhongLab/RSTrack.

</details>


### [8] [Scale Your Instructions: Enhance the Instruction-Following Fidelity of Unified Image Generation Model by Self-Adaptive Attention Scaling](https://arxiv.org/abs/2507.16240)
*Chao Zhou,Tianyi Wei,Nenghai Yu*

Main category: cs.CV

TL;DR: 该论文针对统一图像生成模型（如OmniGen）在处理多子指令时存在的文本指令忽略问题，提出了自适应注意力缩放（SaaS）方法，通过动态调整注意力激活来提高指令遵循保真度，无需额外训练即可有效改善多子指令的执行效果。


<details>
  <summary>Details</summary>
Motivation: 统一图像生成模型虽然能在单一框架内处理多样化的图像生成和编辑任务，但存在文本指令忽略问题，特别是当文本指令包含多个子指令时，模型无法准确遵循所有指令内容，影响了用户体验和模型的实用性。

Method: 提出自适应注意力缩放（SaaS）方法，首先通过扰动分析识别关键步骤和层，然后检查交叉注意力图发现被忽略的子指令与输入图像激活之间的冲突，最后利用相邻时间步之间交叉注意力的一致性来动态缩放每个子指令的注意力激活。

Result: 在基于指令的图像编辑和视觉条件图像生成任务上的实验结果表明，SaaS方法在指令遵循保真度方面优于现有方法，能够有效解决多子指令处理中的文本指令忽略问题。

Conclusion: SaaS方法成功解决了统一图像生成模型中的文本指令忽略问题，特别是多子指令场景下的处理能力，该方法无需额外训练或测试时优化，为统一图像生成模型的指令遵循能力提供了有效的改进方案。

Abstract: Recent advancements in unified image generation models, such as OmniGen, have enabled the handling of diverse image generation and editing tasks within a single framework, accepting multimodal, interleaved texts and images in free form. This unified architecture eliminates the need for text encoders, greatly reducing model complexity and standardizing various image generation and editing tasks, making it more user-friendly. However, we found that it suffers from text instruction neglect, especially when the text instruction contains multiple sub-instructions. To explore this issue, we performed a perturbation analysis on the input to identify critical steps and layers. By examining the cross-attention maps of these key steps, we observed significant conflicts between neglected sub-instructions and the activations of the input image. In response, we propose Self-Adaptive Attention Scaling (SaaS), a method that leverages the consistency of cross-attention between adjacent timesteps to dynamically scale the attention activation for each sub-instruction. Our SaaS enhances instruction-following fidelity without requiring additional training or test-time optimization. Experimental results on instruction-based image editing and visual conditional image generation validate the effectiveness of our SaaS, showing superior instruction-following fidelity over existing methods. The code is available https://github.com/zhouchao-ops/SaaS.

</details>


### [9] [MotionShot: Adaptive Motion Transfer across Arbitrary Objects for Text-to-Video Generation](https://arxiv.org/abs/2507.16310)
*Yanchen Liu,Yanan Sun,Zhening Xing,Junyao Gao,Kai Chen,Wenjie Pei*

Main category: cs.CV

TL;DR: MotionShot是一个无需训练的框架，能够在外观或结构差异显著的物体间实现高保真度的运动迁移，通过语义特征匹配和形态学对齐来解决现有文本到视频方法在运动迁移方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到视频方法在处理外观或结构差异显著的参考物体和目标物体之间的运动迁移时存在困难，无法实现平滑的运动传递，这限制了视频生成的质量和一致性。

Method: MotionShot采用无需训练的框架，首先通过语义特征匹配确保参考物体和目标物体之间的高级对齐，然后通过参考到目标的形状重定向建立低级形态学对齐，最后使用时间注意力机制编码运动信息，实现精细化的参考-目标对应关系解析。

Result: 通过广泛的实验验证，MotionShot能够在存在显著外观和结构差异的情况下，在不同物体间连贯地传递运动，同时保持外观的一致性，实现了高保真度的运动迁移效果。

Conclusion: MotionShot成功解决了现有文本到视频方法在跨物体运动迁移方面的挑战，通过多层次的对齐策略和时间注意力机制，为视频生成领域的运动传递问题提供了有效的解决方案。

Abstract: Existing text-to-video methods struggle to transfer motion smoothly from a reference object to a target object with significant differences in appearance or structure between them. To address this challenge, we introduce MotionShot, a training-free framework capable of parsing reference-target correspondences in a fine-grained manner, thereby achieving high-fidelity motion transfer while preserving coherence in appearance. To be specific, MotionShot first performs semantic feature matching to ensure high-level alignments between the reference and target objects. It then further establishes low-level morphological alignments through reference-to-target shape retargeting. By encoding motion with temporal attention, our MotionShot can coherently transfer motion across objects, even in the presence of significant appearance and structure disparities, demonstrated by extensive experiments. The project page is available at: https://motionshot.github.io/.

</details>


### [10] [Scene Text Detection and Recognition "in light of" Challenging Environmental Conditions using Aria Glasses Egocentric Vision Cameras](https://arxiv.org/abs/2507.16330)
*Joseph De Mathia,Carlos Francisco Moreno-García*

Main category: cs.CV

TL;DR: 本研究使用Meta的Project Aria智能眼镜探索环境变量对场景文本检测识别(STDR)算法性能的影响，发现分辨率和距离是关键因素，图像上采样能显著提升识别准确率，并展示了眼动追踪优化处理效率的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴技术的发展，从第一人称视角进行场景文本检测识别成为重要应用方向。现有研究缺乏对真实环境条件下STDR算法性能的系统评估，特别是光照、距离、分辨率等环境变量对算法性能的具体影响机制尚不明确。

Method: 使用Meta的Project Aria智能眼镜在受控条件下构建定制数据集，评估两种OCR流水线(EAST+CRNN和EAST+PyTesseract)的性能。系统分析光照、距离和分辨率等环境变量对STDR算法的影响，并探索图像上采样等预处理技术的效果。同时集成眼动追踪技术来优化处理效率。

Result: 研究发现分辨率和距离对识别准确率有显著影响，而光照的作用较难预测。图像上采样作为关键预处理技术，能将字符错误率(CER)从0.65降低到0.48。眼动追踪技术通过聚焦用户注意区域展现出优化处理效率的潜力。

Conclusion: 本工作在真实条件下对STDR性能进行了基准测试，为开发自适应、用户感知的AR系统奠定了基础。研究成果旨在推动面向辅助应用和研究导向应用(如资产检查和营养分析)的鲁棒、上下文敏感文本识别技术的未来发展。

Abstract: In an era where wearable technology is reshaping applications, Scene Text Detection and Recognition (STDR) becomes a straightforward choice through the lens of egocentric vision. Leveraging Meta's Project Aria smart glasses, this paper investigates how environmental variables, such as lighting, distance, and resolution, affect the performance of state-of-the-art STDR algorithms in real-world scenarios. We introduce a novel, custom-built dataset captured under controlled conditions and evaluate two OCR pipelines: EAST with CRNN, and EAST with PyTesseract. Our findings reveal that resolution and distance significantly influence recognition accuracy, while lighting plays a less predictable role. Notably, image upscaling emerged as a key pre-processing technique, reducing Character Error Rate (CER) from 0.65 to 0.48. We further demonstrate the potential of integrating eye-gaze tracking to optimise processing efficiency by focusing on user attention zones. This work not only benchmarks STDR performance under realistic conditions but also lays the groundwork for adaptive, user-aware AR systems. Our contributions aim to inspire future research in robust, context-sensitive text recognition for assistive and research-oriented applications, such as asset inspection and nutrition analysis. The code is available at https://github.com/josepDe/Project_Aria_STR.

</details>


### [11] [Navigating Large-Pose Challenge for High-Fidelity Face Reenactment with Video Diffusion Model](https://arxiv.org/abs/2507.16341)
*Mingtao Guo,Guanyu Xing,Yanci Zhang,Yanli Liu*

Main category: cs.CV

TL;DR: 本文提出了面部重演视频扩散模型(FRVD)，通过运动提取器和变形特征映射器解决大姿态变化下的面部重演问题，在姿态准确性、身份保持和视觉质量方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的面部重演方法在处理大姿态变化时存在困难，基于隐式或显式关键点的方法会产生变形伪影或受到粗糙面部标志点的限制，难以在极端姿态变化下保持高保真度的面部重演效果。

Method: 提出FRVD框架，包含：1)运动提取器从源图像和驱动图像中提取隐式面部关键点来表示细粒度运动；2)通过变形模块进行运动对齐；3)引入变形特征映射器(WFM)将变形后的源图像映射到预训练图像到视频模型的运动感知潜在空间中，利用大规模视频数据学习的面部动态先验知识。

Result: 在姿态准确性、身份保持和视觉质量方面均优于现有方法，特别在极端姿态变化的挑战性场景中表现出色，能够有效纠正变形伪影并增强时间连贯性。

Conclusion: FRVD通过结合运动提取、变形对齐和特征映射技术，成功解决了大姿态变化下的高保真面部重演问题，为面部重演领域提供了新的有效解决方案。

Abstract: Face reenactment aims to generate realistic talking head videos by transferring motion from a driving video to a static source image while preserving the source identity. Although existing methods based on either implicit or explicit keypoints have shown promise, they struggle with large pose variations due to warping artifacts or the limitations of coarse facial landmarks. In this paper, we present the Face Reenactment Video Diffusion model (FRVD), a novel framework for high-fidelity face reenactment under large pose changes. Our method first employs a motion extractor to extract implicit facial keypoints from the source and driving images to represent fine-grained motion and to perform motion alignment through a warping module. To address the degradation introduced by warping, we introduce a Warping Feature Mapper (WFM) that maps the warped source image into the motion-aware latent space of a pretrained image-to-video (I2V) model. This latent space encodes rich priors of facial dynamics learned from large-scale video data, enabling effective warping correction and enhancing temporal coherence. Extensive experiments show that FRVD achieves superior performance over existing methods in terms of pose accuracy, identity preservation, and visual quality, especially in challenging scenarios with extreme pose variations.

</details>


### [12] [STAR: A Benchmark for Astronomical Star Fields Super-Resolution](https://arxiv.org/abs/2507.16385)
*Kuo-Cheng Wu,Guohang Zhuang,Jinyang Huang,Xiang Zhang,Wanli Ouyang,Yan Lu*

Main category: cs.CV

TL;DR: 本文提出了STAR数据集，这是一个包含54,738对通量一致的星场图像对的大规模天文超分辨率数据集，并开发了通量不变超分辨率(FISR)模型，在通量一致性指标上比现有方法提升24.84%


<details>
  <summary>Details</summary>
Motivation: 现有天文超分辨率数据集存在三个关键限制：通量不一致、对象裁剪设置和数据多样性不足，这些问题严重阻碍了天文超分辨率技术的发展

Method: 提出STAR数据集，包含哈勃空间望远镜高分辨率观测数据和通过通量保持数据生成管道生成的物理真实低分辨率对应图像；引入通量误差(FE)评估指标；开发通量不变超分辨率(FISR)模型

Result: STAR数据集包含54,738对通量一致的星场图像对，覆盖广泛天体区域；FISR模型在通量一致性指标上比最先进的超分辨率方法提升24.84%；广泛实验证明了方法的有效性和数据集的价值

Conclusion: 通过构建大规模通量一致的天文超分辨率数据集STAR和开发FISR模型，显著提升了天文超分辨率的性能，为天体物理学应用提供了优先级更高的解决方案

Abstract: Super-resolution (SR) advances astronomical imaging by enabling cost-effective high-resolution capture, crucial for detecting faraway celestial objects and precise structural analysis. However, existing datasets for astronomical SR (ASR) exhibit three critical limitations: flux inconsistency, object-crop setting, and insufficient data diversity, significantly impeding ASR development. We propose STAR, a large-scale astronomical SR dataset containing 54,738 flux-consistent star field image pairs covering wide celestial regions. These pairs combine Hubble Space Telescope high-resolution observations with physically faithful low-resolution counterparts generated through a flux-preserving data generation pipeline, enabling systematic development of field-level ASR models. To further empower the ASR community, STAR provides a novel Flux Error (FE) to evaluate SR models in physical view. Leveraging this benchmark, we propose a Flux-Invariant Super Resolution (FISR) model that could accurately infer the flux-consistent high-resolution images from input photometry, suppressing several SR state-of-the-art methods by 24.84% on a novel designed flux consistency metric, showing the priority of our method for astrophysics. Extensive experiments demonstrate the effectiveness of our proposed method and the value of our dataset. Code and models are available at https://github.com/GuoCheng12/STAR.

</details>


### [13] [Sparse-View 3D Reconstruction: Recent Advances and Open Challenges](https://arxiv.org/abs/2507.16406)
*Tanveer Younis,Zhanglin Cheng*

Main category: cs.CV

TL;DR: 这篇综述论文系统回顾了稀疏视角3D重建的最新进展，包括神经隐式模型(如NeRF)、显式点云方法(如3DGS)和结合扩散模型的混合框架，分析了它们在解决稀疏视角重建挑战方面的优缺点和权衡关系。


<details>
  <summary>Details</summary>
Motivation: 在机器人技术、AR/VR和自动驾驶等应用中，密集图像采集往往不现实，稀疏视角下图像重叠度极小导致传统SfM和MVS方法失效，因此需要综述和分析能够处理稀疏视角3D重建问题的新兴方法。

Method: 采用综述研究方法，系统分析了三类主要方法：1)神经隐式模型(NeRF及其正则化版本)；2)显式点云方法(如3D高斯溅射)；3)结合扩散模型和视觉基础模型先验的混合框架。重点分析几何正则化、显式形状建模和生成推理如何缓解稀疏视角设置中的漂浮物和姿态歧义等问题。

Result: 通过标准基准测试的对比结果，揭示了重建精度、效率和泛化能力之间的关键权衡关系。与以往综述不同，本文提供了基于几何、神经隐式和生成(基于扩散)方法的统一视角，识别了领域泛化和无姿态重建等持续挑战。

Conclusion: 强调了开发3D原生生成先验和实现实时、无约束稀疏视角重建的未来发展方向，为该领域的进一步研究提供了清晰的路线图和指导。

Abstract: Sparse-view 3D reconstruction is essential for applications in which dense image acquisition is impractical, such as robotics, augmented/virtual reality (AR/VR), and autonomous systems. In these settings, minimal image overlap prevents reliable correspondence matching, causing traditional methods, such as structure-from-motion (SfM) and multiview stereo (MVS), to fail. This survey reviews the latest advances in neural implicit models (e.g., NeRF and its regularized versions), explicit point-cloud-based approaches (e.g., 3D Gaussian Splatting), and hybrid frameworks that leverage priors from diffusion and vision foundation models (VFMs).We analyze how geometric regularization, explicit shape modeling, and generative inference are used to mitigate artifacts such as floaters and pose ambiguities in sparse-view settings. Comparative results on standard benchmarks reveal key trade-offs between the reconstruction accuracy, efficiency, and generalization. Unlike previous reviews, our survey provides a unified perspective on geometry-based, neural implicit, and generative (diffusion-based) methods. We highlight the persistent challenges in domain generalization and pose-free reconstruction and outline future directions for developing 3D-native generative priors and achieving real-time, unconstrained sparse-view reconstruction.

</details>


### [14] [Robust Noisy Pseudo-label Learning for Semi-supervised Medical Image Segmentation Using Diffusion Model](https://arxiv.org/abs/2507.16429)
*Lin Xi,Yingliang Ma,Cheng Wang,Sandra Howell,Aldo Rinaldi,Kawal S. Rhode*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的半监督医学图像分割框架，通过原型对比一致性约束改善语义分布结构，并引入了新的X射线血管造影视频多目标分割基准数据集MOSXAV。


<details>
  <summary>Details</summary>
Motivation: 医学领域像素级标注既昂贵又耗时，需要临床专家与开发者密切合作。现有半监督方法由于伪标签噪声，在潜在空间中难以构建良好的语义分布结构。

Method: 提出基于扩散模型的半监督医学图像分割框架，在去噪扩散过程中通过强制执行基于原型的对比一致性来约束语义标签的潜在结构。使用类原型作为潜在空间中集中语义表示的锚点，而非显式划定语义边界。

Result: 在EndoScapes2023和MOSXAV数据集上的大量实验表明，该方法在半监督学习设置下优于最先进的医学图像分割方法。同时引入了新的公开基准数据集MOSXAV，为X射线血管造影视频中的多个解剖结构提供详细的手动标注分割真值。

Conclusion: 提出了一个鲁棒且数据高效的扩散模型，提供了增强的灵活性，在广泛的临床应用中具有强大潜力。该方法特别在存在噪声伪标签时改善了密集预测的鲁棒性。

Abstract: Obtaining pixel-level annotations in the medical domain is both expensive and time-consuming, often requiring close collaboration between clinical experts and developers. Semi-supervised medical image segmentation aims to leverage limited annotated data alongside abundant unlabeled data to achieve accurate segmentation. However, existing semi-supervised methods often struggle to structure semantic distributions in the latent space due to noise introduced by pseudo-labels. In this paper, we propose a novel diffusion-based framework for semi-supervised medical image segmentation. Our method introduces a constraint into the latent structure of semantic labels during the denoising diffusion process by enforcing prototype-based contrastive consistency. Rather than explicitly delineating semantic boundaries, the model leverages class prototypes centralized semantic representations in the latent space as anchors. This strategy improves the robustness of dense predictions, particularly in the presence of noisy pseudo-labels. We also introduce a new publicly available benchmark: Multi-Object Segmentation in X-ray Angiography Videos (MOSXAV), which provides detailed, manually annotated segmentation ground truth for multiple anatomical structures in X-ray angiography videos. Extensive experiments on the EndoScapes2023 and MOSXAV datasets demonstrate that our method outperforms state-of-the-art medical image segmentation approaches under the semi-supervised learning setting. This work presents a robust and data-efficient diffusion model that offers enhanced flexibility and strong potential for a wide range of clinical applications.

</details>


### [15] [DenseSR: Image Shadow Removal as Dense Prediction](https://arxiv.org/abs/2507.16472)
*Yu-Fan Lin,Chia-Ming Lee,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: 提出了DenseSR框架，通过几何语义先验和Dense Fusion Block解决单图像阴影去除中的内容降解不均匀和固有模糊性问题，实现高质量阴影去除


<details>
  <summary>Details</summary>
Motivation: 传统单图像阴影去除方法在间接光照等挑战性条件下，由于非均匀内容降解和固有模糊性，无法同时恢复阴影内部细节和保持清晰边界，导致恢复不一致和模糊问题，影响下游应用和视觉体验

Method: 提出DenseSR框架，采用密集预测视角，结合两个关键策略：(1)利用几何语义先验进行深度场景理解来解决模糊性并隐式定位阴影；(2)通过解码器中的Dense Fusion Block实现高保真恢复，DFB包含自适应内容平滑模块(ACSM)和纹理边界恢复模块(TBRM)，分别处理外观一致性和精细纹理边界

Result: 大量实验结果表明该方法相比现有方法具有优势，能够有效解决不一致恢复和模糊问题，在保持一致性的同时提高恢复保真度

Conclusion: DenseSR通过结合几何语义先验的深度场景理解和Dense Fusion Block的自适应组件处理，成功解决了单图像阴影去除中的关键挑战，实现了高质量的阴影去除效果

Abstract: Shadows are a common factor degrading image quality. Single-image shadow removal (SR), particularly under challenging indirect illumination, is hampered by non-uniform content degradation and inherent ambiguity. Consequently, traditional methods often fail to simultaneously recover intra-shadow details and maintain sharp boundaries, resulting in inconsistent restoration and blurring that negatively affect both downstream applications and the overall viewing experience. To overcome these limitations, we propose the DenseSR, approaching the problem from a dense prediction perspective to emphasize restoration quality. This framework uniquely synergizes two key strategies: (1) deep scene understanding guided by geometric-semantic priors to resolve ambiguity and implicitly localize shadows, and (2) high-fidelity restoration via a novel Dense Fusion Block (DFB) in the decoder. The DFB employs adaptive component processing-using an Adaptive Content Smoothing Module (ACSM) for consistent appearance and a Texture-Boundary Recuperation Module (TBRM) for fine textures and sharp boundaries-thereby directly tackling the inconsistent restoration and blurring issues. These purposefully processed components are effectively fused, yielding an optimized feature representation preserving both consistency and fidelity. Extensive experimental results demonstrate the merits of our approach over existing methods. Our code can be available on https://github$.$com/VanLinLin/DenseSR

</details>


### [16] [Dyna3DGR: 4D Cardiac Motion Tracking with Dynamic 3D Gaussian Representation](https://arxiv.org/abs/2507.16608)
*Xueming Fu,Pei Wu,Yingtai Li,Xin Luo,Zihang Jiang,Junhao Mei,Jian Lu,Gao-Jun Teng,S. Kevin Zhou*

Main category: cs.CV

TL;DR: 提出了Dynamic 3D Gaussian Representation (Dyna3DGR)框架，结合显式3D高斯表示和隐式神经运动场建模，用于精确的心脏运动分析，在ACDC数据集上超越了现有深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 心脏运动分析对评估心脏功能至关重要，但由于心肌组织的同质性和缺乏特征性标记，现有的基于图像和基于表示的方法都存在局限性：图像方法在拓扑一致性方面存在困难或需要大量训练数据，表示方法容易丢失图像级细节。

Method: 提出Dyna3DGR框架，结合显式3D高斯表示和隐式神经运动场建模。该方法通过自监督方式同时优化心脏结构和运动，采用可微分体积渲染技术，在保持拓扑和时间一致性的同时，有效连接连续运动表示与图像空间对齐。

Result: 在ACDC数据集上的综合评估显示，该方法在跟踪精度方面超越了现有的基于深度学习的微分同胚配准方法，无需大量训练数据或点对点对应关系。

Conclusion: Dyna3DGR成功解决了心脏运动跟踪中的关键挑战，通过结合显式和隐式表示的优势，在保持拓扑和时间一致性的同时实现了更高的跟踪精度，为心脏功能评估提供了有效的新方法。

Abstract: Accurate analysis of cardiac motion is crucial for evaluating cardiac function. While dynamic cardiac magnetic resonance imaging (CMR) can capture detailed tissue motion throughout the cardiac cycle, the fine-grained 4D cardiac motion tracking remains challenging due to the homogeneous nature of myocardial tissue and the lack of distinctive features. Existing approaches can be broadly categorized into image based and representation-based, each with its limitations. Image-based methods, including both raditional and deep learning-based registration approaches, either struggle with topological consistency or rely heavily on extensive training data. Representation-based methods, while promising, often suffer from loss of image-level details. To address these limitations, we propose Dynamic 3D Gaussian Representation (Dyna3DGR), a novel framework that combines explicit 3D Gaussian representation with implicit neural motion field modeling. Our method simultaneously optimizes cardiac structure and motion in a self-supervised manner, eliminating the need for extensive training data or point-to-point correspondences. Through differentiable volumetric rendering, Dyna3DGR efficiently bridges continuous motion representation with image-space alignment while preserving both topological and temporal consistency. Comprehensive evaluations on the ACDC dataset demonstrate that our approach surpasses state-of-the-art deep learning-based diffeomorphic registration methods in tracking accuracy. The code will be available in https://github.com/windrise/Dyna3DGR.

</details>


### [17] [HarmonPaint: Harmonized Training-Free Diffusion Inpainting](https://arxiv.org/abs/2507.16732)
*Ying Li,Xinzhe Li,Yong Du,Yangyang Xu,Junyu Dong,Shengfeng He*

Main category: cs.CV

TL;DR: HarmonPaint是一个无需训练的图像修复框架，通过利用扩散模型的注意力机制，在不进行任何训练或微调的情况下实现高质量、和谐的图像修复。


<details>
  <summary>Details</summary>
Motivation: 现有的图像修复方法通常需要大量的重新训练或微调才能无缝集成新内容，但在保持修复区域与周围背景在结构和风格上的一致性方面存在困难。

Method: HarmonPaint通过在自注意力机制中使用掩码策略来确保结构保真度，同时利用扩散模型的内在属性将风格信息从未掩码区域传递到掩码区域，实现风格的和谐融合。

Result: 广泛的实验证明了HarmonPaint在不同场景和风格下的有效性，验证了其多功能性和性能表现。

Conclusion: HarmonPaint成功实现了无需训练的高质量图像修复，在结构保真度和风格和谐性方面都表现出色，为图像修复领域提供了一种新的解决方案。

Abstract: Existing inpainting methods often require extensive retraining or fine-tuning to integrate new content seamlessly, yet they struggle to maintain coherence in both structure and style between inpainted regions and the surrounding background. Motivated by these limitations, we introduce HarmonPaint, a training-free inpainting framework that seamlessly integrates with the attention mechanisms of diffusion models to achieve high-quality, harmonized image inpainting without any form of training. By leveraging masking strategies within self-attention, HarmonPaint ensures structural fidelity without model retraining or fine-tuning. Additionally, we exploit intrinsic diffusion model properties to transfer style information from unmasked to masked regions, achieving a harmonious integration of styles. Extensive experiments demonstrate the effectiveness of HarmonPaint across diverse scenes and styles, validating its versatility and performance.

</details>


### [18] [Denoising-While-Completing Network (DWCNet): Robust Point Cloud Completion Under Corruption](https://arxiv.org/abs/2507.16743)
*Keneni W. Tesema,Lyndon Hill,Mark W. Jones,Gary K. L. Tam*

Main category: cs.CV

TL;DR: 本文提出了DWCNet网络来解决真实世界中噪声和遮挡导致的点云补全问题，并引入了CPCCD数据集来评估方法的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有的点云补全网络主要在合成数据上训练，在面对真实世界中的噪声和遮挡等多种退化时表现不佳，需要开发能够同时处理点云补全和去噪的鲁棒方法

Method: 提出DWCNet（边去噪边补全网络），包含噪声管理模块（NMM），该模块利用对比学习和自注意力机制来抑制噪声并建模结构关系；同时引入CPCCD数据集来评估不同退化条件下的方法性能

Result: DWCNet在干净和损坏的合成数据集以及真实世界数据集上都达到了最先进的性能，证明了其在处理多种同时退化的点云补全任务中的有效性

Conclusion: DWCNet成功解决了真实世界点云补全中的噪声和遮挡问题，通过噪声管理模块有效提升了补全网络的鲁棒性，为自动驾驶、增强现实和机器人等应用提供了更可靠的3D视觉解决方案

Abstract: Point cloud completion is crucial for 3D computer vision tasks in autonomous driving, augmented reality, and robotics. However, obtaining clean and complete point clouds from real-world environments is challenging due to noise and occlusions. Consequently, most existing completion networks -- trained on synthetic data -- struggle with real-world degradations. In this work, we tackle the problem of completing and denoising highly corrupted partial point clouds affected by multiple simultaneous degradations. To benchmark robustness, we introduce the Corrupted Point Cloud Completion Dataset (CPCCD), which highlights the limitations of current methods under diverse corruptions. Building on these insights, we propose DWCNet (Denoising-While-Completing Network), a completion framework enhanced with a Noise Management Module (NMM) that leverages contrastive learning and self-attention to suppress noise and model structural relationships. DWCNet achieves state-of-the-art performance on both clean and corrupted, synthetic and real-world datasets. The dataset and code will be publicly available at https://github.com/keneniwt/DWCNET-Robust-Point-Cloud-Completion-against-Corruptions

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [19] [Systole-Conditioned Generative Cardiac Motion](https://arxiv.org/abs/2507.15894)
*Shahar Zuler,Gal Lifshitz,Hadar Averbuch-Elor,Dan Raviv*

Main category: eess.IV

TL;DR: 本文提出了一种基于条件变分自编码器(CVAE)的方法，用于合成带有密集3D流场标注的心脏CT图像对，以解决心脏运动估计中标注数据不足的问题


<details>
  <summary>Details</summary>
Motivation: 心脏CT成像中准确的运动估计对于评估心脏功能和手术规划至关重要，但数据驱动的密集运动估计方法需要大量带有密集真值运动标注的标记数据，而这些数据往往难以获得

Method: 提出了一种基于条件变分自编码器(CVAE)的新方法，该方法结合了新颖的多尺度特征条件机制，训练生成以单帧CT图像为条件的3D流场。通过将生成的流场应用于扭曲给定帧，创建模拟心肌在心动周期中真实变形的图像对

Result: 该方法能够生成具有真实外观的心脏CT图像对，并配有密集的3D流场标注，这些图像对可作为完全标注的数据样本，提供光流真值标注

Conclusion: 该数据生成管道能够支持训练和验证更复杂、更准确的心肌运动模型，显著减少对人工标注的依赖

Abstract: Accurate motion estimation in cardiac computed tomography (CT) imaging is critical for assessing cardiac function and surgical planning. Data-driven methods have become the standard approach for dense motion estimation, but they rely on vast amounts of labeled data with dense ground-truth (GT) motion annotations, which are often unfeasible to obtain. To address this limitation, we present a novel approach that synthesizes realistically looking pairs of cardiac CT frames enriched with dense 3D flow field annotations.   Our method leverages a conditional Variational Autoencoder (CVAE), which incorporates a novel multi-scale feature conditioning mechanism and is trained to generate 3D flow fields conditioned on a single CT frame. By applying the generated flow field to warp the given frame, we create pairs of frames that simulate realistic myocardium deformations across the cardiac cycle. These pairs serve as fully annotated data samples, providing optical flow GT annotations. Our data generation pipeline could enable the training and validation of more complex and accurate myocardium motion models, allowing for substantially reducing reliance on manual annotations.   Our code, along with animated generated samples and additional material, is available on our project page: https://shaharzuler.github.io/GenerativeCardiacMotion_Page.

</details>


### [20] [A High Magnifications Histopathology Image Dataset for Oral Squamous Cell Carcinoma Diagnosis and Prognosis](https://arxiv.org/abs/2507.16360)
*Jinquan Guan,Junhong Guo,Qi Chen,Jian Chen,Yongkang Cai,Yilin He,Zhiquan Huang,Yan Wang,Yutong Xie*

Main category: eess.IV

TL;DR: 研究者构建了包含1,325名口腔鳞状细胞癌患者的Multi-OSCC数据集，每位患者包含6张不同放大倍数的病理图像，涵盖6个临床任务的标注，并通过系统性评估发现最佳模型在复发预测上达到94.72%的AUC，但多任务学习相比单任务模型平均性能下降3.34%。


<details>
  <summary>Details</summary>
Motivation: 现有公开的口腔鳞状细胞癌数据集存在患者队列有限、仅关注诊断或预后单一任务的问题，限制了综合性和可泛化模型的开发，需要构建更全面的数据集来弥补这一空白。

Method: 构建了Multi-OSCC病理图像数据集，包含1,325名患者，每位患者有6张高分辨率病理图像（x200、x400、x1000三种放大倍数，每种倍数2张，覆盖肿瘤核心和边缘区域），标注了6个关键临床任务，并系统评估了不同视觉编码器、多图像融合技术、染色标准化和多任务学习框架的影响。

Result: 最佳模型在复发预测(REC)任务上达到94.72%的AUC，肿瘤分化(TD)任务达到81.23%的AUC，所有任务均超过70%的AUC；染色标准化对诊断任务有益但对复发预测有负面影响；多任务学习相比单任务模型平均AUC下降3.34%。

Conclusion: Multi-OSCC数据集为口腔鳞状细胞癌的计算机辅助诊断和预后提供了重要资源，基准测试揭示了不同技术方法的效果差异，特别是多任务学习在该数据集上面临的挑战，为未来研究提供了有价值的见解和公开资源。

Abstract: Oral Squamous Cell Carcinoma (OSCC) is a prevalent and aggressive malignancy where deep learning-based computer-aided diagnosis and prognosis can enhance clinical assessments.However, existing publicly available OSCC datasets often suffer from limited patient cohorts and a restricted focus on either diagnostic or prognostic tasks, limiting the development of comprehensive and generalizable models. To bridge this gap, we introduce Multi-OSCC, a new histopathology image dataset comprising 1,325 OSCC patients, integrating both diagnostic and prognostic information to expand existing public resources. Each patient is represented by six high resolution histopathology images captured at x200, x400, and x1000 magnifications-two per magnification-covering both the core and edge tumor regions.The Multi-OSCC dataset is richly annotated for six critical clinical tasks: recurrence prediction (REC), lymph node metastasis (LNM), tumor differentiation (TD), tumor invasion (TI), cancer embolus (CE), and perineural invasion (PI). To benchmark this dataset, we systematically evaluate the impact of different visual encoders, multi-image fusion techniques, stain normalization, and multi-task learning frameworks. Our analysis yields several key insights: (1) The top-performing models achieve excellent results, with an Area Under the Curve (AUC) of 94.72% for REC and 81.23% for TD, while all tasks surpass 70% AUC; (2) Stain normalization benefits diagnostic tasks but negatively affects recurrence prediction; (3) Multi-task learning incurs a 3.34% average AUC degradation compared to single-task models in our multi-task benchmark, underscoring the challenge of balancing multiple tasks in our dataset. To accelerate future research, we publicly release the Multi-OSCC dataset and baseline models at https://github.com/guanjinquan/OSCC-PathologyImageDataset.

</details>


### [21] [Pyramid Hierarchical Masked Diffusion Model for Imaging Synthesis](https://arxiv.org/abs/2507.16579)
*Xiaojiao Xiao,Qinmin Vivian Hu,Guanghui Wang*

Main category: eess.IV

TL;DR: 本文提出了PHMDiff（金字塔分层掩码扩散模型），这是一种新颖的医学图像合成网络，通过多尺度分层方法和随机多尺度高比例掩码来加速训练，在医学图像合成任务中实现了优异的性能表现。


<details>
  <summary>Details</summary>
Motivation: 医学图像合成在临床工作流程中起着关键作用，主要解决由于扫描时间延长、扫描损坏、伪影、患者运动和对比剂不耐受等因素导致的成像模态缺失问题。现有方法在生成高质量、结构完整的医学图像方面仍存在挑战。

Method: 提出了金字塔分层掩码扩散模型（PHMDiff），采用多尺度分层方法对不同分辨率和层次的高质量图像合成进行更精细的控制。模型利用随机多尺度高比例掩码加速扩散模型训练，平衡细节保真度和整体结构。集成基于Transformer的扩散模型过程，结合跨粒度正则化，建模各粒度潜在空间的互信息一致性，提升像素级感知精度。

Result: 在两个具有挑战性的数据集上进行的综合实验表明，PHMDiff在峰值信噪比（PSNR）和结构相似性指数测量（SSIM）方面都取得了优异性能，展现了其生成具有出色结构完整性的高质量合成图像的能力。消融研究进一步证实了各组件的贡献。

Conclusion: PHMDiff模型作为一个跨医学成像模态和模态内的多尺度图像合成框架，相比其他方法显示出显著优势。该模型在医学图像合成领域具有重要的应用价值，能够有效解决临床实践中的成像模态缺失问题。

Abstract: Medical image synthesis plays a crucial role in clinical workflows, addressing the common issue of missing imaging modalities due to factors such as extended scan times, scan corruption, artifacts, patient motion, and intolerance to contrast agents. The paper presents a novel image synthesis network, the Pyramid Hierarchical Masked Diffusion Model (PHMDiff), which employs a multi-scale hierarchical approach for more detailed control over synthesizing high-quality images across different resolutions and layers. Specifically, this model utilizes randomly multi-scale high-proportion masks to speed up diffusion model training, and balances detail fidelity and overall structure. The integration of a Transformer-based Diffusion model process incorporates cross-granularity regularization, modeling the mutual information consistency across each granularity's latent spaces, thereby enhancing pixel-level perceptual accuracy. Comprehensive experiments on two challenging datasets demonstrate that PHMDiff achieves superior performance in both the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM), highlighting its capability to produce high-quality synthesized images with excellent structural integrity. Ablation studies further confirm the contributions of each component. Furthermore, the PHMDiff model, a multi-scale image synthesis framework across and within medical imaging modalities, shows significant advantages over other methods. The source code is available at https://github.com/xiaojiao929/PHMDiff

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [22] [Parallel Ray Tracing of Black Hole Images Using the Schwarzschild Metric](https://arxiv.org/abs/2507.16165)
*Liam Naddell,Marcelo Ponce*

Main category: cs.DC

TL;DR: 本文实现了一个并行开源程序，用于在黑洞几何环境中进行光线追踪渲染，结合了数学近似、科学库和并行计算技术。


<details>
  <summary>Details</summary>
Motivation: 黑洞图像渲染在科学和天体物理可视化中广泛应用，同时光线追踪技术在计算机图形学领域也有重要作用，需要一个高效的并行计算解决方案来处理黑洞几何环境下的复杂光线追踪计算。

Method: 结合多种并行科学计算技术，包括数学近似方法、科学计算库的使用、共享内存并行和分布式内存并行技术，开发并行开源程序来实现黑洞几何环境下的光线追踪。

Result: 成功实现了一个能够在黑洞几何条件下进行光线追踪的并行开源程序，该程序整合了多种并行计算技术和数学方法。

Conclusion: 通过结合数学近似、科学库和多种并行计算技术，成功开发了用于黑洞几何环境光线追踪的并行开源程序，为科学可视化和天体物理研究提供了有效工具。

Abstract: Rendering images of black holes by utilizing ray tracing techniques is a common methodology employed in many aspects of scientific and astrophysical visualizations. Similarly, general ray tracing techniques are widely used in areas related to computer graphics. In this work we describe the implementation of a parallel open-source program that can ray trace images in the presence of a black hole geometry. We do this by combining a couple of different techniques usually present in parallel scientific computing, such as, mathematical approximations, utilization of scientific libraries, shared-memory and distributed-memory parallelism.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [Towards Resilient Safety-driven Unlearning for Diffusion Models against Downstream Fine-tuning](https://arxiv.org/abs/2507.16302)
*Boheng Li,Renjie Gu,Junjie Wang,Leyi Qi,Yiming Li,Run Wang,Zhan Qin,Tianwei Zhang*

Main category: cs.LG

TL;DR: 本文提出了ResAlign框架，解决文本到图像扩散模型在个性化微调后安全性失效的问题，通过Moreau包络优化和元学习策略增强安全遗忘方法对下游微调的抗性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型虽然图像生成质量很高，但从有毒预训练数据中继承了不安全行为。虽然安全驱动的遗忘方法在抑制模型毒性方面取得了进展，但这些方法在下游微调时表现脆弱，即使在完全良性的数据集上微调也会失去有效性。

Method: 提出ResAlign框架，将下游微调建模为带Moreau包络重新表述的隐式优化问题，实现高效梯度估计以最小化有害行为的恢复。同时采用元学习策略模拟多样化的微调场景分布以提高泛化能力。

Result: 在广泛的数据集、微调方法和配置上的大量实验表明，ResAlign在保持下游微调后的安全性方面始终优于先前的遗忘方法，同时很好地保持了良性生成能力。

Conclusion: ResAlign框架有效解决了安全遗忘方法在下游微调中的脆弱性问题，通过创新的优化策略和元学习方法，成功增强了文本到图像模型安全性的持久性，为个性化AI应用的安全部署提供了重要保障。

Abstract: Text-to-image (T2I) diffusion models have achieved impressive image generation quality and are increasingly fine-tuned for personalized applications. However, these models often inherit unsafe behaviors from toxic pretraining data, raising growing safety concerns. While recent safety-driven unlearning methods have made promising progress in suppressing model toxicity, they are identified to be fragile to downstream fine-tuning, where we reveal that state-of-the-art methods largely fail to retain their effectiveness even when fine-tuned on entirely benign datasets. To mitigate this problem, in this paper, we propose ResAlign, a safety-driven unlearning framework with enhanced resilience against downstream fine-tuning. By modeling downstream fine-tuning as an implicit optimization problem with a Moreau Envelope-based reformulation, ResAlign enables efficient gradient estimation to minimize the recovery of harmful behaviors. Additionally, a meta-learning strategy is proposed to simulate a diverse distribution of fine-tuning scenarios to improve generalization. Extensive experiments across a wide range of datasets, fine-tuning methods, and configurations demonstrate that ResAlign consistently outperforms prior unlearning approaches in retaining safety after downstream fine-tuning while preserving benign generation capability well.

</details>
