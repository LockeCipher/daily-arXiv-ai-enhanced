<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 5]
- [cs.CV](#cs.CV) [Total: 8]
- [quant-ph](#quant-ph) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [LuxDiT: Lighting Estimation with Video Diffusion Transformer](https://arxiv.org/abs/2509.03680)
*Ruofan Liang,Kai He,Zan Gojcic,Igor Gilitschenski,Sanja Fidler,Nandita Vijaykumar,Zian Wang*

Main category: cs.GR

TL;DR: LuxDiT是一个基于视频扩散变换器的数据驱动方法，通过微调生成HDR环境光照图，解决了单图像/视频光照估计的挑战，在合成和真实场景中都表现出色。


<details>
  <summary>Details</summary>
Motivation: 学习式光照估计方法受限于真实HDR环境图的稀缺性，这些数据采集成本高且多样性有限。传统方法难以从间接视觉线索推断全局上下文并恢复高动态范围输出。

Method: 提出LuxDiT方法，微调视频扩散变换器来生成基于视觉输入的条件化HDR环境图。使用大型合成数据集训练，引入低秩适应微调策略提高输入与预测环境图之间的语义对齐。

Result: 该方法能够产生具有真实角度高频细节的准确光照预测，在定量和定性评估中都优于现有最先进技术，并能有效泛化到真实世界场景。

Conclusion: LuxDiT通过结合扩散变换器和精心设计的训练策略，成功解决了单图像光照估计的关键挑战，为计算机视觉和图形学中的光照恢复提供了有效的解决方案。

Abstract: Estimating scene lighting from a single image or video remains a longstanding challenge in computer vision and graphics. Learning-based approaches are constrained by the scarcity of ground-truth HDR environment maps, which are expensive to capture and limited in diversity. While recent generative models offer strong priors for image synthesis, lighting estimation remains difficult due to its reliance on indirect visual cues, the need to infer global (non-local) context, and the recovery of high-dynamic-range outputs. We propose LuxDiT, a novel data-driven approach that fine-tunes a video diffusion transformer to generate HDR environment maps conditioned on visual input. Trained on a large synthetic dataset with diverse lighting conditions, our model learns to infer illumination from indirect visual cues and generalizes effectively to real-world scenes. To improve semantic alignment between the input and the predicted environment map, we introduce a low-rank adaptation finetuning strategy using a collected dataset of HDR panoramas. Our method produces accurate lighting predictions with realistic angular high-frequency details, outperforming existing state-of-the-art techniques in both quantitative and qualitative evaluations.

</details>


### [2] [ContraGS: Codebook-Condensed and Trainable Gaussian Splatting for Fast, Memory-Efficient Reconstruction](https://arxiv.org/abs/2509.03775)
*Sankeerth Durvasula,Sharanshangar Muhunthan,Zain Moustafa,Richard Chen,Ruofan Liang,Yushi Guan,Nilesh Ahuja,Nilesh Jain,Selvakumar Panneer,Nandita Vijaykumar*

Main category: cs.GR

TL;DR: ContraGS是一种直接在压缩的3D高斯泼溅表示上进行训练的方法，通过使用码本存储高斯参数向量，显著减少内存消耗，同时保持接近最先进的质量。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅技术需要大量3D高斯来获得高质量表示，但这会显著增加GPU内存使用并降低训练/渲染效率。现有方法无法直接在压缩表示上进行训练。

Method: 使用码本紧凑存储高斯参数向量，将参数估计建模为贝叶斯推断问题，采用MCMC采样在压缩表示的后验分布上进行采样。

Result: 训练峰值内存平均减少3.49倍，训练和渲染速度分别平均提升1.36倍和1.88倍，同时保持接近最先进的质量。

Conclusion: ContraGS成功解决了在码本压缩表示上进行直接训练的挑战，实现了内存效率、训练速度和模型质量的良好平衡。

Abstract: 3D Gaussian Splatting (3DGS) is a state-of-art technique to model real-world scenes with high quality and real-time rendering. Typically, a higher quality representation can be achieved by using a large number of 3D Gaussians. However, using large 3D Gaussian counts significantly increases the GPU device memory for storing model parameters. A large model thus requires powerful GPUs with high memory capacities for training and has slower training/rendering latencies due to the inefficiencies of memory access and data movement. In this work, we introduce ContraGS, a method to enable training directly on compressed 3DGS representations without reducing the Gaussian Counts, and thus with a little loss in model quality. ContraGS leverages codebooks to compactly store a set of Gaussian parameter vectors throughout the training process, thereby significantly reducing memory consumption. While codebooks have been demonstrated to be highly effective at compressing fully trained 3DGS models, directly training using codebook representations is an unsolved challenge. ContraGS solves the problem of learning non-differentiable parameters in codebook-compressed representations by posing parameter estimation as a Bayesian inference problem. To this end, ContraGS provides a framework that effectively uses MCMC sampling to sample over a posterior distribution of these compressed representations. With ContraGS, we demonstrate that ContraGS significantly reduces the peak memory during training (on average 3.49X) and accelerated training and rendering (1.36X and 1.88X on average, respectively), while retraining close to state-of-art quality.

</details>


### [3] [TensoIS: A Step Towards Feed-Forward Tensorial Inverse Subsurface Scattering for Perlin Distributed Heterogeneous Media](https://arxiv.org/abs/2509.04047)
*Ashish Tiwari,Satyam Bhardwaj,Yash Bachwana,Parag Sarvoday Sahu,T. M. Feroz Ali,Bhargava Chintalapati,Shanmuganathan Raman*

Main category: cs.GR

TL;DR: 这篇论文提出了一种基于学习的前向框架TensoIS，通过Fractal Perlin噪声模型和低秩张量表示来估计异质媒介的散射参数，解决了传统方法在异质媒介逆向散射问题中的挑战。


<details>
  <summary>Details</summary>
Motivation: 异质媒介散射参数估计是个极其难的问题，现有方法多假设均质媒介，而真实世界中缺乏明确的异质散射参数分布模型。

Method: 首先创建HeteroSynth合成数据集，使用Fractal Perlin噪声模型异质散射参数；然后提出TensoIS框架，用可学习的低秩张量组件表示散射体积，从稀疏多视角图像估计参数。

Result: 在HeteroSynth测试集、烟雾云层几何体和实际样品上评估，证明了TensoIS在逆向散射问题中的有效性。

Conclusion: 这是首次尝试使用Perlin噪声分布来模拟真实世界异质散射的研究，为前向式逆向散射提供了新的解决方案。

Abstract: Estimating scattering parameters of heterogeneous media from images is a severely under-constrained and challenging problem. Most of the existing approaches model BSSRDF either through an analysis-by-synthesis approach, approximating complex path integrals, or using differentiable volume rendering techniques to account for heterogeneity. However, only a few studies have applied learning-based methods to estimate subsurface scattering parameters, but they assume homogeneous media. Interestingly, no specific distribution is known to us that can explicitly model the heterogeneous scattering parameters in the real world. Notably, procedural noise models such as Perlin and Fractal Perlin noise have been effective in representing intricate heterogeneities of natural, organic, and inorganic surfaces. Leveraging this, we first create HeteroSynth, a synthetic dataset comprising photorealistic images of heterogeneous media whose scattering parameters are modeled using Fractal Perlin noise. Furthermore, we propose Tensorial Inverse Scattering (TensoIS), a learning-based feed-forward framework to estimate these Perlin-distributed heterogeneous scattering parameters from sparse multi-view image observations. Instead of directly predicting the 3D scattering parameter volume, TensoIS uses learnable low-rank tensor components to represent the scattering volume. We evaluate TensoIS on unseen heterogeneous variations over shapes from the HeteroSynth test set, smoke and cloud geometries obtained from open-source realistic volumetric simulations, and some real-world samples to establish its effectiveness for inverse scattering. Overall, this study is an attempt to explore Perlin noise distribution, given the lack of any such well-defined distribution in literature, to potentially model real-world heterogeneous scattering in a feed-forward manner.

</details>


### [4] [SMooGPT: Stylized Motion Generation using Large Language Models](https://arxiv.org/abs/2509.04058)
*Lei Zhong,Yi Yang,Changjian Li*

Main category: cs.GR

TL;DR: 基于LLM的文本驱动风格化运动生成方法SMooGPT，通过身体部件文本空间中间表示实现高可解释性和精细控制


<details>
  <summary>Details</summary>
Motivation: 现有风格化运动生成方法存在低可解释性、控制限制、对新风格的沿生性差以及受数据集偏差影响只能生成"行走"运动等问题

Method: 提出理解-组合-生成的新视角，使用身体部件文本空间作为中间表示，基于细调的LLM模型SMooGPT作为理解器、组合器和生成器

Result: 方法在身体部件文本空间执行，具有更高的可解释性和精细控制能力，有效解决运动内容与风格的冲突，并利用LLM的开政词能力良好沿生到新风格

Conclusion: 综合实验、评估和用户感知研究证明了该方法的有效性，尤其在纯文本驱动的风格化运动生成任务中表现优异

Abstract: Stylized motion generation is actively studied in computer graphics, especially benefiting from the rapid advances in diffusion models. The goal of this task is to produce a novel motion respecting both the motion content and the desired motion style, e.g., ``walking in a loop like a Monkey''. Existing research attempts to address this problem via motion style transfer or conditional motion generation. They typically embed the motion style into a latent space and guide the motion implicitly in a latent space as well. Despite the progress, their methods suffer from low interpretability and control, limited generalization to new styles, and fail to produce motions other than ``walking'' due to the strong bias in the public stylization dataset. In this paper, we propose to solve the stylized motion generation problem from a new perspective of reasoning-composition-generation, based on our observations: i) human motion can often be effectively described using natural language in a body-part centric manner, ii) LLMs exhibit a strong ability to understand and reason about human motion, and iii) human motion has an inherently compositional nature, facilitating the new motion content or style generation via effective recomposing. We thus propose utilizing body-part text space as an intermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a reasoner, composer, and generator when generating the desired stylized motion. Our method executes in the body-part text space with much higher interpretability, enabling fine-grained motion control, effectively resolving potential conflicts between motion content and style, and generalizes well to new styles thanks to the open-vocabulary ability of LLMs. Comprehensive experiments and evaluations, and a user perceptual study, demonstrate the effectiveness of our approach, especially under the pure text-driven stylized motion generation.

</details>


### [5] [Hyper Diffusion Avatars: Dynamic Human Avatar Generation using Network Weight Space Diffusion](https://arxiv.org/abs/2509.04145)
*Dongliang Cao,Guoxing Sun,Marc Habermann,Florian Bernard*

Main category: cs.GR

TL;DR: 通过结合个人特定渲染和温度模型的优势，提出了一种能够生成具有高实体感咈位依赖变形的动态人体虚拟影像的方法。


<details>
  <summary>Details</summary>
Motivation: 解决个人特定渲染法无法跨身份通用的问题，以及生成式方法渲染质量低且无法抓取姿势依赖变形的限制。

Method: 两阶段流水线：首先优化一组个人特定UNet网络来抓取细致的姿势依赖变形，然后训练一个超温度模型来学习这些网络权重的分布。推理时生成网络权重进行实时可控渲染。

Result: 在大规模跨身份多视角视频数据集上，该方法超越了最先进的人体虚拟影像生成方法。

Conclusion: 该方法成功结合了个人特定渲染的高保真度和生成式模型的通用性，能够生成具有现实姿势依赖变形的高质量动态人体虚拟影像。

Abstract: Creating human avatars is a highly desirable yet challenging task. Recent advancements in radiance field rendering have achieved unprecedented photorealism and real-time performance for personalized dynamic human avatars. However, these approaches are typically limited to person-specific rendering models trained on multi-view video data for a single individual, limiting their ability to generalize across different identities. On the other hand, generative approaches leveraging prior knowledge from pre-trained 2D diffusion models can produce cartoonish, static human avatars, which are animated through simple skeleton-based articulation. Therefore, the avatars generated by these methods suffer from lower rendering quality compared to person-specific rendering methods and fail to capture pose-dependent deformations such as cloth wrinkles. In this paper, we propose a novel approach that unites the strengths of person-specific rendering and diffusion-based generative modeling to enable dynamic human avatar generation with both high photorealism and realistic pose-dependent deformations. Our method follows a two-stage pipeline: first, we optimize a set of person-specific UNets, with each network representing a dynamic human avatar that captures intricate pose-dependent deformations. In the second stage, we train a hyper diffusion model over the optimized network weights. During inference, our method generates network weights for real-time, controllable rendering of dynamic human avatars. Using a large-scale, cross-identity, multi-view video dataset, we demonstrate that our approach outperforms state-of-the-art human avatar generation methods.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [6] [Singular Value Few-shot Adaptation of Vision-Language Models](https://arxiv.org/abs/2509.03740)
*Taha Koleilat,Hassan Rivaz,Yiming Xiao*

Main category: cs.CV

TL;DR: CLIP-SVD是一种新颖的多模态参数高效适应技术，通过奇异值分解(SVD)修改CLIP内部参数空间，仅需微调0.04%的参数即可实现领域适应，在11个自然和10个生物医学数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型(VLMs)如CLIP在新细粒度领域适应时依赖提示工程和全模型微调，成本高且可能破坏预训练知识。现有方法通过添加额外组件可能限制适应质量并破坏模型稳定性。

Method: 利用奇异值分解(SVD)技术，仅微调CLIP参数矩阵的奇异值来重新缩放基向量，实现领域适应，同时保留预训练模型，不注入额外模块。

Result: 在11个自然数据集和10个生物医学数据集上实现了最先进的分类结果，在少样本设置下在准确性和泛化能力方面均优于先前方法，仅使用模型总参数的0.04%。

Conclusion: CLIP-SVD提供了一种高效且可解释的CLIP适应方法，能够更好地保持模型的泛化能力，同时实现优异的领域适应性能。

Abstract: Vision-language models (VLMs) like CLIP have shown impressive zero-shot and few-shot learning capabilities across diverse applications. However, adapting these models to new fine-grained domains remains difficult due to reliance on prompt engineering and the high cost of full model fine-tuning. Existing adaptation approaches rely on augmented components, such as prompt tokens and adapter modules, which could limit adaptation quality, destabilize the model, and compromise the rich knowledge learned during pretraining. In this work, we present \textbf{CLIP-SVD}, a novel \textit{multi-modal} and \textit{parameter-efficient} adaptation technique that leverages Singular Value Decomposition (SVD) to modify the internal parameter space of CLIP without injecting additional modules. Specifically, we fine-tune only the singular values of the CLIP parameter matrices to rescale the basis vectors for domain adaptation while retaining the pretrained model. This design enables enhanced adaptation performance using only \textbf{0.04\%} of the model's total parameters and better preservation of its generalization ability. CLIP-SVD achieves state-of-the-art classification results on 11 natural and 10 biomedical datasets, outperforming previous methods in both accuracy and generalization under few-shot settings. Additionally, we leverage a natural language-based approach to analyze the effectiveness and dynamics of the CLIP adaptation to allow interpretability of CLIP-SVD. The code is publicly available at https://github.com/HealthX-Lab/CLIP-SVD.

</details>


### [7] [Causality-guided Prompt Learning for Vision-language Models via Visual Granulation](https://arxiv.org/abs/2509.03803)
*Mengyu Gao,Qiulei Dong*

Main category: cs.CV

TL;DR: CaPL是一种基于因果推理的视觉粒度化提示学习方法，通过属性解耦和粒度学习模块，显著提升了CLIP模型在细粒度识别任务上的性能


<details>
  <summary>Details</summary>
Motivation: 现有CLIP提示学习方法在细粒度数据集上表现有限，需要一种能够捕捉细粒度类别间细微差异的方法

Method: 提出CaPL方法，包含属性解耦模块（使用布朗桥扩散模型分解视觉特征）和粒度学习模块（通过因果推理策略构建视觉粒度）

Result: 在15个数据集上的实验表明，CaPL显著优于最先进的提示学习方法，特别是在细粒度数据集上

Conclusion: 通过视觉粒度化和因果推理，CaPL能够学习更具判别性的文本提示，有效提升CLIP在细粒度识别任务中的性能

Abstract: Prompt learning has recently attracted much attention for adapting pre-trained vision-language models (e.g., CLIP) to downstream recognition tasks. However, most of the existing CLIP-based prompt learning methods only show a limited ability for handling fine-grained datasets. To address this issue, we propose a causality-guided text prompt learning method via visual granulation for CLIP, called CaPL, where the explored visual granulation technique could construct sets of visual granules for the text prompt to capture subtle discrepancies among different fine-grained classes through casual inference. The CaPL method contains the following two modules: (1) An attribute disentanglement module is proposed to decompose visual features into non-individualized attributes (shared by some classes) and individualized attributes (specific to single classes) using a Brownian Bridge Diffusion Model; (2) A granule learning module is proposed to construct visual granules by integrating the aforementioned attributes for recognition under two causal inference strategies. Thanks to the learned visual granules, more discriminative text prompt is expected to be learned. Extensive experimental results on 15 datasets demonstrate that our CaPL method significantly outperforms the state-of-the-art prompt learning methods, especially on fine-grained datasets.

</details>


### [8] [Human Motion Video Generation: A Survey](https://arxiv.org/abs/2509.03883)
*Haiwei Xue,Xiangyang Luo,Zhanghao Hu,Xin Zhang,Xunzhi Xiang,Yuqin Dai,Jianzhuang Liu,Zhensong Zhang,Minglei Li,Jian Yang,Fei Ma,Zhiyong Wu,Changpeng Yang,Zonghong Dai,Fei Richard Yu*

Main category: cs.CV

TL;DR: 这是一篇关于人体运动视频生成的综述性论文，系统分析了这一领域的10个子任务和5个关键生成阶段，包含超过200篇论文的综述。


<details>
  <summary>Details</summary>
Motivation: 现有的综述太过关注单个方法，缺乏对整个生成过程的全面概览。本文希望填补这一空白，提供一个系统的研究视角。

Method: 通过将生成过程分解为输入、运动规划、运动视频生成、精炼和输出五个阶段，并从视觉、文本和音频三种主要模态进行分析。

Result: 本文第一次讨论了大语言模型在人体运动视频生成中的潜力，并给出了领域内里程碑式的重要工作。

Conclusion: 这篇综述为人体运动视频生成领域提供了全面的概览，并为数字人类的综合应用提供了价值资源。

Abstract: Human motion video generation has garnered significant research interest due to its broad applications, enabling innovations such as photorealistic singing heads or dynamic avatars that seamlessly dance to music. However, existing surveys in this field focus on individual methods, lacking a comprehensive overview of the entire generative process. This paper addresses this gap by providing an in-depth survey of human motion video generation, encompassing over ten sub-tasks, and detailing the five key phases of the generation process: input, motion planning, motion video generation, refinement, and output. Notably, this is the first survey that discusses the potential of large language models in enhancing human motion video generation. Our survey reviews the latest developments and technological trends in human motion video generation across three primary modalities: vision, text, and audio. By covering over two hundred papers, we offer a thorough overview of the field and highlight milestone works that have driven significant technological breakthroughs. Our goal for this survey is to unveil the prospects of human motion video generation and serve as a valuable resource for advancing the comprehensive applications of digital humans. A complete list of the models examined in this survey is available in Our Repository https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.

</details>


### [9] [MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image Generation](https://arxiv.org/abs/2509.04126)
*Yuan Zhao,Liu Lin*

Main category: cs.CV

TL;DR: MEPG框架通过位置-风格感知LLM和多专家扩散模块，解决了文本到图像生成中复杂多元素提示和风格多样性不足的问题，显著提升了图像质量和风格多样性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像扩散模型在处理复杂多元素提示时存在困难，且风格多样性有限，需要一种能够精确控制空间布局和风格的专业化生成框架。

Method: 提出多专家规划与生成框架(MEPG)，包含：1)位置-风格感知模块(PSA)，使用微调LLM分解提示为空间坐标和风格指令；2)多专家扩散模块(MED)，通过注意力门控机制动态路由专家模型进行跨区域生成。

Result: 实验表明MEPG在相同骨干网络下显著优于基线模型，在图像质量和风格多样性方面都有明显提升。

Conclusion: MEPG框架通过专业化专家模块的协同工作，有效解决了复杂多元素图像生成的挑战，具有良好的扩展性和实时编辑能力。

Abstract: Text-to-image diffusion models have achieved remarkable image quality, but they still struggle with complex, multiele ment prompts, and limited stylistic diversity. To address these limitations, we propose a Multi-Expert Planning and Gen eration Framework (MEPG) that synergistically integrates position- and style-aware large language models (LLMs) with spatial-semantic expert modules. The framework comprises two core components: (1) a Position-Style-Aware (PSA) module that utilizes a supervised fine-tuned LLM to decom pose input prompts into precise spatial coordinates and style encoded semantic instructions; and (2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera tion through dynamic expert routing across both local regions and global areas. During the generation process for each lo cal region, specialized models (e.g., realism experts, styliza tion specialists) are selectively activated for each spatial par tition via attention-based gating mechanisms. The architec ture supports lightweight integration and replacement of ex pert models, providing strong extensibility. Additionally, an interactive interface enables real-time spatial layout editing and per-region style selection from a portfolio of experts. Ex periments show that MEPG significantly outperforms base line models with the same backbone in both image quality   and style diversity.

</details>


### [10] [TauGenNet: Plasma-Driven Tau PET Image Synthesis via Text-Guided 3D Diffusion Models](https://arxiv.org/abs/2509.04269)
*Yuxin Gong,Se-in Jang,Wei Shao,Yi Su,Kuang Gong*

Main category: cs.CV

TL;DR: 通过结构性MRI和血液p-tau217测量的多模态条件，使用文本引导的3D滴渏模型生成实际的正电子发射断展糖原图，以降低成本和扩展应用


<details>
  <summary>Details</summary>
Motivation: 正电子发射断展糖原图对防治和监测阿尔茫海默病至关重要，但成本高、可用性限制了其广泛应用，而结构性磁共振成像和血液标记物提供了无侵入性、广泛可用的补充信息

Method: 提出一种文本引导的3D滴渏模型，利用来自结构性MRI的解剖结构约束和来自血液p-tau217测量的文本提示，进行3D正电子发射断展糖原图生成，使用ADNI数据库的临床AV1451数据进行训练和评估

Result: 实验结果表明，该方法能够在不同疾病阶段生成实际、具有临床意义的3D正电子发射断展糖原图

Conclusion: 该框架可用于正电子发射断展糖原图数据增强、提供一种无侵入性、成本效益高的滴渏病理可视化替代方案，并支持在不同血液标记物水平和认知条件下的疾病进展模拟

Abstract: Accurate quantification of tau pathology via tau positron emission tomography (PET) scan is crucial for diagnosing and monitoring Alzheimer's disease (AD). However, the high cost and limited availability of tau PET restrict its widespread use. In contrast, structural magnetic resonance imaging (MRI) and plasma-based biomarkers provide non-invasive and widely available complementary information related to brain anatomy and disease progression. In this work, we propose a text-guided 3D diffusion model for 3D tau PET image synthesis, leveraging multimodal conditions from both structural MRI and plasma measurement. Specifically, the textual prompt is from the plasma p-tau217 measurement, which is a key indicator of AD progression, while MRI provides anatomical structure constraints. The proposed framework is trained and evaluated using clinical AV1451 tau PET data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. Experimental results demonstrate that our approach can generate realistic, clinically meaningful 3D tau PET across a range of disease stages. The proposed framework can help perform tau PET data augmentation under different settings, provide a non-invasive, cost-effective alternative for visualizing tau pathology, and support the simulation of disease progression under varying plasma biomarker levels and cognitive conditions.

</details>


### [11] [SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer](https://arxiv.org/abs/2509.04379)
*Jimin Xu,Bosheng Qin,Tao Jin,Zhou Zhao,Zhenhui Ye,Jun Yu,Fei Wu*

Main category: cs.CV

TL;DR: 提出了一种基于2D扩散模型先验知识的新型3D风格迁移管道，通过跨视角风格对齐和实例级风格迁移技术，解决了现有方法在高级风格语义提取和结构清晰度方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的3D风格迁移方法难以有效提取和迁移参考风格图像的高级语义特征，且风格化结果缺乏结构清晰度和实例分离性，导致难以区分3D场景中的不同对象。

Method: 采用两阶段管道：首先利用扩散先验生成关键视角的风格化渲染，然后将风格化关键视图转移到3D表示中。包含两个创新设计：跨视角风格对齐（在UNet最后上采样块插入跨视角注意力）和实例级风格迁移。

Result: 在广泛的场景中（从正向视角到挑战性的360度环境），该方法在定性和定量实验中都显著优于现有最先进方法，实现了更具结构性、视觉连贯性和艺术丰富性的风格化效果。

Conclusion: 该3D风格迁移管道通过有效整合2D扩散模型的先验知识，成功解决了风格语义提取和结构清晰度问题，为3D场景风格化提供了更优质的解决方案。

Abstract: Recent advancements in neural representations, such as Neural Radiance Fields and 3D Gaussian Splatting, have increased interest in applying style transfer to 3D scenes. While existing methods can transfer style patterns onto 3D-consistent neural representations, they struggle to effectively extract and transfer high-level style semantics from the reference style image. Additionally, the stylized results often lack structural clarity and separation, making it difficult to distinguish between different instances or objects within the 3D scene. To address these limitations, we propose a novel 3D style transfer pipeline that effectively integrates prior knowledge from pretrained 2D diffusion models. Our pipeline consists of two key stages: First, we leverage diffusion priors to generate stylized renderings of key viewpoints. Then, we transfer the stylized key views onto the 3D representation. This process incorporates two innovative designs. The first is cross-view style alignment, which inserts cross-view attention into the last upsampling block of the UNet, allowing feature interactions across multiple key views. This ensures that the diffusion model generates stylized key views that maintain both style fidelity and instance-level consistency. The second is instance-level style transfer, which effectively leverages instance-level consistency across stylized key views and transfers it onto the 3D representation. This results in a more structured, visually coherent, and artistically enriched stylization. Extensive qualitative and quantitative experiments demonstrate that our 3D style transfer pipeline significantly outperforms state-of-the-art methods across a wide range of scenes, from forward-facing to challenging 360-degree environments. Visit our project page https://jm-xu.github.io/SSGaussian for immersive visualization.

</details>


### [12] [Durian: Dual Reference-guided Portrait Animation with Attribute Transfer](https://arxiv.org/abs/2509.04434)
*Hyunsoo Cha,Byungjun Kim,Hanbyul Joo*

Main category: cs.CV

TL;DR: Durian是首个零样本肖像动画生成方法，能够从参考图像向目标肖像进行面部属性迁移，通过双参考网络和扩散模型实现高保真度、空间一致的跨帧属性迁移。


<details>
  <summary>Details</summary>
Motivation: 为了解决肖像动画中面部属性迁移的挑战，特别是在零样本设置下实现高保真度和空间一致性的跨帧属性转移。

Method: 采用双参考网络将肖像和属性图像的空间特征注入扩散模型的去噪过程；使用自重建公式进行训练，通过掩码扩展策略和空间/外观级变换增强鲁棒性。

Result: 在肖像动画属性迁移任务上达到最先进性能，双参考设计支持单次生成过程中进行多属性组合而无需额外训练。

Conclusion: Durian方法有效解决了零样本肖像动画属性迁移问题，通过创新的双参考网络设计和训练策略，实现了优秀的泛化能力和多属性组合功能。

Abstract: We present Durian, the first method for generating portrait animation videos with facial attribute transfer from a given reference image to a target portrait in a zero-shot manner. To enable high-fidelity and spatially consistent attribute transfer across frames, we introduce dual reference networks that inject spatial features from both the portrait and attribute images into the denoising process of a diffusion model. We train the model using a self-reconstruction formulation, where two frames are sampled from the same portrait video: one is treated as the attribute reference and the other as the target portrait, and the remaining frames are reconstructed conditioned on these inputs and their corresponding masks. To support the transfer of attributes with varying spatial extent, we propose a mask expansion strategy using keypoint-conditioned image generation for training. In addition, we further augment the attribute and portrait images with spatial and appearance-level transformations to improve robustness to positional misalignment between them. These strategies allow the model to effectively generalize across diverse attributes and in-the-wild reference combinations, despite being trained without explicit triplet supervision. Durian achieves state-of-the-art performance on portrait animation with attribute transfer, and notably, its dual reference design enables multi-attribute composition in a single generation pass without additional training.

</details>


### [13] [Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual Try-On from a Single Image -- Technical Preview](https://arxiv.org/abs/2509.04450)
*Jun-Kun Chen,Aayush Bansal,Minh Phuoc Vo,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: VFR是一个创新的视频生成模型，能够生成任意长度的虚拟试穿视频，通过分段自回归生成解决了长视频生成的计算资源问题，并利用前缀视频条件和锚点视频确保局部平滑性和全局时间一致性。


<details>
  <summary>Details</summary>
Motivation: 解决传统虚拟试穿视频生成需要大量计算资源和长视频数据的问题，同时实现任意长度视频的生成，并确保视频的局部平滑性和全局时间一致性。

Method: 采用分段自回归生成方法，使用前缀视频条件确保相邻片段间的平滑性，通过360度锚点视频维持不同片段间的全局时间一致性。

Result: 成功生成了分钟级别的虚拟试穿视频，在各种动作下都能保持局部平滑和全局一致性，成为长虚拟试穿视频生成领域的开创性工作。

Conclusion: VFR框架有效解决了长视频生成的资源消耗和一致性挑战，为虚拟试穿视频生成提供了灵活且高效的解决方案，具有重要的应用价值。

Abstract: We introduce the Virtual Fitting Room (VFR), a novel video generative model that produces arbitrarily long virtual try-on videos. Our VFR models long video generation tasks as an auto-regressive, segment-by-segment generation process, eliminating the need for resource-intensive generation and lengthy video data, while providing the flexibility to generate videos of arbitrary length. The key challenges of this task are twofold: ensuring local smoothness between adjacent segments and maintaining global temporal consistency across different segments. To address these challenges, we propose our VFR framework, which ensures smoothness through a prefix video condition and enforces consistency with the anchor video -- a 360-degree video that comprehensively captures the human's wholebody appearance. Our VFR generates minute-scale virtual try-on videos with both local smoothness and global temporal consistency under various motions, making it a pioneering work in long virtual try-on video generation.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [14] [The Chaotic Art: Quantum Representation and Manipulation of Color](https://arxiv.org/abs/2509.03542)
*Guosheng Hu*

Main category: quant-ph

TL;DR: 量子计算技术通过颜色卡比特表示、颜色通道处理和量子图像生成等方式，为颜色计算提供新技术路径，建立了经典色彩学与量子图形学之间的桥梁


<details>
  <summary>Details</summary>
Motivation: 利用量子计算的独特计算原理来深刻改变颜色艺术的景观，探索量子计算环境下的颜色计算新技术路径

Method: 通过在Qiskit和IBM Q中进行编程实验，实现颜色卡比特表示、颜色通道处理和量子图像生成，将数字颜色在量子中表示、运算和测量，然后恢复为经典计算机的计算结果

Result: 该方法已经被证明作为颜色卡比特表示和量子计算的艺术技术是可行的，建立了经典色彩学与量子图形学之间的联系

Conclusion: 量子计算机可用于信息可视化、图像处理和更多颜色计算任务，并有望推动新的颜色理论和艺术概念的发展

Abstract: Due to its unique computing principles, quantum computing technology will profoundly change the spectacle of color art. Focusing on experimental exploration of color qubit representation, color channel processing, and color image generation via quantum computing, this article proposes a new technical path for color computing in quantum computing environment, by which digital color is represented, operated, and measured in quantum bits, and then restored for classical computers as computing results. This method has been proved practicable as an artistic technique of color qubit representation and quantum computing via programming experiments in Qiskit and IBM Q. By building a bridge between classical chromatics and quantum graphics, quantum computers can be used for information visualization, image processing, and more color computing tasks. Furthermore, quantum computing can be expected to facilitate new color theories and artistic concepts.

</details>
