<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 4]
- [cs.CV](#cs.CV) [Total: 39]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.CY](#cs.CY) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D Gaussian Splatting](https://arxiv.org/abs/2507.19718)
*David Bauer,Qi Wu,Hamid Gadirov,Kwan-Liu Ma*

Main category: cs.GR

TL;DR: 提出了一种基于路径追踪的体积渲染新方法，通过动态训练的多级路径空间辐射缓存，显著提升渲染质量并保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 科学可视化中，真实感渲染技术因蒙特卡洛积分导致的高像素方差和低性能问题面临挑战。

Method: 采用3D高斯散射作为可动态训练的多级路径空间辐射缓存，适应场景参数变化。

Result: 相比基线路径追踪器和神经辐射缓存，新方法在保持计算效率的同时显著减少噪声并提升图像质量。

Conclusion: 路径空间辐射缓存是一种易于集成且高效的解决方案，显著提升了体积可视化应用的渲染质量。

Abstract: Real-time path tracing is rapidly becoming the standard for rendering in entertainment and professional applications. In scientific visualization, volume rendering plays a crucial role in helping researchers analyze and interpret complex 3D data. Recently, photorealistic rendering techniques have gained popularity in scientific visualization, yet they face significant challenges. One of the most prominent issues is slow rendering performance and high pixel variance caused by Monte Carlo integration. In this work, we introduce a novel radiance caching approach for path-traced volume rendering. Our method leverages advances in volumetric scene representation and adapts 3D Gaussian splatting to function as a multi-level, path-space radiance cache. This cache is designed to be trainable on the fly, dynamically adapting to changes in scene parameters such as lighting configurations and transfer functions. By incorporating our cache, we achieve less noisy, higher-quality images without increasing rendering costs. To evaluate our approach, we compare it against a baseline path tracer that supports uniform sampling and next-event estimation and the state-of-the-art for neural radiance caching. Through both quantitative and qualitative analyses, we demonstrate that our path-space radiance cache is a robust solution that is easy to integrate and significantly enhances the rendering quality of volumetric visualization applications while maintaining comparable computational efficiency.

</details>


### [2] [Taking Language Embedded 3D Gaussian Splatting into the Wild](https://arxiv.org/abs/2507.19830)
*Yuze Wang,Yue Qi*

Main category: cs.GR

TL;DR: 论文提出了一种基于无约束照片集合的开放词汇场景理解框架，结合语言嵌入的3D高斯泼溅技术，实现了对建筑组件的沉浸式理解。


<details>
  <summary>Details</summary>
Motivation: 当前利用互联网照片进行3D重建的技术虽能实现地标和古迹的虚拟探索，但对建筑风格和结构知识的沉浸式理解仍局限于静态图文浏览。

Method: 扩展语言嵌入的3D高斯泼溅技术，提出多外观CLIP特征提取、语言特征不确定性映射、瞬态不确定性感知自编码器等创新方法。

Result: 实验结果表明，该方法在开放词汇分割任务上优于现有方法，支持交互式漫游、建筑风格识别和3D场景编辑等应用。

Conclusion: 该方法为无约束照片集合的开放词汇场景理解提供了有效解决方案，扩展了3D重建技术的应用场景。

Abstract: Recent advances in leveraging large-scale Internet photo collections for 3D reconstruction have enabled immersive virtual exploration of landmarks and historic sites worldwide. However, little attention has been given to the immersive understanding of architectural styles and structural knowledge, which remains largely confined to browsing static text-image pairs. Therefore, can we draw inspiration from 3D in-the-wild reconstruction techniques and use unconstrained photo collections to create an immersive approach for understanding the 3D structure of architectural components? To this end, we extend language embedded 3D Gaussian splatting (3DGS) and propose a novel framework for open-vocabulary scene understanding from unconstrained photo collections. Specifically, we first render multiple appearance images from the same viewpoint as the unconstrained image with the reconstructed radiance field, then extract multi-appearance CLIP features and two types of language feature uncertainty maps-transient and appearance uncertainty-derived from the multi-appearance features to guide the subsequent optimization process. Next, we propose a transient uncertainty-aware autoencoder, a multi-appearance language field 3DGS representation, and a post-ensemble strategy to effectively compress, learn, and fuse language features from multiple appearances. Finally, to quantitatively evaluate our method, we introduce PT-OVS, a new benchmark dataset for assessing open-vocabulary segmentation performance on unconstrained photo collections. Experimental results show that our method outperforms existing methods, delivering accurate open-vocabulary segmentation and enabling applications such as interactive roaming with open-vocabulary queries, architectural style pattern recognition, and 3D scene editing.

</details>


### [3] [ChoreoMuse: Robust Music-to-Dance Video Generation with Style Transfer and Beat-Adherent Motion](https://arxiv.org/abs/2507.19836)
*Xuanchen Wang,Heng Wang,Weidong Cai*

Main category: cs.GR

TL;DR: ChoreoMuse是一个基于扩散模型的框架，通过SMPL参数作为音乐与视频生成的中间媒介，支持风格可控的高质量舞蹈视频生成，适应多样音乐风格和舞者特性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在生成高质量舞蹈视频时难以同时适应音乐节奏和用户定义编舞风格的问题。

Method: 使用SMPL参数作为中介，结合新颖的音乐编码器MotionTune从音频中提取运动线索，确保生成的编舞与音乐节拍和表达特性一致。

Result: ChoreoMuse在视频质量、节拍对齐、舞蹈多样性和风格一致性方面表现优异。

Conclusion: ChoreoMuse为广泛创意应用提供了强大的解决方案。

Abstract: Modern artistic productions increasingly demand automated choreography generation that adapts to diverse musical styles and individual dancer characteristics. Existing approaches often fail to produce high-quality dance videos that harmonize with both musical rhythm and user-defined choreography styles, limiting their applicability in real-world creative contexts. To address this gap, we introduce ChoreoMuse, a diffusion-based framework that uses SMPL format parameters and their variation version as intermediaries between music and video generation, thereby overcoming the usual constraints imposed by video resolution. Critically, ChoreoMuse supports style-controllable, high-fidelity dance video generation across diverse musical genres and individual dancer characteristics, including the flexibility to handle any reference individual at any resolution. Our method employs a novel music encoder MotionTune to capture motion cues from audio, ensuring that the generated choreography closely follows the beat and expressive qualities of the input music. To quantitatively evaluate how well the generated dances match both musical and choreographic styles, we introduce two new metrics that measure alignment with the intended stylistic cues. Extensive experiments confirm that ChoreoMuse achieves state-of-the-art performance across multiple dimensions, including video quality, beat alignment, dance diversity, and style adherence, demonstrating its potential as a robust solution for a wide range of creative applications. Video results can be found on our project page: https://choreomuse.github.io.

</details>


### [4] [Neural Shell Texture Splatting: More Details and Fewer Primitives](https://arxiv.org/abs/2507.20200)
*Xin Zhang,Anpei Chen,Jincheng Xiong,Pinxuan Dai,Yujun Shen,Weiwei Xu*

Main category: cs.GR

TL;DR: 论文提出了一种神经外壳纹理方法，通过解耦几何与外观，显著减少了高斯溅射技术所需的基元数量，同时保持了高质量的重建效果。


<details>
  <summary>Details</summary>
Motivation: 高斯溅射技术在视角合成中表现出色，但需要大量基元，问题源于几何与外观的耦合。

Method: 引入神经外壳纹理作为全局表示，用高斯基元同时作为几何表示和纹理采样器，高效地将纹理特征溅射到图像空间。

Result: 解耦方法实现了高参数效率、精细纹理细节重建和易于提取纹理网格，且基元数量显著减少。

Conclusion: 该方法有效解决了高斯溅射中基元数量过多的问题，同时保持了高质量的重建效果。

Abstract: Gaussian splatting techniques have shown promising results in novel view synthesis, achieving high fidelity and efficiency. However, their high reconstruction quality comes at the cost of requiring a large number of primitives. We identify this issue as stemming from the entanglement of geometry and appearance in Gaussian Splatting. To address this, we introduce a neural shell texture, a global representation that encodes texture information around the surface. We use Gaussian primitives as both a geometric representation and texture field samplers, efficiently splatting texture features into image space. Our evaluation demonstrates that this disentanglement enables high parameter efficiency, fine texture detail reconstruction, and easy textured mesh extraction, all while using significantly fewer primitives.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [5] [MoFRR: Mixture of Diffusion Models for Face Retouching Restoration](https://arxiv.org/abs/2507.19770)
*Jiaxin Liu,Qichao Ying,Zhenxing Qian,Sheng Li,Runqi Zhang,Jian Liu,Xinpeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为MoFRR的新方法，用于从修饰过的面部图像中恢复原始面部，通过混合扩散模型处理不同类型的修饰。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上广泛使用的面部修饰技术引发了对面部图像真实性的担忧，现有方法仅关注检测修饰，而如何从修饰图像中准确恢复原始面部尚未解决。

Method: 提出MoFRR方法，采用混合扩散模型，结合稀疏激活的专家处理特定修饰类型和共享专家处理通用修饰痕迹，每个专家采用双分支结构（低频分支和高频分支）。

Result: 在新建的数据集RetouchingFFHQ++上进行了大量实验，证明了MoFRR的有效性。

Conclusion: MoFRR能够有效恢复修饰过的面部图像，解决了现有方法未涉及的恢复问题。

Abstract: The widespread use of face retouching on social media platforms raises concerns about the authenticity of face images. While existing methods focus on detecting face retouching, how to accurately recover the original faces from the retouched ones has yet to be answered. This paper introduces Face Retouching Restoration (FRR), a novel computer vision task aimed at restoring original faces from their retouched counterparts. FRR differs from traditional image restoration tasks by addressing the complex retouching operations with various types and degrees, which focuses more on the restoration of the low-frequency information of the faces. To tackle this challenge, we propose MoFRR, Mixture of Diffusion Models for FRR. Inspired by DeepSeek's expert isolation strategy, the MoFRR uses sparse activation of specialized experts handling distinct retouching types and the engagement of a shared expert dealing with universal retouching traces. Each specialized expert follows a dual-branch structure with a DDIM-based low-frequency branch guided by an Iterative Distortion Evaluation Module (IDEM) and a Cross-Attention-based High-Frequency branch (HFCAM) for detail refinement. Extensive experiments on a newly constructed face retouching dataset, RetouchingFFHQ++, demonstrate the effectiveness of MoFRR for FRR.

</details>


### [6] [Smaller, Faster, Cheaper: Architectural Designs for Efficient Machine Learning](https://arxiv.org/abs/2507.19795)
*Steven Walton*

Main category: cs.CV

TL;DR: 论文探讨了如何在减少计算资源需求的同时提升计算机视觉模型的性能，提出了三个方向的研究：数据输入输出优化、核心神经网络架构改进以及利用Normalizing Flows的特性。


<details>
  <summary>Details</summary>
Motivation: 随着计算机视觉模型在多样化且资源受限的环境中部署，需要设计更高效的架构以降低计算需求。

Method: 通过优化数据输入输出、改进核心神经网络架构（如受限注意力机制）以及利用Normalizing Flows的特性，提升模型效率。

Result: 研究表明，精心设计的神经网络架构可以提高机器学习算法的效率，使其更小、更快、更经济。

Conclusion: 通过架构优化，可以在减少计算资源的同时实现高性能的计算机视觉模型。

Abstract: Major advancements in the capabilities of computer vision models have been primarily fueled by rapid expansion of datasets, model parameters, and computational budgets, leading to ever-increasing demands on computational infrastructure. However, as these models are deployed in increasingly diverse and resource-constrained environments, there is a pressing need for architectures that can deliver high performance while requiring fewer computational resources.   This dissertation focuses on architectural principles through which models can achieve increased performance while reducing their computational demands. We discuss strides towards this goal through three directions. First, we focus on data ingress and egress, investigating how information may be passed into and retrieved from our core neural processing units. This ensures that our models make the most of available data, allowing smaller architectures to become more performant. Second, we investigate modifications to the core neural architecture, applied to restricted attention in vision transformers. This section explores how removing uniform context windows in restricted attention increases the expressivity of the underlying neural architecture. Third, we explore the natural structures of Normalizing Flows and how we can leverage these properties to better distill model knowledge.   These contributions demonstrate that careful design of neural architectures can increase the efficiency of machine learning algorithms, allowing them to become smaller, faster, and cheaper.

</details>


### [7] [SeeDiff: Off-the-Shelf Seeded Mask Generation from Diffusion Models](https://arxiv.org/abs/2507.19808)
*Joon Hyun Park,Kumju Jo,Sungyong Baik*

Main category: cs.CV

TL;DR: SeeDiff利用Stable Diffusion的注意力机制生成高质量像素级标注掩码，无需额外训练或提示调优。


<details>
  <summary>Details</summary>
Motivation: 减少人工标注的负担，利用文本到图像生成模型的潜力自动生成像素级标注掩码。

Method: 通过分析Stable Diffusion的注意力机制，结合交叉注意力和自注意力，迭代扩展种子区域以覆盖整个目标类别，并利用背景掩码进行细化。

Result: SeeDiff能够直接从Stable Diffusion生成高质量的标注掩码，无需额外训练或提示调优。

Conclusion: SeeDiff展示了利用生成模型的注意力机制实现高效像素级标注的潜力。

Abstract: Entrusted with the goal of pixel-level object classification, the semantic segmentation networks entail the laborious preparation of pixel-level annotation masks. To obtain pixel-level annotation masks for a given class without human efforts, recent few works have proposed to generate pairs of images and annotation masks by employing image and text relationships modeled by text-to-image generative models, especially Stable Diffusion. However, these works do not fully exploit the capability of text-guided Diffusion models and thus require a pre-trained segmentation network, careful text prompt tuning, or the training of a segmentation network to generate final annotation masks. In this work, we take a closer look at attention mechanisms of Stable Diffusion, from which we draw connections with classical seeded segmentation approaches. In particular, we show that cross-attention alone provides very coarse object localization, which however can provide initial seeds. Then, akin to region expansion in seeded segmentation, we utilize the semantic-correspondence-modeling capability of self-attention to iteratively spread the attention to the whole class from the seeds using multi-scale self-attention maps. We also observe that a simple-text-guided synthetic image often has a uniform background, which is easier to find correspondences, compared to complex-structured objects. Thus, we further refine a mask using a more accurate background mask. Our proposed method, dubbed SeeDiff, generates high-quality masks off-the-shelf from Stable Diffusion, without additional training procedure, prompt tuning, or a pre-trained segmentation network.

</details>


### [8] [Knowledge Regularized Negative Feature Tuning for Out-of-Distribution Detection with Vision-Language Models](https://arxiv.org/abs/2507.19847)
*Wenjie Zhu,Yabin Zhang,Xin Jin,Wenjun Zeng,Lei Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为KR-NFT的新方法，通过负特征调优和知识正则化策略，提升视觉语言模型的OOD检测能力，同时保持对未见类和风格的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有的负提示调优方法在提升OOD检测能力时，往往牺牲了对未见类和风格的泛化性能，KR-NFT旨在解决这一问题。

Method: KR-NFT结合了负特征调优（NFT）和知识正则化（KR）策略。NFT通过分布感知变换分离正负特征，KR优化策略增强ID与OOD的区分并减少预训练知识遗忘。

Result: 在ImageNet数据集上，KR-NFT显著提升了ID分类准确性和OOD检测性能，FPR95降低了5.44%。

Conclusion: KR-NFT在提升OOD检测能力的同时，有效保持了模型的泛化性能，适用于未见ID类别的场景。

Abstract: Out-of-distribution (OOD) detection is crucial for building reliable machine learning models. Although negative prompt tuning has enhanced the OOD detection capabilities of vision-language models, these tuned models often suffer from reduced generalization performance on unseen classes and styles. To address this challenge, we propose a novel method called Knowledge Regularized Negative Feature Tuning (KR-NFT), which integrates an innovative adaptation architecture termed Negative Feature Tuning (NFT) and a corresponding knowledge-regularization (KR) optimization strategy. Specifically, NFT applies distribution-aware transformations to pre-trained text features, effectively separating positive and negative features into distinct spaces. This separation maximizes the distinction between in-distribution (ID) and OOD images. Additionally, we introduce image-conditional learnable factors through a lightweight meta-network, enabling dynamic adaptation to individual images and mitigating sensitivity to class and style shifts. Compared to traditional negative prompt tuning, NFT demonstrates superior efficiency and scalability. To optimize this adaptation architecture, the KR optimization strategy is designed to enhance the discrimination between ID and OOD sets while mitigating pre-trained knowledge forgetting. This enhances OOD detection performance on trained ID classes while simultaneously improving OOD detection on unseen ID datasets. Notably, when trained with few-shot samples from ImageNet dataset, KR-NFT not only improves ID classification accuracy and OOD detection but also significantly reduces the FPR95 by 5.44\% under an unexplored generalization setting with unseen ID categories. Codes can be found at \href{https://github.com/ZhuWenjie98/KRNFT}{https://github.com/ZhuWenjie98/KRNFT}.

</details>


### [9] [RaGS: Unleashing 3D Gaussian Splatting from 4D Radar and Monocular Cues for 3D Object Detection](https://arxiv.org/abs/2507.19856)
*Xiaokai Bai,Chenxu Zhou,Lianqing Zheng,Si-Yuan Cao,Jianan Liu,Xiaohan Zhang,Zhengzhuang Zhang,Hui-liang Shen*

Main category: cs.CV

TL;DR: RaGS框架利用3D高斯泼溅技术融合4D雷达和单目图像，实现高效的3D物体检测，通过动态资源分配和灵活表示提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有融合方法缺乏全局场景理解或受限于固定网格结构，RaGS旨在解决这些问题。

Method: 采用3D高斯泼溅表示场景，通过FLI、IMA和MGF三阶段流程动态优化高斯场并生成BEV特征。

Result: 在多个基准测试中表现优异，达到最先进水平。

Conclusion: RaGS通过动态聚焦和灵活表示，为3D物体检测提供了高效解决方案。

Abstract: 4D millimeter-wave radar has emerged as a promising sensor for autonomous driving, but effective 3D object detection from both 4D radar and monocular images remains a challenge. Existing fusion approaches typically rely on either instance-based proposals or dense BEV grids, which either lack holistic scene understanding or are limited by rigid grid structures. To address these, we propose RaGS, the first framework to leverage 3D Gaussian Splatting (GS) as representation for fusing 4D radar and monocular cues in 3D object detection. 3D GS naturally suits 3D object detection by modeling the scene as a field of Gaussians, dynamically allocating resources on foreground objects and providing a flexible, resource-efficient solution. RaGS uses a cascaded pipeline to construct and refine the Gaussian field. It starts with the Frustum-based Localization Initiation (FLI), which unprojects foreground pixels to initialize coarse 3D Gaussians positions. Then, the Iterative Multimodal Aggregation (IMA) fuses semantics and geometry, refining the limited Gaussians to the regions of interest. Finally, the Multi-level Gaussian Fusion (MGF) renders the Gaussians into multi-level BEV features for 3D object detection. By dynamically focusing on sparse objects within scenes, RaGS enable object concentrating while offering comprehensive scene perception. Extensive experiments on View-of-Delft, TJ4DRadSet, and OmniHD-Scenes benchmarks demonstrate its state-of-the-art performance. Code will be released.

</details>


### [10] [All-in-One Medical Image Restoration with Latent Diffusion-Enhanced Vector-Quantized Codebook Prior](https://arxiv.org/abs/2507.19874)
*Haowei Chen,Zhiwen Yang,Haotian Hou,Hui Zhang,Bingzheng Wei,Gang Zhou,Yan Xu*

Main category: cs.CV

TL;DR: DiffCode是一个新型框架，通过潜在扩散增强的向量量化码本先验，解决多任务医学图像恢复（MedIR）中的异质性问题。


<details>
  <summary>Details</summary>
Motivation: 多任务MedIR面临不同任务间信息损失的多样性挑战，现有方法难以处理。

Method: DiffCode构建任务自适应码本库整合任务特定高质量先验特征，并引入潜在扩散策略优化特征分布。

Result: DiffCode在MRI超分辨率、CT去噪和PET合成三个任务中表现出色。

Conclusion: DiffCode通过任务自适应码本和潜在扩散策略，显著提升了多任务MedIR的性能。

Abstract: All-in-one medical image restoration (MedIR) aims to address multiple MedIR tasks using a unified model, concurrently recovering various high-quality (HQ) medical images (e.g., MRI, CT, and PET) from low-quality (LQ) counterparts. However, all-in-one MedIR presents significant challenges due to the heterogeneity across different tasks. Each task involves distinct degradations, leading to diverse information losses in LQ images. Existing methods struggle to handle these diverse information losses associated with different tasks. To address these challenges, we propose a latent diffusion-enhanced vector-quantized codebook prior and develop \textbf{DiffCode}, a novel framework leveraging this prior for all-in-one MedIR. Specifically, to compensate for diverse information losses associated with different tasks, DiffCode constructs a task-adaptive codebook bank to integrate task-specific HQ prior features across tasks, capturing a comprehensive prior. Furthermore, to enhance prior retrieval from the codebook bank, DiffCode introduces a latent diffusion strategy that utilizes the diffusion model's powerful mapping capabilities to iteratively refine the latent feature distribution, estimating more accurate HQ prior features during restoration. With the help of the task-adaptive codebook bank and latent diffusion strategy, DiffCode achieves superior performance in both quantitative metrics and visual quality across three MedIR tasks: MRI super-resolution, CT denoising, and PET synthesis.

</details>


### [11] [HumanSAM: Classifying Human-centric Forgery Videos in Human Spatial, Appearance, and Motion Anomaly](https://arxiv.org/abs/2507.19924)
*Chang Liu,Yunfan Ye,Fan Zhang,Qingyang Zhou,Yuchuan Luo,Zhiping Cai*

Main category: cs.CV

TL;DR: HumanSAM是一个新框架，旨在细粒度分类人类中心伪造视频的三种异常类型（空间、外观和运动），通过融合视频理解和空间深度特征，提升检测的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 生成模型合成的视频对人类信息安全和真实性构成威胁，现有检测方法缺乏对伪造类型的细粒度理解，影响可靠性和可解释性。

Method: 提出HumanSAM框架，融合视频理解和空间深度特征生成伪造表示，采用基于排名的置信增强策略，并构建首个公开基准数据集HFV。

Result: 实验表明，HumanSAM在二分类和多分类伪造检测中优于现有方法。

Conclusion: HumanSAM通过细粒度分类和特征融合，显著提升了伪造视频检测的性能和可解释性。

Abstract: Numerous synthesized videos from generative models, especially human-centric ones that simulate realistic human actions, pose significant threats to human information security and authenticity. While progress has been made in binary forgery video detection, the lack of fine-grained understanding of forgery types raises concerns regarding both reliability and interpretability, which are critical for real-world applications. To address this limitation, we propose HumanSAM, a new framework that builds upon the fundamental challenges of video generation models. Specifically, HumanSAM aims to classify human-centric forgeries into three distinct types of artifacts commonly observed in generated content: spatial, appearance, and motion anomaly.To better capture the features of geometry, semantics and spatiotemporal consistency, we propose to generate the human forgery representation by fusing two branches of video understanding and spatial depth. We also adopt a rank-based confidence enhancement strategy during the training process to learn more robust representation by introducing three prior scores. For training and evaluation, we construct the first public benchmark, the Human-centric Forgery Video (HFV) dataset, with all types of forgeries carefully annotated semi-automatically. In our experiments, HumanSAM yields promising results in comparison with state-of-the-art methods, both in binary and multi-class forgery classification.

</details>


### [12] [LLMControl: Grounded Control of Text-to-Image Diffusion-based Synthesis with Multimodal LLMs](https://arxiv.org/abs/2507.19939)
*Jiaze Wang,Rui Chen,Haowang Cui*

Main category: cs.CV

TL;DR: LLM_Control是一个基于LLM的框架，用于改进文本到图像生成中的空间控制，解决多对象和复杂空间组合的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂文本提示下难以精确控制图像生成，需要更有效的解决方案。

Method: 利用多模态LLM作为全局控制器，优化空间布局和语义描述，并将控制信号注入去噪网络以增强注意力图。

Result: LLM_Control在多种预训练T2I模型上表现出色，优于现有方法。

Conclusion: LLM_Control为复杂条件下的可控图像生成提供了有效解决方案。

Abstract: Recent spatial control methods for text-to-image (T2I) diffusion models have shown compelling results. However, these methods still fail to precisely follow the control conditions and generate the corresponding images, especially when encountering the textual prompts that contain multiple objects or have complex spatial compositions. In this work, we present a LLM-guided framework called LLM\_Control to address the challenges of the controllable T2I generation task. By improving grounding capabilities, LLM\_Control is introduced to accurately modulate the pre-trained diffusion models, where visual conditions and textual prompts influence the structures and appearance generation in a complementary way. We utilize the multimodal LLM as a global controller to arrange spatial layouts, augment semantic descriptions and bind object attributes. The obtained control signals are injected into the denoising network to refocus and enhance attention maps according to novel sampling constraints. Extensive qualitative and quantitative experiments have demonstrated that LLM\_Control achieves competitive synthesis quality compared to other state-of-the-art methods across various pre-trained T2I models. It is noteworthy that LLM\_Control allows the challenging input conditions on which most of the existing methods

</details>


### [13] [SCALAR: Scale-wise Controllable Visual Autoregressive Learning](https://arxiv.org/abs/2507.19946)
*Ryan Xu,Dongyang Jin,Yancheng Bai,Rui Lan,Xu Duan,Lei Sun,Xiangxiang Chu*

Main category: cs.CV

TL;DR: SCALAR提出了一种基于VAR的可控图像生成方法，通过尺度条件解码机制解决了现有方法在控制编码和注入机制上的低效问题。


<details>
  <summary>Details</summary>
Motivation: 可控图像合成是视觉生成建模的关键方向，但VAR模型由于其层次化的预测方式难以实现高效控制。

Method: SCALAR采用尺度条件解码机制，优化控制编码和注入过程。

Result: 该方法在保持生成质量的同时提高了控制效率。

Conclusion: SCALAR为VAR模型的可控生成提供了一种高效解决方案。

Abstract: Controllable image synthesis, which enables fine-grained control over generated outputs, has emerged as a key focus in visual generative modeling. However, controllable generation remains challenging for Visual Autoregressive (VAR) models due to their hierarchical, next-scale prediction style. Existing VAR-based methods often suffer from inefficient control encoding and disruptive injection mechanisms that compromise both fidelity and efficiency. In this work, we present SCALAR, a controllable generation method based on VAR, incorporating a novel Scale-wise Conditional Decoding mechanism. SCALAR leverages a

</details>


### [14] [KB-DMGen: Knowledge-Based Global Guidance and Dynamic Pose Masking for Human Image Generation](https://arxiv.org/abs/2507.20083)
*Shibang Liu,Xuemei Xie,Guangming Shi*

Main category: cs.CV

TL;DR: 提出了一种结合知识库和动态掩码的扩散模型（KB-DMGen），用于提升人像生成的全局质量和姿态准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注姿态准确性，但忽略了全局图像质量，KB-DMGen旨在同时优化两者。

Method: 通过知识库（KB）增强姿态准确性并利用图像特征保持全局质量，动态掩码（DM）调整姿态相关区域的重要性。

Result: 在HumanArt数据集上取得AP和CAP的新最优结果。

Conclusion: KB-DMGen在姿态准确性和全局质量上均表现优异，代码将开源。

Abstract: Recent methods using diffusion models have made significant progress in human image generation with various control signals such as pose priors. In portrait generation, both the accuracy of human pose and the overall visual quality are crucial for realistic synthesis. Most existing methods focus on controlling the accuracy of generated poses, but ignore the quality assurance of the entire image. In order to ensure the global image quality and pose accuracy, we propose Knowledge-Based Global Guidance and Dynamic pose Masking for human image Generation (KB-DMGen). The Knowledge Base (KB) is designed not only to enhance pose accuracy but also to leverage image feature information to maintain overall image quality. Dynamic Masking (DM) dynamically adjusts the importance of pose-related regions. Experiments demonstrate the effectiveness of our model, achieving new state-of-the-art results in terms of AP and CAP on the HumanArt dataset. The code will be made publicly available.

</details>


### [15] [Local Prompt Adaptation for Style-Consistent Multi-Object Generation in Diffusion Models](https://arxiv.org/abs/2507.20094)
*Ankit Sanjyal*

Main category: cs.CV

TL;DR: 提出了一种无需训练的局部提示适应方法（LPA），用于提升扩散模型在复杂提示下的风格一致性和空间一致性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在多对象和风格复杂的提示下生成效果不佳，缺乏风格统一和空间连贯性。

Method: 将提示分解为内容和风格标记，选择性注入U-Net注意力层，早期注入对象标记，后期注入风格标记。

Result: 在50个风格丰富的提示上评估，LPA在CLIP分数和风格一致性上优于现有方法。

Conclusion: LPA为可控、表达性强的扩散生成提供了新方向。

Abstract: Diffusion models have become a powerful backbone for text-to-image generation, enabling users to synthesize high-quality visuals from natural language prompts. However, they often struggle with complex prompts involving multiple objects and global or local style specifications. In such cases, the generated scenes tend to lack style uniformity and spatial coherence, limiting their utility in creative and controllable content generation. In this paper, we propose a simple, training-free architectural method called Local Prompt Adaptation (LPA). Our method decomposes the prompt into content and style tokens, and injects them selectively into the U-Net's attention layers at different stages. By conditioning object tokens early and style tokens later in the generation process, LPA enhances both layout control and stylistic consistency. We evaluate our method on a custom benchmark of 50 style-rich prompts across five categories and compare against strong baselines including Composer, MultiDiffusion, Attend-and-Excite, LoRA, and SDXL. Our approach outperforms prior work on both CLIP score and style consistency metrics, offering a new direction for controllable, expressive diffusion-based generation.

</details>


### [16] [Hybrid-Domain Synergistic Transformer for Hyperspectral Image Denoising](https://arxiv.org/abs/2507.20099)
*Haoyue Li,Di Wu*

Main category: cs.CV

TL;DR: 本文提出了一种基于频域增强和多尺度建模的HSI去噪框架HDST，通过空间、频率和通道域的三维协同处理，有效解决了HSI去噪中的多维耦合问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法主要针对RGB图像，难以有效处理HSI独特的空间-光谱特性和复杂噪声分布。

Method: HDST框架包含三个关键机制：FFT预处理模块提取跨波段相关性，动态跨域注意力模块融合空间和频域特征，以及分层架构实现全局噪声统计和细节恢复。

Result: 实验表明HDST在真实和合成数据集上显著提升了去噪性能，同时保持了计算效率。

Conclusion: 该研究为HSI和其他高维视觉数据中的复杂噪声耦合问题提供了新见解和通用框架。

Abstract: Hyperspectral image denoising faces the challenge of multi-dimensional coupling of spatially non-uniform noise and spectral correlation interference. Existing deep learning methods mostly focus on RGB images and struggle to effectively handle the unique spatial-spectral characteristics and complex noise distributions of hyperspectral images (HSI). This paper proposes an HSI denoising framework, Hybrid-Domain Synergistic Transformer Network (HDST), based on frequency domain enhancement and multiscale modeling, achieving three-dimensional collaborative processing of spatial, frequency and channel domains. The method innovatively integrates three key mechanisms: (1) introducing an FFT preprocessing module with multi-band convolution to extract cross-band correlations and decouple spectral noise components; (2) designing a dynamic cross-domain attention module that adaptively fuses spatial domain texture features and frequency domain noise priors through a learnable gating mechanism; (3) building a hierarchical architecture where shallow layers capture global noise statistics using multiscale atrous convolution, and deep layers achieve detail recovery through frequency domain postprocessing. Experiments on both real and synthetic datasets demonstrate that HDST significantly improves denoising performance while maintaining computational efficiency, validating the effectiveness of the proposed method. This research provides new insights and a universal framework for addressing complex noise coupling issues in HSI and other high-dimensional visual data. The code is available at https://github.com/lhy-cn/HDST-HSIDenoise.

</details>


### [17] [NeuroVoxel-LM: Language-Aligned 3D Perception via Dynamic Voxelization and Meta-Embedding](https://arxiv.org/abs/2507.20110)
*Shiyu Liu,Lianlei Shan*

Main category: cs.CV

TL;DR: NeuroVoxel-LM提出了一种结合NeRF的动态分辨率体素化和轻量级元嵌入的新框架，解决了现有3D语言模型在大规模点云中特征提取慢和表示精度低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D语言模型在处理稀疏、大规模点云时存在特征提取速度慢和表示精度不足的问题，限制了语言驱动认知的进展。

Method: 提出动态分辨率多尺度体素化（DR-MSV）技术，根据几何和结构复杂度自适应调整体素粒度；引入基于注意力的轻量级元嵌入机制（TAP-LME）增强语义表示。

Result: DR-MSV显著提高了点云特征提取的效率和精度，TAP-LME在捕捉NeRF权重的细粒度语义方面优于传统最大池化。

Conclusion: NeuroVoxel-LM通过动态体素化和元嵌入机制，有效提升了3D场景感知的语言驱动认知能力。

Abstract: Recent breakthroughs in Visual Language Models (VLMs) and Multimodal Large Language Models (MLLMs) have significantly advanced 3D scene perception towards language-driven cognition. However, existing 3D language models struggle with sparse, large-scale point clouds due to slow feature extraction and limited representation accuracy. To address these challenges, we propose NeuroVoxel-LM, a novel framework that integrates Neural Radiance Fields (NeRF) with dynamic resolution voxelization and lightweight meta-embedding. Specifically, we introduce a Dynamic Resolution Multiscale Voxelization (DR-MSV) technique that adaptively adjusts voxel granularity based on geometric and structural complexity, reducing computational cost while preserving reconstruction fidelity. In addition, we propose the Token-level Adaptive Pooling for Lightweight Meta-Embedding (TAP-LME) mechanism, which enhances semantic representation through attention-based weighting and residual fusion. Experimental results demonstrate that DR-MSV significantly improves point cloud feature extraction efficiency and accuracy, while TAP-LME outperforms conventional max-pooling in capturing fine-grained semantics from NeRF weights.

</details>


### [18] [Local2Global query Alignment for Video Instance Segmentation](https://arxiv.org/abs/2507.20120)
*Rajat Koner,Zhipeng Wang,Srinivas Parthasarathy,Chinghang Chen*

Main category: cs.CV

TL;DR: Local2Global框架通过局部和全局查询的早期对齐，实现了在线视频实例分割的高性能和时序一致性。


<details>
  <summary>Details</summary>
Motivation: 解决在线视频分割中时序一致性不足的问题，尤其是噪声积累、遮挡和场景转换带来的挑战。

Method: 引入局部和全局查询，利用L2G-aligner轻量级解码器实现早期对齐，无需复杂启发式或内存机制。

Result: 在多个数据集上表现优异，如Youtube-VIS-19/-21和OVIS，AP分别达到54.3、49.4和37.0。

Conclusion: Local2Global框架通过简单在线训练实现了高性能和时序一致性，超越了现有基准。

Abstract: Online video segmentation methods excel at handling long sequences and capturing gradual changes, making them ideal for real-world applications. However, achieving temporally consistent predictions remains a challenge, especially with gradual accumulation of noise or drift in on-line propagation, abrupt occlusions and scene transitions. This paper introduces Local2Global, an online framework, for video instance segmentation, exhibiting state-of-the-art performance with simple baseline and training purely in online fashion. Leveraging the DETR-based query propagation framework, we introduce two novel sets of queries:(1) local queries that capture initial object-specific spatial features from each frame and (2) global queries containing past spatio-temporal representations. We propose the L2G-aligner, a novel lightweight transformer decoder, to facilitate an early alignment between local and global queries. This alignment allows our model to effectively utilize current frame information while maintaining temporal consistency, producing a smooth transition between frames. Furthermore, L2G-aligner is integrated within the segmentation model, without relying on additional complex heuristics, or memory mechanisms. Extensive experiments across various challenging VIS and VPS datasets showcase the superiority of our method with simple online training, surpassing current benchmarks without bells and rings. For instance, we achieve 54.3 and 49.4 AP on Youtube-VIS-19/-21 datasets and 37.0 AP on OVIS dataset respectively withthe ResNet-50 backbone.

</details>


### [19] [GT-Mean Loss: A Simple Yet Effective Solution for Brightness Mismatch in Low-Light Image Enhancement](https://arxiv.org/abs/2507.20148)
*Jingxi Liao,Shijie Hao,Richang Hong,Meng Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为GT-mean loss的损失函数，用于解决低光图像增强中亮度不匹配的问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有监督式低光图像增强研究中，增强图像与真实图像之间的亮度不匹配问题被忽视，影响了模型训练效果。

Method: 提出GT-mean loss，从概率角度直接建模图像均值，灵活扩展现有损失函数，计算成本低。

Result: 实验表明，GT-mean loss能一致提升不同方法和数据集的性能。

Conclusion: GT-mean loss简单有效，解决了亮度不匹配问题，为低光图像增强提供了新思路。

Abstract: Low-light image enhancement (LLIE) aims to improve the visual quality of images captured under poor lighting conditions. In supervised LLIE research, there exists a significant yet often overlooked inconsistency between the overall brightness of an enhanced image and its ground truth counterpart, referred to as brightness mismatch in this study. Brightness mismatch negatively impact supervised LLIE models by misleading model training. However, this issue is largely neglected in current research. In this context, we propose the GT-mean loss, a simple yet effective loss function directly modeling the mean values of images from a probabilistic perspective. The GT-mean loss is flexible, as it extends existing supervised LLIE loss functions into the GT-mean form with minimal additional computational costs. Extensive experiments demonstrate that the incorporation of the GT-mean loss results in consistent performance improvements across various methods and datasets.

</details>


### [20] [AnimeColor: Reference-based Animation Colorization with Diffusion Transformers](https://arxiv.org/abs/2507.20158)
*Yuhong Zhang,Liyao Wang,Han Wang,Danni Wu,Zuzeng Lin,Feng Wang,Li Song*

Main category: cs.CV

TL;DR: AnimeColor是一种基于参考的动画着色框架，利用扩散变换器（DiT）解决现有方法在颜色准确性和时间一致性上的不足。


<details>
  <summary>Details</summary>
Motivation: 动画着色在动画制作中至关重要，但现有方法难以实现颜色准确性和时间一致性。

Method: 提出AnimeColor框架，结合DiT和草图序列，引入高级颜色提取器（HCE）和低级颜色引导器（LCG），并采用多阶段训练策略。

Result: 实验表明，AnimeColor在颜色准确性、草图对齐、时间一致性和视觉质量上优于现有方法。

Conclusion: AnimeColor不仅推动了动画着色技术的进步，还为工业应用提供了实用解决方案。

Abstract: Animation colorization plays a vital role in animation production, yet existing methods struggle to achieve color accuracy and temporal consistency. To address these challenges, we propose \textbf{AnimeColor}, a novel reference-based animation colorization framework leveraging Diffusion Transformers (DiT). Our approach integrates sketch sequences into a DiT-based video diffusion model, enabling sketch-controlled animation generation. We introduce two key components: a High-level Color Extractor (HCE) to capture semantic color information and a Low-level Color Guider (LCG) to extract fine-grained color details from reference images. These components work synergistically to guide the video diffusion process. Additionally, we employ a multi-stage training strategy to maximize the utilization of reference image color information. Extensive experiments demonstrate that AnimeColor outperforms existing methods in color accuracy, sketch alignment, temporal consistency, and visual quality. Our framework not only advances the state of the art in animation colorization but also provides a practical solution for industrial applications. The code will be made publicly available at \href{https://github.com/IamCreateAI/AnimeColor}{https://github.com/IamCreateAI/AnimeColor}.

</details>


### [21] [PUMPS: Skeleton-Agnostic Point-based Universal Motion Pre-Training for Synthesis in Human Motion Tasks](https://arxiv.org/abs/2507.20170)
*Clinton Ansun Mo,Kun Hu,Chengjiang Long,Dong Yuan,Wan-Chi Siu,Zhiyong Wang*

Main category: cs.CV

TL;DR: 论文提出PUMPS，一种针对TPC数据的自编码器架构，解决了TPC格式在运动任务学习中的挑战，并在多项任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决不同骨骼结构间运动数据难以迁移的问题，TPC作为一种跨兼容的运动表示形式，但其在直接运动任务学习中的应用尚未探索。

Method: 提出PUMPS架构，通过独立降维帧级点云为可采样特征向量，解码器利用潜在高斯噪声向量提取时序点，并引入线性分配优化重建过程。

Result: PUMPS在运动预测、过渡生成和关键帧插值等任务中表现优异，无需原生数据集监督即可达到先进水平。

Conclusion: PUMPS是一种通用架构，在多项运动任务中表现优异，展示了TPC数据在运动合成中的潜力。

Abstract: Motion skeletons drive 3D character animation by transforming bone hierarchies, but differences in proportions or structure make motion data hard to transfer across skeletons, posing challenges for data-driven motion synthesis. Temporal Point Clouds (TPCs) offer an unstructured, cross-compatible motion representation. Though reversible with skeletons, TPCs mainly serve for compatibility, not for direct motion task learning. Doing so would require data synthesis capabilities for the TPC format, which presents unexplored challenges regarding its unique temporal consistency and point identifiability. Therefore, we propose PUMPS, the primordial autoencoder architecture for TPC data. PUMPS independently reduces frame-wise point clouds into sampleable feature vectors, from which a decoder extracts distinct temporal points using latent Gaussian noise vectors as sampling identifiers. We introduce linear assignment-based point pairing to optimise the TPC reconstruction process, and negate the use of expensive point-wise attention mechanisms in the architecture. Using these latent features, we pre-train a motion synthesis model capable of performing motion prediction, transition generation, and keyframe interpolation. For these pre-training tasks, PUMPS performs remarkably well even without native dataset supervision, matching state-of-the-art performance. When fine-tuned for motion denoising or estimation, PUMPS outperforms many respective methods without deviating from its generalist architecture.

</details>


### [22] [MambaMap: Online Vectorized HD Map Construction using State Space Model](https://arxiv.org/abs/2507.20224)
*Ruizi Yang,Xiaolu Liu,Junbo Chen,Jianke Zhu*

Main category: cs.CV

TL;DR: MambaMap是一个高效融合长时程时空特征的框架，用于在线构建向量化高清地图，显著提升自动驾驶中的地图预测精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 高清地图对自动驾驶至关重要，但现有方法未能充分利用时空信息或计算开销过大。

Method: 提出MambaMap框架，结合记忆库动态更新BEV特征和实例查询，引入门控机制和多方向时空扫描策略。

Result: 在nuScenes和Argoverse2数据集上表现优于现有方法。

Conclusion: MambaMap通过高效时空建模显著提升了高清地图构建的性能和一致性。

Abstract: High-definition (HD) maps are essential for autonomous driving, as they provide precise road information for downstream tasks. Recent advances highlight the potential of temporal modeling in addressing challenges like occlusions and extended perception range. However, existing methods either fail to fully exploit temporal information or incur substantial computational overhead in handling extended sequences. To tackle these challenges, we propose MambaMap, a novel framework that efficiently fuses long-range temporal features in the state space to construct online vectorized HD maps. Specifically, MambaMap incorporates a memory bank to store and utilize information from historical frames, dynamically updating BEV features and instance queries to improve robustness against noise and occlusions. Moreover, we introduce a gating mechanism in the state space, selectively integrating dependencies of map elements in high computational efficiency. In addition, we design innovative multi-directional and spatial-temporal scanning strategies to enhance feature extraction at both BEV and instance levels. These strategies significantly boost the prediction accuracy of our approach while ensuring robust temporal consistency. Extensive experiments on the nuScenes and Argoverse2 datasets demonstrate that our proposed MambaMap approach outperforms state-of-the-art methods across various splits and perception ranges. Source code will be available at https://github.com/ZiziAmy/MambaMap.

</details>


### [23] [Decomposing Densification in Gaussian Splatting for Faster 3D Scene Reconstruction](https://arxiv.org/abs/2507.20239)
*Binxiao Huang,Zhengwu Liu,Ngai Wong*

Main category: cs.CV

TL;DR: 本文提出了一种全局到局部的高斯分布策略和能量引导的多分辨率训练框架，显著加速了3D高斯泼溅的训练过程，并提升了重建性能。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅（GS）在场景重建中表现优异，但其训练过程因高斯原语的空间分布不优和低效的密集化而收敛缓慢。

Method: 通过分析密集化阶段的拆分和克隆操作，提出全局到局部密集化策略，并结合能量引导的多分辨率训练框架和动态剪枝。

Result: 在多个数据集上实验表明，该方法实现了超过2倍的训练加速，使用更少的高斯原语且重建性能更优。

Conclusion: 该方法通过优化高斯原语的分布和训练策略，显著提升了3D高斯泼溅的效率和性能。

Abstract: 3D Gaussian Splatting (GS) has emerged as a powerful representation for high-quality scene reconstruction, offering compelling rendering quality. However, the training process of GS often suffers from slow convergence due to inefficient densification and suboptimal spatial distribution of Gaussian primitives. In this work, we present a comprehensive analysis of the split and clone operations during the densification phase, revealing their distinct roles in balancing detail preservation and computational efficiency. Building upon this analysis, we propose a global-to-local densification strategy, which facilitates more efficient growth of Gaussians across the scene space, promoting both global coverage and local refinement. To cooperate with the proposed densification strategy and promote sufficient diffusion of Gaussian primitives in space, we introduce an energy-guided coarse-to-fine multi-resolution training framework, which gradually increases resolution based on energy density in 2D images. Additionally, we dynamically prune unnecessary Gaussian primitives to speed up the training. Extensive experiments on MipNeRF-360, Deep Blending, and Tanks & Temples datasets demonstrate that our approach significantly accelerates training,achieving over 2x speedup with fewer Gaussian primitives and superior reconstruction performance.

</details>


### [24] [Fine-structure Preserved Real-world Image Super-resolution via Transfer VAE Training](https://arxiv.org/abs/2507.20291)
*Qiaosi Yi,Shuai Li,Rongyuan Wu,Lingchen Sun,Yuhui Wu,Lei Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种Transfer VAE Training (TVT)策略，通过将8倍下采样的VAE转换为4倍下采样，同时适应预训练的UNet，以改善图像超分辨率中精细结构的重建。


<details>
  <summary>Details</summary>
Motivation: 现有基于稳定扩散(SD)模型的图像超分辨率方法在重建图像精细结构（如小字符和纹理）时表现不佳，主要原因是SD模型中VAE的激进分辨率降低（8倍下采样）。

Method: 提出TVT策略：先基于原始VAE编码器的输出特征训练4倍解码器，再训练4倍编码器并固定解码器。同时优化VAE和UNet的网络结构以减少计算成本。

Result: 实验表明，TVT方法显著改善了精细结构的保留，且计算量低于当前最优的一步扩散模型。

Conclusion: TVT策略有效解决了SD模型中VAE下采样率过高的问题，提升了图像超分辨率的性能，同时降低了计算成本。

Abstract: Impressive results on real-world image super-resolution (Real-ISR) have been achieved by employing pre-trained stable diffusion (SD) models. However, one critical issue of such methods lies in their poor reconstruction of image fine structures, such as small characters and textures, due to the aggressive resolution reduction of the VAE (eg., 8$\times$ downsampling) in the SD model. One solution is to employ a VAE with a lower downsampling rate for diffusion; however, adapting its latent features with the pre-trained UNet while mitigating the increased computational cost poses new challenges. To address these issues, we propose a Transfer VAE Training (TVT) strategy to transfer the 8$\times$ downsampled VAE into a 4$\times$ one while adapting to the pre-trained UNet. Specifically, we first train a 4$\times$ decoder based on the output features of the original VAE encoder, then train a 4$\times$ encoder while keeping the newly trained decoder fixed. Such a TVT strategy aligns the new encoder-decoder pair with the original VAE latent space while enhancing image fine details. Additionally, we introduce a compact VAE and compute-efficient UNet by optimizing their network architectures, reducing the computational cost while capturing high-resolution fine-scale features. Experimental results demonstrate that our TVT method significantly improves fine-structure preservation, which is often compromised by other SD-based methods, while requiring fewer FLOPs than state-of-the-art one-step diffusion models. The official code can be found at https://github.com/Joyies/TVT.

</details>


### [25] [From Gallery to Wrist: Realistic 3D Bracelet Insertion in Videos](https://arxiv.org/abs/2507.20331)
*Chenjian Gao,Lihe Ding,Rui Han,Zhanpeng Huang,Zibin Wang,Tianfan Xue*

Main category: cs.CV

TL;DR: 提出了一种结合3D渲染和2D扩散模型的混合方法，用于在动态视频中插入3D对象，以实现时间一致性和真实光照。


<details>
  <summary>Details</summary>
Motivation: 解决在动态场景中插入3D对象时的时间一致性和真实光照问题，结合2D扩散模型和3D渲染的优势。

Method: 使用3D高斯散射（3DGS）进行初始渲染，并通过2D扩散模型增强光照效果，同时优化多帧加权调整以保持时间一致性。

Result: 实现了在动态视频中插入3D对象时的高时间一致性和真实光照效果。

Conclusion: 该方法首次结合了3D渲染和2D扩散模型，为视频编辑提供了更真实和一致的解决方案。

Abstract: Inserting 3D objects into videos is a longstanding challenge in computer graphics with applications in augmented reality, virtual try-on, and video composition. Achieving both temporal consistency, or realistic lighting remains difficult, particularly in dynamic scenarios with complex object motion, perspective changes, and varying illumination. While 2D diffusion models have shown promise for producing photorealistic edits, they often struggle with maintaining temporal coherence across frames. Conversely, traditional 3D rendering methods excel in spatial and temporal consistency but fall short in achieving photorealistic lighting. In this work, we propose a hybrid object insertion pipeline that combines the strengths of both paradigms. Specifically, we focus on inserting bracelets into dynamic wrist scenes, leveraging the high temporal consistency of 3D Gaussian Splatting (3DGS) for initial rendering and refining the results using a 2D diffusion-based enhancement model to ensure realistic lighting interactions. Our method introduces a shading-driven pipeline that separates intrinsic object properties (albedo, shading, reflectance) and refines both shading and sRGB images for photorealism. To maintain temporal coherence, we optimize the 3DGS model with multi-frame weighted adjustments. This is the first approach to synergize 3D rendering and 2D diffusion for video object insertion, offering a robust solution for realistic and consistent video editing. Project Page: https://cjeen.github.io/BraceletPaper/

</details>


### [26] [Generative Pre-training for Subjective Tasks: A Diffusion Transformer-Based Framework for Facial Beauty Prediction](https://arxiv.org/abs/2507.20363)
*Djamel Eddine Boukhari,Ali chemsa*

Main category: cs.CV

TL;DR: 论文提出Diff-FBP框架，利用生成模型提升面部美感预测任务性能，通过两阶段训练实现SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 面部美感预测（FBP）因主观性和复杂特征而具挑战性，现有方法难以学习与美学评估对齐的特征表示。

Method: 两阶段框架：1）自监督去噪任务预训练Diffusion Transformer；2）冻结编码器作为特征提取器，微调轻量回归头。

Result: 在FBP5500数据集上达到PCC 0.932，显著优于现有方法。

Conclusion: 生成式预训练策略是关键，能学习更具语义潜力的特征表示。

Abstract: Facial Beauty Prediction (FBP) is a challenging computer vision task due to its subjective nature and the subtle, holistic features that influence human perception. Prevailing methods, often based on deep convolutional networks or standard Vision Transformers pre-trained on generic object classification (e.g., ImageNet), struggle to learn feature representations that are truly aligned with high-level aesthetic assessment. In this paper, we propose a novel two-stage framework that leverages the power of generative models to create a superior, domain-specific feature extractor. In the first stage, we pre-train a Diffusion Transformer on a large-scale, unlabeled facial dataset (FFHQ) through a self-supervised denoising task. This process forces the model to learn the fundamental data distribution of human faces, capturing nuanced details and structural priors essential for aesthetic evaluation. In the second stage, the pre-trained and frozen encoder of our Diffusion Transformer is used as a backbone feature extractor, with only a lightweight regression head being fine-tuned on the target FBP dataset (FBP5500). Our method, termed Diff-FBP, sets a new state-of-the-art on the FBP5500 benchmark, achieving a Pearson Correlation Coefficient (PCC) of 0.932, significantly outperforming prior art based on general-purpose pre-training. Extensive ablation studies validate that our generative pre-training strategy is the key contributor to this performance leap, creating feature representations that are more semantically potent for subjective visual tasks.

</details>


### [27] [MagicAnime: A Hierarchically Annotated, Multimodal and Multitasking Dataset with Benchmarks for Cartoon Animation Generation](https://arxiv.org/abs/2507.20368)
*Shuolin Xu,Bingyuan Wang,Zeyu Cai,Fangteng Fu,Yue Ma,Tongyi Lee,Hongchuan Yu,Zeyu Wang*

Main category: cs.CV

TL;DR: 论文提出了MagicAnime数据集和MagicAnime-Bench基准，用于支持多模态卡通动画生成任务，填补了真实视频与卡通动画之间的领域差距。


<details>
  <summary>Details</summary>
Motivation: 卡通动画生成面临非人类角色复杂性、多样化动作和精细情感的挑战，且缺乏大规模多模态标注数据。

Method: 构建了包含40万视频片段的多层次标注数据集MagicAnime，并设计了多任务基准MagicAnime-Bench。

Result: 在四项任务（视频驱动面部动画、音频驱动面部动画、图像到视频动画、姿态驱动角色动画）中验证了数据集的有效性。

Conclusion: MagicAnime数据集和基准支持高保真、细粒度且可控的卡通动画生成。

Abstract: Generating high-quality cartoon animations multimodal control is challenging due to the complexity of non-human characters, stylistically diverse motions and fine-grained emotions. There is a huge domain gap between real-world videos and cartoon animation, as cartoon animation is usually abstract and has exaggerated motion. Meanwhile, public multimodal cartoon data are extremely scarce due to the difficulty of large-scale automatic annotation processes compared with real-life scenarios. To bridge this gap, We propose the MagicAnime dataset, a large-scale, hierarchically annotated, and multimodal dataset designed to support multiple video generation tasks, along with the benchmarks it includes. Containing 400k video clips for image-to-video generation, 50k pairs of video clips and keypoints for whole-body annotation, 12k pairs of video clips for video-to-video face animation, and 2.9k pairs of video and audio clips for audio-driven face animation. Meanwhile, we also build a set of multi-modal cartoon animation benchmarks, called MagicAnime-Bench, to support the comparisons of different methods in the tasks above. Comprehensive experiments on four tasks, including video-driven face animation, audio-driven face animation, image-to-video animation, and pose-driven character animation, validate its effectiveness in supporting high-fidelity, fine-grained, and controllable generation.

</details>


### [28] [ModalFormer: Multimodal Transformer for Low-Light Image Enhancement](https://arxiv.org/abs/2507.20388)
*Alexandru Brateanu,Raul Balmez,Ciprian Orhei,Codruta Ancuti,Cosmin Ancuti*

Main category: cs.CV

TL;DR: ModalFormer是一个基于多模态的大规模低光照图像增强框架，通过整合九种辅助模态信息，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 低光照图像增强任务面临噪声、细节丢失和对比度差等问题，现有方法仅依赖RGB图像的像素级变换，忽略了多模态的丰富上下文信息。

Method: 提出ModalFormer框架，包含跨模态Transformer（CM-T）和多个辅助子网络，利用新颖的跨模态多头自注意力机制（CM-MSA）融合RGB数据与多模态特征。

Result: 在多个基准数据集上的实验表明，ModalFormer在低光照图像增强任务中达到了最先进的性能。

Conclusion: ModalFormer通过充分利用多模态信息，显著提升了低光照图像增强的效果。

Abstract: Low-light image enhancement (LLIE) is a fundamental yet challenging task due to the presence of noise, loss of detail, and poor contrast in images captured under insufficient lighting conditions. Recent methods often rely solely on pixel-level transformations of RGB images, neglecting the rich contextual information available from multiple visual modalities. In this paper, we present ModalFormer, the first large-scale multimodal framework for LLIE that fully exploits nine auxiliary modalities to achieve state-of-the-art performance. Our model comprises two main components: a Cross-modal Transformer (CM-T) designed to restore corrupted images while seamlessly integrating multimodal information, and multiple auxiliary subnetworks dedicated to multimodal feature reconstruction. Central to the CM-T is our novel Cross-modal Multi-headed Self-Attention mechanism (CM-MSA), which effectively fuses RGB data with modality-specific features--including deep feature embeddings, segmentation information, geometric cues, and color information--to generate information-rich hybrid attention maps. Extensive experiments on multiple benchmark datasets demonstrate ModalFormer's state-of-the-art performance in LLIE. Pre-trained models and results are made available at https://github.com/albrateanu/ModalFormer.

</details>


### [29] [Frequency-Aware Autoregressive Modeling for Efficient High-Resolution Image Synthesis](https://arxiv.org/abs/2507.20454)
*Zhuokun Chen,Jugang Fan,Zhuowei Yu,Bohan Zhuang,Mingkui Tan*

Main category: cs.CV

TL;DR: SparseVAR是一种用于视觉自回归模型的加速框架，通过动态排除低频标记减少计算开销，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 传统视觉自回归模型在高分辨率阶段计算开销大，低频标记对图像质量影响小且相似度高，因此提出动态排除低频标记的加速方法。

Method: SparseVAR利用轻量级MSE指标识别低频标记，并通过锚点标记保留区域保真度，无需额外训练。

Result: 在Infinity-2B模型中，SparseVAR实现了2倍加速且图像质量损失极小。

Conclusion: SparseVAR通过动态排除低频标记显著降低计算成本，同时保持高质量图像生成。

Abstract: Visual autoregressive modeling, based on the next-scale prediction paradigm, exhibits notable advantages in image quality and model scalability over traditional autoregressive and diffusion models. It generates images by progressively refining resolution across multiple stages. However, the computational overhead in high-resolution stages remains a critical challenge due to the substantial number of tokens involved. In this paper, we introduce SparseVAR, a plug-and-play acceleration framework for next-scale prediction that dynamically excludes low-frequency tokens during inference without requiring additional training. Our approach is motivated by the observation that tokens in low-frequency regions have a negligible impact on image quality in high-resolution stages and exhibit strong similarity with neighboring tokens. Additionally, we observe that different blocks in the next-scale prediction model focus on distinct regions, with some concentrating on high-frequency areas. SparseVAR leverages these insights by employing lightweight MSE-based metrics to identify low-frequency tokens while preserving the fidelity of excluded regions through a small set of uniformly sampled anchor tokens. By significantly reducing the computational cost while maintaining high image generation quality, SparseVAR achieves notable acceleration in both HART and Infinity. Specifically, SparseVAR achieves up to a 2 times speedup with minimal quality degradation in Infinity-2B.

</details>


### [30] [Automated 3D-GS Registration and Fusion via Skeleton Alignment and Gaussian-Adaptive Features](https://arxiv.org/abs/2507.20480)
*Shiyang Liu,Dianyi Yang,Yu Gao,Bohan Ren,Yi Yang,Mengyin Fu*

Main category: cs.CV

TL;DR: 提出了一种自动化的3D-GS子地图对齐与融合方法，无需人工干预，提升了配准精度和融合质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖人工干预且融合后渲染质量下降，需解决多子地图自动配准与高质量融合问题。

Method: 提取几何骨架并利用椭球感知卷积捕捉属性，提出多因素高斯融合策略。

Result: 配准误差降低41.9%，融合PSNR提升10.11dB。

Conclusion: 方法显著提升了场景对齐与重建质量，适用于机器人感知与自主导航。

Abstract: In recent years, 3D Gaussian Splatting (3D-GS)-based scene representation demonstrates significant potential in real-time rendering and training efficiency. However, most existing methods primarily focus on single-map reconstruction, while the registration and fusion of multiple 3D-GS sub-maps remain underexplored. Existing methods typically rely on manual intervention to select a reference sub-map as a template and use point cloud matching for registration. Moreover, hard-threshold filtering of 3D-GS primitives often degrades rendering quality after fusion. In this paper, we present a novel approach for automated 3D-GS sub-map alignment and fusion, eliminating the need for manual intervention while enhancing registration accuracy and fusion quality. First, we extract geometric skeletons across multiple scenes and leverage ellipsoid-aware convolution to capture 3D-GS attributes, facilitating robust scene registration. Second, we introduce a multi-factor Gaussian fusion strategy to mitigate the scene element loss caused by rigid thresholding. Experiments on the ScanNet-GSReg and our Coord datasets demonstrate the effectiveness of the proposed method in registration and fusion. For registration, it achieves a 41.9\% reduction in RRE on complex scenes, ensuring more precise pose estimation. For fusion, it improves PSNR by 10.11 dB, highlighting superior structural preservation. These results confirm its ability to enhance scene alignment and reconstruction fidelity, ensuring more consistent and accurate 3D scene representation for robotic perception and autonomous navigation.

</details>


### [31] [GaRe: Relightable 3D Gaussian Splatting for Outdoor Scenes from Unconstrained Photo Collections](https://arxiv.org/abs/2507.20512)
*Haiyang Bai,Jiaqi Zhu,Songru Jiang,Wei Huang,Tao Lu,Yuanqi Li,Jie Guo,Runze Fu,Yanwen Guo,Lijun Chen*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯泼溅的户外重光照框架，通过本征图像分解精确整合阳光、天空辐射和间接光照，支持多样的阴影操作和动态阴影效果生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法将全局光照压缩为单一潜在向量，限制了光照分解的多样性和阴影效果的真实性。本文旨在解决这一问题。

Method: 1. 基于残差的阳光可见性提取方法；2. 区域监督框架与结构一致性损失；3. 基于光线追踪的阴影模拟技术。

Result: 实验表明，该方法在生成新视图时具有高保真度，并能产生更自然、多面的光照和阴影效果。

Conclusion: 该框架在户外重光照任务中表现出色，优于现有方法。

Abstract: We propose a 3D Gaussian splatting-based framework for outdoor relighting that leverages intrinsic image decomposition to precisely integrate sunlight, sky radiance, and indirect lighting from unconstrained photo collections. Unlike prior methods that compress the per-image global illumination into a single latent vector, our approach enables simultaneously diverse shading manipulation and the generation of dynamic shadow effects. This is achieved through three key innovations: (1) a residual-based sun visibility extraction method to accurately separate direct sunlight effects, (2) a region-based supervision framework with a structural consistency loss for physically interpretable and coherent illumination decomposition, and (3) a ray-tracing-based technique for realistic shadow simulation. Extensive experiments demonstrate that our framework synthesizes novel views with competitive fidelity against state-of-the-art relighting solutions and produces more natural and multifaceted illumination and shadow effects.

</details>


### [32] [T2I-Copilot: A Training-Free Multi-Agent Text-to-Image System for Enhanced Prompt Interpretation and Interactive Generation](https://arxiv.org/abs/2507.20536)
*Chieh-Yun Chen,Min Shi,Gong Zhang,Humphrey Shi*

Main category: cs.CV

TL;DR: T2I-Copilot是一个无需训练的多智能体系统，通过协作优化文本到图像生成，显著提升生成质量和文本-图像对齐。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型对提示词敏感，需要反复优化且缺乏明确反馈，现有技术可控性有限或需额外训练。

Method: T2I-Copilot由三个智能体组成：输入解析器、生成引擎和质量评估器，实现自动化提示词优化和模型选择。

Result: 在开源模型上，T2I-Copilot性能接近商业模型，成本仅为FLUX1.1-pro的16.59%，且显著优于其他开源模型。

Conclusion: T2I-Copilot通过多智能体协作，简化了提示工程，提升了生成质量，支持全自动或人工干预模式。

Abstract: Text-to-Image (T2I) generative models have revolutionized content creation but remain highly sensitive to prompt phrasing, often requiring users to repeatedly refine prompts multiple times without clear feedback. While techniques such as automatic prompt engineering, controlled text embeddings, denoising, and multi-turn generation mitigate these issues, they offer limited controllability, or often necessitate additional training, restricting the generalization abilities. Thus, we introduce T2I-Copilot, a training-free multi-agent system that leverages collaboration between (Multimodal) Large Language Models to automate prompt phrasing, model selection, and iterative refinement. This approach significantly simplifies prompt engineering while enhancing generation quality and text-image alignment compared to direct generation. Specifically, T2I-Copilot consists of three agents: (1) Input Interpreter, which parses the input prompt, resolves ambiguities, and generates a standardized report; (2) Generation Engine, which selects the appropriate model from different types of T2I models and organizes visual and textual prompts to initiate generation; and (3) Quality Evaluator, which assesses aesthetic quality and text-image alignment, providing scores and feedback for potential regeneration. T2I-Copilot can operate fully autonomously while also supporting human-in-the-loop intervention for fine-grained control. On GenAI-Bench, using open-source generation models, T2I-Copilot achieves a VQA score comparable to commercial models RecraftV3 and Imagen 3, surpasses FLUX1.1-pro by 6.17% at only 16.59% of its cost, and outperforms FLUX.1-dev and SD 3.5 Large by 9.11% and 6.36%. Code will be released at: https://github.com/SHI-Labs/T2I-Copilot.

</details>


### [33] [Harnessing Diffusion-Yielded Score Priors for Image Restoration](https://arxiv.org/abs/2507.20590)
*Xinqi Lin,Fanghua Yu,Jinfan Hu,Zhiyuan You,Wu Shi,Jimmy S. Ren,Jinjin Gu,Chao Dong*

Main category: cs.CV

TL;DR: HYPIR是一种新型图像修复方法，结合预训练扩散模型和对抗训练，解决了现有方法在修复质量、保真度和速度之间的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有图像修复方法（如MSE、GAN和扩散模型）难以平衡修复质量、保真度和速度，HYPIR旨在解决这一问题。

Method: HYPIR通过预训练扩散模型初始化修复模型，再通过对抗训练微调，无需扩散损失或迭代采样。

Result: HYPIR在修复质量、速度和用户控制方面优于现有方法，实验证明其高效且高质量。

Conclusion: HYPIR通过结合扩散模型和对抗训练，实现了高效、高质量的图像修复，并具备丰富的用户控制功能。

Abstract: Deep image restoration models aim to learn a mapping from degraded image space to natural image space. However, they face several critical challenges: removing degradation, generating realistic details, and ensuring pixel-level consistency. Over time, three major classes of methods have emerged, including MSE-based, GAN-based, and diffusion-based methods. However, they fail to achieve a good balance between restoration quality, fidelity, and speed. We propose a novel method, HYPIR, to address these challenges. Our solution pipeline is straightforward: it involves initializing the image restoration model with a pre-trained diffusion model and then fine-tuning it with adversarial training. This approach does not rely on diffusion loss, iterative sampling, or additional adapters. We theoretically demonstrate that initializing adversarial training from a pre-trained diffusion model positions the initial restoration model very close to the natural image distribution. Consequently, this initialization improves numerical stability, avoids mode collapse, and substantially accelerates the convergence of adversarial training. Moreover, HYPIR inherits the capabilities of diffusion models with rich user control, enabling text-guided restoration and adjustable texture richness. Requiring only a single forward pass, it achieves faster convergence and inference speed than diffusion-based methods. Extensive experiments show that HYPIR outperforms previous state-of-the-art methods, achieving efficient and high-quality image restoration.

</details>


### [34] [Lightweight Transformer-Driven Segmentation of Hotspots and Snail Trails in Solar PV Thermal Imagery](https://arxiv.org/abs/2507.20680)
*Deepak Joshi,Mayukha Pal*

Main category: cs.CV

TL;DR: 提出了一种基于SegFormer的轻量级语义分割模型，用于光伏模块热红外图像中的缺陷检测，性能优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 光伏模块中的热点和蜗牛纹等缺陷影响能源效率和系统可靠性，需要高效准确的检测方法。

Method: 使用277张无人机拍摄的热红外图像数据集，预处理包括图像调整、CLAHE对比增强、去噪和归一化；开发基于SegFormer的轻量模型，定制Transformer编码器和简化解码器。

Result: SegFormer模型在Dice分数、F1分数、Cohen's kappa、平均IoU和像素精度等指标上优于U-Net、DeepLabV3、PSPNet和Mask2Former。

Conclusion: 该模型轻量高效，适合实时部署于边缘设备，可集成至无人机系统用于大规模太阳能农场自动化检测。

Abstract: Accurate detection of defects such as hotspots and snail trails in photovoltaic modules is essential for maintaining energy efficiency and system reliablility. This work presents a supervised deep learning framework for segmenting thermal infrared images of PV panels, using a dataset of 277 aerial thermographic images captured by zenmuse XT infrared camera mounted on a DJI Matrice 100 drone. The preprocessing pipeline includes image resizing, CLAHE based contrast enhancement, denoising, and normalisation. A lightweight semantic segmentation model based on SegFormer is developed, featuring a customised Transformwer encoder and streamlined decoder, and fine-tuned on annotated images with manually labeled defect regions. To evaluate performance, we benchmark our model against U-Net, DeepLabV3, PSPNet, and Mask2Former using consistent preprocessing and augmentation. Evaluation metrices includes per-class Dice score, F1-score, Cohen's kappa, mean IoU, and pixel accuracy. The SegFormer-based model outperforms baselines in accuracy and efficiency, particularly for segmenting small and irregular defects. Its lightweight design real-time deployment on edge devices and seamless integration with drone-based systems for automated inspection of large-scale solar farms.

</details>


### [35] [AIComposer: Any Style and Content Image Composition via Feature Integration](https://arxiv.org/abs/2507.20721)
*Haowen Li,Zhenfeng Fan,Zhang Wen,Zhengzhou Zhu,Yunjin Li*

Main category: cs.CV

TL;DR: 本文提出了一种无需文本提示的跨域图像合成方法，通过简单高效的步骤实现自然风格化和无缝合成，显著提升了性能指标。


<details>
  <summary>Details</summary>
Motivation: 跨域图像合成因扩散模型的随机性和输入图像风格差异而面临挑战，现有方法依赖文本提示，限制了实际应用。

Method: 采用反向反转和正向去噪的轻量步骤，结合多层感知机整合CLIP特征，利用局部交叉注意力策略操纵扩散过程。

Result: 在定性和定量评估中均优于现有技术，LPIPS分数提升30.5%，CSD指标提升18.1%。

Conclusion: 该方法为跨域图像合成提供了高效解决方案，推动了未来研究和应用的发展。

Abstract: Image composition has advanced significantly with large-scale pre-trained T2I diffusion models. Despite progress in same-domain composition, cross-domain composition remains under-explored. The main challenges are the stochastic nature of diffusion models and the style gap between input images, leading to failures and artifacts. Additionally, heavy reliance on text prompts limits practical applications. This paper presents the first cross-domain image composition method that does not require text prompts, allowing natural stylization and seamless compositions. Our method is efficient and robust, preserving the diffusion prior, as it involves minor steps for backward inversion and forward denoising without training the diffuser. Our method also uses a simple multilayer perceptron network to integrate CLIP features from foreground and background, manipulating diffusion with a local cross-attention strategy. It effectively preserves foreground content while enabling stable stylization without a pre-stylization network. Finally, we create a benchmark dataset with diverse contents and styles for fair evaluation, addressing the lack of testing datasets for cross-domain image composition. Our method outperforms state-of-the-art techniques in both qualitative and quantitative evaluations, significantly improving the LPIPS score by 30.5% and the CSD metric by 18.1%. We believe our method will advance future research and applications. Code and benchmark at https://github.com/sherlhw/AIComposer.

</details>


### [36] [Regularizing Subspace Redundancy of Low-Rank Adaptation](https://arxiv.org/abs/2507.20745)
*Yue Zhu,Haiwen Diao,Shang Gao,Jiazuo Yu,Jiawen Zhu,Yunzhi Zhuge,Shuai Hao,Xu Jia,Lu Zhang,Ying Zhang,Huchuan Lu*

Main category: cs.CV

TL;DR: ReSoRA提出了一种自适应正则化低秩适应（LoRA）子空间冗余的方法，通过分解子矩阵并应用去冗余约束，提升了参数高效迁移学习的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA及其变体在训练中投影矩阵不受限制，导致表示冗余和特征适应效果下降，现有方法缺乏灵活性和泛化能力。

Method: ReSoRA将低秩子矩阵分解为多个等价子空间，并系统地对不同投影的特征分布应用去冗余约束。

Result: 实验表明，ReSoRA在多种骨干网络和数据集上提升了现有PETL方法的性能，且无需额外推理成本。

Conclusion: ReSoRA通过显式建模和正则化子空间冗余，显著提升了参数高效迁移学习的灵活性和性能。

Abstract: Low-Rank Adaptation (LoRA) and its variants have delivered strong capability in Parameter-Efficient Transfer Learning (PETL) by minimizing trainable parameters and benefiting from reparameterization. However, their projection matrices remain unrestricted during training, causing high representation redundancy and diminishing the effectiveness of feature adaptation in the resulting subspaces. While existing methods mitigate this by manually adjusting the rank or implicitly applying channel-wise masks, they lack flexibility and generalize poorly across various datasets and architectures. Hence, we propose ReSoRA, a method that explicitly models redundancy between mapping subspaces and adaptively Regularizes Subspace redundancy of Low-Rank Adaptation. Specifically, it theoretically decomposes the low-rank submatrices into multiple equivalent subspaces and systematically applies de-redundancy constraints to the feature distributions across different projections. Extensive experiments validate that our proposed method consistently facilitates existing state-of-the-art PETL methods across various backbones and datasets in vision-language retrieval and standard visual classification benchmarks. Besides, as a training supervision, ReSoRA can be seamlessly integrated into existing approaches in a plug-and-play manner, with no additional inference costs. Code is publicly available at: https://github.com/Lucenova/ReSoRA.

</details>


### [37] [Investigation of Accuracy and Bias in Face Recognition Trained with Synthetic Data](https://arxiv.org/abs/2507.20782)
*Pavel Korshunov,Ketan Kotwal,Christophe Ecabert,Vidit Vidit,Amir Mohammadi,Sebastien Marcel*

Main category: cs.CV

TL;DR: 论文探讨了合成数据在训练人脸识别模型中的潜力，评估了其对性能和偏见的双重影响，并提出了构建更公平系统的实用指南。


<details>
  <summary>Details</summary>
Motivation: 研究合成数据是否能在保持高准确性的同时减少人脸识别系统的偏见。

Method: 使用Flux.1-dev和Stable Diffusion v3.5生成平衡数据集FairFaceGen，结合多种身份增强方法，并与真实数据集进行比较。

Result: 合成数据在IJB-B/C上泛化能力稍逊，但SD35生成的平衡数据集在减少偏见方面表现良好。

Conclusion: 合成数据在构建公平的人脸识别系统中有潜力，但需关注增强的数量和质量。

Abstract: Synthetic data has emerged as a promising alternative for training face recognition (FR) models, offering advantages in scalability, privacy compliance, and potential for bias mitigation. However, critical questions remain on whether both high accuracy and fairness can be achieved with synthetic data. In this work, we evaluate the impact of synthetic data on bias and performance of FR systems. We generate balanced face dataset, FairFaceGen, using two state of the art text-to-image generators, Flux.1-dev and Stable Diffusion v3.5 (SD35), and combine them with several identity augmentation methods, including Arc2Face and four IP-Adapters. By maintaining equal identity count across synthetic and real datasets, we ensure fair comparisons when evaluating FR performance on standard (LFW, AgeDB-30, etc.) and challenging IJB-B/C benchmarks and FR bias on Racial Faces in-the-Wild (RFW) dataset. Our results demonstrate that although synthetic data still lags behind the real datasets in the generalization on IJB-B/C, demographically balanced synthetic datasets, especially those generated with SD35, show potential for bias mitigation. We also observe that the number and quality of intra-class augmentations significantly affect FR accuracy and fairness. These findings provide practical guidelines for constructing fairer FR systems using synthetic data.

</details>


### [38] [FantasyID: A dataset for detecting digital manipulations of ID-documents](https://arxiv.org/abs/2507.20808)
*Pavel Korshunov,Amir Mohammadi,Vidit Vidit,Christophe Ecabert,Sébastien Marcel*

Main category: cs.CV

TL;DR: 论文提出了一个名为FantasyID的新公开数据集，用于检测伪造身份证件，模拟真实KYC场景，挑战现有检测算法。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成技术的进步，伪造身份证件变得容易，这对KYC应用构成威胁，需要开发更强大的检测系统。

Method: 创建FantasyID数据集，模拟真实ID设计风格和语言，并打印后通过不同设备拍摄，同时模拟数字伪造攻击。

Result: 现有检测算法（如TruFor、MMFusion等）在FantasyID数据集上表现不佳，假阴性率接近50%。

Conclusion: FantasyID数据集复杂度高，适合作为检测算法的评估基准。

Abstract: Advancements in image generation led to the availability of easy-to-use tools for malicious actors to create forged images. These tools pose a serious threat to the widespread Know Your Customer (KYC) applications, requiring robust systems for detection of the forged Identity Documents (IDs). To facilitate the development of the detection algorithms, in this paper, we propose a novel publicly available (including commercial use) dataset, FantasyID, which mimics real-world IDs but without tampering with legal documents and, compared to previous public datasets, it does not contain generated faces or specimen watermarks. FantasyID contains ID cards with diverse design styles, languages, and faces of real people. To simulate a realistic KYC scenario, the cards from FantasyID were printed and captured with three different devices, constituting the bonafide class. We have emulated digital forgery/injection attacks that could be performed by a malicious actor to tamper the IDs using the existing generative tools. The current state-of-the-art forgery detection algorithms, such as TruFor, MMFusion, UniFD, and FatFormer, are challenged by FantasyID dataset. It especially evident, in the evaluation conditions close to practical, with the operational threshold set on validation set so that false positive rate is at 10%, leading to false negative rates close to 50% across the board on the test set. The evaluation experiments demonstrate that FantasyID dataset is complex enough to be used as an evaluation benchmark for detection algorithms.

</details>


### [39] [Compositional Video Synthesis by Temporal Object-Centric Learning](https://arxiv.org/abs/2507.20855)
*Adil Kaan Akan,Yucel Yemez*

Main category: cs.CV

TL;DR: 提出了一种基于对象中心表示的视频合成框架，扩展了之前的SlotAdapt方法，利用扩散模型实现高质量、时序一致的视频生成和编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏生成能力或忽略对象级结构，无法实现时序一致的对象编辑。

Method: 通过学习姿态不变的对象中心槽，并结合预训练扩散模型，实现高质量视频合成和编辑。

Result: 在视频生成质量和时序一致性上优于现有方法，支持对象插入、删除和替换。

Conclusion: 该方法在交互式视频生成和语义编辑方面具有潜力，推动了动态场景理解的发展。

Abstract: We present a novel framework for compositional video synthesis that leverages temporally consistent object-centric representations, extending our previous work, SlotAdapt, from images to video. While existing object-centric approaches either lack generative capabilities entirely or treat video sequences holistically, thus neglecting explicit object-level structure, our approach explicitly captures temporal dynamics by learning pose invariant object-centric slots and conditioning them on pretrained diffusion models. This design enables high-quality, pixel-level video synthesis with superior temporal coherence, and offers intuitive compositional editing capabilities such as object insertion, deletion, or replacement, maintaining consistent object identities across frames. Extensive experiments demonstrate that our method sets new benchmarks in video generation quality and temporal consistency, outperforming previous object-centric generative methods. Although our segmentation performance closely matches state-of-the-art methods, our approach uniquely integrates this capability with robust generative performance, significantly advancing interactive and controllable video generation and opening new possibilities for advanced content creation, semantic editing, and dynamic scene understanding.

</details>


### [40] [Exploring text-to-image generation for historical document image retrieval](https://arxiv.org/abs/2507.20934)
*Melissa Cote,Alexandra Branzan Albu*

Main category: cs.CV

TL;DR: 论文提出了一种结合生成式AI的方法T2I-QBE，通过文本到图像生成技术创建查询图像，用于历史文档图像检索，弥补了传统QBE和ABDIR的不足。


<details>
  <summary>Details</summary>
Motivation: 传统QBE搜索需要实际查询文档样本，而ABDIR虽然灵活但依赖视觉属性描述。本文旨在利用生成式AI（如文本到图像生成）填补两者之间的空白，尤其针对历史文档的多样性。

Method: 提出T2I-QBE方法，使用Leonardo.Ai生成基于ABDIR属性描述的查询图像，再通过传统QBE范式进行检索。

Result: 在HisIR19数据集上的实验验证了T2I-QBE的可行性，表明其可用于历史文档图像检索。

Conclusion: T2I-QBE是首次将文本到图像生成技术应用于文档图像检索的创新尝试，为历史文档检索提供了新思路。

Abstract: Attribute-based document image retrieval (ABDIR) was recently proposed as an alternative to query-by-example (QBE) searches, the dominant document image retrieval (DIR) paradigm. One drawback of QBE searches is that they require sample query documents on hand that may not be available. ABDIR aims to offer users a flexible way to retrieve document images based on memorable visual features of document contents, describing document images with combinations of visual attributes determined via convolutional neural network (CNN)-based binary classifiers. We present an exploratory study of the use of generative AI to bridge the gap between QBE and ABDIR, focusing on historical documents as a use case for their diversity and uniqueness in visual features. We hypothesize that text-to-image (T2I) generation can be leveraged to create query document images using text prompts based on ABDIR-like attributes. We propose T2I-QBE, which uses Leonardo.Ai as the T2I generator with prompts that include a rough description of the desired document type and a list of the desired ABDIR-style attributes. This creates query images that are then used within the traditional QBE paradigm, which compares CNN-extracted query features to those of the document images in the dataset to retrieve the most relevant documents. Experiments on the HisIR19 dataset of historical documents confirm our hypothesis and suggest that T2I-QBE is a viable option for historical document image retrieval. To the authors' knowledge, this is the first attempt at utilizing T2I generation for DIR.

</details>


### [41] [Mask-Free Audio-driven Talking Face Generation for Enhanced Visual Quality and Identity Preservation](https://arxiv.org/abs/2507.20953)
*Dogucan Yaman,Fevziye Irem Eyiokur,Leonard Bärmann,Hazım Kemal Ekenel,Alexander Waibel*

Main category: cs.CV

TL;DR: 论文提出了一种无需掩码的说话人脸生成方法，通过两步地标转换生成闭口图像，避免了传统掩码方法的信息丢失和身份参考问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于掩码的说话人脸生成方法中的信息丢失、身份参考不一致及负面干扰问题。

Method: 采用两步地标转换将输入图像转换为闭口状态，再通过唇部适配模型生成与音频同步的唇部动作。

Result: 在LRS2和HDTF数据集上验证了方法的有效性，无需掩码或身份参考图像。

Conclusion: 提出的无掩码方法在保持身份细节和视觉质量方面优于传统掩码策略。

Abstract: Audio-Driven Talking Face Generation aims at generating realistic videos of talking faces, focusing on accurate audio-lip synchronization without deteriorating any identity-related visual details. Recent state-of-the-art methods are based on inpainting, meaning that the lower half of the input face is masked, and the model fills the masked region by generating lips aligned with the given audio. Hence, to preserve identity-related visual details from the lower half, these approaches additionally require an unmasked identity reference image randomly selected from the same video. However, this common masking strategy suffers from (1) information loss in the input faces, significantly affecting the networks' ability to preserve visual quality and identity details, (2) variation between identity reference and input image degrading reconstruction performance, and (3) the identity reference negatively impacting the model, causing unintended copying of elements unaligned with the audio. To address these issues, we propose a mask-free talking face generation approach while maintaining the 2D-based face editing task. Instead of masking the lower half, we transform the input images to have closed mouths, using a two-step landmark-based approach trained in an unpaired manner. Subsequently, we provide these edited but unmasked faces to a lip adaptation model alongside the audio to generate appropriate lip movements. Thus, our approach needs neither masked input images nor identity reference images. We conduct experiments on the benchmark LRS2 and HDTF datasets and perform various ablation studies to validate our contributions.

</details>


### [42] [GTAD: Global Temporal Aggregation Denoising Learning for 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2507.20963)
*Tianhao Li,Yang Li,Mengtian Li,Yisheng Deng,Weifeng Ge*

Main category: cs.CV

TL;DR: 论文提出了一种名为GTAD的全局时间聚合去噪网络，用于高效利用历史观测中的全局时间信息，提升动态环境感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用全局时间序列信息，主要依赖相邻帧的局部时间交互，限制了动态环境感知的准确性和全面性。

Method: 提出GTAD网络，结合局部时间特征（当前帧）和全局时间特征（历史序列），通过去噪网络实现全局时间信息的高效聚合。

Result: 在nuScenes和Occ3D-nuScenes基准测试中表现出优越性，实现了更连贯和全面的环境理解。

Conclusion: GTAD通过全局时间信息聚合，显著提升了动态环境感知的准确性和全面性，为3D场景理解提供了新范式。

Abstract: Accurately perceiving dynamic environments is a fundamental task for autonomous driving and robotic systems. Existing methods inadequately utilize temporal information, relying mainly on local temporal interactions between adjacent frames and failing to leverage global sequence information effectively. To address this limitation, we investigate how to effectively aggregate global temporal features from temporal sequences, aiming to achieve occupancy representations that efficiently utilize global temporal information from historical observations. For this purpose, we propose a global temporal aggregation denoising network named GTAD, introducing a global temporal information aggregation framework as a new paradigm for holistic 3D scene understanding. Our method employs an in-model latent denoising network to aggregate local temporal features from the current moment and global temporal features from historical sequences. This approach enables the effective perception of both fine-grained temporal information from adjacent frames and global temporal patterns from historical observations. As a result, it provides a more coherent and comprehensive understanding of the environment. Extensive experiments on the nuScenes and Occ3D-nuScenes benchmark and ablation studies demonstrate the superiority of our method.

</details>


### [43] [JWB-DH-V1: Benchmark for Joint Whole-Body Talking Avatar and Speech Generation Version 1](https://arxiv.org/abs/2507.20987)
*Xinhan Di,Kristin Qi,Pengqian Yu*

Main category: cs.CV

TL;DR: 论文介绍了JWB-DH-V1数据集和评估协议，用于解决当前扩散视频生成中全身动作与语音联合生成的多模态一致性问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法在联合生成全身动作和自然语音时缺乏多模态一致性，且缺乏全面的评估框架和区域性能分析基准。

Method: 提出了JWB-DH-V1，包含大规模多模态数据集和评估协议，用于评估全身可动画化虚拟形象的联合音视频生成。

Result: 评估显示，现有模型在面部/手部与全身性能之间存在显著差异，为未来研究指明了方向。

Conclusion: JWB-DH-V1为全身动作与语音联合生成提供了数据集和评估工具，填补了研究空白。

Abstract: Recent advances in diffusion-based video generation have enabled photo-realistic short clips, but current methods still struggle to achieve multi-modal consistency when jointly generating whole-body motion and natural speech. Current approaches lack comprehensive evaluation frameworks that assess both visual and audio quality, and there are insufficient benchmarks for region-specific performance analysis. To address these gaps, we introduce the Joint Whole-Body Talking Avatar and Speech Generation Version I(JWB-DH-V1), comprising a large-scale multi-modal dataset with 10,000 unique identities across 2 million video samples, and an evaluation protocol for assessing joint audio-video generation of whole-body animatable avatars. Our evaluation of SOTA models reveals consistent performance disparities between face/hand-centric and whole-body performance, which incidates essential areas for future research. The dataset and evaluation tools are publicly available at https://github.com/deepreasonings/WholeBodyBenchmark.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [44] [ConSeg: Contextual Backdoor Attack Against Semantic Segmentation](https://arxiv.org/abs/2507.19905)
*Bilal Hussain Abbasi,Zirui Gong,Yanjun Zhang,Shang Gao,Antonio Robles-Kelly,Leo Zhang*

Main category: cs.CR

TL;DR: 论文提出了一种名为ConSeg的后门攻击方法，利用语义分割模型的上下文信息提升攻击效果，实验表明其攻击成功率比现有方法提高15.55%，且能抵抗先进防御。


<details>
  <summary>Details</summary>
Motivation: 语义分割模型易受后门攻击威胁，现有方法效果有限，需探索更有效的攻击方式。

Method: 提出ConSeg方法，通过模拟目标类的上下文信息并在受害区域重建，增强后门攻击效果。

Result: ConSeg攻击成功率比现有方法提高15.55%，且对先进防御具有鲁棒性。

Conclusion: ConSeg是一种简单有效的后门攻击方法，利用上下文信息显著提升攻击性能。

Abstract: Despite significant advancements in computer vision, semantic segmentation models may be susceptible to backdoor attacks. These attacks, involving hidden triggers, aim to cause the models to misclassify instances of the victim class as the target class when triggers are present, posing serious threats to the reliability of these models. To further explore the field of backdoor attacks against semantic segmentation, in this paper, we propose a simple yet effective backdoor attack called Contextual Segmentation Backdoor Attack (ConSeg). ConSeg leverages the contextual information inherent in semantic segmentation models to enhance backdoor performance. Our method is motivated by an intriguing observation, i.e., when the target class is set as the `co-occurring' class of the victim class, the victim class can be more easily `mis-segmented'. Building upon this insight, ConSeg mimics the contextual information of the target class and rebuilds it in the victim region to establish the contextual relationship between the target class and the victim class, making the attack easier. Our experiments reveal that ConSeg achieves improvements in Attack Success Rate (ASR) with increases of 15.55\%, compared to existing methods, while exhibiting resilience against state-of-the-art backdoor defenses.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [CLoRA: Parameter-Efficient Continual Learning with Low-Rank Adaptation](https://arxiv.org/abs/2507.19887)
*Shishir Muralidhara,Didier Stricker,René Schuster*

Main category: cs.LG

TL;DR: CLoRA利用低秩适应（LoRA）方法，解决了持续学习中计算资源受限的问题，显著减少了训练所需的硬件资源。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，持续学习方法通常需要大量计算资源重新训练模型，限制了其在资源受限环境中的应用。

Method: CLoRA采用低秩适应（LoRA）方法，仅使用少量参数进行任务学习，避免了全模型重新训练。

Result: CLoRA在性能上与基线方法相当甚至更优，同时显著降低了硬件需求。

Conclusion: CLoRA为资源受限环境下的持续学习提供了高效解决方案。

Abstract: In the past, continual learning (CL) was mostly concerned with the problem of catastrophic forgetting in neural networks, that arises when incrementally learning a sequence of tasks. Current CL methods function within the confines of limited data access, without any restrictions imposed on computational resources. However, in real-world scenarios, the latter takes precedence as deployed systems are often computationally constrained. A major drawback of most CL methods is the need to retrain the entire model for each new task. The computational demands of retraining large models can be prohibitive, limiting the applicability of CL in environments with limited resources. Through CLoRA, we explore the applicability of Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method for class-incremental semantic segmentation. CLoRA leverages a small set of parameters of the model and uses the same set for learning across all tasks. Results demonstrate the efficacy of CLoRA, achieving performance on par with and exceeding the baseline methods. We further evaluate CLoRA using NetScore, underscoring the need to factor in resource efficiency and evaluate CL methods beyond task performance. CLoRA significantly reduces the hardware requirements for training, making it well-suited for CL in resource-constrained environments after deployment.

</details>


### [46] [WEEP: A Differentiable Nonconvex Sparse Regularizer via Weakly-Convex Envelope](https://arxiv.org/abs/2507.20447)
*Takanobu Furuhashi,Hidekata Hontani,Tatsuya Yokota*

Main category: cs.LG

TL;DR: 论文提出了一种名为WEEP的新型稀疏正则化方法，解决了传统稀疏正则化方法在非可微性与梯度优化器兼容性之间的冲突。


<details>
  <summary>Details</summary>
Motivation: 稀疏正则化在信号处理中至关重要，但最强的稀疏诱导惩罚通常不可微，与主流的梯度优化器不兼容。

Method: 通过弱凸包络框架，提出了完全可微的稀疏正则化方法WEEP，兼具强稀疏性和L-平滑性。

Result: 在信号和图像去噪任务中，WEEP表现优于L1范数和其他非凸稀疏正则化方法。

Conclusion: WEEP解决了统计性能与计算可行性之间的冲突，为稀疏正则化提供了新的解决方案。

Abstract: Sparse regularization is fundamental in signal processing for efficient signal recovery and feature extraction. However, it faces a fundamental dilemma: the most powerful sparsity-inducing penalties are often non-differentiable, conflicting with gradient-based optimizers that dominate the field. We introduce WEEP (Weakly-convex Envelope of Piecewise Penalty), a novel, fully differentiable sparse regularizer derived from the weakly-convex envelope framework. WEEP provides strong, unbiased sparsity while maintaining full differentiability and L-smoothness, making it natively compatible with any gradient-based optimizer. This resolves the conflict between statistical performance and computational tractability. We demonstrate superior performance compared to the L1-norm and other established non-convex sparse regularizers on challenging signal and image denoising tasks.

</details>


### [47] [Model-Agnostic Gender Bias Control for Text-to-Image Generation via Sparse Autoencoder](https://arxiv.org/abs/2507.20973)
*Chao Wu,Zhenyi Wang,Kangxian Xie,Naresh Kumar Devulapally,Vishnu Suresh Lokhande,Mingchen Gao*

Main category: cs.LG

TL;DR: SAE Debias是一个轻量级、模型无关的框架，用于减少文本到图像（T2I）扩散模型中的性别偏见，通过稀疏自编码器在特征空间中识别和抑制性别相关方向。


<details>
  <summary>Details</summary>
Motivation: T2I扩散模型常表现出性别偏见，例如将职业与特定性别刻板关联。现有方法依赖CLIP过滤或提示工程，效果有限且需模型特定调整。

Method: 利用预训练的k稀疏自编码器在稀疏潜在空间中识别性别相关方向，构建职业相关的偏见方向并在推理时抑制，以生成更性别平衡的图像。

Result: 在多个T2I模型（如Stable Diffusion系列）上评估，SAE Debias显著减少性别偏见，同时保持生成质量。

Conclusion: SAE Debias是首个利用稀疏自编码器识别和干预T2I模型中性别偏见的工作，为构建社会责任的生成AI提供了可解释且模型无关的工具。

Abstract: Text-to-image (T2I) diffusion models often exhibit gender bias, particularly by generating stereotypical associations between professions and gendered subjects. This paper presents SAE Debias, a lightweight and model-agnostic framework for mitigating such bias in T2I generation. Unlike prior approaches that rely on CLIP-based filtering or prompt engineering, which often require model-specific adjustments and offer limited control, SAE Debias operates directly within the feature space without retraining or architectural modifications. By leveraging a k-sparse autoencoder pre-trained on a gender bias dataset, the method identifies gender-relevant directions within the sparse latent space, capturing professional stereotypes. Specifically, a biased direction per profession is constructed from sparse latents and suppressed during inference to steer generations toward more gender-balanced outputs. Trained only once, the sparse autoencoder provides a reusable debiasing direction, offering effective control and interpretable insight into biased subspaces. Extensive evaluations across multiple T2I models, including Stable Diffusion 1.4, 1.5, 2.1, and SDXL, demonstrate that SAE Debias substantially reduces gender bias while preserving generation quality. To the best of our knowledge, this is the first work to apply sparse autoencoders for identifying and intervening in gender bias within T2I models. These findings contribute toward building socially responsible generative AI, providing an interpretable and model-agnostic tool to support fairness in text-to-image generation.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [48] [Rainbow Noise: Stress-Testing Multimodal Harmful-Meme Detectors on LGBTQ Content](https://arxiv.org/abs/2507.19551)
*Ran Tong,Songtao Wei,Jiaqi Liu,Lanruo Wang*

Main category: cs.CY

TL;DR: 论文构建了首个针对LGBTQ+社区仇恨模因的鲁棒性基准，测试了两种先进检测器（MemeCLIP和MemeBLIP2）在多种攻击下的表现，并提出了轻量级文本去噪适配器（TDA）以提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 仇恨模因通过修改文本或图像逃避检测，现有模型在此类攻击下表现不佳，亟需提升鲁棒性。

Method: 结合四种文本攻击和三种图像扰动，测试两种检测器（MemeCLIP和MemeBLIP2）的性能，并引入TDA增强MemeBLIP2。

Result: MemeCLIP表现更稳定，MemeBLIP2对文本攻击敏感，但TDA使其成为最鲁棒的模型。

Conclusion: 当前多模态安全模型存在漏洞，轻量级模块（如TDA）是提升防御能力的有效途径。

Abstract: Hateful memes aimed at LGBTQ\,+ communities often evade detection by tweaking either the caption, the image, or both. We build the first robustness benchmark for this setting, pairing four realistic caption attacks with three canonical image corruptions and testing all combinations on the PrideMM dataset. Two state-of-the-art detectors, MemeCLIP and MemeBLIP2, serve as case studies, and we introduce a lightweight \textbf{Text Denoising Adapter (TDA)} to enhance the latter's resilience. Across the grid, MemeCLIP degrades more gently, while MemeBLIP2 is particularly sensitive to the caption edits that disrupt its language processing. However, the addition of the TDA not only remedies this weakness but makes MemeBLIP2 the most robust model overall. Ablations reveal that all systems lean heavily on text, but architectural choices and pre-training data significantly impact robustness. Our benchmark exposes where current multimodal safety models crack and demonstrates that targeted, lightweight modules like the TDA offer a powerful path towards stronger defences.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [49] [Review of Deep Learning Applications to Structural Proteomics Enabled by Cryogenic Electron Microscopy and Tomography](https://arxiv.org/abs/2507.19565)
*Brady K. Zhou,Jason J. Hu,Jane K. J. Lee,Z. Hong Zhou,Demetri Terzopoulos*

Main category: q-bio.QM

TL;DR: 综述探讨了深度学习在冷冻电镜（cryoEM）和冷冻电子断层扫描（cryoET）中的应用，解决了信号噪声、取向偏差等问题，并实现了自动化原子模型构建。


<details>
  <summary>Details</summary>
Motivation: 解决冷冻电镜技术中的低信噪比、取向偏差和缺失楔形问题，提高效率和可扩展性。

Method: 利用卷积神经网络（如Topaz、crYOLO）进行自动颗粒挑选，采用U-Net架构（如IsoNet）进行缺失楔形校正和降噪，以及自动化原子模型构建工具（如ModelAngelo）。

Result: 实现了近原子分辨率的重建，解决了严重取向偏差的数据集，并成功应用于多种生物系统。

Conclusion: 随着深度学习的进步，未来将实现更高级的自动化和可访问性，推动对生物大分子结构和功能的理解。

Abstract: The past decade's "cryoEM revolution" has produced exponential growth in high-resolution structural data through advances in cryogenic electron microscopy (cryoEM) and tomography (cryoET). Deep learning integration into structural proteomics workflows addresses longstanding challenges including low signal-to-noise ratios, preferred orientation artifacts, and missing-wedge problems that historically limited efficiency and scalability. This review examines AI applications across the entire cryoEM pipeline, from automated particle picking using convolutional neural networks (Topaz, crYOLO, CryoSegNet) to computational solutions for preferred orientation bias (spIsoNet, cryoPROS) and advanced denoising algorithms (Topaz-Denoise). In cryoET, tools like IsoNet employ U-Net architectures for simultaneous missing-wedge correction and noise reduction, while TomoNet streamlines subtomogram averaging through AI-driven particle detection. The workflow culminates with automated atomic model building using sophisticated tools like ModelAngelo, DeepTracer, and CryoREAD that translate density maps into interpretable biological structures. These AI-enhanced approaches have achieved near-atomic resolution reconstructions with minimal manual intervention, resolved previously intractable datasets suffering from severe orientation bias, and enabled successful application to diverse biological systems from HIV virus-like particles to in situ ribosomal complexes. As deep learning evolves, particularly with large language models and vision transformers, the future promises sophisticated automation and accessibility in structural biology, potentially revolutionizing our understanding of macromolecular architecture and function.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [50] [SkinDualGen: Prompt-Driven Diffusion for Simultaneous Image-Mask Generation in Skin Lesions](https://arxiv.org/abs/2507.19970)
*Zhaobin Xu*

Main category: eess.IV

TL;DR: 提出一种基于Stable Diffusion-2.0的方法，生成高质量合成皮肤病变图像和分割掩码，解决数据稀缺和类别不平衡问题，显著提升分类和分割模型的性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分析在疾病早期诊断中至关重要，但数据稀缺和类别不平衡限制了深度学习模型的性能。

Method: 通过领域特定的LoRA微调和多目标损失函数联合优化，使Stable Diffusion-2.0能一步生成临床相关图像和分割掩码。

Result: 生成的图像质量接近真实图像（FID验证），混合数据集显著提升模型性能（准确率和F1-score提高8%-15%，其他关键指标如Dice系数和IoU也有提升）。

Conclusion: 该方法为医学影像数据挑战提供了可扩展的解决方案，提高了罕见疾病诊断的准确性和可靠性。

Abstract: Medical image analysis plays a pivotal role in the early diagnosis of diseases such as skin lesions. However, the scarcity of data and the class imbalance significantly hinder the performance of deep learning models. We propose a novel method that leverages the pretrained Stable Diffusion-2.0 model to generate high-quality synthetic skin lesion images and corresponding segmentation masks. This approach augments training datasets for classification and segmentation tasks. We adapt Stable Diffusion-2.0 through domain-specific Low-Rank Adaptation (LoRA) fine-tuning and joint optimization of multi-objective loss functions, enabling the model to simultaneously generate clinically relevant images and segmentation masks conditioned on textual descriptions in a single step. Experimental results show that the generated images, validated by FID scores, closely resemble real images in quality. A hybrid dataset combining real and synthetic data markedly enhances the performance of classification and segmentation models, achieving substantial improvements in accuracy and F1-score of 8% to 15%, with additional positive gains in other key metrics such as the Dice coefficient and IoU. Our approach offers a scalable solution to address the challenges of medical imaging data, contributing to improved accuracy and reliability in diagnosing rare diseases.

</details>


### [51] [Onboard Hyperspectral Super-Resolution with Deep Pushbroom Neural Network](https://arxiv.org/abs/2507.20765)
*Davide Piccinini,Diego Valsesia,Enrico Magli*

Main category: eess.IV

TL;DR: 论文提出了一种轻量级的神经网络设计DPSR，用于实时提升卫星上高光谱图像的空间分辨率，满足星载实时处理需求。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像具有精细的光谱特征，但空间分辨率有限。提升分辨率有助于下游任务，同时星载实时处理需求推动了轻量级超分辨率方法的发展。

Method: 设计了DPSR神经网络，通过逐行处理图像并利用因果记忆机制，降低内存和计算复杂度，实现星载实时性能。

Result: 实验表明，DPSR在超分辨率图像质量上优于或媲美更复杂的现有方法。

Conclusion: DPSR是一种高效且轻量级的超分辨率方法，适用于星载实时处理需求。

Abstract: Hyperspectral imagers on satellites obtain the fine spectral signatures essential for distinguishing one material from another at the expense of limited spatial resolution. Enhancing the latter is thus a desirable preprocessing step in order to further improve the detection capabilities offered by hyperspectral images on downstream tasks. At the same time, there is a growing interest towards deploying inference methods directly onboard of satellites, which calls for lightweight image super-resolution methods that can be run on the payload in real time. In this paper, we present a novel neural network design, called Deep Pushbroom Super-Resolution (DPSR) that matches the pushbroom acquisition of hyperspectral sensors by processing an image line by line in the along-track direction with a causal memory mechanism to exploit previously acquired lines. This design greatly limits memory requirements and computational complexity, achieving onboard real-time performance, i.e., the ability to super-resolve a line in the time it takes to acquire the next one, on low-power hardware. Experiments show that the quality of the super-resolved images is competitive or even outperforms state-of-the-art methods that are significantly more complex.

</details>
