{"id": "2507.03256", "pdf": "https://arxiv.org/pdf/2507.03256", "abs": "https://arxiv.org/abs/2507.03256", "authors": ["Xinyang Li", "Gen Li", "Zhihui Lin", "Yichen Qian", "GongXin Yao", "Weinan Jia", "Weihua Chen", "Fan Wang"], "title": "MoDA: Multi-modal Diffusion Architecture for Talking Head Generation", "categories": ["cs.GR", "cs.CV"], "comment": "12 pages, 7 figures", "summary": "Talking head generation with arbitrary identities and speech audio remains a crucial problem in the realm of digital humans and the virtual metaverse. Recently, diffusion models have become a popular generative technique in this field with their strong generation and generalization capabilities. However, several challenges remain for diffusion-based methods: 1) inefficient inference and visual artifacts, which arise from the implicit latent space of Variational Auto-Encoders (VAE), complicating the diffusion process; 2) authentic facial expressions and head movements, resulting from insufficient multi-modal information interaction. In this paper, MoDA handle these challenges by 1) defines a joint parameter space to bridge motion generation and neural rendering, and leverages flow matching to simplify the diffusion learning process; 2) introduces a multi-modal diffusion architecture to model the interaction among noisy motion, audio, and auxiliary conditions, ultimately enhancing overall facial expressiveness. Subsequently, a coarse-to-fine fusion strategy is adopted to progressively integrate different modalities, ensuring effective integration across feature spaces. Experimental results demonstrate that MoDA significantly improves video diversity, realism, and efficiency, making it suitable for real-world applications.", "AI": {"tldr": "MoDA\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u4efb\u610f\u8eab\u4efd\u548c\u8bed\u97f3\u97f3\u9891\u7684\u903c\u771f\u8bf4\u8bdd\u5934\u90e8\u89c6\u9891\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u4f4e\u6548\u63a8\u7406\u548c\u89c6\u89c9\u4f2a\u5f71\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u8bf4\u8bdd\u5934\u90e8\u751f\u6210\u4e2d\u7684\u4f4e\u6548\u63a8\u7406\u3001\u89c6\u89c9\u4f2a\u5f71\u4ee5\u53ca\u591a\u6a21\u6001\u4fe1\u606f\u4ea4\u4e92\u4e0d\u8db3\u5bfc\u81f4\u7684\u771f\u5b9e\u6027\u95ee\u9898\u3002", "method": "1) \u5b9a\u4e49\u8054\u5408\u53c2\u6570\u7a7a\u95f4\u8fde\u63a5\u8fd0\u52a8\u751f\u6210\u4e0e\u795e\u7ecf\u6e32\u67d3\uff0c\u5229\u7528\u6d41\u5339\u914d\u7b80\u5316\u6269\u6563\u5b66\u4e60\uff1b2) \u5f15\u5165\u591a\u6a21\u6001\u6269\u6563\u67b6\u6784\u5efa\u6a21\u566a\u58f0\u8fd0\u52a8\u3001\u97f3\u9891\u548c\u8f85\u52a9\u6761\u4ef6\u7684\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMoDA\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u7684\u591a\u6837\u6027\u3001\u771f\u5b9e\u6027\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "MoDA\u901a\u8fc7\u8054\u5408\u53c2\u6570\u7a7a\u95f4\u548c\u591a\u6a21\u6001\u67b6\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u8bf4\u8bdd\u5934\u90e8\u751f\u6210\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2507.03836", "pdf": "https://arxiv.org/pdf/2507.03836", "abs": "https://arxiv.org/abs/2507.03836", "authors": ["Jianxin Sun", "David Lenz", "Hongfeng Yu", "Tom Peterka"], "title": "F-Hash: Feature-Based Hash Design for Time-Varying Volume Visualization via Multi-Resolution Tesseract Encoding", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Interactive time-varying volume visualization is challenging due to its complex spatiotemporal features and sheer size of the dataset. Recent works transform the original discrete time-varying volumetric data into continuous Implicit Neural Representations (INR) to address the issues of compression, rendering, and super-resolution in both spatial and temporal domains. However, training the INR takes a long time to converge, especially when handling large-scale time-varying volumetric datasets. In this work, we proposed F-Hash, a novel feature-based multi-resolution Tesseract encoding architecture to greatly enhance the convergence speed compared with existing input encoding methods for modeling time-varying volumetric data. The proposed design incorporates multi-level collision-free hash functions that map dynamic 4D multi-resolution embedding grids without bucket waste, achieving high encoding capacity with compact encoding parameters. Our encoding method is agnostic to time-varying feature detection methods, making it a unified encoding solution for feature tracking and evolution visualization. Experiments show the F-Hash achieves state-of-the-art convergence speed in training various time-varying volumetric datasets for diverse features. We also proposed an adaptive ray marching algorithm to optimize the sample streaming for faster rendering of the time-varying neural representation.", "AI": {"tldr": "F-Hash\u662f\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u7684\u591a\u5206\u8fa8\u7387Tesseract\u7f16\u7801\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u53d8\u5316\u4f53\u79ef\u6570\u636e\u7684\u5efa\u6a21\u6536\u655b\u901f\u5ea6\uff0c\u540c\u65f6\u652f\u6301\u7279\u5f81\u8ddf\u8e2a\u548c\u6f14\u5316\u53ef\u89c6\u5316\u3002", "motivation": "\u65f6\u95f4\u53d8\u5316\u4f53\u79ef\u6570\u636e\u7684\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u56e0\u590d\u6742\u7684\u65f6\u7a7a\u7279\u5f81\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u800c\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u4f46\u8bad\u7ec3\u6536\u655b\u901f\u5ea6\u6162\u3002", "method": "\u63d0\u51faF-Hash\uff0c\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u7684\u591a\u5206\u8fa8\u7387Tesseract\u7f16\u7801\u67b6\u6784\uff0c\u7ed3\u5408\u591a\u7ea7\u65e0\u78b0\u649e\u54c8\u5e0c\u51fd\u6570\uff0c\u9ad8\u6548\u6620\u5c04\u52a8\u60014D\u591a\u5206\u8fa8\u7387\u5d4c\u5165\u7f51\u683c\u3002", "result": "F-Hash\u5728\u591a\u79cd\u65f6\u95f4\u53d8\u5316\u4f53\u79ef\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8bad\u7ec3\u6536\u655b\u901f\u5ea6\uff0c\u5e76\u652f\u6301\u7279\u5f81\u8ddf\u8e2a\u548c\u6f14\u5316\u53ef\u89c6\u5316\u3002", "conclusion": "F-Hash\u4e3a\u65f6\u95f4\u53d8\u5316\u4f53\u79ef\u6570\u636e\u7684\u5efa\u6a21\u548c\u6e32\u67d3\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u7f16\u7801\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.04147", "pdf": "https://arxiv.org/pdf/2507.04147", "abs": "https://arxiv.org/abs/2507.04147", "authors": ["Shuo Xin", "Haiyu Wang", "Sai Qian Zhang"], "title": "A3FR: Agile 3D Gaussian Splatting with Incremental Gaze Tracked Foveated Rendering in Virtual Reality", "categories": ["cs.GR", "cs.CV", "cs.DC"], "comment": "ACM International Conference on Supercomputing 2025", "summary": "Virtual reality (VR) significantly transforms immersive digital interfaces, greatly enhancing education, professional practices, and entertainment by increasing user engagement and opening up new possibilities in various industries. Among its numerous applications, image rendering is crucial. Nevertheless, rendering methodologies like 3D Gaussian Splatting impose high computational demands, driven predominantly by user expectations for superior visual quality. This results in notable processing delays for real-time image rendering, which greatly affects the user experience. Additionally, VR devices such as head-mounted displays (HMDs) are intricately linked to human visual behavior, leveraging knowledge from perception and cognition to improve user experience. These insights have spurred the development of foveated rendering, a technique that dynamically adjusts rendering resolution based on the user's gaze direction. The resultant solution, known as gaze-tracked foveated rendering, significantly reduces the computational burden of the rendering process.   Although gaze-tracked foveated rendering can reduce rendering costs, the computational overhead of the gaze tracking process itself can sometimes outweigh the rendering savings, leading to increased processing latency. To address this issue, we propose an efficient rendering framework called~\\textit{A3FR}, designed to minimize the latency of gaze-tracked foveated rendering via the parallelization of gaze tracking and foveated rendering processes. For the rendering algorithm, we utilize 3D Gaussian Splatting, a state-of-the-art neural rendering technique. Evaluation results demonstrate that A3FR can reduce end-to-end rendering latency by up to $2\\times$ while maintaining visual quality.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aA3FR\u7684\u9ad8\u6548\u6e32\u67d3\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u884c\u5316\u89c6\u7ebf\u8ddf\u8e2a\u548c\u6ce8\u89c6\u70b9\u6e32\u67d3\u8fc7\u7a0b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6e32\u67d3\u5ef6\u8fdf\u3002", "motivation": "\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u56fe\u50cf\u6e32\u67d3\u5bf9\u8ba1\u7b97\u8d44\u6e90\u8981\u6c42\u9ad8\uff0c\u800c\u73b0\u6709\u7684\u6ce8\u89c6\u70b9\u6e32\u67d3\u6280\u672f\u867d\u7136\u80fd\u51cf\u5c11\u6e32\u67d3\u6210\u672c\uff0c\u4f46\u89c6\u7ebf\u8ddf\u8e2a\u8fc7\u7a0b\u672c\u8eab\u7684\u8ba1\u7b97\u5f00\u9500\u53ef\u80fd\u62b5\u6d88\u5176\u4f18\u52bf\uff0c\u5bfc\u81f4\u5ef6\u8fdf\u589e\u52a0\u3002", "method": "\u91c7\u75283D\u9ad8\u65af\u6cfc\u6e85\u4f5c\u4e3a\u6e32\u67d3\u7b97\u6cd5\uff0c\u5e76\u8bbe\u8ba1A3FR\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u884c\u5316\u89c6\u7ebf\u8ddf\u8e2a\u548c\u6ce8\u89c6\u70b9\u6e32\u67d3\u6765\u4f18\u5316\u6027\u80fd\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cA3FR\u80fd\u5c06\u7aef\u5230\u7aef\u6e32\u67d3\u5ef6\u8fdf\u964d\u4f4e\u81f3\u591a2\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "A3FR\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6ce8\u89c6\u70b9\u6e32\u67d3\u4e2d\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u4e3a\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u5b9e\u65f6\u56fe\u50cf\u6e32\u67d3\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.02900", "pdf": "https://arxiv.org/pdf/2507.02900", "abs": "https://arxiv.org/abs/2507.02900", "authors": ["Vineet Kumar Rakesh", "Soumya Mazumdar", "Research Pratim Maity", "Sarbajit Pal", "Amitabha Das", "Tapas Samanta"], "title": "Advancing Talking Head Generation: A Comprehensive Survey of Multi-Modal Methodologies, Datasets, Evaluation Metrics, and Loss Functions", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.HC", "cs.MM"], "comment": null, "summary": "Talking Head Generation (THG) has emerged as a transformative technology in computer vision, enabling the synthesis of realistic human faces synchronized with image, audio, text, or video inputs. This paper provides a comprehensive review of methodologies and frameworks for talking head generation, categorizing approaches into 2D--based, 3D--based, Neural Radiance Fields (NeRF)--based, diffusion--based, parameter-driven techniques and many other techniques. It evaluates algorithms, datasets, and evaluation metrics while highlighting advancements in perceptual realism and technical efficiency critical for applications such as digital avatars, video dubbing, ultra-low bitrate video conferencing, and online education. The study identifies challenges such as reliance on pre--trained models, extreme pose handling, multilingual synthesis, and temporal consistency. Future directions include modular architectures, multilingual datasets, hybrid models blending pre--trained and task-specific layers, and innovative loss functions. By synthesizing existing research and exploring emerging trends, this paper aims to provide actionable insights for researchers and practitioners in the field of talking head generation. For the complete survey, code, and curated resource list, visit our GitHub repository: https://github.com/VineetKumarRakesh/thg.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u8bf4\u8bdd\u5934\u751f\u6210\uff08THG\uff09\u7684\u6280\u672f\uff0c\u5206\u7c7b\u4e86\u591a\u79cd\u65b9\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u7b97\u6cd5\u3001\u6570\u636e\u96c6\u548c\u6307\u6807\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "THG\u6280\u672f\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5b58\u5728\u6280\u672f\u6311\u6218\uff0c\u9700\u8981\u7cfb\u7edf\u603b\u7ed3\u548c\u672a\u6765\u65b9\u5411\u63a2\u8ba8\u3002", "method": "\u5206\u7c7b\u4e862D\u30013D\u3001NeRF\u3001\u6269\u6563\u6a21\u578b\u7b49\u591a\u79cd\u65b9\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u76f8\u5173\u7b97\u6cd5\u3001\u6570\u636e\u96c6\u548c\u6307\u6807\u3002", "result": "\u603b\u7ed3\u4e86THG\u7684\u8fdb\u5c55\uff0c\u6307\u51fa\u4e86\u4f9d\u8d56\u9884\u8bad\u7ec3\u6a21\u578b\u3001\u6781\u7aef\u59ff\u6001\u5904\u7406\u7b49\u6311\u6218\u3002", "conclusion": "\u672a\u6765\u65b9\u5411\u5305\u62ec\u6a21\u5757\u5316\u67b6\u6784\u3001\u591a\u8bed\u8a00\u6570\u636e\u96c6\u7b49\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2507.03166", "pdf": "https://arxiv.org/pdf/2507.03166", "abs": "https://arxiv.org/abs/2507.03166", "authors": ["Daniel Berio", "Guillaume Clivaz", "Michael Stroh", "Oliver Deussen", "R\u00e9jean Plamondon", "Sylvain Calinon", "Frederic Fol Leymarie"], "title": "Image-driven Robot Drawing with Rapid Lognormal Movements", "categories": ["cs.RO", "cs.GR"], "comment": "Accepted at IEEE RO-MAN 2025", "summary": "Large image generation and vision models, combined with differentiable rendering technologies, have become powerful tools for generating paths that can be drawn or painted by a robot. However, these tools often overlook the intrinsic physicality of the human drawing/writing act, which is usually executed with skillful hand/arm gestures. Taking this into account is important for the visual aesthetics of the results and for the development of closer and more intuitive artist-robot collaboration scenarios. We present a method that bridges this gap by enabling gradient-based optimization of natural human-like motions guided by cost functions defined in image space. To this end, we use the sigma-lognormal model of human hand/arm movements, with an adaptation that enables its use in conjunction with a differentiable vector graphics (DiffVG) renderer. We demonstrate how this pipeline can be used to generate feasible trajectories for a robot by combining image-driven objectives with a minimum-time smoothing criterion. We demonstrate applications with generation and robotic reproduction of synthetic graffiti as well as image abstraction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eba\u7c7b\u624b/\u81c2\u8fd0\u52a8\u6a21\u578b\u548c\u53ef\u5fae\u5206\u6e32\u67d3\u6280\u672f\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u673a\u5668\u4eba\u7ed8\u753b\u7684\u81ea\u7136\u52a8\u4f5c\u8f68\u8ff9\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u56fe\u50cf\u751f\u6210\u548c\u89c6\u89c9\u6a21\u578b\u5ffd\u7565\u4e86\u4eba\u7c7b\u7ed8\u753b/\u4e66\u5199\u884c\u4e3a\u7684\u7269\u7406\u7279\u6027\uff0c\u5f71\u54cd\u89c6\u89c9\u6548\u679c\u548c\u827a\u672f\u5bb6-\u673a\u5668\u4eba\u534f\u4f5c\u7684\u76f4\u89c2\u6027\u3002", "method": "\u4f7f\u7528sigma-lognormal\u6a21\u578b\u63cf\u8ff0\u4eba\u7c7b\u624b/\u81c2\u8fd0\u52a8\uff0c\u5e76\u7ed3\u5408\u53ef\u5fae\u5206\u77e2\u91cf\u56fe\u5f62\u6e32\u67d3\u5668\uff08DiffVG\uff09\uff0c\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\u751f\u6210\u81ea\u7136\u52a8\u4f5c\u8f68\u8ff9\u3002", "result": "\u8be5\u65b9\u6cd5\u6210\u529f\u751f\u6210\u4e86\u673a\u5668\u4eba\u53ef\u884c\u7684\u8f68\u8ff9\uff0c\u5e76\u5e94\u7528\u4e8e\u5408\u6210\u6d82\u9e26\u548c\u56fe\u50cf\u62bd\u8c61\u7684\u751f\u6210\u4e0e\u518d\u73b0\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u56fe\u50cf\u9a71\u52a8\u76ee\u6807\u548c\u6700\u5c0f\u65f6\u95f4\u5e73\u6ed1\u51c6\u5219\uff0c\u8be5\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u7ed8\u753b\u63d0\u4f9b\u4e86\u66f4\u81ea\u7136\u548c\u76f4\u89c2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.02973", "pdf": "https://arxiv.org/pdf/2507.02973", "abs": "https://arxiv.org/abs/2507.02973", "authors": ["Willem Th. van Peursen", "Samuel E. Entsua-Mensah"], "title": "Mimesis, Poiesis, and Imagination: Exploring Text-to-Image Generation of Biblical Narratives", "categories": ["cs.CV"], "comment": null, "summary": "This study explores the intersection of artificial intelligence and the visualization of Biblical narratives by analyzing AI-generated images of Exodus 2:5-9 (Moses found in River Nile) using MidJourney. Drawing on the classical concepts of mimesis (imitation) and poiesis (creative generation), the authors investigate how text-to-image (T2I) models reproduce or reimagine sacred narratives. Through comparative visual analysis, including Google image results and classical paintings, the research evaluates the stylistic, theological, and cultural dimensions of AI-generated depictions. Findings show that while AI excels in producing aesthetically rich and imaginative visuals, it also reflects the biases and limitations of its training data. The study highlights AI's potential to augment human imagination but questions its capacity for genuine creativity, authorial intent, and theological depth. It concludes by suggesting that AI can serve as a creative partner in reinterpreting biblical texts, though its role in sacred art remains complex and contested.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8AI\u5982\u4f55\u901a\u8fc7MidJourney\u751f\u6210\u300a\u51fa\u57c3\u53ca\u8bb0\u300b2:5-9\u7684\u56fe\u50cf\uff0c\u5206\u6790\u5176\u6a21\u4eff\u4e0e\u521b\u9020\u6027\uff0c\u5e76\u8bc4\u4f30\u5176\u98ce\u683c\u3001\u795e\u5b66\u548c\u6587\u5316\u7ef4\u5ea6\u3002", "motivation": "\u63a2\u7d22AI\u5728\u91cd\u73b0\u6216\u91cd\u65b0\u60f3\u8c61\u5723\u7ecf\u53d9\u4e8b\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u53ca\u5176\u5728\u795e\u5723\u827a\u672f\u4e2d\u7684\u89d2\u8272\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u89c6\u89c9\u5206\u6790\uff08\u5305\u62ecGoogle\u56fe\u50cf\u548c\u53e4\u5178\u7ed8\u753b\uff09\uff0c\u8bc4\u4f30AI\u751f\u6210\u56fe\u50cf\u7684\u98ce\u683c\u3001\u795e\u5b66\u548c\u6587\u5316\u7684\u8868\u73b0\u3002", "result": "AI\u80fd\u751f\u6210\u7f8e\u5b66\u4e30\u5bcc\u4e14\u5bcc\u6709\u60f3\u8c61\u529b\u7684\u56fe\u50cf\uff0c\u4f46\u4e5f\u53cd\u6620\u5176\u8bad\u7ec3\u6570\u636e\u7684\u504f\u89c1\u548c\u5c40\u9650\u6027\u3002", "conclusion": "AI\u53ef\u4f5c\u4e3a\u91cd\u65b0\u89e3\u8bfb\u5723\u7ecf\u6587\u672c\u7684\u521b\u610f\u4f19\u4f34\uff0c\u4f46\u5176\u5728\u795e\u5723\u827a\u672f\u4e2d\u7684\u89d2\u8272\u4ecd\u590d\u6742\u4e14\u6709\u4e89\u8bae\u3002"}}
{"id": "2507.02995", "pdf": "https://arxiv.org/pdf/2507.02995", "abs": "https://arxiv.org/abs/2507.02995", "authors": ["Guang Yang"], "title": "FreqCross: A Multi-Modal Frequency-Spatial Fusion Network for Robust Detection of Stable Diffusion 3.5 Generated Images", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "The rapid advancement of diffusion models, particularly Stable Diffusion 3.5, has enabled the generation of highly photorealistic synthetic images that pose significant challenges to existing detection methods. This paper presents FreqCross, a novel multi-modal fusion network that combines spatial RGB features, frequency domain artifacts, and radial energy distribution patterns to achieve robust detection of AI-generated images. Our approach leverages a three-branch architecture: (1) a ResNet-18 backbone for spatial feature extraction, (2) a lightweight CNN for processing 2D FFT magnitude spectra, and (3) a multi-layer perceptron for analyzing radial energy profiles. We introduce a novel radial energy distribution analysis that captures characteristic frequency artifacts inherent in diffusion-generated images, and fuse it with spatial and spectral cues via simple feature concatenation followed by a compact classification head. Extensive experiments on a dataset of 10,000 paired real (MS-COCO) and synthetic (Stable Diffusion 3.5) images demonstrate that FreqCross achieves 97.8\\% accuracy, outperforming state-of-the-art baselines by 5.2\\%. The frequency analysis further reveals that synthetic images exhibit distinct spectral signatures in the 0.1--0.4 normalised frequency range, providing theoretical foundation for our approach. Code and pre-trained models are publicly available to facilitate reproducible research.", "AI": {"tldr": "FreqCross\u662f\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u878d\u5408\u7f51\u7edc\uff0c\u7ed3\u5408\u7a7a\u95f4RGB\u7279\u5f81\u3001\u9891\u57df\u4f2a\u5f71\u548c\u5f84\u5411\u80fd\u91cf\u5206\u5e03\u6a21\u5f0f\uff0c\u7528\u4e8e\u68c0\u6d4bAI\u751f\u6210\u56fe\u50cf\uff0c\u51c6\u786e\u7387\u8fbe97.8%\u3002", "motivation": "\u968f\u7740Stable Diffusion 3.5\u7b49\u6269\u6563\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u751f\u6210\u7684\u5408\u6210\u56fe\u50cf\u9ad8\u5ea6\u903c\u771f\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u9762\u4e34\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e09\u5206\u652f\u67b6\u6784\uff1aResNet-18\u63d0\u53d6\u7a7a\u95f4\u7279\u5f81\uff0c\u8f7b\u91cfCNN\u5904\u74062D FFT\u9891\u8c31\uff0c\u591a\u5c42\u611f\u77e5\u673a\u5206\u6790\u5f84\u5411\u80fd\u91cf\u5206\u5e03\u3002\u901a\u8fc7\u7279\u5f81\u62fc\u63a5\u548c\u5206\u7c7b\u5934\u5b9e\u73b0\u591a\u6a21\u6001\u878d\u5408\u3002", "result": "\u572810,000\u5bf9\u771f\u5b9e\u4e0e\u5408\u6210\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0cFreqCross\u51c6\u786e\u7387\u8fbe97.8%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd55.2%\u3002\u9891\u57df\u5206\u6790\u663e\u793a\u5408\u6210\u56fe\u50cf\u57280.1--0.4\u5f52\u4e00\u5316\u9891\u7387\u8303\u56f4\u5185\u5177\u6709\u72ec\u7279\u7279\u5f81\u3002", "conclusion": "FreqCross\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u9891\u57df\u5206\u6790\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.03054", "pdf": "https://arxiv.org/pdf/2507.03054", "abs": "https://arxiv.org/abs/2507.03054", "authors": ["Ana Vasilcoiu", "Ivona Najdenkoska", "Zeno Geradts", "Marcel Worring"], "title": "LATTE: Latent Trajectory Embedding for Diffusion-Generated Image Detection", "categories": ["cs.CV", "cs.AI", "I.2.10; I.4.8; I.5"], "comment": "10 pages, 6 figures, submitted to NeurIPS 2025, includes benchmark   evaluations on GenImage and Diffusion Forensics", "summary": "The rapid advancement of diffusion-based image generators has made it increasingly difficult to distinguish generated from real images. This can erode trust in digital media, making it critical to develop generalizable detectors for generated images. Recent methods leverage diffusion denoising cues, but mainly focus on single-step reconstruction errors, ignoring the inherent sequential nature of the denoising process. In this work, we propose LATTE - Latent Trajectory Embedding - a novel approach that models the evolution of latent embeddings across several denoising timesteps. By modeling the trajectory of such embeddings rather than single-step errors, LATTE captures subtle, discriminative patterns that distinguish real from generated images. Each latent is refined by employing our latent-visual feature refinement module and aggregated into a unified representation. Afterwards, it is fused with the visual features and finally passed into a lightweight classifier. Our experiments demonstrate that LATTE surpasses the baselines on several established benchmarks, such as GenImage and DiffusionFake. Moreover, it demonstrates strong performance in cross-generator and cross-datasets settings, highlighting the potential of using the trajectory of latent embeddings for generated image detection. The code is available on the following link: https://github.com/AnaMVasilcoiu/LATTE-Diffusion-Detector.", "AI": {"tldr": "LATTE\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u8f68\u8ff9\u5d4c\u5165\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u53bb\u566a\u8fc7\u7a0b\u4e2d\u7684\u6f5c\u5728\u5d4c\u5165\u6f14\u5316\uff0c\u6709\u6548\u533a\u5206\u751f\u6210\u56fe\u50cf\u4e0e\u771f\u5b9e\u56fe\u50cf\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u751f\u6210\u5668\u5feb\u901f\u53d1\u5c55\uff0c\u533a\u5206\u751f\u6210\u56fe\u50cf\u4e0e\u771f\u5b9e\u56fe\u50cf\u53d8\u5f97\u56f0\u96be\uff0c\u4e9f\u9700\u5f00\u53d1\u901a\u7528\u7684\u68c0\u6d4b\u5668\u3002", "method": "LATTE\u901a\u8fc7\u5efa\u6a21\u6f5c\u5728\u5d4c\u5165\u5728\u591a\u6b65\u53bb\u566a\u4e2d\u7684\u8f68\u8ff9\uff0c\u7ed3\u5408\u6f5c\u5728-\u89c6\u89c9\u7279\u5f81\u7ec6\u5316\u6a21\u5757\u548c\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\uff0c\u5b9e\u73b0\u9ad8\u6548\u68c0\u6d4b\u3002", "result": "LATTE\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982GenImage\u548cDiffusionFake\uff09\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u8de8\u751f\u6210\u5668\u548c\u8de8\u6570\u636e\u96c6\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "LATTE\u5c55\u793a\u4e86\u6f5c\u5728\u5d4c\u5165\u8f68\u8ff9\u5728\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.03257", "pdf": "https://arxiv.org/pdf/2507.03257", "abs": "https://arxiv.org/abs/2507.03257", "authors": ["L\u00e9opold Maillard", "Tom Durand", "Adrien Ramanana Rahary", "Maks Ovsjanikov"], "title": "LACONIC: A 3D Layout Adapter for Controllable Image Creation", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025. Preprint version", "summary": "Existing generative approaches for guided image synthesis of multi-object scenes typically rely on 2D controls in the image or text space. As a result, these methods struggle to maintain and respect consistent three-dimensional geometric structure, underlying the scene. In this paper, we propose a novel conditioning approach, training method and adapter network that can be plugged into pretrained text-to-image diffusion models. Our approach provides a way to endow such models with 3D-awareness, while leveraging their rich prior knowledge. Our method supports camera control, conditioning on explicit 3D geometries and, for the first time, accounts for the entire context of a scene, i.e., both on and off-screen items, to synthesize plausible and semantically rich images. Despite its multi-modal nature, our model is lightweight, requires a reasonable number of data for supervised learning and shows remarkable generalization power. We also introduce methods for intuitive and consistent image editing and restyling, e.g., by positioning, rotating or resizing individual objects in a scene. Our method integrates well within various image creation workflows and enables a richer set of applications compared to previous approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u76843D\u611f\u77e5\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u9002\u914d\u5668\u7f51\u7edc\u589e\u5f3a\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u652f\u6301\u76f8\u673a\u63a7\u5236\u548c3D\u51e0\u4f55\u6761\u4ef6\uff0c\u5b9e\u73b0\u573a\u666f\u4e00\u81f4\u7684\u56fe\u50cf\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d562D\u63a7\u5236\uff0c\u96be\u4ee5\u4fdd\u63013D\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u9650\u5236\u4e86\u573a\u666f\u5408\u6210\u7684\u771f\u5b9e\u6027\u548c\u4e30\u5bcc\u6027\u3002", "method": "\u63d0\u51fa\u6761\u4ef6\u65b9\u6cd5\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u9002\u914d\u5668\u7f51\u7edc\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u652f\u63013D\u51e0\u4f55\u6761\u4ef6\u548c\u76f8\u673a\u63a7\u5236\uff0c\u5e76\u8003\u8651\u573a\u666f\u6574\u4f53\u4e0a\u4e0b\u6587\u3002", "result": "\u6a21\u578b\u8f7b\u91cf\u3001\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u652f\u6301\u76f4\u89c2\u7684\u56fe\u50cf\u7f16\u8f91\uff08\u5982\u5bf9\u8c61\u5b9a\u4f4d\u3001\u65cb\u8f6c\u548c\u7f29\u653e\uff09\uff0c\u4e30\u5bcc\u4e86\u5e94\u7528\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e863D\u611f\u77e5\u7684\u56fe\u50cf\u5408\u6210\u80fd\u529b\uff0c\u6269\u5c55\u4e86\u751f\u6210\u6a21\u578b\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2507.03283", "pdf": "https://arxiv.org/pdf/2507.03283", "abs": "https://arxiv.org/abs/2507.03283", "authors": ["Deepan Adak", "Yogesh Singh Rawat", "Shruti Vyas"], "title": "MolVision: Molecular Property Prediction with Vision Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Molecular property prediction is a fundamental task in computational chemistry with critical applications in drug discovery and materials science. While recent works have explored Large Language Models (LLMs) for this task, they primarily rely on textual molecular representations such as SMILES/SELFIES, which can be ambiguous and structurally less informative. In this work, we introduce MolVision, a novel approach that leverages Vision-Language Models (VLMs) by integrating both molecular structure as images and textual descriptions to enhance property prediction. We construct a benchmark spanning ten diverse datasets, covering classification, regression and description tasks. Evaluating nine different VLMs in zero-shot, few-shot, and fine-tuned settings, we find that visual information improves prediction performance, particularly when combined with efficient fine-tuning strategies such as LoRA. Our results reveal that while visual information alone is insufficient, multimodal fusion significantly enhances generalization across molecular properties. Adaptation of vision encoder for molecular images in conjunction with LoRA further improves the performance. The code and data is available at : $\\href{https://molvision.github.io/MolVision/}{https://molvision.github.io/MolVision/}$.", "AI": {"tldr": "MolVision\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7ed3\u5408\u5206\u5b50\u7ed3\u6784\u56fe\u50cf\u548c\u6587\u672c\u63cf\u8ff0\uff0c\u63d0\u5347\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u89c6\u89c9\u4fe1\u606f\u4e0e\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\uff08\u5982LoRA\uff09\u7ed3\u5408\u6548\u679c\u663e\u8457\u3002", "motivation": "\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u5728\u836f\u7269\u53d1\u73b0\u548c\u6750\u6599\u79d1\u5b66\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u8868\u793a\uff08\u5982SMILES/SELFIES\uff09\uff0c\u7f3a\u4e4f\u7ed3\u6784\u4fe1\u606f\u3002", "method": "\u63d0\u51faMolVision\uff0c\u6574\u5408\u5206\u5b50\u7ed3\u6784\u56fe\u50cf\u548c\u6587\u672c\u63cf\u8ff0\uff0c\u6784\u5efa\u6db5\u76d610\u4e2a\u6570\u636e\u96c6\u7684\u57fa\u51c6\uff0c\u8bc4\u4f309\u79cdVLMs\u5728\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u89c6\u89c9\u4fe1\u606f\u5355\u72ec\u4e0d\u8db3\uff0c\u4f46\u591a\u6a21\u6001\u878d\u5408\u663e\u8457\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff1b\u7ed3\u5408LoRA\u5fae\u8c03\u89c6\u89c9\u7f16\u7801\u5668\u8fdb\u4e00\u6b65\u4f18\u5316\u6027\u80fd\u3002", "conclusion": "MolVision\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.03295", "pdf": "https://arxiv.org/pdf/2507.03295", "abs": "https://arxiv.org/abs/2507.03295", "authors": ["Xiangning Zhang", "Jinnan Chen", "Qingwei Zhang", "Yaqi Wang", "Chengfeng Zhou", "Xiaobo Li", "Dahong Qian"], "title": "CPKD: Clinical Prior Knowledge-Constrained Diffusion Models for Surgical Phase Recognition in Endoscopic Submucosal Dissection", "categories": ["cs.CV"], "comment": null, "summary": "Gastrointestinal malignancies constitute a leading cause of cancer-related mortality worldwide, with advanced-stage prognosis remaining particularly dismal. Originating as a groundbreaking technique for early gastric cancer treatment, Endoscopic Submucosal Dissection has evolved into a versatile intervention for diverse gastrointestinal lesions. While computer-assisted systems significantly enhance procedural precision and safety in ESD, their clinical adoption faces a critical bottleneck: reliable surgical phase recognition within complex endoscopic workflows. Current state-of-the-art approaches predominantly rely on multi-stage refinement architectures that iteratively optimize temporal predictions. In this paper, we present Clinical Prior Knowledge-Constrained Diffusion (CPKD), a novel generative framework that reimagines phase recognition through denoising diffusion principles while preserving the core iterative refinement philosophy. This architecture progressively reconstructs phase sequences starting from random noise and conditioned on visual-temporal features. To better capture three domain-specific characteristics, including positional priors, boundary ambiguity, and relation dependency, we design a conditional masking strategy. Furthermore, we incorporate clinical prior knowledge into the model training to improve its ability to correct phase logical errors. Comprehensive evaluations on ESD820, Cholec80, and external multi-center demonstrate that our proposed CPKD achieves superior or comparable performance to state-of-the-art approaches, validating the effectiveness of diffusion-based generative paradigms for surgical phase recognition.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCPKD\u7684\u65b0\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u53bb\u566a\u6269\u6563\u539f\u7406\u91cd\u65b0\u6784\u60f3\u624b\u672f\u9636\u6bb5\u8bc6\u522b\uff0c\u540c\u65f6\u4fdd\u7559\u6838\u5fc3\u8fed\u4ee3\u4f18\u5316\u7406\u5ff5\u3002", "motivation": "\u80c3\u80a0\u9053\u6076\u6027\u80bf\u7624\u662f\u5168\u7403\u764c\u75c7\u76f8\u5173\u6b7b\u4ea1\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u800c\u5185\u955c\u9ecf\u819c\u4e0b\u5265\u79bb\u672f\uff08ESD\uff09\u7684\u4e34\u5e8a\u5e94\u7528\u4e2d\uff0c\u53ef\u9760\u7684\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u662f\u5173\u952e\u74f6\u9888\u3002", "method": "CPKD\u6846\u67b6\u57fa\u4e8e\u53bb\u566a\u6269\u6563\u539f\u7406\uff0c\u9010\u6b65\u4ece\u968f\u673a\u566a\u58f0\u91cd\u5efa\u9636\u6bb5\u5e8f\u5217\uff0c\u5e76\u7ed3\u5408\u89c6\u89c9-\u65f6\u95f4\u7279\u5f81\u3002\u8bbe\u8ba1\u4e86\u6761\u4ef6\u63a9\u7801\u7b56\u7565\u4ee5\u6355\u6349\u9886\u57df\u7279\u6027\uff0c\u5e76\u878d\u5165\u4e34\u5e8a\u5148\u9a8c\u77e5\u8bc6\u4ee5\u7ea0\u6b63\u903b\u8f91\u9519\u8bef\u3002", "result": "\u5728ESD820\u3001Cholec80\u53ca\u5916\u90e8\u591a\u4e2d\u5fc3\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cCPKD\u6027\u80fd\u4f18\u4e8e\u6216\u5ab2\u7f8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "conclusion": "\u6269\u6563\u751f\u6210\u8303\u5f0f\u5728\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u4e2d\u5177\u6709\u6709\u6548\u6027\uff0cCPKD\u4e3a\u590d\u6742\u5185\u955c\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.03313", "pdf": "https://arxiv.org/pdf/2507.03313", "abs": "https://arxiv.org/abs/2507.03313", "authors": ["Sagar Gandhi", "Vishal Gandhi"], "title": "Personalized Image Generation from an Author Writing Style", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Translating nuanced, textually-defined authorial writing styles into compelling visual representations presents a novel challenge in generative AI. This paper introduces a pipeline that leverages Author Writing Sheets (AWS) - structured summaries of an author's literary characteristics - as input to a Large Language Model (LLM, Claude 3.7 Sonnet). The LLM interprets the AWS to generate three distinct, descriptive text-to-image prompts, which are then rendered by a diffusion model (Stable Diffusion 3.5 Medium). We evaluated our approach using 49 author styles from Reddit data, with human evaluators assessing the stylistic match and visual distinctiveness of the generated images. Results indicate a good perceived alignment between the generated visuals and the textual authorial profiles (mean style match: $4.08/5$), with images rated as moderately distinctive. Qualitative analysis further highlighted the pipeline's ability to capture mood and atmosphere, while also identifying challenges in representing highly abstract narrative elements. This work contributes a novel end-to-end methodology for visual authorial style personalization and provides an initial empirical validation, opening avenues for applications in creative assistance and cross-modal understanding.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u4f5c\u8005\u5199\u4f5c\u98ce\u683c\u8f6c\u5316\u4e3a\u89c6\u89c9\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528AWS\u548cLLM\u751f\u6210\u6587\u672c\u63d0\u793a\uff0c\u518d\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u56fe\u50cf\u3002\u8bc4\u4f30\u663e\u793a\u751f\u6210\u56fe\u50cf\u4e0e\u4f5c\u8005\u98ce\u683c\u5339\u914d\u5ea6\u8f83\u9ad8\u3002", "motivation": "\u89e3\u51b3\u5c06\u6587\u672c\u5b9a\u4e49\u7684\u4f5c\u8005\u5199\u4f5c\u98ce\u683c\u8f6c\u5316\u4e3a\u89c6\u89c9\u8868\u793a\u7684\u6311\u6218\uff0c\u63a2\u7d22\u751f\u6210AI\u5728\u521b\u4f5c\u8f85\u52a9\u548c\u8de8\u6a21\u6001\u7406\u89e3\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5229\u7528AWS\u4f5c\u4e3a\u8f93\u5165\uff0c\u901a\u8fc7LLM\u751f\u6210\u6587\u672c\u63d0\u793a\uff0c\u518d\u4f7f\u7528\u6269\u6563\u6a21\u578b\uff08Stable Diffusion\uff09\u751f\u6210\u56fe\u50cf\u3002", "result": "\u751f\u6210\u56fe\u50cf\u4e0e\u4f5c\u8005\u98ce\u683c\u5339\u914d\u5ea6\u8f83\u9ad8\uff08\u5e73\u57474.08/5\uff09\uff0c\u89c6\u89c9\u72ec\u7279\u6027\u4e2d\u7b49\uff0c\u80fd\u6355\u6349\u60c5\u7eea\u548c\u6c1b\u56f4\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u89c6\u89c9\u4f5c\u8005\u98ce\u683c\u4e2a\u6027\u5316\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u521d\u6b65\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u4e3a\u521b\u610f\u8f85\u52a9\u548c\u8de8\u6a21\u6001\u7406\u89e3\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.03326", "pdf": "https://arxiv.org/pdf/2507.03326", "abs": "https://arxiv.org/abs/2507.03326", "authors": ["Zhao Wang", "Bowen Chen", "Yotaro Shimose", "Sota Moriyama", "Heng Wang", "Shingo Takamatsu"], "title": "Mirror in the Model: Ad Banner Image Generation via Reflective Multi-LLM and Multi-modal Agents", "categories": ["cs.CV"], "comment": null, "summary": "Recent generative models such as GPT-4o have shown strong capabilities in producing high-quality images with accurate text rendering. However, commercial design tasks like advertising banners demand more than visual fidelity -- they require structured layouts, precise typography, consistent branding, and more. In this paper, we introduce MIMO (Mirror In-the-Model), an agentic refinement framework for automatic ad banner generation. MIMO combines a hierarchical multi-modal agent system (MIMO-Core) with a coordination loop (MIMO-Loop) that explores multiple stylistic directions and iteratively improves design quality. Requiring only a simple natural language based prompt and logo image as input, MIMO automatically detects and corrects multiple types of errors during generation. Experiments show that MIMO significantly outperforms existing diffusion and LLM-based baselines in real-world banner design scenarios.", "AI": {"tldr": "MIMO\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u4ee3\u7406\u7cfb\u7edf\u548c\u534f\u8c03\u5faa\u73af\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e7f\u544a\u6a2a\u5e45\u7684\u81ea\u52a8\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\uff08\u5982GPT-4o\uff09\u5728\u5e7f\u544a\u6a2a\u5e45\u8bbe\u8ba1\u4e2d\u65e0\u6cd5\u6ee1\u8db3\u7ed3\u6784\u5316\u5e03\u5c40\u3001\u7cbe\u786e\u6392\u7248\u548c\u54c1\u724c\u4e00\u81f4\u6027\u7b49\u9700\u6c42\u3002", "method": "MIMO\u7ed3\u5408\u4e86\u5206\u5c42\u591a\u6a21\u6001\u4ee3\u7406\u7cfb\u7edf\uff08MIMO-Core\uff09\u548c\u534f\u8c03\u5faa\u73af\uff08MIMO-Loop\uff09\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u548c\u6807\u5fd7\u56fe\u50cf\u8f93\u5165\uff0c\u81ea\u52a8\u68c0\u6d4b\u5e76\u7ea0\u6b63\u751f\u6210\u4e2d\u7684\u9519\u8bef\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMIMO\u5728\u771f\u5b9e\u6a2a\u5e45\u8bbe\u8ba1\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u548cLLM\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MIMO\u4e3a\u5e7f\u544a\u6a2a\u5e45\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.03339", "pdf": "https://arxiv.org/pdf/2507.03339", "abs": "https://arxiv.org/abs/2507.03339", "authors": ["Sheng Liu", "Yiheng Yu", "Yuan Feng", "Min Xu", "Zhelun Jin", "Yining Jiang", "Tiantian Yuan"], "title": "DESign: Dynamic Context-Aware Convolution and Efficient Subnet Regularization for Continuous Sign Language Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Current continuous sign language recognition (CSLR) methods struggle with handling diverse samples. Although dynamic convolutions are ideal for this task, they mainly focus on spatial modeling and fail to capture the temporal dynamics and contextual dependencies. To address this, we propose DESign, a novel framework that incorporates Dynamic Context-Aware Convolution (DCAC) and Subnet Regularization Connectionist Temporal Classification (SR-CTC). DCAC dynamically captures the inter-frame motion cues that constitute signs and uniquely adapts convolutional weights in a fine-grained manner based on contextual information, enabling the model to better generalize across diverse signing behaviors and boost recognition accuracy. Furthermore, we observe that existing methods still rely on only a limited number of frames for parameter updates during training, indicating that CTC learning overfits to a dominant path. To address this, SR-CTC regularizes training by applying supervision to subnetworks, encouraging the model to explore diverse CTC alignment paths and effectively preventing overfitting. A classifier-sharing strategy in SR-CTC further strengthens multi-scale consistency. Notably, SR-CTC introduces no inference overhead and can be seamlessly integrated into existing CSLR models to boost performance. Extensive ablations and visualizations further validate the effectiveness of the proposed methods. Results on mainstream CSLR datasets (i.e., PHOENIX14, PHOENIX14-T, CSL-Daily) demonstrate that DESign achieves state-of-the-art performance.", "AI": {"tldr": "DESign\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u4e0a\u4e0b\u6587\u611f\u77e5\u5377\u79ef\uff08DCAC\uff09\u548c\u5b50\u7f51\u6b63\u5219\u5316CTC\uff08SR-CTC\uff09\u63d0\u5347\u8fde\u7eed\u624b\u8bed\u8bc6\u522b\uff08CSLR\uff09\u6027\u80fd\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u7a7a\u5efa\u6a21\u548c\u8fc7\u62df\u5408\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709CSLR\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u591a\u6837\u6837\u672c\uff0c\u4e14\u52a8\u6001\u5377\u79ef\u4ec5\u5173\u6ce8\u7a7a\u95f4\u5efa\u6a21\uff0c\u5ffd\u7565\u65f6\u95f4\u52a8\u6001\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u3002", "method": "\u63d0\u51faDCAC\u52a8\u6001\u6355\u6349\u5e27\u95f4\u8fd0\u52a8\u7ebf\u7d22\u5e76\u81ea\u9002\u5e94\u5377\u79ef\u6743\u91cd\uff0cSR-CTC\u901a\u8fc7\u5b50\u7f51\u76d1\u7763\u9632\u6b62CTC\u8fc7\u62df\u5408\u3002", "result": "\u5728PHOENIX14\u7b49\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "DESign\u6709\u6548\u63d0\u5347CSLR\u7684\u6cdb\u5316\u80fd\u529b\u548c\u51c6\u786e\u6027\uff0c\u4e14SR-CTC\u65e0\u9700\u989d\u5916\u63a8\u7406\u5f00\u9500\u3002"}}
{"id": "2507.03393", "pdf": "https://arxiv.org/pdf/2507.03393", "abs": "https://arxiv.org/abs/2507.03393", "authors": ["Yufan Zhou", "Zhaobo Qi", "Lingshuai Lin", "Junqi Jing", "Tingting Chai", "Beichen Zhang", "Shuhui Wang", "Weigang Zhang"], "title": "Masked Temporal Interpolation Diffusion for Procedure Planning in Instructional Videos", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we address the challenge of procedure planning in instructional videos, aiming to generate coherent and task-aligned action sequences from start and end visual observations. Previous work has mainly relied on text-level supervision to bridge the gap between observed states and unobserved actions, but it struggles with capturing intricate temporal relationships among actions. Building on these efforts, we propose the Masked Temporal Interpolation Diffusion (MTID) model that introduces a latent space temporal interpolation module within the diffusion model. This module leverages a learnable interpolation matrix to generate intermediate latent features, thereby augmenting visual supervision with richer mid-state details. By integrating this enriched supervision into the model, we enable end-to-end training tailored to task-specific requirements, significantly enhancing the model's capacity to predict temporally coherent action sequences. Additionally, we introduce an action-aware mask projection mechanism to restrict the action generation space, combined with a task-adaptive masked proximity loss to prioritize more accurate reasoning results close to the given start and end states over those in intermediate steps. Simultaneously, it filters out task-irrelevant action predictions, leading to contextually aware action sequences. Experimental results across three widely used benchmark datasets demonstrate that our MTID achieves promising action planning performance on most metrics. The code is available at https://github.com/WiserZhou/MTID.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMTID\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u6559\u5b66\u89c6\u9891\u4e2d\u7684\u7a0b\u5e8f\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u65f6\u95f4\u63d2\u503c\u6a21\u5757\u548c\u4efb\u52a1\u611f\u77e5\u63a9\u7801\u673a\u5236\uff0c\u751f\u6210\u8fde\u8d2f\u4e14\u4efb\u52a1\u5bf9\u9f50\u7684\u52a8\u4f5c\u5e8f\u5217\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u7ea7\u76d1\u7763\uff0c\u96be\u4ee5\u6355\u6349\u52a8\u4f5c\u95f4\u590d\u6742\u7684\u65f6\u5e8f\u5173\u7cfb\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u589e\u5f3a\u89c6\u89c9\u76d1\u7763\u5e76\u751f\u6210\u66f4\u7cbe\u51c6\u52a8\u4f5c\u5e8f\u5217\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51faMTID\u6a21\u578b\uff0c\u7ed3\u5408\u6f5c\u5728\u7a7a\u95f4\u65f6\u95f4\u63d2\u503c\u6a21\u5757\u548c\u52a8\u4f5c\u611f\u77e5\u63a9\u7801\u6295\u5f71\u673a\u5236\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u4e2d\u95f4\u6f5c\u5728\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u4efb\u52a1\u81ea\u9002\u5e94\u63a9\u7801\u90bb\u8fd1\u635f\u5931\u4f18\u5316\u52a8\u4f5c\u751f\u6210\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMTID\u5728\u5927\u591a\u6570\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u52a8\u4f5c\u89c4\u5212\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "MTID\u901a\u8fc7\u589e\u5f3a\u89c6\u89c9\u76d1\u7763\u548c\u4efb\u52a1\u611f\u77e5\u673a\u5236\uff0c\u80fd\u591f\u751f\u6210\u66f4\u8fde\u8d2f\u4e14\u4efb\u52a1\u5bf9\u9f50\u7684\u52a8\u4f5c\u5e8f\u5217\uff0c\u4e3a\u7a0b\u5e8f\u89c4\u5212\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.03394", "pdf": "https://arxiv.org/pdf/2507.03394", "abs": "https://arxiv.org/abs/2507.03394", "authors": ["Qing Li", "Huifang Feng", "Xun Gong", "Yu-Shen Liu"], "title": "Learning Normals of Noisy Points by Local Gradient-Aware Surface Filtering", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025. Code: https://github.com/LeoQLi/LGSF", "summary": "Estimating normals for noisy point clouds is a persistent challenge in 3D geometry processing, particularly for end-to-end oriented normal estimation. Existing methods generally address relatively clean data and rely on supervised priors to fit local surfaces within specific neighborhoods. In this paper, we propose a novel approach for learning normals from noisy point clouds through local gradient-aware surface filtering. Our method projects noisy points onto the underlying surface by utilizing normals and distances derived from an implicit function constrained by local gradients. We start by introducing a distance measurement operator for global surface fitting on noisy data, which integrates projected distances along normals. Following this, we develop an implicit field-based filtering approach for surface point construction, adding projection constraints on these points during filtering. To address issues of over-smoothing and gradient degradation, we further incorporate local gradient consistency constraints, as well as local gradient orientation and aggregation. Comprehensive experiments on normal estimation, surface reconstruction, and point cloud denoising demonstrate the state-of-the-art performance of our method. The source code and trained models are available at https://github.com/LeoQLi/LGSF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5c40\u90e8\u68af\u5ea6\u611f\u77e5\u8868\u9762\u6ee4\u6ce2\u4ece\u566a\u58f0\u70b9\u4e91\u4e2d\u5b66\u4e60\u6cd5\u7ebf\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u5168\u5c40\u8868\u9762\u62df\u5408\u548c\u5c40\u90e8\u68af\u5ea6\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cd5\u7ebf\u4f30\u8ba1\u548c\u53bb\u566a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u8f83\u5e72\u51c0\u6570\u636e\u4e14\u4f9d\u8d56\u76d1\u7763\u5148\u9a8c\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u566a\u58f0\u70b9\u4e91\u7684\u6cd5\u7ebf\u4f30\u8ba1\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u9690\u51fd\u6570\u7ea6\u675f\u5c40\u90e8\u68af\u5ea6\uff0c\u5229\u7528\u6cd5\u7ebf\u548c\u8ddd\u79bb\u5c06\u566a\u58f0\u70b9\u6295\u5f71\u5230\u6f5c\u5728\u8868\u9762\uff0c\u7ed3\u5408\u5168\u5c40\u8ddd\u79bb\u6d4b\u91cf\u548c\u5c40\u90e8\u68af\u5ea6\u4e00\u81f4\u6027\u7ea6\u675f\u3002", "result": "\u5728\u6cd5\u7ebf\u4f30\u8ba1\u3001\u8868\u9762\u91cd\u5efa\u548c\u70b9\u4e91\u53bb\u566a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u68af\u5ea6\u611f\u77e5\u6ee4\u6ce2\u548c\u5c40\u90e8\u7ea6\u675f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u566a\u58f0\u70b9\u4e91\u7684\u6cd5\u7ebf\u4f30\u8ba1\u95ee\u9898\u3002"}}
{"id": "2507.03458", "pdf": "https://arxiv.org/pdf/2507.03458", "abs": "https://arxiv.org/abs/2507.03458", "authors": ["Leyan Xue", "Zongbo Han", "Guangyu Wang", "Qinghua Hu", "Mingyue Cheng", "Changqing Zhang"], "title": "Helping CLIP See Both the Forest and the Trees: A Decomposition and Description Approach", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) like CLIP achieve cross-modal semantic alignment through contrastive learning, exhibiting robust zero-shot generalization. Traditional prompt engineering, however, predominantly relies on coarse-grained category labels, neglecting fine-grained local semantics. Existing approaches assume that VLMs inherently recognize localized visual details and attempt to enhance classification by augmenting text prompts with attribute descriptors generated by large language models. However, our systematic experiments reveal critical limitations: CLIP's strong bias toward global image patterns hinders its ability to process localized visual descriptors. To address this fundamental constraint, we propose a simple, effective, and plug-and-play solution that enables CLIP to ``See Both the Forest and the Trees.\" Specifically, we employ stochastic multi-crop augmentation to activate CLIP's latent capacity for localized feature analysis. By cropping only partial regions, the approach effectively constrains the model's receptive field and recalibrates its attention mechanism, thereby mitigating its inherent bias. We evaluate the proposed method under zero-shot, few-shot, and test-time adaptation settings, and extensive experiments demonstrate that D&D achieves promising performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u968f\u673a\u591a\u88c1\u526a\u589e\u5f3a\u6fc0\u6d3bCLIP\u5c40\u90e8\u7279\u5f81\u5206\u6790\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5176\u504f\u5411\u5168\u5c40\u56fe\u50cf\u6a21\u5f0f\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u63d0\u793a\u5de5\u7a0b\u4f9d\u8d56\u7c97\u7c92\u5ea6\u7c7b\u522b\u6807\u7b7e\uff0c\u5ffd\u89c6\u7ec6\u7c92\u5ea6\u5c40\u90e8\u8bed\u4e49\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5047\u8bbeVLMs\u80fd\u8bc6\u522b\u5c40\u90e8\u7ec6\u8282\u3002\u7136\u800c\u5b9e\u9a8c\u53d1\u73b0CLIP\u5bf9\u5168\u5c40\u6a21\u5f0f\u7684\u504f\u89c1\u9650\u5236\u4e86\u5176\u5904\u7406\u5c40\u90e8\u63cf\u8ff0\u7b26\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528\u968f\u673a\u591a\u88c1\u526a\u589e\u5f3a\uff0c\u901a\u8fc7\u88c1\u526a\u90e8\u5206\u533a\u57df\u7ea6\u675f\u6a21\u578b\u611f\u53d7\u91ce\u5e76\u91cd\u65b0\u6821\u51c6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ece\u800c\u6fc0\u6d3bCLIP\u7684\u5c40\u90e8\u7279\u5f81\u5206\u6790\u80fd\u529b\u3002", "result": "\u5728\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u6d4b\u8bd5\u65f6\u9002\u5e94\u8bbe\u7f6e\u4e0b\uff0cD&D\u65b9\u6cd5\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u53ef\u5373\u63d2\u5373\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86CLIP\u5904\u7406\u5c40\u90e8\u89c6\u89c9\u63cf\u8ff0\u7b26\u7684\u80fd\u529b\u3002"}}
{"id": "2507.03463", "pdf": "https://arxiv.org/pdf/2507.03463", "abs": "https://arxiv.org/abs/2507.03463", "authors": ["Matthias Zeller", "Vardeep S. Sandhu", "Benedikt Mersch", "Jens Behley", "Michael Heidingsfeld", "Cyrill Stachniss"], "title": "Radar Velocity Transformer: Single-scan Moving Object Segmentation in Noisy Radar Point Clouds", "categories": ["cs.CV"], "comment": "Proc. of the IEEE Intl. Conf. on Robotics & Automation (ICRA)", "summary": "The awareness about moving objects in the surroundings of a self-driving vehicle is essential for safe and reliable autonomous navigation. The interpretation of LiDAR and camera data achieves exceptional results but typically requires to accumulate and process temporal sequences of data in order to extract motion information. In contrast, radar sensors, which are already installed in most recent vehicles, can overcome this limitation as they directly provide the Doppler velocity of the detections and, hence incorporate instantaneous motion information within a single measurement. % In this paper, we tackle the problem of moving object segmentation in noisy radar point clouds. We also consider differentiating parked from moving cars, to enhance scene understanding. Instead of exploiting temporal dependencies to identify moving objects, we develop a novel transformer-based approach to perform single-scan moving object segmentation in sparse radar scans accurately. The key to our Radar Velocity Transformer is to incorporate the valuable velocity information throughout each module of the network, thereby enabling the precise segmentation of moving and non-moving objects. Additionally, we propose a transformer-based upsampling, which enhances the performance by adaptively combining information and overcoming the limitation of interpolation of sparse point clouds. Finally, we create a new radar moving object segmentation benchmark based on the RadarScenes dataset and compare our approach to other state-of-the-art methods. Our network runs faster than the frame rate of the sensor and shows superior segmentation results using only single-scan radar data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u5355\u6b21\u96f7\u8fbe\u626b\u63cf\u79fb\u52a8\u7269\u4f53\u5206\u5272\u65b9\u6cd5\uff0c\u5229\u7528\u96f7\u8fbe\u7684\u591a\u666e\u52d2\u901f\u5ea6\u4fe1\u606f\uff0c\u65e0\u9700\u4f9d\u8d56\u65f6\u5e8f\u6570\u636e\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9700\u5b9e\u65f6\u611f\u77e5\u5468\u56f4\u79fb\u52a8\u7269\u4f53\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u65f6\u5e8f\u6570\u636e\uff0c\u96f7\u8fbe\u53ef\u76f4\u63a5\u63d0\u4f9b\u901f\u5ea6\u4fe1\u606f\uff0c\u56e0\u6b64\u5f00\u53d1\u5355\u6b21\u626b\u63cf\u5206\u5272\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRadar Velocity Transformer\uff0c\u5c06\u901f\u5ea6\u4fe1\u606f\u878d\u5165\u7f51\u7edc\u5404\u6a21\u5757\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8eTransformer\u7684\u4e0a\u91c7\u6837\u65b9\u6cd5\uff0c\u63d0\u5347\u7a00\u758f\u70b9\u4e91\u7684\u5206\u5272\u6027\u80fd\u3002", "result": "\u65b9\u6cd5\u5728RadarScenes\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u901f\u5ea6\u5feb\u4e8e\u4f20\u611f\u5668\u5e27\u7387\uff0c\u4ec5\u9700\u5355\u6b21\u626b\u63cf\u5373\u53ef\u5b9e\u73b0\u7cbe\u786e\u5206\u5272\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u96f7\u8fbe\u79fb\u52a8\u7269\u4f53\u5206\u5272\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u3002"}}
{"id": "2507.03737", "pdf": "https://arxiv.org/pdf/2507.03737", "abs": "https://arxiv.org/abs/2507.03737", "authors": ["Chong Cheng", "Sicheng Yu", "Zijian Wang", "Yifan Zhou", "Hao Wang"], "title": "Outdoor Monocular SLAM with Global Scale-Consistent 3D Gaussian Pointmaps", "categories": ["cs.CV"], "comment": "Accepted by ICCV2025", "summary": "3D Gaussian Splatting (3DGS) has become a popular solution in SLAM due to its high-fidelity and real-time novel view synthesis performance. However, some previous 3DGS SLAM methods employ a differentiable rendering pipeline for tracking, \\textbf{lack geometric priors} in outdoor scenes. Other approaches introduce separate tracking modules, but they accumulate errors with significant camera movement, leading to \\textbf{scale drift}. To address these challenges, we propose a robust RGB-only outdoor 3DGS SLAM method: S3PO-GS. Technically, we establish a self-consistent tracking module anchored in the 3DGS pointmap, which avoids cumulative scale drift and achieves more precise and robust tracking with fewer iterations. Additionally, we design a patch-based pointmap dynamic mapping module, which introduces geometric priors while avoiding scale ambiguity. This significantly enhances tracking accuracy and the quality of scene reconstruction, making it particularly suitable for complex outdoor environments. Our experiments on the Waymo, KITTI, and DL3DV datasets demonstrate that S3PO-GS achieves state-of-the-art results in novel view synthesis and outperforms other 3DGS SLAM methods in tracking accuracy. Project page: https://3dagentworld.github.io/S3PO-GS/.", "AI": {"tldr": "S3PO-GS\u662f\u4e00\u79cd\u9488\u5bf9\u6237\u5916\u573a\u666f\u7684RGB-only 3D\u9ad8\u65af\u6e85\u5c04SLAM\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u4e00\u81f4\u7684\u8ddf\u8e2a\u6a21\u5757\u548c\u57fa\u4e8e\u8865\u4e01\u7684\u52a8\u6001\u6620\u5c04\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u5c3a\u5ea6\u6f02\u79fb\u548c\u51e0\u4f55\u5148\u9a8c\u7f3a\u5931\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u573a\u666f\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u73b0\u67093DGS SLAM\u65b9\u6cd5\u5728\u6237\u5916\u573a\u666f\u4e2d\u7f3a\u4e4f\u51e0\u4f55\u5148\u9a8c\u6216\u5b58\u5728\u5c3a\u5ea6\u6f02\u79fb\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u81ea\u4e00\u81f4\u7684\u8ddf\u8e2a\u6a21\u5757\u548c\u57fa\u4e8e\u8865\u4e01\u7684\u52a8\u6001\u6620\u5c04\u6a21\u5757\uff0c\u907f\u514d\u5c3a\u5ea6\u6f02\u79fb\u5e76\u5f15\u5165\u51e0\u4f55\u5148\u9a8c\u3002", "result": "\u5728Waymo\u3001KITTI\u548cDL3DV\u6570\u636e\u96c6\u4e0a\uff0cS3PO-GS\u5728\u65b0\u89c6\u89d2\u5408\u6210\u548c\u8ddf\u8e2a\u7cbe\u5ea6\u4e0a\u8fbe\u5230\u6700\u4f18\u3002", "conclusion": "S3PO-GS\u5728\u590d\u6742\u6237\u5916\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.03745", "pdf": "https://arxiv.org/pdf/2507.03745", "abs": "https://arxiv.org/abs/2507.03745", "authors": ["Akio Kodaira", "Tingbo Hou", "Ji Hou", "Masayoshi Tomizuka", "Yue Zhao"], "title": "StreamDiT: Real-Time Streaming Text-to-Video Generation", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "Recently, great progress has been achieved in text-to-video (T2V) generation by scaling transformer-based diffusion models to billions of parameters, which can generate high-quality videos. However, existing models typically produce only short clips offline, restricting their use cases in interactive and real-time applications. This paper addresses these challenges by proposing StreamDiT, a streaming video generation model. StreamDiT training is based on flow matching by adding a moving buffer. We design mixed training with different partitioning schemes of buffered frames to boost both content consistency and visual quality. StreamDiT modeling is based on adaLN DiT with varying time embedding and window attention. To practice the proposed method, we train a StreamDiT model with 4B parameters. In addition, we propose a multistep distillation method tailored for StreamDiT. Sampling distillation is performed in each segment of a chosen partitioning scheme. After distillation, the total number of function evaluations (NFEs) is reduced to the number of chunks in a buffer. Finally, our distilled model reaches real-time performance at 16 FPS on one GPU, which can generate video streams at 512p resolution. We evaluate our method through both quantitative metrics and human evaluation. Our model enables real-time applications, e.g. streaming generation, interactive generation, and video-to-video. We provide video results and more examples in our project website: <a href=\"https://cumulo-autumn.github.io/StreamDiT/\">this https URL.</a>", "AI": {"tldr": "StreamDiT\u662f\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u548c\u79fb\u52a8\u7f13\u51b2\u533a\u7684\u5b9e\u65f6\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u6df7\u5408\u8bad\u7ec3\u548c\u5206\u533a\u65b9\u6848\u63d0\u5347\u5185\u5bb9\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\uff0c\u6700\u7ec8\u5728\u5355GPU\u4e0a\u5b9e\u73b016 FPS\u7684\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u901a\u5e38\u4ec5\u80fd\u79bb\u7ebf\u751f\u6210\u77ed\u89c6\u9891\uff0c\u9650\u5236\u4e86\u5176\u5728\u4ea4\u4e92\u5f0f\u548c\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002", "method": "\u63d0\u51faStreamDiT\uff0c\u57fa\u4e8e\u6d41\u5339\u914d\u548c\u79fb\u52a8\u7f13\u51b2\u533a\u8bad\u7ec3\uff0c\u91c7\u7528\u6df7\u5408\u8bad\u7ec3\u548c\u5206\u533a\u65b9\u6848\uff0c\u7ed3\u5408adaLN DiT\u5efa\u6a21\u548c\u591a\u6b65\u84b8\u998f\u65b9\u6cd5\u3002", "result": "\u8bad\u7ec3\u4e86\u4e00\u4e2a4B\u53c2\u6570\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u84b8\u998f\u5c06\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\u51cf\u5c11\u5230\u7f13\u51b2\u533a\u5757\u6570\uff0c\u5b9e\u73b016 FPS\u7684\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "StreamDiT\u652f\u6301\u5b9e\u65f6\u5e94\u7528\uff0c\u5982\u6d41\u751f\u6210\u548c\u4ea4\u4e92\u751f\u6210\uff0c\u5e76\u901a\u8fc7\u5b9a\u91cf\u6307\u6807\u548c\u4eba\u5de5\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.03846", "pdf": "https://arxiv.org/pdf/2507.03846", "abs": "https://arxiv.org/abs/2507.03846", "authors": ["Nicola Bernold", "Moritz Vandenhirtz", "Alice Bizeul", "Julia E. Vogt"], "title": "Interpretable Diffusion Models with B-cos Networks", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Text-to-image diffusion models generate images by iteratively denoising random noise, conditioned on a prompt. While these models have enabled impressive progress in image generation, they often fail to accurately reflect all semantic information described in the prompt -- failures that are difficult to detect automatically. In this work, we introduce a diffusion model architecture built with B-cos modules that offers inherent interpretability. Our approach provides insight into how individual prompt tokens affect the generated image by producing explanations that highlight the pixel regions influenced by each token. We demonstrate that B-cos diffusion models can produce high-quality images while providing meaningful insights into prompt-image alignment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eB-cos\u6a21\u5757\u7684\u53ef\u89e3\u91ca\u6027\u6269\u6563\u6a21\u578b\u67b6\u6784\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u5e76\u63ed\u793a\u63d0\u793a\u8bcd\u5bf9\u56fe\u50cf\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u96be\u4ee5\u51c6\u786e\u53cd\u6620\u63d0\u793a\u4e2d\u7684\u6240\u6709\u8bed\u4e49\u4fe1\u606f\uff0c\u4e14\u7f3a\u4e4f\u81ea\u52a8\u68c0\u6d4b\u5931\u8d25\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528B-cos\u6a21\u5757\u6784\u5efa\u6269\u6563\u6a21\u578b\uff0c\u751f\u6210\u89e3\u91ca\u6027\u4fe1\u606f\uff0c\u663e\u793a\u6bcf\u4e2a\u63d0\u793a\u8bcd\u5bf9\u56fe\u50cf\u50cf\u7d20\u533a\u57df\u7684\u5f71\u54cd\u3002", "result": "\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u63d0\u793a\u4e0e\u56fe\u50cf\u5bf9\u9f50\u7684\u6709\u610f\u4e49\u89c1\u89e3\u3002", "conclusion": "B-cos\u6269\u6563\u6a21\u578b\u517c\u5177\u751f\u6210\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u63d0\u793a-\u56fe\u50cf\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.03886", "pdf": "https://arxiv.org/pdf/2507.03886", "abs": "https://arxiv.org/abs/2507.03886", "authors": ["Guile Wu", "Dongfeng Bai", "Bingbing Liu"], "title": "ArmGS: Composite Gaussian Appearance Refinement for Modeling Dynamic Urban Environments", "categories": ["cs.CV"], "comment": "Technical report", "summary": "This work focuses on modeling dynamic urban environments for autonomous driving simulation. Contemporary data-driven methods using neural radiance fields have achieved photorealistic driving scene modeling, but they suffer from low rendering efficacy. Recently, some approaches have explored 3D Gaussian splatting for modeling dynamic urban scenes, enabling high-fidelity reconstruction and real-time rendering. However, these approaches often neglect to model fine-grained variations between frames and camera viewpoints, leading to suboptimal results. In this work, we propose a new approach named ArmGS that exploits composite driving Gaussian splatting with multi-granularity appearance refinement for autonomous driving scene modeling. The core idea of our approach is devising a multi-level appearance modeling scheme to optimize a set of transformation parameters for composite Gaussian refinement from multiple granularities, ranging from local Gaussian level to global image level and dynamic actor level. This not only models global scene appearance variations between frames and camera viewpoints, but also models local fine-grained changes of background and objects. Extensive experiments on multiple challenging autonomous driving datasets, namely, Waymo, KITTI, NOTR and VKITTI2, demonstrate the superiority of our approach over the state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aArmGS\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u57ce\u5e02\u73af\u5883\u5efa\u6a21\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u5916\u89c2\u4f18\u5316\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7684\u6e32\u67d3\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u57ce\u5e02\u573a\u666f\u5efa\u6a21\u4e2d\u5ffd\u7565\u4e86\u5e27\u95f4\u548c\u89c6\u89d2\u95f4\u7684\u7ec6\u7c92\u5ea6\u53d8\u5316\uff0c\u5bfc\u81f4\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u591a\u7ea7\u5916\u89c2\u5efa\u6a21\u65b9\u6848\uff0c\u4ece\u5c40\u90e8\u9ad8\u65af\u5230\u5168\u5c40\u56fe\u50cf\u548c\u52a8\u6001\u89d2\u8272\u7ea7\u522b\u4f18\u5316\u53d8\u6362\u53c2\u6570\u3002", "result": "\u5728\u591a\u4e2a\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ArmGS\u65b9\u6cd5\u5728\u52a8\u6001\u573a\u666f\u5efa\u6a21\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u91cd\u5efa\u548c\u5b9e\u65f6\u6e32\u67d3\u3002"}}
{"id": "2507.03893", "pdf": "https://arxiv.org/pdf/2507.03893", "abs": "https://arxiv.org/abs/2507.03893", "authors": ["Yi Li", "Xiaoxiong Wang", "Jiawei Wang", "Yi Chang", "Kai Cao", "Luxin Yan"], "title": "Hierarchical Semantic-Visual Fusion of Visible and Near-infrared Images for Long-range Haze Removal", "categories": ["cs.CV", "cs.AI"], "comment": "This work has been accepted by IEEE Transactions on Multimedia for   publication", "summary": "While image dehazing has advanced substantially in the past decade, most efforts have focused on short-range scenarios, leaving long-range haze removal under-explored. As distance increases, intensified scattering leads to severe haze and signal loss, making it impractical to recover distant details solely from visible images. Near-infrared, with superior fog penetration, offers critical complementary cues through multimodal fusion. However, existing methods focus on content integration while often neglecting haze embedded in visible images, leading to results with residual haze. In this work, we argue that the infrared and visible modalities not only provide complementary low-level visual features, but also share high-level semantic consistency. Motivated by this, we propose a Hierarchical Semantic-Visual Fusion (HSVF) framework, comprising a semantic stream to reconstruct haze-free scenes and a visual stream to incorporate structural details from the near-infrared modality. The semantic stream first acquires haze-robust semantic prediction by aligning modality-invariant intrinsic representations. Then the shared semantics act as strong priors to restore clear and high-contrast distant scenes under severe haze degradation. In parallel, the visual stream focuses on recovering lost structural details from near-infrared by fusing complementary cues from both visible and near-infrared images. Through the cooperation of dual streams, HSVF produces results that exhibit both high-contrast scenes and rich texture details. Moreover, we introduce a novel pixel-aligned visible-infrared haze dataset with semantic labels to facilitate benchmarking. Extensive experiments demonstrate the superiority of our method over state-of-the-art approaches in real-world long-range haze removal.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c42\u6b21\u5316\u8bed\u4e49-\u89c6\u89c9\u878d\u5408\uff08HSVF\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u957f\u8ddd\u79bb\u56fe\u50cf\u53bb\u96fe\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u8fd1\u7ea2\u5916\u548c\u53ef\u89c1\u5149\u6a21\u6001\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u9ad8\u5bf9\u6bd4\u5ea6\u548c\u4e30\u5bcc\u7eb9\u7406\u7684\u53bb\u96fe\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u77ed\u8ddd\u79bb\u53bb\u96fe\uff0c\u800c\u957f\u8ddd\u79bb\u53bb\u96fe\u7531\u4e8e\u6563\u5c04\u589e\u5f3a\u548c\u4fe1\u53f7\u4e22\u5931\u95ee\u9898\u88ab\u5ffd\u89c6\u3002\u8fd1\u7ea2\u5916\u6a21\u6001\u5177\u6709\u66f4\u597d\u7684\u96fe\u7a7f\u900f\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u878d\u5408\u65f6\u5ffd\u7565\u4e86\u53ef\u89c1\u5149\u56fe\u50cf\u4e2d\u7684\u96fe\u6b8b\u7559\u3002", "method": "\u63d0\u51faHSVF\u6846\u67b6\uff0c\u5305\u542b\u8bed\u4e49\u6d41\u548c\u89c6\u89c9\u6d41\u3002\u8bed\u4e49\u6d41\u901a\u8fc7\u6a21\u6001\u4e0d\u53d8\u7684\u5185\u5728\u8868\u793a\u91cd\u5efa\u65e0\u96fe\u573a\u666f\uff0c\u89c6\u89c9\u6d41\u4ece\u8fd1\u7ea2\u5916\u6a21\u6001\u6062\u590d\u7ed3\u6784\u7ec6\u8282\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHSVF\u5728\u771f\u5b9e\u957f\u8ddd\u79bb\u53bb\u96fe\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u9ad8\u5bf9\u6bd4\u5ea6\u548c\u4e30\u5bcc\u7eb9\u7406\u7684\u7ed3\u679c\u3002", "conclusion": "HSVF\u901a\u8fc7\u8bed\u4e49\u548c\u89c6\u89c9\u6d41\u7684\u534f\u4f5c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u8ddd\u79bb\u53bb\u96fe\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u96c6\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2507.03905", "pdf": "https://arxiv.org/pdf/2507.03905", "abs": "https://arxiv.org/abs/2507.03905", "authors": ["Rang Meng", "Yan Wang", "Weipeng Wu", "Ruobing Zheng", "Yuming Li", "Chenguang Ma"], "title": "EchoMimicV3: 1.3B Parameters are All You Need for Unified Multi-Modal and Multi-Task Human Animation", "categories": ["cs.CV"], "comment": null, "summary": "Human animation recently has advanced rapidly, achieving increasingly realistic and vivid results, especially with the integration of large-scale video generation models. However, the slow inference speed and high computational cost of these large models bring significant challenges for practical applications. Additionally, various tasks in human animation, such as lip-syncing, audio-driven full-body animation, and video generation from start and end frames, often require different specialized models. The introduction of large video models has not alleviated this dilemma. This raises an important question: Can we make human animation Faster, Higher in quality, Stronger in generalization, and make various tasks Together in one model? To address this, we dive into video generation models and discover that the devil lies in the details: Inspired by MAE, we propose a novel unified Multi-Task paradigm for human animation, treating diverse generation tasks as spatial-temporal local reconstructions, requiring modifications only on the input side; Given the interplay and division among multi-modal conditions including text, image, and audio, we introduce a multi-modal decoupled cross-attention module to fuse multi-modals in a divide-and-conquer manner; We propose a new SFT+Reward alternating training paradigm, enabling the minimal model with 1.3B parameters to achieve generation quality comparable to models with 10 times the parameters count. Through these innovations, our work paves the way for efficient, high-quality, and versatile digital human generation, addressing both performance and practicality challenges in the field. Extensive experiments demonstrate that EchoMimicV3 outperforms existing models in both facial and semi-body video generation, providing precise text-based control for creating videos in a wide range of scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u4efb\u52a1\u8303\u5f0fEchoMimicV3\uff0c\u901a\u8fc7\u7a7a\u95f4-\u65f6\u95f4\u5c40\u90e8\u91cd\u5efa\u5904\u7406\u591a\u6837\u5316\u751f\u6210\u4efb\u52a1\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u89e3\u8026\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u548cSFT+Reward\u4ea4\u66ff\u8bad\u7ec3\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u3001\u591a\u529f\u80fd\u7684\u6570\u5b57\u4eba\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u63a8\u7406\u901f\u5ea6\u3001\u8ba1\u7b97\u6210\u672c\u548c\u4efb\u52a1\u591a\u6837\u6027\u4e0a\u7684\u6311\u6218\uff0c\u63a2\u7d22\u5982\u4f55\u5b9e\u73b0\u66f4\u5feb\u3001\u66f4\u9ad8\u8d28\u3001\u66f4\u5f3a\u6cdb\u5316\u4e14\u591a\u4efb\u52a1\u7edf\u4e00\u7684\u4eba\u7c7b\u52a8\u753b\u751f\u6210\u3002", "method": "1. \u5c06\u591a\u6837\u5316\u751f\u6210\u4efb\u52a1\u89c6\u4e3a\u7a7a\u95f4-\u65f6\u95f4\u5c40\u90e8\u91cd\u5efa\uff1b2. \u5f15\u5165\u591a\u6a21\u6001\u89e3\u8026\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff1b3. \u63d0\u51faSFT+Reward\u4ea4\u66ff\u8bad\u7ec3\u8303\u5f0f\u3002", "result": "EchoMimicV3\u5728\u9762\u90e8\u548c\u534a\u8eab\u89c6\u9891\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751f\u6210\u8d28\u91cf\u5ab2\u7f8e\u53c2\u6570\u91cf\u592710\u500d\u7684\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u7cbe\u786e\u7684\u6587\u672c\u63a7\u5236\u3002", "conclusion": "\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u3001\u591a\u529f\u80fd\u7684\u6570\u5b57\u4eba\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u6027\u80fd\u548c\u5b9e\u7528\u6027\u7684\u53cc\u91cd\u6311\u6218\u3002"}}
{"id": "2507.03924", "pdf": "https://arxiv.org/pdf/2507.03924", "abs": "https://arxiv.org/abs/2507.03924", "authors": ["Rongjia Zheng", "Qing Zhang", "Chengjiang Long", "Wei-Shi Zheng"], "title": "DNF-Intrinsic: Deterministic Noise-Free Diffusion for Indoor Inverse Rendering", "categories": ["cs.CV"], "comment": null, "summary": "Recent methods have shown that pre-trained diffusion models can be fine-tuned to enable generative inverse rendering by learning image-conditioned noise-to-intrinsic mapping. Despite their remarkable progress, they struggle to robustly produce high-quality results as the noise-to-intrinsic paradigm essentially utilizes noisy images with deteriorated structure and appearance for intrinsic prediction, while it is common knowledge that structure and appearance information in an image are crucial for inverse rendering. To address this issue, we present DNF-Intrinsic, a robust yet efficient inverse rendering approach fine-tuned from a pre-trained diffusion model, where we propose to take the source image rather than Gaussian noise as input to directly predict deterministic intrinsic properties via flow matching. Moreover, we design a generative renderer to constrain that the predicted intrinsic properties are physically faithful to the source image. Experiments on both synthetic and real-world datasets show that our method clearly outperforms existing state-of-the-art methods.", "AI": {"tldr": "DNF-Intrinsic\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u9006\u6e32\u67d3\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u4f7f\u7528\u6e90\u56fe\u50cf\u800c\u975e\u566a\u58f0\u8f93\u5165\uff0c\u63d0\u5347\u4e86\u751f\u6210\u7ed3\u679c\u7684\u9c81\u68d2\u6027\u548c\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u566a\u58f0\u56fe\u50cf\u8fdb\u884c\u9006\u6e32\u67d3\uff0c\u5bfc\u81f4\u7ed3\u6784\u4fe1\u606f\u9000\u5316\uff0c\u5f71\u54cd\u7ed3\u679c\u8d28\u91cf\u3002DNF-Intrinsic\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6e90\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\uff0c\u901a\u8fc7\u6d41\u5339\u914d\u76f4\u63a5\u9884\u6d4b\u786e\u5b9a\u6027\u672c\u5f81\u5c5e\u6027\uff0c\u5e76\u8bbe\u8ba1\u751f\u6210\u6e32\u67d3\u5668\u786e\u4fdd\u7269\u7406\u771f\u5b9e\u6027\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cDNF-Intrinsic\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DNF-Intrinsic\u901a\u8fc7\u6539\u8fdb\u8f93\u5165\u548c\u7ea6\u675f\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9006\u6e32\u67d3\u7684\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.03953", "pdf": "https://arxiv.org/pdf/2507.03953", "abs": "https://arxiv.org/abs/2507.03953", "authors": ["Kai Ye", "Tianyi Chen", "Zhen Wang"], "title": "Evaluating Adversarial Protections for Diffusion Personalization: A Comprehensive Study", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to the 2nd Workshop on Reliable and Responsible Foundation   Models (R2-FM 2025) at ICML. 8 pages, 3 figures", "summary": "With the increasing adoption of diffusion models for image generation and personalization, concerns regarding privacy breaches and content misuse have become more pressing. In this study, we conduct a comprehensive comparison of eight perturbation based protection methods: AdvDM, ASPL, FSGM, MetaCloak, Mist, PhotoGuard, SDS, and SimAC--across both portrait and artwork domains. These methods are evaluated under varying perturbation budgets, using a range of metrics to assess visual imperceptibility and protective efficacy. Our results offer practical guidance for method selection. Code is available at: https://github.com/vkeilo/DiffAdvPerturbationBench.", "AI": {"tldr": "\u672c\u6587\u5bf9\u516b\u79cd\u57fa\u4e8e\u6270\u52a8\u7684\u4fdd\u62a4\u65b9\u6cd5\uff08AdvDM\u3001ASPL\u3001FSGM\u3001MetaCloak\u3001Mist\u3001PhotoGuard\u3001SDS\u548cSimAC\uff09\u5728\u8096\u50cf\u548c\u827a\u672f\u4f5c\u54c1\u9886\u57df\u8fdb\u884c\u4e86\u5168\u9762\u6bd4\u8f83\uff0c\u8bc4\u4f30\u4e86\u89c6\u89c9\u4e0d\u53ef\u611f\u77e5\u6027\u548c\u4fdd\u62a4\u6548\u679c\uff0c\u4e3a\u65b9\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "motivation": "\u968f\u7740\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u548c\u4e2a\u6027\u5316\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9690\u79c1\u6cc4\u9732\u548c\u5185\u5bb9\u6ee5\u7528\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u56e0\u6b64\u9700\u8981\u8bc4\u4f30\u548c\u6bd4\u8f83\u73b0\u6709\u7684\u4fdd\u62a4\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u516b\u79cd\u57fa\u4e8e\u6270\u52a8\u7684\u4fdd\u62a4\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u6270\u52a8\u9884\u7b97\u4e0b\uff0c\u4f7f\u7528\u591a\u79cd\u6307\u6807\u8bc4\u4f30\u89c6\u89c9\u4e0d\u53ef\u611f\u77e5\u6027\u548c\u4fdd\u62a4\u6548\u679c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u4e3a\u65b9\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5e76\u516c\u5f00\u4e86\u4ee3\u7801\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6269\u6563\u6a21\u578b\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u5e76\u4fc3\u8fdb\u4e86\u76f8\u5173\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2507.03976", "pdf": "https://arxiv.org/pdf/2507.03976", "abs": "https://arxiv.org/abs/2507.03976", "authors": ["Ze Li", "Feng Zhang", "Xiatian Zhu", "Meng Zhang", "Yanghong Zhou", "P. Y. Mok"], "title": "Robust Low-light Scene Restoration via Illumination Transition", "categories": ["cs.CV"], "comment": "10 pages, 5 figures", "summary": "Synthesizing normal-light novel views from low-light multiview images is an important yet challenging task, given the low visibility and high ISO noise present in the input images. Existing low-light enhancement methods often struggle to effectively preprocess such low-light inputs, as they fail to consider correlations among multiple views. Although other state-of-the-art methods have introduced illumination-related components offering alternative solutions to the problem, they often result in drawbacks such as color distortions and artifacts, and they provide limited denoising effectiveness. In this paper, we propose a novel Robust Low-light Scene Restoration framework (RoSe), which enables effective synthesis of novel views in normal lighting conditions from low-light multiview image inputs, by formulating the task as an illuminance transition estimation problem in 3D space, conceptualizing it as a specialized rendering task. This multiview-consistent illuminance transition field establishes a robust connection between low-light and normal-light conditions. By further exploiting the inherent low-rank property of illumination to constrain the transition representation, we achieve more effective denoising without complex 2D techniques or explicit noise modeling. To implement RoSe, we design a concise dual-branch architecture and introduce a low-rank denoising module. Experiments demonstrate that RoSe significantly outperforms state-of-the-art models in both rendering quality and multiview consistency on standard benchmarks. The codes and data are available at https://pegasus2004.github.io/RoSe.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRoSe\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u4f4e\u5149\u591a\u89c6\u89d2\u56fe\u50cf\u5408\u6210\u6b63\u5e38\u5149\u7167\u7684\u65b0\u89c6\u89d2\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9884\u5904\u7406\u548c\u53bb\u566a\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u4f4e\u5149\u589e\u5f3a\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5904\u7406\u591a\u89c6\u89d2\u76f8\u5173\u6027\uff0c\u4e14\u5176\u4ed6\u5148\u8fdb\u65b9\u6cd5\u5b58\u5728\u8272\u5f69\u5931\u771f\u548c\u53bb\u566a\u6548\u679c\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c06\u4efb\u52a1\u5efa\u6a21\u4e3a3D\u7a7a\u95f4\u4e2d\u7684\u5149\u7167\u8fc7\u6e21\u4f30\u8ba1\u95ee\u9898\uff0c\u5229\u7528\u4f4e\u79e9\u7279\u6027\u7ea6\u675f\u8fc7\u6e21\u8868\u793a\uff0c\u8bbe\u8ba1\u4e86\u53cc\u5206\u652f\u67b6\u6784\u548c\u4f4e\u79e9\u53bb\u566a\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRoSe\u5728\u6e32\u67d3\u8d28\u91cf\u548c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RoSe\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u5149\u591a\u89c6\u89d2\u56fe\u50cf\u5408\u6210\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u6e32\u67d3\u7ed3\u679c\u3002"}}
{"id": "2507.03979", "pdf": "https://arxiv.org/pdf/2507.03979", "abs": "https://arxiv.org/abs/2507.03979", "authors": ["Tianyao He", "Runqi Wang", "Yang Chen", "Dejia Song", "Nemo Chen", "Xu Tang", "Yao Hu"], "title": "Flux-Sculptor: Text-Driven Rich-Attribute Portrait Editing through Decomposed Spatial Flow Control", "categories": ["cs.CV"], "comment": "17 pages, 17 figures", "summary": "Text-driven portrait editing holds significant potential for various applications but also presents considerable challenges. An ideal text-driven portrait editing approach should achieve precise localization and appropriate content modification, yet existing methods struggle to balance reconstruction fidelity and editing flexibility. To address this issue, we propose Flux-Sculptor, a flux-based framework designed for precise text-driven portrait editing. Our framework introduces a Prompt-Aligned Spatial Locator (PASL) to accurately identify relevant editing regions and a Structure-to-Detail Edit Control (S2D-EC) strategy to spatially guide the denoising process through sequential mask-guided fusion of latent representations and attention values. Extensive experiments demonstrate that Flux-Sculptor surpasses existing methods in rich-attribute editing and facial information preservation, making it a strong candidate for practical portrait editing applications. Project page is available at https://flux-sculptor.github.io/.", "AI": {"tldr": "Flux-Sculptor\u662f\u4e00\u4e2a\u57fa\u4e8e\u901a\u91cf\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u7cbe\u786e\u7684\u6587\u672c\u9a71\u52a8\u8096\u50cf\u7f16\u8f91\uff0c\u901a\u8fc7Prompt-Aligned Spatial Locator\u548cStructure-to-Detail Edit Control\u7b56\u7565\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u7f16\u8f91\u7075\u6d3b\u6027\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u786e\u7684\u6587\u672c\u9a71\u52a8\u8096\u50cf\u7f16\u8f91\u65b9\u6cd5\u3002", "method": "\u63d0\u51faFlux-Sculptor\u6846\u67b6\uff0c\u5305\u542bPrompt-Aligned Spatial Locator\uff08PASL\uff09\u548cStructure-to-Detail Edit Control\uff08S2D-EC\uff09\u7b56\u7565\uff0c\u901a\u8fc7\u6f5c\u5728\u8868\u793a\u548c\u6ce8\u610f\u529b\u503c\u7684\u5e8f\u5217\u63a9\u7801\u5f15\u5bfc\u878d\u5408\u5b9e\u73b0\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFlux-Sculptor\u5728\u4e30\u5bcc\u5c5e\u6027\u7f16\u8f91\u548c\u9762\u90e8\u4fe1\u606f\u4fdd\u7559\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Flux-Sculptor\u662f\u4e00\u4e2a\u5b9e\u7528\u7684\u8096\u50cf\u7f16\u8f91\u5de5\u5177\uff0c\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2507.04036", "pdf": "https://arxiv.org/pdf/2507.04036", "abs": "https://arxiv.org/abs/2507.04036", "authors": ["Jingwei Shi", "Zeyu Zhang", "Biao Wu", "Yanjie Liang", "Meng Fang", "Ling Chen", "Yang Zhao"], "title": "PresentAgent: Multimodal Agent for Presentation Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "We present PresentAgent, a multimodal agent that transforms long-form documents into narrated presentation videos. While existing approaches are limited to generating static slides or text summaries, our method advances beyond these limitations by producing fully synchronized visual and spoken content that closely mimics human-style presentations. To achieve this integration, PresentAgent employs a modular pipeline that systematically segments the input document, plans and renders slide-style visual frames, generates contextual spoken narration with large language models and Text-to-Speech models, and seamlessly composes the final video with precise audio-visual alignment. Given the complexity of evaluating such multimodal outputs, we introduce PresentEval, a unified assessment framework powered by Vision-Language Models that comprehensively scores videos across three critical dimensions: content fidelity, visual clarity, and audience comprehension through prompt-based evaluation. Our experimental validation on a curated dataset of 30 document-presentation pairs demonstrates that PresentAgent approaches human-level quality across all evaluation metrics. These results highlight the significant potential of controllable multimodal agents in transforming static textual materials into dynamic, effective, and accessible presentation formats. Code will be available at https://github.com/AIGeeksGroup/PresentAgent.", "AI": {"tldr": "PresentAgent\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u4ee3\u7406\uff0c\u5c06\u957f\u6587\u6863\u8f6c\u6362\u4e3a\u5e26\u65c1\u767d\u7684\u6f14\u793a\u89c6\u9891\uff0c\u8d85\u8d8a\u9759\u6001\u5e7b\u706f\u7247\u6216\u6587\u672c\u6458\u8981\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u89c6\u542c\u540c\u6b65\u7684\u4eba\u7c7b\u98ce\u683c\u6f14\u793a\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u80fd\u751f\u6210\u9759\u6001\u5e7b\u706f\u7247\u6216\u6587\u672c\u6458\u8981\uff0c\u65e0\u6cd5\u6ee1\u8db3\u52a8\u6001\u3001\u89c6\u542c\u540c\u6b65\u7684\u6f14\u793a\u9700\u6c42\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u6d41\u7a0b\uff0c\u5305\u62ec\u6587\u6863\u5206\u6bb5\u3001\u5e7b\u706f\u7247\u89c6\u89c9\u5e27\u89c4\u5212\u4e0e\u6e32\u67d3\u3001\u4e0a\u4e0b\u6587\u65c1\u767d\u751f\u6210\uff08\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u548cTTS\u6a21\u578b\uff09\uff0c\u4ee5\u53ca\u7cbe\u786e\u7684\u89c6\u542c\u5bf9\u9f50\u5408\u6210\u3002", "result": "\u572830\u4e2a\u6587\u6863-\u6f14\u793a\u5bf9\u7684\u6570\u636e\u96c6\u4e0a\uff0cPresentAgent\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u7684\u8d28\u91cf\uff0c\u8bc4\u4f30\u6307\u6807\u5305\u62ec\u5185\u5bb9\u4fdd\u771f\u5ea6\u3001\u89c6\u89c9\u6e05\u6670\u5ea6\u548c\u53d7\u4f17\u7406\u89e3\u5ea6\u3002", "conclusion": "\u53ef\u63a7\u591a\u6a21\u6001\u4ee3\u7406\u5728\u5c06\u9759\u6001\u6587\u672c\u8f6c\u6362\u4e3a\u52a8\u6001\u3001\u9ad8\u6548\u4e14\u6613\u8bbf\u95ee\u7684\u6f14\u793a\u683c\u5f0f\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.04061", "pdf": "https://arxiv.org/pdf/2507.04061", "abs": "https://arxiv.org/abs/2507.04061", "authors": ["Hanghui Guo", "Weijie Shi", "Mengze Li", "Juncheng Li", "Hao Chen", "Yue Cui", "Jiajie Xu", "Jia Zhu", "Jiawei Shen", "Zhangze Chen", "Sirui Han"], "title": "Consistent and Invariant Generalization Learning for Short-video Misinformation Detection", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted to ACM MM 2025,15 pages, 16figures", "summary": "Short-video misinformation detection has attracted wide attention in the multi-modal domain, aiming to accurately identify the misinformation in the video format accompanied by the corresponding audio. Despite significant advancements, current models in this field, trained on particular domains (source domains), often exhibit unsatisfactory performance on unseen domains (target domains) due to domain gaps. To effectively realize such domain generalization on the short-video misinformation detection task, we propose deep insights into the characteristics of different domains: (1) The detection on various domains may mainly rely on different modalities (i.e., mainly focusing on videos or audios). To enhance domain generalization, it is crucial to achieve optimal model performance on all modalities simultaneously. (2) For some domains focusing on cross-modal joint fraud, a comprehensive analysis relying on cross-modal fusion is necessary. However, domain biases located in each modality (especially in each frame of videos) will be accumulated in this fusion process, which may seriously damage the final identification of misinformation. To address these issues, we propose a new DOmain generalization model via ConsisTency and invariance learning for shORt-video misinformation detection (named DOCTOR), which contains two characteristic modules: (1) We involve the cross-modal feature interpolation to map multiple modalities into a shared space and the interpolation distillation to synchronize multi-modal learning; (2) We design the diffusion model to add noise to retain core features of multi modal and enhance domain invariant features through cross-modal guided denoising. Extensive experiments demonstrate the effectiveness of our proposed DOCTOR model. Our code is public available at https://github.com/ghh1125/DOCTOR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDOCTOR\u6a21\u578b\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u5b66\u4e60\u548c\u4e0d\u53d8\u6027\u5b66\u4e60\uff0c\u63d0\u5347\u77ed\u89c6\u9891\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u7684\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u8bad\u7ec3\u540e\uff0c\u5728\u672a\u89c1\u9886\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u9886\u57df\u5dee\u8ddd\u548c\u6a21\u6001\u4f9d\u8d56\u95ee\u9898\u3002", "method": "DOCTOR\u6a21\u578b\u5305\u542b\u8de8\u6a21\u6001\u7279\u5f81\u63d2\u503c\u548c\u63d2\u503c\u84b8\u998f\u6a21\u5757\uff0c\u4ee5\u53ca\u6269\u6563\u6a21\u578b\u589e\u5f3a\u9886\u57df\u4e0d\u53d8\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eDOCTOR\u6a21\u578b\u5728\u8de8\u9886\u57df\u77ed\u89c6\u9891\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DOCTOR\u901a\u8fc7\u591a\u6a21\u6001\u4e00\u81f4\u6027\u5b66\u4e60\u548c\u7279\u5f81\u589e\u5f3a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9886\u57df\u6cdb\u5316\u95ee\u9898\u3002"}}
{"id": "2507.04118", "pdf": "https://arxiv.org/pdf/2507.04118", "abs": "https://arxiv.org/abs/2507.04118", "authors": ["Wenyang Liu", "Chen Cai", "Jianjun Gao", "Kejun Wu", "Yi Wang", "Kim-Hui Yap", "Lap-Pui Chau"], "title": "PromptSR: Cascade Prompting for Lightweight Image Super-Resolution", "categories": ["cs.CV"], "comment": "Accepted in TMM", "summary": "Although the lightweight Vision Transformer has significantly advanced image super-resolution (SR), it faces the inherent challenge of a limited receptive field due to the window-based self-attention modeling. The quadratic computational complexity relative to window size restricts its ability to use a large window size for expanding the receptive field while maintaining low computational costs. To address this challenge, we propose PromptSR, a novel prompt-empowered lightweight image SR method. The core component is the proposed cascade prompting block (CPB), which enhances global information access and local refinement via three cascaded prompting layers: a global anchor prompting layer (GAPL) and two local prompting layers (LPLs). The GAPL leverages downscaled features as anchors to construct low-dimensional anchor prompts (APs) through cross-scale attention, significantly reducing computational costs. These APs, with enhanced global perception, are then used to provide global prompts, efficiently facilitating long-range token connections. The two LPLs subsequently combine category-based self-attention and window-based self-attention to refine the representation in a coarse-to-fine manner. They leverage attention maps from the GAPL as additional global prompts, enabling them to perceive features globally at different granularities for adaptive local refinement. In this way, the proposed CPB effectively combines global priors and local details, significantly enlarging the receptive field while maintaining the low computational costs of our PromptSR. The experimental results demonstrate the superiority of our method, which outperforms state-of-the-art lightweight SR methods in quantitative, qualitative, and complexity evaluations. Our code will be released at https://github.com/wenyang001/PromptSR.", "AI": {"tldr": "PromptSR\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea7\u8054\u63d0\u793a\u5757\uff08CPB\uff09\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\uff0c\u6269\u5927\u611f\u53d7\u91ce\u5e76\u4fdd\u6301\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u7a97\u53e3\u7684\u81ea\u6ce8\u610f\u529b\u6a21\u578b\u5728\u8f7b\u91cf\u7ea7Vision Transformer\u4e2d\u611f\u53d7\u91ce\u53d7\u9650\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u589e\u52a0\u3002", "method": "\u63d0\u51faCPB\u6a21\u5757\uff0c\u5305\u542b\u5168\u5c40\u951a\u70b9\u63d0\u793a\u5c42\uff08GAPL\uff09\u548c\u4e24\u4e2a\u5c40\u90e8\u63d0\u793a\u5c42\uff08LPLs\uff09\uff0c\u901a\u8fc7\u8de8\u5c3a\u5ea6\u6ce8\u610f\u529b\u6784\u5efa\u4f4e\u7ef4\u951a\u70b9\u63d0\u793a\uff0c\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u7ec6\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPromptSR\u5728\u5b9a\u91cf\u3001\u5b9a\u6027\u548c\u590d\u6742\u5ea6\u8bc4\u4f30\u4e0a\u4f18\u4e8e\u73b0\u6709\u8f7b\u91cf\u7ea7\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u3002", "conclusion": "PromptSR\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u5148\u9a8c\u548c\u5c40\u90e8\u7ec6\u8282\uff0c\u6709\u6548\u6269\u5927\u4e86\u611f\u53d7\u91ce\u5e76\u4fdd\u6301\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5177\u6709\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2507.04141", "pdf": "https://arxiv.org/pdf/2507.04141", "abs": "https://arxiv.org/abs/2507.04141", "authors": ["Mohsen Azarmi", "Mahdi Rezaei", "He Wang"], "title": "Pedestrian Intention Prediction via Vision-Language Foundation Models", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.LG", "cs.RO"], "comment": null, "summary": "Prediction of pedestrian crossing intention is a critical function in autonomous vehicles. Conventional vision-based methods of crossing intention prediction often struggle with generalizability, context understanding, and causal reasoning. This study explores the potential of vision-language foundation models (VLFMs) for predicting pedestrian crossing intentions by integrating multimodal data through hierarchical prompt templates. The methodology incorporates contextual information, including visual frames, physical cues observations, and ego-vehicle dynamics, into systematically refined prompts to guide VLFMs effectively in intention prediction. Experiments were conducted on three common datasets-JAAD, PIE, and FU-PIP. Results demonstrate that incorporating vehicle speed, its variations over time, and time-conscious prompts significantly enhances the prediction accuracy up to 19.8%. Additionally, optimised prompts generated via an automatic prompt engineering framework yielded 12.5% further accuracy gains. These findings highlight the superior performance of VLFMs compared to conventional vision-based models, offering enhanced generalisation and contextual understanding for autonomous driving applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff08VLFMs\uff09\u901a\u8fc7\u5206\u5c42\u63d0\u793a\u6a21\u677f\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u884c\u4eba\u8fc7\u9a6c\u8def\u610f\u56fe\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c6\u89c9\u7684\u884c\u4eba\u8fc7\u9a6c\u8def\u610f\u56fe\u9884\u6d4b\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u3001\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u56e0\u679c\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7814\u7a76\u63a2\u7d22VLFMs\u7684\u6f5c\u529b\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u5c42\u63d0\u793a\u6a21\u677f\u6574\u5408\u89c6\u89c9\u5e27\u3001\u7269\u7406\u7ebf\u7d22\u89c2\u5bdf\u548c\u81ea\u8f66\u52a8\u6001\u7b49\u591a\u6a21\u6001\u6570\u636e\uff0c\u5e76\u5229\u7528\u81ea\u52a8\u63d0\u793a\u5de5\u7a0b\u6846\u67b6\u4f18\u5316\u63d0\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u8f66\u901f\u53ca\u5176\u65f6\u95f4\u53d8\u5316\u4ee5\u53ca\u65f6\u95f4\u654f\u611f\u7684\u63d0\u793a\uff0c\u9884\u6d4b\u51c6\u786e\u6027\u63d0\u534719.8%\uff1b\u4f18\u5316\u63d0\u793a\u8fdb\u4e00\u6b65\u5e26\u676512.5%\u7684\u51c6\u786e\u6027\u63d0\u5347\u3002", "conclusion": "VLFMs\u5728\u884c\u4eba\u8fc7\u9a6c\u8def\u610f\u56fe\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u89c6\u89c9\u6a21\u578b\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u3002"}}
{"id": "2507.04151", "pdf": "https://arxiv.org/pdf/2507.04151", "abs": "https://arxiv.org/abs/2507.04151", "authors": ["Fernando Gabriela Garcia", "Spencer Burns", "Ryan Shaw", "Hunter Young"], "title": "Unlocking Compositional Control: Self-Supervision for LVLM-Based Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces Hierarchical Self-Supervised LVLM (Hi-SSLVLM), a novel generative model designed to significantly advance text-to-image synthesis, particularly for complex and compositionally challenging prompts. Traditional methods often grapple with the high cost of meticulously curated paired image-text datasets and struggle with precise control over fine-grained visual attributes and intricate spatial relationships. Our Hi-SSLVLM addresses these limitations through a unique two-stage self-supervised learning strategy. The first stage, Multi-Granularity Visual-Language Grounding, enables the Large Vision-Language Model (LVLM) backbone to autonomously generate and align hierarchical captions (global and local) to images, cultivating a deep internal semantic understanding without reliance on extensive human annotation. The second stage, Self-Refinement and Guided Image Generation, leverages this acquired knowledge by an Internal Compositional Planning (ICP) mechanism, where the LVLM first formulates detailed textual sub-prompts to guide the image generation process, complemented by a novel Semantic Consistency Loss for precise output alignment. Comprehensive experiments against leading baselines, including Janus-Pro-1B, Stable Diffusion XL 1.0, DeepFloyd IF v1.0, and ControlNet-XL, on multi-dimensional benchmarks such as Gemini-2.0-Flash and InternVL3-78B, demonstrate Hi-SSLVLM's superior performance across all fine-grained metrics. An in-depth ablation study confirms the critical role of each proposed component. Furthermore, human evaluations corroborate our quantitative findings, highlighting Hi-SSLVLM's enhanced fidelity to prompt, compositional accuracy, and overall aesthetic quality, marking a significant step towards more controllable and semantically consistent open-ended text-to-image generation.", "AI": {"tldr": "Hi-SSLVLM\u662f\u4e00\u79cd\u65b0\u578b\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u81ea\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u63d0\u5347\u590d\u6742\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u4e14\u96be\u4ee5\u7cbe\u786e\u63a7\u5236\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5c5e\u6027\u548c\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u3002Hi-SSLVLM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u7b56\u7565\uff1a1) \u591a\u7c92\u5ea6\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\uff0c\u81ea\u4e3b\u751f\u6210\u5c42\u6b21\u5316\u6807\u9898\uff1b2) \u81ea\u4f18\u5316\u548c\u5f15\u5bfc\u56fe\u50cf\u751f\u6210\uff0c\u7ed3\u5408\u5185\u90e8\u7ec4\u5408\u89c4\u5212\u673a\u5236\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u635f\u5931\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8eJanus-Pro-1B\u3001Stable Diffusion XL\u7b49\uff0c\u4eba\u7c7b\u8bc4\u4f30\u4e5f\u8bc1\u5b9e\u5176\u66f4\u9ad8\u7684\u751f\u6210\u8d28\u91cf\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "conclusion": "Hi-SSLVLM\u5728\u53ef\u63a7\u6027\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4e3a\u5f00\u653e\u5f0f\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.04152", "pdf": "https://arxiv.org/pdf/2507.04152", "abs": "https://arxiv.org/abs/2507.04152", "authors": ["Spencer Ramsey", "Jeffrey Lee", "Amina Grant"], "title": "LVLM-Composer's Explicit Planning for Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "The burgeoning field of generative artificial intelligence has fundamentally reshaped our approach to content creation, with Large Vision-Language Models (LVLMs) standing at its forefront. While current LVLMs have demonstrated impressive capabilities in text-to-image generation, they often falter when confronted with complex textual descriptions demanding precise compositional understanding and visual planning. This limitation particularly impacts the accurate rendering of multiple objects, their attributes, spatial relationships, and specific poses within intricate scenes, as evidenced by benchmarks like LongBench-T2I. To address these challenges, we introduce LVLM-Composer, a novel 10-billion parameter scale LVLM specifically engineered for enhanced compositional image synthesis. Our method incorporates a Hierarchical Semantic Planning Module for structured prompt decomposition and a Fine-Grained Feature Alignment Mechanism for precise visual guidance during generation. We propose a multi-stage training paradigm, featuring Hierarchical Semantic-Visual Grounding Pre-training and Compositional Planning Reinforcement Learning with Self-Correction, to instill robust compositional reasoning. Extensive experiments on the LongBench-T2I benchmark, utilizing automatic evaluation by Gemini-2.0-Flash and InternVL3-78B, demonstrate LVLM-Composer's superior performance across critical compositional dimensions including object accuracy, composition fidelity, and pose accuracy, significantly outperforming state-of-the-art baselines. An in-depth ablation study further validates the indispensable contribution of our proposed modules, while human evaluations confirm the perceptual superiority of our generated images. LVLM-Composer represents a significant step towards truly controllable and compositionally accurate open-ended text-to-image generation.", "AI": {"tldr": "LVLM-Composer\u662f\u4e00\u79cd\u65b0\u578b\u7684\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u5c42\u8bed\u4e49\u89c4\u5212\u548c\u7ec6\u7c92\u5ea6\u7279\u5f81\u5bf9\u9f50\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u6587\u672c\u63cf\u8ff0\u7684\u56fe\u50cf\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6587\u672c\u63cf\u8ff0\u7684\u56fe\u50cf\u751f\u6210\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u591a\u5bf9\u8c61\u3001\u5c5e\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\u7684\u51c6\u786e\u6e32\u67d3\u3002", "method": "\u63d0\u51faLVLM-Composer\uff0c\u91c7\u7528\u5206\u5c42\u8bed\u4e49\u89c4\u5212\u6a21\u5757\u548c\u7ec6\u7c92\u5ea6\u7279\u5f81\u5bf9\u9f50\u673a\u5236\uff0c\u7ed3\u5408\u591a\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\u3002", "result": "\u5728LongBench-T2I\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LVLM-Composer\u4e3a\u53ef\u63a7\u4e14\u51c6\u786e\u7684\u5f00\u653e\u5f0f\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2507.04183", "pdf": "https://arxiv.org/pdf/2507.04183", "abs": "https://arxiv.org/abs/2507.04183", "authors": ["Fengrui Tian", "Tianjiao Ding", "Jinqi Luo", "Hancheng Min", "Ren\u00e9 Vidal"], "title": "Voyaging into Unbounded Dynamic Scenes from a Single View", "categories": ["cs.CV"], "comment": "Accepted by International Conference on Computer Vision (ICCV) 2025.   Project Page: https://tianfr.github.io/DynamicVoyager", "summary": "This paper studies the problem of generating an unbounded dynamic scene from a single view, which has wide applications in augmented/virtual reality and robotics. Since the scene is changing over time, different generated views need to be consistent with the underlying 3D motions. While previous works learn such consistency by training from multiple views, the generated scene regions are bounded to be close to the training views with limited camera movements. To address this issue, we propose DynamicVoyager that reformulates the dynamic scene generation as a scene outpainting process for new dynamic content. As 2D outpainting models can hardly generate 3D consistent motions from only 2D pixels at a single view, we consider pixels as rays to enrich the pixel input with the ray context, so that the 3D motion consistency can be learned from the ray information. More specifically, we first map the single-view video input to a dynamic point cloud with the estimated video depths. Then we render the partial video at a novel view and outpaint the video with ray contexts from the point cloud to generate 3D consistent motions. We employ the outpainted video to update the point cloud, which is used for scene outpainting from future novel views. Experiments show that our model is able to generate unbounded scenes with consistent motions along fly-through cameras, and the generated contents can be controlled with scene prompts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDynamicVoyager\uff0c\u901a\u8fc7\u5c06\u52a8\u6001\u573a\u666f\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u573a\u666f\u5916\u7ed8\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u5355\u89c6\u89d2\u751f\u6210\u65e0\u754c\u52a8\u6001\u573a\u666f\u76843D\u8fd0\u52a8\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u5355\u89c6\u89d2\u751f\u6210\u65e0\u754c\u52a8\u6001\u573a\u666f\u7684\u95ee\u9898\uff0c\u5e94\u7528\u4e8e\u589e\u5f3a/\u865a\u62df\u73b0\u5b9e\u548c\u673a\u5668\u4eba\u9886\u57df\uff0c\u9700\u786e\u4fdd\u751f\u6210\u89c6\u56fe\u4e0e3D\u8fd0\u52a8\u4e00\u81f4\u3002", "method": "\u5c06\u5355\u89c6\u89d2\u89c6\u9891\u8f93\u5165\u6620\u5c04\u4e3a\u52a8\u6001\u70b9\u4e91\uff0c\u5229\u7528\u5c04\u7ebf\u4e0a\u4e0b\u6587\u6e32\u67d3\u90e8\u5206\u89c6\u9891\u5e76\u5916\u7ed8\uff0c\u751f\u62103D\u4e00\u81f4\u8fd0\u52a8\uff0c\u66f4\u65b0\u70b9\u4e91\u4ee5\u652f\u6301\u672a\u6765\u89c6\u56fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6a21\u578b\u80fd\u751f\u6210\u65e0\u754c\u573a\u666f\uff0c\u8fd0\u52a8\u4e00\u81f4\u4e14\u53ef\u901a\u8fc7\u573a\u666f\u63d0\u793a\u63a7\u5236\u751f\u6210\u5185\u5bb9\u3002", "conclusion": "DynamicVoyager\u901a\u8fc7\u5c04\u7ebf\u4e0a\u4e0b\u6587\u548c\u70b9\u4e91\u66f4\u65b0\uff0c\u5b9e\u73b0\u4e86\u5355\u89c6\u89d2\u4e0b\u65e0\u754c\u52a8\u6001\u573a\u666f\u76843D\u4e00\u81f4\u751f\u6210\u3002"}}
{"id": "2507.04207", "pdf": "https://arxiv.org/pdf/2507.04207", "abs": "https://arxiv.org/abs/2507.04207", "authors": ["Yu-Shan Tai", "An-Yeu", "Wu"], "title": "Quick Bypass Mechanism of Zero-Shot Diffusion-Based Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in diffusion models have demonstrated remarkable success in various image generation tasks. Building upon these achievements, diffusion models have also been effectively adapted to image restoration tasks, e.g., super-resolution and deblurring, aiming to recover high-quality images from degraded inputs. Although existing zero-shot approaches enable pretrained diffusion models to perform restoration tasks without additional fine-tuning, these methods often suffer from prolonged iteration times in the denoising process. To address this limitation, we propose a Quick Bypass Mechanism (QBM), a strategy that significantly accelerates the denoising process by initializing from an intermediate approximation, effectively bypassing early denoising steps. Furthermore, recognizing that approximation may introduce inconsistencies, we introduce a Revised Reverse Process (RRP), which adjusts the weighting of random noise to enhance the stochasticity and mitigate potential disharmony. We validate proposed methods on ImageNet-1K and CelebA-HQ across multiple image restoration tasks, e.g., super-resolution, deblurring, and compressed sensing. Our experimental results show that the proposed methods can effectively accelerate existing methods while maintaining original performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u7ed5\u8fc7\u673a\u5236\uff08QBM\uff09\u548c\u4fee\u8ba2\u53cd\u5411\u8fc7\u7a0b\uff08RRP\uff09\uff0c\u663e\u8457\u52a0\u901f\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u7684\u53bb\u566a\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672c\u65b9\u6cd5\u5728\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u56e0\u53bb\u566a\u8fc7\u7a0b\u8017\u65f6\u8f83\u957f\u800c\u53d7\u9650\uff0c\u9700\u6539\u8fdb\u4ee5\u63d0\u5347\u6548\u7387\u3002", "method": "\u901a\u8fc7QBM\u4ece\u4e2d\u95f4\u8fd1\u4f3c\u521d\u59cb\u5316\u4ee5\u7ed5\u8fc7\u65e9\u671f\u53bb\u566a\u6b65\u9aa4\uff0c\u5e76\u901a\u8fc7RRP\u8c03\u6574\u968f\u673a\u566a\u58f0\u6743\u91cd\u4ee5\u589e\u5f3a\u968f\u673a\u6027\u3002", "result": "\u5728ImageNet-1K\u548cCelebA-HQ\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cQBM\u548cRRP\u80fd\u6709\u6548\u52a0\u901f\u73b0\u6709\u65b9\u6cd5\u4e14\u4e0d\u964d\u4f4e\u6027\u80fd\u3002", "conclusion": "QBM\u548cRRP\u4e3a\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.04218", "pdf": "https://arxiv.org/pdf/2507.04218", "abs": "https://arxiv.org/abs/2507.04218", "authors": ["Xiwei Hu", "Haokun Chen", "Zhongqi Qi", "Hui Zhang", "Dexiang Hong", "Jie Shao", "Xinglong Wu"], "title": "DreamPoster: A Unified Framework for Image-Conditioned Generative Poster Design", "categories": ["cs.CV"], "comment": null, "summary": "We present DreamPoster, a Text-to-Image generation framework that intelligently synthesizes high-quality posters from user-provided images and text prompts while maintaining content fidelity and supporting flexible resolution and layout outputs. Specifically, DreamPoster is built upon our T2I model, Seedream3.0 to uniformly process different poster generating types. For dataset construction, we propose a systematic data annotation pipeline that precisely annotates textual content and typographic hierarchy information within poster images, while employing comprehensive methodologies to construct paired datasets comprising source materials (e.g., raw graphics/text) and their corresponding final poster outputs. Additionally, we implement a progressive training strategy that enables the model to hierarchically acquire multi-task generation capabilities while maintaining high-quality generation. Evaluations on our testing benchmarks demonstrate DreamPoster's superiority over existing methods, achieving a high usability rate of 88.55\\%, compared to GPT-4o (47.56\\%) and SeedEdit3.0 (25.96\\%). DreamPoster will be online in Jimeng and other Bytedance Apps.", "AI": {"tldr": "DreamPoster\u662f\u4e00\u4e2a\u57fa\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u7528\u6237\u63d0\u4f9b\u7684\u56fe\u50cf\u548c\u6587\u672c\u63d0\u793a\u4e2d\u667a\u80fd\u5408\u6210\u9ad8\u8d28\u91cf\u6d77\u62a5\uff0c\u540c\u65f6\u4fdd\u6301\u5185\u5bb9\u4fdd\u771f\u5ea6\u5e76\u652f\u6301\u7075\u6d3b\u7684\u5206\u8fa8\u7387\u548c\u5e03\u5c40\u8f93\u51fa\u3002", "motivation": "\u73b0\u6709\u7684\u6d77\u62a5\u751f\u6210\u65b9\u6cd5\u5728\u5185\u5bb9\u4fdd\u771f\u5ea6\u548c\u7075\u6d3b\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0cDreamPoster\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u57fa\u4e8eT2I\u6a21\u578bSeedream3.0\uff0c\u91c7\u7528\u7cfb\u7edf\u5316\u7684\u6570\u636e\u6807\u6ce8\u6d41\u7a0b\u548c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\uff0c\u6784\u5efa\u591a\u4efb\u52a1\u751f\u6210\u80fd\u529b\u3002", "result": "\u5728\u6d4b\u8bd5\u57fa\u51c6\u4e2d\uff0cDreamPoster\u7684\u53ef\u7528\u6027\u8fbe\u523088.55%\uff0c\u663e\u8457\u4f18\u4e8eGPT-4o\uff0847.56%\uff09\u548cSeedEdit3.0\uff0825.96%\uff09\u3002", "conclusion": "DreamPoster\u5c55\u793a\u4e86\u5728\u6d77\u62a5\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u5c06\u5e94\u7528\u4e8e\u5b57\u8282\u8df3\u52a8\u7684\u591a\u4e2a\u5e73\u53f0\u3002"}}
{"id": "2507.04243", "pdf": "https://arxiv.org/pdf/2507.04243", "abs": "https://arxiv.org/abs/2507.04243", "authors": ["Xinbo Wang", "Wenju Xu", "Qing Zhang", "Wei-Shi Zheng"], "title": "Domain Generalizable Portrait Style Transfer", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ICCV2025", "summary": "This paper presents a portrait style transfer method that generalizes well to various different domains while enabling high-quality semantic-aligned stylization on regions including hair, eyes, eyelashes, skins, lips, and background. To this end, we propose to establish dense semantic correspondence between the given input and reference portraits based on a pre-trained model and a semantic adapter, with which we obtain a warped reference semantically aligned with the input. To ensure effective yet controllable style transfer, we devise an AdaIN-Wavelet transform to balance content preservation and stylization by blending low-frequency information of the warped reference with high-frequency information of the input in the latent space. A style adapter is also designed to provide style guidance from the warped reference. With the stylized latent from AdaIN-Wavelet transform, we employ a dual-conditional diffusion model that integrates a ControlNet recording high-frequency information and the style guidance to generate the final result. Extensive experiments demonstrate the superiority of our method. Our code and trained model are available at https://github.com/wangxb29/DGPST.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8096\u50cf\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u5bf9\u9f50\u548cAdaIN-Wavelet\u53d8\u6362\u5b9e\u73b0\u9ad8\u8d28\u91cf\u98ce\u683c\u5316\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u4e0d\u540c\u9886\u57df\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u548c\u8bed\u4e49\u5bf9\u9f50\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u8bed\u4e49\u9002\u914d\u5668\u5efa\u7acb\u5bc6\u96c6\u8bed\u4e49\u5bf9\u5e94\uff0c\u4f7f\u7528AdaIN-Wavelet\u53d8\u6362\u5e73\u8861\u5185\u5bb9\u4fdd\u7559\u4e0e\u98ce\u683c\u5316\uff0c\u7ed3\u5408\u53cc\u6761\u4ef6\u6269\u6563\u6a21\u578b\u751f\u6210\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u98ce\u683c\u8fc1\u79fb\u8d28\u91cf\u548c\u8bed\u4e49\u5bf9\u9f50\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u53ef\u63a7\u7684\u8096\u50cf\u98ce\u683c\u8fc1\u79fb\uff0c\u5e76\u5728\u591a\u4e2a\u533a\u57df\uff08\u5982\u5934\u53d1\u3001\u773c\u775b\u7b49\uff09\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.04277", "pdf": "https://arxiv.org/pdf/2507.04277", "abs": "https://arxiv.org/abs/2507.04277", "authors": ["Guangrui Bai", "Hailong Yan", "Wenhai Liu", "Yahui Deng", "Erbao Dong"], "title": "Towards Lightest Low-Light Image Enhancement Architecture for Mobile Devices", "categories": ["cs.CV"], "comment": "Submitted to ESWA", "summary": "Real-time low-light image enhancement on mobile and embedded devices requires models that balance visual quality and computational efficiency. Existing deep learning methods often rely on large networks and labeled datasets, limiting their deployment on resource-constrained platforms. In this paper, we propose LiteIE, an ultra-lightweight unsupervised enhancement framework that eliminates dependence on large-scale supervision and generalizes well across diverse conditions. We design a backbone-agnostic feature extractor with only two convolutional layers to produce compact image features enhancement tensors. In addition, we develop a parameter-free Iterative Restoration Module, which reuses the extracted features to progressively recover fine details lost in earlier enhancement steps, without introducing any additional learnable parameters. We further propose an unsupervised training objective that integrates exposure control, edge-aware smoothness, and multi-scale color consistency losses. Experiments on the LOL dataset, LiteIE achieves 19.04 dB PSNR, surpassing SOTA by 1.4 dB while using only 0.07\\% of its parameters. On a Snapdragon 8 Gen 3 mobile processor, LiteIE runs at 30 FPS for 4K images with just 58 parameters, enabling real-time deployment on edge devices. These results establish LiteIE as an efficient and practical solution for low-light enhancement on resource-limited platforms.", "AI": {"tldr": "LiteIE\u662f\u4e00\u79cd\u8d85\u8f7b\u91cf\u7ea7\u65e0\u76d1\u7763\u589e\u5f3a\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u79fb\u52a8\u548c\u5d4c\u5165\u5f0f\u8bbe\u5907\uff0c\u5e73\u8861\u89c6\u89c9\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5927\u7f51\u7edc\u548c\u6807\u6ce8\u6570\u636e\uff0c\u9650\u5236\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u7684\u90e8\u7f72\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4ec5\u542b\u4e24\u4e2a\u5377\u79ef\u5c42\u7684\u9aa8\u5e72\u65e0\u5173\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u4ee5\u53ca\u65e0\u53c2\u6570\u7684\u8fed\u4ee3\u6062\u590d\u6a21\u5757\uff0c\u7ed3\u5408\u65e0\u76d1\u7763\u8bad\u7ec3\u76ee\u6807\u3002", "result": "\u5728LOL\u6570\u636e\u96c6\u4e0aPSNR\u8fbe19.04 dB\uff0c\u53c2\u6570\u4ec50.07%\uff0c\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b030 FPS\u76844K\u56fe\u50cf\u5904\u7406\u3002", "conclusion": "LiteIE\u662f\u8d44\u6e90\u6709\u9650\u5e73\u53f0\u4e0a\u4f4e\u5149\u589e\u5f3a\u7684\u9ad8\u6548\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.04290", "pdf": "https://arxiv.org/pdf/2507.04290", "abs": "https://arxiv.org/abs/2507.04290", "authors": ["Weilun Feng", "Chuanguang Yang", "Haotong Qin", "Yuqi Li", "Xiangqi Li", "Zhulin An", "Libo Huang", "Boyu Diao", "Fuzhen Zhuang", "Michele Magno", "Yongjun Xu", "Yingli Tian", "Tingwen Huang"], "title": "MPQ-DMv2: Flexible Residual Mixed Precision Quantization for Low-Bit Diffusion Models with Temporal Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have demonstrated remarkable performance on vision generation tasks. However, the high computational complexity hinders its wide application on edge devices. Quantization has emerged as a promising technique for inference acceleration and memory reduction. However, existing quantization methods do not generalize well under extremely low-bit (2-4 bit) quantization. Directly applying these methods will cause severe performance degradation. We identify that the existing quantization framework suffers from the outlier-unfriendly quantizer design, suboptimal initialization, and optimization strategy. We present MPQ-DMv2, an improved \\textbf{M}ixed \\textbf{P}recision \\textbf{Q}uantization framework for extremely low-bit \\textbf{D}iffusion \\textbf{M}odels. For the quantization perspective, the imbalanced distribution caused by salient outliers is quantization-unfriendly for uniform quantizer. We propose \\textit{Flexible Z-Order Residual Mixed Quantization} that utilizes an efficient binary residual branch for flexible quant steps to handle salient error. For the optimization framework, we theoretically analyzed the convergence and optimality of the LoRA module and propose \\textit{Object-Oriented Low-Rank Initialization} to use prior quantization error for informative initialization. We then propose \\textit{Memory-based Temporal Relation Distillation} to construct an online time-aware pixel queue for long-term denoising temporal information distillation, which ensures the overall temporal consistency between quantized and full-precision model. Comprehensive experiments on various generation tasks show that our MPQ-DMv2 surpasses current SOTA methods by a great margin on different architectures, especially under extremely low-bit widths.", "AI": {"tldr": "MPQ-DMv2\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u6781\u4f4e\u6bd4\u7279\u6269\u6563\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u5728\u6781\u4f4e\u6bd4\u7279\u4e0b\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u89c6\u89c9\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u9650\u5236\u4e86\u5176\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u3002\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u5728\u6781\u4f4e\u6bd4\u7279\uff082-4\u4f4d\uff09\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u3002", "method": "\u63d0\u51faFlexible Z-Order Residual Mixed Quantization\u5904\u7406\u5f02\u5e38\u503c\uff0c\u91c7\u7528Object-Oriented Low-Rank Initialization\u4f18\u5316\u521d\u59cb\u5316\uff0c\u5e76\u901a\u8fc7Memory-based Temporal Relation Distillation\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5728\u5404\u79cd\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMPQ-DMv2\u5728\u4e0d\u540c\u67b6\u6784\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u6781\u4f4e\u6bd4\u7279\u5bbd\u5ea6\u4e0b\u3002", "conclusion": "MPQ-DMv2\u901a\u8fc7\u6539\u8fdb\u91cf\u5316\u8bbe\u8ba1\u548c\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6781\u4f4e\u6bd4\u7279\u6269\u6563\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.04403", "pdf": "https://arxiv.org/pdf/2507.04403", "abs": "https://arxiv.org/abs/2507.04403", "authors": ["Tongyan Hua", "Lutao Jiang", "Ying-Cong Chen", "Wufan Zhao"], "title": "Sat2City: 3D City Generation from A Single Satellite Image with Cascaded Latent Diffusion", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "Recent advancements in generative models have enabled 3D urban scene generation from satellite imagery, unlocking promising applications in gaming, digital twins, and beyond. However, most existing methods rely heavily on neural rendering techniques, which hinder their ability to produce detailed 3D structures on a broader scale, largely due to the inherent structural ambiguity derived from relatively limited 2D observations. To address this challenge, we propose Sat2City, a novel framework that synergizes the representational capacity of sparse voxel grids with latent diffusion models, tailored specifically for our novel 3D city dataset. Our approach is enabled by three key components: (1) A cascaded latent diffusion framework that progressively recovers 3D city structures from satellite imagery, (2) a Re-Hash operation at its Variational Autoencoder (VAE) bottleneck to compute multi-scale feature grids for stable appearance optimization and (3) an inverse sampling strategy enabling implicit supervision for smooth appearance transitioning.To overcome the challenge of collecting real-world city-scale 3D models with high-quality geometry and appearance, we introduce a dataset of synthesized large-scale 3D cities paired with satellite-view height maps. Validated on this dataset, our framework generates detailed 3D structures from a single satellite image, achieving superior fidelity compared to existing city generation models.", "AI": {"tldr": "Sat2City\u6846\u67b6\u7ed3\u5408\u7a00\u758f\u4f53\u7d20\u7f51\u683c\u548c\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u4ece\u536b\u661f\u56fe\u50cf\u751f\u6210\u8be6\u7ec63D\u57ce\u5e02\u573a\u666f\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u795e\u7ecf\u6e32\u67d3\u6280\u672f\uff0c\u96be\u4ee5\u4ece\u6709\u9650\u76842D\u89c2\u6d4b\u4e2d\u751f\u6210\u5927\u89c4\u6a21\u8be6\u7ec63D\u7ed3\u6784\u3002", "method": "\u63d0\u51faSat2City\u6846\u67b6\uff0c\u5305\u542b\u7ea7\u8054\u6f5c\u5728\u6269\u6563\u6a21\u578b\u3001Re-Hash\u64cd\u4f5c\u548c\u9006\u91c7\u6837\u7b56\u7565\uff0c\u7ed3\u5408\u5408\u6210\u6570\u636e\u96c6\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u751f\u6210\u9ad8\u4fdd\u771f3D\u7ed3\u6784\uff0c\u4f18\u4e8e\u73b0\u6709\u57ce\u5e02\u751f\u6210\u6a21\u578b\u3002", "conclusion": "Sat2City\u4e3a3D\u57ce\u5e02\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.04408", "pdf": "https://arxiv.org/pdf/2507.04408", "abs": "https://arxiv.org/abs/2507.04408", "authors": ["Aoxiang Fan", "Corentin Dumery", "Nicolas Talabot", "Pascal Fua"], "title": "A View-consistent Sampling Method for Regularized Training of Neural Radiance Fields", "categories": ["cs.CV"], "comment": "ICCV 2025 accepted", "summary": "Neural Radiance Fields (NeRF) has emerged as a compelling framework for scene representation and 3D recovery. To improve its performance on real-world data, depth regularizations have proven to be the most effective ones. However, depth estimation models not only require expensive 3D supervision in training, but also suffer from generalization issues. As a result, the depth estimations can be erroneous in practice, especially for outdoor unbounded scenes. In this paper, we propose to employ view-consistent distributions instead of fixed depth value estimations to regularize NeRF training. Specifically, the distribution is computed by utilizing both low-level color features and high-level distilled features from foundation models at the projected 2D pixel-locations from per-ray sampled 3D points. By sampling from the view-consistency distributions, an implicit regularization is imposed on the training of NeRF. We also utilize a depth-pushing loss that works in conjunction with the sampling technique to jointly provide effective regularizations for eliminating the failure modes. Extensive experiments conducted on various scenes from public datasets demonstrate that our proposed method can generate significantly better novel view synthesis results than state-of-the-art NeRF variants as well as different depth regularization methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u56fe\u4e00\u81f4\u6027\u5206\u5e03\u7684\u6df1\u5ea6\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdbNeRF\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u7684\u8868\u73b0\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u7684\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u9700\u8981\u6602\u8d35\u76843D\u76d1\u7763\u8bad\u7ec3\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u5c24\u5176\u5728\u6237\u5916\u65e0\u754c\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5229\u7528\u89c6\u56fe\u4e00\u81f4\u6027\u5206\u5e03\uff08\u7ed3\u5408\u4f4e\u7ea7\u989c\u8272\u7279\u5f81\u548c\u9ad8\u7ea7\u84b8\u998f\u7279\u5f81\uff09\u548c\u6df1\u5ea6\u63a8\u52a8\u635f\u5931\uff0c\u5bf9NeRF\u8bad\u7ec3\u8fdb\u884c\u9690\u5f0f\u6b63\u5219\u5316\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709NeRF\u53d8\u4f53\u548c\u6df1\u5ea6\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u89c6\u56fe\u4e00\u81f4\u6027\u5206\u5e03\u548c\u6df1\u5ea6\u63a8\u52a8\u635f\u5931\uff0c\u6709\u6548\u63d0\u5347\u4e86NeRF\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2507.04447", "pdf": "https://arxiv.org/pdf/2507.04447", "abs": "https://arxiv.org/abs/2507.04447", "authors": ["Wenyao Zhang", "Hongsi Liu", "Zekun Qi", "Yunnan Wang", "XinQiang Yu", "Jiazhao Zhang", "Runpei Dong", "Jiawei He", "He Wang", "Zhizheng Zhang", "Li Yi", "Wenjun Zeng", "Xin Jin"], "title": "DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Recent advances in vision-language-action (VLA) models have shown promise in integrating image generation with action prediction to improve generalization and reasoning in robot manipulation. However, existing methods are limited to challenging image-based forecasting, which suffers from redundant information and lacks comprehensive and critical world knowledge, including dynamic, spatial and semantic information. To address these limitations, we propose DreamVLA, a novel VLA framework that integrates comprehensive world knowledge forecasting to enable inverse dynamics modeling, thereby establishing a perception-prediction-action loop for manipulation tasks. Specifically, DreamVLA introduces a dynamic-region-guided world knowledge prediction, integrated with the spatial and semantic cues, which provide compact yet comprehensive representations for action planning. This design aligns with how humans interact with the world by first forming abstract multimodal reasoning chains before acting. To mitigate interference among the dynamic, spatial and semantic information during training, we adopt a block-wise structured attention mechanism that masks their mutual attention, preventing information leakage and keeping each representation clean and disentangled. Moreover, to model the conditional distribution over future actions, we employ a diffusion-based transformer that disentangles action representations from shared latent features. Extensive experiments on both real-world and simulation environments demonstrate that DreamVLA achieves 76.7% success rate on real robot tasks and 4.44 average length on the CALVIN ABC-D benchmarks.", "AI": {"tldr": "DreamVLA\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5168\u9762\u7684\u4e16\u754c\u77e5\u8bc6\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u3001\u7a7a\u95f4\u548c\u8bed\u4e49\u4fe1\u606f\u4e0a\u7684\u4e0d\u8db3\uff0c\u5b9e\u73b0\u4e86\u611f\u77e5-\u9884\u6d4b-\u52a8\u4f5c\u7684\u95ed\u73af\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e0e\u52a8\u4f5c\u9884\u6d4b\u7ed3\u5408\u4e2d\u5b58\u5728\u5197\u4f59\u4fe1\u606f\u548c\u4e16\u754c\u77e5\u8bc6\u4e0d\u5168\u9762\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6cdb\u5316\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "DreamVLA\u5f15\u5165\u4e86\u52a8\u6001\u533a\u57df\u5f15\u5bfc\u7684\u4e16\u754c\u77e5\u8bc6\u9884\u6d4b\uff0c\u7ed3\u5408\u7a7a\u95f4\u548c\u8bed\u4e49\u7ebf\u7d22\uff0c\u5e76\u91c7\u7528\u5757\u72b6\u7ed3\u6784\u5316\u6ce8\u610f\u529b\u673a\u5236\u9632\u6b62\u4fe1\u606f\u5e72\u6270\uff0c\u540c\u65f6\u4f7f\u7528\u6269\u6563\u53d8\u6362\u5668\u89e3\u8026\u52a8\u4f5c\u8868\u793a\u3002", "result": "\u5728\u771f\u5b9e\u548c\u4eff\u771f\u73af\u5883\u4e2d\uff0cDreamVLA\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8fbe\u523076.7%\u7684\u6210\u529f\u7387\uff0c\u5728CALVIN ABC-D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u957f\u5ea6\u4e3a4.44\u3002", "conclusion": "DreamVLA\u901a\u8fc7\u6574\u5408\u5168\u9762\u7684\u4e16\u754c\u77e5\u8bc6\u9884\u6d4b\u548c\u89e3\u8026\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.04451", "pdf": "https://arxiv.org/pdf/2507.04451", "abs": "https://arxiv.org/abs/2507.04451", "authors": ["Zheyuan Liu", "Munan Ning", "Qihui Zhang", "Shuo Yang", "Zhongrui Wang", "Yiwei Yang", "Xianzhe Xu", "Yibing Song", "Weihua Chen", "Fan Wang", "Li Yuan"], "title": "CoT-lized Diffusion: Let's Reinforce T2I Generation Step-by-step", "categories": ["cs.CV"], "comment": null, "summary": "Current text-to-image (T2I) generation models struggle to align spatial composition with the input text, especially in complex scenes. Even layout-based approaches yield suboptimal spatial control, as their generation process is decoupled from layout planning, making it difficult to refine the layout during synthesis. We present CoT-Diff, a framework that brings step-by-step CoT-style reasoning into T2I generation by tightly integrating Multimodal Large Language Model (MLLM)-driven 3D layout planning with the diffusion process. CoT-Diff enables layout-aware reasoning inline within a single diffusion round: at each denoising step, the MLLM evaluates intermediate predictions, dynamically updates the 3D scene layout, and continuously guides the generation process. The updated layout is converted into semantic conditions and depth maps, which are fused into the diffusion model via a condition-aware attention mechanism, enabling precise spatial control and semantic injection. Experiments on 3D Scene benchmarks show that CoT-Diff significantly improves spatial alignment and compositional fidelity, and outperforms the state-of-the-art method by 34.7% in complex scene spatial accuracy, thereby validating the effectiveness of this entangled generation paradigm.", "AI": {"tldr": "CoT-Diff\u901a\u8fc7\u5c06MLLM\u9a71\u52a8\u76843D\u5e03\u5c40\u89c4\u5212\u4e0e\u6269\u6563\u8fc7\u7a0b\u7d27\u5bc6\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u7a7a\u95f4\u5bf9\u9f50\u548c\u7ec4\u5408\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e2d\u96be\u4ee5\u5b9e\u73b0\u7a7a\u95f4\u5bf9\u9f50\uff0c\u5e03\u5c40\u89c4\u5212\u4e0e\u751f\u6210\u8fc7\u7a0b\u8131\u8282\u5bfc\u81f4\u63a7\u5236\u4e0d\u8db3\u3002", "method": "CoT-Diff\u6846\u67b6\u5c06MLLM\u9a71\u52a8\u76843D\u5e03\u5c40\u89c4\u5212\u4e0e\u6269\u6563\u8fc7\u7a0b\u7ed3\u5408\uff0c\u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u5e03\u5c40\u548c\u6761\u4ef6\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236\u3002", "result": "\u57283D\u573a\u666f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoT-Diff\u7684\u7a7a\u95f4\u5bf9\u9f50\u548c\u7ec4\u5408\u4fdd\u771f\u5ea6\u663e\u8457\u63d0\u5347\uff0c\u590d\u6742\u573a\u666f\u7a7a\u95f4\u51c6\u786e\u7387\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad834.7%\u3002", "conclusion": "CoT-Diff\u9a8c\u8bc1\u4e86\u8fd9\u79cd\u7d27\u5bc6\u8026\u5408\u751f\u6210\u8303\u5f0f\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.04456", "pdf": "https://arxiv.org/pdf/2507.04456", "abs": "https://arxiv.org/abs/2507.04456", "authors": ["Haotong Qin", "Xianglong Liu", "Xudong Ma", "Lei Ke", "Yulun Zhang", "Jie Luo", "Michele Magno"], "title": "BiVM: Accurate Binarized Neural Network for Efficient Video Matting", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural networks for real-time video matting suffer significant computational limitations on edge devices, hindering their adoption in widespread applications such as online conferences and short-form video production. Binarization emerges as one of the most common compression approaches with compact 1-bit parameters and efficient bitwise operations. However, accuracy and efficiency limitations exist in the binarized video matting network due to its degenerated encoder and redundant decoder. Following a theoretical analysis based on the information bottleneck principle, the limitations are mainly caused by the degradation of prediction-relevant information in the intermediate features and the redundant computation in prediction-irrelevant areas. We present BiVM, an accurate and resource-efficient Binarized neural network for Video Matting. First, we present a series of binarized computation structures with elastic shortcuts and evolvable topologies, enabling the constructed encoder backbone to extract high-quality representation from input videos for accurate prediction. Second, we sparse the intermediate feature of the binarized decoder by masking homogeneous parts, allowing the decoder to focus on representation with diverse details while alleviating the computation burden for efficient inference. Furthermore, we construct a localized binarization-aware mimicking framework with the information-guided strategy, prompting matting-related representation in full-precision counterparts to be accurately and fully utilized. Comprehensive experiments show that the proposed BiVM surpasses alternative binarized video matting networks, including state-of-the-art (SOTA) binarization methods, by a substantial margin. Moreover, our BiVM achieves significant savings of 14.3x and 21.6x in computation and storage costs, respectively. We also evaluate BiVM on ARM CPU hardware.", "AI": {"tldr": "BiVM\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u4e8c\u503c\u5316\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u5b9e\u65f6\u89c6\u9891\u62a0\u56fe\uff0c\u901a\u8fc7\u5f39\u6027\u5feb\u6377\u8fde\u63a5\u548c\u53ef\u8fdb\u5316\u62d3\u6251\u7ed3\u6784\u63d0\u5347\u7f16\u7801\u5668\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u7a00\u758f\u89e3\u7801\u5668\u7279\u5f81\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u89c6\u9891\u62a0\u56fe\u56e0\u8ba1\u7b97\u9650\u5236\u96be\u4ee5\u5e7f\u6cdb\u5e94\u7528\uff0c\u73b0\u6709\u4e8c\u503c\u5316\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u5f39\u6027\u5feb\u6377\u8fde\u63a5\u548c\u53ef\u8fdb\u5316\u62d3\u6251\u7ed3\u6784\u7684\u7f16\u7801\u5668\uff0c\u7a00\u758f\u89e3\u7801\u5668\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u4fe1\u606f\u5f15\u5bfc\u7684\u5c40\u90e8\u4e8c\u503c\u5316\u6a21\u4eff\u6846\u67b6\u3002", "result": "BiVM\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8ba1\u7b97\u548c\u5b58\u50a8\u6210\u672c\u5206\u522b\u8282\u770114.3\u500d\u548c21.6\u500d\u3002", "conclusion": "BiVM\u901a\u8fc7\u4f18\u5316\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u4e8c\u503c\u5316\u89c6\u9891\u62a0\u56fe\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u3002"}}
{"id": "2507.04482", "pdf": "https://arxiv.org/pdf/2507.04482", "abs": "https://arxiv.org/abs/2507.04482", "authors": ["Kyoungmin Lee", "Jihun Park", "Jongmin Gim", "Wonhyeok Choi", "Kyumin Hwang", "Jaeyeul Kim", "Sunghoon Im"], "title": "A Training-Free Style-Personalization via Scale-wise Autoregressive Model", "categories": ["cs.CV"], "comment": "13 pages, 10 figures", "summary": "We present a training-free framework for style-personalized image generation that controls content and style information during inference using a scale-wise autoregressive model. Our method employs a three-path design--content, style, and generation--each guided by a corresponding text prompt, enabling flexible and efficient control over image semantics without any additional training. A central contribution of this work is a step-wise and attention-wise intervention analysis. Through systematic prompt and feature injection, we find that early-to-middle generation steps play a pivotal role in shaping both content and style, and that query features predominantly encode content-specific information. Guided by these insights, we introduce two targeted mechanisms: Key Stage Attention Sharing, which aligns content and style during the semantically critical steps, and Adaptive Query Sharing, which reinforces content semantics in later steps through similarity-aware query blending. Extensive experiments demonstrate that our method achieves competitive style fidelity and prompt fidelity compared to fine-tuned baselines, while offering faster inference and greater deployment flexibility.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8def\u5f84\u8bbe\u8ba1\u548c\u5e72\u9884\u5206\u6790\u5b9e\u73b0\u5185\u5bb9\u548c\u98ce\u683c\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u989d\u5916\u8bad\u7ec3\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u7075\u6d3b\u9ad8\u6548\u7684\u56fe\u50cf\u751f\u6210\u63a7\u5236\u3002", "method": "\u91c7\u7528\u4e09\u8def\u5f84\u8bbe\u8ba1\uff08\u5185\u5bb9\u3001\u98ce\u683c\u3001\u751f\u6210\uff09\uff0c\u7ed3\u5408\u5173\u952e\u9636\u6bb5\u6ce8\u610f\u529b\u5171\u4eab\u548c\u81ea\u9002\u5e94\u67e5\u8be2\u5171\u4eab\u673a\u5236\u3002", "result": "\u5728\u98ce\u683c\u4fdd\u771f\u5ea6\u548c\u63d0\u793a\u4fdd\u771f\u5ea6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63a8\u7406\u901f\u5ea6\u66f4\u5feb\uff0c\u90e8\u7f72\u66f4\u7075\u6d3b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.04559", "pdf": "https://arxiv.org/pdf/2507.04559", "abs": "https://arxiv.org/abs/2507.04559", "authors": ["Dawit Mureja Argaw", "Xian Liu", "Joon Son Chung", "Ming-Yu Liu", "Fitsum Reda"], "title": "MambaVideo for Discrete Video Tokenization with Channel-Split Quantization", "categories": ["cs.CV"], "comment": "Project website:   https://research.nvidia.com/labs/dir/mamba-tokenizer/", "summary": "Discrete video tokenization is essential for efficient autoregressive generative modeling due to the high dimensionality of video data. This work introduces a state-of-the-art discrete video tokenizer with two key contributions. First, we propose a novel Mamba-based encoder-decoder architecture that overcomes the limitations of previous sequencebased tokenizers. Second, we introduce a new quantization scheme, channel-split quantization, which significantly enhances the representational power of quantized latents while preserving the token count. Our model sets a new state-of-the-art, outperforming both causal 3D convolutionbased and Transformer-based approaches across multiple datasets. Experimental results further demonstrate its robustness as a tokenizer for autoregressive video generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u548c\u901a\u9053\u5206\u5272\u91cf\u5316\u65b9\u6848\u7684\u65b0\u578b\u79bb\u6563\u89c6\u9891\u6807\u8bb0\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6a21\u578b\u7684\u6548\u7387\u548c\u8868\u73b0\u529b\u3002", "motivation": "\u89c6\u9891\u6570\u636e\u7684\u9ad8\u7ef4\u5ea6\u7279\u6027\u4f7f\u5f97\u79bb\u6563\u6807\u8bb0\u5316\u6210\u4e3a\u81ea\u56de\u5f52\u751f\u6210\u5efa\u6a21\u7684\u5173\u952e\u9700\u6c42\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528Mamba\u67b6\u6784\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\uff0c\u5e76\u63d0\u51fa\u901a\u9053\u5206\u5272\u91cf\u5316\u65b9\u6848\u4ee5\u589e\u5f3a\u91cf\u5316\u6f5c\u5728\u8868\u793a\u7684\u80fd\u529b\u3002", "result": "\u6a21\u578b\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u57fa\u4e8e\u56e0\u679c3D\u5377\u79ef\u548cTransformer\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5f3a\u5927\u7684\u6807\u8bb0\u5316\u5de5\u5177\u3002"}}
{"id": "2507.04584", "pdf": "https://arxiv.org/pdf/2507.04584", "abs": "https://arxiv.org/abs/2507.04584", "authors": ["Xudong Liu", "Zikun Chen", "Ruowei Jiang", "Ziyi Wu", "Kejia Yin", "Han Zhao", "Parham Aarabi", "Igor Gilitschenski"], "title": "S$^2$Edit: Text-Guided Image Editing with Precise Semantic and Spatial Control", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in diffusion models have enabled high-quality generation and manipulation of images guided by texts, as well as concept learning from images. However, naive applications of existing methods to editing tasks that require fine-grained control, e.g., face editing, often lead to suboptimal solutions with identity information and high-frequency details lost during the editing process, or irrelevant image regions altered due to entangled concepts. In this work, we propose S$^2$Edit, a novel method based on a pre-trained text-to-image diffusion model that enables personalized editing with precise semantic and spatial control. We first fine-tune our model to embed the identity information into a learnable text token. During fine-tuning, we disentangle the learned identity token from attributes to be edited by enforcing an orthogonality constraint in the textual feature space. To ensure that the identity token only affects regions of interest, we apply object masks to guide the cross-attention maps. At inference time, our method performs localized editing while faithfully preserving the original identity with semantically disentangled and spatially focused identity token learned. Extensive experiments demonstrate the superiority of S$^2$Edit over state-of-the-art methods both quantitatively and qualitatively. Additionally, we showcase several compositional image editing applications of S$^2$Edit such as makeup transfer.", "AI": {"tldr": "S$^2$Edit\u662f\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u652f\u6301\u4e2a\u6027\u5316\u7f16\u8f91\uff0c\u5177\u6709\u7cbe\u786e\u7684\u8bed\u4e49\u548c\u7a7a\u95f4\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u9700\u8981\u7ec6\u7c92\u5ea6\u63a7\u5236\u7684\u7f16\u8f91\u4efb\u52a1\uff08\u5982\u4eba\u8138\u7f16\u8f91\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5bb9\u6613\u4e22\u5931\u8eab\u4efd\u4fe1\u606f\u6216\u6539\u53d8\u65e0\u5173\u533a\u57df\u3002", "method": "\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u5c06\u8eab\u4efd\u4fe1\u606f\u5d4c\u5165\u53ef\u5b66\u4e60\u7684\u6587\u672c\u6807\u8bb0\uff0c\u5e76\u5728\u6587\u672c\u7279\u5f81\u7a7a\u95f4\u4e2d\u65bd\u52a0\u6b63\u4ea4\u7ea6\u675f\u4ee5\u89e3\u8026\u8eab\u4efd\u4e0e\u5c5e\u6027\u3002\u4f7f\u7528\u5bf9\u8c61\u63a9\u7801\u5f15\u5bfc\u4ea4\u53c9\u6ce8\u610f\u529b\u56fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660eS$^2$Edit\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u5b9e\u73b0\u5982\u5316\u5986\u8f6c\u79fb\u7b49\u5e94\u7528\u3002", "conclusion": "S$^2$Edit\u5728\u4fdd\u6301\u8eab\u4efd\u4fe1\u606f\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u5c40\u90e8\u7f16\u8f91\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.04599", "pdf": "https://arxiv.org/pdf/2507.04599", "abs": "https://arxiv.org/abs/2507.04599", "authors": ["Jiahui Yang", "Yongjia Ma", "Donglin Di", "Hao Li", "Wei Chen", "Yan Xie", "Jianxun Cui", "Xun Yang", "Wangmeng Zuo"], "title": "QR-LoRA: Efficient and Disentangled Fine-tuning via QR Decomposition for Customized Generation", "categories": ["cs.CV"], "comment": "ICCV 2025, 30 pages, 26 figures", "summary": "Existing text-to-image models often rely on parameter fine-tuning techniques such as Low-Rank Adaptation (LoRA) to customize visual attributes. However, when combining multiple LoRA models for content-style fusion tasks, unstructured modifications of weight matrices often lead to undesired feature entanglement between content and style attributes. We propose QR-LoRA, a novel fine-tuning framework leveraging QR decomposition for structured parameter updates that effectively separate visual attributes. Our key insight is that the orthogonal Q matrix naturally minimizes interference between different visual features, while the upper triangular R matrix efficiently encodes attribute-specific transformations. Our approach fixes both Q and R matrices while only training an additional task-specific $\\Delta R$ matrix. This structured design reduces trainable parameters to half of conventional LoRA methods and supports effective merging of multiple adaptations without cross-contamination due to the strong disentanglement properties between $\\Delta R$ matrices. Experiments demonstrate that QR-LoRA achieves superior disentanglement in content-style fusion tasks, establishing a new paradigm for parameter-efficient, disentangled fine-tuning in generative models.", "AI": {"tldr": "\u63d0\u51faQR-LoRA\u6846\u67b6\uff0c\u901a\u8fc7QR\u5206\u89e3\u5b9e\u73b0\u7ed3\u6784\u5316\u53c2\u6570\u66f4\u65b0\uff0c\u6709\u6548\u5206\u79bb\u89c6\u89c9\u5c5e\u6027\uff0c\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u5e76\u907f\u514d\u7279\u5f81\u7ea0\u7f20\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u7ed3\u5408\u591a\u4e2aLoRA\u6a21\u578b\u65f6\uff0c\u6743\u91cd\u77e9\u9635\u7684\u975e\u7ed3\u6784\u5316\u4fee\u6539\u4f1a\u5bfc\u81f4\u5185\u5bb9\u4e0e\u98ce\u683c\u5c5e\u6027\u7684\u7279\u5f81\u7ea0\u7f20\u3002", "method": "\u5229\u7528QR\u5206\u89e3\uff0c\u56fa\u5b9aQ\u548cR\u77e9\u9635\uff0c\u4ec5\u8bad\u7ec3\u989d\u5916\u7684\u4efb\u52a1\u7279\u5b9a\u0394R\u77e9\u9635\uff0c\u51cf\u5c11\u53c2\u6570\u5e76\u589e\u5f3a\u5c5e\u6027\u5206\u79bb\u3002", "result": "QR-LoRA\u5728\u5185\u5bb9-\u98ce\u683c\u878d\u5408\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u89e3\u8026\u80fd\u529b\uff0c\u53c2\u6570\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "QR-LoRA\u4e3a\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u3001\u89e3\u8026\u7684\u5fae\u8c03\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.04631", "pdf": "https://arxiv.org/pdf/2507.04631", "abs": "https://arxiv.org/abs/2507.04631", "authors": ["Yun Wang", "Longguang Wang", "Chenghao Zhang", "Yongjian Zhang", "Zhanjie Zhang", "Ao Ma", "Chenyou Fan", "Tin Lun Lam", "Junjie Hu"], "title": "Learning Robust Stereo Matching in the Wild with Selective Mixture-of-Experts", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Recently, learning-based stereo matching networks have advanced significantly. However, they often lack robustness and struggle to achieve impressive cross-domain performance due to domain shifts and imbalanced disparity distributions among diverse datasets. Leveraging Vision Foundation Models (VFMs) can intuitively enhance the model's robustness, but integrating such a model into stereo matching cost-effectively to fully realize their robustness remains a key challenge. To address this, we propose SMoEStereo, a novel framework that adapts VFMs for stereo matching through a tailored, scene-specific fusion of Low-Rank Adaptation (LoRA) and Mixture-of-Experts (MoE) modules. SMoEStereo introduces MoE-LoRA with adaptive ranks and MoE-Adapter with adaptive kernel sizes. The former dynamically selects optimal experts within MoE to adapt varying scenes across domains, while the latter injects inductive bias into frozen VFMs to improve geometric feature extraction. Importantly, to mitigate computational overhead, we further propose a lightweight decision network that selectively activates MoE modules based on input complexity, balancing efficiency with accuracy. Extensive experiments demonstrate that our method exhibits state-of-the-art cross-domain and joint generalization across multiple benchmarks without dataset-specific adaptation. The code is available at \\textcolor{red}{https://github.com/cocowy1/SMoE-Stereo}.", "AI": {"tldr": "SMoEStereo\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LoRA\u548cMoE\u6a21\u5757\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u4e13\u5bb6\u548c\u81ea\u9002\u5e94\u6838\u5927\u5c0f\uff0c\u63d0\u5347\u4e86\u7acb\u4f53\u5339\u914d\u7684\u8de8\u57df\u6027\u80fd\uff0c\u540c\u65f6\u901a\u8fc7\u8f7b\u91cf\u7ea7\u51b3\u7b56\u7f51\u7edc\u5e73\u8861\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684\u7acb\u4f53\u5339\u914d\u7f51\u7edc\u5728\u8de8\u57df\u6027\u80fd\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u57df\u504f\u79fb\u548c\u4e0d\u5e73\u8861\u7684\u89c6\u5dee\u5206\u5e03\u3002\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\u53ef\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u4f46\u5982\u4f55\u9ad8\u6548\u6574\u5408\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faSMoEStereo\u6846\u67b6\uff0c\u7ed3\u5408LoRA\u548cMoE\u6a21\u5757\uff0c\u5305\u62ec\u52a8\u6001\u9009\u62e9\u4e13\u5bb6\u7684MoE-LoRA\u548c\u81ea\u9002\u5e94\u6838\u5927\u5c0f\u7684MoE-Adapter\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u51b3\u7b56\u7f51\u7edc\u9009\u62e9\u6027\u6fc0\u6d3b\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8de8\u57df\u548c\u8054\u5408\u6cdb\u5316\u6027\u80fd\uff0c\u65e0\u9700\u7279\u5b9a\u6570\u636e\u96c6\u9002\u914d\u3002", "conclusion": "SMoEStereo\u901a\u8fc7\u521b\u65b0\u6a21\u5757\u8bbe\u8ba1\u548c\u8f7b\u91cf\u7ea7\u51b3\u7b56\u7f51\u7edc\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7acb\u4f53\u5339\u914d\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2507.04678", "pdf": "https://arxiv.org/pdf/2507.04678", "abs": "https://arxiv.org/abs/2507.04678", "authors": ["Zhenghui Zhao", "Chen Wu", "Di Wang", "Hongruixuan Chen", "Zhuo Zheng"], "title": "ChangeBridge: Spatiotemporal Image Generation with Multimodal Controls for Remote Sensing", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in generative methods, especially diffusion models, have made great progress in remote sensing image synthesis. Despite these advancements, existing methods have not explored the simulation of future scenarios based on given scenario images. This simulation capability has wide applications for urban planning, land managementChangeBridge: Spatiotemporal Image Generation with Multimodal Controls, and beyond. In this work, we propose ChangeBridge, a conditional spatiotemporal diffusion model. Given pre-event images and conditioned on multimodal spatial controls (e.g., text prompts, instance layouts, and semantic maps), ChangeBridge can synthesize post-event images. The core idea behind ChangeBridge is to modeling the noise-to-image diffusion model, as a pre-to-post diffusion bridge. Conditioned on multimodal controls, ChangeBridge leverages a stochastic Brownian-bridge diffusion, directly modeling the spatiotemporal evolution between pre-event and post-event states. To the best of our knowledge, ChangeBridge is the first spatiotemporal generative model with multimodal controls for remote sensing. Experimental results demonstrate that ChangeBridge can simulate high-fidelity future scenarios aligned with given conditions, including event and event-driven background variations. Code will be available.", "AI": {"tldr": "ChangeBridge\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u63a7\u5236\u7684\u65f6\u7a7a\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u7ed9\u5b9a\u573a\u666f\u56fe\u50cf\u6a21\u62df\u672a\u6765\u573a\u666f\uff0c\u9002\u7528\u4e8e\u57ce\u5e02\u89c4\u5212\u7b49\u9886\u57df\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u65b9\u6cd5\u672a\u63a2\u7d22\u57fa\u4e8e\u7ed9\u5b9a\u573a\u666f\u56fe\u50cf\u6a21\u62df\u672a\u6765\u573a\u666f\u7684\u80fd\u529b\uff0c\u800c\u8fd9\u4e00\u80fd\u529b\u5728\u57ce\u5e02\u89c4\u5212\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u63d0\u51faChangeBridge\uff0c\u4e00\u79cd\u6761\u4ef6\u65f6\u7a7a\u6269\u6563\u6a21\u578b\uff0c\u5229\u7528\u591a\u6a21\u6001\u7a7a\u95f4\u63a7\u5236\uff08\u5982\u6587\u672c\u63d0\u793a\u3001\u5b9e\u4f8b\u5e03\u5c40\u548c\u8bed\u4e49\u56fe\uff09\u4ece\u4e8b\u4ef6\u524d\u56fe\u50cf\u5408\u6210\u4e8b\u4ef6\u540e\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eChangeBridge\u80fd\u6a21\u62df\u9ad8\u4fdd\u771f\u672a\u6765\u573a\u666f\uff0c\u5e76\u4e0e\u7ed9\u5b9a\u6761\u4ef6\uff08\u5982\u4e8b\u4ef6\u548c\u4e8b\u4ef6\u9a71\u52a8\u7684\u80cc\u666f\u53d8\u5316\uff09\u5bf9\u9f50\u3002", "conclusion": "ChangeBridge\u662f\u9996\u4e2a\u5177\u6709\u591a\u6a21\u6001\u63a7\u5236\u7684\u65f6\u7a7a\u751f\u6210\u6a21\u578b\uff0c\u4e3a\u9065\u611f\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u6a21\u62df\u80fd\u529b\u3002"}}
{"id": "2507.04685", "pdf": "https://arxiv.org/pdf/2507.04685", "abs": "https://arxiv.org/abs/2507.04685", "authors": ["Changsong Lei", "Yaqian Liang", "Shaofeng Wang", "Jiajia Dai", "Yong-Jin Liu"], "title": "TeethGenerator: A two-stage framework for paired pre- and post-orthodontic 3D dental data generation", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Digital orthodontics represents a prominent and critical application of computer vision technology in the medical field. So far, the labor-intensive process of collecting clinical data, particularly in acquiring paired 3D orthodontic teeth models, constitutes a crucial bottleneck for developing tooth arrangement neural networks. Although numerous general 3D shape generation methods have been proposed, most of them focus on single-object generation and are insufficient for generating anatomically structured teeth models, each comprising 24-32 segmented teeth. In this paper, we propose TeethGenerator, a novel two-stage framework designed to synthesize paired 3D teeth models pre- and post-orthodontic, aiming to facilitate the training of downstream tooth arrangement networks. Specifically, our approach consists of two key modules: (1) a teeth shape generation module that leverages a diffusion model to learn the distribution of morphological characteristics of teeth, enabling the generation of diverse post-orthodontic teeth models; and (2) a teeth style generation module that synthesizes corresponding pre-orthodontic teeth models by incorporating desired styles as conditional inputs. Extensive qualitative and quantitative experiments demonstrate that our synthetic dataset aligns closely with the distribution of real orthodontic data, and promotes tooth alignment performance significantly when combined with real data for training. The code and dataset are available at https://github.com/lcshhh/teeth_generator.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTeethGenerator\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u6b63\u7578\u524d\u540e\u7684\u914d\u5bf93D\u7259\u9f7f\u6a21\u578b\uff0c\u4ee5\u652f\u6301\u7259\u9f7f\u6392\u5217\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u3002", "motivation": "\u5f53\u524d3D\u5f62\u72b6\u751f\u6210\u65b9\u6cd5\u591a\u5173\u6ce8\u5355\u5bf9\u8c61\u751f\u6210\uff0c\u65e0\u6cd5\u6ee1\u8db3\u751f\u6210\u89e3\u5256\u7ed3\u6784\u590d\u6742\u7684\u7259\u9f7f\u6a21\u578b\u9700\u6c42\uff0c\u4e14\u4e34\u5e8a\u6570\u636e\u6536\u96c6\u8017\u65f6\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7259\u9f7f\u5f62\u72b6\u751f\u6210\u6a21\u5757\uff08\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u5b66\u4e60\u7259\u9f7f\u5f62\u6001\u5206\u5e03\uff09\u548c\u7259\u9f7f\u98ce\u683c\u751f\u6210\u6a21\u5757\uff08\u4ee5\u98ce\u683c\u4e3a\u6761\u4ef6\u751f\u6210\u6b63\u7578\u524d\u6a21\u578b\uff09\u3002", "result": "\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u5206\u5e03\u9ad8\u5ea6\u4e00\u81f4\uff0c\u7ed3\u5408\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u7259\u9f7f\u6392\u5217\u6027\u80fd\u3002", "conclusion": "TeethGenerator\u4e3a\u7259\u9f7f\u6392\u5217\u7f51\u7edc\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u4e34\u5e8a\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002"}}
{"id": "2507.04692", "pdf": "https://arxiv.org/pdf/2507.04692", "abs": "https://arxiv.org/abs/2507.04692", "authors": ["Wanchang Yu", "Qing Zhang", "Rongjia Zheng", "Wei-Shi Zheng"], "title": "Structure-Guided Diffusion Models for High-Fidelity Portrait Shadow Removal", "categories": ["cs.CV"], "comment": null, "summary": "We present a diffusion-based portrait shadow removal approach that can robustly produce high-fidelity results. Unlike previous methods, we cast shadow removal as diffusion-based inpainting. To this end, we first train a shadow-independent structure extraction network on a real-world portrait dataset with various synthetic lighting conditions, which allows to generate a shadow-independent structure map including facial details while excluding the unwanted shadow boundaries. The structure map is then used as condition to train a structure-guided inpainting diffusion model for removing shadows in a generative manner. Finally, to restore the fine-scale details (e.g., eyelashes, moles and spots) that may not be captured by the structure map, we take the gradients inside the shadow regions as guidance and train a detail restoration diffusion model to refine the shadow removal result. Extensive experiments on the benchmark datasets show that our method clearly outperforms existing methods, and is effective to avoid previously common issues such as facial identity tampering, shadow residual, color distortion, structure blurring, and loss of details. Our code is available at https://github.com/wanchang-yu/Structure-Guided-Diffusion-for-Portrait-Shadow-Removal.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u8096\u50cf\u9634\u5f71\u53bb\u9664\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u4fee\u590d\u548c\u7ec6\u8282\u6062\u590d\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u53bb\u9664\u8096\u50cf\u9634\u5f71\u65f6\u5bb9\u6613\u51fa\u73b0\u9762\u90e8\u8eab\u4efd\u7be1\u6539\u3001\u9634\u5f71\u6b8b\u7559\u3001\u989c\u8272\u5931\u771f\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u4e14\u9ad8\u4fdd\u771f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u8bad\u7ec3\u9634\u5f71\u65e0\u5173\u7684\u7ed3\u6784\u63d0\u53d6\u7f51\u7edc\u751f\u6210\u7ed3\u6784\u56fe\uff1b2. \u8bad\u7ec3\u7ed3\u6784\u5f15\u5bfc\u7684\u6269\u6563\u4fee\u590d\u6a21\u578b\u53bb\u9664\u9634\u5f71\uff1b3. \u8bad\u7ec3\u7ec6\u8282\u6062\u590d\u6269\u6563\u6a21\u578b\u4f18\u5316\u7ed3\u679c\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u907f\u514d\u4e86\u5e38\u89c1\u95ee\u9898\u5982\u8eab\u4efd\u7be1\u6539\u3001\u9634\u5f71\u6b8b\u7559\u7b49\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u7ed3\u6784\u5f15\u5bfc\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u8096\u50cf\u9634\u5f71\u53bb\u9664\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.04705", "pdf": "https://arxiv.org/pdf/2507.04705", "abs": "https://arxiv.org/abs/2507.04705", "authors": ["Yuji Wang", "Moran Li", "Xiaobin Hu", "Ran Yi", "Jiangning Zhang", "Han Feng", "Weijian Cao", "Yabiao Wang", "Chengjie Wang", "Lizhuang Ma"], "title": "Identity-Preserving Text-to-Video Generation Guided by Simple yet Effective Spatial-Temporal Decoupled Representations", "categories": ["cs.CV"], "comment": null, "summary": "Identity-preserving text-to-video (IPT2V) generation, which aims to create high-fidelity videos with consistent human identity, has become crucial for downstream applications. However, current end-to-end frameworks suffer a critical spatial-temporal trade-off: optimizing for spatially coherent layouts of key elements (e.g., character identity preservation) often compromises instruction-compliant temporal smoothness, while prioritizing dynamic realism risks disrupting the spatial coherence of visual structures. To tackle this issue, we propose a simple yet effective spatial-temporal decoupled framework that decomposes representations into spatial features for layouts and temporal features for motion dynamics. Specifically, our paper proposes a semantic prompt optimization mechanism and stage-wise decoupled generation paradigm. The former module decouples the prompt into spatial and temporal components. Aligned with the subsequent stage-wise decoupled approach, the spatial prompts guide the text-to-image (T2I) stage to generate coherent spatial features, while the temporal prompts direct the sequential image-to-video (I2V) stage to ensure motion consistency. Experimental results validate that our approach achieves excellent spatiotemporal consistency, demonstrating outstanding performance in identity preservation, text relevance, and video quality. By leveraging this simple yet robust mechanism, our algorithm secures the runner-up position in 2025 ACM MultiMedia Challenge.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7a7a\u95f4-\u65f6\u95f4\u89e3\u8026\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u63d0\u793a\u4f18\u5316\u548c\u5206\u9636\u6bb5\u751f\u6210\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u4e2d\u7684\u8eab\u4efd\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u6846\u67b6\u5728\u7a7a\u95f4\u5e03\u5c40\u548c\u65f6\u95f4\u52a8\u6001\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u52a8\u6001\u6d41\u7545\u6027\u3002", "method": "\u91c7\u7528\u7a7a\u95f4-\u65f6\u95f4\u89e3\u8026\u6846\u67b6\uff0c\u5c06\u8868\u793a\u5206\u89e3\u4e3a\u7a7a\u95f4\u7279\u5f81\uff08\u5e03\u5c40\uff09\u548c\u65f6\u95f4\u7279\u5f81\uff08\u52a8\u6001\uff09\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u63d0\u793a\u4f18\u5316\u548c\u5206\u9636\u6bb5\u751f\u6210\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u65f6\u7a7a\u4e00\u81f4\u6027\u3001\u8eab\u4efd\u4fdd\u7559\u3001\u6587\u672c\u76f8\u5173\u6027\u548c\u89c6\u9891\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u83b7\u5f972025\u5e74ACM MultiMedia Challenge\u4e9a\u519b\u3002", "conclusion": "\u8be5\u6846\u67b6\u7b80\u5355\u6709\u6548\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.04726", "pdf": "https://arxiv.org/pdf/2507.04726", "abs": "https://arxiv.org/abs/2507.04726", "authors": ["Raz Lapid", "Almog Dubin"], "title": "Losing Control: Data Poisoning Attack on Guided Diffusion via ControlNet", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-image diffusion models have achieved remarkable success in translating textual prompts into high-fidelity images. ControlNets further extend these models by allowing precise, image-based conditioning (e.g., edge maps, depth, pose), enabling fine-grained control over structure and style. However, their dependence on large, publicly scraped datasets -- and the increasing use of community-shared data for fine-tuning -- exposes them to stealthy data poisoning attacks. In this work, we introduce a novel data poisoning method that manipulates ControlNets to generate images containing specific content without any text triggers. By injecting poisoned samples -- each pairing a subtly triggered input with an NSFW target -- the model retains clean-prompt fidelity yet reliably produces NSFW outputs when the trigger is present. On large-scale, high-quality datasets, our backdoor achieves high attack success rate while remaining imperceptible in raw inputs. These results reveal a critical vulnerability in open-source ControlNets pipelines and underscore the need for robust data sanitization and defense mechanisms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9ControlNets\u7684\u65b0\u578b\u6570\u636e\u6295\u6bd2\u653b\u51fb\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u65e0\u6587\u672c\u89e6\u53d1\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u7279\u5b9a\u5185\u5bb9\uff0c\u63ed\u793a\u4e86\u5f00\u6e90ControlNets\u7ba1\u9053\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "ControlNets\u4f9d\u8d56\u516c\u5f00\u6570\u636e\u96c6\u548c\u793e\u533a\u5171\u4eab\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u5bb9\u6613\u53d7\u5230\u9690\u853d\u7684\u6570\u636e\u6295\u6bd2\u653b\u51fb\uff0c\u672c\u6587\u65e8\u5728\u63ed\u793a\u8fd9\u4e00\u6f0f\u6d1e\u3002", "method": "\u901a\u8fc7\u6ce8\u5165\u6bd2\u5316\u6837\u672c\uff08\u5c06\u8f7b\u5fae\u89e6\u53d1\u7684\u8f93\u5165\u4e0eNSFW\u76ee\u6807\u914d\u5bf9\uff09\uff0c\u4f7f\u6a21\u578b\u5728\u89e6\u53d1\u65f6\u751f\u6210NSFW\u5185\u5bb9\uff0c\u540c\u65f6\u4fdd\u6301\u5e72\u51c0\u63d0\u793a\u7684\u4fdd\u771f\u5ea6\u3002", "result": "\u5728\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u4e0a\uff0c\u653b\u51fb\u6210\u529f\u7387\u9ad8\u4e14\u89e6\u53d1\u8f93\u5165\u96be\u4ee5\u5bdf\u89c9\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f00\u6e90ControlNets\u7ba1\u9053\u7684\u5173\u952e\u6f0f\u6d1e\uff0c\u5f3a\u8c03\u4e86\u6570\u636e\u51c0\u5316\u548c\u9632\u5fa1\u673a\u5236\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.04749", "pdf": "https://arxiv.org/pdf/2507.04749", "abs": "https://arxiv.org/abs/2507.04749", "authors": ["Chengyu Wang", "Isabella Bennett", "Henry Scott", "Liang Zhang", "Mei Chen", "Hao Li", "Rui Zhao"], "title": "MatDecompSDF: High-Fidelity 3D Shape and PBR Material Decomposition from Multi-View Images", "categories": ["cs.CV", "68U05", "I.3.7; I.3.3; I.4.1"], "comment": "12 pages, 4 figures", "summary": "We present MatDecompSDF, a novel framework for recovering high-fidelity 3D shapes and decomposing their physically-based material properties from multi-view images. The core challenge of inverse rendering lies in the ill-posed disentanglement of geometry, materials, and illumination from 2D observations. Our method addresses this by jointly optimizing three neural components: a neural Signed Distance Function (SDF) to represent complex geometry, a spatially-varying neural field for predicting PBR material parameters (albedo, roughness, metallic), and an MLP-based model for capturing unknown environmental lighting. The key to our approach is a physically-based differentiable rendering layer that connects these 3D properties to the input images, allowing for end-to-end optimization. We introduce a set of carefully designed physical priors and geometric regularizations, including a material smoothness loss and an Eikonal loss, to effectively constrain the problem and achieve robust decomposition. Extensive experiments on both synthetic and real-world datasets (e.g., DTU) demonstrate that MatDecompSDF surpasses state-of-the-art methods in geometric accuracy, material fidelity, and novel view synthesis. Crucially, our method produces editable and relightable assets that can be seamlessly integrated into standard graphics pipelines, validating its practical utility for digital content creation.", "AI": {"tldr": "MatDecompSDF\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u591a\u89c6\u89d2\u56fe\u50cf\u4e2d\u6062\u590d\u9ad8\u4fdd\u771f3D\u5f62\u72b6\u5e76\u5206\u89e3\u5176\u57fa\u4e8e\u7269\u7406\u7684\u6750\u6599\u5c5e\u6027\u3002", "motivation": "\u9006\u5411\u6e32\u67d3\u7684\u6838\u5fc3\u6311\u6218\u662f\u4ece2D\u89c2\u6d4b\u4e2d\u89e3\u8026\u51e0\u4f55\u3001\u6750\u6599\u548c\u5149\u7167\u7684\u6b20\u5b9a\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u4f18\u5316\u4e09\u4e2a\u795e\u7ecf\u7ec4\u4ef6\uff1a\u795e\u7ecfSDF\u8868\u793a\u51e0\u4f55\u3001\u795e\u7ecf\u573a\u9884\u6d4bPBR\u6750\u6599\u53c2\u6570\u3001MLP\u6a21\u578b\u6355\u83b7\u73af\u5883\u5149\u7167\uff0c\u5e76\u7ed3\u5408\u7269\u7406\u53ef\u5fae\u6e32\u67d3\u5c42\u8fdb\u884c\u7aef\u5230\u7aef\u4f18\u5316\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\uff08\u5982DTU\uff09\u4e0a\uff0cMatDecompSDF\u5728\u51e0\u4f55\u7cbe\u5ea6\u3001\u6750\u6599\u4fdd\u771f\u5ea6\u548c\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u9762\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u751f\u6210\u53ef\u7f16\u8f91\u548c\u53ef\u91cd\u7167\u660e\u7684\u8d44\u4ea7\uff0c\u9002\u7528\u4e8e\u6570\u5b57\u5185\u5bb9\u521b\u4f5c\u3002"}}
{"id": "2507.04765", "pdf": "https://arxiv.org/pdf/2507.04765", "abs": "https://arxiv.org/abs/2507.04765", "authors": ["Weilin Lai", "Tie Xu", "Hu Wang"], "title": "GraphBrep: Learning B-Rep in Graph Structure for Efficient CAD Generation", "categories": ["cs.CV"], "comment": null, "summary": "Direct B-Rep generation is increasingly important in CAD workflows, eliminating costly modeling sequence data and supporting complex features. A key challenge is modeling joint distribution of the misaligned geometry and topology. Existing methods tend to implicitly embed topology into the geometric features of edges. Although this integration ensures feature alignment, it also causes edge geometry to carry more redundant structural information compared to the original B-Rep, leading to significantly higher computational cost. To reduce redundancy, we propose GraphBrep, a B-Rep generation model that explicitly represents and learns compact topology. Following the original structure of B-Rep, we construct an undirected weighted graph to represent surface topology. A graph diffusion model is employed to learn topology conditioned on surface features, serving as the basis for determining connectivity between primitive surfaces. The explicit representation ensures a compact data structure, effectively reducing computational cost during both training and inference. Experiments on two large-scale unconditional datasets and one category-conditional dataset demonstrate the proposed method significantly reduces training and inference times (up to 31.3% and 56.3% for given datasets, respectively) while maintaining high-quality CAD generation compared with SOTA.", "AI": {"tldr": "GraphBrep\u63d0\u51fa\u4e86\u4e00\u79cd\u663e\u5f0f\u8868\u793a\u548c\u5b66\u4e60\u7d27\u51d1\u62d3\u6251\u7684B-Rep\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u56fe\u6269\u6563\u6a21\u578b\u51cf\u5c11\u5197\u4f59\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u76f4\u63a5B-Rep\u751f\u6210\u5728CAD\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5c06\u62d3\u6251\u9690\u5f0f\u5d4c\u5165\u51e0\u4f55\u7279\u5f81\uff0c\u5bfc\u81f4\u5197\u4f59\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u6784\u5efa\u65e0\u5411\u52a0\u6743\u56fe\u8868\u793a\u8868\u9762\u62d3\u6251\uff0c\u5229\u7528\u56fe\u6269\u6563\u6a21\u578b\u5b66\u4e60\u62d3\u6251\u6761\u4ef6\uff0c\u663e\u5f0f\u8868\u793a\u786e\u4fdd\u6570\u636e\u7ed3\u6784\u7d27\u51d1\u3002", "result": "\u5728\u4e24\u4e2a\u5927\u89c4\u6a21\u65e0\u6761\u4ef6\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u7c7b\u522b\u6761\u4ef6\u6570\u636e\u96c6\u4e0a\uff0c\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u5206\u522b\u51cf\u5c1131.3%\u548c56.3%\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u751f\u6210\u3002", "conclusion": "GraphBrep\u901a\u8fc7\u663e\u5f0f\u62d3\u6251\u8868\u793a\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4e3aCAD\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.04840", "pdf": "https://arxiv.org/pdf/2507.04840", "abs": "https://arxiv.org/abs/2507.04840", "authors": ["Sourav Ghosh", "Chayan Maitra", "Rajat K. De"], "title": "CMET: Clustering guided METric for quantifying embedding quality", "categories": ["cs.CV"], "comment": "22 pages, 19 figures", "summary": "Due to rapid advancements in technology, datasets are available from various domains. In order to carry out more relevant and appropriate analysis, it is often necessary to project the dataset into a higher or lower dimensional space based on requirement. Projecting the data in a higher-dimensional space helps in unfolding intricate patterns, enhancing the performance of the underlying models. On the other hand, dimensionality reduction is helpful in denoising data while capturing maximal information, as well as reducing execution time and memory.In this context, it is not always statistically evident whether the transformed embedding retains the local and global structure of the original data. Most of the existing metrics that are used for comparing the local and global shape of the embedding against the original one are highly expensive in terms of time and space complexity. In order to address this issue, the objective of this study is to formulate a novel metric, called Clustering guided METric (CMET), for quantifying embedding quality. It is effective to serve the purpose of quantitative comparison between an embedding and the original data. CMET consists of two scores, viz., CMET_L and CMET_G, that measure the degree of local and global shape preservation capability, respectively. The efficacy of CMET has been demonstrated on a wide variety of datasets, including four synthetic, two biological, and two image datasets. Results reflect the favorable performance of CMET against the state-of-the-art methods. Capability to handle both small and large data, low algorithmic complexity, better and stable performance across all kinds of data, and different choices of hyper-parameters feature CMET as a reliable metric.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCMET\u7684\u65b0\u5ea6\u91cf\u6807\u51c6\uff0c\u7528\u4e8e\u91cf\u5316\u5d4c\u5165\u8d28\u91cf\uff0c\u901a\u8fc7\u5c40\u90e8\u548c\u5168\u5c40\u8bc4\u5206\uff08CMET_L\u548cCMET_G\uff09\u8bc4\u4f30\u6570\u636e\u5d4c\u5165\u7684\u4fdd\u5f62\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5ea6\u91cf\u6807\u51c6\u5728\u65f6\u95f4\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\u4e0a\u8f83\u9ad8\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u6570\u636e\u5d4c\u5165\u5c40\u90e8\u548c\u5168\u5c40\u7ed3\u6784\u7684\u7edf\u8ba1\u9a8c\u8bc1\u3002", "method": "\u5f00\u53d1\u4e86CMET\u5ea6\u91cf\u6807\u51c6\uff0c\u5305\u542bCMET_L\u548cCMET_G\u4e24\u4e2a\u8bc4\u5206\uff0c\u5206\u522b\u8861\u91cf\u5c40\u90e8\u548c\u5168\u5c40\u5f62\u72b6\u4fdd\u7559\u80fd\u529b\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86CMET\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5305\u62ec\u5408\u6210\u3001\u751f\u7269\u548c\u56fe\u50cf\u6570\u636e\u3002", "conclusion": "CMET\u56e0\u5176\u9ad8\u6548\u6027\u3001\u7a33\u5b9a\u6027\u548c\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u6210\u4e3a\u4e00\u79cd\u53ef\u9760\u7684\u5d4c\u5165\u8d28\u91cf\u5ea6\u91cf\u6807\u51c6\u3002"}}
{"id": "2507.04909", "pdf": "https://arxiv.org/pdf/2507.04909", "abs": "https://arxiv.org/abs/2507.04909", "authors": ["Yuxuan Cai", "Jiangning Zhang", "Zhenye Gan", "Qingdong He", "Xiaobin Hu", "Junwei Zhu", "Yabiao Wang", "Chengjie Wang", "Zhucun Xue", "Xinwei He", "Xiang Bai"], "title": "HV-MMBench: Benchmarking MLLMs for Human-Centric Video Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "Under review", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant advances in visual understanding tasks involving both images and videos. However, their capacity to comprehend human-centric video data remains underexplored, primarily due to the absence of comprehensive and high-quality evaluation benchmarks. Existing human-centric benchmarks predominantly emphasize video generation quality and action recognition, while overlooking essential perceptual and cognitive abilities required in human-centered scenarios. Furthermore, they are often limited by single-question paradigms and overly simplistic evaluation metrics. To address above limitations, we propose a modern HV-MMBench, a rigorously curated benchmark designed to provide a more holistic evaluation of MLLMs in human-centric video understanding. Compared to existing human-centric video benchmarks, our work offers the following key features: (1) Diverse evaluation dimensions: HV-MMBench encompasses 15 tasks, ranging from basic attribute perception (e.g., age estimation, emotion recognition) to advanced cognitive reasoning (e.g., social relationship prediction, intention prediction), enabling comprehensive assessment of model capabilities; (2) Varied data types: The benchmark includes multiple-choice, fill-in-blank, true/false, and open-ended question formats, combined with diverse evaluation metrics, to more accurately and robustly reflect model performance; (3) Multi-domain video coverage: The benchmark spans 50 distinct visual scenarios, enabling comprehensive evaluation across fine-grained scene variations; (4) Temporal coverage: The benchmark covers videos from short-term (10 seconds) to long-term (up to 30min) durations, supporting systematic analysis of models temporal reasoning abilities across diverse contextual lengths.", "AI": {"tldr": "HV-MMBench\u662f\u4e00\u4e2a\u9488\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u4eba\u7c7b\u4e2d\u5fc3\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5168\u9762\u8bc4\u4f30\u57fa\u51c6\uff0c\u6db5\u76d615\u4e2a\u4efb\u52a1\u3001\u591a\u79cd\u6570\u636e\u683c\u5f0f\u548c\u591a\u9886\u57df\u89c6\u9891\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u7c7b\u4e2d\u5fc3\u89c6\u9891\u57fa\u51c6\u8fc7\u4e8e\u5173\u6ce8\u751f\u6210\u8d28\u91cf\u548c\u52a8\u4f5c\u8bc6\u522b\uff0c\u7f3a\u4e4f\u5bf9\u611f\u77e5\u548c\u8ba4\u77e5\u80fd\u529b\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u4e14\u8bc4\u4ef7\u6307\u6807\u5355\u4e00\u3002", "method": "\u63d0\u51faHV-MMBench\uff0c\u5305\u542b\u591a\u6837\u5316\u7684\u4efb\u52a1\uff08\u5982\u5c5e\u6027\u611f\u77e5\u3001\u8ba4\u77e5\u63a8\u7406\uff09\u3001\u591a\u79cd\u6570\u636e\u683c\u5f0f\uff08\u9009\u62e9\u9898\u3001\u586b\u7a7a\u9898\u7b49\uff09\u548c\u591a\u9886\u57df\u89c6\u9891\u573a\u666f\u3002", "result": "HV-MMBench\u652f\u6301\u5bf9MLLMs\u5728\u4eba\u7c7b\u4e2d\u5fc3\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u6db5\u76d6\u77ed\u671f\u5230\u957f\u671f\u89c6\u9891\uff0c\u5e76\u5f15\u5165\u591a\u6837\u5316\u7684\u8bc4\u4ef7\u6307\u6807\u3002", "conclusion": "HV-MMBench\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u4e0d\u8db3\uff0c\u4e3aMLLMs\u5728\u4eba\u7c7b\u4e2d\u5fc3\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u5de5\u5177\u3002"}}
{"id": "2507.04930", "pdf": "https://arxiv.org/pdf/2507.04930", "abs": "https://arxiv.org/abs/2507.04930", "authors": ["Paula Harder", "Luca Schmidt", "Francis Pelletier", "Nicole Ludwig", "Matthew Chantry", "Christian Lessig", "Alex Hernandez-Garcia", "David Rolnick"], "title": "RainShift: A Benchmark for Precipitation Downscaling Across Geographies", "categories": ["cs.CV"], "comment": null, "summary": "Earth System Models (ESM) are our main tool for projecting the impacts of climate change. However, running these models at sufficient resolution for local-scale risk-assessments is not computationally feasible. Deep learning-based super-resolution models offer a promising solution to downscale ESM outputs to higher resolutions by learning from data. Yet, due to regional variations in climatic processes, these models typically require retraining for each geographical area-demanding high-resolution observational data, which is unevenly available across the globe. This highlights the need to assess how well these models generalize across geographic regions. To address this, we introduce RainShift, a dataset and benchmark for evaluating downscaling under geographic distribution shifts. We evaluate state-of-the-art downscaling approaches including GANs and diffusion models in generalizing across data gaps between the Global North and Global South. Our findings reveal substantial performance drops in out-of-distribution regions, depending on model and geographic area. While expanding the training domain generally improves generalization, it is insufficient to overcome shifts between geographically distinct regions. We show that addressing these shifts through, for example, data alignment can improve spatial generalization. Our work advances the global applicability of downscaling methods and represents a step toward reducing inequities in access to high-resolution climate information.", "AI": {"tldr": "RainShift\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u4e86\u6df1\u5ea6\u5b66\u4e60\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u5728\u5730\u7406\u5206\u5e03\u53d8\u5316\u4e0b\u7684\u964d\u5c3a\u5ea6\u6027\u80fd\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u5206\u5e03\u5916\u533a\u57df\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u6570\u636e\u5bf9\u9f50\u7b49\u65b9\u6cd5\u53ef\u6539\u5584\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5730\u7403\u7cfb\u7edf\u6a21\u578b\uff08ESM\uff09\u5728\u5c40\u90e8\u5c3a\u5ea6\u98ce\u9669\u8bc4\u4f30\u4e2d\u9ad8\u5206\u8fa8\u7387\u8ba1\u7b97\u4e0d\u53ef\u884c\u7684\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u6df1\u5ea6\u5b66\u4e60\u964d\u5c3a\u5ea6\u6a21\u578b\u5728\u5730\u7406\u533a\u57df\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f15\u5165RainShift\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u8bc4\u4f30GAN\u548c\u6269\u6563\u6a21\u578b\u7b49\u964d\u5c3a\u5ea6\u65b9\u6cd5\u5728\u5357\u5317\u534a\u7403\u6570\u636e\u5dee\u5f02\u4e0b\u7684\u6cdb\u5316\u8868\u73b0\u3002", "result": "\u6a21\u578b\u5728\u5206\u5e03\u5916\u533a\u57df\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u6269\u5c55\u8bad\u7ec3\u57df\u53ef\u90e8\u5206\u6539\u5584\u4f46\u4e0d\u8db3\u4ee5\u514b\u670d\u5730\u7406\u5dee\u5f02\uff0c\u6570\u636e\u5bf9\u9f50\u7b49\u65b9\u6cd5\u80fd\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u63d0\u5347\u4e86\u964d\u5c3a\u5ea6\u65b9\u6cd5\u7684\u5168\u7403\u9002\u7528\u6027\uff0c\u4e3a\u51cf\u5c11\u9ad8\u5206\u8fa8\u7387\u6c14\u5019\u4fe1\u606f\u83b7\u53d6\u7684\u4e0d\u5e73\u7b49\u8fc8\u51fa\u4e00\u6b65\u3002"}}
{"id": "2507.04946", "pdf": "https://arxiv.org/pdf/2507.04946", "abs": "https://arxiv.org/abs/2507.04946", "authors": ["Jianjiang Yang", "Ziyan Huang"], "title": "Taming the Tri-Space Tension: ARC-Guided Hallucination Modeling and Control for Text-to-Image Generation", "categories": ["cs.CV", "cs.CL"], "comment": "12 pages, 6 figures, 4 tables", "summary": "Despite remarkable progress in image quality and prompt fidelity, text-to-image (T2I) diffusion models continue to exhibit persistent \"hallucinations\", where generated content subtly or significantly diverges from the intended prompt semantics. While often regarded as unpredictable artifacts, we argue that these failures reflect deeper, structured misalignments within the generative process. In this work, we propose a cognitively inspired perspective that reinterprets hallucinations as trajectory drift within a latent alignment space. Empirical observations reveal that generation unfolds within a multiaxial cognitive tension field, where the model must continuously negotiate competing demands across three key critical axes: semantic coherence, structural alignment, and knowledge grounding. We then formalize this three-axis space as the \\textbf{Hallucination Tri-Space} and introduce the Alignment Risk Code (ARC): a dynamic vector representation that quantifies real-time alignment tension during generation. The magnitude of ARC captures overall misalignment, its direction identifies the dominant failure axis, and its imbalance reflects tension asymmetry. Based on this formulation, we develop the TensionModulator (TM-ARC): a lightweight controller that operates entirely in latent space. TM-ARC monitors ARC signals and applies targeted, axis-specific interventions during the sampling process. Extensive experiments on standard T2I benchmarks demonstrate that our approach significantly reduces hallucination without compromising image quality or diversity. This framework offers a unified and interpretable approach for understanding and mitigating generative failures in diffusion-based T2I systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba4\u77e5\u542f\u53d1\u7684\u89c6\u89d2\uff0c\u5c06\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6269\u6563\u6a21\u578b\u4e2d\u7684\u201c\u5e7b\u89c9\u201d\u91cd\u65b0\u89e3\u91ca\u4e3a\u6f5c\u5728\u5bf9\u9f50\u7a7a\u95f4\u4e2d\u7684\u8f68\u8ff9\u6f02\u79fb\uff0c\u5e76\u5f15\u5165\u4e86\u5bf9\u9f50\u98ce\u9669\u7801\uff08ARC\uff09\u548cTensionModulator\uff08TM-ARC\uff09\u6765\u91cf\u5316\u548c\u5e72\u9884\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1T2I\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u63d0\u793a\u4fdd\u771f\u5ea6\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u5b58\u5728\u201c\u5e7b\u89c9\u201d\u95ee\u9898\uff0c\u5373\u751f\u6210\u5185\u5bb9\u4e0e\u63d0\u793a\u8bed\u4e49\u4e0d\u4e00\u81f4\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e9b\u5931\u8d25\u53cd\u6620\u4e86\u751f\u6210\u8fc7\u7a0b\u4e2d\u66f4\u6df1\u5c42\u6b21\u7684\u7ed3\u6784\u6027\u9519\u4f4d\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u89c2\u5bdf\uff0c\u4f5c\u8005\u5c06\u751f\u6210\u8fc7\u7a0b\u63cf\u8ff0\u4e3a\u591a\u8f74\u8ba4\u77e5\u5f20\u529b\u573a\u4e2d\u7684\u52a8\u6001\u5e73\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u201c\u5e7b\u89c9\u4e09\u7a7a\u95f4\u201d\u6982\u5ff5\u548cARC\u52a8\u6001\u5411\u91cf\u8868\u793a\u3002\u57fa\u4e8e\u6b64\uff0c\u5f00\u53d1\u4e86TM-ARC\u63a7\u5236\u5668\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u5e72\u9884\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTM-ARC\u663e\u8457\u51cf\u5c11\u4e86\u5e7b\u89c9\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u56fe\u50cf\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7406\u89e3\u548c\u7f13\u89e3T2I\u6269\u6563\u6a21\u578b\u4e2d\u7684\u751f\u6210\u5931\u8d25\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.04947", "pdf": "https://arxiv.org/pdf/2507.04947", "abs": "https://arxiv.org/abs/2507.04947", "authors": ["Yecheng Wu", "Junyu Chen", "Zhuoyang Zhang", "Enze Xie", "Jincheng Yu", "Junsong Chen", "Jinyi Hu", "Yao Lu", "Song Han", "Han Cai"], "title": "DC-AR: Efficient Masked Autoregressive Image Generation with Deep Compression Hybrid Tokenizer", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV 2025", "summary": "We introduce DC-AR, a novel masked autoregressive (AR) text-to-image generation framework that delivers superior image generation quality with exceptional computational efficiency. Due to the tokenizers' limitations, prior masked AR models have lagged behind diffusion models in terms of quality or efficiency. We overcome this limitation by introducing DC-HT - a deep compression hybrid tokenizer for AR models that achieves a 32x spatial compression ratio while maintaining high reconstruction fidelity and cross-resolution generalization ability. Building upon DC-HT, we extend MaskGIT and create a new hybrid masked autoregressive image generation framework that first produces the structural elements through discrete tokens and then applies refinements via residual tokens. DC-AR achieves state-of-the-art results with a gFID of 5.49 on MJHQ-30K and an overall score of 0.69 on GenEval, while offering 1.5-7.9x higher throughput and 2.0-3.5x lower latency compared to prior leading diffusion and autoregressive models.", "AI": {"tldr": "DC-AR\u662f\u4e00\u79cd\u65b0\u578b\u7684\u63a9\u7801\u81ea\u56de\u5f52\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7DC-HT\u6df1\u5ea6\u538b\u7f29\u6df7\u5408\u5206\u8bcd\u5668\u5b9e\u73b0\u9ad8\u6548\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u63a9\u7801\u81ea\u56de\u5f52\u6a21\u578b\u56e0\u5206\u8bcd\u5668\u9650\u5236\u5728\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u843d\u540e\u4e8e\u6269\u6563\u6a21\u578b\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165DC-HT\u5206\u8bcd\u5668\u5b9e\u73b032\u500d\u7a7a\u95f4\u538b\u7f29\uff0c\u5e76\u57fa\u4e8eMaskGIT\u6784\u5efa\u6df7\u5408\u63a9\u7801\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u5148\u901a\u8fc7\u79bb\u6563\u4ee4\u724c\u751f\u6210\u7ed3\u6784\u5143\u7d20\uff0c\u518d\u901a\u8fc7\u6b8b\u5dee\u4ee4\u724c\u8fdb\u884c\u7ec6\u5316\u3002", "result": "\u5728MJHQ-30K\u4e0agFID\u4e3a5.49\uff0cGenEval\u5f97\u5206\u4e3a0.69\uff0c\u541e\u5410\u91cf\u63d0\u9ad81.5-7.9\u500d\uff0c\u5ef6\u8fdf\u964d\u4f4e2.0-3.5\u500d\u3002", "conclusion": "DC-AR\u5728\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u8fbe\u5230\u9886\u5148\u6c34\u5e73\u3002"}}
{"id": "2507.04961", "pdf": "https://arxiv.org/pdf/2507.04961", "abs": "https://arxiv.org/abs/2507.04961", "authors": ["Minghao Wen", "Shengjie Wu", "Kangkan Wang", "Dong Liang"], "title": "InterGSEdit: Interactive 3D Gaussian Splatting Editing with 3D Geometry-Consistent Attention Prior", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting based 3D editing has demonstrated impressive performance in recent years. However, the multi-view editing often exhibits significant local inconsistency, especially in areas of non-rigid deformation, which lead to local artifacts, texture blurring, or semantic variations in edited 3D scenes. We also found that the existing editing methods, which rely entirely on text prompts make the editing process a \"one-shot deal\", making it difficult for users to control the editing degree flexibly. In response to these challenges, we present InterGSEdit, a novel framework for high-quality 3DGS editing via interactively selecting key views with users' preferences. We propose a CLIP-based Semantic Consistency Selection (CSCS) strategy to adaptively screen a group of semantically consistent reference views for each user-selected key view. Then, the cross-attention maps derived from the reference views are used in a weighted Gaussian Splatting unprojection to construct the 3D Geometry-Consistent Attention Prior ($GAP^{3D}$). We project $GAP^{3D}$ to obtain 3D-constrained attention, which are fused with 2D cross-attention via Attention Fusion Network (AFN). AFN employs an adaptive attention strategy that prioritizes 3D-constrained attention for geometric consistency during early inference, and gradually prioritizes 2D cross-attention maps in diffusion for fine-grained features during the later inference. Extensive experiments demonstrate that InterGSEdit achieves state-of-the-art performance, delivering consistent, high-fidelity 3DGS editing with improved user experience.", "AI": {"tldr": "InterGSEdit\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ea4\u4e92\u5f0f\u5173\u952e\u89c6\u56fe\u9009\u62e9\u76843D\u9ad8\u65af\u6cfc\u6e85\u7f16\u8f91\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u56fe\u7f16\u8f91\u4e2d\u7684\u5c40\u90e8\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7CLIP\u8bed\u4e49\u4e00\u81f4\u6027\u9009\u62e9\u548c\u6ce8\u610f\u529b\u878d\u5408\u7f51\u7edc\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u76843D\u7f16\u8f91\u3002", "motivation": "\u73b0\u67093D\u9ad8\u65af\u6cfc\u6e85\u7f16\u8f91\u65b9\u6cd5\u5728\u591a\u89c6\u56fe\u7f16\u8f91\u4e2d\u5b58\u5728\u5c40\u90e8\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e14\u4f9d\u8d56\u6587\u672c\u63d0\u793a\u7684\u7f16\u8f91\u65b9\u5f0f\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51faCLIP\u8bed\u4e49\u4e00\u81f4\u6027\u9009\u62e9\u7b56\u7565\u7b5b\u9009\u53c2\u8003\u89c6\u56fe\uff0c\u6784\u5efa3D\u51e0\u4f55\u4e00\u81f4\u6027\u6ce8\u610f\u529b\u5148\u9a8c\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u878d\u5408\u7f51\u7edc\u7ed3\u54082D\u548c3D\u6ce8\u610f\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cInterGSEdit\u57283D\u9ad8\u65af\u6cfc\u6e85\u7f16\u8f91\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u4e14\u4e00\u81f4\u7684\u7f16\u8f91\u6548\u679c\u3002", "conclusion": "InterGSEdit\u901a\u8fc7\u4ea4\u4e92\u5f0f\u9009\u62e9\u548c\u6ce8\u610f\u529b\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u7f16\u8f91\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2507.04984", "pdf": "https://arxiv.org/pdf/2507.04984", "abs": "https://arxiv.org/abs/2507.04984", "authors": ["Zonglin Lyu", "Chen Chen"], "title": "TLB-VFI: Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation", "categories": ["cs.CV"], "comment": null, "summary": "Video Frame Interpolation (VFI) aims to predict the intermediate frame $I_n$ (we use n to denote time in videos to avoid notation overload with the timestep $t$ in diffusion models) based on two consecutive neighboring frames $I_0$ and $I_1$. Recent approaches apply diffusion models (both image-based and video-based) in this task and achieve strong performance. However, image-based diffusion models are unable to extract temporal information and are relatively inefficient compared to non-diffusion methods. Video-based diffusion models can extract temporal information, but they are too large in terms of training scale, model size, and inference time. To mitigate the above issues, we propose Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI), an efficient video-based diffusion model. By extracting rich temporal information from video inputs through our proposed 3D-wavelet gating and temporal-aware autoencoder, our method achieves 20% improvement in FID on the most challenging datasets over recent SOTA of image-based diffusion models. Meanwhile, due to the existence of rich temporal information, our method achieves strong performance while having 3times fewer parameters. Such a parameter reduction results in 2.3x speed up. By incorporating optical flow guidance, our method requires 9000x less training data and achieves over 20x fewer parameters than video-based diffusion models. Codes and results are available at our project page: https://zonglinl.github.io/tlbvfi_page.", "AI": {"tldr": "TLB-VFI\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u57fa\u4e8e\u89c6\u9891\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc73D\u5c0f\u6ce2\u95e8\u63a7\u548c\u65f6\u95f4\u611f\u77e5\u81ea\u7f16\u7801\u5668\u63d0\u53d6\u65f6\u95f4\u4fe1\u606f\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u56fe\u50cf\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u89c6\u9891\u5e27\u63d2\u503c\u4efb\u52a1\u4e2d\u7684\u6548\u7387\u548c\u4fe1\u606f\u63d0\u53d6\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u63d0\u51faTemporal-Aware Latent Brownian Bridge Diffusion\uff0c\u7ed3\u54083D-wavelet gating\u548ctemporal-aware autoencoder\u63d0\u53d6\u65f6\u95f4\u4fe1\u606f\uff0c\u5e76\u5f15\u5165\u5149\u6d41\u6307\u5bfc\u3002", "result": "\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0aFID\u63d0\u534720%\uff0c\u53c2\u6570\u51cf\u5c113\u500d\uff0c\u901f\u5ea6\u63d0\u53472.3\u500d\uff0c\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u51cf\u5c119000\u500d\u3002", "conclusion": "TLB-VFI\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u89c6\u9891\u5e27\u63d2\u503c\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.05029", "pdf": "https://arxiv.org/pdf/2507.05029", "abs": "https://arxiv.org/abs/2507.05029", "authors": ["Ricardo Cardoso", "Plinio Moreno"], "title": "Estimating Object Physical Properties from RGB-D Vision and Depth Robot Sensors Using Deep Learning", "categories": ["cs.CV"], "comment": null, "summary": "Inertial mass plays a crucial role in robotic applications such as object grasping, manipulation, and simulation, providing a strong prior for planning and control. Accurately estimating an object's mass before interaction can significantly enhance the performance of various robotic tasks. However, mass estimation using only vision sensors is a relatively underexplored area. This paper proposes a novel approach combining sparse point-cloud data from depth images with RGB images to estimate the mass of objects. We evaluate a range of point-cloud processing architectures, alongside RGB-only methods. To overcome the limited availability of training data, we create a synthetic dataset using ShapeNetSem 3D models, simulating RGBD images via a Kinect camera. This synthetic data is used to train an image generation model for estimating dense depth maps, which we then use to augment an existing dataset of images paired with mass values. Our approach significantly outperforms existing benchmarks across all evaluated metrics. The data generation (https://github.com/RavineWindteer/ShapenetSem-to-RGBD) as well as the training of the depth estimator (https://github.com/RavineWindteer/GLPDepth-Edited) and the mass estimator (https://github.com/RavineWindteer/Depth-mass-estimator) are available online.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7a00\u758f\u70b9\u4e91\u6570\u636e\u548cRGB\u56fe\u50cf\u6765\u4f30\u8ba1\u7269\u4f53\u8d28\u91cf\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\uff0c\u51c6\u786e\u4f30\u8ba1\u7269\u4f53\u8d28\u91cf\u5bf9\u4efb\u52a1\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u4ec5\u4f9d\u8d56\u89c6\u89c9\u4f20\u611f\u5668\u7684\u8d28\u91cf\u4f30\u8ba1\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u4f7f\u7528ShapeNetSem 3D\u6a21\u578b\u751f\u6210\u5408\u6210RGBD\u6570\u636e\uff0c\u8bad\u7ec3\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4ee5\u4f30\u8ba1\u5bc6\u96c6\u6df1\u5ea6\u56fe\uff0c\u5e76\u7ed3\u5408\u73b0\u6709\u6570\u636e\u96c6\u8bad\u7ec3\u8d28\u91cf\u4f30\u8ba1\u5668\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002", "conclusion": "\u7ed3\u5408\u70b9\u4e91\u548cRGB\u6570\u636e\u7684\u65b9\u6cd5\u5728\u7269\u4f53\u8d28\u91cf\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u5f00\u6e90\u4e86\u6570\u636e\u548c\u6a21\u578b\u3002"}}
{"id": "2507.05063", "pdf": "https://arxiv.org/pdf/2507.05063", "abs": "https://arxiv.org/abs/2507.05063", "authors": ["Jan Carreras Boada", "Rao Muhammad Umer", "Carsten Marr"], "title": "AI-Driven Cytomorphology Image Synthesis for Medical Diagnostics", "categories": ["cs.CV", "cs.CL", "cs.LG", "I.2.10; I.4.9; J.3"], "comment": "8 pages, 6 figures, 2 tables. Final Degree Project (TFG) submitted at   ESCI-UPF and conducted at Helmholtz Munich", "summary": "Biomedical datasets often contain a large sample imbalance and are subject to strict privacy constraints, which together hinder the development of accurate machine learning models. One potential solution is to generate synthetic images, as this can improve data availability while preserving patient privacy. However, it remains difficult to generate synthetic images of sufficient quality for training robust classifiers. In this work, we focus on the classification of single white blood cells, a key component in the diagnosis of hematological diseases such as acute myeloid leukemia (AML), a severe blood cancer. We demonstrate how synthetic images generated with a fine-tuned stable diffusion model using LoRA weights when guided by real few-shot samples of the target white blood cell classes, can enhance classifier performance for limited data. When training a ResNet classifier, accuracy increased from 27.3\\% to 78.4\\% (+51.1\\%) by adding 5000 synthetic images per class to a small and highly imbalanced real dataset. For a CLIP-based classifier, the accuracy improved from 61.8\\% to 76.8\\% (+15.0\\%). The synthetic images are highly similar to real images, and they can help overcome dataset limitations, enhancing model generalization. Our results establish synthetic images as a tool in biomedical research, improving machine learning models, and facilitating medical diagnosis and research.", "AI": {"tldr": "\u4f7f\u7528\u5fae\u8c03\u7684\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u751f\u6210\u5408\u6210\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347\u5355\u6838\u767d\u7ec6\u80de\u5206\u7c7b\u5668\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u751f\u7269\u533b\u5b66\u6570\u636e\u4e0d\u5e73\u8861\u548c\u9690\u79c1\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u6570\u636e\u96c6\u901a\u5e38\u5b58\u5728\u6837\u672c\u4e0d\u5e73\u8861\u548c\u9690\u79c1\u9650\u5236\uff0c\u963b\u788d\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5f00\u53d1\u3002\u5408\u6210\u56fe\u50cf\u53ef\u4ee5\u6539\u5584\u6570\u636e\u53ef\u7528\u6027\u5e76\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u901a\u8fc7\u5fae\u8c03\u7684\u7a33\u5b9a\u6269\u6563\u6a21\u578b\uff08\u4f7f\u7528LoRA\u6743\u91cd\uff09\u751f\u6210\u5408\u6210\u56fe\u50cf\uff0c\u5e76\u7ed3\u5408\u5c11\u91cf\u771f\u5b9e\u6837\u672c\u6307\u5bfc\uff0c\u7528\u4e8e\u8bad\u7ec3ResNet\u548cCLIP\u5206\u7c7b\u5668\u3002", "result": "\u6dfb\u52a05000\u5f20\u5408\u6210\u56fe\u50cf\u540e\uff0cResNet\u5206\u7c7b\u5668\u51c6\u786e\u7387\u4ece27.3%\u63d0\u5347\u81f378.4%\uff0cCLIP\u5206\u7c7b\u5668\u4ece61.8%\u63d0\u5347\u81f376.8%\u3002\u5408\u6210\u56fe\u50cf\u4e0e\u771f\u5b9e\u56fe\u50cf\u9ad8\u5ea6\u76f8\u4f3c\u3002", "conclusion": "\u5408\u6210\u56fe\u50cf\u662f\u751f\u7269\u533b\u5b66\u7814\u7a76\u4e2d\u514b\u670d\u6570\u636e\u9650\u5236\u3001\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u6709\u6548\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u533b\u5b66\u8bca\u65ad\u548c\u7814\u7a76\u3002"}}
{"id": "2507.05068", "pdf": "https://arxiv.org/pdf/2507.05068", "abs": "https://arxiv.org/abs/2507.05068", "authors": ["Hongyao Yu", "Yixiang Qiu", "Yiheng Yang", "Hao Fang", "Tianqu Zhuang", "Jiaxin Hong", "Bin Chen", "Hao Wu", "Shu-Tao Xia"], "title": "ICAS: Detecting Training Data from Autoregressive Image Generative Models", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": "ACM MM 2025", "summary": "Autoregressive image generation has witnessed rapid advancements, with prominent models such as scale-wise visual auto-regression pushing the boundaries of visual synthesis. However, these developments also raise significant concerns regarding data privacy and copyright. In response, training data detection has emerged as a critical task for identifying unauthorized data usage in model training. To better understand the vulnerability of autoregressive image generative models to such detection, we conduct the first study applying membership inference to this domain. Our approach comprises two key components: implicit classification and an adaptive score aggregation strategy. First, we compute the implicit token-wise classification score within the query image. Then we propose an adaptive score aggregation strategy to acquire a final score, which places greater emphasis on the tokens with lower scores. A higher final score indicates that the sample is more likely to be involved in the training set. To validate the effectiveness of our method, we adapt existing detection algorithms originally designed for LLMs to visual autoregressive models. Extensive experiments demonstrate the superiority of our method in both class-conditional and text-to-image scenarios. Moreover, our approach exhibits strong robustness and generalization under various data transformations. Furthermore, sufficient experiments suggest two novel key findings: (1) A linear scaling law on membership inference, exposing the vulnerability of large foundation models. (2) Training data from scale-wise visual autoregressive models is easier to detect than other autoregressive paradigms.Our code is available at https://github.com/Chrisqcwx/ImageAR-MIA.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7814\u7a76\u4e86\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u6210\u5458\u63a8\u7406\u653b\u51fb\u4e2d\u7684\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u5206\u7c7b\u548c\u81ea\u9002\u5e94\u5206\u6570\u805a\u5408\u7684\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u6570\u636e\u9690\u79c1\u548c\u7248\u6743\u4fdd\u62a4\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u5f15\u53d1\u4e86\u6570\u636e\u9690\u79c1\u548c\u7248\u6743\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u68c0\u6d4b\u672a\u7ecf\u6388\u6743\u7684\u8bad\u7ec3\u6570\u636e\u4f7f\u7528\u3002", "method": "\u7ed3\u5408\u9690\u5f0f\u5206\u7c7b\u548c\u81ea\u9002\u5e94\u5206\u6570\u805a\u5408\u7b56\u7565\uff0c\u8ba1\u7b97\u67e5\u8be2\u56fe\u50cf\u7684\u9690\u5f0f\u5206\u7c7b\u5206\u6570\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u805a\u5408\u751f\u6210\u6700\u7ec8\u5206\u6570\u4ee5\u5224\u65ad\u6837\u672c\u662f\u5426\u5c5e\u4e8e\u8bad\u7ec3\u96c6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u7c7b\u522b\u6761\u4ef6\u548c\u6587\u672c\u5230\u56fe\u50cf\u573a\u666f\u4e2d\uff0c\u5e76\u63ed\u793a\u4e86\u5927\u578b\u57fa\u7840\u6a21\u578b\u7684\u8106\u5f31\u6027\u4ee5\u53ca\u5c3a\u5ea6\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u6570\u636e\u7684\u6613\u68c0\u6d4b\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5e76\u4e3a\u81ea\u56de\u5f52\u6a21\u578b\u7684\u8106\u5f31\u6027\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2507.05092", "pdf": "https://arxiv.org/pdf/2507.05092", "abs": "https://arxiv.org/abs/2507.05092", "authors": ["Yucheng Wang", "Dan Xu"], "title": "MoDiT: Learning Highly Consistent 3D Motion Coefficients with Diffusion Transformer for Talking Head Generation", "categories": ["cs.CV"], "comment": null, "summary": "Audio-driven talking head generation is critical for applications such as virtual assistants, video games, and films, where natural lip movements are essential. Despite progress in this field, challenges remain in producing both consistent and realistic facial animations. Existing methods, often based on GANs or UNet-based diffusion models, face three major limitations: (i) temporal jittering caused by weak temporal constraints, resulting in frame inconsistencies; (ii) identity drift due to insufficient 3D information extraction, leading to poor preservation of facial identity; and (iii) unnatural blinking behavior due to inadequate modeling of realistic blink dynamics. To address these issues, we propose MoDiT, a novel framework that combines the 3D Morphable Model (3DMM) with a Diffusion-based Transformer. Our contributions include: (i) A hierarchical denoising strategy with revised temporal attention and biased self/cross-attention mechanisms, enabling the model to refine lip synchronization and progressively enhance full-face coherence, effectively mitigating temporal jittering. (ii) The integration of 3DMM coefficients to provide explicit spatial constraints, ensuring accurate 3D-informed optical flow prediction and improved lip synchronization using Wav2Lip results, thereby preserving identity consistency. (iii) A refined blinking strategy to model natural eye movements, with smoother and more realistic blinking behaviors.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMoDiT\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u54083DMM\u548c\u57fa\u4e8e\u6269\u6563\u7684Transformer\uff0c\u89e3\u51b3\u4e86\u97f3\u9891\u9a71\u52a8\u8bf4\u8bdd\u5934\u751f\u6210\u4e2d\u7684\u65f6\u95f4\u6296\u52a8\u3001\u8eab\u4efd\u6f02\u79fb\u548c\u4e0d\u81ea\u7136\u7728\u773c\u95ee\u9898\u3002", "motivation": "\u97f3\u9891\u9a71\u52a8\u8bf4\u8bdd\u5934\u751f\u6210\u5728\u865a\u62df\u52a9\u624b\u3001\u89c6\u9891\u6e38\u620f\u548c\u7535\u5f71\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u65f6\u95f4\u6296\u52a8\u3001\u8eab\u4efd\u6f02\u79fb\u548c\u4e0d\u81ea\u7136\u7728\u773c\u7b49\u95ee\u9898\u3002", "method": "MoDiT\u6846\u67b6\u7ed3\u54083DMM\u548c\u6269\u6563Transformer\uff0c\u91c7\u7528\u5206\u5c42\u53bb\u566a\u7b56\u7565\u30013DMM\u7cfb\u6570\u96c6\u6210\u548c\u6539\u8fdb\u7684\u7728\u773c\u7b56\u7565\u3002", "result": "\u6a21\u578b\u6709\u6548\u51cf\u5c11\u4e86\u65f6\u95f4\u6296\u52a8\uff0c\u4fdd\u6301\u4e86\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u5e76\u751f\u6210\u4e86\u66f4\u81ea\u7136\u7684\u7728\u773c\u884c\u4e3a\u3002", "conclusion": "MoDiT\u6846\u67b6\u5728\u97f3\u9891\u9a71\u52a8\u8bf4\u8bdd\u5934\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.05163", "pdf": "https://arxiv.org/pdf/2507.05163", "abs": "https://arxiv.org/abs/2507.05163", "authors": ["Yutian Chen", "Shi Guo", "Tianshuo Yang", "Lihe Ding", "Xiuyuan Yu", "Jinwei Gu", "Tianfan Xue"], "title": "4DSloMo: 4D Reconstruction for High Speed Scene with Asynchronous Capture", "categories": ["cs.CV"], "comment": "Webpage: https://openimaginglab.github.io/4DSloMo/", "summary": "Reconstructing fast-dynamic scenes from multi-view videos is crucial for high-speed motion analysis and realistic 4D reconstruction. However, the majority of 4D capture systems are limited to frame rates below 30 FPS (frames per second), and a direct 4D reconstruction of high-speed motion from low FPS input may lead to undesirable results. In this work, we propose a high-speed 4D capturing system only using low FPS cameras, through novel capturing and processing modules. On the capturing side, we propose an asynchronous capture scheme that increases the effective frame rate by staggering the start times of cameras. By grouping cameras and leveraging a base frame rate of 25 FPS, our method achieves an equivalent frame rate of 100-200 FPS without requiring specialized high-speed cameras. On processing side, we also propose a novel generative model to fix artifacts caused by 4D sparse-view reconstruction, as asynchrony reduces the number of viewpoints at each timestamp. Specifically, we propose to train a video-diffusion-based artifact-fix model for sparse 4D reconstruction, which refines missing details, maintains temporal consistency, and improves overall reconstruction quality. Experimental results demonstrate that our method significantly enhances high-speed 4D reconstruction compared to synchronous capture.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u4f4e\u5e27\u7387\u76f8\u673a\u5b9e\u73b0\u9ad8\u901f4D\u573a\u666f\u91cd\u5efa\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f02\u6b65\u6355\u83b7\u548c\u751f\u6210\u6a21\u578b\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u73b0\u67094D\u6355\u6349\u7cfb\u7edf\u5e27\u7387\u4f4e\uff0c\u76f4\u63a5\u91cd\u5efa\u9ad8\u901f\u8fd0\u52a8\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u5f02\u6b65\u6355\u83b7\u65b9\u6848\u63d0\u9ad8\u6709\u6548\u5e27\u7387\uff0c\u7ed3\u5408\u89c6\u9891\u6269\u6563\u6a21\u578b\u4fee\u590d\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u7684\u4f2a\u5f71\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u540c\u6b65\u6355\u83b7\uff0c\u5b9e\u73b0\u7b49\u6548100-200 FPS\u7684\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "conclusion": "\u901a\u8fc7\u5f02\u6b65\u6355\u83b7\u548c\u751f\u6210\u6a21\u578b\uff0c\u6210\u529f\u5b9e\u73b0\u4f4e\u6210\u672c\u9ad8\u901f4D\u91cd\u5efa\u3002"}}
{"id": "2507.05173", "pdf": "https://arxiv.org/pdf/2507.05173", "abs": "https://arxiv.org/abs/2507.05173", "authors": ["Yijia Hong", "Jiangning Zhang", "Ran Yi", "Yuji Wang", "Weijian Cao", "Xiaobin Hu", "Zhucun Xue", "Yabiao Wang", "Chengjie Wang", "Lizhuang Ma"], "title": "Semantic Frame Interpolation", "categories": ["cs.CV"], "comment": "https://github.com/hyj542682306/Semantic-Frame-Interpolation", "summary": "Generating intermediate video content of varying lengths based on given first and last frames, along with text prompt information, offers significant research and application potential. However, traditional frame interpolation tasks primarily focus on scenarios with a small number of frames, no text control, and minimal differences between the first and last frames. Recent community developers have utilized large video models represented by Wan to endow frame-to-frame capabilities. However, these models can only generate a fixed number of frames and often fail to produce satisfactory results for certain frame lengths, while this setting lacks a clear official definition and a well-established benchmark. In this paper, we first propose a new practical Semantic Frame Interpolation (SFI) task from the perspective of academic definition, which covers the above two settings and supports inference at multiple frame rates. To achieve this goal, we propose a novel SemFi model building upon Wan2.1, which incorporates a Mixture-of-LoRA module to ensure the generation of high-consistency content that aligns with control conditions across various frame length limitations. Furthermore, we propose SFI-300K, the first general-purpose dataset and benchmark specifically designed for SFI. To support this, we collect and process data from the perspective of SFI, carefully designing evaluation metrics and methods to assess the model's performance across multiple dimensions, encompassing image and video, and various aspects, including consistency and diversity. Through extensive experiments on SFI-300K, we demonstrate that our method is particularly well-suited to meet the requirements of the SFI task.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bed\u4e49\u5e27\u63d2\u503c\uff08SFI\uff09\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86SemFi\u6a21\u578b\u548cSFI-300K\u6570\u636e\u96c6\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u5e27\u63d2\u503c\u4efb\u52a1\u5728\u6587\u672c\u63a7\u5236\u548c\u591a\u5e27\u7387\u751f\u6210\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u5e27\u63d2\u503c\u4efb\u52a1\u7f3a\u4e4f\u6587\u672c\u63a7\u5236\u548c\u591a\u5e27\u7387\u652f\u6301\uff0c\u4e14\u751f\u6210\u6548\u679c\u53d7\u9650\u4e8e\u56fa\u5b9a\u5e27\u6570\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u57fa\u4e8eWan2.1\u6784\u5efaSemFi\u6a21\u578b\uff0c\u5f15\u5165Mixture-of-LoRA\u6a21\u5757\uff0c\u652f\u6301\u591a\u5e27\u7387\u751f\u6210\uff0c\u5e76\u521b\u5efaSFI-300K\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6807\u51c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSemFi\u5728SFI\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u4e00\u81f4\u6027\u548c\u591a\u6837\u6027\u65b9\u9762\u3002", "conclusion": "SemFi\u548cSFI-300K\u4e3a\u8bed\u4e49\u5e27\u63d2\u503c\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.05256", "pdf": "https://arxiv.org/pdf/2507.05256", "abs": "https://arxiv.org/abs/2507.05256", "authors": ["Jiahao Zhu", "Zixuan Chen", "Guangcong Wang", "Xiaohua Xie", "Yi Zhou"], "title": "SegmentDreamer: Towards High-fidelity Text-to-3D Synthesis with Segmented Consistency Trajectory Distillation", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025, project page: https://zjhjojo.github.io/", "summary": "Recent advancements in text-to-3D generation improve the visual quality of Score Distillation Sampling (SDS) and its variants by directly connecting Consistency Distillation (CD) to score distillation. However, due to the imbalance between self-consistency and cross-consistency, these CD-based methods inherently suffer from improper conditional guidance, leading to sub-optimal generation results. To address this issue, we present SegmentDreamer, a novel framework designed to fully unleash the potential of consistency models for high-fidelity text-to-3D generation. Specifically, we reformulate SDS through the proposed Segmented Consistency Trajectory Distillation (SCTD), effectively mitigating the imbalance issues by explicitly defining the relationship between self- and cross-consistency. Moreover, SCTD partitions the Probability Flow Ordinary Differential Equation (PF-ODE) trajectory into multiple sub-trajectories and ensures consistency within each segment, which can theoretically provide a significantly tighter upper bound on distillation error. Additionally, we propose a distillation pipeline for a more swift and stable generation. Extensive experiments demonstrate that our SegmentDreamer outperforms state-of-the-art methods in visual quality, enabling high-fidelity 3D asset creation through 3D Gaussian Splatting (3DGS).", "AI": {"tldr": "SegmentDreamer\u901a\u8fc7Segmented Consistency Trajectory Distillation (SCTD)\u89e3\u51b3CD-based\u65b9\u6cd5\u5728text-to-3D\u751f\u6210\u4e2d\u7684\u6761\u4ef6\u6307\u5bfc\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eCD\u7684\u65b9\u6cd5\u56e0\u81ea\u4e00\u81f4\u6027\u548c\u4ea4\u53c9\u4e00\u81f4\u6027\u4e0d\u5e73\u8861\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u4e0d\u7406\u60f3\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faSCTD\uff0c\u91cd\u65b0\u5b9a\u4e49\u81ea\u4e00\u81f4\u6027\u548c\u4ea4\u53c9\u4e00\u81f4\u6027\u5173\u7cfb\uff0c\u5e76\u5206\u5272PF-ODE\u8f68\u8ff9\u4ee5\u786e\u4fdd\u6bb5\u5185\u4e00\u81f4\u6027\uff0c\u964d\u4f4e\u84b8\u998f\u8bef\u5dee\u3002", "result": "SegmentDreamer\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u9ad8\u4fdd\u771f3D\u751f\u6210\u3002", "conclusion": "SegmentDreamer\u901a\u8fc7SCTD\u6709\u6548\u89e3\u51b3\u4e86\u6761\u4ef6\u6307\u5bfc\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86text-to-3D\u751f\u6210\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2507.03184", "pdf": "https://arxiv.org/pdf/2507.03184", "abs": "https://arxiv.org/abs/2507.03184", "authors": ["WenJie Cai", "Qingguo Meng", "Zhenyu Wang", "Xingbo Dong", "Zhe Jin"], "title": "EvRWKV: A RWKV Framework for Effective Event-guided Low-Light Image Enhancement", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Capturing high-quality visual content under low-light conditions remains a challenging problem due to severe noise, motion blur, and underexposure, which degrade the performance of downstream applications. Traditional frame-based low-light enhancement methods often amplify noise or fail to preserve structural details, especially in real-world scenarios. Event cameras, offering high dynamic range and microsecond temporal resolution by asynchronously capturing brightness changes, emerge as promising alternatives for low-light imaging. However, existing event-image fusion methods suffer from simplistic fusion strategies and inadequate handling of spatial-temporal misalignment and noise. To address these challenges, we propose EvRWKV, a novel framework that enables continuous cross-modal interaction through dual-domain processing. Our approach incorporates a Cross-RWKV module, leveraging the Receptance Weighted Key Value (RWKV) architecture for fine-grained temporal and cross-modal fusion, and an Event Image Spectral Fusion Enhancer (EISFE) module, which jointly performs adaptive frequency-domain noise suppression and spatial-domain deformable convolution alignment. Extensive qualitative and quantitative evaluations on real-world low-light datasets(SDE, SDSD, RELED) demonstrate that EvRWKV achieves state-of-the-art performance, effectively enhancing image quality by suppressing noise, restoring structural details, and improving visual clarity in challenging low-light conditions.", "AI": {"tldr": "EvRWKV\u662f\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u57df\u5904\u7406\u548c\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u6709\u6548\u89e3\u51b3\u4f4e\u5149\u6761\u4ef6\u4e0b\u7684\u56fe\u50cf\u589e\u5f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u4f4e\u5149\u6761\u4ef6\u4e0b\u56fe\u50cf\u8d28\u91cf\u5dee\uff08\u566a\u58f0\u3001\u6a21\u7cca\u3001\u66dd\u5149\u4e0d\u8db3\uff09\u5f71\u54cd\u4e0b\u6e38\u5e94\u7528\u6027\u80fd\uff0c\u4f20\u7edf\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u4e8b\u4ef6\u76f8\u673a\u867d\u6709\u6f5c\u529b\u4f46\u73b0\u6709\u878d\u5408\u65b9\u6cd5\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faEvRWKV\u6846\u67b6\uff0c\u5305\u542bCross-RWKV\u6a21\u5757\uff08\u7528\u4e8e\u7cbe\u7ec6\u65f6\u7a7a\u548c\u8de8\u6a21\u6001\u878d\u5408\uff09\u548cEISFE\u6a21\u5757\uff08\u81ea\u9002\u5e94\u9891\u57df\u566a\u58f0\u6291\u5236\u548c\u7a7a\u95f4\u57df\u5bf9\u9f50\uff09\u3002", "result": "\u5728\u771f\u5b9e\u4f4e\u5149\u6570\u636e\u96c6\uff08SDE\u3001SDSD\u3001RELED\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u566a\u58f0\u6291\u5236\u3001\u7ec6\u8282\u6062\u590d\u548c\u89c6\u89c9\u6e05\u6670\u5ea6\u63d0\u5347\u663e\u8457\u3002", "conclusion": "EvRWKV\u5728\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4e2d\u8fbe\u5230\u9886\u5148\u6c34\u5e73\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.03341", "pdf": "https://arxiv.org/pdf/2507.03341", "abs": "https://arxiv.org/abs/2507.03341", "authors": ["Zhuo Li", "Xuhang Chen", "Shuqiang Wang"], "title": "UltraDfeGAN: Detail-Enhancing Generative Adversarial Networks for High-Fidelity Functional Ultrasound Synthesis", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "comment": null, "summary": "Functional ultrasound (fUS) is a neuroimaging technique known for its high spatiotemporal resolution, enabling non-invasive observation of brain activity through neurovascular coupling. Despite its potential in clinical applications such as neonatal monitoring and intraoperative guidance, the development of fUS faces challenges related to data scarcity and limitations in generating realistic fUS images. This paper explores the use of a generative adversarial network (GAN) framework tailored for fUS image synthesis. The proposed method incorporates architectural enhancements, including feature enhancement modules and normalization techniques, aiming to improve the fidelity and physiological plausibility of generated images. The study evaluates the performance of the framework against existing generative models, demonstrating its capability to produce high-quality fUS images under various experimental conditions. Additionally, the synthesized images are assessed for their utility in downstream tasks, showing improvements in classification accuracy when used for data augmentation. Experimental results are based on publicly available fUS datasets, highlighting the framework's effectiveness in addressing data limitations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGAN\u7684fUS\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u67b6\u6784\u6539\u8fdb\u63d0\u5347\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u548c\u751f\u7406\u5408\u7406\u6027\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "fUS\u6280\u672f\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u56fe\u50cf\u771f\u5b9e\u6027\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u91c7\u7528GAN\u6846\u67b6\uff0c\u7ed3\u5408\u7279\u5f81\u589e\u5f3a\u6a21\u5757\u548c\u5f52\u4e00\u5316\u6280\u672f\uff0c\u4f18\u5316fUS\u56fe\u50cf\u5408\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u9ad8\u8d28\u91cffUS\u56fe\u50cf\uff0c\u5e76\u5728\u6570\u636e\u589e\u5f3a\u4efb\u52a1\u4e2d\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86fUS\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4e3a\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2507.03916", "pdf": "https://arxiv.org/pdf/2507.03916", "abs": "https://arxiv.org/abs/2507.03916", "authors": ["Yifan Jiang", "Yibo Xue", "Yukun Kang", "Pin Zheng", "Jian Peng", "Feiran Wu", "Changliang Xu"], "title": "Animation Needs Attention: A Holistic Approach to Slides Animation Comprehension with Visual-Language Models", "categories": ["cs.AI", "cs.CV", "68T01"], "comment": "Appendix at:   https://github.com/PAMPAS-Lab/ANA-PPT-Anamation/blob/main/Appendix.pdf", "summary": "Slide animations, such as fade-ins, fly-ins, and wipes, are critical for audience engagement, efficient information delivery, and vivid visual expression. However, most AI-driven slide-generation tools still lack native animation support, and existing vision-language models (VLMs) struggle with animation tasks due to the absence of public datasets and limited temporal-reasoning capabilities. To address this gap, we release the first public dataset for slide-animation modeling: 12,000 triplets of natural-language descriptions, animation JSON files, and rendered videos, collectively covering every built-in PowerPoint effect. Using this resource, we fine-tune Qwen-2.5-VL-7B with Low-Rank Adaptation (LoRA) and achieve consistent improvements over GPT-4.1 and Gemini-2.5-Pro in BLEU-4, ROUGE-L, SPICE, and our Coverage-Order-Detail Assessment (CODA) metric, which evaluates action coverage, temporal order, and detail fidelity. On a manually curated test set of slides, the LoRA model increases BLEU-4 by around 60%, ROUGE-L by 30%, and shows significant improvements in CODA-detail. This demonstrates that low-rank adaptation enables reliable temporal reasoning and generalization beyond synthetic data. Overall, our dataset, LoRA-enhanced model, and CODA metric provide a rigorous benchmark and foundation for future research on VLM-based dynamic slide generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u516c\u5f00\u7684\u5e7b\u706f\u7247\u52a8\u753b\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528LoRA\u5fae\u8c03Qwen-2.5-VL-7B\u6a21\u578b\uff0c\u5728\u52a8\u753b\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8eGPT-4.1\u548cGemini-2.5-Pro\u3002", "motivation": "\u89e3\u51b3AI\u9a71\u52a8\u7684\u5e7b\u706f\u7247\u751f\u6210\u5de5\u5177\u7f3a\u4e4f\u52a8\u753b\u652f\u6301\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u753b\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "method": "\u53d1\u5e03\u5305\u542b12,000\u4e2a\u4e09\u5143\u7ec4\uff08\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u3001\u52a8\u753bJSON\u6587\u4ef6\u3001\u6e32\u67d3\u89c6\u9891\uff09\u7684\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528LoRA\u5fae\u8c03Qwen-2.5-VL-7B\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728BLEU-4\u3001ROUGE-L\u3001SPICE\u548cCODA\u6307\u6807\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u5c24\u5176\u5728CODA\u7ec6\u8282\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u6570\u636e\u96c6\u3001LoRA\u6a21\u578b\u548cCODA\u6307\u6807\u4e3a\u672a\u6765\u57fa\u4e8eVLM\u7684\u52a8\u6001\u5e7b\u706f\u7247\u751f\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u548c\u57fa\u7840\u3002"}}
{"id": "2507.03937", "pdf": "https://arxiv.org/pdf/2507.03937", "abs": "https://arxiv.org/abs/2507.03937", "authors": ["Hyunwoo Cho", "Jongsoo Lee", "Jinbum Kang", "Yangmo Yoo"], "title": "EdgeSRIE: A hybrid deep learning framework for real-time speckle reduction and image enhancement on portable ultrasound systems", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Speckle patterns in ultrasound images often obscure anatomical details, leading to diagnostic uncertainty. Recently, various deep learning (DL)-based techniques have been introduced to effectively suppress speckle; however, their high computational costs pose challenges for low-resource devices, such as portable ultrasound systems. To address this issue, EdgeSRIE, which is a lightweight hybrid DL framework for real-time speckle reduction and image enhancement in portable ultrasound imaging, is introduced. The proposed framework consists of two main branches: an unsupervised despeckling branch, which is trained by minimizing a loss function between speckled images, and a deblurring branch, which restores blurred images to sharp images. For hardware implementation, the trained network is quantized to 8-bit integer precision and deployed on a low-resource system-on-chip (SoC) with limited power consumption. In the performance evaluation with phantom and in vivo analyses, EdgeSRIE achieved the highest contrast-to-noise ratio (CNR) and average gradient magnitude (AGM) compared with the other baselines (different 2-rule-based methods and other 4-DL-based methods). Furthermore, EdgeSRIE enabled real-time inference at over 60 frames per second while satisfying computational requirements (< 20K parameters) on actual portable ultrasound hardware. These results demonstrated the feasibility of EdgeSRIE for real-time, high-quality ultrasound imaging in resource-limited environments.", "AI": {"tldr": "EdgeSRIE\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4fbf\u643a\u5f0f\u8d85\u58f0\u6210\u50cf\u4e2d\u7684\u5b9e\u65f6\u6591\u70b9\u51cf\u5c11\u548c\u56fe\u50cf\u589e\u5f3a\uff0c\u5177\u6709\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u9ad8\u6548\u6027\u80fd\u3002", "motivation": "\u8d85\u58f0\u56fe\u50cf\u4e2d\u7684\u6591\u70b9\u6a21\u5f0f\u4f1a\u63a9\u76d6\u89e3\u5256\u7ec6\u8282\uff0c\u5bfc\u81f4\u8bca\u65ad\u4e0d\u786e\u5b9a\u6027\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5728\u4f4e\u8d44\u6e90\u8bbe\u5907\u4e0a\u5e94\u7528\u3002", "method": "EdgeSRIE\u5305\u542b\u4e24\u4e2a\u5206\u652f\uff1a\u65e0\u76d1\u7763\u53bb\u6591\u70b9\u5206\u652f\u548c\u53bb\u6a21\u7cca\u5206\u652f\uff0c\u7f51\u7edc\u91cf\u5316\u540e\u90e8\u7f72\u5728\u4f4e\u529f\u8017SoC\u4e0a\u3002", "result": "\u5728\u6027\u80fd\u8bc4\u4f30\u4e2d\uff0cEdgeSRIE\u5728CNR\u548cAGM\u4e0a\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5b9e\u73b060\u5e27/\u79d2\u7684\u5b9e\u65f6\u63a8\u7406\u3002", "conclusion": "EdgeSRIE\u5728\u8d44\u6e90\u6709\u9650\u73af\u5883\u4e0b\u5b9e\u73b0\u4e86\u5b9e\u65f6\u9ad8\u8d28\u91cf\u8d85\u58f0\u6210\u50cf\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2507.04021", "pdf": "https://arxiv.org/pdf/2507.04021", "abs": "https://arxiv.org/abs/2507.04021", "authors": ["Niklas Vaara", "Pekka Sangi", "Miguel Bordallo L\u00f3pez", "Janne Heikkil\u00e4"], "title": "Differentiable High-Performance Ray Tracing-Based Simulation of Radio Propagation with Point Clouds", "categories": ["eess.SP", "cs.CV"], "comment": null, "summary": "Ray tracing is a widely used deterministic method for radio propagation simulations, capable of producing physically accurate multipath components. The accuracy depends on the quality of the environment model and its electromagnetic properties. Recent advances in computer vision and machine learning have made it possible to reconstruct detailed environment models augmented with semantic segmentation labels.   In this letter, we propose a differentiable ray tracing-based radio propagation simulator that operates directly on point clouds. We showcase the efficiency of our method by simulating multi-bounce propagation paths with up to five interactions with specular reflections and diffuse scattering in two indoor scenarios, each completing in less than 90 ms. Lastly, we demonstrate how the differentiability of electromagnetic computations can be combined with segmentation labels to learn the electromagnetic properties of the environment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u70b9\u4e91\u7684\u53ef\u5fae\u5206\u5c04\u7ebf\u8ffd\u8e2a\u65e0\u7ebf\u7535\u4f20\u64ad\u6a21\u62df\u5668\uff0c\u5c55\u793a\u4e86\u5176\u9ad8\u6548\u6027\uff0c\u5e76\u5229\u7528\u5206\u5272\u6807\u7b7e\u5b66\u4e60\u73af\u5883\u7684\u7535\u78c1\u7279\u6027\u3002", "motivation": "\u5c04\u7ebf\u8ffd\u8e2a\u662f\u65e0\u7ebf\u7535\u4f20\u64ad\u6a21\u62df\u7684\u5e38\u7528\u65b9\u6cd5\uff0c\u4f46\u5176\u51c6\u786e\u6027\u4f9d\u8d56\u4e8e\u73af\u5883\u6a21\u578b\u7684\u8d28\u91cf\u3002\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u7684\u8fdb\u6b65\u4f7f\u5f97\u91cd\u5efa\u5e26\u6709\u8bed\u4e49\u5206\u5272\u6807\u7b7e\u7684\u8be6\u7ec6\u73af\u5883\u6a21\u578b\u6210\u4e3a\u53ef\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u64cd\u4f5c\u70b9\u4e91\u7684\u53ef\u5fae\u5206\u5c04\u7ebf\u8ffd\u8e2a\u6a21\u62df\u5668\uff0c\u6a21\u62df\u4e86\u591a\u8fbe\u4e94\u6b21\u53cd\u5c04\u548c\u6563\u5c04\u7684\u591a\u8def\u5f84\u4f20\u64ad\u3002", "result": "\u5728\u4e24\u4e2a\u5ba4\u5185\u573a\u666f\u4e2d\uff0c\u6a21\u62df\u65f6\u95f4\u5c11\u4e8e90\u6beb\u79d2\uff0c\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u9ad8\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u53ef\u5fae\u5206\u7535\u78c1\u8ba1\u7b97\u548c\u5206\u5272\u6807\u7b7e\uff0c\u80fd\u591f\u5b66\u4e60\u73af\u5883\u7684\u7535\u78c1\u7279\u6027\u3002"}}
{"id": "2507.04252", "pdf": "https://arxiv.org/pdf/2507.04252", "abs": "https://arxiv.org/abs/2507.04252", "authors": ["Yinuo Wang", "Juhyun Bae", "Ka Ho Chow", "Shenyang Chen", "Shreyash Gupta"], "title": "Deep-Learning-Assisted Highly-Accurate COVID-19 Diagnosis on Lung Computed Tomography Images", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "COVID-19 is a severe and acute viral disease that can cause symptoms consistent with pneumonia in which inflammation is caused in the alveolous regions of the lungs leading to a build-up of fluid and breathing difficulties. Thus, the diagnosis of COVID using CT scans has been effective in assisting with RT-PCR diagnosis and severity classifications. In this paper, we proposed a new data quality control pipeline to refine the quality of CT images based on GAN and sliding windows. Also, we use class-sensitive cost functions including Label Distribution Aware Loss(LDAM Loss) and Class-balanced(CB) Loss to solve the long-tail problem existing in datasets. Our model reaches more than 0.983 MCC in the benchmark test dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGAN\u548c\u6ed1\u52a8\u7a97\u53e3\u7684CT\u56fe\u50cf\u8d28\u91cf\u63a7\u5236\u6d41\u7a0b\uff0c\u5e76\u91c7\u7528LDAM Loss\u548cCB Loss\u89e3\u51b3\u6570\u636e\u957f\u5c3e\u95ee\u9898\uff0c\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0aMCC\u8d85\u8fc70.983\u3002", "motivation": "COVID-19\u7684CT\u8bca\u65ad\u9700\u8981\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\uff0c\u4f46\u73b0\u6709\u6570\u636e\u5b58\u5728\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u4f7f\u7528GAN\u548c\u6ed1\u52a8\u7a97\u53e3\u4f18\u5316CT\u56fe\u50cf\u8d28\u91cf\uff0c\u7ed3\u5408LDAM Loss\u548cCB Loss\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u7684MCC\u8fbe\u52300.983\u4ee5\u4e0a\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86CT\u56fe\u50cf\u8d28\u91cf\u548c\u6a21\u578b\u6027\u80fd\uff0c\u4e3aCOVID-19\u8bca\u65ad\u63d0\u4f9b\u4e86\u53ef\u9760\u652f\u6301\u3002"}}
{"id": "2507.04547", "pdf": "https://arxiv.org/pdf/2507.04547", "abs": "https://arxiv.org/abs/2507.04547", "authors": ["Xin You", "Runze Yang", "Chuyan Zhang", "Zhongliang Jiang", "Jie Yang", "Nassir Navab"], "title": "FB-Diff: Fourier Basis-guided Diffusion for Temporal Interpolation of 4D Medical Imaging", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "The temporal interpolation task for 4D medical imaging, plays a crucial role in clinical practice of respiratory motion modeling. Following the simplified linear-motion hypothesis, existing approaches adopt optical flow-based models to interpolate intermediate frames. However, realistic respiratory motions should be nonlinear and quasi-periodic with specific frequencies. Intuited by this property, we resolve the temporal interpolation task from the frequency perspective, and propose a Fourier basis-guided Diffusion model, termed FB-Diff. Specifically, due to the regular motion discipline of respiration, physiological motion priors are introduced to describe general characteristics of temporal data distributions. Then a Fourier motion operator is elaborately devised to extract Fourier bases by incorporating physiological motion priors and case-specific spectral information in the feature space of Variational Autoencoder. Well-learned Fourier bases can better simulate respiratory motions with motion patterns of specific frequencies. Conditioned on starting and ending frames, the diffusion model further leverages well-learned Fourier bases via the basis interaction operator, which promotes the temporal interpolation task in a generative manner. Extensive results demonstrate that FB-Diff achieves state-of-the-art (SOTA) perceptual performance with better temporal consistency while maintaining promising reconstruction metrics. Codes are available.", "AI": {"tldr": "FB-Diff\u662f\u4e00\u79cd\u57fa\u4e8e\u5085\u91cc\u53f6\u57fa\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e4D\u533b\u5b66\u56fe\u50cf\u7684\u65f6\u95f4\u63d2\u503c\u4efb\u52a1\uff0c\u901a\u8fc7\u7ed3\u5408\u751f\u7406\u8fd0\u52a8\u5148\u9a8c\u548c\u9891\u7387\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u547c\u5438\u8fd0\u52a8\u5efa\u6a21\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u57fa\u4e8e\u7ebf\u6027\u8fd0\u52a8\u5047\u8bbe\uff0c\u4f46\u5b9e\u9645\u547c\u5438\u8fd0\u52a8\u662f\u975e\u7ebf\u6027\u548c\u51c6\u5468\u671f\u7684\uff0c\u56e0\u6b64\u9700\u8981\u4ece\u9891\u7387\u89d2\u5ea6\u89e3\u51b3\u65f6\u95f4\u63d2\u503c\u95ee\u9898\u3002", "method": "\u63d0\u51faFB-Diff\u6a21\u578b\uff0c\u7ed3\u5408\u751f\u7406\u8fd0\u52a8\u5148\u9a8c\u548c\u5085\u91cc\u53f6\u57fa\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u4e2d\u95f4\u5e27\u3002", "result": "FB-Diff\u5728\u611f\u77e5\u6027\u80fd\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\u4e0a\u8fbe\u5230SOTA\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u597d\u91cd\u5efa\u6307\u6807\u3002", "conclusion": "FB-Diff\u4e3a\u975e\u7ebf\u6027\u547c\u5438\u8fd0\u52a8\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.04704", "pdf": "https://arxiv.org/pdf/2507.04704", "abs": "https://arxiv.org/abs/2507.04704", "authors": ["Zhenglun Kong", "Mufan Qiu", "John Boesen", "Xiang Lin", "Sukwon Yun", "Tianlong Chen", "Manolis Kellis", "Marinka Zitnik"], "title": "SPATIA: Multimodal Model for Prediction and Generation of Spatial Cell Phenotypes", "categories": ["q-bio.QM", "cs.AI", "cs.CV"], "comment": null, "summary": "Understanding how cellular morphology, gene expression, and spatial organization jointly shape tissue function is a central challenge in biology. Image-based spatial transcriptomics technologies now provide high-resolution measurements of cell images and gene expression profiles, but machine learning methods typically analyze these modalities in isolation or at limited resolution. We address the problem of learning unified, spatially aware representations that integrate cell morphology, gene expression, and spatial context across biological scales. This requires models that can operate at single-cell resolution, reason across spatial neighborhoods, and generalize to whole-slide tissue organization. Here, we introduce SPATIA, a multi-scale generative and predictive model for spatial transcriptomics. SPATIA learns cell-level embeddings by fusing image-derived morphological tokens and transcriptomic vector tokens using cross-attention and then aggregates them at niche and tissue levels using transformer modules to capture spatial dependencies. SPATIA incorporates token merging in its generative diffusion decoder to synthesize high-resolution cell images conditioned on gene expression. We assembled a multi-scale dataset consisting of 17 million cell-gene pairs, 1 million niche-gene pairs, and 10,000 tissue-gene pairs across 49 donors, 17 tissue types, and 12 disease states. We benchmark SPATIA against 13 existing models across 12 individual tasks, which span several categories including cell annotation, cell clustering, gene imputation, cross-modal prediction, and image generation. SPATIA achieves improved performance over all baselines and generates realistic cell morphologies that reflect transcriptomic perturbations.", "AI": {"tldr": "SPATIA\u662f\u4e00\u4e2a\u591a\u5c3a\u5ea6\u751f\u6210\u548c\u9884\u6d4b\u6a21\u578b\uff0c\u7528\u4e8e\u6574\u5408\u7ec6\u80de\u5f62\u6001\u3001\u57fa\u56e0\u8868\u8fbe\u548c\u7a7a\u95f4\u80cc\u666f\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u5b66\u4e60\u548c\u7a7a\u95f4\u4f9d\u8d56\u5efa\u6a21\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u7406\u89e3\u7ec6\u80de\u5f62\u6001\u3001\u57fa\u56e0\u8868\u8fbe\u548c\u7a7a\u95f4\u7ec4\u7ec7\u5982\u4f55\u5171\u540c\u5f71\u54cd\u7ec4\u7ec7\u529f\u80fd\u662f\u751f\u7269\u5b66\u4e2d\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ec5\u5206\u6790\u5355\u4e00\u6a21\u6001\u6216\u5206\u8fa8\u7387\u6709\u9650\u3002", "method": "SPATIA\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u878d\u5408\u7ec6\u80de\u5f62\u6001\u548c\u57fa\u56e0\u8868\u8fbe\u7279\u5f81\uff0c\u4f7f\u7528Transformer\u6a21\u5757\u5728\u591a\u4e2a\u5c3a\u5ea6\uff08\u7ec6\u80de\u3001\u751f\u6001\u4f4d\u3001\u7ec4\u7ec7\uff09\u4e0a\u5efa\u6a21\u7a7a\u95f4\u4f9d\u8d56\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u6269\u6563\u89e3\u7801\u5668\u5408\u6210\u9ad8\u5206\u8fa8\u7387\u7ec6\u80de\u56fe\u50cf\u3002", "result": "SPATIA\u572812\u4e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e13\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0c\u80fd\u591f\u751f\u6210\u53cd\u6620\u8f6c\u5f55\u7ec4\u6270\u52a8\u7684\u771f\u5b9e\u7ec6\u80de\u5f62\u6001\u3002", "conclusion": "SPATIA\u4e3a\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u3001\u7a7a\u95f4\u611f\u77e5\u7684\u591a\u5c3a\u5ea6\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.05077", "pdf": "https://arxiv.org/pdf/2507.05077", "abs": "https://arxiv.org/abs/2507.05077", "authors": ["Tarun G", "Naman Malpani", "Gugan Thoppe", "Sridharan Devarajan"], "title": "Sequential Attention-based Sampling for Histopathological Analysis", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Deep neural networks are increasingly applied for automated histopathology. Yet, whole-slide images (WSIs) are often acquired at gigapixel sizes, rendering it computationally infeasible to analyze them entirely at high resolution. Diagnostic labels are largely available only at the slide-level, because expert annotation of images at a finer (patch) level is both laborious and expensive. Moreover, regions with diagnostic information typically occupy only a small fraction of the WSI, making it inefficient to examine the entire slide at full resolution. Here, we propose SASHA -- {\\it S}equential {\\it A}ttention-based {\\it S}ampling for {\\it H}istopathological {\\it A}nalysis -- a deep reinforcement learning approach for efficient analysis of histopathological images. First, SASHA learns informative features with a lightweight hierarchical, attention-based multiple instance learning (MIL) model. Second, SASHA samples intelligently and zooms selectively into a small fraction (10-20\\%) of high-resolution patches, to achieve reliable diagnosis. We show that SASHA matches state-of-the-art methods that analyze the WSI fully at high-resolution, albeit at a fraction of their computational and memory costs. In addition, it significantly outperforms competing, sparse sampling methods. We propose SASHA as an intelligent sampling model for medical imaging challenges that involve automated diagnosis with exceptionally large images containing sparsely informative features.", "AI": {"tldr": "SASHA\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u5206\u6790\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\uff0c\u901a\u8fc7\u667a\u80fd\u91c7\u6837\u548c\u9009\u62e9\u6027\u653e\u5927\u5b9e\u73b0\u53ef\u9760\u8bca\u65ad\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u3002", "motivation": "\u5168\u5e7b\u706f\u7247\u56fe\u50cf\uff08WSI\uff09\u901a\u5e38\u4ee5\u5343\u5146\u50cf\u7d20\u5927\u5c0f\u83b7\u53d6\uff0c\u8ba1\u7b97\u4e0a\u96be\u4ee5\u5b8c\u5168\u5206\u6790\uff0c\u4e14\u8bca\u65ad\u6807\u7b7e\u901a\u5e38\u4ec5\u5728\u5e7b\u706f\u7247\u7ea7\u522b\u53ef\u7528\uff0c\u7cbe\u7ec6\u6807\u6ce8\u6210\u672c\u9ad8\u3002", "method": "SASHA\u7ed3\u5408\u8f7b\u91cf\u7ea7\u5206\u5c42\u6ce8\u610f\u529b\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u6a21\u578b\u5b66\u4e60\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u667a\u80fd\u91c7\u6837\u9009\u62e9\u6027\u653e\u5927\u9ad8\u5206\u8fa8\u7387\u533a\u57df\uff0810-20%\uff09\u3002", "result": "SASHA\u5728\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u663e\u8457\u964d\u4f4e\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u4e0e\u5168\u9ad8\u5206\u8fa8\u7387\u5206\u6790\u65b9\u6cd5\u76f8\u5f53\uff0c\u5e76\u4f18\u4e8e\u5176\u4ed6\u7a00\u758f\u91c7\u6837\u65b9\u6cd5\u3002", "conclusion": "SASHA\u662f\u4e00\u79cd\u9002\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u81ea\u52a8\u8bca\u65ad\u7684\u667a\u80fd\u91c7\u6837\u6a21\u578b\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5305\u542b\u7a00\u758f\u4fe1\u606f\u7279\u5f81\u7684\u5927\u56fe\u50cf\u3002"}}
{"id": "2507.05148", "pdf": "https://arxiv.org/pdf/2507.05148", "abs": "https://arxiv.org/abs/2507.05148", "authors": ["Chun Xie", "Yuichi Yoshii", "Itaru Kitahara"], "title": "SV-DRR: High-Fidelity Novel View X-Ray Synthesis Using Diffusion Model", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by MICCAI2025", "summary": "X-ray imaging is a rapid and cost-effective tool for visualizing internal human anatomy. While multi-view X-ray imaging provides complementary information that enhances diagnosis, intervention, and education, acquiring images from multiple angles increases radiation exposure and complicates clinical workflows. To address these challenges, we propose a novel view-conditioned diffusion model for synthesizing multi-view X-ray images from a single view. Unlike prior methods, which are limited in angular range, resolution, and image quality, our approach leverages the Diffusion Transformer to preserve fine details and employs a weak-to-strong training strategy for stable high-resolution image generation. Experimental results demonstrate that our method generates higher-resolution outputs with improved control over viewing angles. This capability has significant implications not only for clinical applications but also for medical education and data extension, enabling the creation of diverse, high-quality datasets for training and analysis. Our code is available at GitHub.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u5355\u89c6\u89d2X\u5c04\u7ebf\u56fe\u50cf\u751f\u6210\u591a\u89c6\u89d2\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u89d2\u5ea6\u8303\u56f4\u3001\u5206\u8fa8\u7387\u548c\u56fe\u50cf\u8d28\u91cf\u4e0a\u7684\u9650\u5236\u3002", "motivation": "\u591a\u89c6\u89d2X\u5c04\u7ebf\u6210\u50cf\u867d\u80fd\u63d0\u4f9b\u4e92\u8865\u4fe1\u606f\uff0c\u4f46\u4f1a\u589e\u52a0\u8f90\u5c04\u66b4\u9732\u548c\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u590d\u6742\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u5355\u89c6\u89d2\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6269\u6563\u53d8\u6362\u5668\uff08Diffusion Transformer\uff09\u548c\u5f31\u5230\u5f3a\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee5\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u3001\u591a\u89c6\u89d2\u7684X\u5c04\u7ebf\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u66f4\u9ad8\u5206\u8fa8\u7387\u3001\u89c6\u89d2\u53ef\u63a7\u7684\u591a\u89c6\u89d2\u56fe\u50cf\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e34\u5e8a\u3001\u533b\u5b66\u6559\u80b2\u548c\u6570\u636e\u6269\u5c55\u65b9\u9762\u5177\u6709\u91cd\u8981\u5e94\u7528\u6f5c\u529b\uff0c\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u6837\u5316\u6570\u636e\u96c6\u3002"}}
{"id": "2507.05198", "pdf": "https://arxiv.org/pdf/2507.05198", "abs": "https://arxiv.org/abs/2507.05198", "authors": ["Boyuan Wang", "Xinpan Meng", "Xiaofeng Wang", "Zheng Zhu", "Angen Ye", "Yang Wang", "Zhiqin Yang", "Chaojun Ni", "Guan Huang", "Xingang Wang"], "title": "EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Project Page: https://embodiedreamer.github.io/", "summary": "The rapid advancement of Embodied AI has led to an increasing demand for large-scale, high-quality real-world data. However, collecting such embodied data remains costly and inefficient. As a result, simulation environments have become a crucial surrogate for training robot policies. Yet, the significant Real2Sim2Real gap remains a critical bottleneck, particularly in terms of physical dynamics and visual appearance. To address this challenge, we propose EmbodieDreamer, a novel framework that reduces the Real2Sim2Real gap from both the physics and appearance perspectives. Specifically, we propose PhysAligner, a differentiable physics module designed to reduce the Real2Sim physical gap. It jointly optimizes robot-specific parameters such as control gains and friction coefficients to better align simulated dynamics with real-world observations. In addition, we introduce VisAligner, which incorporates a conditional video diffusion model to bridge the Sim2Real appearance gap by translating low-fidelity simulated renderings into photorealistic videos conditioned on simulation states, enabling high-fidelity visual transfer. Extensive experiments validate the effectiveness of EmbodieDreamer. The proposed PhysAligner reduces physical parameter estimation error by 3.74% compared to simulated annealing methods while improving optimization speed by 89.91\\%. Moreover, training robot policies in the generated photorealistic environment leads to a 29.17% improvement in the average task success rate across real-world tasks after reinforcement learning. Code, model and data will be publicly available.", "AI": {"tldr": "EmbodieDreamer\u6846\u67b6\u901a\u8fc7\u7269\u7406\u548c\u89c6\u89c9\u5bf9\u9f50\u6a21\u5757\u51cf\u5c11Real2Sim2Real\u5dee\u8ddd\uff0c\u63d0\u5347\u673a\u5668\u4eba\u7b56\u7565\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u89e3\u51b3Embodied AI\u4e2d\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u3001\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u4eff\u771f\u73af\u5883\u4e0e\u771f\u5b9e\u4e16\u754c\u5728\u7269\u7406\u52a8\u6001\u548c\u89c6\u89c9\u5916\u89c2\u4e0a\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51faPhysAligner\uff08\u53ef\u5fae\u5206\u7269\u7406\u6a21\u5757\uff09\u4f18\u5316\u7269\u7406\u53c2\u6570\uff0c\u4ee5\u53caVisAligner\uff08\u6761\u4ef6\u89c6\u9891\u6269\u6563\u6a21\u578b\uff09\u63d0\u5347\u89c6\u89c9\u903c\u771f\u5ea6\u3002", "result": "PhysAligner\u964d\u4f4e\u7269\u7406\u53c2\u6570\u4f30\u8ba1\u8bef\u5dee3.74%\uff0c\u4f18\u5316\u901f\u5ea6\u63d0\u534789.91%\uff1bVisAligner\u4f7f\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad829.17%\u3002", "conclusion": "EmbodieDreamer\u6709\u6548\u7f29\u5c0fReal2Sim2Real\u5dee\u8ddd\uff0c\u63d0\u5347\u673a\u5668\u4eba\u7b56\u7565\u5728\u771f\u5b9e\u4e16\u754c\u7684\u8868\u73b0\u3002"}}
