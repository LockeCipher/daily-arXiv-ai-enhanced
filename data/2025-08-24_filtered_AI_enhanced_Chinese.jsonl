{"id": "2508.14933", "pdf": "https://arxiv.org/pdf/2508.14933", "abs": "https://arxiv.org/abs/2508.14933", "authors": ["Lucas S. Kupssinsk\u00fc", "Marco N. Bochernitsan", "Jordan Kopper", "Ot\u00e1vio Parraga", "Rodrigo C. Barros"], "title": "Inference Time Debiasing Concepts in Diffusion Models", "categories": ["cs.GR", "cs.AI", "cs.LG"], "comment": null, "summary": "We propose DeCoDi, a debiasing procedure for text-to-image diffusion-based models that changes the inference procedure, does not significantly change image quality, has negligible compute overhead, and can be applied in any diffusion-based image generation model. DeCoDi changes the diffusion process to avoid latent dimension regions of biased concepts. While most deep learning debiasing methods require complex or compute-intensive interventions, our method is designed to change only the inference procedure. Therefore, it is more accessible to a wide range of practitioners. We show the effectiveness of the method by debiasing for gender, ethnicity, and age for the concepts of nurse, firefighter, and CEO. Two distinct human evaluators manually inspect 1,200 generated images. Their evaluation results provide evidence that our method is effective in mitigating biases based on gender, ethnicity, and age. We also show that an automatic bias evaluation performed by the GPT4o is not significantly statistically distinct from a human evaluation. Our evaluation shows promising results, with reliable levels of agreement between evaluators and more coverage of protected attributes. Our method has the potential to significantly improve the diversity of images it generates by diffusion-based text-to-image generative models.", "AI": {"tldr": "DeCoDi\u662f\u4e00\u79cd\u7528\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u53bb\u504f\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u53d8\u63a8\u7406\u8fc7\u7a0b\u907f\u514d\u6f5c\u5728\u7ef4\u5ea6\u4e2d\u7684\u504f\u89c1\u6982\u5ff5\u533a\u57df\uff0c\u65e0\u9700\u590d\u6742\u5e72\u9884\u6216\u663e\u8457\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u53bb\u504f\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u590d\u6742\u6216\u8ba1\u7b97\u5bc6\u96c6\u7684\u5e72\u9884\uff0c\u800cDeCoDi\u65e8\u5728\u4ec5\u6539\u53d8\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f7f\u5176\u5bf9\u66f4\u5e7f\u6cdb\u7684\u5b9e\u8df5\u8005\u66f4\u52a0\u53ef\u8bbf\u95ee\u3002", "method": "\u6539\u53d8\u6269\u6563\u8fc7\u7a0b\u4ee5\u907f\u514d\u6f5c\u5728\u7ef4\u5ea6\u4e2d\u7684\u504f\u89c1\u6982\u5ff5\u533a\u57df\uff0c\u8be5\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u4efb\u4f55\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u5bf9\u62a4\u58eb\u3001\u6d88\u9632\u5458\u548cCEO\u7b49\u6982\u5ff5\u8fdb\u884c\u6027\u522b\u3001\u79cd\u65cf\u548c\u5e74\u9f84\u53bb\u504f\uff0c\u4eba\u5de5\u8bc4\u4f301200\u5f20\u751f\u6210\u56fe\u50cf\u663e\u793a\u65b9\u6cd5\u6709\u6548\u3002GPT4o\u7684\u81ea\u52a8\u504f\u89c1\u8bc4\u4f30\u4e0e\u4eba\u5de5\u8bc4\u4f30\u65e0\u663e\u8457\u7edf\u8ba1\u5dee\u5f02\u3002", "conclusion": "DeCoDi\u65b9\u6cd5\u6709\u6f5c\u529b\u663e\u8457\u6539\u5584\u57fa\u4e8e\u6269\u6563\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u751f\u6210\u56fe\u50cf\u7684\u591a\u6837\u6027\uff0c\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u8bc4\u4f30\u8005\u95f4\u4e00\u81f4\u6027\u53ef\u9760\uff0c\u5bf9\u53d7\u4fdd\u62a4\u5c5e\u6027\u7684\u8986\u76d6\u66f4\u5168\u9762\u3002"}}
{"id": "2508.14931", "pdf": "https://arxiv.org/pdf/2508.14931", "abs": "https://arxiv.org/abs/2508.14931", "authors": ["Zahra TehraniNasab", "Amar Kumar", "Tal Arbel"], "title": "Pixels Under Pressure: Exploring Fine-Tuning Paradigms for Foundation Models in High-Resolution Medical Imaging", "categories": ["eess.IV", "cs.GR"], "comment": null, "summary": "Advancements in diffusion-based foundation models have improved text-to-image generation, yet most efforts have been limited to low-resolution settings. As high-resolution image synthesis becomes increasingly essential for various applications, particularly in medical imaging domains, fine-tuning emerges as a crucial mechanism for adapting these powerful pre-trained models to task-specific requirements and data distributions. In this work, we present a systematic study, examining the impact of various fine-tuning techniques on image generation quality when scaling to high resolution 512x512 pixels. We benchmark a diverse set of fine-tuning methods, including full fine-tuning strategies and parameter-efficient fine-tuning (PEFT). We dissect how different fine-tuning methods influence key quality metrics, including Fr\\'echet Inception Distance (FID), Vendi score, and prompt-image alignment. We also evaluate the utility of generated images in a downstream classification task under data-scarce conditions, demonstrating that specific fine-tuning strategies improve both generation fidelity and downstream performance when synthetic images are used for classifier training and evaluation on real images. Our code is accessible through the project website - https://tehraninasab.github.io/PixelUPressure/.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u4e0d\u540c\u5fae\u8c03\u6280\u672f\u5bf9512x512\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u6bd4\u8f83\u4e86\u5168\u5fae\u8c03\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u53d1\u73b0\u7279\u5b9a\u5fae\u8c03\u7b56\u7565\u80fd\u540c\u65f6\u63d0\u5347\u751f\u6210\u4fdd\u771f\u5ea6\u548c\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5408\u6210\u5728\u533b\u7597\u5f71\u50cf\u7b49\u9886\u57df\u7684\u91cd\u8981\u6027\u65e5\u76ca\u589e\u52a0\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u5fae\u8c03\u6280\u672f\u5c06\u9884\u8bad\u7ec3\u7684\u6269\u6563\u57fa\u7840\u6a21\u578b\u9002\u914d\u5230\u7279\u5b9a\u4efb\u52a1\u548c\u6570\u636e\u5206\u5e03\uff0c\u4ee5\u63d0\u5347\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002", "method": "\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u4e86\u591a\u79cd\u5fae\u8c03\u6280\u672f\uff0c\u5305\u62ec\u5168\u5fae\u8c03\u7b56\u7565\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u5fae\u8c03\u65b9\u6cd5\u5bf9FID\u3001Vendi\u5206\u6570\u548c\u63d0\u793a-\u56fe\u50cf\u5bf9\u9f50\u7b49\u5173\u952e\u8d28\u91cf\u6307\u6807\u7684\u5f71\u54cd\uff0c\u5e76\u5728\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u8bc4\u4f30\u751f\u6210\u56fe\u50cf\u5728\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7279\u5b9a\u7684\u5fae\u8c03\u7b56\u7565\u80fd\u591f\u540c\u65f6\u6539\u5584\u751f\u6210\u4fdd\u771f\u5ea6\u548c\u4e0b\u6e38\u6027\u80fd\uff0c\u5f53\u4f7f\u7528\u5408\u6210\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\u5668\u8bad\u7ec3\u5e76\u5728\u771f\u5b9e\u56fe\u50cf\u4e0a\u8bc4\u4f30\u65f6\uff0c\u8fd9\u4e9b\u7b56\u7565\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6548\u679c\u3002", "conclusion": "\u5fae\u8c03\u662f\u63d0\u5347\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u7279\u5b9a\u5fae\u8c03\u7b56\u7565\u80fd\u591f\u663e\u8457\u6539\u5584\u751f\u6210\u56fe\u50cf\u7684\u4fdd\u771f\u5ea6\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u4e3a\u533b\u7597\u5f71\u50cf\u7b49\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2508.15372", "pdf": "https://arxiv.org/pdf/2508.15372", "abs": "https://arxiv.org/abs/2508.15372", "authors": ["Xinshuang Liu", "Runfa Blark Li", "Keito Suzuki", "Truong Nguyen"], "title": "Image-Conditioned 3D Gaussian Splat Quantization", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has attracted considerable attention for enabling high-quality real-time rendering. Although 3DGS compression methods have been proposed for deployment on storage-constrained devices, two limitations hinder archival use: (1) they compress medium-scale scenes only to the megabyte range, which remains impractical for large-scale scenes or extensive scene collections; and (2) they lack mechanisms to accommodate scene changes after long-term archival. To address these limitations, we propose an Image-Conditioned Gaussian Splat Quantizer (ICGS-Quantizer) that substantially enhances compression efficiency and provides adaptability to scene changes after archiving. ICGS-Quantizer improves quantization efficiency by jointly exploiting inter-Gaussian and inter-attribute correlations and by using shared codebooks across all training scenes, which are then fixed and applied to previously unseen test scenes, eliminating the overhead of per-scene codebooks. This approach effectively reduces the storage requirements for 3DGS to the kilobyte range while preserving visual fidelity. To enable adaptability to post-archival scene changes, ICGS-Quantizer conditions scene decoding on images captured at decoding time. The encoding, quantization, and decoding processes are trained jointly, ensuring that the codes, which are quantized representations of the scene, are effective for conditional decoding. We evaluate ICGS-Quantizer on 3D scene compression and 3D scene updating. Experimental results show that ICGS-Quantizer consistently outperforms state-of-the-art methods in compression efficiency and adaptability to scene changes. Our code, model, and data will be publicly available on GitHub.", "AI": {"tldr": "\u63d0\u51faICGS-Quantizer\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u5229\u7528\u9ad8\u65af\u95f4\u548c\u5c5e\u6027\u95f4\u76f8\u5173\u6027\u3001\u4f7f\u7528\u8de8\u573a\u666f\u5171\u4eab\u7801\u672c\uff0c\u5c063D\u9ad8\u65af\u6e85\u5c04\u538b\u7f29\u5230\u5343\u5b57\u8282\u7ea7\u522b\uff0c\u5e76\u652f\u6301\u57fa\u4e8e\u56fe\u50cf\u7684\u6761\u4ef6\u89e3\u7801\u4ee5\u9002\u5e94\u573a\u666f\u53d8\u5316\u3002", "motivation": "\u73b0\u67093DGS\u538b\u7f29\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a(1)\u53ea\u80fd\u5c06\u4e2d\u7b49\u89c4\u6a21\u573a\u666f\u538b\u7f29\u5230\u5146\u5b57\u8282\u7ea7\u522b\uff0c\u5bf9\u5927\u89c4\u6a21\u573a\u666f\u6216\u573a\u666f\u96c6\u5408\u4ecd\u4e0d\u5b9e\u7528\uff1b(2)\u7f3a\u4e4f\u9002\u5e94\u957f\u671f\u5f52\u6863\u540e\u573a\u666f\u53d8\u5316\u7684\u673a\u5236\u3002", "method": "\u63d0\u51fa\u56fe\u50cf\u6761\u4ef6\u9ad8\u65af\u6e85\u5c04\u91cf\u5316\u5668(ICGS-Quantizer)\uff0c\u901a\u8fc7\u8054\u5408\u5229\u7528\u9ad8\u65af\u95f4\u548c\u5c5e\u6027\u95f4\u76f8\u5173\u6027\u3001\u4f7f\u7528\u56fa\u5b9a\u5171\u4eab\u7801\u672c\uff0c\u5e76\u57fa\u4e8e\u89e3\u7801\u65f6\u6355\u83b7\u7684\u56fe\u50cf\u8fdb\u884c\u6761\u4ef6\u89e3\u7801\u3002\u7f16\u7801\u3001\u91cf\u5316\u548c\u89e3\u7801\u8fc7\u7a0b\u8054\u5408\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cICGS-Quantizer\u5728\u538b\u7f29\u6548\u7387\u548c\u573a\u666f\u53d8\u5316\u9002\u5e94\u6027\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5c063DGS\u5b58\u50a8\u9700\u6c42\u964d\u4f4e\u5230\u5343\u5b57\u8282\u8303\u56f4\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "conclusion": "ICGS-Quantizer\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u6548\u7387\u5e76\u63d0\u4f9b\u4e86\u5f52\u6863\u540e\u573a\u666f\u53d8\u5316\u7684\u9002\u5e94\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u67093DGS\u538b\u7f29\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.15773", "pdf": "https://arxiv.org/pdf/2508.15773", "abs": "https://arxiv.org/abs/2508.15773", "authors": ["Gaurav Parmar", "Or Patashnik", "Daniil Ostashev", "Kuan-Chieh Wang", "Kfir Aberman", "Srinivasa Narasimhan", "Jun-Yan Zhu"], "title": "Scaling Group Inference for Diverse and High-Quality Generation", "categories": ["cs.CV", "cs.GR", "cs.LG"], "comment": "Project website: https://www.cs.cmu.edu/~group-inference, GitHub:   https://github.com/GaParmar/group-inference", "summary": "Generative models typically sample outputs independently, and recent inference-time guidance and scaling algorithms focus on improving the quality of individual samples. However, in real-world applications, users are often presented with a set of multiple images (e.g., 4-8) for each prompt, where independent sampling tends to lead to redundant results, limiting user choices and hindering idea exploration. In this work, we introduce a scalable group inference method that improves both the diversity and quality of a group of samples. We formulate group inference as a quadratic integer assignment problem: candidate outputs are modeled as graph nodes, and a subset is selected to optimize sample quality (unary term) while maximizing group diversity (binary term). To substantially improve runtime efficiency, we progressively prune the candidate set using intermediate predictions, allowing our method to scale up to large candidate sets. Extensive experiments show that our method significantly improves group diversity and quality compared to independent sampling baselines and recent inference algorithms. Our framework generalizes across a wide range of tasks, including text-to-image, image-to-image, image prompting, and video generation, enabling generative models to treat multiple outputs as cohesive groups rather than independent samples.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u7ec4\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7ec4\u63a8\u7406\u5efa\u6a21\u4e3a\u4e8c\u6b21\u6574\u6570\u5206\u914d\u95ee\u9898\uff0c\u5728\u63d0\u9ad8\u6837\u672c\u8d28\u91cf\u7684\u540c\u65f6\u6700\u5927\u5316\u7ec4\u591a\u6837\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u72ec\u7acb\u91c7\u6837\u5bfc\u81f4\u7684\u5197\u4f59\u95ee\u9898\u3002", "motivation": "\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\uff0c\u7528\u6237\u901a\u5e38\u9700\u8981\u83b7\u5f97\u4e00\u7ec4\u591a\u6837\u5316\u7684\u8f93\u51fa\uff08\u59824-8\u5f20\u56fe\u50cf\uff09\uff0c\u4f46\u4f20\u7edf\u751f\u6210\u6a21\u578b\u7684\u72ec\u7acb\u91c7\u6837\u5f80\u5f80\u5bfc\u81f4\u7ed3\u679c\u5197\u4f59\uff0c\u9650\u5236\u4e86\u7528\u6237\u9009\u62e9\u548c\u521b\u610f\u63a2\u7d22\u3002", "method": "\u5c06\u7ec4\u63a8\u7406\u5efa\u6a21\u4e3a\u4e8c\u6b21\u6574\u6570\u5206\u914d\u95ee\u9898\uff1a\u5019\u9009\u8f93\u51fa\u4f5c\u4e3a\u56fe\u8282\u70b9\uff0c\u901a\u8fc7\u4f18\u5316\u6837\u672c\u8d28\u91cf\uff08\u4e00\u5143\u9879\uff09\u548c\u6700\u5927\u5316\u7ec4\u591a\u6837\u6027\uff08\u4e8c\u5143\u9879\uff09\u6765\u9009\u62e9\u5b50\u96c6\uff1b\u91c7\u7528\u6e10\u8fdb\u5f0f\u526a\u679d\u7b56\u7565\u63d0\u9ad8\u8fd0\u884c\u6548\u7387\uff0c\u53ef\u6269\u5c55\u5230\u5927\u89c4\u6a21\u5019\u9009\u96c6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u72ec\u7acb\u91c7\u6837\u57fa\u7ebf\u548c\u6700\u65b0\u63a8\u7406\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7ec4\u591a\u6837\u6027\u548c\u8d28\u91cf\uff1b\u6846\u67b6\u53ef\u6cdb\u5316\u5230\u6587\u672c\u5230\u56fe\u50cf\u3001\u56fe\u50cf\u5230\u56fe\u50cf\u3001\u56fe\u50cf\u63d0\u793a\u548c\u89c6\u9891\u751f\u6210\u7b49\u591a\u79cd\u4efb\u52a1\u3002", "conclusion": "\u8be5\u6846\u67b6\u4f7f\u751f\u6210\u6a21\u578b\u80fd\u591f\u5c06\u591a\u4e2a\u8f93\u51fa\u89c6\u4e3a\u6709\u51dd\u805a\u529b\u7684\u7ec4\u800c\u975e\u72ec\u7acb\u6837\u672c\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u548c\u591a\u6837\u5316\u7684\u751f\u6210\u7ed3\u679c\u9009\u62e9\u3002"}}
{"id": "2508.15020", "pdf": "https://arxiv.org/pdf/2508.15020", "abs": "https://arxiv.org/abs/2508.15020", "authors": ["Susim Roy", "Anubhooti Jain", "Mayank Vatsa", "Richa Singh"], "title": "TAIGen: Training-Free Adversarial Image Generation via Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at ICCVW-CV4BIOM 2025", "summary": "Adversarial attacks from generative models often produce low-quality images and require substantial computational resources. Diffusion models, though capable of high-quality generation, typically need hundreds of sampling steps for adversarial generation. This paper introduces TAIGen, a training-free black-box method for efficient adversarial image generation. TAIGen produces adversarial examples using only 3-20 sampling steps from unconditional diffusion models. Our key finding is that perturbations injected during the mixing step interval achieve comparable attack effectiveness without processing all timesteps. We develop a selective RGB channel strategy that applies attention maps to the red channel while using GradCAM-guided perturbations on green and blue channels. This design preserves image structure while maximizing misclassification in target models. TAIGen maintains visual quality with PSNR above 30 dB across all tested datasets. On ImageNet with VGGNet as source, TAIGen achieves 70.6% success against ResNet, 80.8% against MNASNet, and 97.8% against ShuffleNet. The method generates adversarial examples 10x faster than existing diffusion-based attacks. Our method achieves the lowest robust accuracy, indicating it is the most impactful attack as the defense mechanism is least successful in purifying the images generated by TAIGen.", "AI": {"tldr": "TAIGen\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9ed1\u76d2\u5bf9\u6297\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ec5\u4f7f\u75283-20\u4e2a\u91c7\u6837\u6b65\u9aa4\u4ece\u65e0\u6761\u4ef6\u6269\u6563\u6a21\u578b\u4e2d\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u5bf9\u6297\u6837\u672c\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb10\u500d", "motivation": "\u89e3\u51b3\u751f\u6210\u6a21\u578b\u5bf9\u6297\u653b\u51fb\u4e2d\u56fe\u50cf\u8d28\u91cf\u4f4e\u3001\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u6269\u6563\u6a21\u578b\u9700\u8981\u6570\u767e\u4e2a\u91c7\u6837\u6b65\u9aa4\u7684\u5c40\u9650\u6027", "method": "\u5728\u6df7\u5408\u6b65\u9aa4\u533a\u95f4\u6ce8\u5165\u6270\u52a8\uff0c\u91c7\u7528\u9009\u62e9\u6027RGB\u901a\u9053\u7b56\u7565\uff1a\u5bf9\u7ea2\u8272\u901a\u9053\u5e94\u7528\u6ce8\u610f\u529b\u56fe\uff0c\u5bf9\u7eff\u8272\u548c\u84dd\u8272\u901a\u9053\u4f7f\u7528GradCAM\u5f15\u5bfc\u7684\u6270\u52a8\uff0c\u4fdd\u6301\u56fe\u50cf\u7ed3\u6784\u7684\u540c\u65f6\u6700\u5927\u5316\u76ee\u6807\u6a21\u578b\u7684\u8bef\u5206\u7c7b", "result": "\u5728ImageNet\u6570\u636e\u96c6\u4e0a\uff0c\u9488\u5bf9VGGNet\u6e90\u6a21\u578b\uff0c\u5bf9ResNet\u6210\u529f\u7387\u4e3a70.6%\uff0c\u5bf9MNASNet\u4e3a80.8%\uff0c\u5bf9ShuffleNet\u4e3a97.8%\uff1bPSNR\u8d85\u8fc730dB\uff0c\u89c6\u89c9\u8d28\u91cf\u4f18\u79c0", "conclusion": "TAIGen\u662f\u6700\u5177\u5f71\u54cd\u529b\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u9632\u5fa1\u673a\u5236\u5bf9\u5176\u751f\u6210\u7684\u56fe\u50cf\u51c0\u5316\u6548\u679c\u6700\u5dee\uff0c\u5b9e\u73b0\u4e86\u6700\u4f4e\u7684\u9c81\u68d2\u51c6\u786e\u7387"}}
{"id": "2508.15027", "pdf": "https://arxiv.org/pdf/2508.15027", "abs": "https://arxiv.org/abs/2508.15027", "authors": ["Chunming He", "Fengyang Xiao", "Rihan Zhang", "Chengyu Fang", "Deng-Ping Fan", "Sina Farsiu"], "title": "Reversible Unfolding Network for Concealed Visual Perception with Generative Refinement", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "18 pages, 21 tables, 13 figures", "summary": "Existing methods for concealed visual perception (CVP) often leverage reversible strategies to decrease uncertainty, yet these are typically confined to the mask domain, leaving the potential of the RGB domain underexplored. To address this, we propose a reversible unfolding network with generative refinement, termed RUN++. Specifically, RUN++ first formulates the CVP task as a mathematical optimization problem and unfolds the iterative solution into a multi-stage deep network. This approach provides a principled way to apply reversible modeling across both mask and RGB domains while leveraging a diffusion model to resolve the resulting uncertainty. Each stage of the network integrates three purpose-driven modules: a Concealed Object Region Extraction (CORE) module applies reversible modeling to the mask domain to identify core object regions; a Context-Aware Region Enhancement (CARE) module extends this principle to the RGB domain to foster better foreground-background separation; and a Finetuning Iteration via Noise-based Enhancement (FINE) module provides a final refinement. The FINE module introduces a targeted Bernoulli diffusion model that refines only the uncertain regions of the segmentation mask, harnessing the generative power of diffusion for fine-detail restoration without the prohibitive computational cost of a full-image process. This unique synergy, where the unfolding network provides a strong uncertainty prior for the diffusion model, allows RUN++ to efficiently direct its focus toward ambiguous areas, significantly mitigating false positives and negatives. Furthermore, we introduce a new paradigm for building robust CVP systems that remain effective under real-world degradations and extend this concept into a broader bi-level optimization framework.", "AI": {"tldr": "RUN++\u662f\u4e00\u4e2a\u53ef\u9006\u5c55\u5f00\u7f51\u7edc\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u7cbe\u70bc\u89e3\u51b3\u9690\u853d\u89c6\u89c9\u611f\u77e5\u95ee\u9898\uff0c\u5728\u63a9\u7801\u548cRGB\u53cc\u57df\u5e94\u7528\u53ef\u9006\u5efa\u6a21\uff0c\u5e76\u4f7f\u7528\u9488\u5bf9\u6027\u6269\u6563\u6a21\u578b\u4f18\u5316\u4e0d\u786e\u5b9a\u533a\u57df", "motivation": "\u73b0\u6709\u9690\u853d\u89c6\u89c9\u611f\u77e5\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u63a9\u7801\u57df\u7684\u53ef\u9006\u7b56\u7565\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528RGB\u57df\u7684\u6f5c\u529b\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u53cc\u57df\u540c\u65f6\u5e94\u7528\u53ef\u9006\u5efa\u6a21\u5e76\u6709\u6548\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5", "method": "\u5c06CVP\u4efb\u52a1\u516c\u5f0f\u5316\u4e3a\u6570\u5b66\u4f18\u5316\u95ee\u9898\u5e76\u5c55\u5f00\u4e3a\u591a\u9636\u6bb5\u6df1\u5ea6\u7f51\u7edc\uff0c\u5305\u542bCORE\u6a21\u5757\uff08\u63a9\u7801\u57df\u53ef\u9006\u5efa\u6a21\uff09\u3001CARE\u6a21\u5757\uff08RGB\u57df\u53ef\u9006\u5efa\u6a21\uff09\u548cFINE\u6a21\u5757\uff08\u9488\u5bf9\u6027Bernoulli\u6269\u6563\u6a21\u578b\u7cbe\u70bc\uff09", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u5c55\u5f00\u7f51\u7edc\u63d0\u4f9b\u5f3a\u4e0d\u786e\u5b9a\u6027\u5148\u9a8c\uff0c\u4f7f\u6269\u6563\u6a21\u578b\u80fd\u9ad8\u6548\u805a\u7126\u6a21\u7cca\u533a\u57df\uff0c\u663e\u8457\u51cf\u5c11\u5047\u9633\u6027\u548c\u5047\u9634\u6027\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u9000\u5316\u6761\u4ef6\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027", "conclusion": "RUN++\u901a\u8fc7\u53ef\u9006\u5c55\u5f00\u7f51\u7edc\u4e0e\u751f\u6210\u5f0f\u7cbe\u70bc\u7684\u72ec\u7279\u534f\u540c\uff0c\u5efa\u7acb\u4e86\u65b0\u7684CVP\u7cfb\u7edf\u8303\u5f0f\uff0c\u6269\u5c55\u4e3a\u53cc\u5c42\u4f18\u5316\u6846\u67b6\uff0c\u4e3a\u9690\u853d\u89c6\u89c9\u611f\u77e5\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.15093", "pdf": "https://arxiv.org/pdf/2508.15093", "abs": "https://arxiv.org/abs/2508.15093", "authors": ["Yan Luo", "Drake Du", "Hao Huang", "Yi Fang", "Mengyu Wang"], "title": "CurveFlow: Curvature-Guided Flow Matching for Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Existing rectified flow models are based on linear trajectories between data and noise distributions. This linearity enforces zero curvature, which can inadvertently force the image generation process through low-probability regions of the data manifold. A key question remains underexplored: how does the curvature of these trajectories correlate with the semantic alignment between generated images and their corresponding captions, i.e., instructional compliance? To address this, we introduce CurveFlow, a novel flow matching framework designed to learn smooth, non-linear trajectories by directly incorporating curvature guidance into the flow path. Our method features a robust curvature regularization technique that penalizes abrupt changes in the trajectory's intrinsic dynamics.Extensive experiments on MS COCO 2014 and 2017 demonstrate that CurveFlow achieves state-of-the-art performance in text-to-image generation, significantly outperforming both standard rectified flow variants and other non-linear baselines like Rectified Diffusion. The improvements are especially evident in semantic consistency metrics such as BLEU, METEOR, ROUGE, and CLAIR. This confirms that our curvature-aware modeling substantially enhances the model's ability to faithfully follow complex instructions while simultaneously maintaining high image quality. The code is made publicly available at https://github.com/Harvard-AI-and-Robotics-Lab/CurveFlow.", "AI": {"tldr": "CurveFlow\u901a\u8fc7\u5f15\u5165\u66f2\u7387\u5f15\u5bfc\u7684\u975e\u7ebf\u6027\u8f68\u8ff9\u6765\u6539\u8fdb\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff0c\u76f8\u6bd4\u7ebf\u6027\u8f68\u8ff9\u7684rectified flow\u6a21\u578b\u80fd\u66f4\u597d\u5730\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b", "motivation": "\u73b0\u6709\u7684rectified flow\u6a21\u578b\u4f7f\u7528\u7ebf\u6027\u8f68\u8ff9\uff0c\u5f3a\u5236\u96f6\u66f2\u7387\u53ef\u80fd\u5bfc\u81f4\u751f\u6210\u8fc7\u7a0b\u7ecf\u8fc7\u6570\u636e\u6d41\u5f62\u7684\u4f4e\u6982\u7387\u533a\u57df\uff0c\u5f71\u54cd\u8bed\u4e49\u5bf9\u9f50\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b", "method": "\u63d0\u51faCurveFlow\u6846\u67b6\uff0c\u901a\u8fc7\u66f2\u7387\u6b63\u5219\u5316\u6280\u672f\u5b66\u4e60\u5e73\u6ed1\u7684\u975e\u7ebf\u6027\u8f68\u8ff9\uff0c\u60e9\u7f5a\u8f68\u8ff9\u5185\u5728\u52a8\u6001\u7684\u7a81\u53d8", "result": "\u5728MS COCO\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5728BLEU\u3001METEOR\u3001ROUGE\u3001CLAIR\u7b49\u8bed\u4e49\u4e00\u81f4\u6027\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u6807\u51c6rectified flow\u53d8\u4f53\u548c\u5176\u4ed6\u975e\u7ebf\u6027\u57fa\u7ebf", "conclusion": "\u66f2\u7387\u611f\u77e5\u5efa\u6a21\u663e\u8457\u589e\u5f3a\u4e86\u6a21\u578b\u5fe0\u5b9e\u9075\u5faa\u590d\u6742\u6307\u4ee4\u7684\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u56fe\u50cf\u8d28\u91cf"}}
{"id": "2508.15168", "pdf": "https://arxiv.org/pdf/2508.15168", "abs": "https://arxiv.org/abs/2508.15168", "authors": ["Masato Ito", "Kaito Tanaka", "Keisuke Matsuda", "Aya Nakayama"], "title": "XDR-LVLM: An Explainable Vision-Language Large Model for Diabetic Retinopathy Diagnosis", "categories": ["cs.CV"], "comment": null, "summary": "Diabetic Retinopathy (DR) is a major cause of global blindness, necessitating early and accurate diagnosis. While deep learning models have shown promise in DR detection, their black-box nature often hinders clinical adoption due to a lack of transparency and interpretability. To address this, we propose XDR-LVLM (eXplainable Diabetic Retinopathy Diagnosis with LVLM), a novel framework that leverages Vision-Language Large Models (LVLMs) for high-precision DR diagnosis coupled with natural language-based explanations. XDR-LVLM integrates a specialized Medical Vision Encoder, an LVLM Core, and employs Multi-task Prompt Engineering and Multi-stage Fine-tuning to deeply understand pathological features within fundus images and generate comprehensive diagnostic reports. These reports explicitly include DR severity grading, identification of key pathological concepts (e.g., hemorrhages, exudates, microaneurysms), and detailed explanations linking observed features to the diagnosis. Extensive experiments on the Diabetic Retinopathy (DDR) dataset demonstrate that XDR-LVLM achieves state-of-the-art performance, with a Balanced Accuracy of 84.55% and an F1 Score of 79.92% for disease diagnosis, and superior results for concept detection (77.95% BACC, 66.88% F1). Furthermore, human evaluations confirm the high fluency, accuracy, and clinical utility of the generated explanations, showcasing XDR-LVLM's ability to bridge the gap between automated diagnosis and clinical needs by providing robust and interpretable insights.", "AI": {"tldr": "XDR-LVLM\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u5927\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8bca\u65ad\u548c\u900f\u660e\u5316\u62a5\u544a\u751f\u6210", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u533b\u7597\u8bca\u65ad\u4e2d\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u7684\u95ee\u9898\uff0c\u4fc3\u8fdb\u4e34\u5e8a\u91c7\u7528", "method": "\u6574\u5408\u4e13\u7528\u533b\u5b66\u89c6\u89c9\u7f16\u7801\u5668\u548cLVLM\u6838\u5fc3\uff0c\u91c7\u7528\u591a\u4efb\u52a1\u63d0\u793a\u5de5\u7a0b\u548c\u591a\u9636\u6bb5\u5fae\u8c03\uff0c\u4ece\u773c\u5e95\u56fe\u50cf\u4e2d\u7406\u89e3\u75c5\u7406\u7279\u5f81\u5e76\u751f\u6210\u5305\u542bDR\u5206\u7ea7\u3001\u5173\u952e\u75c5\u7406\u6982\u5ff5\u8bc6\u522b\u548c\u8be6\u7ec6\u89e3\u91ca\u7684\u8bca\u65ad\u62a5\u544a", "result": "\u5728DDR\u6570\u636e\u96c6\u4e0a\u8fbe\u523084.55%\u7684\u5e73\u8861\u51c6\u786e\u7387\u548c79.92%\u7684F1\u5206\u6570\uff0c\u6982\u5ff5\u68c0\u6d4b\u8fbe\u523077.95% BACC\u548c66.88% F1\uff0c\u4eba\u7c7b\u8bc4\u4f30\u786e\u8ba4\u751f\u6210\u89e3\u91ca\u7684\u9ad8\u6d41\u7545\u6027\u3001\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u5b9e\u7528\u6027", "conclusion": "XDR-LVLM\u80fd\u591f\u901a\u8fc7\u63d0\u4f9b\u7a33\u5065\u4e14\u53ef\u89e3\u91ca\u7684\u89c1\u89e3\uff0c\u5f25\u5408\u81ea\u52a8\u5316\u8bca\u65ad\u4e0e\u4e34\u5e8a\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd"}}
{"id": "2508.15169", "pdf": "https://arxiv.org/pdf/2508.15169", "abs": "https://arxiv.org/abs/2508.15169", "authors": ["Xuyang Chen", "Zhijun Zhai", "Kaixuan Zhou", "Zengmao Wang", "Jianan He", "Dong Wang", "Yanfeng Zhang", "mingwei Sun", "R\u00fcdiger Westermann", "Konrad Schindler", "Liqiu Meng"], "title": "MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View Consistent Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Mesh models have become increasingly accessible for numerous cities; however, the lack of realistic textures restricts their application in virtual urban navigation and autonomous driving. To address this, this paper proposes MeSS (Meshbased Scene Synthesis) for generating high-quality, styleconsistent outdoor scenes with city mesh models serving as the geometric prior. While image and video diffusion models can leverage spatial layouts (such as depth maps or HD maps) as control conditions to generate street-level perspective views, they are not directly applicable to 3D scene generation. Video diffusion models excel at synthesizing consistent view sequences that depict scenes but often struggle to adhere to predefined camera paths or align accurately with rendered control videos. In contrast, image diffusion models, though unable to guarantee cross-view visual consistency, can produce more geometry-aligned results when combined with ControlNet. Building on this insight, our approach enhances image diffusion models by improving cross-view consistency. The pipeline comprises three key stages: first, we generate geometrically consistent sparse views using Cascaded Outpainting ControlNets; second, we propagate denser intermediate views via a component dubbed AGInpaint; and third, we globally eliminate visual inconsistencies (e.g., varying exposure) using the GCAlign module. Concurrently with generation, a 3D Gaussian Splatting (3DGS) scene is reconstructed by initializing Gaussian balls on the mesh surface. Our method outperforms existing approaches in both geometric alignment and generation quality. Once synthesized, the scene can be rendered in diverse styles through relighting and style transfer techniques.", "AI": {"tldr": "MeSS\u65b9\u6cd5\u5229\u7528\u57ce\u5e02\u7f51\u683c\u6a21\u578b\u4f5c\u4e3a\u51e0\u4f55\u5148\u9a8c\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u56fe\u50cf\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u98ce\u683c\u4e00\u81f4\u7684\u5ba4\u5916\u573a\u666f\u7eb9\u7406\uff0c\u89e3\u51b3\u4e86\u73b0\u67093D\u57ce\u5e02\u6a21\u578b\u7f3a\u4e4f\u771f\u5b9e\u7eb9\u7406\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u57ce\u5e02\u7f51\u683c\u6a21\u578b\u7f3a\u4e4f\u771f\u5b9e\u7eb9\u7406\uff0c\u9650\u5236\u4e86\u5728\u865a\u62df\u57ce\u5e02\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5e94\u7528\u3002\u867d\u7136\u56fe\u50cf\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u5229\u7528\u7a7a\u95f4\u5e03\u5c40\u751f\u6210\u8857\u666f\u89c6\u89d2\uff0c\u4f46\u65e0\u6cd5\u76f4\u63a5\u7528\u4e8e3D\u573a\u666f\u751f\u6210\u6216\u4fdd\u8bc1\u89c6\u56fe\u4e00\u81f4\u6027\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u7ba1\u9053\uff1a1) \u4f7f\u7528\u7ea7\u8054\u5916\u7ed8ControlNet\u751f\u6210\u51e0\u4f55\u4e00\u81f4\u7684\u7a00\u758f\u89c6\u56fe\uff1b2) \u901a\u8fc7AGInpaint\u7ec4\u4ef6\u4f20\u64ad\u66f4\u5bc6\u96c6\u7684\u4e2d\u95f4\u89c6\u56fe\uff1b3) \u4f7f\u7528GCAlign\u6a21\u5757\u5168\u5c40\u6d88\u9664\u89c6\u89c9\u4e0d\u4e00\u81f4\u6027\u3002\u540c\u65f6\u57fa\u4e8e\u7f51\u683c\u8868\u9762\u521d\u59cb\u53163D\u9ad8\u65af\u6e85\u5c04\u573a\u666f\u91cd\u5efa\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u51e0\u4f55\u5bf9\u9f50\u548c\u751f\u6210\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u98ce\u683c\u4e00\u81f4\u7684\u5ba4\u5916\u573a\u666f\u7eb9\u7406\u3002", "conclusion": "MeSS\u6210\u529f\u89e3\u51b3\u4e86\u57ce\u5e02\u7f51\u683c\u6a21\u578b\u7684\u7eb9\u7406\u751f\u6210\u95ee\u9898\uff0c\u751f\u6210\u7684\u573a\u666f\u53ef\u901a\u8fc7\u91cd\u7167\u660e\u548c\u98ce\u683c\u8fc1\u79fb\u6280\u672f\u4ee5\u4e0d\u540c\u98ce\u683c\u6e32\u67d3\uff0c\u4e3a\u865a\u62df\u57ce\u5e02\u5bfc\u822a\u548c\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.15228", "pdf": "https://arxiv.org/pdf/2508.15228", "abs": "https://arxiv.org/abs/2508.15228", "authors": ["Ziang Cao", "Zhaoxi Chen", "Liang Pan", "Ziwei Liu"], "title": "Collaborative Multi-Modal Coding for High-Quality 3D Generation", "categories": ["cs.CV"], "comment": null, "summary": "3D content inherently encompasses multi-modal characteristics and can be projected into different modalities (e.g., RGB images, RGBD, and point clouds). Each modality exhibits distinct advantages in 3D asset modeling: RGB images contain vivid 3D textures, whereas point clouds define fine-grained 3D geometries. However, most existing 3D-native generative architectures either operate predominantly within single-modality paradigms-thus overlooking the complementary benefits of multi-modality data-or restrict themselves to 3D structures, thereby limiting the scope of available training datasets. To holistically harness multi-modalities for 3D modeling, we present TriMM, the first feed-forward 3D-native generative model that learns from basic multi-modalities (e.g., RGB, RGBD, and point cloud). Specifically, 1) TriMM first introduces collaborative multi-modal coding, which integrates modality-specific features while preserving their unique representational strengths. 2) Furthermore, auxiliary 2D and 3D supervision are introduced to raise the robustness and performance of multi-modal coding. 3) Based on the embedded multi-modal code, TriMM employs a triplane latent diffusion model to generate 3D assets of superior quality, enhancing both the texture and the geometric detail. Extensive experiments on multiple well-known datasets demonstrate that TriMM, by effectively leveraging multi-modality, achieves competitive performance with models trained on large-scale datasets, despite utilizing a small amount of training data. Furthermore, we conduct additional experiments on recent RGB-D datasets, verifying the feasibility of incorporating other multi-modal datasets into 3D generation.", "AI": {"tldr": "TriMM\u662f\u9996\u4e2a\u524d\u9988\u5f0f3D\u539f\u751f\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u534f\u4f5c\u7f16\u7801\uff08RGB\u3001RGBD\u3001\u70b9\u4e91\uff09\u548ctriplane\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u6709\u6548\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\u63d0\u53473D\u8d44\u4ea7\u751f\u6210\u7684\u7eb9\u7406\u548c\u51e0\u4f55\u7ec6\u8282\u8d28\u91cf\u3002", "motivation": "\u73b0\u67093D\u751f\u6210\u67b6\u6784\u5927\u591a\u5c40\u9650\u4e8e\u5355\u4e00\u6a21\u6001\u62163D\u7ed3\u6784\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u9650\u5236\u4e86\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u8303\u56f4\u548c\u751f\u6210\u8d28\u91cf\u3002", "method": "1\uff09\u5f15\u5165\u534f\u4f5c\u591a\u6a21\u6001\u7f16\u7801\u6574\u5408\u6a21\u6001\u7279\u5b9a\u7279\u5f81\uff1b2\uff09\u91c7\u7528\u8f85\u52a92D\u548c3D\u76d1\u7763\u63d0\u5347\u9c81\u68d2\u6027\uff1b3\uff09\u57fa\u4e8etriplane\u6f5c\u5728\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf3D\u8d44\u4ea7", "result": "\u5728\u591a\u4e2a\u77e5\u540d\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTriMM\u901a\u8fc7\u6709\u6548\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\uff0c\u5728\u4f7f\u7528\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u4e0e\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\u76f8\u7ade\u4e89\u7684\u6027\u80fd", "conclusion": "TriMM\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u6570\u636e\u57283D\u751f\u6210\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u6574\u5408\u5176\u4ed6\u591a\u6a21\u6001\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\u9a8c\u8bc1"}}
{"id": "2508.15233", "pdf": "https://arxiv.org/pdf/2508.15233", "abs": "https://arxiv.org/abs/2508.15233", "authors": ["Wenju Xu"], "title": "Pretrained Diffusion Models Are Inherently Skipped-Step Samplers", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Diffusion models have been achieving state-of-the-art results across various generation tasks. However, a notable drawback is their sequential generation process, requiring long-sequence step-by-step generation. Existing methods, such as DDIM, attempt to reduce sampling steps by constructing a class of non-Markovian diffusion processes that maintain the same training objective. However, there remains a gap in understanding whether the original diffusion process can achieve the same efficiency without resorting to non-Markovian processes. In this paper, we provide a confirmative answer and introduce skipped-step sampling, a mechanism that bypasses multiple intermediate denoising steps in the iterative generation process, in contrast with the traditional step-by-step refinement of standard diffusion inference. Crucially, we demonstrate that this skipped-step sampling mechanism is derived from the same training objective as the standard diffusion model, indicating that accelerated sampling via skipped-step sampling via a Markovian way is an intrinsic property of pretrained diffusion models. Additionally, we propose an enhanced generation method by integrating our accelerated sampling technique with DDIM. Extensive experiments on popular pretrained diffusion models, including the OpenAI ADM, Stable Diffusion, and Open Sora models, show that our method achieves high-quality generation with significantly reduced sampling steps.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8df3\u8fc7\u6b65\u91c7\u6837\u673a\u5236\uff0c\u53ef\u4ee5\u76f4\u63a5\u7ed5\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u591a\u4e2a\u4e2d\u95f4\u53bb\u566a\u6b65\u9aa4\uff0c\u5b9e\u73b0\u52a0\u901f\u91c7\u6837\uff0c\u4e14\u8be5\u65b9\u6cd5\u4e0e\u6807\u51c6\u6269\u6563\u6a21\u578b\u5177\u6709\u76f8\u540c\u7684\u8bad\u7ec3\u76ee\u6807\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u867d\u7136\u5728\u5404\u751f\u6210\u4efb\u52a1\u4e2d\u53d6\u5f97\u6700\u4f18\u6548\u679c\uff0c\u4f46\u5176\u987a\u5e8f\u751f\u6210\u8fc7\u7a0b\u9700\u8981\u591a\u6b65\u8fed\u4ee3\uff0c\u6548\u7387\u8f83\u4f4e\u3002\u73b0\u6709\u65b9\u6cd5\u5982DDIM\u901a\u8fc7\u975e\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\u51cf\u5c11\u91c7\u6837\u6b65\u6570\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u539f\u59cb\u6269\u6563\u8fc7\u7a0b\u80fd\u5426\u4ee5\u9a6c\u5c14\u53ef\u592b\u65b9\u5f0f\u5b9e\u73b0\u76f8\u540c\u6548\u7387\u7684\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u8df3\u8fc7\u6b65\u91c7\u6837\u673a\u5236\uff0c\u5141\u8bb8\u5728\u8fed\u4ee3\u751f\u6210\u8fc7\u7a0b\u4e2d\u7ed5\u8fc7\u591a\u4e2a\u4e2d\u95f4\u53bb\u566a\u6b65\u9aa4\uff0c\u800c\u4e0d\u662f\u4f20\u7edf\u7684\u9010\u6b65\u7ec6\u5316\u3002\u8be5\u65b9\u6cd5\u4ece\u4e0e\u6807\u51c6\u6269\u6563\u6a21\u578b\u76f8\u540c\u7684\u8bad\u7ec3\u76ee\u6807\u63a8\u5bfc\u800c\u6765\uff0c\u8868\u660e\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u65b9\u5f0f\u7684\u52a0\u901f\u91c7\u6837\u662f\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u5185\u5728\u5c5e\u6027\u3002", "result": "\u5728OpenAI ADM\u3001Stable Diffusion\u548cOpen Sora\u7b49\u6d41\u884c\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4ee5\u663e\u8457\u51cf\u5c11\u7684\u91c7\u6837\u6b65\u9aa4\u5b9e\u73b0\u9ad8\u8d28\u91cf\u751f\u6210\u3002", "conclusion": "\u8df3\u8fc7\u6b65\u91c7\u6837\u662f\u6269\u6563\u6a21\u578b\u7684\u5185\u5728\u7279\u6027\uff0c\u53ef\u4ee5\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u65b9\u5f0f\u5b9e\u73b0\u9ad8\u6548\u91c7\u6837\uff0c\u8be5\u65b9\u6cd5\u8fd8\u53ef\u4e0eDDIM\u7ed3\u5408\u8fdb\u4e00\u6b65\u63d0\u5347\u751f\u6210\u6548\u679c\u3002"}}
{"id": "2508.15298", "pdf": "https://arxiv.org/pdf/2508.15298", "abs": "https://arxiv.org/abs/2508.15298", "authors": ["Darya Taratynova", "Alya Almsouti", "Beknur Kalmakhanbet", "Numan Saeed", "Mohammad Yaqub"], "title": "TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect Classification", "categories": ["cs.CV"], "comment": null, "summary": "Congenital heart defect (CHD) detection in ultrasound videos is hindered by image noise and probe positioning variability. While automated methods can reduce operator dependence, current machine learning approaches often neglect temporal information, limit themselves to binary classification, and do not account for prediction calibration. We propose Temporal Prompt Alignment (TPA), a method leveraging foundation image-text model and prompt-aware contrastive learning to classify fetal CHD on cardiac ultrasound videos. TPA extracts features from each frame of video subclips using an image encoder, aggregates them with a trainable temporal extractor to capture heart motion, and aligns the video representation with class-specific text prompts via a margin-hinge contrastive loss. To enhance calibration for clinical reliability, we introduce a Conditional Variational Autoencoder Style Modulation (CVAESM) module, which learns a latent style vector to modulate embeddings and quantifies classification uncertainty. Evaluated on a private dataset for CHD detection and on a large public dataset, EchoNet-Dynamic, for systolic dysfunction, TPA achieves state-of-the-art macro F1 scores of 85.40% for CHD diagnosis, while also reducing expected calibration error by 5.38% and adaptive ECE by 6.8%. On EchoNet-Dynamic's three-class task, it boosts macro F1 by 4.73% (from 53.89% to 58.62%). Temporal Prompt Alignment (TPA) is a framework for fetal congenital heart defect (CHD) classification in ultrasound videos that integrates temporal modeling, prompt-aware contrastive learning, and uncertainty quantification.", "AI": {"tldr": "TPA\u662f\u4e00\u4e2a\u7528\u4e8e\u80ce\u513f\u5148\u5929\u6027\u5fc3\u810f\u75c5\u8d85\u58f0\u89c6\u9891\u5206\u7c7b\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u65f6\u5e8f\u5efa\u6a21\u3001\u63d0\u793a\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u5728CHD\u68c0\u6d4b\u548c\u5fc3\u810f\u529f\u80fd\u8bc4\u4f30\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u8d85\u58f0\u89c6\u9891CHD\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u56fe\u50cf\u566a\u58f0\u3001\u63a2\u5934\u4f4d\u7f6e\u53d8\u5f02\u6027\u95ee\u9898\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u65f6\u5e8f\u4fe1\u606f\u3001\u4ec5\u9650\u4e8e\u4e8c\u5143\u5206\u7c7b\u4e14\u7f3a\u4e4f\u9884\u6d4b\u6821\u51c6\u3002", "method": "TPA\u5229\u7528\u57fa\u7840\u56fe\u50cf-\u6587\u672c\u6a21\u578b\u548c\u63d0\u793a\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\uff0c\u901a\u8fc7\u56fe\u50cf\u7f16\u7801\u5668\u63d0\u53d6\u89c6\u9891\u5b50\u7247\u6bb5\u7279\u5f81\uff0c\u53ef\u8bad\u7ec3\u65f6\u5e8f\u63d0\u53d6\u5668\u805a\u5408\u7279\u5f81\u6355\u83b7\u5fc3\u810f\u8fd0\u52a8\uff0c\u5e76\u901a\u8fc7\u8fb9\u754c\u94f0\u94fe\u5bf9\u6bd4\u635f\u5931\u5c06\u89c6\u9891\u8868\u793a\u4e0e\u7c7b\u522b\u7279\u5b9a\u6587\u672c\u63d0\u793a\u5bf9\u9f50\u3002\u5f15\u5165CVAESM\u6a21\u5757\u5b66\u4e60\u6f5c\u5728\u98ce\u683c\u5411\u91cf\u6765\u8c03\u8282\u5d4c\u5165\u5e76\u91cf\u5316\u5206\u7c7b\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728CHD\u68c0\u6d4b\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u8fbe\u523085.40%\u7684\u5b8fF1\u5206\u6570\uff0c\u540c\u65f6\u5c06\u9884\u671f\u6821\u51c6\u8bef\u5dee\u964d\u4f4e5.38%\uff0c\u81ea\u9002\u5e94ECE\u964d\u4f4e6.8%\u3002\u5728EchoNet-Dynamic\u4e09\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0c\u5b8fF1\u63d0\u53474.73%\uff08\u4ece53.89%\u523058.62%\uff09\u3002", "conclusion": "TPA\u6846\u67b6\u901a\u8fc7\u6574\u5408\u65f6\u5e8f\u5efa\u6a21\u3001\u63d0\u793a\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u5728\u80ce\u513f\u5148\u5929\u6027\u5fc3\u810f\u75c5\u8d85\u58f0\u89c6\u9891\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u9ad8\u4e86\u4e34\u5e8a\u53ef\u9760\u6027\u3002"}}
{"id": "2508.15367", "pdf": "https://arxiv.org/pdf/2508.15367", "abs": "https://arxiv.org/abs/2508.15367", "authors": ["Jacinto Colan", "Ana Davila", "Yasuhisa Hasegawa"], "title": "Transfer learning optimization based on evolutionary selective fine tuning", "categories": ["cs.CV"], "comment": "Presented at the Workshop artiFicial And bio-inspIred netwoRked   intelliGence foR cOnstrained aUtoNomous Devices (FAIRGROUND). 2025   International Joint Conference on Neural Networks (IJCNN)", "summary": "Deep learning has shown substantial progress in image analysis. However, the computational demands of large, fully trained models remain a consideration. Transfer learning offers a strategy for adapting pre-trained models to new tasks. Traditional fine-tuning often involves updating all model parameters, which can potentially lead to overfitting and higher computational costs. This paper introduces BioTune, an evolutionary adaptive fine-tuning technique that selectively fine-tunes layers to enhance transfer learning efficiency. BioTune employs an evolutionary algorithm to identify a focused set of layers for fine-tuning, aiming to optimize model performance on a given target task. Evaluation across nine image classification datasets from various domains indicates that BioTune achieves competitive or improved accuracy and efficiency compared to existing fine-tuning methods such as AutoRGN and LoRA. By concentrating the fine-tuning process on a subset of relevant layers, BioTune reduces the number of trainable parameters, potentially leading to decreased computational cost and facilitating more efficient transfer learning across diverse data characteristics and distributions.", "AI": {"tldr": "BioTune\u662f\u4e00\u79cd\u8fdb\u5316\u81ea\u9002\u5e94\u5fae\u8c03\u6280\u672f\uff0c\u901a\u8fc7\u8fdb\u5316\u7b97\u6cd5\u9009\u62e9\u6027\u5730\u5fae\u8c03\u7279\u5b9a\u5c42\u6765\u63d0\u5347\u8fc1\u79fb\u5b66\u4e60\u6548\u7387\uff0c\u5728\u591a\u4e2a\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u9700\u8981\u66f4\u65b0\u6240\u6709\u6a21\u578b\u53c2\u6570\uff0c\u5bb9\u6613\u5bfc\u81f4\u8fc7\u62df\u5408\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u8fdb\u5316\u7b97\u6cd5\u8bc6\u522b\u9700\u8981\u5fae\u8c03\u7684\u5c42\u5b50\u96c6\uff0c\u53ea\u5bf9\u76f8\u5173\u5c42\u8fdb\u884c\u9009\u62e9\u6027\u5fae\u8c03\uff0c\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\u3002", "result": "\u57289\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cBioTune\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u90fd\u4f18\u4e8eAutoRGN\u548cLoRA\u7b49\u73b0\u6709\u5fae\u8c03\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u96c6\u4e2d\u5fae\u8c03\u76f8\u5173\u5c42\u5b50\u96c6\uff0cBioTune\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u4fc3\u8fdb\u4e86\u8de8\u4e0d\u540c\u6570\u636e\u7279\u5f81\u548c\u5206\u5e03\u7684\u9ad8\u6548\u8fc1\u79fb\u5b66\u4e60\u3002"}}
{"id": "2508.15376", "pdf": "https://arxiv.org/pdf/2508.15376", "abs": "https://arxiv.org/abs/2508.15376", "authors": ["Cong Wang", "Xianda Guo", "Wenbo Xu", "Wei Tian", "Ruiqi Song", "Chenming Zhang", "Lingxi Li", "Long Chen"], "title": "DriveSplat: Decoupled Driving Scene Reconstruction with Geometry-enhanced Partitioned Neural Gaussians", "categories": ["cs.CV"], "comment": null, "summary": "In the realm of driving scenarios, the presence of rapidly moving vehicles, pedestrians in motion, and large-scale static backgrounds poses significant challenges for 3D scene reconstruction. Recent methods based on 3D Gaussian Splatting address the motion blur problem by decoupling dynamic and static components within the scene. However, these decoupling strategies overlook background optimization with adequate geometry relationships and rely solely on fitting each training view by adding Gaussians. Therefore, these models exhibit limited robustness in rendering novel views and lack an accurate geometric representation. To address the above issues, we introduce DriveSplat, a high-quality reconstruction method for driving scenarios based on neural Gaussian representations with dynamic-static decoupling. To better accommodate the predominantly linear motion patterns of driving viewpoints, a region-wise voxel initialization scheme is employed, which partitions the scene into near, middle, and far regions to enhance close-range detail representation. Deformable neural Gaussians are introduced to model non-rigid dynamic actors, whose parameters are temporally adjusted by a learnable deformation network. The entire framework is further supervised by depth and normal priors from pre-trained models, improving the accuracy of geometric structures. Our method has been rigorously evaluated on the Waymo and KITTI datasets, demonstrating state-of-the-art performance in novel-view synthesis for driving scenarios.", "AI": {"tldr": "DriveSplat\u662f\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u9ad8\u65af\u8868\u793a\u7684\u52a8\u6001-\u9759\u6001\u89e3\u8026\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u9a7e\u9a76\u573a\u666f\u7684\u9ad8\u8d28\u91cf3D\u91cd\u5efa\uff0c\u901a\u8fc7\u533a\u57df\u5316\u4f53\u7d20\u521d\u59cb\u5316\u3001\u53ef\u53d8\u5f62\u795e\u7ecf\u9ad8\u65af\u548c\u6df1\u5ea6/\u6cd5\u7ebf\u5148\u9a8c\u76d1\u7763\uff0c\u5728Waymo\u548cKITTI\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u65b0\u89c6\u89d2\u5408\u6210\u6027\u80fd\u3002", "motivation": "\u9a7e\u9a76\u573a\u666f\u4e2d\u5b58\u5728\u5feb\u901f\u79fb\u52a8\u7684\u8f66\u8f86\u3001\u884c\u4eba\u548c\u5927\u89c4\u6a21\u9759\u6001\u80cc\u666f\uff0c\u7ed93D\u573a\u666f\u91cd\u5efa\u5e26\u6765\u6311\u6218\u3002\u73b0\u6709\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u65b9\u6cd5\u901a\u8fc7\u89e3\u8026\u52a8\u6001\u548c\u9759\u6001\u7ec4\u4ef6\u6765\u89e3\u51b3\u8fd0\u52a8\u6a21\u7cca\u95ee\u9898\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5ffd\u89c6\u4e86\u5177\u6709\u5145\u5206\u51e0\u4f55\u5173\u7cfb\u7684\u80cc\u666f\u4f18\u5316\uff0c\u4ec5\u901a\u8fc7\u6dfb\u52a0\u9ad8\u65af\u6765\u62df\u5408\u6bcf\u4e2a\u8bad\u7ec3\u89c6\u56fe\uff0c\u5bfc\u81f4\u65b0\u89c6\u89d2\u6e32\u67d3\u9c81\u68d2\u6027\u6709\u9650\u4e14\u7f3a\u4e4f\u51c6\u786e\u7684\u51e0\u4f55\u8868\u793a\u3002", "method": "1) \u91c7\u7528\u533a\u57df\u5316\u4f53\u7d20\u521d\u59cb\u5316\u65b9\u6848\uff0c\u5c06\u573a\u666f\u5212\u5206\u4e3a\u8fd1\u3001\u4e2d\u3001\u8fdc\u533a\u57df\u4ee5\u589e\u5f3a\u8fd1\u8ddd\u79bb\u7ec6\u8282\u8868\u793a\uff1b2) \u5f15\u5165\u53ef\u53d8\u5f62\u795e\u7ecf\u9ad8\u65af\u6765\u5efa\u6a21\u975e\u521a\u6027\u52a8\u6001\u5bf9\u8c61\uff0c\u5176\u53c2\u6570\u901a\u8fc7\u53ef\u5b66\u4e60\u53d8\u5f62\u7f51\u7edc\u8fdb\u884c\u65f6\u95f4\u8c03\u6574\uff1b3) \u6574\u4e2a\u6846\u67b6\u901a\u8fc7\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6df1\u5ea6\u548c\u6cd5\u7ebf\u5148\u9a8c\u8fdb\u884c\u76d1\u7763\uff0c\u63d0\u9ad8\u51e0\u4f55\u7ed3\u6784\u7684\u51c6\u786e\u6027\u3002", "result": "\u5728Waymo\u548cKITTI\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u4e25\u683c\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u5728\u9a7e\u9a76\u573a\u666f\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u9762\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "DriveSplat\u901a\u8fc7\u521b\u65b0\u7684\u52a8\u6001-\u9759\u6001\u89e3\u8026\u7b56\u7565\u548c\u51e0\u4f55\u5148\u9a8c\u76d1\u7763\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9a7e\u9a76\u573a\u666f3D\u91cd\u5efa\u4e2d\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u65b0\u89c6\u89d2\u5408\u6210\u548c\u51c6\u786e\u7684\u51e0\u4f55\u8868\u793a\u3002"}}
{"id": "2508.15457", "pdf": "https://arxiv.org/pdf/2508.15457", "abs": "https://arxiv.org/abs/2508.15457", "authors": ["Zongqi He", "Hanmin Li", "Kin-Chung Chan", "Yushen Zuo", "Hao Xie", "Zhe Xiao", "Jun Xiao", "Kin-Man Lam"], "title": "Enhancing Novel View Synthesis from extremely sparse views with SfM-free 3D Gaussian Splatting Framework", "categories": ["cs.CV"], "comment": "13 pages, 4 figures", "summary": "3D Gaussian Splatting (3DGS) has demonstrated remarkable real-time performance in novel view synthesis, yet its effectiveness relies heavily on dense multi-view inputs with precisely known camera poses, which are rarely available in real-world scenarios. When input views become extremely sparse, the Structure-from-Motion (SfM) method that 3DGS depends on for initialization fails to accurately reconstruct the 3D geometric structures of scenes, resulting in degraded rendering quality. In this paper, we propose a novel SfM-free 3DGS-based method that jointly estimates camera poses and reconstructs 3D scenes from extremely sparse-view inputs. Specifically, instead of SfM, we propose a dense stereo module to progressively estimates camera pose information and reconstructs a global dense point cloud for initialization. To address the inherent problem of information scarcity in extremely sparse-view settings, we propose a coherent view interpolation module that interpolates camera poses based on training view pairs and generates viewpoint-consistent content as additional supervision signals for training. Furthermore, we introduce multi-scale Laplacian consistent regularization and adaptive spatial-aware multi-scale geometry regularization to enhance the quality of geometrical structures and rendered content. Experiments show that our method significantly outperforms other state-of-the-art 3DGS-based approaches, achieving a remarkable 2.75dB improvement in PSNR under extremely sparse-view conditions (using only 2 training views). The images synthesized by our method exhibit minimal distortion while preserving rich high-frequency details, resulting in superior visual quality compared to existing techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700SfM\u76843D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u6781\u7a00\u758f\u89c6\u89d2\u8f93\u5165\u4e0b\u8054\u5408\u4f30\u8ba1\u76f8\u673a\u4f4d\u59ff\u548c\u91cd\u5efa3D\u573a\u666f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u8d28\u91cf", "motivation": "\u4f20\u7edf3DGS\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u5bc6\u96c6\u591a\u89c6\u89d2\u8f93\u5165\u548c\u7cbe\u786e\u76f8\u673a\u4f4d\u59ff\uff0c\u5728\u6781\u7a00\u758f\u89c6\u89d2\u4e0bSfM\u521d\u59cb\u5316\u5931\u8d25\u5bfc\u81f4\u6e32\u67d3\u8d28\u91cf\u4e0b\u964d", "method": "\u4f7f\u7528\u7a20\u5bc6\u7acb\u4f53\u6a21\u5757\u9010\u6b65\u4f30\u8ba1\u76f8\u673a\u4f4d\u59ff\u5e76\u91cd\u5efa\u5168\u5c40\u7a20\u5bc6\u70b9\u4e91\u521d\u59cb\u5316\uff1b\u63d0\u51fa\u8fde\u8d2f\u89c6\u89d2\u63d2\u503c\u6a21\u5757\u751f\u6210\u989d\u5916\u76d1\u7763\u4fe1\u53f7\uff1b\u5f15\u5165\u591a\u5c3a\u5ea6\u62c9\u666e\u62c9\u65af\u4e00\u81f4\u6027\u6b63\u5219\u5316\u548c\u81ea\u9002\u5e94\u7a7a\u95f4\u611f\u77e5\u591a\u5c3a\u5ea6\u51e0\u4f55\u6b63\u5219\u5316", "result": "\u5728\u6781\u7a00\u758f\u89c6\u89d2\u6761\u4ef6\u4e0b\uff08\u4ec5\u4f7f\u75282\u4e2a\u8bad\u7ec3\u89c6\u89d2\uff09PSNR\u663e\u8457\u63d0\u53472.75dB\uff0c\u5408\u6210\u56fe\u50cf\u5931\u771f\u6700\u5c0f\u4e14\u4fdd\u7559\u4e30\u5bcc\u9ad8\u9891\u7ec6\u8282", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6781\u7a00\u758f\u89c6\u89d2\u8f93\u5165\u4e0b\u5b9e\u73b0\u4e86\u5353\u8d8a\u76843D\u91cd\u5efa\u548c\u6e32\u67d3\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.15535", "pdf": "https://arxiv.org/pdf/2508.15535", "abs": "https://arxiv.org/abs/2508.15535", "authors": ["Guotao Liang", "Juncheng Hu", "Ximing Xing", "Jing Zhang", "Qian Yu"], "title": "Multi-Object Sketch Animation with Grouping and Motion Trajectory Priors", "categories": ["cs.CV"], "comment": "Accepted by ACM MM 2025", "summary": "We introduce GroupSketch, a novel method for vector sketch animation that effectively handles multi-object interactions and complex motions. Existing approaches struggle with these scenarios, either being limited to single-object cases or suffering from temporal inconsistency and poor generalization. To address these limitations, our method adopts a two-stage pipeline comprising Motion Initialization and Motion Refinement. In the first stage, the input sketch is interactively divided into semantic groups and key frames are defined, enabling the generation of a coarse animation via interpolation. In the second stage, we propose a Group-based Displacement Network (GDN), which refines the coarse animation by predicting group-specific displacement fields, leveraging priors from a text-to-video model. GDN further incorporates specialized modules, such as Context-conditioned Feature Enhancement (CCFE), to improve temporal consistency. Extensive experiments demonstrate that our approach significantly outperforms existing methods in generating high-quality, temporally consistent animations for complex, multi-object sketches, thus expanding the practical applications of sketch animation.", "AI": {"tldr": "GroupSketch\u662f\u4e00\u79cd\u65b0\u9896\u7684\u77e2\u91cf\u8349\u56fe\u52a8\u753b\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u6d41\u7a0b\uff08\u8fd0\u52a8\u521d\u59cb\u5316\u548c\u8fd0\u52a8\u7ec6\u5316\uff09\u6709\u6548\u5904\u7406\u591a\u76ee\u6807\u4ea4\u4e92\u548c\u590d\u6742\u8fd0\u52a8\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u591a\u76ee\u6807\u4ea4\u4e92\u548c\u590d\u6742\u8fd0\u52a8\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u8981\u4e48\u4ec5\u9650\u4e8e\u5355\u76ee\u6807\u60c5\u51b5\uff0c\u8981\u4e48\u5b58\u5728\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a1\uff09\u8fd0\u52a8\u521d\u59cb\u5316\u9636\u6bb5\u4ea4\u4e92\u5f0f\u5206\u5272\u8bed\u4e49\u7ec4\u5e76\u5b9a\u4e49\u5173\u952e\u5e27\uff0c\u901a\u8fc7\u63d2\u503c\u751f\u6210\u7c97\u7565\u52a8\u753b\uff1b2\uff09\u8fd0\u52a8\u7ec6\u5316\u9636\u6bb5\u4f7f\u7528\u57fa\u4e8e\u7ec4\u7684\u4f4d\u79fb\u7f51\u7edc\uff08GDN\uff09\u9884\u6d4b\u7ec4\u7279\u5b9a\u4f4d\u79fb\u573a\uff0c\u5e76\u6574\u5408\u4e0a\u4e0b\u6587\u6761\u4ef6\u7279\u5f81\u589e\u5f3a\uff08CCFE\uff09\u7b49\u6a21\u5757\u63d0\u5347\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u590d\u6742\u591a\u76ee\u6807\u8349\u56fe\u7684\u9ad8\u8d28\u91cf\u3001\u65f6\u95f4\u4e00\u81f4\u52a8\u753b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GroupSketch\u6269\u5c55\u4e86\u8349\u56fe\u52a8\u753b\u7684\u5b9e\u9645\u5e94\u7528\u8303\u56f4\uff0c\u4e3a\u5904\u7406\u591a\u76ee\u6807\u4ea4\u4e92\u548c\u590d\u6742\u8fd0\u52a8\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.15641", "pdf": "https://arxiv.org/pdf/2508.15641", "abs": "https://arxiv.org/abs/2508.15641", "authors": ["Pengcheng Fang", "Yuxia Chen", "Rui Guo"], "title": "When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Understanding videos requires more than answering open ended questions, it demands the ability to pinpoint when events occur and how entities interact across time. While recent Video LLMs have achieved remarkable progress in holistic reasoning, they remain coarse in temporal perception: timestamps are encoded only implicitly, frame level features are weak in capturing continuity, and language vision alignment often drifts from the entities of interest. In this paper, we present Grounded VideoDiT, a Video LLM designed to overcome these limitations by introducing three key innovations. First, a Diffusion Temporal Latent (DTL) encoder enhances boundary sensitivity and maintains temporal consistency. Second, object grounded representations explicitly bind query entities to localized visual evidence, strengthening alignment. Third, a mixed token scheme with discrete temporal tokens provides explicit timestamp modeling, enabling fine grained temporal reasoning. Together, these designs equip Grounded VideoDiT with robust grounding capabilities, as validated by state of the art results on Charades STA, NExT GQA, and multiple VideoQA benchmarks.", "AI": {"tldr": "Grounded VideoDiT\u662f\u4e00\u4e2a\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6269\u6563\u65f6\u95f4\u6f5c\u5728\u7f16\u7801\u5668\u3001\u5bf9\u8c61\u63a5\u5730\u8868\u793a\u548c\u6df7\u5408\u4ee4\u724c\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u65f6\u95f4\u611f\u77e5\u548c\u5b9e\u4f53\u4ea4\u4e92\u7684\u7cbe\u786e\u5ea6\u3002", "motivation": "\u73b0\u6709\u89c6\u9891LLM\u5728\u65f6\u95f4\u611f\u77e5\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff1a\u65f6\u95f4\u6233\u7f16\u7801\u9690\u5f0f\u3001\u5e27\u7ea7\u7279\u5f81\u8fde\u7eed\u6027\u5dee\u3001\u8bed\u8a00\u89c6\u89c9\u5bf9\u9f50\u6f02\u79fb\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u65f6\u95f4\u5b9a\u4f4d\u548c\u5b9e\u4f53\u4ea4\u4e92\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1\uff09\u6269\u6563\u65f6\u95f4\u6f5c\u5728\u7f16\u7801\u5668\u589e\u5f3a\u8fb9\u754c\u654f\u611f\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\uff1b2\uff09\u5bf9\u8c61\u63a5\u5730\u8868\u793a\u5c06\u67e5\u8be2\u5b9e\u4f53\u663e\u5f0f\u7ed1\u5b9a\u5230\u5c40\u90e8\u89c6\u89c9\u8bc1\u636e\uff1b3\uff09\u6df7\u5408\u4ee4\u724c\u65b9\u6848\u4f7f\u7528\u79bb\u6563\u65f6\u95f4\u4ee4\u724c\u8fdb\u884c\u663e\u5f0f\u65f6\u95f4\u6233\u5efa\u6a21\u3002", "result": "\u5728Charades STA\u3001NExT GQA\u548c\u591a\u4e2aVideoQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u5f3a\u5927\u63a5\u5730\u80fd\u529b\u3002", "conclusion": "Grounded VideoDiT\u901a\u8fc7\u521b\u65b0\u7684\u65f6\u95f4\u7f16\u7801\u548c\u5b9e\u4f53\u7ed1\u5b9a\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u65f6\u95f4\u611f\u77e5\u7cbe\u5ea6\u95ee\u9898\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u65f6\u95f4\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.15720", "pdf": "https://arxiv.org/pdf/2508.15720", "abs": "https://arxiv.org/abs/2508.15720", "authors": ["Zhiheng Liu", "Xueqing Deng", "Shoufa Chen", "Angtian Wang", "Qiushan Guo", "Mingfei Han", "Zeyue Xue", "Mengzhao Chen", "Ping Luo", "Linjie Yang"], "title": "WorldWeaver: Generating Long-Horizon Video Worlds via Rich Perception", "categories": ["cs.CV"], "comment": "Project page: https://johanan528.github.io/worldweaver_web/", "summary": "Generative video modeling has made significant strides, yet ensuring structural and temporal consistency over long sequences remains a challenge. Current methods predominantly rely on RGB signals, leading to accumulated errors in object structure and motion over extended durations. To address these issues, we introduce WorldWeaver, a robust framework for long video generation that jointly models RGB frames and perceptual conditions within a unified long-horizon modeling scheme. Our training framework offers three key advantages. First, by jointly predicting perceptual conditions and color information from a unified representation, it significantly enhances temporal consistency and motion dynamics. Second, by leveraging depth cues, which we observe to be more resistant to drift than RGB, we construct a memory bank that preserves clearer contextual information, improving quality in long-horizon video generation. Third, we employ segmented noise scheduling for training prediction groups, which further mitigates drift and reduces computational cost. Extensive experiments on both diffusion- and rectified flow-based models demonstrate the effectiveness of WorldWeaver in reducing temporal drift and improving the fidelity of generated videos.", "AI": {"tldr": "WorldWeaver\u662f\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u957f\u89c6\u9891\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21RGB\u5e27\u548c\u611f\u77e5\u6761\u4ef6\u6765\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u8fd0\u52a8\u52a8\u6001\uff0c\u5229\u7528\u6df1\u5ea6\u7ebf\u7d22\u51cf\u5c11\u6f02\u79fb\uff0c\u5e76\u91c7\u7528\u5206\u6bb5\u566a\u58f0\u8c03\u5ea6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u89c6\u9891\u5efa\u6a21\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56RGB\u4fe1\u53f7\uff0c\u5728\u957f\u5e8f\u5217\u4e2d\u4f1a\u5bfc\u81f4\u5bf9\u8c61\u7ed3\u6784\u548c\u8fd0\u52a8\u8bef\u5dee\u7d2f\u79ef\uff0c\u9700\u8981\u89e3\u51b3\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7ed3\u6784\u4fdd\u6301\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u957f\u65f6\u57df\u5efa\u6a21\u65b9\u6848\uff0c\u8054\u5408\u9884\u6d4b\u611f\u77e5\u6761\u4ef6\u548c\u989c\u8272\u4fe1\u606f\uff1b\u5229\u7528\u6df1\u5ea6\u7ebf\u7d22\u6784\u5efa\u8bb0\u5fc6\u5e93\u4fdd\u6301\u4e0a\u4e0b\u6587\u4fe1\u606f\uff1b\u91c7\u7528\u5206\u6bb5\u566a\u58f0\u8c03\u5ea6\u8bad\u7ec3\u9884\u6d4b\u7ec4\u3002", "result": "\u5728\u57fa\u4e8e\u6269\u6563\u548c\u6574\u6d41\u6d41\u7684\u6a21\u578b\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660eWorldWeaver\u80fd\u6709\u6548\u51cf\u5c11\u65f6\u95f4\u6f02\u79fb\u5e76\u63d0\u9ad8\u751f\u6210\u89c6\u9891\u7684\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u8054\u5408\u5efa\u6a21\u548c\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u89c6\u9891\u751f\u6210\u7684\u8d28\u91cf\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.15761", "pdf": "https://arxiv.org/pdf/2508.15761", "abs": "https://arxiv.org/abs/2508.15761", "authors": ["Yifu Zhang", "Hao Yang", "Yuqi Zhang", "Yifei Hu", "Fengda Zhu", "Chuang Lin", "Xiaofeng Mei", "Yi Jiang", "Zehuan Yuan", "Bingyue Peng"], "title": "Waver: Wave Your Way to Lifelike Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "We present Waver, a high-performance foundation model for unified image and video generation. Waver can directly generate videos with durations ranging from 5 to 10 seconds at a native resolution of 720p, which are subsequently upscaled to 1080p. The model simultaneously supports text-to-video (T2V), image-to-video (I2V), and text-to-image (T2I) generation within a single, integrated framework. We introduce a Hybrid Stream DiT architecture to enhance modality alignment and accelerate training convergence. To ensure training data quality, we establish a comprehensive data curation pipeline and manually annotate and train an MLLM-based video quality model to filter for the highest-quality samples. Furthermore, we provide detailed training and inference recipes to facilitate the generation of high-quality videos. Building on these contributions, Waver excels at capturing complex motion, achieving superior motion amplitude and temporal consistency in video synthesis. Notably, it ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial Analysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming existing open-source models and matching or surpassing state-of-the-art commercial solutions. We hope this technical report will help the community more efficiently train high-quality video generation models and accelerate progress in video generation technologies. Official page: https://github.com/FoundationVision/Waver.", "AI": {"tldr": "Waver\u662f\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u7edf\u4e00\u8fdb\u884c\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\uff0c\u652f\u6301\u6587\u672c\u5230\u89c6\u9891\u3001\u56fe\u50cf\u5230\u89c6\u9891\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6392\u540d\u524d\u4e09\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u8fd0\u52a8\u6355\u6349\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u591a\u6a21\u6001\u7edf\u4e00\u751f\u6210\u65b9\u9762\u7684\u6311\u6218\uff0c\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u5904\u7406\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u7684\u9ad8\u6027\u80fd\u57fa\u7840\u6a21\u578b\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6d41DiT\u67b6\u6784\u589e\u5f3a\u6a21\u6001\u5bf9\u9f50\u548c\u52a0\u901f\u8bad\u7ec3\u6536\u655b\uff0c\u5efa\u7acb\u5168\u9762\u7684\u6570\u636e\u7b5b\u9009\u6d41\u7a0b\uff0c\u4f7f\u7528MLLM\u89c6\u9891\u8d28\u91cf\u6a21\u578b\u8fc7\u6ee4\u9ad8\u8d28\u91cf\u6837\u672c\uff0c\u5e76\u63d0\u4f9b\u8be6\u7ec6\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u65b9\u6848\u3002", "result": "\u6a21\u578b\u80fd\u591f\u76f4\u63a5\u751f\u62105-10\u79d2\u7684720p\u539f\u751f\u5206\u8fa8\u7387\u89c6\u9891\uff08\u53ef\u5347\u7ea7\u81f31080p\uff09\uff0c\u5728\u590d\u6742\u8fd0\u52a8\u6355\u6349\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5728T2V\u548cI2V\u6392\u884c\u699c\u4e0a\u5747\u4f4d\u5217\u524d\u4e09\uff0c\u8d85\u8d8a\u73b0\u6709\u5f00\u6e90\u6a21\u578b\u5e76\u5ab2\u7f8e\u5546\u4e1a\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "Waver\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u9ad8\u6548\u8bad\u7ec3\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u6280\u672f\u8def\u5f84\uff0c\u6709\u671b\u52a0\u901f\u89c6\u9891\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\uff0c\u76f8\u5173\u4ee3\u7801\u5df2\u5f00\u6e90\u4f9b\u7814\u7a76\u4f7f\u7528\u3002"}}
{"id": "2508.15772", "pdf": "https://arxiv.org/pdf/2508.15772", "abs": "https://arxiv.org/abs/2508.15772", "authors": ["Qingyang Mao", "Qi Cai", "Yehao Li", "Yingwei Pan", "Mingyue Cheng", "Ting Yao", "Qi Liu", "Tao Mei"], "title": "Visual Autoregressive Modeling for Instruction-Guided Image Editing", "categories": ["cs.CV", "cs.MM"], "comment": "Source codes and models are available at   https://github.com/HiDream-ai/VAREdit", "summary": "Recent advances in diffusion models have brought remarkable visual fidelity to instruction-guided image editing. However, their global denoising process inherently entangles the edited region with the entire image context, leading to unintended spurious modifications and compromised adherence to editing instructions. In contrast, autoregressive models offer a distinct paradigm by formulating image synthesis as a sequential process over discrete visual tokens. Their causal and compositional mechanism naturally circumvents the adherence challenges of diffusion-based methods. In this paper, we present VAREdit, a visual autoregressive (VAR) framework that reframes image editing as a next-scale prediction problem. Conditioned on source image features and text instructions, VAREdit generates multi-scale target features to achieve precise edits. A core challenge in this paradigm is how to effectively condition the source image tokens. We observe that finest-scale source features cannot effectively guide the prediction of coarser target features. To bridge this gap, we introduce a Scale-Aligned Reference (SAR) module, which injects scale-matched conditioning information into the first self-attention layer. VAREdit demonstrates significant advancements in both editing adherence and efficiency. On standard benchmarks, it outperforms leading diffusion-based methods by 30\\%+ higher GPT-Balance score. Moreover, it completes a $512\\times512$ editing in 1.2 seconds, making it 2.2$\\times$ faster than the similarly sized UltraEdit. The models are available at https://github.com/HiDream-ai/VAREdit.", "AI": {"tldr": "VAREdit\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u7684\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u9884\u6d4b\u548c\u5c3a\u5ea6\u5bf9\u9f50\u53c2\u8003\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u6307\u4ee4\u5f15\u5bfc\u7f16\u8f91\u4e2d\u7684\u5168\u5c40\u7ea0\u7f20\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u7f16\u8f91\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u6307\u4ee4\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u4e2d\u5b58\u5728\u5168\u5c40\u53bb\u566a\u8fc7\u7a0b\u5bfc\u81f4\u7f16\u8f91\u533a\u57df\u4e0e\u6574\u4e2a\u56fe\u50cf\u4e0a\u4e0b\u6587\u7ea0\u7f20\u7684\u95ee\u9898\uff0c\u4f1a\u4ea7\u751f\u610f\u5916\u7684\u4f2a\u4fee\u6539\u5e76\u5f71\u54cd\u5bf9\u7f16\u8f91\u6307\u4ee4\u7684\u9075\u5faa\u3002\u81ea\u56de\u5f52\u6a21\u578b\u901a\u8fc7\u5e8f\u5217\u5316\u5408\u6210\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u7ec4\u5408\u673a\u5236\u3002", "method": "\u63d0\u51faVAREdit\u6846\u67b6\uff0c\u5c06\u56fe\u50cf\u7f16\u8f91\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e0b\u4e00\u5c3a\u5ea6\u9884\u6d4b\u95ee\u9898\u3002\u901a\u8fc7\u6e90\u56fe\u50cf\u7279\u5f81\u548c\u6587\u672c\u6307\u4ee4\u6761\u4ef6\u5316\uff0c\u751f\u6210\u591a\u5c3a\u5ea6\u76ee\u6807\u7279\u5f81\u3002\u5f15\u5165\u5c3a\u5ea6\u5bf9\u9f50\u53c2\u8003(SAR)\u6a21\u5757\uff0c\u5728\u7b2c\u4e00\u4e2a\u81ea\u6ce8\u610f\u529b\u5c42\u6ce8\u5165\u5c3a\u5ea6\u5339\u914d\u7684\u6761\u4ef6\u4fe1\u606f\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVAREdit\u6bd4\u9886\u5148\u7684\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u9ad8\u51fa30%\u4ee5\u4e0a\u7684GPT-Balance\u5206\u6570\u3002\u5b8c\u6210512\u00d7512\u56fe\u50cf\u7f16\u8f91\u4ec5\u97001.2\u79d2\uff0c\u6bd4\u540c\u7b49\u89c4\u6a21\u7684UltraEdit\u5feb2.2\u500d\u3002", "conclusion": "VAREdit\u901a\u8fc7\u81ea\u56de\u5f52\u8303\u5f0f\u548c\u591a\u5c3a\u5ea6\u6761\u4ef6\u5316\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u7f16\u8f91\u7684\u6307\u4ee4\u9075\u5faa\u6027\u548c\u6548\u7387\uff0c\u4e3a\u6307\u4ee4\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.15774", "pdf": "https://arxiv.org/pdf/2508.15774", "abs": "https://arxiv.org/abs/2508.15774", "authors": ["Haonan Qiu", "Ning Yu", "Ziqi Huang", "Paul Debevec", "Ziwei Liu"], "title": "CineScale: Free Lunch in High-Resolution Cinematic Visual Generation", "categories": ["cs.CV"], "comment": "CineScale is an extended work of FreeScale (ICCV 2025). Project Page:   https://eyeline-labs.github.io/CineScale/, Code Repo:   https://github.com/Eyeline-Labs/CineScale", "summary": "Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained models. However, these methods are still prone to producing low-quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. In this work, we propose CineScale, a novel inference paradigm to enable higher-resolution visual generation. To tackle the various issues introduced by the two types of video generation architectures, we propose dedicated variants tailored to each. Unlike existing baseline methods that are confined to high-resolution T2I and T2V generation, CineScale broadens the scope by enabling high-resolution I2V and V2V synthesis, built atop state-of-the-art open-source video generation frameworks. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Remarkably, our approach enables 8k image generation without any fine-tuning, and achieves 4k video generation with only minimal LoRA fine-tuning. Generated video samples are available at our website: https://eyeline-labs.github.io/CineScale/.", "AI": {"tldr": "CineScale\u662f\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u8303\u5f0f\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u9ad8\u5206\u8fa8\u7387\u7684\u89c6\u89c9\u751f\u6210\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u751f\u62108K\u56fe\u50cf\uff0c\u4ec5\u9700\u5c11\u91cfLoRA\u5fae\u8c03\u5373\u53ef\u751f\u62104K\u89c6\u9891\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u6269\u6563\u6a21\u578b\u7531\u4e8e\u7f3a\u4e4f\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\uff0c\u901a\u5e38\u5728\u6709\u9650\u5206\u8fa8\u7387\u4e0b\u8bad\u7ec3\uff0c\u5bfc\u81f4\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u5185\u5bb9\u65f6\u51fa\u73b0\u91cd\u590d\u6a21\u5f0f\u548c\u4f4e\u8d28\u91cf\u95ee\u9898\u3002", "method": "\u9488\u5bf9\u4e24\u79cd\u89c6\u9891\u751f\u6210\u67b6\u6784\u63d0\u51fa\u4e13\u95e8\u7684\u53d8\u4f53\uff0c\u6784\u5efa\u5728\u5f00\u6e90\u89c6\u9891\u751f\u6210\u6846\u67b6\u4e4b\u4e0a\uff0c\u901a\u8fc7\u65b0\u7684\u63a8\u7406\u8303\u5f0f\u89e3\u51b3\u9ad8\u5206\u8fa8\u7387\u751f\u6210\u65f6\u9ad8\u9891\u4fe1\u606f\u589e\u52a0\u5bfc\u81f4\u7684\u7d2f\u79ef\u8bef\u5dee\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u8303\u5f0f\u5728\u6269\u5c55\u56fe\u50cf\u548c\u89c6\u9891\u6a21\u578b\u9ad8\u5206\u8fa8\u7387\u751f\u6210\u80fd\u529b\u65b9\u9762\u7684\u4f18\u8d8a\u6027\uff0c\u5b9e\u73b0\u4e868K\u56fe\u50cf\u751f\u6210\u548c4K\u89c6\u9891\u751f\u6210\u3002", "conclusion": "CineScale\u6210\u529f\u6269\u5c55\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u751f\u6210\u80fd\u529b\uff0c\u4e3aT2I\u3001T2V\u3001I2V\u548cV2V\u5408\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.14917", "pdf": "https://arxiv.org/pdf/2508.14917", "abs": "https://arxiv.org/abs/2508.14917", "authors": ["Weichien Liao"], "title": "Scalable FPGA Framework for Real-Time Denoising in High-Throughput Imaging: A DRAM-Optimized Pipeline using High-Level Synthesis", "categories": ["cs.AR", "cs.CV", "cs.DC", "eess.IV", "eess.SP", "physics.ins-det"], "comment": "FPGA-based denoising pipeline for PRISM-scale imaging. Real-time   frame subtraction and averaging via burst-mode AXI4 and DRAM buffering.   Benchmarked against CPU/GPU workflows; scalable across multi-bank FPGA setups", "summary": "High-throughput imaging workflows, such as Parallel Rapid Imaging with Spectroscopic Mapping (PRISM), generate data at rates that exceed conventional real-time processing capabilities. We present a scalable FPGA-based preprocessing pipeline for real-time denoising, implemented via High-Level Synthesis (HLS) and optimized for DRAM-backed buffering. Our architecture performs frame subtraction and averaging directly on streamed image data, minimizing latency through burst-mode AXI4 interfaces. The resulting kernel operates below the inter-frame interval, enabling inline denoising and reducing dataset size for downstream CPU/GPU analysis. Validated under PRISM-scale acquisition, this modular FPGA framework offers a practical solution for latency-sensitive imaging workflows in spectroscopy and microscopy.", "AI": {"tldr": "FPGA\u5b9e\u65f6\u9884\u5904\u7406\u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u9ad8\u901a\u91cf\u6210\u50cf\u6570\u636e\u7684\u53bb\u566a\u548c\u964d\u7ef4\uff0c\u901a\u8fc7HLS\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u5904\u7406", "motivation": "\u9ad8\u901a\u91cf\u6210\u50cf\u6280\u672f\uff08\u5982PRISM\uff09\u4ea7\u751f\u6570\u636e\u7684\u901f\u5ea6\u8d85\u8fc7\u4f20\u7edf\u5b9e\u65f6\u5904\u7406\u80fd\u529b\uff0c\u9700\u8981\u4e13\u95e8\u7684\u786c\u4ef6\u52a0\u901f\u89e3\u51b3\u65b9\u6848", "method": "\u57fa\u4e8eFPGA\u7684\u53ef\u6269\u5c55\u9884\u5904\u7406\u6d41\u6c34\u7ebf\uff0c\u4f7f\u7528\u9ad8\u7ea7\u7efc\u5408\uff08HLS\uff09\u5b9e\u73b0\uff0c\u91c7\u7528DRAM\u7f13\u51b2\u548c\u7a81\u53d1\u6a21\u5f0fAXI4\u63a5\u53e3\u8fdb\u884c\u5e27\u51cf\u6cd5\u548c\u5e73\u5747\u64cd\u4f5c", "result": "\u5185\u6838\u64cd\u4f5c\u65f6\u95f4\u4f4e\u4e8e\u5e27\u95f4\u95f4\u9694\uff0c\u5b9e\u73b0\u5185\u8054\u53bb\u566a\u5e76\u51cf\u5c11\u4e0b\u6e38CPU/GPU\u5206\u6790\u7684\u6570\u636e\u96c6\u5927\u5c0f\uff0c\u5728PRISM\u89c4\u6a21\u91c7\u96c6\u4e0b\u9a8c\u8bc1\u6709\u6548", "conclusion": "\u8be5\u6a21\u5757\u5316FPGA\u6846\u67b6\u4e3a\u5149\u8c31\u5b66\u548c\u663e\u5fae\u955c\u5b66\u4e2d\u7684\u5ef6\u8fdf\u654f\u611f\u6210\u50cf\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.15151", "pdf": "https://arxiv.org/pdf/2508.15151", "abs": "https://arxiv.org/abs/2508.15151", "authors": ["Jeonghyun Noh", "Hyun-Jic Oh", "Byungju Chae", "Won-Ki Jeong"], "title": "Zero-shot Volumetric CT Super-Resolution using 3D Gaussian Splatting with Upsampled 2D X-ray Projection Priors", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Computed tomography (CT) is widely used in clinical diagnosis, but acquiring high-resolution (HR) CT is limited by radiation exposure risks. Deep learning-based super-resolution (SR) methods have been studied to reconstruct HR from low-resolution (LR) inputs. While supervised SR approaches have shown promising results, they require large-scale paired LR-HR volume datasets that are often unavailable. In contrast, zero-shot methods alleviate the need for paired data by using only a single LR input, but typically struggle to recover fine anatomical details due to limited internal information. To overcome these, we propose a novel zero-shot 3D CT SR framework that leverages upsampled 2D X-ray projection priors generated by a diffusion model. Exploiting the abundance of HR 2D X-ray data, we train a diffusion model on large-scale 2D X-ray projection and introduce a per-projection adaptive sampling strategy. It selects the generative process for each projection, thus providing HR projections as strong external priors for 3D CT reconstruction. These projections serve as inputs to 3D Gaussian splatting for reconstructing a 3D CT volume. Furthermore, we propose negative alpha blending (NAB-GS) that allows negative values in Gaussian density representation. NAB-GS enables residual learning between LR and diffusion-based projections, thereby enhancing high-frequency structure reconstruction. Experiments on two datasets show that our method achieves superior quantitative and qualitative results for 3D CT SR.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u7684\u65e0\u76d1\u77633D CT\u8d85\u5206\u8fa8\u739f\u6784\u6846\u67b6\uff0c\u5229\u7528\u6fc0\u5149\u6269\u65632D X\u5149\u6295\u5f71\u4f5c\u4e3a\u5916\u90e8\u5148\u9a8c\u77e5\u8bc6\uff0c\u901a\u8fc73D\u9ad8\u65af\u62df\u5408\u548c\u8d1f\u503c\u878d\u5408\u6280\u672f\u63d0\u9ad8\u7ec6\u8282\u6062\u590d\u6548\u679c", "motivation": "\u89e3\u51b3\u4f20\u7edf\u76d1\u7763\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6210\u5bf9LR-HR\u6570\u636e\u96c6\u4ee5\u53ca\u96f6\u68c0\u67e5\u65b9\u6cd5\u5185\u90e8\u4fe1\u606f\u6709\u9650\u65e0\u6cd5\u6062\u590d\u7ec6\u5fae\u89e3\u5256\u7ec6\u8282\u7684\u95ee\u9898", "method": "\u4f7f\u7528\u6fc0\u5149\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u5206\u8fa8\u73872D X\u5149\u6295\u5f71\u4f5c\u4e3a\u5916\u90e8\u5148\u9a8c\uff0c\u901a\u8fc7\u6bcf\u5f20\u6295\u5f71\u9002\u914d\u91c7\u6837\u7b56\u7565\u9009\u62e9\u751f\u6210\u8fc7\u7a0b\uff0c\u7136\u540e\u75283D\u9ad8\u65af\u62df\u5408\u91cd\u5efa3D CT\u4f53\u79ef\uff0c\u5e76\u63d0\u51fa\u652f\u6301\u8d1f\u503c\u7684\u8d1f\u503c\u878d\u5408\u6280\u672f\u4ee5\u4fc3\u8fdb\u9ad8\u9891\u7ed3\u6784\u6062\u590d", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u91cf\u548c\u8d28\u91cf\u4e0a\u90fd\u83b7\u5f97\u4e86\u4f18\u5f02\u76843D CT\u8d85\u5206\u8fa8\u739f\u6784\u7ed3\u679c", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u7ed3\u5408\u4e86\u5916\u90e8\u5148\u9a8c\u77e5\u8bc6\u548c\u9ad8\u7ea7\u91cd\u5efa\u6280\u672f\uff0c\u4e3a\u65e0\u76d1\u77633D CT\u8d85\u5206\u8fa8\u739f\u6784\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6062\u590d\u7ec6\u5fae\u7684\u89e3\u5256\u7ec6\u8282"}}
{"id": "2508.15222", "pdf": "https://arxiv.org/pdf/2508.15222", "abs": "https://arxiv.org/abs/2508.15222", "authors": ["Hantao Zhang", "Jingyang Liu", "Ed Li"], "title": "See it. Say it. Sorted: Agentic System for Compositional Diagram Generation", "categories": ["cs.AI", "cs.CV", "cs.MA"], "comment": null, "summary": "We study sketch-to-diagram generation: converting rough hand sketches into precise, compositional diagrams. Diffusion models excel at photorealism but struggle with the spatial precision, alignment, and symbolic structure required for flowcharts. We introduce See it. Say it. Sorted., a training-free agentic system that couples a Vision-Language Model (VLM) with Large Language Models (LLMs) to produce editable Scalable Vector Graphics (SVG) programs. The system runs an iterative loop in which a Critic VLM proposes a small set of qualitative, relational edits; multiple candidate LLMs synthesize SVG updates with diverse strategies (conservative->aggressive, alternative, focused); and a Judge VLM selects the best candidate, ensuring stable improvement. This design prioritizes qualitative reasoning over brittle numerical estimates, preserves global constraints (e.g., alignment, connectivity), and naturally supports human-in-the-loop corrections. On 10 sketches derived from flowcharts in published papers, our method more faithfully reconstructs layout and structure than two frontier closed-source image generation LLMs (GPT-5 and Gemini-2.5-Pro), accurately composing primitives (e.g., multi-headed arrows) without inserting unwanted text. Because outputs are programmatic SVGs, the approach is readily extensible to presentation tools (e.g., PowerPoint) via APIs and can be specialized with improved prompts and task-specific tools. The codebase is open-sourced at https://github.com/hantaoZhangrichard/see_it_say_it_sorted.git.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u4e2a\u8bad\u7ec3\u514d\u8d39\u7684\u4ee3\u7406\u7cfb\u7edfSee it. Say it. Sorted.\uff0c\u901a\u8fc7VLM\u548cLLM\u534f\u4f5c\u5c06\u624b\u7ed8\u8349\u56fe\u8f6c\u6362\u4e3a\u7cbe\u786e\u7684\u53ef\u7f16\u8f91SVG\u56fe\u8868\uff0c\u5728\u6d41\u7a0b\u56fe\u751f\u6210\u4e0a\u8d85\u8d8a\u4e86GPT-5\u548cGemini-2.5-Pro\u3002", "motivation": "\u6563\u5e03\u6a21\u578b\u5728\u7167\u7247\u5b9e\u9645\u6548\u679c\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7a7a\u95f4\u7cbe\u5ea6\u3001\u5bf9\u9f50\u548c\u7b26\u53f7\u7ed3\u6784\u65b9\u9762\u9047\u5230\u56f0\u96be\uff0c\u7279\u522b\u662f\u6d41\u7a0b\u56fe\u751f\u6210\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8fed\u4ee3\u5faa\u73af\u7cfb\u7edf\uff1aCritic VLM\u63d0\u51fa\u5b9a\u6027\u5173\u7cfb\u7f16\u8f91\u5efa\u8bae\uff0c\u591a\u4e2aLLM\u5019\u9009\u751f\u6210\u4e0d\u540c\u7b56\u7565\u7684SVG\u66f4\u65b0\uff0cJudge VLM\u9009\u62e9\u6700\u4f73\u65b9\u6848\uff0c\u786e\u4fdd\u7a33\u5b9a\u6539\u8fdb\u3002", "result": "\u572810\u4e2a\u6765\u81ea\u53d1\u8868\u8bba\u6587\u7684\u6d41\u7a0b\u56fe\u8349\u56fe\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6bd4GPT-5\u548cGemini-2.5-Pro\u66f4\u51c6\u786e\u5730\u91cd\u5efa\u4e86\u5e03\u5c40\u548c\u7ed3\u6784\uff0c\u51c6\u786e\u7ec4\u5408\u539f\u8bed\uff08\u5982\u591a\u5934\u7bad\u5934\uff09\u800c\u4e0d\u63d2\u5165\u4e0d\u5e94\u6709\u7684\u6587\u672c\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u91cd\u89c6\u5b9a\u6027\u63a8\u7406\u800c\u975e\u8106\u5f31\u7684\u6570\u503c\u4f30\u8ba1\uff0c\u4fdd\u6301\u5168\u5c40\u7ea6\u675f\uff0c\u652f\u6301\u4eba\u5728\u5faa\u73af\u4e2d\u7684\u4fee\u6b63\uff0c\u4e14\u7531\u4e8e\u8f93\u51fa\u4e3a\u7a0b\u5e8f\u5316SVG\uff0c\u65b9\u6cd5\u6613\u4e8e\u6269\u5c55\u5230\u5c55\u793a\u5de5\u5177\u3002"}}
{"id": "2508.15236", "pdf": "https://arxiv.org/pdf/2508.15236", "abs": "https://arxiv.org/abs/2508.15236", "authors": ["Jiamu Wang", "Keunho Byeon", "Jinsol Song", "Anh Nguyen", "Sangjeong Ahn", "Sung Hak Lee", "Jin Tae Kwak"], "title": "Pathology-Informed Latent Diffusion Model for Anomaly Detection in Lymph Node Metastasis", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Anomaly detection is an emerging approach in digital pathology for its ability to efficiently and effectively utilize data for disease diagnosis. While supervised learning approaches deliver high accuracy, they rely on extensively annotated datasets, suffering from data scarcity in digital pathology. Unsupervised anomaly detection, however, offers a viable alternative by identifying deviations from normal tissue distributions without requiring exhaustive annotations. Recently, denoising diffusion probabilistic models have gained popularity in unsupervised anomaly detection, achieving promising performance in both natural and medical imaging datasets. Building on this, we incorporate a vision-language model with a diffusion model for unsupervised anomaly detection in digital pathology, utilizing histopathology prompts during reconstruction. Our approach employs a set of pathology-related keywords associated with normal tissues to guide the reconstruction process, facilitating the differentiation between normal and abnormal tissues. To evaluate the effectiveness of the proposed method, we conduct experiments on a gastric lymph node dataset from a local hospital and assess its generalization ability under domain shift using a public breast lymph node dataset. The experimental results highlight the potential of the proposed method for unsupervised anomaly detection across various organs in digital pathology. Code: https://github.com/QuIIL/AnoPILaD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u7684\u6570\u5b57\u75c5\u7406\u5b66\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528\u7ec4\u7ec7\u75c5\u7406\u5b66\u63d0\u793a\u6307\u5bfc\u91cd\u5efa\u8fc7\u7a0b\uff0c\u5728\u80c3\u548c\u4e73\u817a\u6dcb\u5df4\u7ed3\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027", "motivation": "\u6570\u5b57\u75c5\u7406\u5b66\u4e2d\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u4e25\u91cd\u3002\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u80fd\u591f\u8bc6\u522b\u6b63\u5e38\u7ec4\u7ec7\u5206\u5e03\u7684\u504f\u5dee\uff0c\u800c\u65e0\u9700\u8be6\u5c3d\u6807\u6ce8\u3002\u6269\u6563\u6982\u7387\u6a21\u578b\u5728\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5c1a\u672a\u5145\u5206\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b", "method": "\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u6269\u6563\u6a21\u578b\u7ed3\u5408\uff0c\u5728\u91cd\u5efa\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u4e00\u7ec4\u4e0e\u6b63\u5e38\u7ec4\u7ec7\u76f8\u5173\u7684\u75c5\u7406\u5b66\u5173\u952e\u8bcd\u4f5c\u4e3a\u63d0\u793a\uff0c\u5f15\u5bfc\u91cd\u5efa\u8fc7\u7a0b\uff0c\u4ece\u800c\u533a\u5206\u6b63\u5e38\u548c\u5f02\u5e38\u7ec4\u7ec7", "result": "\u5728\u672c\u5730\u533b\u9662\u80c3\u6dcb\u5df4\u7ed3\u6570\u636e\u96c6\u548c\u516c\u5171\u4e73\u817a\u6dcb\u5df4\u7ed3\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u5668\u5b98\u7684\u6570\u5b57\u75c5\u7406\u5b66\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5177\u6709\u826f\u597d\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u5728\u57df\u504f\u79fb\u60c5\u51b5\u4e0b\u7684\u6cdb\u5316\u80fd\u529b", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u6570\u5b57\u75c5\u7406\u5b66\u4e2d\u8de8\u5668\u5b98\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u7684\u6f5c\u529b\uff0c\u4e3a\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2508.15553", "pdf": "https://arxiv.org/pdf/2508.15553", "abs": "https://arxiv.org/abs/2508.15553", "authors": ["Jin Ye", "Jingran Wang", "Fengchao Xiong", "Jingzhou Chen", "Yuntao Qian"], "title": "Deep Equilibrium Convolutional Sparse Coding for Hyperspectral Image Denoising", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Hyperspectral images (HSIs) play a crucial role in remote sensing but are often degraded by complex noise patterns. Ensuring the physical property of the denoised HSIs is vital for robust HSI denoising, giving the rise of deep unfolding-based methods. However, these methods map the optimization of a physical model to a learnable network with a predefined depth, which lacks convergence guarantees. In contrast, Deep Equilibrium (DEQ) models treat the hidden layers of deep networks as the solution to a fixed-point problem and models them as infinite-depth networks, naturally consistent with the optimization. Under the framework of DEQ, we propose a Deep Equilibrium Convolutional Sparse Coding (DECSC) framework that unifies local spatial-spectral correlations, nonlocal spatial self-similarities, and global spatial consistency for robust HSI denoising. Within the convolutional sparse coding (CSC) framework, we enforce shared 2D convolutional sparse representation to ensure global spatial consistency across bands, while unshared 3D convolutional sparse representation captures local spatial-spectral details. To further exploit nonlocal self-similarities, a transformer block is embedded after the 2D CSC. Additionally, a detail enhancement module is integrated with the 3D CSC to promote image detail preservation. We formulate the proximal gradient descent of the CSC model as a fixed-point problem and transform the iterative updates into a learnable network architecture within the framework of DEQ. Experimental results demonstrate that our DECSC method achieves superior denoising performance compared to state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5e73\u8861\u6a21\u578b\u7684\u5377\u79ef\u7a00\u758f\u7f16\u7801\u6846\u67b6DECSC\uff0c\u7ed3\u5408\u5c40\u90e8\u7a7a\u95f4-\u5149\u8c31\u76f8\u5173\u6027\u3001\u975e\u5c40\u90e8\u7a7a\u95f4\u81ea\u76f8\u4f3c\u6027\u548c\u5168\u5c40\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u9ad8\u5149\u8c31\u56fe\u50cf\u53bb\u566a", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5c55\u5f00\u65b9\u6cd5\u7f3a\u4e4f\u6536\u655b\u4fdd\u8bc1\uff0c\u800c\u6df1\u5ea6\u5e73\u8861\u6a21\u578b\u81ea\u7136\u7b26\u5408\u4f18\u5316\u8fc7\u7a0b\uff0c\u80fd\u591f\u63d0\u4f9b\u65e0\u9650\u6df1\u5ea6\u7f51\u7edc\u8868\u793a", "method": "\u5728\u5377\u79ef\u7a00\u758f\u7f16\u7801\u6846\u67b6\u4e2d\uff0c\u4f7f\u7528\u5171\u4eab2D\u5377\u79ef\u7a00\u758f\u8868\u793a\u786e\u4fdd\u9891\u5e26\u95f4\u5168\u5c40\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u975e\u5171\u4eab3D\u5377\u79ef\u7a00\u758f\u8868\u793a\u6355\u83b7\u5c40\u90e8\u7a7a\u95f4-\u5149\u8c31\u7ec6\u8282\uff0c\u5d4c\u5165Transformer\u5757\u5229\u7528\u975e\u5c40\u90e8\u81ea\u76f8\u4f3c\u6027\uff0c\u5e76\u96c6\u6210\u7ec6\u8282\u589e\u5f3a\u6a21\u5757", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eDECSC\u65b9\u6cd5\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u53bb\u566a\u6027\u80fd", "conclusion": "\u57fa\u4e8e\u6df1\u5ea6\u5e73\u8861\u6a21\u578b\u7684DECSC\u6846\u67b6\u6709\u6548\u7edf\u4e00\u4e86\u591a\u79cd\u7279\u5f81\u8868\u793a\uff0c\u4e3a\u9ad8\u5149\u8c31\u56fe\u50cf\u53bb\u566a\u63d0\u4f9b\u4e86\u5f3a\u5927\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.15594", "pdf": "https://arxiv.org/pdf/2508.15594", "abs": "https://arxiv.org/abs/2508.15594", "authors": ["Ana C. Perre", "Lu\u00eds A. Alexandre", "Lu\u00eds C. Freire"], "title": "Are Virtual DES Images a Valid Alternative to the Real Ones?", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "10 pages, 4 figures, 3 tables", "summary": "Contrast-enhanced spectral mammography (CESM) is an imaging modality that provides two types of images, commonly known as low-energy (LE) and dual-energy subtracted (DES) images. In many domains, particularly in medicine, the emergence of image-to-image translation techniques has enabled the artificial generation of images using other images as input. Within CESM, applying such techniques to generate DES images from LE images could be highly beneficial, potentially reducing patient exposure to radiation associated with high-energy image acquisition. In this study, we investigated three models for the artificial generation of DES images (virtual DES): a pre-trained U-Net model, a U-Net trained end-to-end model, and a CycleGAN model. We also performed a series of experiments to assess the impact of using virtual DES images on the classification of CESM examinations into malignant and non-malignant categories. To our knowledge, this is the first study to evaluate the impact of virtual DES images on CESM lesion classification. The results demonstrate that the best performance was achieved with the pre-trained U-Net model, yielding an F1 score of 85.59% when using the virtual DES images, compared to 90.35% with the real DES images. This discrepancy likely results from the additional diagnostic information in real DES images, which contributes to a higher classification accuracy. Nevertheless, the potential for virtual DES image generation is considerable and future advancements may narrow this performance gap to a level where exclusive reliance on virtual DES images becomes clinically viable.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cd\u6a21\u578b\uff08\u9884\u8bad\u7ec3U-Net\u3001\u7aef\u5230\u7aef\u8bad\u7ec3U-Net\u3001CycleGAN\uff09\u4ece\u4f4e\u80fd\u91cf\u56fe\u50cf\u751f\u6210\u865a\u62df\u53cc\u80fd\u91cf\u51cf\u5f71\u56fe\u50cf\u7684\u80fd\u529b\uff0c\u5e76\u6d4b\u8bd5\u4e86\u8fd9\u4e9b\u865a\u62df\u56fe\u50cf\u5728\u4e73\u817a\u75c5\u53d8\u826f\u6076\u6027\u5206\u7c7b\u4e2d\u7684\u6548\u679c\u3002", "motivation": "\u901a\u8fc7\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u6280\u672f\u4ece\u4f4e\u80fd\u91cf\u56fe\u50cf\u751f\u6210\u53cc\u80fd\u91cf\u51cf\u5f71\u56fe\u50cf\uff0c\u53ef\u4ee5\u51cf\u5c11\u60a3\u8005\u5728\u9ad8\u80fd\u91cf\u56fe\u50cf\u91c7\u96c6\u8fc7\u7a0b\u4e2d\u7684\u8f90\u5c04\u66b4\u9732\uff0c\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u4e0d\u540c\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u9884\u8bad\u7ec3U-Net\u3001\u7aef\u5230\u7aef\u8bad\u7ec3U-Net\u3001CycleGAN\uff09\u751f\u6210\u865a\u62dfDES\u56fe\u50cf\uff0c\u5e76\u8bc4\u4f30\u8fd9\u4e9b\u56fe\u50cf\u5728CESM\u68c0\u67e5\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "\u9884\u8bad\u7ec3U-Net\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u4f7f\u7528\u865a\u62dfDES\u56fe\u50cf\u83b7\u5f9785.59%\u7684F1\u5206\u6570\uff0c\u800c\u4f7f\u7528\u771f\u5b9eDES\u56fe\u50cf\u4e3a90.35%\uff0c\u5b58\u57284.76%\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u865a\u62dfDES\u56fe\u50cf\u751f\u6210\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u867d\u7136\u76ee\u524d\u4e0e\u771f\u5b9e\u56fe\u50cf\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\uff0c\u4f46\u672a\u6765\u6280\u672f\u8fdb\u6b65\u53ef\u80fd\u4f7f\u5176\u8fbe\u5230\u4e34\u5e8a\u53ef\u7528\u6c34\u5e73\uff0c\u4ece\u800c\u51cf\u5c11\u60a3\u8005\u8f90\u5c04\u66b4\u9732\u3002"}}
{"id": "2508.15737", "pdf": "https://arxiv.org/pdf/2508.15737", "abs": "https://arxiv.org/abs/2508.15737", "authors": ["Joonas J\u00e4rve", "Karl Kaspar Haavel", "Meelis Kull"], "title": "Probability Density from Latent Diffusion Models for Out-of-Distribution Detection", "categories": ["cs.LG", "cs.CV"], "comment": "ECAI 2025", "summary": "Despite rapid advances in AI, safety remains the main bottleneck to deploying machine-learning systems. A critical safety component is out-of-distribution detection: given an input, decide whether it comes from the same distribution as the training data. In generative models, the most natural OOD score is the data likelihood. Actually, under the assumption of uniformly distributed OOD data, the likelihood is even the optimal OOD detector, as we show in this work. However, earlier work reported that likelihood often fails in practice, raising doubts about its usefulness. We explore whether, in practice, the representation space also suffers from the inability to learn good density estimation for OOD detection, or if it is merely a problem of the pixel space typically used in generative models. To test this, we trained a Variational Diffusion Model not on images, but on the representation space of a pre-trained ResNet-18 to assess the performance of our likelihood-based detector in comparison to state-of-the-art methods from the OpenOOD suite.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u6a21\u578b\u4e2d\u57fa\u4e8e\u4f3c\u7136\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u9884\u8bad\u7ec3ResNet-18\u8868\u793a\u7a7a\u95f4\u4e0a\u8bad\u7ec3\u53d8\u5206\u6269\u6563\u6a21\u578b\u6765\u8bc4\u4f30\u6027\u80fd\uff0c\u5e76\u4e0eOpenOOD\u5957\u4ef6\u4e2d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "motivation": "\u5c3d\u7ba1AI\u5feb\u901f\u53d1\u5c55\uff0c\u5b89\u5168\u6027\u4ecd\u662f\u90e8\u7f72\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7684\u4e3b\u8981\u74f6\u9888\u3002OOD\u68c0\u6d4b\u662f\u5173\u952e\u5b89\u5168\u7ec4\u4ef6\uff0c\u800c\u4f3c\u7136\u5728\u7406\u8bba\u4e0a\u662f\u6700\u4f18OOD\u68c0\u6d4b\u5668\uff0c\u4f46\u5b9e\u8df5\u4e2d\u7ecf\u5e38\u5931\u8d25\uff0c\u9700\u8981\u63a2\u7a76\u8fd9\u662f\u8868\u793a\u7a7a\u95f4\u7684\u95ee\u9898\u8fd8\u662f\u50cf\u7d20\u7a7a\u95f4\u7684\u95ee\u9898\u3002", "method": "\u5728\u9884\u8bad\u7ec3ResNet-18\u7684\u8868\u793a\u7a7a\u95f4\u4e0a\u8bad\u7ec3\u53d8\u5206\u6269\u6563\u6a21\u578b\uff0c\u4f7f\u7528\u57fa\u4e8e\u4f3c\u7136\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u4e0eOpenOOD\u5957\u4ef6\u4e2d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\u3002", "result": "\u8bba\u6587\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u57fa\u4e8e\u4f3c\u7136\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4f46\u5177\u4f53\u7ed3\u679c\u6570\u636e\u672a\u5728\u6458\u8981\u4e2d\u63d0\u4f9b\u3002", "conclusion": "\u7814\u7a76\u65e8\u5728\u786e\u5b9a\u8868\u793a\u7a7a\u95f4\u662f\u5426\u4e5f\u5b58\u5728\u65e0\u6cd5\u5b66\u4e60\u826f\u597d\u5bc6\u5ea6\u4f30\u8ba1\u7684\u95ee\u9898\uff0c\u6216\u8005\u4ec5\u4ec5\u662f\u751f\u6210\u6a21\u578b\u4e2d\u901a\u5e38\u4f7f\u7528\u7684\u50cf\u7d20\u7a7a\u95f4\u7684\u95ee\u9898\uff0c\u4e3a\u6539\u8fdbOOD\u68c0\u6d4b\u63d0\u4f9b\u65b0\u7684\u89c6\u89d2\u3002"}}
