{"id": "2507.22100", "pdf": "https://arxiv.org/pdf/2507.22100", "abs": "https://arxiv.org/abs/2507.22100", "authors": ["Sicheng Zhang", "Binzhu Xie", "Zhonghao Yan", "Yuli Zhang", "Donghao Zhou", "Xiaofei Chen", "Shi Qiu", "Jiaqi Liu", "Guoyang Xie", "Zhichao Lu"], "title": "Trade-offs in Image Generation: How Do Different Dimensions Interact?", "categories": ["cs.CV"], "comment": "Accepted in ICCV 2025, Codebase: https://github.com/fesvhtr/TRIG", "summary": "Model performance in text-to-image (T2I) and image-to-image (I2I) generation often depends on multiple aspects, including quality, alignment, diversity, and robustness. However, models' complex trade-offs among these dimensions have rarely been explored due to (1) the lack of datasets that allow fine-grained quantification of these trade-offs, and (2) the use of a single metric for multiple dimensions. To bridge this gap, we introduce TRIG-Bench (Trade-offs in Image Generation), which spans 10 dimensions (Realism, Originality, Aesthetics, Content, Relation, Style, Knowledge, Ambiguity, Toxicity, and Bias), contains 40,200 samples, and covers 132 pairwise dimensional subsets. Furthermore, we develop TRIGScore, a VLM-as-judge metric that automatically adapts to various dimensions. Based on TRIG-Bench and TRIGScore, we evaluate 14 models across T2I and I2I tasks. In addition, we propose the Relation Recognition System to generate the Dimension Trade-off Map (DTM) that visualizes the trade-offs among model-specific capabilities. Our experiments demonstrate that DTM consistently provides a comprehensive understanding of the trade-offs between dimensions for each type of generative model. Notably, we show that the model's dimension-specific weaknesses can be mitigated through fine-tuning on DTM to enhance overall performance. Code is available at: https://github.com/fesvhtr/TRIG"}
{"id": "2507.22194", "pdf": "https://arxiv.org/pdf/2507.22194", "abs": "https://arxiv.org/abs/2507.22194", "authors": ["Christian Ellis", "Maggie Wigness", "Craig Lennon", "Lance Fiondella"], "title": "Temporally Consistent Unsupervised Segmentation for Mobile Robot Perception", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Rapid progress in terrain-aware autonomous ground navigation has been driven by advances in supervised semantic segmentation. However, these methods rely on costly data collection and labor-intensive ground truth labeling to train deep models. Furthermore, autonomous systems are increasingly deployed in unrehearsed, unstructured environments where no labeled data exists and semantic categories may be ambiguous or domain-specific. Recent zero-shot approaches to unsupervised segmentation have shown promise in such settings but typically operate on individual frames, lacking temporal consistency-a critical property for robust perception in unstructured environments. To address this gap we introduce Frontier-Seg, a method for temporally consistent unsupervised segmentation of terrain from mobile robot video streams. Frontier-Seg clusters superpixel-level features extracted from foundation model backbones-specifically DINOv2-and enforces temporal consistency across frames to identify persistent terrain boundaries or frontiers without human supervision. We evaluate Frontier-Seg on a diverse set of benchmark datasets-including RUGD and RELLIS-3D-demonstrating its ability to perform unsupervised segmentation across unstructured off-road environments."}
{"id": "2507.22342", "pdf": "https://arxiv.org/pdf/2507.22342", "abs": "https://arxiv.org/abs/2507.22342", "authors": ["Yuki Fujimura", "Takahiro Kushida", "Kazuya Kitano", "Takuya Funatomi", "Yasuhiro Mukaigawa"], "title": "UFV-Splatter: Pose-Free Feed-Forward 3D Gaussian Splatting Adapted to Unfavorable Views", "categories": ["cs.CV"], "comment": "Project page: https://yfujimura.github.io/UFV-Splatter_page/", "summary": "This paper presents a pose-free, feed-forward 3D Gaussian Splatting (3DGS) framework designed to handle unfavorable input views. A common rendering setup for training feed-forward approaches places a 3D object at the world origin and renders it from cameras pointed toward the origin -- i.e., from favorable views, limiting the applicability of these models to real-world scenarios involving varying and unknown camera poses. To overcome this limitation, we introduce a novel adaptation framework that enables pretrained pose-free feed-forward 3DGS models to handle unfavorable views. We leverage priors learned from favorable images by feeding recentered images into a pretrained model augmented with low-rank adaptation (LoRA) layers. We further propose a Gaussian adapter module to enhance the geometric consistency of the Gaussians derived from the recentered inputs, along with a Gaussian alignment method to render accurate target views for training. Additionally, we introduce a new training strategy that utilizes an off-the-shelf dataset composed solely of favorable images. Experimental results on both synthetic images from the Google Scanned Objects dataset and real images from the OmniObject3D dataset validate the effectiveness of our method in handling unfavorable input views."}
{"id": "2507.22360", "pdf": "https://arxiv.org/pdf/2507.22360", "abs": "https://arxiv.org/abs/2507.22360", "authors": ["Kunyang Li", "Jeffrey A Chan Santiago", "Sarinda Dhanesh Samarasinghe", "Gaowen Liu", "Mubarak Shah"], "title": "GVD: Guiding Video Diffusion Model for Scalable Video Distillation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "To address the larger computation and storage requirements associated with large video datasets, video dataset distillation aims to capture spatial and temporal information in a significantly smaller dataset, such that training on the distilled data has comparable performance to training on all of the data. We propose GVD: Guiding Video Diffusion, the first diffusion-based video distillation method. GVD jointly distills spatial and temporal features, ensuring high-fidelity video generation across diverse actions while capturing essential motion information. Our method's diverse yet representative distillations significantly outperform previous state-of-the-art approaches on the MiniUCF and HMDB51 datasets across 5, 10, and 20 Instances Per Class (IPC). Specifically, our method achieves 78.29 percent of the original dataset's performance using only 1.98 percent of the total number of frames in MiniUCF. Additionally, it reaches 73.83 percent of the performance with just 3.30 percent of the frames in HMDB51. Experimental results across benchmark video datasets demonstrate that GVD not only achieves state-of-the-art performance but can also generate higher resolution videos and higher IPC without significantly increasing computational cost."}
{"id": "2507.22454", "pdf": "https://arxiv.org/pdf/2507.22454", "abs": "https://arxiv.org/abs/2507.22454", "authors": ["Jiuming Liu", "Zheng Huang", "Mengmeng Liu", "Tianchen Deng", "Francesco Nex", "Hao Cheng", "Hesheng Wang"], "title": "TopoLiDM: Topology-Aware LiDAR Diffusion Models for Interpretable and Realistic LiDAR Point Cloud Generation", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted by IROS 2025. Code:https://github.com/IRMVLab/TopoLiDM", "summary": "LiDAR scene generation is critical for mitigating real-world LiDAR data collection costs and enhancing the robustness of downstream perception tasks in autonomous driving. However, existing methods commonly struggle to capture geometric realism and global topological consistency. Recent LiDAR Diffusion Models (LiDMs) predominantly embed LiDAR points into the latent space for improved generation efficiency, which limits their interpretable ability to model detailed geometric structures and preserve global topological consistency. To address these challenges, we propose TopoLiDM, a novel framework that integrates graph neural networks (GNNs) with diffusion models under topological regularization for high-fidelity LiDAR generation. Our approach first trains a topological-preserving VAE to extract latent graph representations by graph construction and multiple graph convolutional layers. Then we freeze the VAE and generate novel latent topological graphs through the latent diffusion models. We also introduce 0-dimensional persistent homology (PH) constraints, ensuring the generated LiDAR scenes adhere to real-world global topological structures. Extensive experiments on the KITTI-360 dataset demonstrate TopoLiDM's superiority over state-of-the-art methods, achieving improvements of 22.6% lower Frechet Range Image Distance (FRID) and 9.2% lower Minimum Matching Distance (MMD). Notably, our model also enables fast generation speed with an average inference time of 1.68 samples/s, showcasing its scalability for real-world applications. We will release the related codes at https://github.com/IRMVLab/TopoLiDM."}
{"id": "2507.22459", "pdf": "https://arxiv.org/pdf/2507.22459", "abs": "https://arxiv.org/abs/2507.22459", "authors": ["Jaeha Kim", "Junghun Oh", "Kyoung Mu Lee"], "title": "Exploiting Diffusion Prior for Task-driven Image Restoration", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "Task-driven image restoration (TDIR) has recently emerged to address performance drops in high-level vision tasks caused by low-quality (LQ) inputs. Previous TDIR methods struggle to handle practical scenarios in which images are degraded by multiple complex factors, leaving minimal clues for restoration. This motivates us to leverage the diffusion prior, one of the most powerful natural image priors. However, while the diffusion prior can help generate visually plausible results, using it to restore task-relevant details remains challenging, even when combined with recent TDIR methods. To address this, we propose EDTR, which effectively harnesses the power of diffusion prior to restore task-relevant details. Specifically, we propose directly leveraging useful clues from LQ images in the diffusion process by generating from pixel-error-based pre-restored LQ images with mild noise added. Moreover, we employ a small number of denoising steps to prevent the generation of redundant details that dilute crucial task-related information. We demonstrate that our method effectively utilizes diffusion prior for TDIR, significantly enhancing task performance and visual quality across diverse tasks with multiple complex degradations."}
{"id": "2507.22469", "pdf": "https://arxiv.org/pdf/2507.22469", "abs": "https://arxiv.org/abs/2507.22469", "authors": ["Viacheslav Pirogov"], "title": "Visual Language Models as Zero-Shot Deepfake Detectors", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted to the ICML 2025 Workshop on Reliable and Responsible   Foundation Models", "summary": "The contemporary phenomenon of deepfakes, utilizing GAN or diffusion models for face swapping, presents a substantial and evolving threat in digital media, identity verification, and a multitude of other systems. The majority of existing methods for detecting deepfakes rely on training specialized classifiers to distinguish between genuine and manipulated images, focusing only on the image domain without incorporating any auxiliary tasks that could enhance robustness. In this paper, inspired by the zero-shot capabilities of Vision Language Models, we propose a novel VLM-based approach to image classification and then evaluate it for deepfake detection. Specifically, we utilize a new high-quality deepfake dataset comprising 60,000 images, on which our zero-shot models demonstrate superior performance to almost all existing methods. Subsequently, we compare the performance of the best-performing architecture, InstructBLIP, on the popular deepfake dataset DFDC-P against traditional methods in two scenarios: zero-shot and in-domain fine-tuning. Our results demonstrate the superiority of VLMs over traditional classifiers."}
{"id": "2507.22498", "pdf": "https://arxiv.org/pdf/2507.22498", "abs": "https://arxiv.org/abs/2507.22498", "authors": ["Yuhwan Jeong", "Yunseo Yang", "Youngjo Yoon", "Kuk-Jin Yoon"], "title": "Robust Adverse Weather Removal via Spectral-based Spatial Grouping", "categories": ["cs.CV", "cs.AI"], "comment": "accepted by ICCV25", "summary": "Adverse weather conditions cause diverse and complex degradation patterns, driving the development of All-in-One (AiO) models. However, recent AiO solutions still struggle to capture diverse degradations, since global filtering methods like direct operations on the frequency domain fail to handle highly variable and localized distortions. To address these issue, we propose Spectral-based Spatial Grouping Transformer (SSGformer), a novel approach that leverages spectral decomposition and group-wise attention for multi-weather image restoration. SSGformer decomposes images into high-frequency edge features using conventional edge detection and low-frequency information via Singular Value Decomposition. We utilize multi-head linear attention to effectively model the relationship between these features. The fused features are integrated with the input to generate a grouping-mask that clusters regions based on the spatial similarity and image texture. To fully leverage this mask, we introduce a group-wise attention mechanism, enabling robust adverse weather removal and ensuring consistent performance across diverse weather conditions. We also propose a Spatial Grouping Transformer Block that uses both channel attention and spatial attention, effectively balancing feature-wise relationships and spatial dependencies. Extensive experiments show the superiority of our approach, validating its effectiveness in handling the varied and intricate adverse weather degradations."}
{"id": "2507.22501", "pdf": "https://arxiv.org/pdf/2507.22501", "abs": "https://arxiv.org/abs/2507.22501", "authors": ["Chang Huang", "Jiahang Cao", "Jun Ma", "Kieren Yu", "Cong Li", "Huayong Yang", "Kaishun Wu"], "title": "DACA-Net: A Degradation-Aware Conditional Diffusion Network for Underwater Image Enhancement", "categories": ["cs.CV", "eess.IV"], "comment": "accepted by ACM MM 2025", "summary": "Underwater images typically suffer from severe colour distortions, low visibility, and reduced structural clarity due to complex optical effects such as scattering and absorption, which greatly degrade their visual quality and limit the performance of downstream visual perception tasks. Existing enhancement methods often struggle to adaptively handle diverse degradation conditions and fail to leverage underwater-specific physical priors effectively. In this paper, we propose a degradation-aware conditional diffusion model to enhance underwater images adaptively and robustly. Given a degraded underwater image as input, we first predict its degradation level using a lightweight dual-stream convolutional network, generating a continuous degradation score as semantic guidance. Based on this score, we introduce a novel conditional diffusion-based restoration network with a Swin UNet backbone, enabling adaptive noise scheduling and hierarchical feature refinement. To incorporate underwater-specific physical priors, we further propose a degradation-guided adaptive feature fusion module and a hybrid loss function that combines perceptual consistency, histogram matching, and feature-level contrast. Comprehensive experiments on benchmark datasets demonstrate that our method effectively restores underwater images with superior colour fidelity, perceptual quality, and structural details. Compared with SOTA approaches, our framework achieves significant improvements in both quantitative metrics and qualitative visual assessments."}
{"id": "2507.22604", "pdf": "https://arxiv.org/pdf/2507.22604", "abs": "https://arxiv.org/abs/2507.22604", "authors": ["Xiefan Guo", "Miaomiao Cui", "Liefeng Bo", "Di Huang"], "title": "ShortFT: Diffusion Model Alignment via Shortcut-based Fine-Tuning", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Backpropagation-based approaches aim to align diffusion models with reward functions through end-to-end backpropagation of the reward gradient within the denoising chain, offering a promising perspective. However, due to the computational costs and the risk of gradient explosion associated with the lengthy denoising chain, existing approaches struggle to achieve complete gradient backpropagation, leading to suboptimal results. In this paper, we introduce Shortcut-based Fine-Tuning (ShortFT), an efficient fine-tuning strategy that utilizes the shorter denoising chain. More specifically, we employ the recently researched trajectory-preserving few-step diffusion model, which enables a shortcut over the original denoising chain, and construct a shortcut-based denoising chain of shorter length. The optimization on this chain notably enhances the efficiency and effectiveness of fine-tuning the foundational model. Our method has been rigorously tested and can be effectively applied to various reward functions, significantly improving alignment performance and surpassing state-of-the-art alternatives."}
{"id": "2507.22615", "pdf": "https://arxiv.org/pdf/2507.22615", "abs": "https://arxiv.org/abs/2507.22615", "authors": ["Daehee Park", "Monu Surana", "Pranav Desai", "Ashish Mehta", "Reuben MV John", "Kuk-Jin Yoon"], "title": "Generative Active Learning for Long-tail Trajectory Prediction via Controllable Diffusion Model", "categories": ["cs.CV"], "comment": "Accepted at ICCV 2025", "summary": "While data-driven trajectory prediction has enhanced the reliability of autonomous driving systems, it still struggles with rarely observed long-tail scenarios. Prior works addressed this by modifying model architectures, such as using hypernetworks. In contrast, we propose refining the training process to unlock each model's potential without altering its structure. We introduce Generative Active Learning for Trajectory prediction (GALTraj), the first method to successfully deploy generative active learning into trajectory prediction. It actively identifies rare tail samples where the model fails and augments these samples with a controllable diffusion model during training. In our framework, generating scenarios that are diverse, realistic, and preserve tail-case characteristics is paramount. Accordingly, we design a tail-aware generation method that applies tailored diffusion guidance to generate trajectories that both capture rare behaviors and respect traffic rules. Unlike prior simulation methods focused solely on scenario diversity, GALTraj is the first to show how simulator-driven augmentation benefits long-tail learning in trajectory prediction. Experiments on multiple trajectory datasets (WOMD, Argoverse2) with popular backbones (QCNet, MTR) confirm that our method significantly boosts performance on tail samples and also enhances accuracy on head samples."}
{"id": "2507.22627", "pdf": "https://arxiv.org/pdf/2507.22627", "abs": "https://arxiv.org/abs/2507.22627", "authors": ["Federico Girella", "Davide Talon", "Ziyue Liu", "Zanxi Ruan", "Yiming Wang", "Marco Cristani"], "title": "LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text Pairing", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at ICCV25 (Oral). Project page:   https://intelligolabs.github.io/lots/", "summary": "Fashion design is a complex creative process that blends visual and textual expressions. Designers convey ideas through sketches, which define spatial structure and design elements, and textual descriptions, capturing material, texture, and stylistic details. In this paper, we present LOcalized Text and Sketch for fashion image generation (LOTS), an approach for compositional sketch-text based generation of complete fashion outlooks. LOTS leverages a global description with paired localized sketch + text information for conditioning and introduces a novel step-based merging strategy for diffusion adaptation. First, a Modularized Pair-Centric representation encodes sketches and text into a shared latent space while preserving independent localized features; then, a Diffusion Pair Guidance phase integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we build on Fashionpedia to release Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Quantitative results show LOTS achieves state-of-the-art image generation performance on both global and localized metrics, while qualitative examples and a human evaluation study highlight its unprecedented level of design customization."}
{"id": "2507.22692", "pdf": "https://arxiv.org/pdf/2507.22692", "abs": "https://arxiv.org/abs/2507.22692", "authors": ["Lemar Abdi", "Amaan Valiuddin", "Francisco Caetano", "Christiaan Viviers", "Fons van der Sommen"], "title": "Zero-Shot Image Anomaly Detection Using Generative Foundation Models", "categories": ["cs.CV"], "comment": "Accepted at the workshop of Anomaly Detection with Foundation Models,   ICCV 2025", "summary": "Detecting out-of-distribution (OOD) inputs is pivotal for deploying safe vision systems in open-world environments. We revisit diffusion models, not as generators, but as universal perceptual templates for OOD detection. This research explores the use of score-based generative models as foundational tools for semantic anomaly detection across unseen datasets. Specifically, we leverage the denoising trajectories of Denoising Diffusion Models (DDMs) as a rich source of texture and semantic information. By analyzing Stein score errors, amplified through the Structural Similarity Index Metric (SSIM), we introduce a novel method for identifying anomalous samples without requiring re-training on each target dataset. Our approach improves over state-of-the-art and relies on training a single model on one dataset -- CelebA -- which we find to be an effective base distribution, even outperforming more commonly used datasets like ImageNet in several settings. Experimental results show near-perfect performance on some benchmarks, with notable headroom on others, highlighting both the strength and future potential of generative foundation models in anomaly detection."}
{"id": "2507.22742", "pdf": "https://arxiv.org/pdf/2507.22742", "abs": "https://arxiv.org/abs/2507.22742", "authors": ["Yang Gao", "Saeed Saadatnejad", "Alexandre Alahi"], "title": "Social-Pose: Enhancing Trajectory Prediction with Human Body Pose", "categories": ["cs.CV"], "comment": "Accepted to IEEE Transactions on Intelligent Transportation Systems   (T-ITS)", "summary": "Accurate human trajectory prediction is one of the most crucial tasks for autonomous driving, ensuring its safety. Yet, existing models often fail to fully leverage the visual cues that humans subconsciously communicate when navigating the space. In this work, we study the benefits of predicting human trajectories using human body poses instead of solely their Cartesian space locations in time. We propose `Social-pose', an attention-based pose encoder that effectively captures the poses of all humans in a scene and their social relations. Our method can be integrated into various trajectory prediction architectures. We have conducted extensive experiments on state-of-the-art models (based on LSTM, GAN, MLP, and Transformer), and showed improvements over all of them on synthetic (Joint Track Auto) and real (Human3.6M, Pedestrians and Cyclists in Road Traffic, and JRDB) datasets. We also explored the advantages of using 2D versus 3D poses, as well as the effect of noisy poses and the application of our pose-based predictor in robot navigation scenarios."}
{"id": "2507.22792", "pdf": "https://arxiv.org/pdf/2507.22792", "abs": "https://arxiv.org/abs/2507.22792", "authors": ["Guoping Xu", "Jayaram K. Udupa", "Yajun Yu", "Hua-Chieh Shao", "Songlin Zhao", "Wei Liu", "You Zhang"], "title": "Segment Anything for Video: A Comprehensive Review of Video Object Segmentation and Tracking from Past to Future", "categories": ["cs.CV"], "comment": "45 pages, 21 figures", "summary": "Video Object Segmentation and Tracking (VOST) presents a complex yet critical challenge in computer vision, requiring robust integration of segmentation and tracking across temporally dynamic frames. Traditional methods have struggled with domain generalization, temporal consistency, and computational efficiency. The emergence of foundation models like the Segment Anything Model (SAM) and its successor, SAM2, has introduced a paradigm shift, enabling prompt-driven segmentation with strong generalization capabilities. Building upon these advances, this survey provides a comprehensive review of SAM/SAM2-based methods for VOST, structured along three temporal dimensions: past, present, and future. We examine strategies for retaining and updating historical information (past), approaches for extracting and optimizing discriminative features from the current frame (present), and motion prediction and trajectory estimation mechanisms for anticipating object dynamics in subsequent frames (future). In doing so, we highlight the evolution from early memory-based architectures to the streaming memory and real-time segmentation capabilities of SAM2. We also discuss recent innovations such as motion-aware memory selection and trajectory-guided prompting, which aim to enhance both accuracy and efficiency. Finally, we identify remaining challenges including memory redundancy, error accumulation, and prompt inefficiency, and suggest promising directions for future research. This survey offers a timely and structured overview of the field, aiming to guide researchers and practitioners in advancing the state of VOST through the lens of foundation models."}
{"id": "2507.22802", "pdf": "https://arxiv.org/pdf/2507.22802", "abs": "https://arxiv.org/abs/2507.22802", "authors": ["Dongli He", "Hu Wang", "Mohammad Yaqub"], "title": "Advancing Fetal Ultrasound Image Quality Assessment in Low-Resource Settings", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to the MICCAI 2025 MIRASOL Workshop", "summary": "Accurate fetal biometric measurements, such as abdominal circumference, play a vital role in prenatal care. However, obtaining high-quality ultrasound images for these measurements heavily depends on the expertise of sonographers, posing a significant challenge in low-income countries due to the scarcity of trained personnel. To address this issue, we leverage FetalCLIP, a vision-language model pretrained on a curated dataset of over 210,000 fetal ultrasound image-caption pairs, to perform automated fetal ultrasound image quality assessment (IQA) on blind-sweep ultrasound data. We introduce FetalCLIP$_{CLS}$, an IQA model adapted from FetalCLIP using Low-Rank Adaptation (LoRA), and evaluate it on the ACOUSLIC-AI dataset against six CNN and Transformer baselines. FetalCLIP$_{CLS}$ achieves the highest F1 score of 0.757. Moreover, we show that an adapted segmentation model, when repurposed for classification, further improves performance, achieving an F1 score of 0.771. Our work demonstrates how parameter-efficient fine-tuning of fetal ultrasound foundation models can enable task-specific adaptations, advancing prenatal care in resource-limited settings. The experimental code is available at: https://github.com/donglihe-hub/FetalCLIP-IQA."}
{"id": "2507.22873", "pdf": "https://arxiv.org/pdf/2507.22873", "abs": "https://arxiv.org/abs/2507.22873", "authors": ["Simon Pochinda", "Momen K. Tageldeen", "Mark Thompson", "Tony Rinaldi", "Troy Giorshev", "Keith Lee", "Jie Zhou", "Frederick Walls"], "title": "LCS: An AI-based Low-Complexity Scaler for Power-Efficient Super-Resolution of Game Content", "categories": ["cs.CV", "cs.LG"], "comment": "8 pages, 3 figures", "summary": "The increasing complexity of content rendering in modern games has led to a problematic growth in the workload of the GPU. In this paper, we propose an AI-based low-complexity scaler (LCS) inspired by state-of-the-art efficient super-resolution (ESR) models which could offload the workload on the GPU to a low-power device such as a neural processing unit (NPU). The LCS is trained on GameIR image pairs natively rendered at low and high resolution. We utilize adversarial training to encourage reconstruction of perceptually important details, and apply reparameterization and quantization techniques to reduce model complexity and size. In our comparative analysis we evaluate the LCS alongside the publicly available AMD hardware-based Edge Adaptive Scaling Function (EASF) and AMD FidelityFX Super Resolution 1 (FSR1) on five different metrics, and find that the LCS achieves better perceptual quality, demonstrating the potential of ESR models for upscaling on resource-constrained devices."}
{"id": "2507.22617", "pdf": "https://arxiv.org/pdf/2507.22617", "abs": "https://arxiv.org/abs/2507.22617", "authors": ["Yiting Qu", "Ziqing Yang", "Yihan Ma", "Michael Backes", "Savvas Zannettou", "Yang Zhang"], "title": "Hate in Plain Sight: On the Risks of Moderating AI-Generated Hateful Illusions", "categories": ["cs.CR", "cs.CV"], "comment": "Accepted at ICCV 2025", "summary": "Recent advances in text-to-image diffusion models have enabled the creation of a new form of digital art: optical illusions--visual tricks that create different perceptions of reality. However, adversaries may misuse such techniques to generate hateful illusions, which embed specific hate messages into harmless scenes and disseminate them across web communities. In this work, we take the first step toward investigating the risks of scalable hateful illusion generation and the potential for bypassing current content moderation models. Specifically, we generate 1,860 optical illusions using Stable Diffusion and ControlNet, conditioned on 62 hate messages. Of these, 1,571 are hateful illusions that successfully embed hate messages, either overtly or subtly, forming the Hateful Illusion dataset. Using this dataset, we evaluate the performance of six moderation classifiers and nine vision language models (VLMs) in identifying hateful illusions. Experimental results reveal significant vulnerabilities in existing moderation models: the detection accuracy falls below 0.245 for moderation classifiers and below 0.102 for VLMs. We further identify a critical limitation in their vision encoders, which mainly focus on surface-level image details while overlooking the secondary layer of information, i.e., hidden messages. To address this risk, we explore preliminary mitigation measures and identify the most effective approaches from the perspectives of image transformations and training-level strategies."}
