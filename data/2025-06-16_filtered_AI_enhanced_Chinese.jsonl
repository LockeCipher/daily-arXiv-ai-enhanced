{"id": "2506.11252", "pdf": "https://arxiv.org/pdf/2506.11252", "abs": "https://arxiv.org/abs/2506.11252", "authors": ["Mae Younes", "Adnane Boukhayma"], "title": "Anti-Aliased 2D Gaussian Splatting", "categories": ["cs.GR", "cs.CV"], "comment": "Code will be available at https://github.com/maeyounes/AA-2DGS", "summary": "2D Gaussian Splatting (2DGS) has recently emerged as a promising method for novel view synthesis and surface reconstruction, offering better view-consistency and geometric accuracy than volumetric 3DGS. However, 2DGS suffers from severe aliasing artifacts when rendering at different sampling rates than those used during training, limiting its practical applications in scenarios requiring camera zoom or varying fields of view. We identify that these artifacts stem from two key limitations: the lack of frequency constraints in the representation and an ineffective screen-space clamping approach. To address these issues, we present AA-2DGS, an antialiased formulation of 2D Gaussian Splatting that maintains its geometric benefits while significantly enhancing rendering quality across different scales. Our method introduces a world space flat smoothing kernel that constrains the frequency content of 2D Gaussian primitives based on the maximal sampling frequency from training views, effectively eliminating high-frequency artifacts when zooming in. Additionally, we derive a novel object space Mip filter by leveraging an affine approximation of the ray-splat intersection mapping, which allows us to efficiently apply proper anti-aliasing directly in the local space of each splat.", "AI": {"tldr": "AA-2DGS\u63d0\u51fa\u4e86\u4e00\u79cd\u6297\u952f\u9f7f\u76842D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e862DGS\u5728\u4e0d\u540c\u91c7\u6837\u7387\u4e0b\u7684\u6e32\u67d3\u95ee\u9898\u3002", "motivation": "2DGS\u5728\u65b0\u89c6\u89d2\u5408\u6210\u548c\u8868\u9762\u91cd\u5efa\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u4e0d\u540c\u91c7\u6837\u7387\u4e0b\u4f1a\u51fa\u73b0\u4e25\u91cd\u7684\u952f\u9f7f\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5f15\u5165\u4e16\u754c\u7a7a\u95f4\u5e73\u6ed1\u6838\u548c\u5bf9\u8c61\u7a7a\u95f4Mip\u6ee4\u6ce2\u5668\uff0c\u7ea6\u675f\u9ad8\u65af\u57fa\u5143\u7684\u9891\u7387\u5185\u5bb9\u5e76\u4f18\u5316\u6297\u952f\u9f7f\u6548\u679c\u3002", "result": "AA-2DGS\u663e\u8457\u63d0\u5347\u4e86\u4e0d\u540c\u5c3a\u5ea6\u4e0b\u7684\u6e32\u67d3\u8d28\u91cf\uff0c\u6d88\u9664\u4e86\u9ad8\u9891\u952f\u9f7f\u3002", "conclusion": "AA-2DGS\u5728\u4fdd\u6301\u51e0\u4f55\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u6709\u6548\u89e3\u51b3\u4e862DGS\u7684\u952f\u9f7f\u95ee\u9898\uff0c\u6269\u5c55\u4e86\u5176\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2506.11273", "pdf": "https://arxiv.org/pdf/2506.11273", "abs": "https://arxiv.org/abs/2506.11273", "authors": ["Daniel Meister", "Jakub Bok\u0161ansk\u00fd", "Michael Guthe", "Ji\u0159\u00ed Bittner"], "title": "On Ray Reordering Techniques for Faster GPU Ray Tracing", "categories": ["cs.GR"], "comment": null, "summary": "We study ray reordering as a tool for increasing the performance of existing GPU ray tracing implementations. We focus on ray reordering that is fully agnostic to the particular trace kernel. We summarize the existing methods for computing the ray sorting keys and discuss their properties. We propose a novel modification of a previously proposed method using the termination point estimation that is well-suited to tracing secondary rays. We evaluate the ray reordering techniques in the context of the wavefront path tracing using the RTX trace kernels. We show that ray reordering yields significantly higher trace speed on recent GPUs (1.3-2.0x), but to recover the reordering overhead in the hardware-accelerated trace phase is problematic.", "AI": {"tldr": "\u7814\u7a76\u5c04\u7ebf\u91cd\u6392\u5e8f\u4ee5\u63d0\u9ad8GPU\u5c04\u7ebf\u8ffd\u8e2a\u6027\u80fd\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7ec8\u6b62\u70b9\u4f30\u8ba1\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u5728RTX\u5185\u6838\u4e2d\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "motivation": "\u63d0\u9ad8\u73b0\u6709GPU\u5c04\u7ebf\u8ffd\u8e2a\u5b9e\u73b0\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u4e8c\u6b21\u5c04\u7ebf\u7684\u8ffd\u8e2a\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7ec8\u6b62\u70b9\u4f30\u8ba1\u7684\u5c04\u7ebf\u91cd\u6392\u5e8f\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6ce2\u524d\u8def\u5f84\u8ffd\u8e2a\u548cRTX\u5185\u6838\u3002", "result": "\u5c04\u7ebf\u91cd\u6392\u5e8f\u663e\u8457\u63d0\u5347\u8ffd\u8e2a\u901f\u5ea6\uff081.3-2.0\u500d\uff09\uff0c\u4f46\u786c\u4ef6\u52a0\u901f\u9636\u6bb5\u7684\u6392\u5e8f\u5f00\u9500\u96be\u4ee5\u5b8c\u5168\u62b5\u6d88\u3002", "conclusion": "\u5c04\u7ebf\u91cd\u6392\u5e8f\u5728\u63d0\u5347\u6027\u80fd\u65b9\u9762\u6709\u6548\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u5e73\u8861\u6392\u5e8f\u5f00\u9500\u3002"}}
{"id": "2506.11510", "pdf": "https://arxiv.org/pdf/2506.11510", "abs": "https://arxiv.org/abs/2506.11510", "authors": ["Anis Benyoub", "Jonathan Dupuy"], "title": "Adaptive Tetrahedral Grids for Volumetric Path-Tracing", "categories": ["cs.GR"], "comment": null, "summary": "We advertise the use of tetrahedral grids constructed via the longest edge bisection algorithm for rendering volumetric data with path tracing. The key benefits of such grids is two-fold. First, they provide a highly adaptive space-partitioning representation that limits the memory footprint of volumetric assets. Second, each (tetrahedral) cell has exactly 4 neighbors within the volume (one per face of each tetrahedron) or less at boundaries. We leverage these properties to devise optimized algorithms and data-structures to compute and path-trace adaptive tetrahedral grids on the GPU. In practice, our GPU implementation outperforms regular grids by up to x30 and renders production assets in real time at 32 samples per pixel.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u6700\u957f\u8fb9\u4e8c\u5206\u7b97\u6cd5\u6784\u5efa\u7684\u56db\u9762\u4f53\u7f51\u683c\u8fdb\u884c\u4f53\u79ef\u6570\u636e\u8def\u5f84\u8ffd\u8e2a\u6e32\u67d3\uff0c\u5176\u4f18\u52bf\u5728\u4e8e\u9ad8\u9002\u5e94\u6027\u7a7a\u95f4\u5206\u533a\u548c\u4f4e\u5185\u5b58\u5360\u7528\uff0cGPU\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "motivation": "\u4f20\u7edf\u7f51\u683c\u5728\u4f53\u79ef\u6570\u636e\u6e32\u67d3\u4e2d\u5b58\u5728\u5185\u5b58\u5360\u7528\u9ad8\u548c\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u56db\u9762\u4f53\u7f51\u683c\u901a\u8fc7\u5176\u72ec\u7279\u7ed3\u6784\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6700\u957f\u8fb9\u4e8c\u5206\u7b97\u6cd5\u6784\u5efa\u56db\u9762\u4f53\u7f51\u683c\uff0c\u5e76\u8bbe\u8ba1\u4f18\u5316\u7684GPU\u7b97\u6cd5\u548c\u6570\u636e\u7ed3\u6784\uff0c\u652f\u6301\u8def\u5f84\u8ffd\u8e2a\u3002", "result": "GPU\u5b9e\u73b0\u6027\u80fd\u6bd4\u5e38\u89c4\u7f51\u683c\u63d0\u5347\u9ad8\u8fbe30\u500d\uff0c\u652f\u6301\u5b9e\u65f6\u6e32\u67d332\u6837\u672c/\u50cf\u7d20\u7684\u751f\u4ea7\u7ea7\u8d44\u4ea7\u3002", "conclusion": "\u56db\u9762\u4f53\u7f51\u683c\u5728\u4f53\u79ef\u6570\u636e\u6e32\u67d3\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u9002\u5408\u9ad8\u6027\u80fdGPU\u5b9e\u73b0\u3002"}}
{"id": "2506.11546", "pdf": "https://arxiv.org/pdf/2506.11546", "abs": "https://arxiv.org/abs/2506.11546", "authors": ["Akshay Jindal", "Nabil Sadaka", "Manu Mathew Thomas", "Anton Sochenov", "Anton Kaplanyan"], "title": "CGVQM+D: Computer Graphics Video Quality Metric and Dataset", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "While existing video and image quality datasets have extensively studied natural videos and traditional distortions, the perception of synthetic content and modern rendering artifacts remains underexplored. We present a novel video quality dataset focused on distortions introduced by advanced rendering techniques, including neural supersampling, novel-view synthesis, path tracing, neural denoising, frame interpolation, and variable rate shading. Our evaluations show that existing full-reference quality metrics perform sub-optimally on these distortions, with a maximum Pearson correlation of 0.78. Additionally, we find that the feature space of pre-trained 3D CNNs aligns strongly with human perception of visual quality. We propose CGVQM, a full-reference video quality metric that significantly outperforms existing metrics while generating both per-pixel error maps and global quality scores. Our dataset and metric implementation is available at https://github.com/IntelLabs/CGVQM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u9ad8\u7ea7\u6e32\u67d3\u6280\u672f\u5f15\u5165\u7684\u5931\u771f\u7684\u89c6\u9891\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u65b0\u7684\u8d28\u91cf\u8bc4\u4f30\u6307\u6807CGVQM\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u548c\u56fe\u50cf\u8d28\u91cf\u6570\u636e\u96c6\u4e3b\u8981\u7814\u7a76\u81ea\u7136\u89c6\u9891\u548c\u4f20\u7edf\u5931\u771f\uff0c\u5bf9\u5408\u6210\u5185\u5bb9\u548c\u73b0\u4ee3\u6e32\u67d3\u5931\u771f\u7684\u611f\u77e5\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u578b\u89c6\u9891\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u795e\u7ecf\u8d85\u91c7\u6837\u3001\u65b0\u89c6\u89d2\u5408\u6210\u7b49\u9ad8\u7ea7\u6e32\u67d3\u6280\u672f\u7684\u5931\u771f\uff0c\u5e76\u5f00\u53d1\u4e86CGVQM\u6307\u6807\u3002", "result": "\u73b0\u6709\u5168\u53c2\u8003\u8d28\u91cf\u6307\u6807\u5728\u8fd9\u4e9b\u5931\u771f\u4e0a\u8868\u73b0\u4e0d\u4f73\uff08\u6700\u5927Pearson\u76f8\u5173\u7cfb\u6570\u4e3a0.78\uff09\uff0cCGVQM\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6307\u6807\u3002", "conclusion": "CGVQM\u5728\u8bc4\u4f30\u6e32\u67d3\u5931\u771f\u65f6\u8868\u73b0\u4f18\u5f02\uff0c\u6570\u636e\u96c6\u548c\u6307\u6807\u5b9e\u73b0\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.11122", "pdf": "https://arxiv.org/pdf/2506.11122", "abs": "https://arxiv.org/abs/2506.11122", "authors": ["Divya Swetha K", "Ziaul Haque Choudhury", "Hemanta Kumar Bhuyan", "Biswajit Brahma", "Nilayam Kumar Kamila"], "title": "Adaptive Object Detection with ESRGAN-Enhanced Resolution & Faster R-CNN", "categories": ["cs.CV"], "comment": null, "summary": "In this study, proposes a method for improved object detection from the low-resolution images by integrating Enhanced Super-Resolution Generative Adversarial Networks (ESRGAN) and Faster Region-Convolutional Neural Network (Faster R-CNN). ESRGAN enhances low-quality images, restoring details and improving clarity, while Faster R-CNN performs accurate object detection on the enhanced images. The combination of these techniques ensures better detection performance, even with poor-quality inputs, offering an effective solution for applications where image resolution is in consistent. ESRGAN is employed as a pre-processing step to enhance the low-resolution input image, effectively restoring lost details and improving overall image quality. Subsequently, the enhanced image is fed into the Faster R-CNN model for accurate object detection and localization. Experimental results demonstrate that this integrated approach yields superior performance compared to traditional methods applied directly to low-resolution images. The proposed framework provides a promising solution for applications where image quality is variable or limited, enabling more robust and reliable object detection in challenging scenarios. It achieves a balance between improved image quality and efficient object detection", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408ESRGAN\u548cFaster R-CNN\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u63d0\u5347\u7269\u4f53\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u7269\u4f53\u68c0\u6d4b\u6027\u80fd\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u56fe\u50cf\u8d28\u91cf\u4e0d\u4e00\u81f4\u7684\u5e94\u7528\u573a\u666f\u3002", "method": "\u4f7f\u7528ESRGAN\u4f5c\u4e3a\u9884\u5904\u7406\u6b65\u9aa4\u589e\u5f3a\u56fe\u50cf\u8d28\u91cf\uff0c\u518d\u901a\u8fc7Faster R-CNN\u8fdb\u884c\u7269\u4f53\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u7684\u68c0\u6d4b\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u56fe\u50cf\u8d28\u91cf\u53d7\u9650\u7684\u573a\u666f\u4e2d\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u53ef\u9760\u7684\u7269\u4f53\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.11124", "pdf": "https://arxiv.org/pdf/2506.11124", "abs": "https://arxiv.org/abs/2506.11124", "authors": ["Yifei Chen", "Ross Greer"], "title": "Technical Report for Argoverse2 Scenario Mining Challenges on Iterative Error Correction and Spatially-Aware Prompting", "categories": ["cs.CV", "cs.SE"], "comment": null, "summary": "Scenario mining from extensive autonomous driving datasets, such as Argoverse 2, is crucial for the development and validation of self-driving systems. The RefAV framework represents a promising approach by employing Large Language Models (LLMs) to translate natural-language queries into executable code for identifying relevant scenarios. However, this method faces challenges, including runtime errors stemming from LLM-generated code and inaccuracies in interpreting parameters for functions that describe complex multi-object spatial relationships. This technical report introduces two key enhancements to address these limitations: (1) a fault-tolerant iterative code-generation mechanism that refines code by re-prompting the LLM with error feedback, and (2) specialized prompt engineering that improves the LLM's comprehension and correct application of spatial-relationship functions. Experiments on the Argoverse 2 validation set with diverse LLMs-Qwen2.5-VL-7B, Gemini 2.5 Flash, and Gemini 2.5 Pro-show consistent gains across multiple metrics; most notably, the proposed system achieves a HOTA-Temporal score of 52.37 on the official test set using Gemini 2.5 Pro. These results underline the efficacy of the proposed techniques for reliable, high-precision scenario mining.", "AI": {"tldr": "RefAV\u6846\u67b6\u901a\u8fc7LLM\u5c06\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u4ee3\u7801\u4ee5\u6316\u6398\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\uff0c\u4f46\u9762\u4e34\u4ee3\u7801\u9519\u8bef\u548c\u53c2\u6570\u89e3\u91ca\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u6539\u8fdb\u65b9\u6cd5\uff1a\u5bb9\u9519\u8fed\u4ee3\u4ee3\u7801\u751f\u6210\u548c\u4e13\u7528\u63d0\u793a\u5de5\u7a0b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3LLM\u751f\u6210\u4ee3\u7801\u7684\u8fd0\u884c\u65f6\u9519\u8bef\u548c\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u53c2\u6570\u89e3\u91ca\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u6316\u6398\u7684\u53ef\u9760\u6027\u3002", "method": "1. \u5bb9\u9519\u8fed\u4ee3\u4ee3\u7801\u751f\u6210\u673a\u5236\uff0c\u901a\u8fc7\u9519\u8bef\u53cd\u9988\u91cd\u65b0\u63d0\u793aLLM\u4f18\u5316\u4ee3\u7801\uff1b2. \u4e13\u7528\u63d0\u793a\u5de5\u7a0b\uff0c\u63d0\u9ad8LLM\u5bf9\u7a7a\u95f4\u5173\u7cfb\u51fd\u6570\u7684\u7406\u89e3\u548c\u5e94\u7528\u51c6\u786e\u6027\u3002", "result": "\u5728Argoverse 2\u9a8c\u8bc1\u96c6\u4e0a\uff0c\u4f7f\u7528\u591a\u79cdLLM\uff08Qwen2.5-VL-7B\u3001Gemini 2.5 Flash\u548cGemini 2.5 Pro\uff09\u5747\u53d6\u5f97\u4e00\u81f4\u63d0\u5347\uff0cGemini 2.5 Pro\u7684HOTA-Temporal\u5f97\u5206\u8fbe52.37\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u6316\u6398\u7684\u53ef\u9760\u6027\u548c\u7cbe\u5ea6\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2506.11136", "pdf": "https://arxiv.org/pdf/2506.11136", "abs": "https://arxiv.org/abs/2506.11136", "authors": ["Paul Couairon", "Loick Chambon", "Louis Serrano", "Jean-Emmanuel Haugeard", "Matthieu Cord", "Nicolas Thome"], "title": "JAFAR: Jack up Any Feature at Any Resolution", "categories": ["cs.CV", "eess.IV"], "comment": "Code available at https://github.com/PaulCouairon/JAFAR", "summary": "Foundation Vision Encoders have become essential for a wide range of dense vision tasks. However, their low-resolution spatial feature outputs necessitate feature upsampling to produce the high-resolution modalities required for downstream tasks. In this work, we introduce JAFAR, a lightweight and flexible feature upsampler that enhances the spatial resolution of visual features from any Foundation Vision Encoder to an arbitrary target resolution. JAFAR employs an attention-based module designed to promote semantic alignment between high-resolution queries, derived from low-level image features, and semantically enriched low-resolution keys, using Spatial Feature Transform (SFT) modulation. Notably, despite the absence of high-resolution supervision, we demonstrate that learning at low upsampling ratios and resolutions generalizes remarkably well to significantly higher output scales. Extensive experiments show that JAFAR effectively recovers fine-grained spatial details and consistently outperforms existing feature upsampling methods across a diverse set of downstream tasks. Project page at https://jafar-upsampler.github.io", "AI": {"tldr": "JAFAR\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u7075\u6d3b\u7684\u7279\u5f81\u4e0a\u91c7\u6837\u5668\uff0c\u7528\u4e8e\u63d0\u5347\u57fa\u7840\u89c6\u89c9\u7f16\u7801\u5668\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\uff0c\u65e0\u9700\u9ad8\u5206\u8fa8\u7387\u76d1\u7763\u5373\u53ef\u6cdb\u5316\u5230\u66f4\u9ad8\u8f93\u51fa\u5c3a\u5ea6\u3002", "motivation": "\u57fa\u7840\u89c6\u89c9\u7f16\u7801\u5668\u7684\u4f4e\u5206\u8fa8\u7387\u7a7a\u95f4\u7279\u5f81\u8f93\u51fa\u9700\u8981\u4e0a\u91c7\u6837\u4ee5\u6ee1\u8db3\u4e0b\u6e38\u4efb\u52a1\u7684\u9ad8\u5206\u8fa8\u7387\u9700\u6c42\u3002", "method": "JAFAR\u91c7\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6a21\u5757\uff0c\u901a\u8fc7\u7a7a\u95f4\u7279\u5f81\u53d8\u6362\uff08SFT\uff09\u8c03\u5236\uff0c\u4fc3\u8fdb\u9ad8\u5206\u8fa8\u7387\u67e5\u8be2\u4e0e\u8bed\u4e49\u4e30\u5bcc\u7684\u4f4e\u5206\u8fa8\u7387\u952e\u4e4b\u95f4\u7684\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cJAFAR\u80fd\u6709\u6548\u6062\u590d\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u7ec6\u8282\uff0c\u5e76\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u4e0a\u91c7\u6837\u65b9\u6cd5\u3002", "conclusion": "JAFAR\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u7279\u5f81\u4e0a\u91c7\u6837\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2506.11144", "pdf": "https://arxiv.org/pdf/2506.11144", "abs": "https://arxiv.org/abs/2506.11144", "authors": ["Chao Liang", "Jianwen Jiang", "Wang Liao", "Jiaqi Yang", "Zerong zheng", "Weihong Zeng", "Han Liang"], "title": "AlignHuman: Improving Motion and Fidelity via Timestep-Segment Preference Optimization for Audio-Driven Human Animation", "categories": ["cs.CV"], "comment": "Homepage: https://alignhuman.github.io/", "summary": "Recent advancements in human video generation and animation tasks, driven by diffusion models, have achieved significant progress. However, expressive and realistic human animation remains challenging due to the trade-off between motion naturalness and visual fidelity. To address this, we propose \\textbf{AlignHuman}, a framework that combines Preference Optimization as a post-training technique with a divide-and-conquer training strategy to jointly optimize these competing objectives. Our key insight stems from an analysis of the denoising process across timesteps: (1) early denoising timesteps primarily control motion dynamics, while (2) fidelity and human structure can be effectively managed by later timesteps, even if early steps are skipped. Building on this observation, we propose timestep-segment preference optimization (TPO) and introduce two specialized LoRAs as expert alignment modules, each targeting a specific dimension in its corresponding timestep interval. The LoRAs are trained using their respective preference data and activated in the corresponding intervals during inference to enhance motion naturalness and fidelity. Extensive experiments demonstrate that AlignHuman improves strong baselines and reduces NFEs during inference, achieving a 3.3$\\times$ speedup (from 100 NFEs to 30 NFEs) with minimal impact on generation quality. Homepage: \\href{https://alignhuman.github.io/}{https://alignhuman.github.io/}", "AI": {"tldr": "AlignHuman\u901a\u8fc7\u504f\u597d\u4f18\u5316\u548c\u5206\u6cbb\u8bad\u7ec3\u7b56\u7565\uff0c\u4f18\u5316\u4e86\u4eba\u7c7b\u89c6\u9891\u751f\u6210\u4e2d\u8fd0\u52a8\u81ea\u7136\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u7684\u6743\u8861\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u4eba\u7c7b\u89c6\u9891\u751f\u6210\u4e2d\u8fd0\u52a8\u81ea\u7136\u6027\u4e0e\u89c6\u89c9\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51faAlignHuman\u6846\u67b6\uff0c\u7ed3\u5408\u504f\u597d\u4f18\u5316\u548c\u5206\u6cbb\u8bad\u7ec3\u7b56\u7565\uff0c\u5229\u7528\u65f6\u95f4\u6b65\u5206\u6bb5\u504f\u597d\u4f18\u5316\uff08TPO\uff09\u548c\u4e24\u4e2a\u4e13\u7528LoRA\u6a21\u5757\u5206\u522b\u4f18\u5316\u8fd0\u52a8\u52a8\u6001\u548c\u4fdd\u771f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAlignHuman\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7ebf\u6027\u80fd\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53473.3\u500d\uff08\u4ece100 NFEs\u964d\u81f330 NFEs\uff09\uff0c\u4e14\u751f\u6210\u8d28\u91cf\u5f71\u54cd\u6781\u5c0f\u3002", "conclusion": "AlignHuman\u901a\u8fc7\u5206\u6cbb\u7b56\u7565\u548c\u504f\u597d\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4eba\u7c7b\u89c6\u9891\u751f\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u548c\u9ad8\u6548\u751f\u6210\u3002"}}
{"id": "2506.11302", "pdf": "https://arxiv.org/pdf/2506.11302", "abs": "https://arxiv.org/abs/2506.11302", "authors": ["H\u00e9ctor Carri\u00f3n", "Yutong Bai", "V\u00edctor A. Hern\u00e1ndez Castro", "Kishan Panaganti", "Ayush Zenith", "Matthew Trang", "Tony Zhang", "Pietro Perona", "Jitendra Malik"], "title": "TARDIS STRIDE: A Spatio-Temporal Road Image Dataset for Exploration and Autonomy", "categories": ["cs.CV", "cs.AI"], "comment": "Computer Vision, Pattern Recognition, LLMs, Dataset, Data   Augmentation", "summary": "World models aim to simulate environments and enable effective agent behavior. However, modeling real-world environments presents unique challenges as they dynamically change across both space and, crucially, time. To capture these composed dynamics, we introduce a Spatio-Temporal Road Image Dataset for Exploration (STRIDE) permuting 360-degree panoramic imagery into rich interconnected observation, state and action nodes. Leveraging this structure, we can simultaneously model the relationship between egocentric views, positional coordinates, and movement commands across both space and time. We benchmark this dataset via TARDIS, a transformer-based generative world model that integrates spatial and temporal dynamics through a unified autoregressive framework trained on STRIDE. We demonstrate robust performance across a range of agentic tasks such as controllable photorealistic image synthesis, instruction following, autonomous self-control, and state-of-the-art georeferencing. These results suggest a promising direction towards sophisticated generalist agents--capable of understanding and manipulating the spatial and temporal aspects of their material environments--with enhanced embodied reasoning capabilities. Training code, datasets, and model checkpoints are made available at https://huggingface.co/datasets/Tera-AI/STRIDE.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSTRIDE\u7684\u6570\u636e\u96c6\u548cTARDIS\u6a21\u578b\uff0c\u7528\u4e8e\u6a21\u62df\u52a8\u6001\u65f6\u7a7a\u73af\u5883\u4e2d\u7684\u667a\u80fd\u4f53\u884c\u4e3a\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u5728\u65f6\u7a7a\u4e0a\u52a8\u6001\u53d8\u5316\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u8fd9\u79cd\u590d\u6742\u6027\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u3002", "method": "\u901a\u8fc7STRIDE\u6570\u636e\u96c6\u5c06360\u5ea6\u5168\u666f\u56fe\u50cf\u8f6c\u5316\u4e3a\u89c2\u6d4b\u3001\u72b6\u6001\u548c\u52a8\u4f5c\u8282\u70b9\uff0c\u5e76\u5229\u7528TARDIS\u6a21\u578b\uff08\u57fa\u4e8eTransformer\uff09\u7edf\u4e00\u5efa\u6a21\u65f6\u7a7a\u52a8\u6001\u3002", "result": "\u5728\u53ef\u63a7\u56fe\u50cf\u5408\u6210\u3001\u6307\u4ee4\u8ddf\u968f\u3001\u81ea\u4e3b\u63a7\u5236\u548c\u5730\u7406\u5b9a\u4f4d\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u80fd\u591f\u7406\u89e3\u548c\u64cd\u7eb5\u65f6\u7a7a\u73af\u5883\u7684\u901a\u7528\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.11371", "pdf": "https://arxiv.org/pdf/2506.11371", "abs": "https://arxiv.org/abs/2506.11371", "authors": ["Yihan Wu", "Xuehao Cui", "Ruibo Chen", "Georgios Milis", "Heng Huang"], "title": "A Watermark for Auto-Regressive Image Generation Models", "categories": ["cs.CV"], "comment": "Technical report", "summary": "The rapid evolution of image generation models has revolutionized visual content creation, enabling the synthesis of highly realistic and contextually accurate images for diverse applications. However, the potential for misuse, such as deepfake generation, image based phishing attacks, and fabrication of misleading visual evidence, underscores the need for robust authenticity verification mechanisms. While traditional statistical watermarking techniques have proven effective for autoregressive language models, their direct adaptation to image generation models encounters significant challenges due to a phenomenon we term retokenization mismatch, a disparity between original and retokenized sequences during the image generation process. To overcome this limitation, we propose C-reweight, a novel, distortion-free watermarking method explicitly designed for image generation models. By leveraging a clustering-based strategy that treats tokens within the same cluster equivalently, C-reweight mitigates retokenization mismatch while preserving image fidelity. Extensive evaluations on leading image generation platforms reveal that C-reweight not only maintains the visual quality of generated images but also improves detectability over existing distortion-free watermarking techniques, setting a new standard for secure and trustworthy image synthesis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aC-reweight\u7684\u65e0\u5931\u771f\u6c34\u5370\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u7684\u91cd\u6807\u8bb0\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u5e76\u63d0\u9ad8\u53ef\u68c0\u6d4b\u6027\u3002", "motivation": "\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u4e86\u6f5c\u5728\u7684\u6ee5\u7528\u98ce\u9669\uff08\u5982\u6df1\u5ea6\u4f2a\u9020\u3001\u9493\u9c7c\u653b\u51fb\u7b49\uff09\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u7684\u771f\u5b9e\u6027\u9a8c\u8bc1\u673a\u5236\u3002\u4f20\u7edf\u6c34\u5370\u6280\u672f\u56e0\u91cd\u6807\u8bb0\u4e0d\u5339\u914d\u95ee\u9898\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u56fe\u50cf\u751f\u6210\u6a21\u578b\u3002", "method": "\u63d0\u51faC-reweight\u65b9\u6cd5\uff0c\u91c7\u7528\u57fa\u4e8e\u805a\u7c7b\u7684\u7b56\u7565\uff0c\u5c06\u540c\u4e00\u805a\u7c7b\u5185\u7684\u6807\u8bb0\u89c6\u4e3a\u7b49\u6548\uff0c\u4ee5\u89e3\u51b3\u91cd\u6807\u8bb0\u4e0d\u5339\u914d\u95ee\u9898\u3002", "result": "\u5728\u4e3b\u6d41\u56fe\u50cf\u751f\u6210\u5e73\u53f0\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cC-reweight\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65e0\u5931\u771f\u6c34\u5370\u6280\u672f\u3002", "conclusion": "C-reweight\u4e3a\u5b89\u5168\u53ef\u9760\u7684\u56fe\u50cf\u5408\u6210\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u89e3\u51b3\u4e86\u91cd\u6807\u8bb0\u4e0d\u5339\u914d\u95ee\u9898\u5e76\u63d0\u9ad8\u4e86\u6c34\u5370\u7684\u53ef\u68c0\u6d4b\u6027\u3002"}}
{"id": "2506.11434", "pdf": "https://arxiv.org/pdf/2506.11434", "abs": "https://arxiv.org/abs/2506.11434", "authors": ["Jie Zhu", "Leye Wang"], "title": "Auditing Data Provenance in Real-world Text-to-Image Diffusion Models for Privacy and Copyright Protection", "categories": ["cs.CV"], "comment": "Under Review; A user-level accuracy of 90% in a real-world auditing   scenario", "summary": "Text-to-image diffusion model since its propose has significantly influenced the content creation due to its impressive generation capability. However, this capability depends on large-scale text-image datasets gathered from web platforms like social media, posing substantial challenges in copyright compliance and personal privacy leakage. Though there are some efforts devoted to explore approaches for auditing data provenance in text-to-image diffusion models, existing work has unrealistic assumptions that can obtain model internal knowledge, e.g., intermediate results, or the evaluation is not reliable. To fill this gap, we propose a completely black-box auditing framework called Feature Semantic Consistency-based Auditing (FSCA). It utilizes two types of semantic connections within the text-to-image diffusion model for auditing, eliminating the need for access to internal knowledge. To demonstrate the effectiveness of our FSCA framework, we perform extensive experiments on LAION-mi dataset and COCO dataset, and compare with eight state-of-the-art baseline approaches. The results show that FSCA surpasses previous baseline approaches across various metrics and different data distributions, showcasing the superiority of our FSCA. Moreover, we introduce a recall balance strategy and a threshold adjustment strategy, which collectively allows FSCA to reach up a user-level accuracy of 90% in a real-world auditing scenario with only 10 samples/user, highlighting its strong auditing potential in real-world applications. Our code is made available at https://github.com/JiePKU/FSCA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFSCA\u7684\u9ed1\u76d2\u5ba1\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6570\u636e\u6765\u6e90\u5ba1\u8ba1\u95ee\u9898\uff0c\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4f9d\u8d56\u4e8e\u5927\u89c4\u6a21\u6587\u672c-\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u4f46\u6570\u636e\u6765\u6e90\u53ef\u80fd\u6d89\u53ca\u7248\u6743\u548c\u9690\u79c1\u95ee\u9898\u3002\u73b0\u6709\u5ba1\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u6216\u8bc4\u4f30\u4e0d\u53ef\u9760\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "FSCA\u5229\u7528\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u7684\u4e24\u79cd\u8bed\u4e49\u8fde\u63a5\u8fdb\u884c\u5ba1\u8ba1\uff0c\u65e0\u9700\u5185\u90e8\u77e5\u8bc6\u3002\u5b9e\u9a8c\u5728LAION-mi\u548cCOCO\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\uff0c\u5e76\u4e0e\u516b\u79cd\u57fa\u7ebf\u65b9\u6cd5\u5bf9\u6bd4\u3002", "result": "FSCA\u5728\u591a\u79cd\u6307\u6807\u548c\u6570\u636e\u5206\u5e03\u4e0b\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7528\u6237\u7ea7\u51c6\u786e\u7387\u8fbe\u523090%\uff08\u4ec5\u970010\u4e2a\u6837\u672c/\u7528\u6237\uff09\u3002", "conclusion": "FSCA\u5c55\u793a\u4e86\u5728\u73b0\u5b9e\u5ba1\u8ba1\u573a\u666f\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.11621", "pdf": "https://arxiv.org/pdf/2506.11621", "abs": "https://arxiv.org/abs/2506.11621", "authors": ["Xu Wang", "Shengeng Tang", "Lechao Cheng", "Feng Li", "Shuo Wang", "Richang Hong"], "title": "SignAligner: Harmonizing Complementary Pose Modalities for Coherent Sign Language Generation", "categories": ["cs.CV"], "comment": null, "summary": "Sign language generation aims to produce diverse sign representations based on spoken language. However, achieving realistic and naturalistic generation remains a significant challenge due to the complexity of sign language, which encompasses intricate hand gestures, facial expressions, and body movements. In this work, we introduce PHOENIX14T+, an extended version of the widely-used RWTH-PHOENIX-Weather 2014T dataset, featuring three new sign representations: Pose, Hamer and Smplerx. We also propose a novel method, SignAligner, for realistic sign language generation, consisting of three stages: text-driven pose modalities co-generation, online collaborative correction of multimodality, and realistic sign video synthesis. First, by incorporating text semantics, we design a joint sign language generator to simultaneously produce posture coordinates, gesture actions, and body movements. The text encoder, based on a Transformer architecture, extracts semantic features, while a cross-modal attention mechanism integrates these features to generate diverse sign language representations, ensuring accurate mapping and controlling the diversity of modal features. Next, online collaborative correction is introduced to refine the generated pose modalities using a dynamic loss weighting strategy and cross-modal attention, facilitating the complementarity of information across modalities, eliminating spatiotemporal conflicts, and ensuring semantic coherence and action consistency. Finally, the corrected pose modalities are fed into a pre-trained video generation network to produce high-fidelity sign language videos. Extensive experiments demonstrate that SignAligner significantly improves both the accuracy and expressiveness of the generated sign videos.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSignAligner\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u903c\u771f\u7684\u624b\u8bed\u89c6\u9891\uff0c\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\u5b9e\u73b0\uff1a\u6587\u672c\u9a71\u52a8\u7684\u59ff\u52bf\u6a21\u6001\u5171\u751f\u6210\u3001\u591a\u6a21\u6001\u5728\u7ebf\u534f\u4f5c\u6821\u6b63\u548c\u903c\u771f\u624b\u8bed\u89c6\u9891\u5408\u6210\u3002", "motivation": "\u7531\u4e8e\u624b\u8bed\u7684\u590d\u6742\u6027\uff08\u5305\u62ec\u624b\u52bf\u3001\u9762\u90e8\u8868\u60c5\u548c\u8eab\u4f53\u52a8\u4f5c\uff09\uff0c\u5b9e\u73b0\u903c\u771f\u548c\u81ea\u7136\u7684\u624b\u8bed\u751f\u6210\u4ecd\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "SignAligner\u65b9\u6cd5\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a1\uff09\u57fa\u4e8e\u6587\u672c\u8bed\u4e49\u7684\u8054\u5408\u624b\u8bed\u751f\u6210\u5668\uff1b2\uff09\u5728\u7ebf\u534f\u4f5c\u6821\u6b63\u591a\u6a21\u6001\uff1b3\uff09\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u7f51\u7edc\u5408\u6210\u9ad8\u8d28\u91cf\u624b\u8bed\u89c6\u9891\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSignAligner\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u624b\u8bed\u89c6\u9891\u7684\u51c6\u786e\u6027\u548c\u8868\u73b0\u529b\u3002", "conclusion": "SignAligner\u901a\u8fc7\u591a\u6a21\u6001\u534f\u4f5c\u548c\u52a8\u6001\u6821\u6b63\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u903c\u771f\u624b\u8bed\u89c6\u9891\u7684\u751f\u6210\u3002"}}
{"id": "2506.11672", "pdf": "https://arxiv.org/pdf/2506.11672", "abs": "https://arxiv.org/abs/2506.11672", "authors": ["Chendi Ge", "Xin Wang", "Zeyang Zhang", "Hong Chen", "Jiapei Fan", "Longtao Huang", "Hui Xue", "Wenwu Zhu"], "title": "Dynamic Mixture of Curriculum LoRA Experts for Continual Multimodal Instruction Tuning", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Continual multimodal instruction tuning is crucial for adapting Multimodal Large Language Models (MLLMs) to evolving tasks. However, most existing methods adopt a fixed architecture, struggling with adapting to new tasks due to static model capacity. We propose to evolve the architecture under parameter budgets for dynamic task adaptation, which remains unexplored and imposes two challenges: 1) task architecture conflict, where different tasks require varying layer-wise adaptations, and 2) modality imbalance, where different tasks rely unevenly on modalities, leading to unbalanced updates. To address these challenges, we propose a novel Dynamic Mixture of Curriculum LoRA Experts (D-MoLE) method, which automatically evolves MLLM's architecture with controlled parameter budgets to continually adapt to new tasks while retaining previously learned knowledge. Specifically, we propose a dynamic layer-wise expert allocator, which automatically allocates LoRA experts across layers to resolve architecture conflicts, and routes instructions layer-wisely to facilitate knowledge sharing among experts. Then, we propose a gradient-based inter-modal continual curriculum, which adjusts the update ratio of each module in MLLM based on the difficulty of each modality within the task to alleviate the modality imbalance problem. Extensive experiments show that D-MoLE significantly outperforms state-of-the-art baselines, achieving a 15% average improvement over the best baseline. To the best of our knowledge, this is the first study of continual learning for MLLMs from an architectural perspective.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6df7\u5408\u8bfe\u7a0bLoRA\u4e13\u5bb6\uff08D-MoLE\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u67b6\u6784\uff0c\u89e3\u51b3\u4efb\u52a1\u67b6\u6784\u51b2\u7a81\u548c\u6a21\u6001\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6301\u7eed\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u91c7\u7528\u56fa\u5b9a\u67b6\u6784\uff0c\u96be\u4ee5\u9002\u5e94\u65b0\u4efb\u52a1\u4e14\u5b58\u5728\u6a21\u6001\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u67b6\u6784\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faD-MoLE\u65b9\u6cd5\uff0c\u5305\u62ec\u52a8\u6001\u5206\u5c42\u4e13\u5bb6\u5206\u914d\u5668\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u8de8\u6a21\u6001\u6301\u7eed\u8bfe\u7a0b\uff0c\u4ee5\u4f18\u5316\u67b6\u6784\u548c\u6a21\u6001\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660eD-MoLE\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e73\u5747\u63d0\u534715%\u3002", "conclusion": "D-MoLE\u9996\u6b21\u4ece\u67b6\u6784\u89d2\u5ea6\u7814\u7a76\u4e86MLLM\u7684\u6301\u7eed\u5b66\u4e60\uff0c\u4e3a\u89e3\u51b3\u4efb\u52a1\u51b2\u7a81\u548c\u6a21\u6001\u4e0d\u5e73\u8861\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2506.11764", "pdf": "https://arxiv.org/pdf/2506.11764", "abs": "https://arxiv.org/abs/2506.11764", "authors": ["Muhammad Sarmad", "Arnt-B\u00f8rre Salberg", "Michael Kampffmeyer"], "title": "DiffFuSR: Super-Resolution of all Sentinel-2 Multispectral Bands using Diffusion Models", "categories": ["cs.CV", "eess.IV"], "comment": "preprint under review", "summary": "This paper presents DiffFuSR, a modular pipeline for super-resolving all 12 spectral bands of Sentinel-2 Level-2A imagery to a unified ground sampling distance (GSD) of 2.5 meters. The pipeline comprises two stages: (i) a diffusion-based super-resolution (SR) model trained on high-resolution RGB imagery from the NAIP and WorldStrat datasets, harmonized to simulate Sentinel-2 characteristics; and (ii) a learned fusion network that upscales the remaining multispectral bands using the super-resolved RGB image as a spatial prior. We introduce a robust degradation model and contrastive degradation encoder to support blind SR. Extensive evaluations of the proposed SR pipeline on the OpenSR benchmark demonstrate that the proposed method outperforms current SOTA baselines in terms of reflectance fidelity, spectral consistency, spatial alignment, and hallucination suppression. Furthermore, the fusion network significantly outperforms classical pansharpening approaches, enabling accurate enhancement of Sentinel-2's 20 m and 60 m bands. This study underscores the power of harmonized learning with generative priors and fusion strategies to create a modular framework for Sentinel-2 SR. Our code and models can be found at https://github.com/NorskRegnesentral/DiffFuSR.", "AI": {"tldr": "DiffFuSR\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7ba1\u9053\uff0c\u7528\u4e8e\u5c06Sentinel-2 Level-2A\u5f71\u50cf\u768412\u4e2a\u5149\u8c31\u6ce2\u6bb5\u8d85\u5206\u8fa8\u7387\u5230\u7edf\u4e00\u76842.5\u7c73\u5730\u9762\u91c7\u6837\u8ddd\u79bb\uff08GSD\uff09\u3002", "motivation": "\u89e3\u51b3Sentinel-2\u5f71\u50cf\u591a\u5149\u8c31\u6ce2\u6bb5\u5206\u8fa8\u7387\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5f71\u50cf\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684RGB\u8d85\u5206\u8fa8\u7387\uff1b2\uff09\u5229\u7528\u8d85\u5206\u8fa8\u7387RGB\u56fe\u50cf\u4f5c\u4e3a\u7a7a\u95f4\u5148\u9a8c\uff0c\u901a\u8fc7\u878d\u5408\u7f51\u7edc\u4e0a\u91c7\u6837\u5176\u4ed6\u591a\u5149\u8c31\u6ce2\u6bb5\u3002", "result": "\u5728OpenSR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u53cd\u5c04\u7387\u4fdd\u771f\u5ea6\u3001\u5149\u8c31\u4e00\u81f4\u6027\u3001\u7a7a\u95f4\u5bf9\u9f50\u548c\u5e7b\u89c9\u6291\u5236\u65b9\u9762\u5747\u6709\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u751f\u6210\u5148\u9a8c\u548c\u878d\u5408\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u6a21\u5757\u5316\u7684Sentinel-2\u8d85\u5206\u8fa8\u7387\u6846\u67b6\u3002"}}
{"id": "2506.11768", "pdf": "https://arxiv.org/pdf/2506.11768", "abs": "https://arxiv.org/abs/2506.11768", "authors": ["Linfeng He", "Meiqin Liu", "Qi Tang", "Chao Yao", "Yao Zhao"], "title": "MambaVSR: Content-Aware Scanning State Space Model for Video Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Video super-resolution (VSR) faces critical challenges in effectively modeling non-local dependencies across misaligned frames while preserving computational efficiency. Existing VSR methods typically rely on optical flow strategies or transformer architectures, which struggle with large motion displacements and long video sequences. To address this, we propose MambaVSR, the first state-space model framework for VSR that incorporates an innovative content-aware scanning mechanism. Unlike rigid 1D sequential processing in conventional vision Mamba methods, our MambaVSR enables dynamic spatiotemporal interactions through the Shared Compass Construction (SCC) and the Content-Aware Sequentialization (CAS). Specifically, the SCC module constructs intra-frame semantic connectivity graphs via efficient sparse attention and generates adaptive spatial scanning sequences through spectral clustering. Building upon SCC, the CAS module effectively aligns and aggregates non-local similar content across multiple frames by interleaving temporal features along the learned spatial order. To bridge global dependencies with local details, the Global-Local State Space Block (GLSSB) synergistically integrates window self-attention operations with SSM-based feature propagation, enabling high-frequency detail recovery under global dependency guidance. Extensive experiments validate MambaVSR's superiority, outperforming the Transformer-based method by 0.58 dB PSNR on the REDS dataset with 55% fewer parameters.", "AI": {"tldr": "MambaVSR\u662f\u4e00\u79cd\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u65f6\u7a7a\u4ea4\u4e92\u548c\u5185\u5bb9\u611f\u77e5\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u5927\u8fd0\u52a8\u4f4d\u79fb\u548c\u957f\u89c6\u9891\u5e8f\u5217\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5728\u5904\u7406\u975e\u5c40\u90e8\u4f9d\u8d56\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u5927\u8fd0\u52a8\u4f4d\u79fb\u548c\u957f\u89c6\u9891\u5e8f\u5217\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faMambaVSR\u6846\u67b6\uff0c\u5305\u542b\u5171\u4eab\u7f57\u76d8\u6784\u5efa\uff08SCC\uff09\u548c\u5185\u5bb9\u611f\u77e5\u5e8f\u5217\u5316\uff08CAS\uff09\u6a21\u5757\uff0c\u4ee5\u53ca\u5168\u5c40-\u5c40\u90e8\u72b6\u6001\u7a7a\u95f4\u5757\uff08GLSSB\uff09\uff0c\u5b9e\u73b0\u52a8\u6001\u65f6\u7a7a\u4ea4\u4e92\u548c\u9ad8\u6548\u7279\u5f81\u4f20\u64ad\u3002", "result": "\u5728REDS\u6570\u636e\u96c6\u4e0a\uff0cMambaVSR\u6bd4\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5PSNR\u63d0\u9ad8\u4e860.58 dB\uff0c\u4e14\u53c2\u6570\u51cf\u5c11\u4e8655%\u3002", "conclusion": "MambaVSR\u901a\u8fc7\u521b\u65b0\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u548c\u5185\u5bb9\u611f\u77e5\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2506.11924", "pdf": "https://arxiv.org/pdf/2506.11924", "abs": "https://arxiv.org/abs/2506.11924", "authors": ["Min-Seop Kwak", "Junho Kim", "Sangdoo Yun", "Dongyoon Han", "Taekyoung Kim", "Seungryong Kim", "Jin-Hwa Kim"], "title": "Aligned Novel View Image and Geometry Synthesis via Cross-modal Attention Instillation", "categories": ["cs.CV"], "comment": null, "summary": "We introduce a diffusion-based framework that performs aligned novel view image and geometry generation via a warping-and-inpainting methodology. Unlike prior methods that require dense posed images or pose-embedded generative models limited to in-domain views, our method leverages off-the-shelf geometry predictors to predict partial geometries viewed from reference images, and formulates novel-view synthesis as an inpainting task for both image and geometry. To ensure accurate alignment between generated images and geometry, we propose cross-modal attention distillation, where attention maps from the image diffusion branch are injected into a parallel geometry diffusion branch during both training and inference. This multi-task approach achieves synergistic effects, facilitating geometrically robust image synthesis as well as well-defined geometry prediction. We further introduce proximity-based mesh conditioning to integrate depth and normal cues, interpolating between point cloud and filtering erroneously predicted geometry from influencing the generation process. Empirically, our method achieves high-fidelity extrapolative view synthesis on both image and geometry across a range of unseen scenes, delivers competitive reconstruction quality under interpolation settings, and produces geometrically aligned colored point clouds for comprehensive 3D completion. Project page is available at https://cvlab-kaist.github.io/MoAI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u626d\u66f2\u548c\u4fee\u590d\u65b9\u6cd5\u5b9e\u73b0\u5bf9\u9f50\u7684\u65b0\u89c6\u89d2\u56fe\u50cf\u548c\u51e0\u4f55\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5bc6\u96c6\u7684\u59ff\u52bf\u56fe\u50cf\u6216\u4ec5\u9650\u4e8e\u57df\u5185\u89c6\u89d2\u7684\u59ff\u52bf\u5d4c\u5165\u751f\u6210\u6a21\u578b\uff0c\u800c\u672c\u6587\u65b9\u6cd5\u5229\u7528\u73b0\u6210\u7684\u51e0\u4f55\u9884\u6d4b\u5668\u9884\u6d4b\u90e8\u5206\u51e0\u4f55\uff0c\u5e76\u5c06\u65b0\u89c6\u89d2\u5408\u6210\u89c6\u4e3a\u56fe\u50cf\u548c\u51e0\u4f55\u7684\u4fee\u590d\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u84b8\u998f\uff0c\u5c06\u56fe\u50cf\u6269\u6563\u5206\u652f\u7684\u6ce8\u610f\u529b\u56fe\u6ce8\u5165\u5e76\u884c\u51e0\u4f55\u6269\u6563\u5206\u652f\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u90bb\u8fd1\u7684\u7f51\u683c\u6761\u4ef6\u4ee5\u6574\u5408\u6df1\u5ea6\u548c\u6cd5\u7ebf\u7ebf\u7d22\u3002", "result": "\u65b9\u6cd5\u5728\u672a\u89c1\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5916\u63a8\u89c6\u89d2\u5408\u6210\uff0c\u5728\u63d2\u503c\u8bbe\u7f6e\u4e0b\u63d0\u4f9b\u6709\u7ade\u4e89\u529b\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u5e76\u751f\u6210\u51e0\u4f55\u5bf9\u9f50\u7684\u5f69\u8272\u70b9\u4e91\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u4efb\u52a1\u534f\u540c\u4f5c\u7528\u4e0b\uff0c\u5b9e\u73b0\u4e86\u51e0\u4f55\u7a33\u5065\u7684\u56fe\u50cf\u5408\u6210\u548c\u660e\u786e\u7684\u51e0\u4f55\u9884\u6d4b\u3002"}}
{"id": "2506.11139", "pdf": "https://arxiv.org/pdf/2506.11139", "abs": "https://arxiv.org/abs/2506.11139", "authors": ["Namhoon Kim", "Sara Fridovich-Keil"], "title": "Grids Often Outperform Implicit Neural Representations", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Implicit Neural Representations (INRs) have recently shown impressive results, but their fundamental capacity, implicit biases, and scaling behavior remain poorly understood. We investigate the performance of diverse INRs across a suite of 2D and 3D real and synthetic signals with varying effective bandwidth, as well as both overfitting and generalization tasks including tomography, super-resolution, and denoising. By stratifying performance according to model size as well as signal type and bandwidth, our results shed light on how different INR and grid representations allocate their capacity. We find that, for most tasks and signals, a simple regularized grid with interpolation trains faster and to higher quality than any INR with the same number of parameters. We also find limited settings where INRs outperform grids -- namely fitting signals with underlying lower-dimensional structure such as shape contours -- to guide future use of INRs towards the most advantageous applications. Code and synthetic signals used in our analysis are available at https://github.com/voilalab/INR-benchmark.", "AI": {"tldr": "\u7814\u7a76\u4e86\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INRs\uff09\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u7b80\u5355\u6b63\u5219\u5316\u7f51\u683c\u5728\u591a\u6570\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eINRs\uff0c\u4ec5\u5728\u67d0\u4e9b\u4f4e\u7ef4\u7ed3\u6784\u4fe1\u53f7\u4e2dINRs\u66f4\u4f18\u3002", "motivation": "\u7406\u89e3INRs\u7684\u57fa\u672c\u80fd\u529b\u3001\u9690\u5f0f\u504f\u5dee\u548c\u6269\u5c55\u884c\u4e3a\uff0c\u586b\u8865\u5f53\u524d\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u591a\u6837\u5316\u7684INRs\u57282D\u548c3D\u4fe1\u53f7\uff08\u5305\u62ec\u771f\u5b9e\u548c\u5408\u6210\u4fe1\u53f7\uff09\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5206\u6790\u6a21\u578b\u5927\u5c0f\u3001\u4fe1\u53f7\u7c7b\u578b\u548c\u5e26\u5bbd\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u591a\u6570\u4efb\u52a1\u4e2d\uff0c\u6b63\u5219\u5316\u7f51\u683c\u6bd4INRs\u66f4\u5feb\u4e14\u8d28\u91cf\u66f4\u9ad8\uff1bINRs\u4ec5\u5728\u4f4e\u7ef4\u7ed3\u6784\u4fe1\u53f7\uff08\u5982\u5f62\u72b6\u8f6e\u5ed3\uff09\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "INRs\u5728\u7279\u5b9a\u4f4e\u7ef4\u7ed3\u6784\u4fe1\u53f7\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u672a\u6765\u5e94\u9488\u5bf9\u8fd9\u4e9b\u5e94\u7528\u573a\u666f\u4f18\u5316\u4f7f\u7528\u3002"}}
{"id": "2506.11183", "pdf": "https://arxiv.org/pdf/2506.11183", "abs": "https://arxiv.org/abs/2506.11183", "authors": ["Yi Zhang"], "title": "DiffPR: Diffusion-Based Phase Reconstruction via Frequency-Decoupled Learning", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Oversmoothing remains a persistent problem when applying deep learning to off-axis quantitative phase imaging (QPI). End-to-end U-Nets favour low-frequency content and under-represent fine, diagnostic detail. We trace this issue to spectral bias and show that the bias is reinforced by high-level skip connections that feed high-frequency features directly into the decoder. Removing those deepest skips thus supervising the network only at a low resolution significantly improves generalisation and fidelity. Building on this insight, we introduce DiffPR, a two-stage frequency-decoupled framework. Stage 1: an asymmetric U-Net with cancelled high-frequency skips predicts a quarter-scale phase map from the interferogram, capturing reliable low-frequency structure while avoiding spectral bias. Stage 2: the upsampled prediction, lightly perturbed with Gaussian noise, is refined by an unconditional diffusion model that iteratively recovers the missing high-frequency residuals through reverse denoising. Experiments on four QPI datasets (B-Cell, WBC, HeLa, 3T3) show that DiffPR outperforms strong U-Net baselines, boosting PSNR by up to 1.1 dB and reducing MAE by 11 percent, while delivering markedly sharper membrane ridges and speckle patterns. The results demonstrate that cancelling high-level skips and delegating detail synthesis to a diffusion prior is an effective remedy for the spectral bias that limits conventional phase-retrieval networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDiffPR\u6846\u67b6\uff0c\u901a\u8fc7\u53d6\u6d88\u9ad8\u9891\u8df3\u8dc3\u8fde\u63a5\u548c\u5f15\u5165\u6269\u6563\u6a21\u578b\uff0c\u89e3\u51b3\u4e86QPI\u4e2d\u7684\u8fc7\u5e73\u6ed1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4e2dU-Net\u7684\u9891\u8c31\u504f\u5dee\u5bfc\u81f4QPI\u56fe\u50cf\u7ec6\u8282\u4e22\u5931\uff0c\u8fc7\u5e73\u6ed1\u95ee\u9898\u4e25\u91cd\uff0c\u5f71\u54cd\u8bca\u65ad\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5DiffPR\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u7528\u53d6\u6d88\u9ad8\u9891\u8df3\u8dc3\u7684\u975e\u5bf9\u79f0U-Net\u9884\u6d4b\u4f4e\u5206\u8fa8\u7387\u76f8\u4f4d\u56fe\uff1b\u7b2c\u4e8c\u9636\u6bb5\u7528\u6269\u6563\u6a21\u578b\u6062\u590d\u9ad8\u9891\u7ec6\u8282\u3002", "result": "\u5728\u56db\u4e2aQPI\u6570\u636e\u96c6\u4e0a\uff0cDiffPR\u6bd4U-Net\u57fa\u7ebfPSNR\u63d0\u53471.1 dB\uff0cMAE\u964d\u4f4e11%\uff0c\u56fe\u50cf\u66f4\u6e05\u6670\u3002", "conclusion": "\u53d6\u6d88\u9ad8\u9891\u8df3\u8dc3\u8fde\u63a5\u5e76\u7528\u6269\u6563\u6a21\u578b\u8865\u5145\u7ec6\u8282\uff0c\u662f\u89e3\u51b3\u9891\u8c31\u504f\u5dee\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2506.11283", "pdf": "https://arxiv.org/pdf/2506.11283", "abs": "https://arxiv.org/abs/2506.11283", "authors": ["Joakim And\u00e9n", "Justus Sagem\u00fcller"], "title": "Joint Denoising of Cryo-EM Projection Images using Polar Transformers", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Deep neural networks~(DNNs) have proven powerful for denoising, but they are ultimately of limited use in high-noise settings, such as for cryogenic electron microscopy~(cryo-EM) projection images. In this setting, however, datasets contain a large number of projections of the same molecule, each taken from a different viewing direction. This redundancy of information is useful in traditional denoising techniques known as class averaging methods, where images are clustered, aligned, and then averaged to reduce the noise level. We present a neural network architecture based on transformers that extends these class averaging methods by simultaneously clustering, aligning, and denoising cryo-EM images. Results on synthetic data show accurate denoising performance using this architecture, reducing the relative mean squared error (MSE) single-image DNNs by $45\\%$ at a signal-to-noise (SNR) of $0.03$.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u540c\u65f6\u805a\u7c7b\u3001\u5bf9\u9f50\u548c\u53bb\u566a\u51b7\u51bb\u7535\u955c\u56fe\u50cf\uff0c\u663e\u8457\u964d\u4f4e\u566a\u58f0\u3002", "motivation": "\u4f20\u7edfDNN\u5728\u9ad8\u566a\u58f0\u73af\u5883\u4e0b\uff08\u5982\u51b7\u51bb\u7535\u955c\u56fe\u50cf\uff09\u6548\u679c\u6709\u9650\uff0c\u800c\u6570\u636e\u5197\u4f59\u6027\u53ef\u7528\u4e8e\u6539\u8fdb\u53bb\u566a\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff0c\u540c\u65f6\u5b9e\u73b0\u56fe\u50cf\u805a\u7c7b\u3001\u5bf9\u9f50\u548c\u53bb\u566a\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u4e0a\uff0c\u76f8\u5bf9MSE\u964d\u4f4e45%\uff08SNR=0.03\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u9ad8\u566a\u58f0\u73af\u5883\u4e0b\u4f18\u4e8e\u5355\u56fe\u50cfDNN\uff0c\u63d0\u5347\u4e86\u51b7\u51bb\u7535\u955c\u56fe\u50cf\u7684\u53bb\u566a\u6548\u679c\u3002"}}
{"id": "2506.11444", "pdf": "https://arxiv.org/pdf/2506.11444", "abs": "https://arxiv.org/abs/2506.11444", "authors": ["Kecen Li", "Zhicong Huang", "Xinwen Hou", "Cheng Hong"], "title": "GaussMarker: Robust Dual-Domain Watermark for Diffusion Models", "categories": ["cs.CR", "cs.CV"], "comment": "Accepted at ICML 2025", "summary": "As Diffusion Models (DM) generate increasingly realistic images, related issues such as copyright and misuse have become a growing concern. Watermarking is one of the promising solutions. Existing methods inject the watermark into the single-domain of initial Gaussian noise for generation, which suffers from unsatisfactory robustness. This paper presents the first dual-domain DM watermarking approach using a pipelined injector to consistently embed watermarks in both the spatial and frequency domains. To further boost robustness against certain image manipulations and advanced attacks, we introduce a model-independent learnable Gaussian Noise Restorer (GNR) to refine Gaussian noise extracted from manipulated images and enhance detection robustness by integrating the detection scores of both watermarks. GaussMarker efficiently achieves state-of-the-art performance under eight image distortions and four advanced attacks across three versions of Stable Diffusion with better recall and lower false positive rates, as preferred in real applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u57df\u6269\u6563\u6a21\u578b\u6c34\u5370\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a7a\u95f4\u548c\u9891\u7387\u57df\u5d4c\u5165\u6c34\u5370\uff0c\u7ed3\u5408\u9ad8\u65af\u566a\u58f0\u6062\u590d\u5668\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u5728\u591a\u79cd\u653b\u51fb\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u968f\u7740\u6269\u6563\u6a21\u578b\u751f\u6210\u56fe\u50cf\u8d8a\u6765\u8d8a\u903c\u771f\uff0c\u7248\u6743\u548c\u6ee5\u7528\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u73b0\u6709\u5355\u57df\u6c34\u5370\u65b9\u6cd5\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u53cc\u57df\u6c34\u5370\u5d4c\u5165\u65b9\u6cd5\uff08\u7a7a\u95f4\u548c\u9891\u7387\u57df\uff09\uff0c\u5e76\u5f15\u5165\u6a21\u578b\u65e0\u5173\u7684\u9ad8\u65af\u566a\u58f0\u6062\u590d\u5668\uff08GNR\uff09\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5728\u591a\u79cd\u56fe\u50cf\u5931\u771f\u548c\u9ad8\u7ea7\u653b\u51fb\u4e0b\uff0cGaussMarker\u5728\u4e09\u4e2aStable Diffusion\u7248\u672c\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u53ec\u56de\u7387\u9ad8\u4e14\u8bef\u62a5\u7387\u4f4e\u3002", "conclusion": "GaussMarker\u901a\u8fc7\u53cc\u57df\u6c34\u5370\u548cGNR\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u6c34\u5370\u5d4c\u5165\u4e0e\u68c0\u6d4b\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2506.11455", "pdf": "https://arxiv.org/pdf/2506.11455", "abs": "https://arxiv.org/abs/2506.11455", "authors": ["Yifei Sun", "Daniel Chahine", "Qinghao Wen", "Tianming Liu", "Xiang Li", "Yixuan Yuan", "Fernando Calamante", "Jinglei Lv"], "title": "Voxel-Level Brain States Prediction Using Swin Transformer", "categories": ["q-bio.NC", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Understanding brain dynamics is important for neuroscience and mental health. Functional magnetic resonance imaging (fMRI) enables the measurement of neural activities through blood-oxygen-level-dependent (BOLD) signals, which represent brain states. In this study, we aim to predict future human resting brain states with fMRI. Due to the 3D voxel-wise spatial organization and temporal dependencies of the fMRI data, we propose a novel architecture which employs a 4D Shifted Window (Swin) Transformer as encoder to efficiently learn spatio-temporal information and a convolutional decoder to enable brain state prediction at the same spatial and temporal resolution as the input fMRI data. We used 100 unrelated subjects from the Human Connectome Project (HCP) for model training and testing. Our novel model has shown high accuracy when predicting 7.2s resting-state brain activities based on the prior 23.04s fMRI time series. The predicted brain states highly resemble BOLD contrast and dynamics. This work shows promising evidence that the spatiotemporal organization of the human brain can be learned by a Swin Transformer model, at high resolution, which provides a potential for reducing the fMRI scan time and the development of brain-computer interfaces in the future.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e4D Swin Transformer\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u4eba\u7c7b\u9759\u606f\u6001\u8111\u6d3b\u52a8\uff0c\u5c55\u793a\u4e86\u9ad8\u7cbe\u5ea6\u548c\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u7406\u89e3\u8111\u52a8\u529b\u5b66\u5bf9\u795e\u7ecf\u79d1\u5b66\u548c\u5fc3\u7406\u5065\u5eb7\u81f3\u5173\u91cd\u8981\uff0cfMRI\u6570\u636e\u7684\u9ad8\u5206\u8fa8\u7387\u9884\u6d4b\u6709\u52a9\u4e8e\u51cf\u5c11\u626b\u63cf\u65f6\u95f4\u548c\u5f00\u53d1\u8111\u673a\u63a5\u53e3\u3002", "method": "\u4f7f\u75284D Swin Transformer\u7f16\u7801\u5668\u5b66\u4e60\u65f6\u7a7a\u4fe1\u606f\uff0c\u5377\u79ef\u89e3\u7801\u5668\u9884\u6d4b\u8111\u72b6\u6001\uff0c\u6570\u636e\u6765\u81eaHCP\u9879\u76ee\u7684100\u540d\u53d7\u8bd5\u8005\u3002", "result": "\u6a21\u578b\u80fd\u9ad8\u7cbe\u5ea6\u9884\u6d4b7.2\u79d2\u7684\u9759\u606f\u6001\u8111\u6d3b\u52a8\uff0c\u9884\u6d4b\u7ed3\u679c\u4e0eBOLD\u4fe1\u53f7\u9ad8\u5ea6\u76f8\u4f3c\u3002", "conclusion": "Swin Transformer\u6a21\u578b\u80fd\u9ad8\u6548\u5b66\u4e60\u4eba\u8111\u65f6\u7a7a\u7ec4\u7ec7\uff0c\u4e3a\u51cf\u5c11fMRI\u626b\u63cf\u65f6\u95f4\u548c\u8111\u673a\u63a5\u53e3\u5f00\u53d1\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2506.11496", "pdf": "https://arxiv.org/pdf/2506.11496", "abs": "https://arxiv.org/abs/2506.11496", "authors": ["Chunlei Li", "Yilei Shi", "Haoxi Hu", "Jingliang Hu", "Xiao Xiang Zhu", "Lichao Mou"], "title": "Taming Stable Diffusion for Computed Tomography Blind Super-Resolution", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "High-resolution computed tomography (CT) imaging is essential for medical diagnosis but requires increased radiation exposure, creating a critical trade-off between image quality and patient safety. While deep learning methods have shown promise in CT super-resolution, they face challenges with complex degradations and limited medical training data. Meanwhile, large-scale pre-trained diffusion models, particularly Stable Diffusion, have demonstrated remarkable capabilities in synthesizing fine details across various vision tasks. Motivated by this, we propose a novel framework that adapts Stable Diffusion for CT blind super-resolution. We employ a practical degradation model to synthesize realistic low-quality images and leverage a pre-trained vision-language model to generate corresponding descriptions. Subsequently, we perform super-resolution using Stable Diffusion with a specialized controlling strategy, conditioned on both low-resolution inputs and the generated text descriptions. Extensive experiments show that our method outperforms existing approaches, demonstrating its potential for achieving high-quality CT imaging at reduced radiation doses. Our code will be made publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eStable Diffusion\u7684CT\u76f2\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u4f4e\u8d28\u91cf\u56fe\u50cf\u548c\u751f\u6210\u6587\u672c\u63cf\u8ff0\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86CT\u56fe\u50cf\u8d28\u91cf\uff0c\u540c\u65f6\u964d\u4f4e\u8f90\u5c04\u5242\u91cf\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387CT\u6210\u50cf\u5bf9\u533b\u7597\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8f90\u5c04\u5242\u91cf\u4e0e\u56fe\u50cf\u8d28\u91cf\u4e4b\u95f4\u5b58\u5728\u77db\u76fe\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u590d\u6742\u9000\u5316\u548c\u6709\u9650\u533b\u5b66\u6570\u636e\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff08\u5982Stable Diffusion\uff09\u5728\u7ec6\u8282\u5408\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u5229\u7528\u5b9e\u9645\u9000\u5316\u6a21\u578b\u5408\u6210\u4f4e\u8d28\u91cfCT\u56fe\u50cf\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u63cf\u8ff0\uff0c\u5e76\u901a\u8fc7Stable Diffusion\u8fdb\u884c\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\uff0c\u540c\u65f6\u63a7\u5236\u8f93\u5165\u6761\u4ef6\u548c\u6587\u672c\u63cf\u8ff0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u964d\u4f4e\u8f90\u5c04\u5242\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684CT\u6210\u50cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u5206\u8fa8\u7387CT\u6210\u50cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2506.11545", "pdf": "https://arxiv.org/pdf/2506.11545", "abs": "https://arxiv.org/abs/2506.11545", "authors": ["Zhaoyang Wang", "Jie Li", "Wen Lu", "Lihuo He", "Maoguo Gong", "Xinbo Gao"], "title": "FCA2: Frame Compression-Aware Autoencoder for Modular and Fast Compressed Video Super-Resolution", "categories": ["eess.IV", "cs.CV"], "comment": "This work has been submitted to the IEEE TMM for possible publication", "summary": "State-of-the-art (SOTA) compressed video super-resolution (CVSR) models face persistent challenges, including prolonged inference time, complex training pipelines, and reliance on auxiliary information. As video frame rates continue to increase, the diminishing inter-frame differences further expose the limitations of traditional frame-to-frame information exploitation methods, which are inadequate for addressing current video super-resolution (VSR) demands. To overcome these challenges, we propose an efficient and scalable solution inspired by the structural and statistical similarities between hyperspectral images (HSI) and video data. Our approach introduces a compression-driven dimensionality reduction strategy that reduces computational complexity, accelerates inference, and enhances the extraction of temporal information across frames. The proposed modular architecture is designed for seamless integration with existing VSR frameworks, ensuring strong adaptability and transferability across diverse applications. Experimental results demonstrate that our method achieves performance on par with, or surpassing, the current SOTA models, while significantly reducing inference time. By addressing key bottlenecks in CVSR, our work offers a practical and efficient pathway for advancing VSR technology. Our code will be publicly available at https://github.com/handsomewzy/FCA2.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u5149\u8c31\u56fe\u50cf\u4e0e\u89c6\u9891\u6570\u636e\u76f8\u4f3c\u6027\u7684\u9ad8\u6548\u538b\u7f29\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u63a8\u7406\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u538b\u7f29\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u5b58\u5728\u63a8\u7406\u65f6\u95f4\u957f\u3001\u8bad\u7ec3\u6d41\u7a0b\u590d\u6742\u3001\u4f9d\u8d56\u8f85\u52a9\u4fe1\u606f\u7b49\u95ee\u9898\uff0c\u4e14\u4f20\u7edf\u5e27\u95f4\u4fe1\u606f\u5229\u7528\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u91c7\u7528\u538b\u7f29\u9a71\u52a8\u7684\u964d\u7ef4\u7b56\u7565\uff0c\u8bbe\u8ba1\u6a21\u5757\u5316\u67b6\u6784\uff0c\u63d0\u5347\u65f6\u95f4\u4fe1\u606f\u63d0\u53d6\u6548\u7387\uff0c\u5e76\u4e0e\u73b0\u6709\u6846\u67b6\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u8fbe\u5230\u6216\u8d85\u8d8a\u5f53\u524dSOTA\u6a21\u578b\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "\u901a\u8fc7\u89e3\u51b3\u5173\u952e\u74f6\u9888\uff0c\u4e3a\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6280\u672f\u63d0\u4f9b\u4e86\u9ad8\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.11753", "pdf": "https://arxiv.org/pdf/2506.11753", "abs": "https://arxiv.org/abs/2506.11753", "authors": ["Zuzanna Skorniewska", "Bartlomiej W. Papiez"], "title": "Exploring the Effectiveness of Deep Features from Domain-Specific Foundation Models in Retinal Image Synthesis", "categories": ["eess.IV", "cs.CV"], "comment": "To be published and presented at the MIUA 2025 conference", "summary": "The adoption of neural network models in medical imaging has been constrained by strict privacy regulations, limited data availability, high acquisition costs, and demographic biases. Deep generative models offer a promising solution by generating synthetic data that bypasses privacy concerns and addresses fairness by producing samples for under-represented groups. However, unlike natural images, medical imaging requires validation not only for fidelity (e.g., Fr\\'echet Inception Score) but also for morphological and clinical accuracy. This is particularly true for colour fundus retinal imaging, which requires precise replication of the retinal vascular network, including vessel topology, continuity, and thickness. In this study, we in-vestigated whether a distance-based loss function based on deep activation layers of a large foundational model trained on large corpus of domain data, colour fundus imaging, offers advantages over a perceptual loss and edge-detection based loss functions. Our extensive validation pipeline, based on both domain-free and domain specific tasks, suggests that domain-specific deep features do not improve autoen-coder image generation. Conversely, our findings highlight the effectiveness of con-ventional edge detection filters in improving the sharpness of vascular structures in synthetic samples.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u533b\u5b66\u5f71\u50cf\u751f\u6210\u4e2d\uff0c\u57fa\u4e8e\u9886\u57df\u7279\u5b9a\u6df1\u5ea6\u7279\u5f81\u7684\u8ddd\u79bb\u635f\u5931\u51fd\u6570\u4e0e\u611f\u77e5\u635f\u5931\u548c\u8fb9\u7f18\u68c0\u6d4b\u635f\u5931\u51fd\u6570\u7684\u6bd4\u8f83\uff0c\u53d1\u73b0\u4f20\u7edf\u8fb9\u7f18\u68c0\u6d4b\u6ee4\u6ce2\u5668\u5728\u63d0\u5347\u8840\u7ba1\u7ed3\u6784\u6e05\u6670\u5ea6\u4e0a\u66f4\u6709\u6548\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u751f\u6210\u9762\u4e34\u9690\u79c1\u3001\u6570\u636e\u7a00\u7f3a\u548c\u516c\u5e73\u6027\u95ee\u9898\uff0c\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9700\u9a8c\u8bc1\u5f62\u6001\u548c\u4e34\u5e8a\u51c6\u786e\u6027\u3002", "method": "\u6bd4\u8f83\u4e86\u57fa\u4e8e\u9886\u57df\u7279\u5b9a\u6df1\u5ea6\u7279\u5f81\u7684\u8ddd\u79bb\u635f\u5931\u51fd\u6570\u4e0e\u611f\u77e5\u635f\u5931\u548c\u8fb9\u7f18\u68c0\u6d4b\u635f\u5931\u51fd\u6570\u7684\u6548\u679c\u3002", "result": "\u9886\u57df\u7279\u5b9a\u6df1\u5ea6\u7279\u5f81\u672a\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u4f20\u7edf\u8fb9\u7f18\u68c0\u6d4b\u6ee4\u6ce2\u5668\u5728\u8840\u7ba1\u7ed3\u6784\u6e05\u6670\u5ea6\u4e0a\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u4f20\u7edf\u8fb9\u7f18\u68c0\u6d4b\u6ee4\u6ce2\u5668\u5728\u533b\u5b66\u5f71\u50cf\u751f\u6210\u4e2d\u66f4\u6709\u6548\uff0c\u9886\u57df\u7279\u5b9a\u6df1\u5ea6\u7279\u5f81\u672a\u5e26\u6765\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2506.11823", "pdf": "https://arxiv.org/pdf/2506.11823", "abs": "https://arxiv.org/abs/2506.11823", "authors": ["Zhangkai Ni", "Yang Zhang", "Wenhan Yang", "Hanli Wang", "Shiqi Wang", "Sam Kwong"], "title": "Structural Similarity-Inspired Unfolding for Lightweight Image Super-Resolution", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted to IEEE Transactions on Image Processing", "summary": "Major efforts in data-driven image super-resolution (SR) primarily focus on expanding the receptive field of the model to better capture contextual information. However, these methods are typically implemented by stacking deeper networks or leveraging transformer-based attention mechanisms, which consequently increases model complexity. In contrast, model-driven methods based on the unfolding paradigm show promise in improving performance while effectively maintaining model compactness through sophisticated module design. Based on these insights, we propose a Structural Similarity-Inspired Unfolding (SSIU) method for efficient image SR. This method is designed through unfolding an SR optimization function constrained by structural similarity, aiming to combine the strengths of both data-driven and model-driven approaches. Our model operates progressively following the unfolding paradigm. Each iteration consists of multiple Mixed-Scale Gating Modules (MSGM) and an Efficient Sparse Attention Module (ESAM). The former implements comprehensive constraints on features, including a structural similarity constraint, while the latter aims to achieve sparse activation. In addition, we design a Mixture-of-Experts-based Feature Selector (MoE-FS) that fully utilizes multi-level feature information by combining features from different steps. Extensive experiments validate the efficacy and efficiency of our unfolding-inspired network. Our model outperforms current state-of-the-art models, boasting lower parameter counts and reduced memory consumption. Our code will be available at: https://github.com/eezkni/SSIU", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u76f8\u4f3c\u6027\u542f\u53d1\u7684\u5c55\u5f00\u65b9\u6cd5\uff08SSIU\uff09\uff0c\u7528\u4e8e\u9ad8\u6548\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff0c\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u548c\u6a21\u578b\u9a71\u52a8\u7684\u4f18\u52bf\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u53c2\u6570\u66f4\u5c11\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9a71\u52a8\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u589e\u52a0\u6a21\u578b\u6df1\u5ea6\u6216\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u6765\u6269\u5c55\u611f\u53d7\u91ce\uff0c\u4f46\u4f1a\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u3002\u6a21\u578b\u9a71\u52a8\u65b9\u6cd5\u901a\u8fc7\u5c55\u5f00\u8303\u5f0f\u5728\u4fdd\u6301\u7d27\u51d1\u6027\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\uff0c\u56e0\u6b64\u63d0\u51fa\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5c55\u5f00\u4e00\u4e2a\u53d7\u7ed3\u6784\u76f8\u4f3c\u6027\u7ea6\u675f\u7684\u8d85\u5206\u8fa8\u7387\u4f18\u5316\u51fd\u6570\u8bbe\u8ba1SSIU\u65b9\u6cd5\uff0c\u5305\u542b\u6df7\u5408\u5c3a\u5ea6\u95e8\u63a7\u6a21\u5757\uff08MSGM\uff09\u548c\u9ad8\u6548\u7a00\u758f\u6ce8\u610f\u529b\u6a21\u5757\uff08ESAM\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u4e13\u5bb6\u6df7\u5408\u7684\u7279\u5f81\u9009\u62e9\u5668\uff08MoE-FS\uff09\u4ee5\u5229\u7528\u591a\u7ea7\u7279\u5f81\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSSIU\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u540c\u65f6\u53c2\u6570\u66f4\u5c11\u3001\u5185\u5b58\u6d88\u8017\u66f4\u4f4e\u3002", "conclusion": "SSIU\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u548c\u6a21\u578b\u9a71\u52a8\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u3002"}}
{"id": "2506.12015", "pdf": "https://arxiv.org/pdf/2506.12015", "abs": "https://arxiv.org/abs/2506.12015", "authors": ["Hsi-Che Lin", "Yu-Chu Yu", "Kai-Po Chang", "Yu-Chiang Frank Wang"], "title": "EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Under review. Project page: https://hsi-che-lin.github.io/EMLoC/", "summary": "Open-source foundation models have seen rapid adoption and development, enabling powerful general-purpose capabilities across diverse domains. However, fine-tuning large foundation models for domain-specific or personalized tasks remains prohibitively expensive for most users due to the significant memory overhead beyond that of inference. We introduce EMLoC, an Emulator-based Memory-efficient fine-tuning framework with LoRA Correction, which enables model fine-tuning within the same memory budget required for inference. EMLoC constructs a task-specific light-weight emulator using activation-aware singular value decomposition (SVD) on a small downstream calibration set. Fine-tuning then is performed on this lightweight emulator via LoRA. To tackle the misalignment between the original model and the compressed emulator, we propose a novel compensation algorithm to correct the fine-tuned LoRA module, which thus can be merged into the original model for inference. EMLoC supports flexible compression ratios and standard training pipelines, making it adaptable to a wide range of applications. Extensive experiments demonstrate that EMLoC outperforms other baselines across multiple datasets and modalities. Moreover, without quantization, EMLoC enables fine-tuning of a 38B model on a single 24GB consumer GPU-bringing efficient and practical model adaptation to individual users.", "AI": {"tldr": "EMLoC\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u62df\u5668\u7684\u5185\u5b58\u9ad8\u6548\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7LoRA\u4fee\u6b63\uff0c\u4f7f\u6a21\u578b\u5fae\u8c03\u80fd\u5728\u4e0e\u63a8\u7406\u76f8\u540c\u7684\u5185\u5b58\u9884\u7b97\u5185\u5b8c\u6210\u3002", "motivation": "\u5f00\u6e90\u57fa\u7840\u6a21\u578b\u867d\u5f3a\u5927\uff0c\u4f46\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u6216\u4e2a\u6027\u5316\u4efb\u52a1\u7684\u5fae\u8c03\u56e0\u5185\u5b58\u5f00\u9500\u5927\u800c\u6602\u8d35\uff0c\u9650\u5236\u4e86\u7528\u6237\u4f7f\u7528\u3002", "method": "EMLoC\u5229\u7528\u6fc0\u6d3b\u611f\u77e5SVD\u6784\u5efa\u8f7b\u91cf\u7ea7\u6a21\u62df\u5668\uff0c\u901a\u8fc7LoRA\u5fae\u8c03\uff0c\u5e76\u63d0\u51fa\u8865\u507f\u7b97\u6cd5\u4fee\u6b63LoRA\u6a21\u5757\u4ee5\u5bf9\u9f50\u539f\u59cb\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEMLoC\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u6001\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u80fd\u572824GB\u6d88\u8d39\u7ea7GPU\u4e0a\u5fae\u8c0338B\u6a21\u578b\u3002", "conclusion": "EMLoC\u4e3a\u4e2a\u4f53\u7528\u6237\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u6a21\u578b\u9002\u914d\u65b9\u6848\u3002"}}
