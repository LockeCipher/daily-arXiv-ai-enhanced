<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 4]
- [cs.CV](#cs.CV) [Total: 39]
- [cs.LG](#cs.LG) [Total: 4]
- [eess.IV](#eess.IV) [Total: 6]
- [cs.AI](#cs.AI) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Real-Time Per-Garment Virtual Try-On with Temporal Consistency for Loose-Fitting Garments](https://arxiv.org/abs/2506.12348)
*Zaiqiang Wu,I-Chao Shen,Takeo Igarashi*

Main category: cs.GR

TL;DR: 提出了一种两阶段方法，通过提取服装不变表示和改进语义图估计，解决了宽松服装虚拟试穿中的语义图不可靠和帧间抖动问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在宽松服装试穿中表现不佳，主要因语义图估计不可靠和缺乏时间信息导致抖动。

Method: 两阶段方法：提取服装不变表示，通过辅助网络估计语义图；引入循环合成框架利用时间依赖性。

Result: 在图像质量和时间一致性上优于现有方法，消融实验验证了方法的有效性。

Conclusion: 提出的方法显著提升了宽松服装虚拟试穿的效果，解决了语义图不可靠和帧间抖动问题。

Abstract: Per-garment virtual try-on methods collect garment-specific datasets and train networks tailored to each garment to achieve superior results. However, these approaches often struggle with loose-fitting garments due to two key limitations: (1) They rely on human body semantic maps to align garments with the body, but these maps become unreliable when body contours are obscured by loose-fitting garments, resulting in degraded outcomes; (2) They train garment synthesis networks on a per-frame basis without utilizing temporal information, leading to noticeable jittering artifacts. To address these challenges, we propose a two-stage approach for robust semantic map estimation. First, we extract a garment-invariant representation from the raw input image. This representation is then passed through an auxiliary network to estimate the semantic map. This enhances the robustness of semantic map estimation under loose-fitting garments during garment-specific dataset generation. Furthermore, we introduce a recurrent garment synthesis framework that incorporates temporal dependencies to improve frame-to-frame coherence while maintaining real-time performance. We conducted qualitative and quantitative evaluations to demonstrate that our method outperforms existing approaches in both image quality and temporal coherence. Ablation studies further validate the effectiveness of the garment-invariant representation and the recurrent synthesis framework.

</details>


### [2] [iDiT-HOI: Inpainting-based Hand Object Interaction Reenactment via Video Diffusion Transformer](https://arxiv.org/abs/2506.12847)
*Zhelun Shen,Chenming Wu,Junsheng Zhou,Chen Zhao,Kaisiyuan Wang,Hang Zhou,Yingying Li,Haocheng Feng,Wei He,Jingdong Wang*

Main category: cs.GR

TL;DR: 本文提出了一种名为iDiT-HOI的新框架，用于生成野外手-物交互（HOI）重演，通过两阶段视频扩散变换器（DiT）模型和统一的基于修复的标记处理方法（Inp-TPU），实现了对未见过的物体和场景的强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 手-物交互（HOI）的复杂性（如遮挡、物体形状变化和精确物理交互需求）使得生成自然且可信的HOI重演具有挑战性。本文旨在解决这些问题，并提升泛化能力。

Method: 提出iDiT-HOI框架，采用两阶段DiT模型：第一阶段生成关键帧，将指定物体插入手部区域；第二阶段确保时间一致性和流畅性。利用预训练模型的上下文感知能力，无需额外参数。

Result: 实验表明，该方法在真实场景中优于现有方法，提供了更高的真实感和更流畅的手-物交互。

Conclusion: iDiT-HOI框架通过创新的Inp-TPU方法和两阶段DiT模型，有效解决了HOI重演的挑战，并展示了强大的泛化能力和长视频生成支持。

Abstract: Digital human video generation is gaining traction in fields like education and e-commerce, driven by advancements in head-body animation and lip-syncing technologies. However, realistic Hand-Object Interaction (HOI) - the complex dynamics between human hands and objects - continues to pose challenges. Generating natural and believable HOI reenactments is difficult due to issues such as occlusion between hands and objects, variations in object shapes and orientations, and the necessity for precise physical interactions, and importantly, the ability to generalize to unseen humans and objects. This paper presents a novel framework iDiT-HOI that enables in-the-wild HOI reenactment generation. Specifically, we propose a unified inpainting-based token process method, called Inp-TPU, with a two-stage video diffusion transformer (DiT) model. The first stage generates a key frame by inserting the designated object into the hand region, providing a reference for subsequent frames. The second stage ensures temporal coherence and fluidity in hand-object interactions. The key contribution of our method is to reuse the pretrained model's context perception capabilities without introducing additional parameters, enabling strong generalization to unseen objects and scenarios, and our proposed paradigm naturally supports long video generation. Comprehensive evaluations demonstrate that our approach outperforms existing methods, particularly in challenging real-world scenes, offering enhanced realism and more seamless hand-object interactions.

</details>


### [3] [TextureSplat: Per-Primitive Texture Mapping for Reflective Gaussian Splatting](https://arxiv.org/abs/2506.13348)
*Mae Younes,Adnane Boukhayma*

Main category: cs.GR

TL;DR: 提出了一种基于高斯泼溅的辐射场方法，用于解决高反射场景中复杂表面光交互的建模问题。


<details>
  <summary>Details</summary>
Motivation: 高反射场景中的复杂光交互建模是优化逆渲染的挑战性问题，需要更强的表示能力。

Method: 通过几何和物理基础的高斯泼溅辐射场，利用局部空间中的法线和材质属性变化，并结合GPU硬件加速渲染。

Result: 方法能够有效建模高频镜面辐射分量，并实现实时渲染。

Conclusion: 该方法为高反射场景的逆渲染提供了一种高效解决方案。

Abstract: Gaussian Splatting have demonstrated remarkable novel view synthesis performance at high rendering frame rates. Optimization-based inverse rendering within complex capture scenarios remains however a challenging problem. A particular case is modelling complex surface light interactions for highly reflective scenes, which results in intricate high frequency specular radiance components. We hypothesize that such challenging settings can benefit from increased representation power. We hence propose a method that tackles this issue through a geometrically and physically grounded Gaussian Splatting borne radiance field, where normals and material properties are spatially variable in the primitive's local space. Using per-primitive texture maps for this purpose, we also propose to harness the GPU hardware to accelerate rendering at test time via unified material texture atlas.

</details>


### [4] [UltraZoom: Generating Gigapixel Images from Regular Photos](https://arxiv.org/abs/2506.13756)
*Jingwei Ma,Vivek Jayaram,Brian Curless,Ira Kemelmacher-Shlizerman,Steven M. Seitz*

Main category: cs.GR

TL;DR: UltraZoom系统通过结合低分辨率全局图像和高分辨率局部图像，生成超高分辨率图像。


<details>
  <summary>Details</summary>
Motivation: 解决从随意拍摄的手机照片中生成超高分辨率图像的需求。

Method: 构建实例配对数据集，调整预训练生成模型，学习从低分辨率到高分辨率的映射，并使用滑动窗口方法推理。

Result: 系统能够生成一致且逼真的千兆像素图像。

Conclusion: UltraZoom系统通过简单有效的方法实现了从少量输入生成高质量超高分辨率图像。

Abstract: We present UltraZoom, a system for generating gigapixel-resolution images of objects from casually captured inputs, such as handheld phone photos. Given a full-shot image (global, low-detail) and one or more close-ups (local, high-detail), UltraZoom upscales the full image to match the fine detail and scale of the close-up examples. To achieve this, we construct a per-instance paired dataset from the close-ups and adapt a pretrained generative model to learn object-specific low-to-high resolution mappings. At inference, we apply the model in a sliding window fashion over the full image. Constructing these pairs is non-trivial: it requires registering the close-ups within the full image for scale estimation and degradation alignment. We introduce a simple, robust method for getting registration on arbitrary materials in casual, in-the-wild captures. Together, these components form a system that enables seamless pan and zoom across the entire object, producing consistent, photorealistic gigapixel imagery from minimal input.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [5] [Doctor Approved: Generating Medically Accurate Skin Disease Images through AI-Expert Feedback](https://arxiv.org/abs/2506.12323)
*Janet Wang,Yunbei Zhang,Zhengming Ding,Jihun Hamm*

Main category: cs.CV

TL;DR: 论文提出MAGIC框架，通过AI与专家协作生成医学准确的皮肤病图像，提升数据增强效果。


<details>
  <summary>Details</summary>
Motivation: 医学数据稀缺限制了诊断ML模型的泛化能力，现有扩散模型生成的图像医学准确性不足。

Method: 利用多模态大语言模型（MLLMs）作为评估器，将专家定义的标准转化为可操作的反馈，优化扩散模型的图像合成。

Result: 生成的皮肤病图像临床质量显著提升，诊断准确率在20种皮肤病分类任务中提高9.02%，少样本设置下提高13.89%。

Conclusion: MAGIC框架有效结合专家知识与AI，生成高质量的医学图像，显著提升诊断模型的性能。

Abstract: Paucity of medical data severely limits the generalizability of diagnostic ML models, as the full spectrum of disease variability can not be represented by a small clinical dataset. To address this, diffusion models (DMs) have been considered as a promising avenue for synthetic image generation and augmentation. However, they frequently produce medically inaccurate images, deteriorating the model performance. Expert domain knowledge is critical for synthesizing images that correctly encode clinical information, especially when data is scarce and quality outweighs quantity. Existing approaches for incorporating human feedback, such as reinforcement learning (RL) and Direct Preference Optimization (DPO), rely on robust reward functions or demand labor-intensive expert evaluations. Recent progress in Multimodal Large Language Models (MLLMs) reveals their strong visual reasoning capabilities, making them adept candidates as evaluators. In this work, we propose a novel framework, coined MAGIC (Medically Accurate Generation of Images through AI-Expert Collaboration), that synthesizes clinically accurate skin disease images for data augmentation. Our method creatively translates expert-defined criteria into actionable feedback for image synthesis of DMs, significantly improving clinical accuracy while reducing the direct human workload. Experiments demonstrate that our method greatly improves the clinical quality of synthesized skin disease images, with outputs aligning with dermatologist assessments. Additionally, augmenting training data with these synthesized images improves diagnostic accuracy by +9.02% on a challenging 20-condition skin disease classification task, and by +13.89% in the few-shot setting.

</details>


### [6] [UniDet-D: A Unified Dynamic Spectral Attention Model for Object Detection under Adverse Weathers](https://arxiv.org/abs/2506.12324)
*Yuantao Wang,Haowei Yang,Wei Zhang,Shijian Lu*

Main category: cs.CV

TL;DR: UniDet-D是一个统一框架，用于解决多种恶劣天气条件下的物体检测问题，结合动态光谱注意力机制，提升特征表示和检测精度。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的物体检测常因恶劣天气条件（如雨、雾、雪等）导致图像退化，现有方法通常针对单一天气设计，泛化能力不足。

Method: 提出UniDet-D框架，结合动态光谱注意力机制，自适应强调信息光谱成分，抑制无关成分，实现物体检测与图像恢复。

Result: 实验表明UniDet-D在多种恶劣天气条件下检测精度优异，且对未见过的新天气条件（如沙尘暴、雨雾混合）具有强泛化能力。

Conclusion: UniDet-D展示了在真实场景中部署的潜力，为恶劣天气下的物体检测提供了统一解决方案。

Abstract: Real-world object detection is a challenging task where the captured images/videos often suffer from complex degradations due to various adverse weather conditions such as rain, fog, snow, low-light, etc. Despite extensive prior efforts, most existing methods are designed for one specific type of adverse weather with constraints of poor generalization, under-utilization of visual features while handling various image degradations. Leveraging a theoretical analysis on how critical visual details are lost in adverse-weather images, we design UniDet-D, a unified framework that tackles the challenge of object detection under various adverse weather conditions, and achieves object detection and image restoration within a single network. Specifically, the proposed UniDet-D incorporates a dynamic spectral attention mechanism that adaptively emphasizes informative spectral components while suppressing irrelevant ones, enabling more robust and discriminative feature representation across various degradation types. Extensive experiments show that UniDet-D achieves superior detection accuracy across different types of adverse-weather degradation. Furthermore, UniDet-D demonstrates superior generalization towards unseen adverse weather conditions such as sandstorms and rain-fog mixtures, highlighting its great potential for real-world deployment.

</details>


### [7] [Perceptual-GS: Scene-adaptive Perceptual Densification for Gaussian Splatting](https://arxiv.org/abs/2506.12400)
*Hongbi Zhou,Zhangkai Ni*

Main category: cs.CV

TL;DR: Perceptual-GS是一种基于人类感知的3D高斯泼溅技术，通过自适应优化高斯基元分布，提升重建质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以根据场景特征自适应优化高斯基元分布，导致重建质量与效率难以平衡。

Method: 提出感知敏感表示模型，结合人类视觉敏感度，并开发感知敏感自适应分布策略，优化高斯基元分配。

Result: 在多个数据集（包括大场景BungeeNeRF）上验证，Perceptual-GS在重建质量、效率和鲁棒性上达到最优。

Conclusion: Perceptual-GS通过感知敏感优化，显著提升了3D高斯泼溅技术的性能。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for novel view synthesis. However, existing methods struggle to adaptively optimize the distribution of Gaussian primitives based on scene characteristics, making it challenging to balance reconstruction quality and efficiency. Inspired by human perception, we propose scene-adaptive perceptual densification for Gaussian Splatting (Perceptual-GS), a novel framework that integrates perceptual sensitivity into the 3DGS training process to address this challenge. We first introduce a perception-aware representation that models human visual sensitivity while constraining the number of Gaussian primitives. Building on this foundation, we develop a \cameraready{perceptual sensitivity-adaptive distribution} to allocate finer Gaussian granularity to visually critical regions, enhancing reconstruction quality and robustness. Extensive evaluations on multiple datasets, including BungeeNeRF for large-scale scenes, demonstrate that Perceptual-GS achieves state-of-the-art performance in reconstruction quality, efficiency, and robustness. The code is publicly available at: https://github.com/eezkni/Perceptual-GS

</details>


### [8] [CLIP-HandID: Vision-Language Model for Hand-Based Person Identification](https://arxiv.org/abs/2506.12447)
*Nathanael L. Baisa,Babu Pallam,Amudhavel Jayavel*

Main category: cs.CV

TL;DR: 提出了一种基于手部图像的人物识别新方法CLIP-HandID，利用预训练的CLIP模型和文本提示增强识别性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在性侵等严重犯罪中，手部图像常是唯一可用的证据，需要高效识别方法。

Method: 结合CLIP模型的视觉语言能力，通过文本反转网络学习伪标记，增强多模态推理。

Result: 在两个大型公开手部数据集上表现显著优于现有方法。

Conclusion: CLIP-HandID为犯罪调查提供了一种高效的手部图像识别解决方案。

Abstract: This paper introduces a new approach to person identification based on hand images, designed specifically for criminal investigations. The method is particularly valuable in serious crimes like sexual abuse, where hand images are often the sole identifiable evidence available. Our proposed method, CLIP-HandID, leverages pre-trained foundational vision-language model, particularly CLIP, to efficiently learn discriminative deep feature representations from hand images given as input to the image encoder of CLIP using textual prompts as semantic guidance. We propose to learn pseudo-tokens that represent specific visual contexts or appearance attributes using textual inversion network since labels of hand images are indexes instead text descriptions. The learned pseudo-tokens are incorporated into textual prompts which are given as input to the text encoder of the CLIP to leverage its multi-modal reasoning to enhance its generalization for identification. Through extensive evaluations on two large, publicly available hand datasets with multi-ethnic representation, we show that our method substantially surpasses existing approaches.

</details>


### [9] [Retrieval Augmented Comic Image Generation](https://arxiv.org/abs/2506.12517)
*Yunhao Shui,Xuekuan Wang,Feng Qiu,Yuqiu Huang,Jinzhu Li,Haoyu Zheng,Jinru Han,Zhuo Zeng,Pengpeng Zhang,Jiarui Han,Keqiang Sun*

Main category: cs.CV

TL;DR: RaCig是一个生成漫画风格图像序列的系统，解决角色一致性和生动手势问题。


<details>
  <summary>Details</summary>
Motivation: 解决漫画生成中角色身份和服装一致性以及手势多样性的挑战。

Method: 结合检索式角色分配模块和区域角色注入机制。

Result: 实验证明RaCig能生成连贯角色和动态互动的漫画叙事。

Conclusion: 系统代码将公开，支持进一步研究。

Abstract: We present RaCig, a novel system for generating comic-style image sequences with consistent characters and expressive gestures. RaCig addresses two key challenges: (1) maintaining character identity and costume consistency across frames, and (2) producing diverse and vivid character gestures. Our approach integrates a retrieval-based character assignment module, which aligns characters in textual prompts with reference images, and a regional character injection mechanism that embeds character features into specified image regions. Experimental results demonstrate that RaCig effectively generates engaging comic narratives with coherent characters and dynamic interactions. The source code will be publicly available to support further research in this area.

</details>


### [10] [Good Noise Makes Good Edits: A Training-Free Diffusion-Based Video Editing with Image and Text Prompts](https://arxiv.org/abs/2506.12520)
*Saemee Choi,Sohyun Jeong,Jaegul Choo,Jinhee Kim*

Main category: cs.CV

TL;DR: ImEdit是一种无需训练、零样本的视频编辑方法，支持图像和文本条件编辑，通过ρ-start采样和扩张双掩码技术实现高质量编辑。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法通常需要训练或样本支持，限制了灵活性和效率。ImEdit旨在提供一种无需训练、零样本的解决方案。

Method: 采用ρ-start采样和扩张双掩码技术构建结构化噪声图，结合零图像引导和可控负提示策略提升视觉保真度。

Result: 定量和定性评估表明，ImEdit在所有指标上均优于现有方法。

Conclusion: ImEdit为视频编辑提供了一种高效、灵活且高质量的零样本解决方案。

Abstract: We propose ImEdit, the first zero-shot, training-free video editing method conditioned on both images and text. The proposed method introduces $\rho$-start sampling and dilated dual masking to construct well-structured noise maps for coherent and accurate edits. We further present zero image guidance, a controllable negative prompt strategy, for visual fidelity. Both quantitative and qualitative evaluations show that our method outperforms state-of-the-art methods across all metrics.

</details>


### [11] [Towards Seamless Borders: A Method for Mitigating Inconsistencies in Image Inpainting and Outpainting](https://arxiv.org/abs/2506.12530)
*Xingzhong Hou,Jie Wu,Boxiao Liu,Yi Zhang,Guanglu Song,Yunpeng Liu,Yu Liu,Haihang You*

Main category: cs.CV

TL;DR: 本文提出了两种新方法，改进基于扩散模型的图像修复技术，解决颜色不匹配和内容不连贯问题。


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型（如扩散模型和GAN）在图像修复方面取得了显著进展，但实现无缝连续性仍是一个挑战。

Method: 1. 引入改进的变分自编码器以校正颜色失衡；2. 提出两步训练策略优化扩散过程中生成内容与现有内容的融合。

Result: 实验表明，该方法有效减少不连续性，生成高质量、连贯且视觉吸引人的修复结果。

Conclusion: 所提方法显著提升了图像修复的连贯性和视觉质量。

Abstract: Image inpainting is the task of reconstructing missing or damaged parts of an image in a way that seamlessly blends with the surrounding content. With the advent of advanced generative models, especially diffusion models and generative adversarial networks, inpainting has achieved remarkable improvements in visual quality and coherence. However, achieving seamless continuity remains a significant challenge. In this work, we propose two novel methods to address discrepancy issues in diffusion-based inpainting models. First, we introduce a modified Variational Autoencoder that corrects color imbalances, ensuring that the final inpainted results are free of color mismatches. Second, we propose a two-step training strategy that improves the blending of generated and existing image content during the diffusion process. Through extensive experiments, we demonstrate that our methods effectively reduce discontinuity and produce high-quality inpainting results that are coherent and visually appealing.

</details>


### [12] [Performance Plateaus in Inference-Time Scaling for Text-to-Image Diffusion Without External Models](https://arxiv.org/abs/2506.12633)
*Changhyun Choi,Sungha Kim,H. Jin Kim*

Main category: cs.CV

TL;DR: 研究显示，通过优化初始噪声提升文本到图像扩散模型性能，但需外部模型评估。本文提出无需外部模型的Best-of-N推理时缩放方法，验证其在小规模优化步骤下即可达到最佳性能。


<details>
  <summary>Details</summary>
Motivation: 解决依赖外部模型评估的限制，尤其是在小VRAM GPU上无法运行的问题。

Method: 应用Best-of-N推理时缩放方法，优化扩散模型初始噪声，无需外部模型。

Result: 实验表明，该方法在小规模优化步骤下即可达到性能峰值。

Conclusion: Best-of-N推理时缩放方法在优化初始噪声时高效且无需外部模型支持。

Abstract: Recently, it has been shown that investing computing resources in searching for good initial noise for a text-to-image diffusion model helps improve performance. However, previous studies required external models to evaluate the resulting images, which is impossible on GPUs with small VRAM. For these reasons, we apply Best-of-N inference-time scaling to algorithms that optimize the initial noise of a diffusion model without external models across multiple datasets and backbones. We demonstrate that inference-time scaling for text-to-image diffusion models in this setting quickly reaches a performance plateau, and a relatively small number of optimization steps suffices to achieve the maximum achievable performance with each algorithm.

</details>


### [13] [3D Hand Mesh-Guided AI-Generated Malformed Hand Refinement with Hand Pose Transformation via Diffusion Model](https://arxiv.org/abs/2506.12680)
*Chen-Bin Feng,Kangdao Liu,Jian Sun,Jiping Jin,Yiguo Jiang,Chi-Man Vong*

Main category: cs.CV

TL;DR: 提出了一种基于3D网格和扩散管线的框架，用于修复AI生成图像中的畸形手部，并通过姿态变换增加多样性。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度的方法因手部深度估计器的性能限制，无法准确表示手部细节，导致生成错误。

Method: 使用先进的3D手部网格估计器提供更多细节，设计扩散修复模型，并引入双检查算法优化推理。

Result: 实验结果表明，该方法在修复畸形手部和姿态变换方面表现优异。

Conclusion: 提出的框架显著提升了手部修复的真实性和多样性，无需额外训练即可实现姿态变换。

Abstract: The malformed hands in the AI-generated images seriously affect the authenticity of the images. To refine malformed hands, existing depth-based approaches use a hand depth estimator to guide the refinement of malformed hands. Due to the performance limitations of the hand depth estimator, many hand details cannot be represented, resulting in errors in the generated hands, such as confusing the palm and the back of the hand. To solve this problem, we propose a 3D mesh-guided refinement framework using a diffusion pipeline. We use a state-of-the-art 3D hand mesh estimator, which provides more details of the hands. For training, we collect and reannotate a dataset consisting of RGB images and 3D hand mesh. Then we design a diffusion inpainting model to generate refined outputs guided by 3D hand meshes. For inference, we propose a double check algorithm to facilitate the 3D hand mesh estimator to obtain robust hand mesh guidance to obtain our refined results. Beyond malformed hand refinement, we propose a novel hand pose transformation method. It increases the flexibility and diversity of the malformed hand refinement task. We made the restored images mimic the hand poses of the reference images. The pose transformation requires no additional training. Extensive experimental results demonstrate the superior performance of our proposed method.

</details>


### [14] [Generative 4D Scene Gaussian Splatting with Object View-Synthesis Priors](https://arxiv.org/abs/2506.12716)
*Wen-Hsuan Chu,Lei Ke,Jianmeng Liu,Mingxiao Huo,Pavel Tokmakov,Katerina Fragkiadaki*

Main category: cs.CV

TL;DR: GenMOJO是一种新方法，通过结合可变形3D高斯优化和生成先验，从单目多目标视频中生成动态4D场景，解决了复杂遮挡场景的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有模型在孤立对象的新视角合成上表现良好，但在复杂、杂乱场景中泛化能力不足。

Method: GenMOJO将场景分解为独立对象，为每个对象优化可变形高斯集，利用对象中心扩散模型推断未观测区域，并通过联合高斯溅射渲染完整场景。

Result: GenMOJO生成了空间和时间上的4D对象重建，并从单目输入中生成准确的2D和3D点轨迹。

Conclusion: GenMOJO在生成场景新视角和点轨迹准确性上优于现有方法，定量评估和人类感知研究证实了其优越性。

Abstract: We tackle the challenge of generating dynamic 4D scenes from monocular, multi-object videos with heavy occlusions, and introduce GenMOJO, a novel approach that integrates rendering-based deformable 3D Gaussian optimization with generative priors for view synthesis. While existing models perform well on novel view synthesis for isolated objects, they struggle to generalize to complex, cluttered scenes. To address this, GenMOJO decomposes the scene into individual objects, optimizing a differentiable set of deformable Gaussians per object. This object-wise decomposition allows leveraging object-centric diffusion models to infer unobserved regions in novel viewpoints. It performs joint Gaussian splatting to render the full scene, capturing cross-object occlusions, and enabling occlusion-aware supervision. To bridge the gap between object-centric priors and the global frame-centric coordinate system of videos, GenMOJO uses differentiable transformations that align generative and rendering constraints within a unified framework. The resulting model generates 4D object reconstructions over space and time, and produces accurate 2D and 3D point tracks from monocular input. Quantitative evaluations and perceptual human studies confirm that GenMOJO generates more realistic novel views of scenes and produces more accurate point tracks compared to existing approaches.

</details>


### [15] [Efficient multi-view training for 3D Gaussian Splatting](https://arxiv.org/abs/2506.12727)
*Minhyuk Choi,Injae Kim,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: 3D高斯泼溅（3DGS）因其渲染速度快成为逆向渲染的首选，但单视图训练导致优化不佳。本文提出多视图训练方法，通过改进光栅化过程和引入新损失函数，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 单视图训练在3DGS中导致梯度方差大，优化效果不佳，需多视图训练。

Method: 改进光栅化以减少多视图训练开销，提出3D距离感知D-SSIM损失和多视图自适应密度控制。

Result: 实验表明，新方法显著提升3DGS及其变体性能。

Conclusion: 多视图训练方法解放了3DGS的单视图限制，提升了性能。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a preferred choice alongside Neural Radiance Fields (NeRF) in inverse rendering due to its superior rendering speed. Currently, the common approach in 3DGS is to utilize "single-view" mini-batch training, where only one image is processed per iteration, in contrast to NeRF's "multi-view" mini-batch training, which leverages multiple images. We observe that such single-view training can lead to suboptimal optimization due to increased variance in mini-batch stochastic gradients, highlighting the necessity for multi-view training. However, implementing multi-view training in 3DGS poses challenges. Simply rendering multiple images per iteration incurs considerable overhead and may result in suboptimal Gaussian densification due to its reliance on single-view assumptions. To address these issues, we modify the rasterization process to minimize the overhead associated with multi-view training and propose a 3D distance-aware D-SSIM loss and multi-view adaptive density control that better suits multi-view scenarios. Our experiments demonstrate that the proposed methods significantly enhance the performance of 3DGS and its variants, freeing 3DGS from the constraints of single-view training.

</details>


### [16] [Adaptive Dropout: Unleashing Dropout across Layers for Generalizable Image Super-Resolution](https://arxiv.org/abs/2506.12738)
*Hang Xu,Wei Yu,Jiangtong Tan,Zhen Zou,Feng Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种名为Adaptive Dropout的新正则化方法，用于解决盲超分辨率（blind SR）中中间层特征泛化不足的问题，通过自适应调整训练策略和特征整合，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 盲超分辨率模型在未知退化情况下泛化能力不足，现有方法仅关注最终层的特征正则化，忽略了中间层特征泛化的重要性，直接应用Dropout会导致性能下降。

Method: 提出Adaptive Dropout，重新设计Dropout形式并自适应整合特征，同时通过分层退火策略解决不同层泛化需求不一致的问题。

Result: 实验表明，该方法在合成和真实数据集上均优于以往正则化方法，并在其他图像修复任务中表现出色。

Conclusion: Adaptive Dropout有效解决了盲超分辨率中特征泛化不足的问题，为图像修复任务提供了新的正则化思路。

Abstract: Blind Super-Resolution (blind SR) aims to enhance the model's generalization ability with unknown degradation, yet it still encounters severe overfitting issues. Some previous methods inspired by dropout, which enhances generalization by regularizing features, have shown promising results in blind SR. Nevertheless, these methods focus solely on regularizing features before the final layer and overlook the need for generalization in features at intermediate layers. Without explicit regularization of features at intermediate layers, the blind SR network struggles to obtain well-generalized feature representations. However, the key challenge is that directly applying dropout to intermediate layers leads to a significant performance drop, which we attribute to the inconsistency in training-testing and across layers it introduced. Therefore, we propose Adaptive Dropout, a new regularization method for blind SR models, which mitigates the inconsistency and facilitates application across intermediate layers of networks. Specifically, for training-testing inconsistency, we re-design the form of dropout and integrate the features before and after dropout adaptively. For inconsistency in generalization requirements across different layers, we innovatively design an adaptive training strategy to strengthen feature propagation by layer-wise annealing. Experimental results show that our method outperforms all past regularization methods on both synthetic and real-world benchmark datasets, also highly effective in other image restoration tasks. Code is available at \href{https://github.com/xuhang07/Adpative-Dropout}{https://github.com/xuhang07/Adpative-Dropout}.

</details>


### [17] [A large-scale, physically-based synthetic dataset for satellite pose estimation](https://arxiv.org/abs/2506.12782)
*Szabolcs Velkei,Csaba Goldschmidt,Károly Vass*

Main category: cs.CV

TL;DR: DLVS3提出了一种用于卫星姿态估计的合成数据集生成器和仿真流程，并发布了DLVS3-HST-V1数据集，专注于哈勃太空望远镜这一复杂目标。


<details>
  <summary>Details</summary>
Motivation: 解决卫星姿态估计中真实数据获取困难的问题，通过合成数据填补领域差距。

Method: 使用实时和离线渲染技术生成高保真3D模型，结合动态光照和物理准确的材质属性，生成大规模、多标注的图像集。

Result: DLVS3-HST-V1数据集支持6-DoF姿态、关键点数据、语义分割、深度和法线图，为深度学习模型提供多样化的训练和测试条件。

Conclusion: DLVS3为自主航天器操作提供了重要的仿真工具，缩小了领域差距。

Abstract: The Deep Learning Visual Space Simulation System (DLVS3) introduces a novel synthetic dataset generator and a simulation pipeline specifically designed for training and testing satellite pose estimation solutions. This work introduces the DLVS3-HST-V1 dataset, which focuses on the Hubble Space Telescope (HST) as a complex, articulated target. The dataset is generated using advanced real-time and offline rendering technologies, integrating high-fidelity 3D models, dynamic lighting (including secondary sources like Earth reflection), and physically accurate material properties. The pipeline supports the creation of large-scale, richly annotated image sets with ground-truth 6-DoF pose and keypoint data, semantic segmentation, depth, and normal maps. This enables the training and benchmarking of deep learning-based pose estimation solutions under realistic, diverse, and challenging visual conditions. The paper details the dataset generation process, the simulation architecture, and the integration with deep learning frameworks, and positions DLVS3 as a significant step toward closing the domain gap for autonomous spacecraft operations in proximity and servicing missions.

</details>


### [18] [Rasterizing Wireless Radiance Field via Deformable 2D Gaussian Splatting](https://arxiv.org/abs/2506.12787)
*Mufan Liu,Cixiao Zhang,Qi Yang,Yujie Cao,Yiling Xu,Yin Xu,Shu Sun,Mingzeng Dai,Yunfeng Guan*

Main category: cs.CV

TL;DR: SwiftWRF是一种基于高斯泼溅的高效无线辐射场建模方法，显著提升了重建速度和信号质量。


<details>
  <summary>Details</summary>
Motivation: 传统无线辐射场建模方法精度有限或依赖强场景先验，而基于NeRF的方法计算成本高，难以实时部署。

Method: 提出SwiftWRF框架，利用可变形2D高斯泼溅和CUDA加速栅格化，结合轻量级MLP建模变形，实现高效重建。

Result: 实验表明，SwiftWRF比现有方法快500倍，信号质量显著提升，适用于AoA和RSSI预测。

Conclusion: SwiftWRF为无线辐射场建模提供了高效且准确的解决方案，具有实际应用潜力。

Abstract: Modeling the wireless radiance field (WRF) is fundamental to modern communication systems, enabling key tasks such as localization, sensing, and channel estimation. Traditional approaches, which rely on empirical formulas or physical simulations, often suffer from limited accuracy or require strong scene priors. Recent neural radiance field (NeRF-based) methods improve reconstruction fidelity through differentiable volumetric rendering, but their reliance on computationally expensive multilayer perceptron (MLP) queries hinders real-time deployment. To overcome these challenges, we introduce Gaussian splatting (GS) to the wireless domain, leveraging its efficiency in modeling optical radiance fields to enable compact and accurate WRF reconstruction. Specifically, we propose SwiftWRF, a deformable 2D Gaussian splatting framework that synthesizes WRF spectra at arbitrary positions under single-sided transceiver mobility. SwiftWRF employs CUDA-accelerated rasterization to render spectra at over 100000 fps and uses a lightweight MLP to model the deformation of 2D Gaussians, effectively capturing mobility-induced WRF variations. In addition to novel spectrum synthesis, the efficacy of SwiftWRF is further underscored in its applications in angle-of-arrival (AoA) and received signal strength indicator (RSSI) prediction. Experiments conducted on both real-world and synthetic indoor scenes demonstrate that SwiftWRF can reconstruct WRF spectra up to 500x faster than existing state-of-the-art methods, while significantly enhancing its signal quality. Code and datasets will be released.

</details>


### [19] [SMPL Normal Map Is All You Need for Single-view Textured Human Reconstruction](https://arxiv.org/abs/2506.12793)
*Wenhao Shen,Gangjian Zhang,Jianfeng Zhang,Yu Feng,Nanjie Yao,Xuanmeng Zhang,Hao Wang*

Main category: cs.CV

TL;DR: SEHR框架通过结合SMPL法线图引导和约束，实现了单视角3D人体重建，无需预设扩散模型，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于稀缺的3D人体数据或易产生2D幻觉，SEHR旨在解决这些问题。

Method: SEHR包含SMPL法线图引导（SNMG）和约束（SNMC），通过辅助网络和额外预测增强重建效果。

Result: 在两个基准数据集上的实验表明，SEHR优于现有最优方法。

Conclusion: SEHR框架有效解决了单视角3D人体重建的挑战，具有显著优势。

Abstract: Single-view textured human reconstruction aims to reconstruct a clothed 3D digital human by inputting a monocular 2D image. Existing approaches include feed-forward methods, limited by scarce 3D human data, and diffusion-based methods, prone to erroneous 2D hallucinations. To address these issues, we propose a novel SMPL normal map Equipped 3D Human Reconstruction (SEHR) framework, integrating a pretrained large 3D reconstruction model with human geometry prior. SEHR performs single-view human reconstruction without using a preset diffusion model in one forward propagation. Concretely, SEHR consists of two key components: SMPL Normal Map Guidance (SNMG) and SMPL Normal Map Constraint (SNMC). SNMG incorporates SMPL normal maps into an auxiliary network to provide improved body shape guidance. SNMC enhances invisible body parts by constraining the model to predict an extra SMPL normal Gaussians. Extensive experiments on two benchmark datasets demonstrate that SEHR outperforms existing state-of-the-art methods.

</details>


### [20] [Learning Unpaired Image Dehazing with Physics-based Rehazy Generation](https://arxiv.org/abs/2506.12824)
*Haoyou Deng,Zhiqiang Li,Feng Zhang,Qingbo Lu,Zisheng Cao,Yuanjie Shao,Shuhang Gu,Changxin Gao,Nong Sang*

Main category: cs.CV

TL;DR: 提出了一种名为Rehazy的新型训练策略，通过利用模糊-再模糊对和双分支框架，显著提升了图像去雾的性能和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在图像去雾中因过拟合合成数据而泛化能力差，且训练不稳定。

Method: 提出Rehazy策略，利用模糊-再模糊对学习真实雾特性，并设计双分支框架和渐进式去雾网络。

Result: 在四个基准测试中表现优异，PSNR指标分别提升3.58 dB和1.85 dB。

Conclusion: Rehazy策略有效解决了训练不稳定和泛化能力差的问题，性能优于现有方法。

Abstract: Overfitting to synthetic training pairs remains a critical challenge in image dehazing, leading to poor generalization capability to real-world scenarios. To address this issue, existing approaches utilize unpaired realistic data for training, employing CycleGAN or contrastive learning frameworks. Despite their progress, these methods often suffer from training instability, resulting in limited dehazing performance. In this paper, we propose a novel training strategy for unpaired image dehazing, termed Rehazy, to improve both dehazing performance and training stability. This strategy explores the consistency of the underlying clean images across hazy images and utilizes hazy-rehazy pairs for effective learning of real haze characteristics. To favorably construct hazy-rehazy pairs, we develop a physics-based rehazy generation pipeline, which is theoretically validated to reliably produce high-quality rehazy images. Additionally, leveraging the rehazy strategy, we introduce a dual-branch framework for dehazing network training, where a clean branch provides a basic dehazing capability in a synthetic manner, and a hazy branch enhances the generalization ability with hazy-rehazy pairs. Moreover, we design a new dehazing network within these branches to improve the efficiency, which progressively restores clean scenes from coarse to fine. Extensive experiments on four benchmarks demonstrate the superior performance of our approach, exceeding the previous state-of-the-art methods by 3.58 dB on the SOTS-Indoor dataset and by 1.85 dB on the SOTS-Outdoor dataset in PSNR. Our code will be publicly available.

</details>


### [21] [DiffS-NOCS: 3D Point Cloud Reconstruction through Coloring Sketches to NOCS Maps Using Diffusion Models](https://arxiv.org/abs/2506.12835)
*Di Kong,Qianhui Wan*

Main category: cs.CV

TL;DR: DiffS-NOCS是一种基于扩散模型的方法，通过生成NOCS地图从2D草图重建3D点云，解决了多模态融合和3D结构重建的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D空间中直接操作，难以从2D草图中准确重建3D结构，且缺乏多模态控制能力。

Method: 使用ControlNet和改进的多视角解码器生成NOCS地图，结合视角编码器和多视角聚合网络提升3D一致性。

Result: 在ShapeNet上的实验表明，DiffS-NOCS能够实现可控且精细的点云重建。

Conclusion: DiffS-NOCS通过多模态融合和多视角信息交换，显著提升了草图到3D点云的重建效果。

Abstract: Reconstructing a 3D point cloud from a given conditional sketch is challenging. Existing methods often work directly in 3D space, but domain variability and difficulty in reconstructing accurate 3D structures from 2D sketches remain significant obstacles. Moreover, ideal models should also accept prompts for control, in addition with the sparse sketch, posing challenges in multi-modal fusion. We propose DiffS-NOCS (Diffusion-based Sketch-to-NOCS Map), which leverages ControlNet with a modified multi-view decoder to generate NOCS maps with embedded 3D structure and position information in 2D space from sketches. The 3D point cloud is reconstructed by combining multiple NOCS maps from different views. To enhance sketch understanding, we integrate a viewpoint encoder for extracting viewpoint features. Additionally, we design a feature-level multi-view aggregation network as the denoising module, facilitating cross-view information exchange and improving 3D consistency in NOCS map generation. Experiments on ShapeNet demonstrate that DiffS-NOCS achieves controllable and fine-grained point cloud reconstruction aligned with sketches.

</details>


### [22] [EraserDiT: Fast Video Inpainting with Diffusion Transformer Model](https://arxiv.org/abs/2506.12853)
*Jie Liu,Zheng Hui*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散变换器（DiT）的视频修复方法，通过结合扩散模型和变换器架构，解决了传统方法在长期时间一致性和大面积掩码修复中的不足。


<details>
  <summary>Details</summary>
Motivation: 传统视频修复方法在长期时间特征利用和时间一致性方面表现不佳，尤其是在处理大面积掩码时效果较差。

Method: 采用扩散变换器（DiT）结合环形位置偏移策略，增强长期时间一致性，并自动检测和移除视频中的指定对象。

Result: 在1080×1920分辨率、121帧的视频上仅需180秒完成修复，实验结果显示在内容保真度、纹理恢复和时间一致性方面表现优越。

Conclusion: 提出的方法显著提升了视频修复的质量和效率，尤其在长期时间一致性方面表现突出。

Abstract: Video object removal and inpainting are critical tasks in the fields of computer vision and multimedia processing, aimed at restoring missing or corrupted regions in video sequences. Traditional methods predominantly rely on flow-based propagation and spatio-temporal Transformers, but these approaches face limitations in effectively leveraging long-term temporal features and ensuring temporal consistency in the completion results, particularly when dealing with large masks. Consequently, performance on extensive masked areas remains suboptimal. To address these challenges, this paper introduces a novel video inpainting approach leveraging the Diffusion Transformer (DiT). DiT synergistically combines the advantages of diffusion models and transformer architectures to maintain long-term temporal consistency while ensuring high-quality inpainting results. We propose a Circular Position-Shift strategy to further enhance long-term temporal consistency during the inference stage. Additionally, the proposed method automatically detects objects within videos, interactively removes specified objects, and generates corresponding prompts. In terms of processing speed, it takes only 180 seconds (testing on one NVIDIA A100 GPU) to complete a video with a resolution of $1080 \times 1920$ with 121 frames without any acceleration method. Experimental results indicate that the proposed method demonstrates superior performance in content fidelity, texture restoration, and temporal consistency. Project page: https://jieliu95.github.io/EraserDiT_demo.

</details>


### [23] [Efficient Neural Video Representation via Structure-Preseving Patch Decoding](https://arxiv.org/abs/2506.12896)
*Taiga Hayami,Kakeru Koizumi,Hiroshi Watanabe*

Main category: cs.CV

TL;DR: 提出了一种基于结构保持块（SPPs）的神经视频表示方法，通过重新排列帧为空间结构化的块帧，提升重建质量和压缩性能。


<details>
  <summary>Details</summary>
Motivation: 传统均匀块划分在块边界处可能导致不连续性，影响全局结构的连贯性。

Method: 使用类似PixelUnshuffle的操作将帧重新排列为空间结构化的块帧，网络学习预测这些块帧，支持全局到局部的拟合策略。

Result: 在标准视频数据集上，该方法相比现有基于INR的视频表示方法，提高了重建质量和压缩性能。

Conclusion: SPPs方法通过保持空间连贯性和局部解码，有效解决了传统块划分的局限性。

Abstract: Implicit Neural Representations (INRs) have attracted significant interest for their ability to model complex signals by mapping spatial and temporal coordinates to signal values. In the context of neural video representation, several decoding strategies have been explored to balance compactness and reconstruction quality, including pixel-wise, frame-wise, and patch-wise methods. Patch-wise decoding aims to combine the flexibility of pixel-based models with the efficiency of frame-based approaches. However, conventional uniform patch division often leads to discontinuities at patch boundaries, as independently reconstructed regions may fail to form a coherent global structure. To address this limitation, we propose a neural video representation method based on Structure-Preserving Patches (SPPs). Our approach rearranges each frame into a set of spatially structured patch frames using a PixelUnshuffle-like operation. This rearrangement maintains the spatial coherence of the original frame while enabling patch-level decoding. The network learns to predict these rearranged patch frames, which supports a global-to-local fitting strategy and mitigates degradation caused by upsampling. Experiments on standard video datasets show that the proposed method improves reconstruction quality and compression performance compared to existing INR-based video representation methods.

</details>


### [24] [Metropolis-Hastings Sampling for 3D Gaussian Reconstruction](https://arxiv.org/abs/2506.12945)
*Hyunjin Kim,Haebeom Jung,Jaesik Park*

Main category: cs.CV

TL;DR: 提出了一种基于Metropolis-Hastings的自适应采样框架，用于3D高斯泼溅（3DGS），通过多视角光度误差信号动态调整高斯分布，减少冗余计算。


<details>
  <summary>Details</summary>
Motivation: 传统3DGS方法依赖启发式密度控制机制（如克隆、分裂和修剪），可能导致冗余计算或过早移除有用高斯。

Method: 将密度控制和修剪重新定义为概率采样过程，基于多视角误差和不透明度分数动态插入和重定位高斯。

Result: 在多个基准数据集上实验表明，该方法减少了所需高斯数量，提升了计算效率，同时保持或略微超越现有模型的视图合成质量。

Conclusion: 该方法通过概率采样减少对启发式方法的依赖，灵活适应场景复杂度，提升了3DGS的效率和质量。

Abstract: We propose an adaptive sampling framework for 3D Gaussian Splatting (3DGS) that leverages comprehensive multi-view photometric error signals within a unified Metropolis-Hastings approach. Traditional 3DGS methods heavily rely on heuristic-based density-control mechanisms (e.g., cloning, splitting, and pruning), which can lead to redundant computations or the premature removal of beneficial Gaussians. Our framework overcomes these limitations by reformulating densification and pruning as a probabilistic sampling process, dynamically inserting and relocating Gaussians based on aggregated multi-view errors and opacity scores. Guided by Bayesian acceptance tests derived from these error-based importance scores, our method substantially reduces reliance on heuristics, offers greater flexibility, and adaptively infers Gaussian distributions without requiring predefined scene complexity. Experiments on benchmark datasets, including Mip-NeRF360, Tanks and Temples, and Deep Blending, show that our approach reduces the number of Gaussians needed, enhancing computational efficiency while matching or modestly surpassing the view-synthesis quality of state-of-the-art models.

</details>


### [25] [GS-2DGS: Geometrically Supervised 2DGS for Reflective Object Reconstruction](https://arxiv.org/abs/2506.13110)
*Jinguang Tong,Xuesong li,Fahira Afzal Maken,Sundaram Muthu,Lars Petersson,Chuong Nguyen,Hongdong Li*

Main category: cs.CV

TL;DR: 提出GS-2DGS方法，结合2D高斯泼溅与几何信息，高效重建高反射物体。


<details>
  <summary>Details</summary>
Motivation: 解决高反射物体3D建模中传统SDF方法耗时且平滑，而3D高斯泼溅缺乏几何约束的问题。

Method: 基于2D高斯泼溅，结合基础模型的几何信息，提出GS-2DGS方法。

Result: 在合成和真实数据集上，重建和重光照效果显著优于高斯方法，速度比SDF方法快一个数量级。

Conclusion: GS-2DGS在高反射物体重建中实现了高效与高质量的平衡。

Abstract: 3D modeling of highly reflective objects remains challenging due to strong view-dependent appearances. While previous SDF-based methods can recover high-quality meshes, they are often time-consuming and tend to produce over-smoothed surfaces. In contrast, 3D Gaussian Splatting (3DGS) offers the advantage of high speed and detailed real-time rendering, but extracting surfaces from the Gaussians can be noisy due to the lack of geometric constraints. To bridge the gap between these approaches, we propose a novel reconstruction method called GS-2DGS for reflective objects based on 2D Gaussian Splatting (2DGS). Our approach combines the rapid rendering capabilities of Gaussian Splatting with additional geometric information from foundation models. Experimental results on synthetic and real datasets demonstrate that our method significantly outperforms Gaussian-based techniques in terms of reconstruction and relighting and achieves performance comparable to SDF-based methods while being an order of magnitude faster. Code is available at https://github.com/hirotong/GS2DGS

</details>


### [26] [STAGE: A Stream-Centric Generative World Model for Long-Horizon Driving-Scene Simulation](https://arxiv.org/abs/2506.13138)
*Jiamin Wang,Yichen Yao,Xiang Feng,Hang Wu,Yaming Wang,Qingqiu Huang,Yuexin Ma,Xinge Zhu*

Main category: cs.CV

TL;DR: STAGE提出了一种新的自回归框架，通过分层特征协调和多阶段优化，解决了长时程驾驶视频生成中的错误累积和特征对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在时空动态解耦和跨帧特征传播方面存在不足，导致错误累积和特征错位，限制了高质量长时程驾驶视频的生成。

Method: STAGE采用分层时间特征传递（HTFT）和多阶段训练策略，分别建模时间和去噪过程，并通过特征传递增强帧间一致性。

Result: 在Nuscenes数据集上，STAGE显著优于现有方法，并成功生成了600帧高质量驾驶视频，远超其他方法的极限。

Conclusion: STAGE通过创新性的分层特征协调和多阶段优化，为长时程驾驶视频生成提供了高效且高质量的解决方案。

Abstract: The generation of temporally consistent, high-fidelity driving videos over extended horizons presents a fundamental challenge in autonomous driving world modeling. Existing approaches often suffer from error accumulation and feature misalignment due to inadequate decoupling of spatio-temporal dynamics and limited cross-frame feature propagation mechanisms. To address these limitations, we present STAGE (Streaming Temporal Attention Generative Engine), a novel auto-regressive framework that pioneers hierarchical feature coordination and multi-phase optimization for sustainable video synthesis. To achieve high-quality long-horizon driving video generation, we introduce Hierarchical Temporal Feature Transfer (HTFT) and a novel multi-stage training strategy. HTFT enhances temporal consistency between video frames throughout the video generation process by modeling the temporal and denoising process separately and transferring denoising features between frames. The multi-stage training strategy is to divide the training into three stages, through model decoupling and auto-regressive inference process simulation, thereby accelerating model convergence and reducing error accumulation. Experiments on the Nuscenes dataset show that STAGE has significantly surpassed existing methods in the long-horizon driving video generation task. In addition, we also explored STAGE's ability to generate unlimited-length driving videos. We generated 600 frames of high-quality driving videos on the Nuscenes dataset, which far exceeds the maximum length achievable by existing methods.

</details>


### [27] [COME: Adding Scene-Centric Forecasting Control to Occupancy World Model](https://arxiv.org/abs/2506.13260)
*Yining Shi,Kun Jiang,Qiang Meng,Ke Wang,Jiabao Wang,Wenchao Sun,Tuopu Wen,Mengmeng Yang,Diange Yang*

Main category: cs.CV

TL;DR: COME框架通过分离环境变化与自车运动，提升了自动驾驶世界模型的预测精度，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以区分自车运动与场景动态变化，导致预测效果不佳。

Method: COME利用场景中心坐标系，通过场景中心预测分支生成与自车无关的未来特征，再通过ControlNet转换为场景条件，注入占用世界模型。

Result: 在nuScenes-Occ3D数据集上，COME比DOME和UniScene分别提升了26.3%和23.7%的mIoU指标。

Conclusion: 解耦表示学习能显著提升世界模型的时空预测准确性。

Abstract: World models are critical for autonomous driving to simulate environmental dynamics and generate synthetic data. Existing methods struggle to disentangle ego-vehicle motion (perspective shifts) from scene evolvement (agent interactions), leading to suboptimal predictions. Instead, we propose to separate environmental changes from ego-motion by leveraging the scene-centric coordinate systems. In this paper, we introduce COME: a framework that integrates scene-centric forecasting Control into the Occupancy world ModEl. Specifically, COME first generates ego-irrelevant, spatially consistent future features through a scene-centric prediction branch, which are then converted into scene condition using a tailored ControlNet. These condition features are subsequently injected into the occupancy world model, enabling more accurate and controllable future occupancy predictions. Experimental results on the nuScenes-Occ3D dataset show that COME achieves consistent and significant improvements over state-of-the-art (SOTA) methods across diverse configurations, including different input sources (ground-truth, camera-based, fusion-based occupancy) and prediction horizons (3s and 8s). For example, under the same settings, COME achieves 26.3% better mIoU metric than DOME and 23.7% better mIoU metric than UniScene. These results highlight the efficacy of disentangled representation learning in enhancing spatio-temporal prediction fidelity for world models. Code and videos will be available at https://github.com/synsin0/COME.

</details>


### [28] [Fair Generation without Unfair Distortions: Debiasing Text-to-Image Generation with Entanglement-Free Attention](https://arxiv.org/abs/2506.13298)
*Jeonghoon Park,Juyoung Lee,Chaeyeon Chung,Jaeseong Lee,Jaegul Choo,Jindong Gu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Entanglement-Free Attention (EFA)的方法，用于解决扩散式文本到图像（T2I）模型中的社会偏见问题，同时避免非目标属性的干扰。


<details>
  <summary>Details</summary>
Motivation: 扩散式T2I模型在生成高质量图像时存在社会偏见（如性别、种族等），现有方法在调整目标属性时会干扰非目标属性，导致不希望的分布偏移。

Method: 提出EFA方法，通过随机采样目标属性并调整交叉注意力层，实现目标属性的公平分布，同时保留非目标属性。

Result: 实验表明，EFA在减少偏见的同时优于现有方法，并能保持原始模型的输出分布和生成能力。

Conclusion: EFA有效解决了偏见问题，同时避免了非目标属性的干扰，为T2I模型的公平性提供了新思路。

Abstract: Recent advancements in diffusion-based text-to-image (T2I) models have enabled the generation of high-quality and photorealistic images from text descriptions. However, they often exhibit societal biases related to gender, race, and socioeconomic status, thereby reinforcing harmful stereotypes and shaping public perception in unintended ways. While existing bias mitigation methods demonstrate effectiveness, they often encounter attribute entanglement, where adjustments to attributes relevant to the bias (i.e., target attributes) unintentionally alter attributes unassociated with the bias (i.e., non-target attributes), causing undesirable distribution shifts. To address this challenge, we introduce Entanglement-Free Attention (EFA), a method that accurately incorporates target attributes (e.g., White, Black, Asian, and Indian) while preserving non-target attributes (e.g., background details) during bias mitigation. At inference time, EFA randomly samples a target attribute with equal probability and adjusts the cross-attention in selected layers to incorporate the sampled attribute, achieving a fair distribution of target attributes. Extensive experiments demonstrate that EFA outperforms existing methods in mitigating bias while preserving non-target attributes, thereby maintaining the output distribution and generation capability of the original model.

</details>


### [29] [Quantitative Comparison of Fine-Tuning Techniques for Pretrained Latent Diffusion Models in the Generation of Unseen SAR Image Concepts](https://arxiv.org/abs/2506.13307)
*Solène Debuysère,Nicolas Trouvé,Nathan Letheule,Olivier Lévêque,Elise Colin*

Main category: cs.CV

TL;DR: 研究探讨了如何将预训练的潜在扩散模型适应于合成孔径雷达（SAR）这一全新成像领域，通过多种微调策略优化模型表现。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练生成模型在自然图像上表现优异，但SAR数据的物理特性、统计分布和视觉特征与之不同，需要专门适配。

Method: 使用大规模SAR数据集，比较了全模型微调和参数高效方法（如LoRA），分别针对UNet扩散主干和文本编码器。

Result: 混合微调策略表现最佳：UNet全微调捕捉SAR低层特征，LoRA部分微调文本编码器保持提示对齐。

Conclusion: 为将基础模型适应于非自然图像领域提供了系统性策略。

Abstract: This work investigates the adaptation of large pre-trained latent diffusion models to a radically new imaging domain: Synthetic Aperture Radar (SAR). While these generative models, originally trained on natural images, demonstrate impressive capabilities in text-to-image synthesis, they are not natively adapted to represent SAR data, which involves different physics, statistical distributions, and visual characteristics. Using a sizeable SAR dataset (on the order of 100,000 to 1 million images), we address the fundamental question of fine-tuning such models for this unseen modality. We explore and compare multiple fine-tuning strategies, including full model fine-tuning and parameter-efficient approaches like Low-Rank Adaptation (LoRA), focusing separately on the UNet diffusion backbone and the text encoder components. To evaluate generative quality, we combine several metrics: statistical distance from real SAR distributions, textural similarity via GLCM descriptors, and semantic alignment assessed with a CLIP model fine-tuned on SAR data. Our results show that a hybrid tuning strategy yields the best performance: full fine-tuning of the UNet is better at capturing low-level SAR-specific patterns, while LoRA-based partial tuning of the text encoder, combined with embedding learning of the <SAR> token, suffices to preserve prompt alignment. This work provides a methodical strategy for adapting foundation models to unconventional imaging modalities beyond natural image domains.

</details>


### [30] [DicFace: Dirichlet-Constrained Variational Codebook Learning for Temporally Coherent Video Face Restoration](https://arxiv.org/abs/2506.13355)
*Yan Chen,Hanlin Shang,Ce Liu,Yuxuan Chen,Hui Li,Weihao Yuan,Hao Zhu,Zilong Dong,Siyu Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种基于VQ-VAEs的视频人脸修复方法，通过变分潜在空间建模实现时间一致性，并在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 视频人脸修复在保持时间一致性的同时恢复细节是一个关键挑战。

Method: 扩展VQ-VAEs为视频框架，将离散码本表示为Dirichlet分布连续变量，结合时空Transformer和拉普拉斯约束重建损失。

Result: 在盲修复、视频修复和着色任务中达到最先进性能。

Conclusion: 该方法为高质量图像先验适应视频修复提供了有效范式，并解决了闪烁问题。

Abstract: Video face restoration faces a critical challenge in maintaining temporal consistency while recovering fine facial details from degraded inputs. This paper presents a novel approach that extends Vector-Quantized Variational Autoencoders (VQ-VAEs), pretrained on static high-quality portraits, into a video restoration framework through variational latent space modeling. Our key innovation lies in reformulating discrete codebook representations as Dirichlet-distributed continuous variables, enabling probabilistic transitions between facial features across frames. A spatio-temporal Transformer architecture jointly models inter-frame dependencies and predicts latent distributions, while a Laplacian-constrained reconstruction loss combined with perceptual (LPIPS) regularization enhances both pixel accuracy and visual quality. Comprehensive evaluations on blind face restoration, video inpainting, and facial colorization tasks demonstrate state-of-the-art performance. This work establishes an effective paradigm for adapting intensive image priors, pretrained on high-quality images, to video restoration while addressing the critical challenge of flicker artifacts. The source code has been open-sourced and is available at https://github.com/fudan-generative-vision/DicFace.

</details>


### [31] [Zero-Shot Solving of Imaging Inverse Problems via Noise-Refined Likelihood Guided Diffusion Models](https://arxiv.org/abs/2506.13391)
*Zhen Wang,Hongyi Liu,Zhihui Wei*

Main category: cs.CV

TL;DR: 提出了一种零样本框架，通过似然引导的噪声细化机制处理多种成像逆问题，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常针对特定退化类型训练模型，限制了其泛化能力。

Method: 引入似然引导的噪声细化机制，结合DDIM采样策略提高推理效率。

Result: 在多种逆问题中表现优异，尤其在压缩感知中，即使采样率极低（5%）也能高质量重建。

Conclusion: 该方法为零样本成像逆问题提供了高效灵活的解决方案。

Abstract: Diffusion models have achieved remarkable success in imaging inverse problems owing to their powerful generative capabilities. However, existing approaches typically rely on models trained for specific degradation types, limiting their generalizability to various degradation scenarios. To address this limitation, we propose a zero-shot framework capable of handling various imaging inverse problems without model retraining. We introduce a likelihood-guided noise refinement mechanism that derives a closed-form approximation of the likelihood score, simplifying score estimation and avoiding expensive gradient computations. This estimated score is subsequently utilized to refine the model-predicted noise, thereby better aligning the restoration process with the generative framework of diffusion models. In addition, we integrate the Denoising Diffusion Implicit Models (DDIM) sampling strategy to further improve inference efficiency. The proposed mechanism can be applied to both optimization-based and sampling-based schemes, providing an effective and flexible zero-shot solution for imaging inverse problems. Extensive experiments demonstrate that our method achieves superior performance across multiple inverse problems, particularly in compressive sensing, delivering high-quality reconstructions even at an extremely low sampling rate (5%).

</details>


### [32] [Overcoming Occlusions in the Wild: A Multi-Task Age Head Approach to Age Estimation](https://arxiv.org/abs/2506.13445)
*Waqar Tanveer,Laura Fernández-Robles,Eduardo Fidalgo,Víctor González-Castro,Enrique Alegre*

Main category: cs.CV

TL;DR: 提出了一种结合GAN和Transformer的新方法，用于遮挡条件下的鲁棒年龄估计，性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 在无约束的真实场景中，遮挡会降低面部年龄估计的准确性，需要一种更鲁棒的方法。

Method: 使用SN-Patch GAN去除遮挡，结合ARCM和Swin Transformer增强特征表示，并引入MTAH进行多任务学习。

Result: 在FG-NET、UTKFace和MORPH数据集上，MAE分别为3.00、4.54和2.53年，优于现有技术。

Conclusion: 提出的方法在遮挡条件下显著提升了年龄估计的准确性和鲁棒性。

Abstract: Facial age estimation has achieved considerable success under controlled conditions. However, in unconstrained real-world scenarios, which are often referred to as 'in the wild', age estimation remains challenging, especially when faces are partially occluded, which may obscure their visibility. To address this limitation, we propose a new approach integrating generative adversarial networks (GANs) and transformer architectures to enable robust age estimation from occluded faces. We employ an SN-Patch GAN to effectively remove occlusions, while an Attentive Residual Convolution Module (ARCM), paired with a Swin Transformer, enhances feature representation. Additionally, we introduce a Multi-Task Age Head (MTAH) that combines regression and distribution learning, further improving age estimation under occlusion. Experimental results on the FG-NET, UTKFace, and MORPH datasets demonstrate that our proposed approach surpasses existing state-of-the-art techniques for occluded facial age estimation by achieving an MAE of $3.00$, $4.54$, and $2.53$ years, respectively.

</details>


### [33] [SA-LUT: Spatial Adaptive 4D Look-Up Table for Photorealistic Style Transfer](https://arxiv.org/abs/2506.13465)
*Zerui Gong,Zhonghua Wu,Qingyi Tao,Qinyue Li,Chen Change Loy*

Main category: cs.CV

TL;DR: SA-LUT是一种结合LUT效率和神经网络适应性的空间自适应4D查找表方法，用于实现逼真的风格迁移。


<details>
  <summary>Details</summary>
Motivation: 现有方法在风格保真度和内容完整性之间存在权衡，SA-LUT旨在填补这一空白。

Method: SA-LUT包括风格引导的4D LUT生成器和基于内容-风格交叉注意力的上下文生成器，实现空间自适应调整。

Result: SA-LUT在LPIPS评分上比3D LUT方法降低了66.7%，并在视频风格化中保持16 FPS的实时性能。

Conclusion: SA-LUT在逼真风格迁移中表现出色，同时提供了首个专门用于PST评估的基准PST50。

Abstract: Photorealistic style transfer (PST) enables real-world color grading by adapting reference image colors while preserving content structure. Existing methods mainly follow either approaches: generation-based methods that prioritize stylistic fidelity at the cost of content integrity and efficiency, or global color transformation methods such as LUT, which preserve structure but lack local adaptability. To bridge this gap, we propose Spatial Adaptive 4D Look-Up Table (SA-LUT), combining LUT efficiency with neural network adaptability. SA-LUT features: (1) a Style-guided 4D LUT Generator that extracts multi-scale features from the style image to predict a 4D LUT, and (2) a Context Generator using content-style cross-attention to produce a context map. This context map enables spatially-adaptive adjustments, allowing our 4D LUT to apply precise color transformations while preserving structural integrity. To establish a rigorous evaluation framework for photorealistic style transfer, we introduce PST50, the first benchmark specifically designed for PST assessment. Experiments demonstrate that SA-LUT substantially outperforms state-of-the-art methods, achieving a 66.7% reduction in LPIPS score compared to 3D LUT approaches, while maintaining real-time performance at 16 FPS for video stylization. Our code and benchmark are available at https://github.com/Ry3nG/SA-LUT

</details>


### [34] [ESRPCB: an Edge guided Super-Resolution model and Ensemble learning for tiny Printed Circuit Board Defect detection](https://arxiv.org/abs/2506.13476)
*Xiem HoangVan,Dang Bui Dinh,Thanh Nguyen Canh,Van-Truong Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种名为ESRPCB的新框架，结合边缘引导超分辨率和集成学习，以提升PCB缺陷检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 小尺寸PCB图像的低分辨率导致缺陷与噪声难以区分，亟需一种方法提升图像质量以准确检测缺陷。

Method: ESRPCB框架利用边缘信息引导EDSR模型，结合ResCat结构重建高分辨率图像，并通过集成学习进行多模态缺陷检测。

Result: 该方法能够有效区分微小缺陷与噪声，提升缺陷检测的准确性。

Conclusion: ESRPCB框架为PCB缺陷检测提供了一种高效且可靠的解决方案。

Abstract: Printed Circuit Boards (PCBs) are critical components in modern electronics, which require stringent quality control to ensure proper functionality. However, the detection of defects in small-scale PCBs images poses significant challenges as a result of the low resolution of the captured images, leading to potential confusion between defects and noise. To overcome these challenges, this paper proposes a novel framework, named ESRPCB (edgeguided super-resolution for PCBs defect detection), which combines edgeguided super-resolution with ensemble learning to enhance PCBs defect detection. The framework leverages the edge information to guide the EDSR (Enhanced Deep Super-Resolution) model with a novel ResCat (Residual Concatenation) structure, enabling it to reconstruct high-resolution images from small PCBs inputs. By incorporating edge features, the super-resolution process preserves critical structural details, ensuring that tiny defects remain distinguishable in the enhanced image. Following this, a multi-modal defect detection model employs ensemble learning to analyze the super-resolved

</details>


### [35] [Deep Diffusion Models and Unsupervised Hyperspectral Unmixing for Realistic Abundance Map Synthesis](https://arxiv.org/abs/2506.13484)
*Martina Pastorino,Michael Alibani,Nicola Acito,Gabriele Moser*

Main category: cs.CV

TL;DR: 提出了一种基于无监督深度学习的超光谱图像生成方法，结合盲线性解混和扩散模型，生成高真实感的丰度图。


<details>
  <summary>Details</summary>
Motivation: 解决超光谱分析中数据增强、算法基准测试和模型评估的需求，特别是在缺乏标记数据的情况下。

Method: 结合盲线性解混提取端元和丰度图，再通过扩散模型生成高真实感的合成丰度图。

Result: 在PRISMA空间任务的实际超光谱数据上验证了方法的有效性，生成的丰度图具有自然场景的空间和光谱特征。

Conclusion: 该方法为超光谱分析提供了一种无监督、适应性强的数据生成工具，适用于多样化的成像条件。

Abstract: This paper presents a novel methodology for generating realistic abundance maps from hyperspectral imagery using an unsupervised, deep-learning-driven approach. Our framework integrates blind linear hyperspectral unmixing with state-of-the-art diffusion models to enhance the realism and diversity of synthetic abundance maps. First, we apply blind unmixing to extract endmembers and abundance maps directly from raw hyperspectral data. These abundance maps then serve as inputs to a diffusion model, which acts as a generative engine to synthesize highly realistic spatial distributions. Diffusion models have recently revolutionized image synthesis by offering superior performance, flexibility, and stability, making them well-suited for high-dimensional spectral data. By leveraging this combination of physically interpretable unmixing and deep generative modeling, our approach enables the simulation of hyperspectral sensor outputs under diverse imaging conditions--critical for data augmentation, algorithm benchmarking, and model evaluation in hyperspectral analysis. Notably, our method is entirely unsupervised, ensuring adaptability to different datasets without the need for labeled training data. We validate our approach using real hyperspectral imagery from the PRISMA space mission for Earth observation, demonstrating its effectiveness in producing realistic synthetic abundance maps that capture the spatial and spectral characteristics of natural scenes.

</details>


### [36] [GeoSDF: Plane Geometry Diagram Synthesis via Signed Distance Field](https://arxiv.org/abs/2506.13492)
*Chengrui Zhang,Maizhen Ning,Zihao Zhou,Jie Sun,Kaizhu Huang,Qiufeng Wang*

Main category: cs.CV

TL;DR: GeoSDF框架利用SDF自动生成几何图，高效且准确，支持自验证，显著提升几何问题求解精度。


<details>
  <summary>Details</summary>
Motivation: 传统几何图生成方法计算成本高，学习型方法虽节省成本但精度和真实感不足。

Method: 基于SDF表示几何元素，构建约束函数并优化，通过渲染生成图，定义符号语言简化表示。

Result: 生成高中和IMO级几何图，真实且准确，几何问题求解精度达95%。

Conclusion: GeoSDF为几何图生成提供高效、准确、灵活的方法，适用于多种应用。

Abstract: Plane Geometry Diagram Synthesis has been a crucial task in computer graphics, with applications ranging from educational tools to AI-driven mathematical reasoning. Traditionally, we rely on computer tools (e.g., Matplotlib and GeoGebra) to manually generate precise diagrams, but it usually requires huge, complicated calculations cost. Recently, researchers start to work on learning-based methods (e.g., Stable Diffusion and GPT4) to automatically generate diagrams, saving operational cost but usually suffering from limited realism and insufficient accuracy. In this paper, we propose a novel framework GeoSDF to automatically generate diagrams efficiently and accurately with Signed Distance Field (SDF). Specifically, we first represent geometric elements in the SDF, then construct a series of constraint functions to represent geometric relationships, next we optimize such constraint functions to get an optimized field of both elements and constraints, finally by rendering the optimized field, we can obtain the synthesized diagram. In our GeoSDF, we define a symbolic language to easily represent geometric elements and those constraints, and our synthesized geometry diagrams can be self-verified in the SDF, ensuring both mathematical accuracy and visual plausibility. In experiments, our GeoSDF synthesized both normal high-school level and IMO-level geometry diagrams. Through both qualitative and quantitative analysis, we can see that synthesized diagrams are realistic and accurate, and our synthesizing process is simple and efficient. Furthermore, we obtain a very high accuracy of solving geometry problems (over 95\% while the current SOTA accuracy is around 75%) by leveraging our self-verification property. All of these demonstrate the advantage of GeoSDF, paving the way for more sophisticated, accurate, and flexible generation of geometric diagrams for a wide array of applications.

</details>


### [37] [Multiview Geometric Regularization of Gaussian Splatting for Accurate Radiance Fields](https://arxiv.org/abs/2506.13508)
*Jungeon Kim,Geonsoo Park,Seungyong Lee*

Main category: cs.CV

TL;DR: 提出了一种多视角几何正则化策略，结合MVS深度、RGB和法线约束，改进了3D高斯泼溅的几何精度和渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如2D高斯泼溅和高斯不透明度场）在颜色变化大的场景中几何重建不准确，需要改进。

Method: 通过多视角立体（MVS）深度、RGB和法线约束，结合高斯泼溅优化，提出中值深度损失和MVS引导的初始化。

Result: 实验表明，该方法在室内外场景中显著提升了几何精度和渲染质量。

Conclusion: 多视角几何正则化策略有效结合了MVS和高斯泼溅的优势，解决了现有方法的局限性。

Abstract: Recent methods, such as 2D Gaussian Splatting and Gaussian Opacity Fields, have aimed to address the geometric inaccuracies of 3D Gaussian Splatting while retaining its superior rendering quality. However, these approaches still struggle to reconstruct smooth and reliable geometry, particularly in scenes with significant color variation across viewpoints, due to their per-point appearance modeling and single-view optimization constraints. In this paper, we propose an effective multiview geometric regularization strategy that integrates multiview stereo (MVS) depth, RGB, and normal constraints into Gaussian Splatting initialization and optimization. Our key insight is the complementary relationship between MVS-derived depth points and Gaussian Splatting-optimized positions: MVS robustly estimates geometry in regions of high color variation through local patch-based matching and epipolar constraints, whereas Gaussian Splatting provides more reliable and less noisy depth estimates near object boundaries and regions with lower color variation. To leverage this insight, we introduce a median depth-based multiview relative depth loss with uncertainty estimation, effectively integrating MVS depth information into Gaussian Splatting optimization. We also propose an MVS-guided Gaussian Splatting initialization to avoid Gaussians falling into suboptimal positions. Extensive experiments validate that our approach successfully combines these strengths, enhancing both geometric accuracy and rendering quality across diverse indoor and outdoor scenes.

</details>


### [38] [Micro-macro Gaussian Splatting with Enhanced Scalability for Unconstrained Scene Reconstruction](https://arxiv.org/abs/2506.13516)
*Yihui Li,Chengxin Lv,Hongyu Yang,Di Huang*

Main category: cs.CV

TL;DR: SMW-GS是一种基于小波的高斯泼溅方法，通过多尺度分解和频率域优化，显著提升了无约束图像集合的3D场景重建质量与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决无约束图像集合中因外观变化导致的3D场景重建挑战。

Method: 提出SMW-GS方法，包括微宏观投影和小波采样，结合大规模场景优化策略。

Result: 在重建质量和可扩展性上显著优于现有方法，尤其适用于大规模城市环境。

Conclusion: SMW-GS通过多尺度分解和频率域优化，实现了高质量、可扩展的3D场景重建。

Abstract: Reconstructing 3D scenes from unconstrained image collections poses significant challenges due to variations in appearance. In this paper, we propose Scalable Micro-macro Wavelet-based Gaussian Splatting (SMW-GS), a novel method that enhances 3D reconstruction across diverse scales by decomposing scene representations into global, refined, and intrinsic components. SMW-GS incorporates the following innovations: Micro-macro Projection, which enables Gaussian points to sample multi-scale details with improved diversity; and Wavelet-based Sampling, which refines feature representations using frequency-domain information to better capture complex scene appearances. To achieve scalability, we further propose a large-scale scene promotion strategy, which optimally assigns camera views to scene partitions by maximizing their contributions to Gaussian points, achieving consistent and high-quality reconstructions even in expansive environments. Extensive experiments demonstrate that SMW-GS significantly outperforms existing methods in both reconstruction quality and scalability, particularly excelling in large-scale urban environments with challenging illumination variations. Project is available at https://github.com/Kidleyh/SMW-GS.

</details>


### [39] [Limited-Angle CBCT Reconstruction via Geometry-Integrated Cycle-domain Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2506.13545)
*Yuan Gao,Shaoyan Pan,Mingzhe Hu,Huiqiao Xie,Jill Remick,Chih-Wei Chang,Justin Roper,Zhen Tian,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 提出了一种基于双域扩散模型的有限角度CBCT重建方法（LA-GICD），显著减少扫描时间和剂量，同时保持高质量图像。


<details>
  <summary>Details</summary>
Motivation: CBCT在放疗中广泛应用，但慢速旋转导致运动伪影和剂量增加，限制了性能。需要一种在有限角度扫描下仍能高质量重建的方法。

Method: 提出LA-GICD框架，结合投影域和图像域的双扩散模型（DDPM），通过解析投影和反投影操作实现高质量重建。

Result: 在90度扫描下，平均绝对误差35.5 HU，SSIM 0.84，PSNR 29.8 dB，伪影减少，软组织清晰度提升。

Conclusion: LA-GICD为有限角度CBCT提供了一种实用解决方案，显著减少扫描时间和剂量，提升放疗中的图像质量。

Abstract: Cone-beam CT (CBCT) is widely used in clinical radiotherapy for image-guided treatment, improving setup accuracy, adaptive planning, and motion management. However, slow gantry rotation limits performance by introducing motion artifacts, blurring, and increased dose. This work aims to develop a clinically feasible method for reconstructing high-quality CBCT volumes from consecutive limited-angle acquisitions, addressing imaging challenges in time- or dose-constrained settings. We propose a limited-angle (LA) geometry-integrated cycle-domain (LA-GICD) framework for CBCT reconstruction, comprising two denoising diffusion probabilistic models (DDPMs) connected via analytic cone-beam forward and back projectors. A Projection-DDPM completes missing projections, followed by back-projection, and an Image-DDPM refines the volume. This dual-domain design leverages complementary priors from projection and image spaces to achieve high-quality reconstructions from limited-angle (<= 90 degrees) scans. Performance was evaluated against full-angle reconstruction. Four board-certified medical physicists conducted assessments. A total of 78 planning CTs in common CBCT geometries were used for training and evaluation. The method achieved a mean absolute error of 35.5 HU, SSIM of 0.84, and PSNR of 29.8 dB, with visibly reduced artifacts and improved soft-tissue clarity. LA-GICD's geometry-aware dual-domain learning, embedded in analytic forward/backward operators, enabled artifact-free, high-contrast reconstructions from a single 90-degree scan, reducing acquisition time and dose four-fold. LA-GICD improves limited-angle CBCT reconstruction with strong data fidelity and anatomical realism. It offers a practical solution for short-arc acquisitions, enhancing CBCT use in radiotherapy by providing clinically applicable images with reduced scan time and dose for more accurate, personalized treatments.

</details>


### [40] [A Comprehensive Survey on Video Scene Parsing:Advances, Challenges, and Prospects](https://arxiv.org/abs/2506.13552)
*Guohuan Xie,Syed Ariff Syed Hesham,Wenya Guo,Bing Li,Ming-Ming Cheng,Guolei Sun,Yun Liu*

Main category: cs.CV

TL;DR: 本文综述了视频场景解析（VSP）的最新进展，涵盖多种视觉任务，分析了从传统手工特征到现代深度学习方法的演变，并探讨了技术挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 视频场景解析在计算机视觉中至关重要，但现有研究分散且缺乏系统性总结，本文旨在填补这一空白。

Method: 系统回顾了VSP相关任务（如VSS、VIS、VPS等）的方法演变，从卷积网络到Transformer架构，并评估其时空上下文捕捉能力。

Result: 总结了现有方法的贡献与不足，比较了数据集和评估指标，提出了未来研究方向。

Conclusion: VSP领域仍有挑战，但新兴技术（如Transformer）和更全面的基准测试将推动其在实际应用中的发展。

Abstract: Video Scene Parsing (VSP) has emerged as a cornerstone in computer vision, facilitating the simultaneous segmentation, recognition, and tracking of diverse visual entities in dynamic scenes. In this survey, we present a holistic review of recent advances in VSP, covering a wide array of vision tasks, including Video Semantic Segmentation (VSS), Video Instance Segmentation (VIS), Video Panoptic Segmentation (VPS), as well as Video Tracking and Segmentation (VTS), and Open-Vocabulary Video Segmentation (OVVS). We systematically analyze the evolution from traditional hand-crafted features to modern deep learning paradigms -- spanning from fully convolutional networks to the latest transformer-based architectures -- and assess their effectiveness in capturing both local and global temporal contexts. Furthermore, our review critically discusses the technical challenges, ranging from maintaining temporal consistency to handling complex scene dynamics, and offers a comprehensive comparative study of datasets and evaluation metrics that have shaped current benchmarking standards. By distilling the key contributions and shortcomings of state-of-the-art methodologies, this survey highlights emerging trends and prospective research directions that promise to further elevate the robustness and adaptability of VSP in real-world applications.

</details>


### [41] [X-Scene: Large-Scale Driving Scene Generation with High Fidelity and Flexible Controllability](https://arxiv.org/abs/2506.13558)
*Yu Yang,Alan Liang,Jianbiao Mei,Yukai Ma,Yong Liu,Gim Hee Lee*

Main category: cs.CV

TL;DR: X-Scene是一个用于大规模驾驶场景生成的新框架，支持多粒度控制和几何与视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在自动驾驶中主要关注时间一致性生成，而大规模3D场景的空间一致性生成研究不足。

Method: X-Scene通过统一管道生成3D语义占据和多视图图像，并通过一致性感知的场景外推扩展场景。

Result: 实验表明，X-Scene显著提升了大规模驾驶场景生成的可控性和保真度。

Conclusion: X-Scene为自动驾驶数据生成和模拟提供了高效可控的解决方案。

Abstract: Diffusion models are advancing autonomous driving by enabling realistic data synthesis, predictive end-to-end planning, and closed-loop simulation, with a primary focus on temporally consistent generation. However, the generation of large-scale 3D scenes that require spatial coherence remains underexplored. In this paper, we propose X-Scene, a novel framework for large-scale driving scene generation that achieves both geometric intricacy and appearance fidelity, while offering flexible controllability. Specifically, X-Scene supports multi-granular control, including low-level conditions such as user-provided or text-driven layout for detailed scene composition and high-level semantic guidance such as user-intent and LLM-enriched text prompts for efficient customization. To enhance geometrical and visual fidelity, we introduce a unified pipeline that sequentially generates 3D semantic occupancy and the corresponding multiview images, while ensuring alignment between modalities. Additionally, we extend the generated local region into a large-scale scene through consistency-aware scene outpainting, which extrapolates new occupancy and images conditioned on the previously generated area, enhancing spatial continuity and preserving visual coherence. The resulting scenes are lifted into high-quality 3DGS representations, supporting diverse applications such as scene exploration. Comprehensive experiments demonstrate that X-Scene significantly advances controllability and fidelity for large-scale driving scene generation, empowering data generation and simulation for autonomous driving.

</details>


### [42] [Omni-AdaVideoRAG: Omni-Contextual Adaptive Retrieval-Augmented for Efficient Long Video Understanding](https://arxiv.org/abs/2506.13589)
*Zhucun Xue,Jiangning Zhang,Xurong Xie,Yuxuan Cai,Yong Liu,Xiangtai Li,Dacheng Tao*

Main category: cs.CV

TL;DR: AdaVideoRAG提出了一种动态调整检索粒度的框架，通过轻量级意图分类器优化长视频理解任务，解决了现有方法在简单查询和复杂任务中的效率与信息损失问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在处理长视频时因固定上下文窗口和弱长期依赖建模能力而表现不佳，现有检索增强生成（RAG）方法采用静态检索策略，导致效率低下和信息丢失。

Method: 提出AdaVideoRAG框架，通过轻量级意图分类器动态调整检索粒度，并利用全知识索引模块构建分层数据库（文本、视觉特征、语义图），实现任务资源最优分配。

Result: 实验表明，AdaVideoRAG在长视频理解任务中提高了效率和准确性，并能无缝集成到现有MLLMs中。

Conclusion: AdaVideoRAG为视频分析中的自适应检索建立了新范式，代码将开源。

Abstract: Multimodal Large Language Models (MLLMs) struggle with long videos due to fixed context windows and weak long-term dependency modeling. Existing Retrieval-Augmented Generation (RAG) methods for videos use static retrieval strategies, leading to inefficiencies for simple queries and information loss for complex tasks. To address this, we propose AdaVideoRAG, a novel framework that dynamically adapts retrieval granularity based on query complexity using a lightweight intent classifier. Our framework employs an Omni-Knowledge Indexing module to build hierarchical databases from text (captions, ASR, OCR), visual features, and semantic graphs, enabling optimal resource allocation across tasks. We also introduce the HiVU benchmark for comprehensive evaluation. Experiments demonstrate improved efficiency and accuracy for long-video understanding, with seamless integration into existing MLLMs. AdaVideoRAG establishes a new paradigm for adaptive retrieval in video analysis. Codes will be open-sourced at https://github.com/xzc-zju/AdaVideoRAG.

</details>


### [43] [UltraVideo: High-Quality UHD Video Dataset with Comprehensive Captions](https://arxiv.org/abs/2506.13691)
*Zhucun Xue,Jiangning Zhang,Teng Hu,Haoyang He,Yinan Chen,Yuxuan Cai,Yabiao Wang,Chengjie Wang,Yong Liu,Xiangtai Li,Dacheng Tao*

Main category: cs.CV

TL;DR: 论文提出高质量UHD-4K文本到视频数据集UltraVideo，并扩展UltraWan模型以生成高质量1K/4K视频。


<details>
  <summary>Details</summary>
Motivation: 现有公共数据集无法满足高质量视频生成研究需求，如电影级UHD视频和4K短视频内容。

Method: 设计四阶段自动化数据筛选流程：视频片段收集、统计过滤、模型净化、生成结构化字幕。

Result: 构建了UltraVideo数据集（含8K视频22.4%），并扩展UltraWan模型以生成高质量视频。

Conclusion: UltraVideo和UltraWan为UHD视频生成研究提供了重要支持。

Abstract: The quality of the video dataset (image quality, resolution, and fine-grained caption) greatly influences the performance of the video generation model. The growing demand for video applications sets higher requirements for high-quality video generation models. For example, the generation of movie-level Ultra-High Definition (UHD) videos and the creation of 4K short video content. However, the existing public datasets cannot support related research and applications. In this paper, we first propose a high-quality open-sourced UHD-4K (22.4\% of which are 8K) text-to-video dataset named UltraVideo, which contains a wide range of topics (more than 100 kinds), and each video has 9 structured captions with one summarized caption (average of 824 words). Specifically, we carefully design a highly automated curation process with four stages to obtain the final high-quality dataset: \textit{i)} collection of diverse and high-quality video clips. \textit{ii)} statistical data filtering. \textit{iii)} model-based data purification. \textit{iv)} generation of comprehensive, structured captions. In addition, we expand Wan to UltraWan-1K/-4K, which can natively generate high-quality 1K/4K videos with more consistent text controllability, demonstrating the effectiveness of our data curation.We believe that this work can make a significant contribution to future research on UHD video generation. UltraVideo dataset and UltraWan models are available at https://xzc-zju.github.io/projects/UltraVideo.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [44] [Dynamic Context-oriented Decomposition for Task-aware Low-rank Adaptation with Less Forgetting and Faster Convergence](https://arxiv.org/abs/2506.13187)
*Yibo Yang,Sihao Liu,Chuan Rao,Bang An,Tiancheng Shen,Philip H. S. Torr,Ming-Hsuan Yang,Bernard Ghanem*

Main category: cs.LG

TL;DR: 论文提出了一种名为CorDA的新型低秩适应方法，通过任务感知的方式初始化适配器，解决了传统方法因忽略数据上下文导致的性能不佳和知识遗忘问题。进一步优化的CorDA++通过动态策略提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统低秩适应方法不考虑数据上下文，导致微调性能不佳和知识遗忘。

Method: 提出上下文导向的奇异值分解（CorDA），通过任务感知初始化适配器，并开发动态策略优化（CorDA++）。

Result: CorDA++在知识保留模式（KPM）和指令预览模式（IPM）下均表现优异，优于LoRA和QLoRA等基线方法。

Conclusion: CorDA++显著提升了低秩适应的性能，并被集成到Hugging Face的PEFT库中。

Abstract: Conventional low-rank adaptation methods build adapters without considering data context, leading to sub-optimal fine-tuning performance and severe forgetting of inherent world knowledge. In this paper, we propose context-oriented decomposition adaptation (CorDA), a novel method that initializes adapters in a task-aware manner. Concretely, we develop context-oriented singular value decomposition, where we collect covariance matrices of input activations for each linear layer using sampled data from the target task, and apply SVD to the product of weight matrix and its corresponding covariance matrix. By doing so, the task-specific capability is compacted into the principal components. Thanks to the task awareness, our method enables two optional adaptation modes, knowledge-preserved mode (KPM) and instruction-previewed mode (IPM), providing flexibility to choose between freezing the principal components to preserve their associated knowledge or adapting them to better learn a new task. We further develop CorDA++ by deriving a metric that reflects the compactness of task-specific principal components, and then introducing dynamic covariance selection and dynamic rank allocation strategies based on the same metric. The two strategies provide each layer with the most representative covariance matrix and a proper rank allocation. Experimental results show that CorDA++ outperforms CorDA by a significant margin. CorDA++ in KPM not only achieves better fine-tuning performance than LoRA, but also mitigates the forgetting of pre-trained knowledge in both large language models and vision language models. For IPM, our method exhibits faster convergence, \emph{e.g.,} 4.5x speedup over QLoRA, and improves adaptation performance in various scenarios, outperforming strong baseline methods. Our method has been integrated into the PEFT library developed by Hugging Face.

</details>


### [45] [Flexible-length Text Infilling for Discrete Diffusion Models](https://arxiv.org/abs/2506.13579)
*Andrew Zhang,Anushka Sivakumar,Chiawei Tang,Chris Thomas*

Main category: cs.LG

TL;DR: DDOT是一种新的离散扩散模型，通过联合去噪标记值和位置，解决了现有模型无法灵活填充文本长度和位置的限制。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在文本生成中具有优势，但无法灵活填充文本长度和位置，限制了其应用。

Method: DDOT通过样本级最优传输耦合，联合去噪标记值和位置，动态调整填充段的位置和长度。

Result: DDOT在文本填充基准测试中表现优于基线模型，与最先进的非自回归模型性能相当，并显著提升了训练效率和灵活性。

Conclusion: DDOT是首个克服离散扩散模型灵活填充限制的方法，具有广泛的应用潜力。

Abstract: Discrete diffusion models are a new class of text generators that offer advantages such as bidirectional context use, parallelizable generation, and flexible prompting compared to autoregressive models. However, a critical limitation of discrete diffusion models is their inability to perform flexible-length or flexible-position text infilling without access to ground-truth positional data. We introduce \textbf{DDOT} (\textbf{D}iscrete \textbf{D}iffusion with \textbf{O}ptimal \textbf{T}ransport Position Coupling), the first discrete diffusion model to overcome this challenge. DDOT jointly denoises token values and token positions, employing a novel sample-level Optimal Transport (OT) coupling. This coupling preserves relative token ordering while dynamically adjusting the positions and length of infilled segments, a capability previously missing in text diffusion. Our method is orthogonal to existing discrete text diffusion methods and is compatible with various pretrained text denoisers. Extensive experiments on text infilling benchmarks such as One-Billion-Word and Yelp demonstrate that DDOT outperforms naive diffusion baselines. Furthermore, DDOT achieves performance on par with state-of-the-art non-autoregressive models and enables significant improvements in training efficiency and flexibility.

</details>


### [46] [VideoPDE: Unified Generative PDE Solving via Video Inpainting Diffusion Models](https://arxiv.org/abs/2506.13754)
*Edward Li,Zichen Wang,Jiahe Huang,Jeong Joon Park*

Main category: cs.LG

TL;DR: 提出了一种基于视频修复扩散变换器的统一框架，用于解决偏微分方程（PDEs），将前向和逆向问题统一为一个灵活的生成框架。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常针对特定问题设计策略，缺乏通用性。本文旨在提供一个统一的解决方案，适用于多种PDEs和问题设置。

Method: 将PDE求解重新定义为广义修复问题，设计基于变换器的架构，利用像素空间视频扩散模型进行精细修复和条件推断。

Result: 实验表明，该方法在多种PDEs和问题设置中表现优异，优于现有基线。

Conclusion: 该方法为PDE求解提供了一个准确且通用的解决方案，具有广泛的应用潜力。

Abstract: We present a unified framework for solving partial differential equations (PDEs) using video-inpainting diffusion transformer models. Unlike existing methods that devise specialized strategies for either forward or inverse problems under full or partial observation, our approach unifies these tasks under a single, flexible generative framework. Specifically, we recast PDE-solving as a generalized inpainting problem, e.g., treating forward prediction as inferring missing spatiotemporal information of future states from initial conditions. To this end, we design a transformer-based architecture that conditions on arbitrary patterns of known data to infer missing values across time and space. Our method proposes pixel-space video diffusion models for fine-grained, high-fidelity inpainting and conditioning, while enhancing computational efficiency through hierarchical modeling. Extensive experiments show that our video inpainting-based diffusion model offers an accurate and versatile solution across a wide range of PDEs and problem setups, outperforming state-of-the-art baselines.

</details>


### [47] [Diagnosing and Improving Diffusion Models by Estimating the Optimal Loss Value](https://arxiv.org/abs/2506.13763)
*Yixian Xu,Shengjie Luo,Liwei Wang,Di He,Chang Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种估计扩散模型最优损失值的方法，用于诊断和改进模型性能，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的损失值无法直接反映数据拟合质量，因为其最优值通常未知，这导致难以区分模型容量不足与最优损失较大的情况。

Method: 推导了扩散模型最优损失的闭式解，并开发了有效的估计器，包括适用于大数据集的可扩展随机变体。

Result: 通过最优损失工具，改进了主流扩散模型的训练质量诊断，并开发了更高效的训练计划。实验表明，减去最优损失后能更好地展示幂律关系。

Conclusion: 估计最优损失为扩散模型的诊断和改进提供了新工具，同时为研究扩散模型的缩放定律提供了更合理的设置。

Abstract: Diffusion models have achieved remarkable success in generative modeling. Despite more stable training, the loss of diffusion models is not indicative of absolute data-fitting quality, since its optimal value is typically not zero but unknown, leading to confusion between large optimal loss and insufficient model capacity. In this work, we advocate the need to estimate the optimal loss value for diagnosing and improving diffusion models. We first derive the optimal loss in closed form under a unified formulation of diffusion models, and develop effective estimators for it, including a stochastic variant scalable to large datasets with proper control of variance and bias. With this tool, we unlock the inherent metric for diagnosing the training quality of mainstream diffusion model variants, and develop a more performant training schedule based on the optimal loss. Moreover, using models with 120M to 1.5B parameters, we find that the power law is better demonstrated after subtracting the optimal loss from the actual training loss, suggesting a more principled setting for investigating the scaling law for diffusion models.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [48] [ICME 2025 Grand Challenge on Video Super-Resolution for Video Conferencing](https://arxiv.org/abs/2506.12269)
*Babak Naderi,Ross Cutler,Juhee Cho,Nabakumar Khongbantabam,Dejan Ivkovic*

Main category: eess.IV

TL;DR: 该论文探讨了视频超分辨率（VSR）在会议场景中的应用，重点研究了低延迟条件下通过因果模型提升视频质量的方法，并开源了一个新的屏幕内容数据集。


<details>
  <summary>Details</summary>
Motivation: 视频超分辨率在会议场景中具有重要意义，但现有方法在低延迟条件下的表现仍有待提升。本文旨在通过因果模型解决这一问题。

Method: 研究采用了局部、单向、双向传播等VSR方法，并在H.265编码的低分辨率视频上进行了实验。挑战赛分为三个赛道，分别针对不同类型视频。

Result: 挑战赛通过主观测试评估了提交的模型，并开源了一个新的屏幕内容数据集，为VSR任务提供了新的资源。

Conclusion: 该研究为视频超分辨率在会议场景中的应用提供了新的方法和数据集，推动了低延迟条件下VSR技术的发展。

Abstract: Super-Resolution (SR) is a critical task in computer vision, focusing on reconstructing high-resolution (HR) images from low-resolution (LR) inputs. The field has seen significant progress through various challenges, particularly in single-image SR. Video Super-Resolution (VSR) extends this to the temporal domain, aiming to enhance video quality using methods like local, uni-, bi-directional propagation, or traditional upscaling followed by restoration. This challenge addresses VSR for conferencing, where LR videos are encoded with H.265 at fixed QPs. The goal is to upscale videos by a specific factor, providing HR outputs with enhanced perceptual quality under a low-delay scenario using causal models. The challenge included three tracks: general-purpose videos, talking head videos, and screen content videos, with separate datasets provided by the organizers for training, validation, and testing. We open-sourced a new screen content dataset for the SR task in this challenge. Submissions were evaluated through subjective tests using a crowdsourced implementation of the ITU-T Rec P.910.

</details>


### [49] [Efficient Star Distillation Attention Network for Lightweight Image Super-Resolution](https://arxiv.org/abs/2506.12475)
*Fangwei Hao,Ji Du,Desheng Kong,Jiesheng Wu,Jing Xu,Ping Li*

Main category: eess.IV

TL;DR: 论文提出了一种Star Distillation Module (SDM)和Multi-shape Multi-scale Large Kernel Attention (MM-LKA)模块，结合为RSDAM，构建了高效的SDAN网络，显著提升了轻量级单图像超分辨率（SISR）的性能。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级SISR方法在信息蒸馏和高维非线性特征空间映射上表现不足，且LKA模块在捕捉多形状多尺度信息和长程依赖时计算负担大。

Method: 提出SDM增强高维非线性特征空间的信息蒸馏，设计MM-LKA模块高效捕捉长程依赖，结合为RSDAM模块，构建SDAN网络。

Result: 实验表明，SDAN在低模型复杂度下，定量和视觉上均优于其他轻量级SISR方法。

Conclusion: SDAN通过SDM和MM-LKA的结合，显著提升了轻量级SISR的性能和重建效率。

Abstract: In recent years, the performance of lightweight Single-Image Super-Resolution (SISR) has been improved significantly with the application of Convolutional Neural Networks (CNNs) and Large Kernel Attention (LKA). However, existing information distillation modules for lightweight SISR struggle to map inputs into High-Dimensional Non-Linear (HDNL) feature spaces, limiting their representation learning. And their LKA modules possess restricted ability to capture the multi-shape multi-scale information for long-range dependencies while encountering a quadratic increase in the computational burden with increasing convolutional kernel size of its depth-wise convolutional layer. To address these issues, we firstly propose a Star Distillation Module (SDM) to enhance the discriminative representation learning via information distillation in the HDNL feature spaces. Besides, we present a Multi-shape Multi-scale Large Kernel Attention (MM-LKA) module to learn representative long-range dependencies while incurring low computational and memory footprints, leading to improving the performance of CNN-based self-attention significantly. Integrating SDM and MM-LKA, we develop a Residual Star Distillation Attention Module (RSDAM) and take it as the building block of the proposed efficient Star Distillation Attention Network (SDAN) which possesses high reconstruction efficiency to recover a higher-quality image from the corresponding low-resolution (LR) counterpart. When compared with other lightweight state-of-the-art SISR methods, extensive experiments show that our SDAN with low model complexity yields superior performance quantitatively and visually.

</details>


### [50] [Zero-shot denoising via neural compression: Theoretical and algorithmic framework](https://arxiv.org/abs/2506.12693)
*Ali Zafari,Xi Chen,Shirin Jalali*

Main category: eess.IV

TL;DR: ZS-NCD是一种基于神经压缩的零样本去噪框架，无需训练样本或干净参考图像，适用于医学或生物等专业领域。


<details>
  <summary>Details</summary>
Motivation: 解决零样本去噪问题，特别是在医学或生物等专业领域中缺乏训练数据的情况。

Method: 利用神经压缩网络作为未训练模型，直接在单个噪声图像的提取块上进行优化，并通过聚合重叠块的输出来重建图像。

Result: ZS-NCD在零样本去噪中达到最先进性能，适用于高斯和泊松噪声，并能泛化到自然和非自然图像。

Conclusion: ZS-NCD通过内置熵约束避免过拟合，无需手动正则化或提前停止，同时提供了压缩去噪的理论基础。

Abstract: Zero-shot denoising aims to denoise observations without access to training samples or clean reference images. This setting is particularly relevant in practical imaging scenarios involving specialized domains such as medical imaging or biology. In this work, we propose the Zero-Shot Neural Compression Denoiser (ZS-NCD), a novel denoising framework based on neural compression. ZS-NCD treats a neural compression network as an untrained model, optimized directly on patches extracted from a single noisy image. The final reconstruction is then obtained by aggregating the outputs of the trained model over overlapping patches. Thanks to the built-in entropy constraints of compression architectures, our method naturally avoids overfitting and does not require manual regularization or early stopping. Through extensive experiments, we show that ZS-NCD achieves state-of-the-art performance among zero-shot denoisers for both Gaussian and Poisson noise, and generalizes well to both natural and non-natural images. Additionally, we provide new finite-sample theoretical results that characterize upper bounds on the achievable reconstruction error of general maximum-likelihood compression-based denoisers. These results further establish the theoretical foundations of compression-based denoising. Our code is available at: github.com/Computational-Imaging-RU/ZS-NCDenoiser.

</details>


### [51] [GM-LDM: Latent Diffusion Model for Brain Biomarker Identification through Functional Data-Driven Gray Matter Synthesis](https://arxiv.org/abs/2506.12719)
*Hu Xu,Yang Jingling,Jia Sihan,Bi Yuda,Calhoun Vince*

Main category: eess.IV

TL;DR: GM-LDM是一种基于潜在扩散模型（LDM）的新框架，用于提升MRI生成任务的效率和精度，支持个性化脑成像和功能到结构信息的转换。


<details>
  <summary>Details</summary>
Motivation: 深度学习生成模型在医学影像中潜力巨大，但需要更高效的MRI生成方法以支持个性化医疗和疾病研究。

Method: 结合3D自动编码器和预训练ViT编码器-解码器，利用KL散度损失实现统计一致性，并灵活整合条件数据（如FNC）。

Result: GM-LDM能够高效生成MRI图像，支持个性化脑成像和疾病（如精神分裂症）的生物标志物识别。

Conclusion: GM-LDM为MRI生成和功能-结构信息转换提供了高效且灵活的解决方案。

Abstract: Generative models based on deep learning have shown significant potential in medical imaging, particularly for modality transformation and multimodal fusion in MRI-based brain imaging. This study introduces GM-LDM, a novel framework that leverages the latent diffusion model (LDM) to enhance the efficiency and precision of MRI generation tasks. GM-LDM integrates a 3D autoencoder, pre-trained on the large-scale ABCD MRI dataset, achieving statistical consistency through KL divergence loss. We employ a Vision Transformer (ViT)-based encoder-decoder as the denoising network to optimize generation quality. The framework flexibly incorporates conditional data, such as functional network connectivity (FNC) data, enabling personalized brain imaging, biomarker identification, and functional-to-structural information translation for brain diseases like schizophrenia.

</details>


### [52] [PRO: Projection Domain Synthesis for CT Imaging](https://arxiv.org/abs/2506.13443)
*Kang Chen,Bin Huang,Xuebin Yang,Junyan Zhang,Qiegen Liu*

Main category: eess.IV

TL;DR: PRO是一种新颖的框架，首次在投影域中使用潜在扩散模型合成高质量CT图像，通过解剖学文本提示实现可控合成，显著提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决CT图像合成中标注数据有限和成像复杂性高的挑战。

Method: 在投影域中利用潜在扩散模型学习结构表示，结合解剖学文本提示进行可控合成。

Result: 实验表明，合成数据显著提升了低剂量和稀疏视图重建等下游任务的性能。

Conclusion: PRO展示了投影域合成在数据增强和CT成像中的潜力和可扩展性。

Abstract: Synthesizing high quality CT images remains a signifi-cant challenge due to the limited availability of annotat-ed data and the complex nature of CT imaging. In this work, we present PRO, a novel framework that, to the best of our knowledge, is the first to perform CT image synthesis in the projection domain using latent diffusion models. Unlike previous approaches that operate in the image domain, PRO learns rich structural representa-tions from raw projection data and leverages anatomi-cal text prompts for controllable synthesis. This projec-tion domain strategy enables more faithful modeling of underlying imaging physics and anatomical structures. Moreover, PRO functions as a foundation model, capa-ble of generalizing across diverse downstream tasks by adjusting its generative behavior via prompt inputs. Experimental results demonstrated that incorporating our synthesized data significantly improves perfor-mance across multiple downstream tasks, including low-dose and sparse-view reconstruction, even with limited training data. These findings underscore the versatility and scalability of PRO in data generation for various CT applications. These results highlight the potential of projection domain synthesis as a powerful tool for data augmentation and robust CT imaging. Our source code is publicly available at: https://github.com/yqx7150/PRO.

</details>


### [53] [MultiViT2: A Data-augmented Multimodal Neuroimaging Prediction Framework via Latent Diffusion Model](https://arxiv.org/abs/2506.13667)
*Bi Yuda,Jia Sihan,Gao Yutong,Abrol Anees,Fu Zening,Calhoun Vince*

Main category: eess.IV

TL;DR: 本文提出了一种基于多模态医学影像的深度学习模型MultiViT2，结合预训练基础模型和视觉Transformer，并通过潜在扩散模型增强数据，显著提升了精神分裂症分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 多模态医学影像（如结构和功能神经影像）能提供互补信息，但现有方法在预测性能和泛化能力上仍有不足。

Method: 提出MultiViT2模型，结合预训练基础模型和视觉Transformer，并开发基于潜在扩散模型的数据增强模块。

Result: MultiViT2在精神分裂症分类中显著优于第一代模型，并表现出良好的可扩展性和移植性。

Conclusion: MultiViT2通过多模态数据融合和增强，显著提升了预测性能，为医学影像分析提供了新思路。

Abstract: Multimodal medical imaging integrates diverse data types, such as structural and functional neuroimaging, to provide complementary insights that enhance deep learning predictions and improve outcomes. This study focuses on a neuroimaging prediction framework based on both structural and functional neuroimaging data. We propose a next-generation prediction model, \textbf{MultiViT2}, which combines a pretrained representative learning base model with a vision transformer backbone for prediction output. Additionally, we developed a data augmentation module based on the latent diffusion model that enriches input data by generating augmented neuroimaging samples, thereby enhancing predictive performance through reduced overfitting and improved generalizability. We show that MultiViT2 significantly outperforms the first-generation model in schizophrenia classification accuracy and demonstrates strong scalability and portability.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [54] [Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model](https://arxiv.org/abs/2506.13642)
*Shaolei Zhang,Shoutao Guo,Qingkai Fang,Yan Zhou,Yang Feng*

Main category: cs.AI

TL;DR: Stream-Omni是一种高效的多模态对齐模型，通过序列维度和层维度映射实现文本、视觉和语音的灵活交互。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型（LMMs）通常依赖大规模数据学习模态对齐，本文旨在更高效地建模模态关系。

Method: Stream-Omni基于LLM主干，通过序列维度对齐视觉与文本，通过CTC层维度映射对齐语音与文本。

Result: 实验表明Stream-Omni在视觉理解、语音交互等任务中表现优异，且能提供中间文本输出。

Conclusion: Stream-Omni实现了高效的多模态对齐，支持灵活交互并减少数据需求。

Abstract: The emergence of GPT-4o-like large multimodal models (LMMs) has raised the exploration of integrating text, vision, and speech modalities to support more flexible multimodal interaction. Existing LMMs typically concatenate representation of modalities along the sequence dimension and feed them into a large language model (LLM) backbone. While sequence-dimension concatenation is straightforward for modality integration, it often relies heavily on large-scale data to learn modality alignments. In this paper, we aim to model the relationships between modalities more purposefully, thereby achieving more efficient and flexible modality alignments. To this end, we propose Stream-Omni, a large language-vision-speech model with efficient modality alignments, which can simultaneously support interactions under various modality combinations. Stream-Omni employs LLM as the backbone and aligns the vision and speech to the text based on their relationships. For vision that is semantically complementary to text, Stream-Omni uses sequence-dimension concatenation to achieve vision-text alignment. For speech that is semantically consistent with text, Stream-Omni introduces a CTC-based layer-dimension mapping to achieve speech-text alignment. In this way, Stream-Omni can achieve modality alignments with less data (especially speech), enabling the transfer of text capabilities to other modalities. Experiments on various benchmarks demonstrate that Stream-Omni achieves strong performance on visual understanding, speech interaction, and vision-grounded speech interaction tasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously provide intermediate text outputs (such as ASR transcriptions and model responses) during speech interaction, offering users a comprehensive multimodal experience.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [55] [Exploiting the Exact Denoising Posterior Score in Training-Free Guidance of Diffusion Models](https://arxiv.org/abs/2506.13614)
*Gregory Bellchambers*

Main category: stat.ML

TL;DR: 本文提出了一种新的精确后验评分表达式，用于纯去噪任务，并通过动态调整步长来最小化误差，适用于多种逆问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在条件采样中的应用引发了兴趣，但现有方法（如DPS）难以直接近似后验评分函数。本文旨在解决这一问题。

Method: 提出了一种新的精确后验评分表达式，动态计算步长以最小化误差，并验证其在多种逆问题中的适用性。

Result: 该方法在去噪、着色、随机修复和超分辨率等任务中表现优异，且采样步数少于DPS。

Conclusion: 尽管方法简单，但能与最先进技术竞争，并减少采样时间。

Abstract: The success of diffusion models has driven interest in performing conditional sampling via training-free guidance of the denoising process to solve image restoration and other inverse problems. A popular class of methods, based on Diffusion Posterior Sampling (DPS), attempts to approximate the intractable posterior score function directly. In this work, we present a novel expression for the exact posterior score for purely denoising tasks that is tractable in terms of the unconditional score function. We leverage this result to analyze the time-dependent error in the DPS score for denoising tasks and compute step sizes on the fly to minimize the error at each time step. We demonstrate that these step sizes are transferable to related inverse problems such as colorization, random inpainting, and super resolution. Despite its simplicity, this approach is competitive with state-of-the-art techniques and enables sampling with fewer time steps than DPS.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [56] [SPLATART: Articulated Gaussian Splatting with Estimated Object Structure](https://arxiv.org/abs/2506.12184)
*Stanley Lewis,Vishal Chandra,Tom Gao,Odest Chadwicke Jenkins*

Main category: cs.RO

TL;DR: SPLATART是一种从带姿态的图像中学习高斯溅射表示的方法，用于解决复杂铰接物体的表示问题，支持更深层次的运动树结构。


<details>
  <summary>Details</summary>
Motivation: 铰接物体（如钳子、夹子或橱柜）的表示不仅需要几何和颜色信息，还需要部分分离、连接性和关节参数化。随着自由度增加，学习这些表示变得更加困难。

Method: SPLATART通过分离部分分割任务和关节估计任务，从带姿态的图像中学习高斯溅射表示，支持更深层次的运动树结构。

Result: 在合成Paris数据集上展示了SPLATART的效果，并在真实物体上提供了稀疏分割监督的定性结果，还展示了在更深运动树结构（如串联机械臂）上的应用。

Conclusion: SPLATART为解决复杂铰接物体的表示问题提供了一种有效方法，尤其适用于运动树结构较深的场景。

Abstract: Representing articulated objects remains a difficult problem within the field of robotics. Objects such as pliers, clamps, or cabinets require representations that capture not only geometry and color information, but also part seperation, connectivity, and joint parametrization. Furthermore, learning these representations becomes even more difficult with each additional degree of freedom. Complex articulated objects such as robot arms may have seven or more degrees of freedom, and the depth of their kinematic tree may be notably greater than the tools, drawers, and cabinets that are the typical subjects of articulated object research. To address these concerns, we introduce SPLATART - a pipeline for learning Gaussian splat representations of articulated objects from posed images, of which a subset contains image space part segmentations. SPLATART disentangles the part separation task from the articulation estimation task, allowing for post-facto determination of joint estimation and representation of articulated objects with deeper kinematic trees than previously exhibited. In this work, we present data on the SPLATART pipeline as applied to the syntheic Paris dataset objects, and qualitative results on a real-world object under spare segmentation supervision. We additionally present on articulated serial chain manipulators to demonstrate usage on deeper kinematic tree structures.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [57] [Restoring Gaussian Blurred Face Images for Deanonymization Attacks](https://arxiv.org/abs/2506.12344)
*Haoyu Zhai,Shuo Wang,Pirouz Naghavi,Qingying Hao,Gang Wang*

Main category: cs.CR

TL;DR: 论文提出了一种名为Revelio的去模糊方法，用于恢复高斯模糊的人脸图像，并验证其在高模糊设置下的有效性。


<details>
  <summary>Details</summary>
Motivation: 探讨高斯模糊处理后的人脸图像能否被有效恢复并用于重新识别，尤其是在高模糊设置下。

Method: 结合生成模型的记忆效应和条件扩散模型进行初步恢复，再通过身份检索模型增强保真度。

Result: Revelio在高模糊设置下表现优异，重新识别准确率达95.9%，优于现有方法。

Conclusion: 高斯模糊不适用于人脸匿名化，未来需探索更安全的替代方案。

Abstract: Gaussian blur is widely used to blur human faces in sensitive photos before the photos are posted on the Internet. However, it is unclear to what extent the blurred faces can be restored and used to re-identify the person, especially under a high-blurring setting. In this paper, we explore this question by developing a deblurring method called Revelio. The key intuition is to leverage a generative model's memorization effect and approximate the inverse function of Gaussian blur for face restoration. Compared with existing methods, we design the deblurring process to be identity-preserving. It uses a conditional Diffusion model for preliminary face restoration and then uses an identity retrieval model to retrieve related images to further enhance fidelity. We evaluate Revelio with large public face image datasets and show that it can effectively restore blurred faces, especially under a high-blurring setting. It has a re-identification accuracy of 95.9%, outperforming existing solutions. The result suggests that Gaussian blur should not be used for face anonymization purposes. We also demonstrate the robustness of this method against mismatched Gaussian kernel sizes and functions, and test preliminary countermeasures and adaptive attacks to inspire future work.

</details>


### [58] [InverTune: Removing Backdoors from Multimodal Contrastive Learning Models via Trigger Inversion and Activation Tuning](https://arxiv.org/abs/2506.12411)
*Mengyuan Sun,Yu Li,Yuchen Liu,Bo Du,Yunjie Ge*

Main category: cs.CR

TL;DR: InverTune是一种针对多模态模型的后门防御框架，无需攻击者知识或中毒数据集，通过对抗模拟、梯度反演和聚类微调，显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 多模态对比学习模型（如CLIP）易受后门攻击，现有防御方法因假设过强或数据需求高而不实用。

Method: InverTune通过对抗模拟暴露攻击特征，梯度反演重建潜在触发器，聚类微调消除后门功能。

Result: 实验显示，InverTune将攻击成功率降低97.87%，仅损失3.07%的干净数据准确率。

Conclusion: InverTune为多模态系统安全提供了新范式，在不影响性能的情况下提升安全性。

Abstract: Multimodal contrastive learning models like CLIP have demonstrated remarkable vision-language alignment capabilities, yet their vulnerability to backdoor attacks poses critical security risks. Attackers can implant latent triggers that persist through downstream tasks, enabling malicious control of model behavior upon trigger presentation. Despite great success in recent defense mechanisms, they remain impractical due to strong assumptions about attacker knowledge or excessive clean data requirements. In this paper, we introduce InverTune, the first backdoor defense framework for multimodal models under minimal attacker assumptions, requiring neither prior knowledge of attack targets nor access to the poisoned dataset. Unlike existing defense methods that rely on the same dataset used in the poisoning stage, InverTune effectively identifies and removes backdoor artifacts through three key components, achieving robust protection against backdoor attacks. Specifically, InverTune first exposes attack signatures through adversarial simulation, probabilistically identifying the target label by analyzing model response patterns. Building on this, we develop a gradient inversion technique to reconstruct latent triggers through activation pattern analysis. Finally, a clustering-guided fine-tuning strategy is employed to erase the backdoor function with only a small amount of arbitrary clean data, while preserving the original model capabilities. Experimental results show that InverTune reduces the average attack success rate (ASR) by 97.87% against the state-of-the-art (SOTA) attacks while limiting clean accuracy (CA) degradation to just 3.07%. This work establishes a new paradigm for securing multimodal systems, advancing security in foundation model deployment without compromising performance.

</details>
