<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 24]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Mixture of Contexts for Long Video Generation](https://arxiv.org/abs/2508.21058)
*Shengqu Cai,Ceyuan Yang,Lvmin Zhang,Yuwei Guo,Junfei Xiao,Ziyan Yang,Yinghao Xu,Zhenheng Yang,Alan Yuille,Leonidas Guibas,Maneesh Agrawala,Lu Jiang,Gordon Wetzstein*

Main category: cs.GR

TL;DR: 通过混合上下文（MoC）注意力机制，解决长视频生成中的长上下文记忆问题，实现超过分钟的一致性生成


<details>
  <summary>Details</summary>
Motivation: 长视频生成本质上是长上下文记忆问题，但相关注意力机制的平方计算成本使得内存和计算无法扩展到长序列

Method: 提出可学习的稀疏注意力路由模块MoC，每个查询动态选择少量信息块和必需锚点进行关注，采用因果路由防止循环闭合

Result: 模型能够在扩展数据和逐渐稀疏化路由的过程中，将计算资源分配给显著历史信息，保持身份、动作和场景在分钟级别内容中的一致性

Conclusion: MoC机制通过信息检索实现了近线性扩展效率，支持实用的训练和合成，在分钟级别尺度上实现了记忆和一致性的出现

Abstract: Long video generation is fundamentally a long context memory problem: models must retain and retrieve salient events across a long range without collapsing or drifting. However, scaling diffusion transformers to generate long-context videos is fundamentally limited by the quadratic cost of self-attention, which makes memory and computation intractable and difficult to optimize for long sequences. We recast long-context video generation as an internal information retrieval task and propose a simple, learnable sparse attention routing module, Mixture of Contexts (MoC), as an effective long-term memory retrieval engine. In MoC, each query dynamically selects a few informative chunks plus mandatory anchors (caption, local windows) to attend to, with causal routing that prevents loop closures. As we scale the data and gradually sparsify the routing, the model allocates compute to salient history, preserving identities, actions, and scenes over minutes of content. Efficiency follows as a byproduct of retrieval (near-linear scaling), which enables practical training and synthesis, and the emergence of memory and consistency at the scale of minutes.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By Value Sign Flip](https://arxiv.org/abs/2508.10931)
*Wenqi Guo,Shan Du*

Main category: cs.CV

TL;DR: VSF是一种简单高效的负向提示引导方法，通过翻转负向提示的注意力值符号来动态抑制不需要的内容，在少步扩散和流匹配模型中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的负向提示引导方法如CFG、NASA和NAG在少步生成模型中效果有限，需要更高效的方法来更好地抑制不需要的内容。

Method: VSF通过动态翻转负向提示的注意力值符号来抑制不需要的内容，计算开销小，兼容MMDiT和交叉注意力架构。

Result: 实验表明VSF在复杂提示对数据集上显著优于现有方法，在少步和非少步模型中都能更好地遵循负向提示，同时保持图像质量。

Conclusion: VSF是一种简单有效的负向提示引导方法，在图像和视频生成任务中都表现出色，代码已开源。

Abstract: We introduce Value Sign Flip (VSF), a simple and efficient method for incorporating negative prompt guidance in few-step diffusion and flow-matching image generation models. Unlike existing approaches such as classifier-free guidance (CFG), NASA, and NAG, VSF dynamically suppresses undesired content by flipping the sign of attention values from negative prompts. Our method requires only small computational overhead and integrates effectively with MMDiT-style architectures such as Stable Diffusion 3.5 Turbo, as well as cross-attention-based models like Wan. We validate VSF on challenging datasets with complex prompt pairs and demonstrate superior performance in both static image and video generation tasks. Experimental results show that VSF significantly improves negative prompt adherence compared to prior methods in few-step models, and even CFG in non-few-step models, while maintaining competitive image quality. Code and ComfyUI node are available in https://github.com/weathon/VSF/tree/main.

</details>


### [3] [SDiFL: Stable Diffusion-Driven Framework for Image Forgery Localization](https://arxiv.org/abs/2508.20182)
*Yang Su,Shunquan Tan,Jiwu Huang*

Main category: cs.CV

TL;DR: 基于Stable Diffusion多模态大模型，提出了一种无需大量标注数据的图像伪造定位新方法，通过将伪造残差作为显式模态融入SD3的潜在空间，显著提升了伪造检测性能


<details>
  <summary>Details</summary>
Motivation: 现有图像伪造定位方法严重依赖人工标注数据，难以跟上多模态大模型驱动的图像操纵技术发展速度，需要更高效准确的解决方案

Method: 利用Stable Diffusion V3的多模态架构，将图像伪造残差（通过高通滤波器提取的高频信号）作为显式模态融入潜在空间训练，同时保留SD3提取的丰富语义信息

Result: 在广泛使用的基准数据集上比当前最先进方法性能提升高达12%，在训练时未见过的真实文档伪造和自然场景伪造图像上也表现出强大性能

Conclusion: 该方法成功将SD的多模态生成和感知能力整合到取证框架中，为图像伪造定位提供了更高效准确的解决方案，具有很好的泛化能力

Abstract: Driven by the new generation of multi-modal large models, such as Stable Diffusion (SD), image manipulation technologies have advanced rapidly, posing significant challenges to image forensics. However, existing image forgery localization methods, which heavily rely on labor-intensive and costly annotated data, are struggling to keep pace with these emerging image manipulation technologies. To address these challenges, we are the first to integrate both image generation and powerful perceptual capabilities of SD into an image forensic framework, enabling more efficient and accurate forgery localization. First, we theoretically show that the multi-modal architecture of SD can be conditioned on forgery-related information, enabling the model to inherently output forgery localization results. Then, building on this foundation, we specifically leverage the multimodal framework of Stable DiffusionV3 (SD3) to enhance forgery localization performance.We leverage the multi-modal processing capabilities of SD3 in the latent space by treating image forgery residuals -- high-frequency signals extracted using specific highpass filters -- as an explicit modality. This modality is fused into the latent space during training to enhance forgery localization performance. Notably, our method fully preserves the latent features extracted by SD3, thereby retaining the rich semantic information of the input image. Experimental results show that our framework achieves up to 12% improvements in performance on widely used benchmarking datasets compared to current state-of-the-art image forgery localization models. Encouragingly, the model demonstrates strong performance on forensic tasks involving real-world document forgery images and natural scene forging images, even when such data were entirely unseen during training.

</details>


### [4] [Audio-Guided Visual Editing with Complex Multi-Modal Prompts](https://arxiv.org/abs/2508.20379)
*Hyeonyu Kim,Seokhoon Jeong,Seonghee Han,Chanhyuk Choi,Taehwan Kim*

Main category: cs.CV

TL;DR: 一种新的音频导向视觉编辑框架，通过多模态编码器和噪声分支技术，在不需额外训练的情况下处理复杂的多模态编辑任务。


<details>
  <summary>Details</summary>
Motivation: 现有的文本导向编辑方法在复杂场景中无法充分描述，需要额外的非文本编辑提示来提升编辑能力。

Method: 利用预训练的多模态编码器，通过消除音频编码空间与提示编码空间的差异，并采用分离噪声分支和适配性片选择来处理多模态编辑提示。

Result: 在多样化编辑任务中表现优异，能够处理仅依靠文本方法无法完成的复杂编辑场景。

Conclusion: 该框架通过音频提供丰富信息，显著提升了复杂视觉编辑任务的能力。

Abstract: Visual editing with diffusion models has made significant progress but often struggles with complex scenarios that textual guidance alone could not adequately describe, highlighting the need for additional non-text editing prompts. In this work, we introduce a novel audio-guided visual editing framework that can handle complex editing tasks with multiple text and audio prompts without requiring additional training. Existing audio-guided visual editing methods often necessitate training on specific datasets to align audio with text, limiting their generalization to real-world situations. We leverage a pre-trained multi-modal encoder with strong zero-shot capabilities and integrate diverse audio into visual editing tasks, by alleviating the discrepancy between the audio encoder space and the diffusion model's prompt encoder space. Additionally, we propose a novel approach to handle complex scenarios with multiple and multi-modal editing prompts through our separate noise branching and adaptive patch selection. Our comprehensive experiments on diverse editing tasks demonstrate that our framework excels in handling complicated editing scenarios by incorporating rich information from audio, where text-only approaches fail.

</details>


### [5] [Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation](https://arxiv.org/abs/2508.20471)
*Jiusi Li,Jackson Jiang,Jinyu Miao,Miao Long,Tuopu Wen,Peijin Jia,Shengxiang Liu,Chunlei Yu,Maolin Liu,Yuzhan Cai,Kun Jiang,Mengmeng Yang,Diange Yang*

Main category: cs.CV

TL;DR: G^2Editor是一个用于驾驶视频中逼真精确物体编辑的框架，通过3D高斯表示和层次化特征控制，在物体重定位、插入和删除任务中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中收集真实世界的极端案例成本高且危险，现有编辑方法存在视觉保真度有限和姿态控制不精确的问题

Method: 利用编辑对象的3D高斯表示作为密集先验注入去噪过程，使用场景级3D边界框布局重建遮挡区域，并引入层次化细粒度特征指导外观细节

Result: 在Waymo Open Dataset上的实验表明，G^2Editor在姿态可控性和视觉质量方面优于现有方法，并能有效支持下游数据驱动任务

Conclusion: G^2Editor提供了一个统一框架，能够生成高质量、精确控制的驾驶场景编辑结果，为自动驾驶系统的训练和验证提供了有效的合成数据生成方案

Abstract: Corner cases are crucial for training and validating autonomous driving systems, yet collecting them from the real world is often costly and hazardous. Editing objects within captured sensor data offers an effective alternative for generating diverse scenarios, commonly achieved through 3D Gaussian Splatting or image generative models. However, these approaches often suffer from limited visual fidelity or imprecise pose control. To address these issues, we propose G^2Editor, a framework designed for photorealistic and precise object editing in driving videos. Our method leverages a 3D Gaussian representation of the edited object as a dense prior, injected into the denoising process to ensure accurate pose control and spatial consistency. A scene-level 3D bounding box layout is employed to reconstruct occluded areas of non-target objects. Furthermore, to guide the appearance details of the edited object, we incorporate hierarchical fine-grained features as additional conditions during generation. Experiments on the Waymo Open Dataset demonstrate that G^2Editor effectively supports object repositioning, insertion, and deletion within a unified framework, outperforming existing methods in both pose controllability and visual quality, while also benefiting downstream data-driven tasks.

</details>


### [6] [Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding](https://arxiv.org/abs/2508.20476)
*Jeong Hun Yeo,Hyeongseop Rha,Sungjune Park,Junil Won,Yong Man Ro*

Main category: cs.CV

TL;DR: 首个统一框架，能够处理手语、唇部动作和音频等多模态输入，用于生成口语文本，性能超越专门模型。


<details>
  <summary>Details</summary>
Motivation: 现有语音识别技术对耳机人群无法使用，而手语译码和视觉语音识别研究多为单模态分离研究，需要统一框架来整合这些模态。

Method: 设计了一个统一的、模态无关的架构，能够有效处理异构输入，并明确将唇部动作模型化为独立模态。

Result: 在SLT、VSR、ASR和AVSR任务上达到或超过了专门模型的最新性能水平，明确模型化唇部动作显著提高了手语译码性能。

Conclusion: 统一框架能够有效整合多模态信息，唇部动作作为非手势线索在手语理解中发挥重要作用，为无声通信提供了更全面的解决方案。

Abstract: Audio is the primary modality for human communication and has driven the success of Automatic Speech Recognition (ASR) technologies. However, such systems remain inherently inaccessible to individuals who are deaf or hard of hearing. Visual alternatives such as sign language and lip reading offer effective substitutes, and recent advances in Sign Language Translation (SLT) and Visual Speech Recognition (VSR) have improved audio-less communication. Yet, these modalities have largely been studied in isolation, and their integration within a unified framework remains underexplored. In this paper, we introduce the first unified framework capable of handling diverse combinations of sign language, lip movements, and audio for spoken-language text generation. We focus on three main objectives: (i) designing a unified, modality-agnostic architecture capable of effectively processing heterogeneous inputs; (ii) exploring the underexamined synergy among modalities, particularly the role of lip movements as non-manual cues in sign language comprehension; and (iii) achieving performance on par with or superior to state-of-the-art models specialized for individual tasks. Building on this framework, we achieve performance on par with or better than task-specific state-of-the-art models across SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that explicitly modeling lip movements as a separate modality significantly improves SLT performance.

</details>


### [7] [Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent](https://arxiv.org/abs/2508.20505)
*En Ci,Shanyan Guan,Yanhao Ge,Yilin Zhang,Wei Li,Zhenyu Zhang,Jian Yang,Ying Tai*

Main category: cs.CV

TL;DR: DescriptiveEdit是一个基于描述性提示的图像编辑框架，将指令式编辑重新定义为基于参考图像的文本到图像生成，避免了重建误差和数据集限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有的语义图像编辑方法存在重建误差（基于反演的方法）和数据集质量限制（基于指令的方法）的问题，需要一种既能保持生成质量又不受数据集限制的解决方案。

Method: 提出Cross-Attentive UNet架构，通过注意力桥接机制将参考图像特征注入到提示到编辑图像的生成过程中，无需架构修改或反演操作。

Result: 在Emu Edit基准测试中显示出编辑准确性和一致性的提升，能够无缝集成ControlNet、IP-Adapter等扩展，具有更好的可扩展性。

Conclusion: DescriptiveEdit通过重新定义问题框架，充分利用了训练良好的文本到图像模型的生成能力，克服了现有方法的局限性，为语义图像编辑提供了更有效的解决方案。

Abstract: Despite the progress in text-to-image generation, semantic image editing remains a challenge. Inversion-based algorithms unavoidably introduce reconstruction errors, while instruction-based models mainly suffer from limited dataset quality and scale. To address these problems, we propose a descriptive-prompt-based editing framework, named DescriptiveEdit. The core idea is to re-frame `instruction-based image editing' as `reference-image-based text-to-image generation', which preserves the generative power of well-trained Text-to-Image models without architectural modifications or inversion. Specifically, taking the reference image and a prompt as input, we introduce a Cross-Attentive UNet, which newly adds attention bridges to inject reference image features into the prompt-to-edit-image generation process. Owing to its text-to-image nature, DescriptiveEdit overcomes limitations in instruction dataset quality, integrates seamlessly with ControlNet, IP-Adapter, and other extensions, and is more scalable. Experiments on the Emu Edit benchmark show it improves editing accuracy and consistency.

</details>


### [8] [Adam SLAM - the last mile of camera calibration with 3DGS](https://arxiv.org/abs/2508.20526)
*Matthieu Gendrin,Stéphane Pateux,Xiaoran Jiang,Théo Ladune,Luce Morin*

Main category: cs.CV

TL;DR: 使用3DGS模型通过反向传播新视角颜色损失来微调相机标定，在参考数据集上平均提升0.4 dB PSNR


<details>
  <summary>Details</summary>
Motivation: 相机标定质量对新颖视角合成至关重要，1像素的标定误差会显著影响重建质量，但真实场景缺乏地面真值

Method: 利用3D高斯散射(3DGS)模型，通过反向传播新视角颜色损失来优化相机参数

Result: 在3DGS使用的参考数据集上，新标定方法平均带来0.4 dB PSNR的提升

Conclusion: 虽然微调过程可能耗时，但对于参考场景的标定（如Mip-NeRF 360），新视角质量是最重要的考量因素

Abstract: The quality of the camera calibration is of major importance for evaluating progresses in novel view synthesis, as a 1-pixel error on the calibration has a significant impact on the reconstruction quality. While there is no ground truth for real scenes, the quality of the calibration is assessed by the quality of the novel view synthesis. This paper proposes to use a 3DGS model to fine tune calibration by backpropagation of novel view color loss with respect to the cameras parameters. The new calibration alone brings an average improvement of 0.4 dB PSNR on the dataset used as reference by 3DGS. The fine tuning may be long and its suitability depends on the criticity of training time, but for calibration of reference scenes, such as Mip-NeRF 360, the stake of novel view quality is the most important.

</details>


### [9] [FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models](https://arxiv.org/abs/2508.20586)
*Zheng Chong,Yanwei Lei,Shiyue Zhang,Zhuandi He,Zhen Wang,Xujie Zhang,Xiao Dong,Yiling Wu,Dongmei Jiang,Xiaodan Liang*

Main category: cs.CV

TL;DR: FastFit是一个基于可缓存扩散架构的高速多参考虚拟试穿框架，通过半注意力机制和类别嵌入实现参考特征的一次计算多次复用，平均加速3.5倍，并在新构建的DressCode-MR数据集上取得优异效果


<details>
  <summary>Details</summary>
Motivation: 当前虚拟试穿技术面临两大挑战：无法支持多参考服装组合（包括服装和配饰），以及由于在每个去噪步骤中重复计算参考特征导致的显著低效率

Method: 提出基于可缓存扩散架构的FastFit框架，采用半注意力机制，用类别嵌入替代传统时间步嵌入，实现参考特征编码与去噪过程的完全解耦，参考特征只需计算一次即可在所有步骤中无损复用

Result: 在VITON-HD、DressCode和新建的DressCode-MR数据集上的广泛实验表明，FastFit在关键保真度指标上超越最先进方法，同时提供显著的推理效率优势，平均加速3.5倍

Conclusion: FastFit通过创新的可缓存架构成功解决了多参考虚拟试穿的效率和功能限制问题，为复杂多参考虚拟试穿研究提供了新的技术方案和数据集支持

Abstract: Despite its great potential, virtual try-on technology is hindered from real-world application by two major challenges: the inability of current methods to support multi-reference outfit compositions (including garments and accessories), and their significant inefficiency caused by the redundant re-computation of reference features in each denoising step. To address these challenges, we propose FastFit, a high-speed multi-reference virtual try-on framework based on a novel cacheable diffusion architecture. By employing a Semi-Attention mechanism and substituting traditional timestep embeddings with class embeddings for reference items, our model fully decouples reference feature encoding from the denoising process with negligible parameter overhead. This allows reference features to be computed only once and losslessly reused across all steps, fundamentally breaking the efficiency bottleneck and achieving an average 3.5x speedup over comparable methods. Furthermore, to facilitate research on complex, multi-reference virtual try-on, we introduce DressCode-MR, a new large-scale dataset. It comprises 28,179 sets of high-quality, paired images covering five key categories (tops, bottoms, dresses, shoes, and bags), constructed through a pipeline of expert models and human feedback refinement. Extensive experiments on the VITON-HD, DressCode, and our DressCode-MR datasets show that FastFit surpasses state-of-the-art methods on key fidelity metrics while offering its significant advantage in inference efficiency.

</details>


### [10] [Physics Informed Generative Models for Magnetic Field Images](https://arxiv.org/abs/2508.20612)
*Aye Phyu Phyu Aung,Lucas Lum,Zhansen Shi,Wen Qiu,Bernice Zee,JM Chin,Yeow Kheng Lim,J. Senthilnath*

Main category: cs.CV

TL;DR: 提出PI-GenMFI方法，使用扩散模型结合物理约束生成合成磁場图像，解决半导体制造中MFI数据集稀缺问题，用于缺陷定位训练。


<details>
  <summary>Details</summary>
Motivation: 半导体制造中X射线检测内存密集且耗时，MFI能高效定位感兴趣区域，但MFI数据集因专有问题稀缺，限制了机器学习模型的训练。

Method: 使用基于扩散模型的物理信息生成模型(PI-GenMFI)，集成特定物理信息生成合成MFI样本，特别是针对电源短路这类常见缺陷类型。

Result: 与最先进的VAE和扩散生成模型比较，通过领域专家评估和多种图像生成、信号处理指标进行定性和定量评估，显示出有希望的结果。

Conclusion: PI-GenMFI方法能有效生成合成MFI图像，为优化缺陷定位过程的机器学习算法提供训练数据，解决了MFI数据集稀缺的瓶颈问题。

Abstract: In semiconductor manufacturing, defect detection and localization are critical to ensuring product quality and yield. While X-ray imaging is a reliable non-destructive testing method, it is memory-intensive and time-consuming for large-scale scanning, Magnetic Field Imaging (MFI) offers a more efficient means to localize regions of interest (ROI) for targeted X-ray scanning. However, the limited availability of MFI datasets due to proprietary concerns presents a significant bottleneck for training machine learning (ML) models using MFI. To address this challenge, we consider an ML-driven approach leveraging diffusion models with two physical constraints. We propose Physics Informed Generative Models for Magnetic Field Images (PI-GenMFI) to generate synthetic MFI samples by integrating specific physical information. We generate MFI images for the most common defect types: power shorts. These synthetic images will serve as training data for ML algorithms designed to localize defect areas efficiently. To evaluate generated MFIs, we compare our model to SOTA generative models from both variational autoencoder (VAE) and diffusion methods. We present a domain expert evaluation to assess the generated samples. In addition, we present qualitative and quantitative evaluation using various metrics used for image generation and signal processing, showing promising results to optimize the defect localization process.

</details>


### [11] [Revisiting the Privacy Risks of Split Inference: A GAN-Based Data Reconstruction Attack via Progressive Feature Optimization](https://arxiv.org/abs/2508.20613)
*Yixiang Qiu,Yanhan Liu,Hongyao Yu,Hao Fang,Bin Chen,Shu-Tao Xia,Ke Xu*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于GAN的进阶特征优化方法，能够在分开推理环境中更有效地恢复敏感输入数据，在多种情况下超越了现有攻击方法。


<details>
  <summary>Details</summary>
Motivation: 分开推理模式中交换的中间特征存在数据恢复攻击风险，现有攻击方法在深度模型和跨数据集情况下效果有限。

Method: 采用GAN框架结合进阶特征优化(PFO)，将生成器分解为层次块并逐步精炼中间表征，以提升语义保真度，同时使用L1-ball约束提高图像真实性。

Result: 实验结果显示该方法在高分辨率场景、分布外设置以及面对更深更复杂DNN时，表现显著超过现有攻击方法。

Conclusion: 该研究提供了一种更有效的数据恢复攻击框架，显示了分开推理中的隐私风险，对于设计更安全的协作学习系统具有重要意义。

Abstract: The growing complexity of Deep Neural Networks (DNNs) has led to the adoption of Split Inference (SI), a collaborative paradigm that partitions computation between edge devices and the cloud to reduce latency and protect user privacy. However, recent advances in Data Reconstruction Attacks (DRAs) reveal that intermediate features exchanged in SI can be exploited to recover sensitive input data, posing significant privacy risks. Existing DRAs are typically effective only on shallow models and fail to fully leverage semantic priors, limiting their reconstruction quality and generalizability across datasets and model architectures. In this paper, we propose a novel GAN-based DRA framework with Progressive Feature Optimization (PFO), which decomposes the generator into hierarchical blocks and incrementally refines intermediate representations to enhance the semantic fidelity of reconstructed images. To stabilize the optimization and improve image realism, we introduce an L1-ball constraint during reconstruction. Extensive experiments show that our method outperforms prior attacks by a large margin, especially in high-resolution scenarios, out-of-distribution settings, and against deeper and more complex DNNs.

</details>


### [12] [AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View Images](https://arxiv.org/abs/2508.20623)
*Shiqi Xin,Xiaolin Zhang,Yanbin Liu,Peng Zhang,Caifeng Shan*

Main category: cs.CV

TL;DR: AvatarBack是一个即插即用的框架，通过生成身份一致的后视伪图像和学习性空间对齐策略，解决了基于高斯泼溅的头像重建方法中后脑区域重建质量差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯泼溅的头像重建方法主要依赖前视图图像，导致后脑区域重建质量差，存在几何不一致、结构模糊和真实感降低等问题，限制了重建头像的保真度。

Method: 提出AvatarBack框架，包含两个核心技术：主题特定生成器（SSG）利用生成先验从稀疏前视输入合成身份一致的后视伪图像；自适应空间对齐策略（ASA）使用可学习变换矩阵优化合成视图与3D高斯表示之间的几何对齐。

Result: 在NeRSemble和K-hairstyle数据集上的实验表明，AvatarBack显著提升了后脑重建质量，同时保持了前视保真度，重建的头像在不同运动下保持一致的视觉真实感且完全可动画化。

Conclusion: AvatarBack通过显式建模缺失的后脑区域，成功解决了现有方法的后脑重建问题，为完整一致的3D高斯头像重建提供了有效解决方案。

Abstract: Recent advances in Gaussian Splatting have significantly boosted the reconstruction of head avatars, enabling high-quality facial modeling by representing an 3D avatar as a collection of 3D Gaussians. However, existing methods predominantly rely on frontal-view images, leaving the back-head poorly constructed. This leads to geometric inconsistencies, structural blurring, and reduced realism in the rear regions, ultimately limiting the fidelity of reconstructed avatars. To address this challenge, we propose AvatarBack, a novel plug-and-play framework specifically designed to reconstruct complete and consistent 3D Gaussian avatars by explicitly modeling the missing back-head regions. AvatarBack integrates two core technical innovations,i.e., the Subject-specific Generator (SSG) and the Adaptive Spatial Alignment Strategy (ASA). The former leverages a generative prior to synthesize identity-consistent, plausible back-view pseudo-images from sparse frontal inputs, providing robust multi-view supervision. To achieve precise geometric alignment between these synthetic views and the 3D Gaussian representation, the later employs learnable transformation matrices optimized during training, effectively resolving inherent pose and coordinate discrepancies. Extensive experiments on NeRSemble and K-hairstyle datasets, evaluated using geometric, photometric, and GPT-4o-based perceptual metrics, demonstrate that AvatarBack significantly enhances back-head reconstruction quality while preserving frontal fidelity. Moreover, the reconstructed avatars maintain consistent visual realism under diverse motions and remain fully animatable.

</details>


### [13] [CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models](https://arxiv.org/abs/2508.20640)
*Ayan Banerjee,Fernando Vilariño,Josep Lladós*

Main category: cs.CV

TL;DR: CraftGraffiti是一个端到端的文本引导涂鸦生成框架，通过风格优先、身份保持的方法解决极端风格化中面部身份保留的挑战，在保持面部识别性的同时实现高质量涂鸦艺术生成。


<details>
  <summary>Details</summary>
Motivation: 解决涂鸦这种高对比度抽象媒介中，极端风格变换导致面部身份识别性丧失的问题，保持个人和文化真实性。

Method: 采用LoRA微调的预训练扩散变换器进行涂鸦风格迁移，通过面部一致性自注意力机制增强身份嵌入，使用CLIP引导的提示扩展实现无关键点的姿态定制。

Result: 在面部特征一致性方面表现优异，获得最先进的美学评分和人类偏好分数，在Cruilla音乐节的实际部署中展现了现实世界的创意影响。

Conclusion: CraftGraffiti推进了身份尊重的AI辅助艺术目标，为创意AI应用中融合风格自由和可识别性提供了原则性方法。

Abstract: Preserving facial identity under extreme stylistic transformation remains a major challenge in generative art. In graffiti, a high-contrast, abstract medium, subtle distortions to the eyes, nose, or mouth can erase the subject's recognizability, undermining both personal and cultural authenticity. We present CraftGraffiti, an end-to-end text-guided graffiti generation framework designed with facial feature preservation as a primary objective. Given an input image and a style and pose descriptive prompt, CraftGraffiti first applies graffiti style transfer via LoRA-fine-tuned pretrained diffusion transformer, then enforces identity fidelity through a face-consistent self-attention mechanism that augments attention layers with explicit identity embeddings. Pose customization is achieved without keypoints, using CLIP-guided prompt extension to enable dynamic re-posing while retaining facial coherence. We formally justify and empirically validate the "style-first, identity-after" paradigm, showing it reduces attribute drift compared to the reverse order. Quantitative results demonstrate competitive facial feature consistency and state-of-the-art aesthetic and human preference scores, while qualitative analyses and a live deployment at the Cruilla Festival highlight the system's real-world creative impact. CraftGraffiti advances the goal of identity-respectful AI-assisted artistry, offering a principled approach for blending stylistic freedom with recognizability in creative AI applications.

</details>


### [14] ["Humor, Art, or Misinformation?": A Multimodal Dataset for Intent-Aware Synthetic Image Detection](https://arxiv.org/abs/2508.20670)
*Anastasios Skoularikis,Stefanos-Iordanis Papadopoulos,Symeon Papadopoulos,Panagiotis C. Petrantonakis*

Main category: cs.CV

TL;DR: S-HArM数据集用于意图感知分类，包含9576个社交媒体图像-文本对，标注为幽默/讽刺、艺术或虚假信息。研究探索了三种提示策略生成合成数据，发现基于图像和多模态引导的数据在真实内容上泛化更好，但整体性能有限。


<details>
  <summary>Details</summary>
Motivation: 现有多模态AI研究主要关注检测合成内容和上下文不一致内容，但忽视了AI生成图像背后的意图。需要填补意图感知分类的研究空白。

Method: 构建S-HArM多模态数据集，包含社交媒体真实图像-文本对。使用三种提示策略（图像引导、描述引导、多模态引导）通过Stable Diffusion生成大规模合成训练数据。比较了模态融合、对比学习、重建网络、注意力机制和大规模视觉语言模型等多种方法。

Result: 基于图像引导和多模态引导数据训练的模型在真实内容上泛化性能更好，因为保留了视觉上下文信息。但整体分类性能仍然有限。

Conclusion: 推断AI生成图像意图具有复杂性，需要专门的架构来提升性能。视觉上下文的保留对模型泛化能力至关重要。

Abstract: Recent advances in multimodal AI have enabled progress in detecting synthetic and out-of-context content. However, existing efforts largely overlook the intent behind AI-generated images. To fill this gap, we introduce S-HArM, a multimodal dataset for intent-aware classification, comprising 9,576 "in the wild" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art, or Misinformation. Additionally, we explore three prompting strategies (image-guided, description-guided, and multimodally-guided) to construct a large-scale synthetic training dataset with Stable Diffusion. We conduct an extensive comparative study including modality fusion, contrastive learning, reconstruction networks, attention mechanisms, and large vision-language models. Our results show that models trained on image- and multimodally-guided data generalize better to "in the wild" content, due to preserved visual context. However, overall performance remains limited, highlighting the complexity of inferring intent and the need for specialized architectures.

</details>


### [15] [CardioMorphNet: Cardiac Motion Prediction Using a Shape-Guided Bayesian Recurrent Deep Network](https://arxiv.org/abs/2508.20734)
*Reza Akbari Movahed,Abuzar Rezaee,Arezoo Zakeri,Colin Berry,Edmond S. L. Ho,Ali Gooya*

Main category: cs.CV

TL;DR: CardioMorphNet是一个基于贝叶斯深度学习的3D心脏形状引导变形配准框架，通过循环变分自编码器建模时空依赖关系，避免了传统强度相似性损失的局限性，在心脏运动估计方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有心脏运动估计方法依赖基于强度的图像配准相似性损失，往往忽略心脏解剖区域，导致运动捕捉不准确。需要一种能够专注于解剖区域的方法来提升心脏功能评估的准确性。

Method: 提出CardioMorphNet循环贝叶斯深度学习框架，使用循环变分自编码器建模心脏周期的时空依赖关系，通过两个后验模型进行双心室分割和运动估计，利用分割图递归配准而不使用强度相似性损失。

Result: 在UK Biobank数据集上验证，CardioMorphNet在心脏运动估计方面表现出色，优于现有最先进方法。不确定性评估显示其在心脏区域产生更低的不确定性值，预测置信度更高。

Conclusion: CardioMorphNet通过形状引导的贝叶斯深度学习框架成功解决了传统强度相似性损失的局限性，为心脏运动估计提供了更准确和可靠的解决方案，具有重要的临床应用价值。

Abstract: Accurate cardiac motion estimation from cine cardiac magnetic resonance (CMR) images is vital for assessing cardiac function and detecting its abnormalities. Existing methods often struggle to capture heart motion accurately because they rely on intensity-based image registration similarity losses that may overlook cardiac anatomical regions. To address this, we propose CardioMorphNet, a recurrent Bayesian deep learning framework for 3D cardiac shape-guided deformable registration using short-axis (SAX) CMR images. It employs a recurrent variational autoencoder to model spatio-temporal dependencies over the cardiac cycle and two posterior models for bi-ventricular segmentation and motion estimation. The derived loss function from the Bayesian formulation guides the framework to focus on anatomical regions by recursively registering segmentation maps without using intensity-based image registration similarity loss, while leveraging sequential SAX volumes and spatio-temporal features. The Bayesian modelling also enables computation of uncertainty maps for the estimated motion fields. Validated on the UK Biobank dataset by comparing warped mask shapes with ground truth masks, CardioMorphNet demonstrates superior performance in cardiac motion estimation, outperforming state-of-the-art methods. Uncertainty assessment shows that it also yields lower uncertainty values for estimated motion fields in the cardiac region compared with other probabilistic-based cardiac registration methods, indicating higher confidence in its predictions.

</details>


### [16] [Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning](https://arxiv.org/abs/2508.20751)
*Yibin Wang,Zhimin Li,Yuhang Zang,Yujie Zhou,Jiazi Bu,Chunyu Wang,Qinglin Lu,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 提出Pref-GRPO方法解决文本到图像生成中的奖励攻击问题，通过成对偏好奖励替代点式评分，并建立UniGenBench综合评估基准


<details>
  <summary>Details</summary>
Motivation: 当前基于点式奖励模型的文本到图像生成方法容易受到奖励攻击，微小的分数差异经过归一化后被放大，导致模型过度优化琐碎增益，破坏生成稳定性

Method: Pref-GRPO方法：使用成对偏好奖励模型，在每组图像中进行成对比较，以胜率作为奖励信号，将优化目标从分数最大化转变为偏好拟合

Result: 实验表明Pref-GRPO能够区分细微的图像质量差异，提供更稳定的优势并缓解奖励攻击问题。UniGenBench基准揭示了开源和闭源T2I模型的优缺点

Conclusion: Pref-GRPO方法有效解决了奖励攻击问题，UniGenBench为全面评估文本到图像生成模型提供了统一标准，验证了新方法的有效性

Abstract: Recent advancements highlight the importance of GRPO-based reinforcement learning methods and benchmarking in enhancing text-to-image (T2I) generation. However, current methods using pointwise reward models (RM) for scoring generated images are susceptible to reward hacking. We reveal that this happens when minimal score differences between images are amplified after normalization, creating illusory advantages that drive the model to over-optimize for trivial gains, ultimately destabilizing the image generation process. To address this, we propose Pref-GRPO, a pairwise preference reward-based GRPO method that shifts the optimization objective from score maximization to preference fitting, ensuring more stable training. In Pref-GRPO, images are pairwise compared within each group using preference RM, and the win rate is used as the reward signal. Extensive experiments demonstrate that PREF-GRPO differentiates subtle image quality differences, providing more stable advantages and mitigating reward hacking. Additionally, existing T2I benchmarks are limited by coarse evaluation criteria, hindering comprehensive model assessment. To solve this, we introduce UniGenBench, a unified T2I benchmark comprising 600 prompts across 5 main themes and 20 subthemes. It evaluates semantic consistency through 10 primary and 27 sub-criteria, leveraging MLLM for benchmark construction and evaluation. Our benchmarks uncover the strengths and weaknesses of both open and closed-source T2I models and validate the effectiveness of Pref-GRPO.

</details>


### [17] [${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale Feature for Generalizable Gaussian Splatting](https://arxiv.org/abs/2508.20754)
*Yuxi Hu,Jun Zhang,Kuangyi Chen,Zhe Zhang,Friedrich Fraundorfer*

Main category: cs.CV

TL;DR: C3-GS是一个通用的高斯泼溅框架，通过上下文感知、跨维度和跨尺度约束来增强特征学习，无需逐场景优化即可实现高质量的新视角合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在稀疏输入视图下难以编码判别性、多视图一致的特征来预测高斯参数，导致几何构建不准确。

Method: 提出C3-GS框架，集成三个轻量级模块到统一渲染管道中，包含上下文感知、跨维度和跨尺度约束，改进特征融合。

Result: 在基准数据集上的广泛实验验证了C3-GS实现了最先进的渲染质量和泛化能力。

Conclusion: 该框架能够实现逼真的合成效果，且不需要额外的监督信息。

Abstract: Generalizable Gaussian Splatting aims to synthesize novel views for unseen scenes without per-scene optimization. In particular, recent advancements utilize feed-forward networks to predict per-pixel Gaussian parameters, enabling high-quality synthesis from sparse input views. However, existing approaches fall short in encoding discriminative, multi-view consistent features for Gaussian predictions, which struggle to construct accurate geometry with sparse views. To address this, we propose $\mathbf{C}^{3}$-GS, a framework that enhances feature learning by incorporating context-aware, cross-dimension, and cross-scale constraints. Our architecture integrates three lightweight modules into a unified rendering pipeline, improving feature fusion and enabling photorealistic synthesis without requiring additional supervision. Extensive experiments on benchmark datasets validate that $\mathbf{C}^{3}$-GS achieves state-of-the-art rendering quality and generalization ability. Code is available at: https://github.com/YuhsiHu/C3-GS.

</details>


### [18] [Estimating 2D Keypoints of Surgical Tools Using Vision-Language Models with Low-Rank Adaptation](https://arxiv.org/abs/2508.20830)
*Krit Duangprom,Tryphon Lambrou,Binod Bhattarai*

Main category: cs.CV

TL;DR: 提出基于视觉语言模型(VLMs)和LoRA微调技术的手术工具2D关键点估计新方法，在小规模医疗数据集上表现优于传统CNN和Transformer方法。


<details>
  <summary>Details</summary>
Motivation: 传统CNN和Transformer方法在小规模医疗数据集上容易过拟合，需要利用预训练VLMs的泛化能力来解决这个问题。

Method: 使用LoRA技术微调预训练VLMs，精心设计提示词构建指令调优数据集，将视觉特征与语义关键点描述对齐。

Result: 仅需2个epoch的微调，适应后的VLM就超越了基线模型，证明了LoRA在低资源场景下的有效性。

Conclusion: 该方法不仅提高了关键点检测性能，还为未来3D手术手和工具姿态估计研究奠定了基础。

Abstract: This paper presents a novel pipeline for 2D keypoint estima- tion of surgical tools by leveraging Vision Language Models (VLMs) fine- tuned using a low rank adjusting (LoRA) technique. Unlike traditional Convolutional Neural Network (CNN) or Transformer-based approaches, which often suffer from overfitting in small-scale medical datasets, our method harnesses the generalization capabilities of pre-trained VLMs. We carefully design prompts to create an instruction-tuning dataset and use them to align visual features with semantic keypoint descriptions. Experimental results show that with only two epochs of fine tuning, the adapted VLM outperforms the baseline models, demonstrating the ef- fectiveness of LoRA in low-resource scenarios. This approach not only improves keypoint detection performance, but also paves the way for future work in 3D surgical hands and tools pose estimation.

</details>


### [19] [Understanding and evaluating computer vision models through the lens of counterfactuals](https://arxiv.org/abs/2508.20881)
*Pushkar Shukla*

Main category: cs.CV

TL;DR: 该论文提出了基于反事实推理的框架，用于解释、审计和减轻视觉分类器和生成模型中的偏见，包括CAVLI、ASAC、TIBET、BiasConnect和InterMit等方法


<details>
  <summary>Details</summary>
Motivation: 反事实推理（通过改变输入观察模型行为变化）已成为可解释和公平AI的核心，需要开发系统化方法来发现虚假相关性、探索因果依赖并构建更鲁棒的系统

Method: 1. CAVLI整合归因分析和概念级分析来量化决策对可解释概念的依赖；2. ASAC通过对抗性反事实扰动受保护属性；3. TIBET提供可扩展的提示敏感偏见评估流程；4. BiasConnect构建因果图诊断交叉偏见；5. InterMit通过因果敏感分数和用户定义的公平目标减轻偏见

Result: 开发了系统化的反事实框架，能够量化模型对无关线索的依赖，提高模型的公平性和准确性，避免刻板印象，实现可扩展的社会责任偏见评估和缓解

Conclusion: 反事实推理为判别性和生成性模型中的可解释性、公平性和因果性提供了统一视角，建立了原则性、可扩展的方法来实现社会责任偏见评估和缓解

Abstract: Counterfactual reasoning -- the practice of asking ``what if'' by varying inputs and observing changes in model behavior -- has become central to interpretable and fair AI. This thesis develops frameworks that use counterfactuals to explain, audit, and mitigate bias in vision classifiers and generative models. By systematically altering semantically meaningful attributes while holding others fixed, these methods uncover spurious correlations, probe causal dependencies, and help build more robust systems.   The first part addresses vision classifiers. CAVLI integrates attribution (LIME) with concept-level analysis (TCAV) to quantify how strongly decisions rely on human-interpretable concepts. With localized heatmaps and a Concept Dependency Score, CAVLI shows when models depend on irrelevant cues like backgrounds. Extending this, ASAC introduces adversarial counterfactuals that perturb protected attributes while preserving semantics. Through curriculum learning, ASAC fine-tunes biased models for improved fairness and accuracy while avoiding stereotype-laden artifacts.   The second part targets generative Text-to-Image (TTI) models. TIBET provides a scalable pipeline for evaluating prompt-sensitive biases by varying identity-related terms, enabling causal auditing of how race, gender, and age affect image generation. To capture interactions, BiasConnect builds causal graphs diagnosing intersectional biases. Finally, InterMit offers a modular, training-free algorithm that mitigates intersectional bias via causal sensitivity scores and user-defined fairness goals.   Together, these contributions show counterfactuals as a unifying lens for interpretability, fairness, and causality in both discriminative and generative models, establishing principled, scalable methods for socially responsible bias evaluation and mitigation.

</details>


### [20] [COMETH: Convex Optimization for Multiview Estimation and Tracking of Humans](https://arxiv.org/abs/2508.20920)
*Enrico Martini,Ho Jin Choi,Nadia Figueroa,Nicola Bombieri*

Main category: cs.CV

TL;DR: COMETH是一个轻量级实时多视角人体姿态融合算法，通过凸优化和生物力学约束提高工业5.0环境下的人体活动监测精度和效率


<details>
  <summary>Details</summary>
Motivation: 工业5.0时代需要实时人体活动监测，但多摄像头集中式系统计算成本高、带宽需求大，而边缘设备处理又会导致精度下降和时空不一致问题

Method: 提出COMETH算法，整合运动学和生物力学约束提高关节定位精度，使用凸优化逆向运动学进行空间融合，并实现状态观测器改善时间一致性

Result: 在公共和工业数据集上评估，在定位、检测和跟踪精度方面优于现有最先进方法

Conclusion: 该融合管道实现了准确且可扩展的人体运动跟踪，非常适合工业和安全关键应用

Abstract: In the era of Industry 5.0, monitoring human activity is essential for ensuring both ergonomic safety and overall well-being. While multi-camera centralized setups improve pose estimation accuracy, they often suffer from high computational costs and bandwidth requirements, limiting scalability and real-time applicability. Distributing processing across edge devices can reduce network bandwidth and computational load. On the other hand, the constrained resources of edge devices lead to accuracy degradation, and the distribution of computation leads to temporal and spatial inconsistencies. We address this challenge by proposing COMETH (Convex Optimization for Multiview Estimation and Tracking of Humans), a lightweight algorithm for real-time multi-view human pose fusion that relies on three concepts: it integrates kinematic and biomechanical constraints to increase the joint positioning accuracy; it employs convex optimization-based inverse kinematics for spatial fusion; and it implements a state observer to improve temporal consistency. We evaluate COMETH on both public and industrial datasets, where it outperforms state-of-the-art methods in localization, detection, and tracking accuracy. The proposed fusion pipeline enables accurate and scalable human motion tracking, making it well-suited for industrial and safety-critical applications. The code is publicly available at https://github.com/PARCO-LAB/COMETH.

</details>


### [21] [POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models](https://arxiv.org/abs/2508.21019)
*Jiaxiang Cheng,Bing Ma,Xuhua Ren,Hongyi Jin,Kai Yu,Peng Zhang,Wenyue Li,Yuan Zhou,Tianxiang Zheng,Qinglin Lu*

Main category: cs.CV

TL;DR: POSE是一个单步蒸馏框架，通过两阶段过程（稳定性预热和统一对抗平衡）来加速大规模视频扩散模型，将采样时间从1000秒减少到10秒，同时保持视频质量。


<details>
  <summary>Details</summary>
Motivation: 视频扩散生成领域面临采样效率瓶颈，现有图像加速方法无法建模视频帧的时间连贯性，也无法为大规模视频模型提供单步蒸馏。

Method: 采用两阶段蒸馏：1）稳定性预热机制稳定对抗蒸馏；2）统一对抗平衡机制在高斯噪声空间实现稳定的单步对抗训练；3）条件对抗一致性方法提升条件视频生成的语义和帧一致性。

Result: 在VBench-I2V上平均提升7.15%的语义对齐、时间一致性和帧质量，将预训练模型延迟降低100倍（从1000秒到10秒）。

Conclusion: POSE成功解决了大规模视频扩散模型的采样效率问题，实现了高质量单步视频生成，在保持性能的同时显著降低了计算成本。

Abstract: The field of video diffusion generation faces critical bottlenecks in sampling efficiency, especially for large-scale models and long sequences. Existing video acceleration methods adopt image-based techniques but suffer from fundamental limitations: they neither model the temporal coherence of video frames nor provide single-step distillation for large-scale video models. To bridge this gap, we propose POSE (Phased One-Step Equilibrium), a distillation framework that reduces the sampling steps of large-scale video diffusion models, enabling the generation of high-quality videos in a single step. POSE employs a carefully designed two-phase process to distill video models:(i) stability priming: a warm-up mechanism to stabilize adversarial distillation that adapts the high-quality trajectory of the one-step generator from high to low signal-to-noise ratio regimes, optimizing the video quality of single-step mappings near the endpoints of flow trajectories. (ii) unified adversarial equilibrium: a flexible self-adversarial distillation mechanism that promotes stable single-step adversarial training towards a Nash equilibrium within the Gaussian noise space, generating realistic single-step videos close to real videos. For conditional video generation, we propose (iii) conditional adversarial consistency, a method to improve both semantic consistency and frame consistency between conditional frames and generated frames. Comprehensive experiments demonstrate that POSE outperforms other acceleration methods on VBench-I2V by average 7.15% in semantic alignment, temporal conference and frame quality, reducing the latency of the pre-trained model by 100$\times$, from 1000 seconds to 10 seconds, while maintaining competitive performance.

</details>


### [22] [Reusing Computation in Text-to-Image Diffusion for Efficient Generation of Image Sets](https://arxiv.org/abs/2508.21032)
*Dale Decatur,Thibault Groueix,Wang Yifan,Rana Hanocka,Vladimir Kim,Matheus Gadelha*

Main category: cs.CV

TL;DR: 提出一种训练免费的方法，通过聚类语义相似的提示词并在早期扩散步骤中共享计算，显著降低文本到图像生成的算力成本并提升图像质量


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型计算成本高昂，虽然已有工作优化单次推理效率，但本文探索减少相关提示词间冗余的替代方法

Method: 利用扩散模型由粗到细的特性，基于语义相似性对提示词进行聚类，在早期去噪步骤中共享计算，并利用UnClip的文本到图像先验优化扩散步骤分配

Result: 实验表明，对于基于图像嵌入训练的模型，该方法显著降低计算成本的同时还提高了图像质量

Conclusion: 该方法可与现有流程无缝集成，随提示词集规模扩展，减轻大规模文本到图像生成的环境和经济负担

Abstract: Text-to-image diffusion models enable high-quality image generation but are computationally expensive. While prior work optimizes per-inference efficiency, we explore an orthogonal approach: reducing redundancy across correlated prompts. Our method leverages the coarse-to-fine nature of diffusion models, where early denoising steps capture shared structures among similar prompts. We propose a training-free approach that clusters prompts based on semantic similarity and shares computation in early diffusion steps. Experiments show that for models trained conditioned on image embeddings, our approach significantly reduces compute cost while improving image quality. By leveraging UnClip's text-to-image prior, we enhance diffusion step allocation for greater efficiency. Our method seamlessly integrates with existing pipelines, scales with prompt sets, and reduces the environmental and financial burden of large-scale text-to-image generation. Project page: https://ddecatur.github.io/hierarchical-diffusion/

</details>


### [23] [FW-GAN: Frequency-Driven Handwriting Synthesis with Wave-Modulated MLP Generator](https://arxiv.org/abs/2508.21040)
*Huynh Tong Dang Khoa,Dang Hoai Nam,Vo Nguyen Le Duy*

Main category: cs.CV

TL;DR: FW-GAN是一个基于频率感知的单样本手写合成框架，通过Wave-MLP和频率引导判别器生成高质量的风格一致手写文本，解决了传统方法在长距离依赖和频率信息建模方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 标记手写数据稀缺限制了识别系统的效果，现有合成方法存在两个主要问题：传统卷积架构难以建模长距离依赖和复杂笔画模式，以及忽视频率信息在捕捉精细风格细节中的关键作用。

Method: 提出FW-GAN框架，包含相位感知Wave-MLP生成器来捕捉空间关系并保留风格线索，频率引导判别器利用高频成分增强真实性检测，以及新颖的频率分布损失来对齐合成与真实手写的频率特征。

Result: 在越南语和英语手写数据集上的实验表明，FW-GAN能够生成高质量、风格一致的手写文本，有效增强低资源手写识别流程的数据。

Conclusion: FW-GAN通过频率感知方法显著提升了手写合成的真实性和风格一致性，为低资源手写识别提供了有价值的数据增强工具。

Abstract: Labeled handwriting data is often scarce, limiting the effectiveness of recognition systems that require diverse, style-consistent training samples. Handwriting synthesis offers a promising solution by generating artificial data to augment training. However, current methods face two major limitations. First, most are built on conventional convolutional architectures, which struggle to model long-range dependencies and complex stroke patterns. Second, they largely ignore the crucial role of frequency information, which is essential for capturing fine-grained stylistic and structural details in handwriting. To address these challenges, we propose FW-GAN, a one-shot handwriting synthesis framework that generates realistic, writer-consistent text from a single example. Our generator integrates a phase-aware Wave-MLP to better capture spatial relationships while preserving subtle stylistic cues. We further introduce a frequency-guided discriminator that leverages high-frequency components to enhance the authenticity detection of generated samples. Additionally, we introduce a novel Frequency Distribution Loss that aligns the frequency characteristics of synthetic and real handwriting, thereby enhancing visual fidelity. Experiments on Vietnamese and English handwriting datasets demonstrate that FW-GAN generates high-quality, style-consistent handwriting, making it a valuable tool for augmenting data in low-resource handwriting recognition (HTR) pipelines. Official implementation is available at https://github.com/DAIR-Group/FW-GAN

</details>


### [24] [OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning](https://arxiv.org/abs/2508.21066)
*Yuan Gong,Xionghui Wang,Jie Wu,Shiyin Wang,Yitong Wang,Xinglong Wu*

Main category: cs.CV

TL;DR: OneReward是一个统一的强化学习框架，使用单一奖励模型提升多任务生成能力，并应用于Seedream 3.0 Fill模型，在多项掩码引导图像生成任务中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多任务生成方法通常需要针对特定任务进行监督微调(SFT)，这限制了模型的泛化能力和训练效率。不同任务虽然共享相同的条件范式，但在数据分布和评估指标上差异显著。

Method: 使用单一视觉语言模型(VLM)作为生成奖励模型，能够区分给定任务和评估标准下的优劣结果。基于OneReward框架，开发了Seedream 3.0 Fill模型，通过多任务强化学习直接在预训练基础模型上进行训练，无需任务特定的SFT。

Result: 实验结果表明，统一的编辑模型在多个评估维度上持续优于商业和开源竞争对手，包括Ideogram、Adobe Photoshop和FLUX Fill [Pro]。

Conclusion: OneReward框架证明了使用单一奖励模型可以有效提升多任务生成能力，Seedream 3.0 Fill模型展示了无需任务特定SFT即可实现优异性能的可行性。

Abstract: In this paper, we introduce OneReward, a unified reinforcement learning framework that enhances the model's generative capabilities across multiple tasks under different evaluation criteria using only \textit{One Reward} model. By employing a single vision-language model (VLM) as the generative reward model, which can distinguish the winner and loser for a given task and a given evaluation criterion, it can be effectively applied to multi-task generation models, particularly in contexts with varied data and diverse task objectives. We utilize OneReward for mask-guided image generation, which can be further divided into several sub-tasks such as image fill, image extend, object removal, and text rendering, involving a binary mask as the edit area. Although these domain-specific tasks share same conditioning paradigm, they differ significantly in underlying data distributions and evaluation metrics. Existing methods often rely on task-specific supervised fine-tuning (SFT), which limits generalization and training efficiency. Building on OneReward, we develop Seedream 3.0 Fill, a mask-guided generation model trained via multi-task reinforcement learning directly on a pre-trained base model, eliminating the need for task-specific SFT. Experimental results demonstrate that our unified edit model consistently outperforms both commercial and open-source competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across multiple evaluation dimensions. Code and model are available at: https://one-reward.github.io

</details>


### [25] [First-Place Solution to NeurIPS 2024 Invisible Watermark Removal Challenge](https://arxiv.org/abs/2508.21072)
*Fahad Shamshad,Tameem Bakr,Yahia Shaaban,Noor Hussein,Karthik Nandakumar,Nils Lukas*

Main category: cs.CV

TL;DR: 本文提出了针对NeurIPS 2024挑战赛的获胜水印移除方案，包含黑盒和白盒两种攻击方法，实现了95.7%的水印移除率且保持图像质量


<details>
  <summary>Details</summary>
Motivation: 测试现有水印技术对抗对抗性攻击的鲁棒性，为开发更强大的水印方法提供参考

Method: 白盒跟踪使用基于VAE的自适应规避攻击，结合测试时优化和CIELAB色彩空间恢复；黑盒跟踪通过图像聚类，应用扩散模型和ChatGPT生成的语义先验进行水印移除

Result: 实现了95.7%的近完美水印移除率，对残余图像质量影响极小

Conclusion: 该方法成功展示了现有水印技术的脆弱性，希望激励开发更鲁棒的水印方法

Abstract: Content watermarking is an important tool for the authentication and copyright protection of digital media. However, it is unclear whether existing watermarks are robust against adversarial attacks. We present the winning solution to the NeurIPS 2024 Erasing the Invisible challenge, which stress-tests watermark robustness under varying degrees of adversary knowledge. The challenge consisted of two tracks: a black-box and beige-box track, depending on whether the adversary knows which watermarking method was used by the provider. For the beige-box track, we leverage an adaptive VAE-based evasion attack, with a test-time optimization and color-contrast restoration in CIELAB space to preserve the image's quality. For the black-box track, we first cluster images based on their artifacts in the spatial or frequency-domain. Then, we apply image-to-image diffusion models with controlled noise injection and semantic priors from ChatGPT-generated captions to each cluster with optimized parameter settings. Empirical evaluations demonstrate that our method successfully achieves near-perfect watermark removal (95.7%) with negligible impact on the residual image's quality. We hope that our attacks inspire the development of more robust image watermarking methods.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [26] [Efficient and Privacy-Protecting Background Removal for 2D Video Streaming using iPhone 15 Pro Max LiDAR](https://arxiv.org/abs/2508.20250)
*Jessica Kinnevan,Naifa Alqahtani,Toral Chauhan*

Main category: eess.IV

TL;DR: iPhone 15 Pro Max的LiDAR技术可替代传统背景移除方法，通过深度信息实现不受光照影响的实时背景移除，处理速度达60fps。


<details>
  <summary>Details</summary>
Motivation: 传统背景移除技术如色度键控和AI模型受光照条件限制，LiDAR的深度信息能提供更稳定可靠的背景移除解决方案。

Method: 集成iPhone 15 Pro Max的LiDAR和彩色摄像头，使用SwiftUI和Swift框架开发用户界面和后端，Metal Shader Language实现实时图像增强处理。

Result: 系统能在60fps下实时处理，深度信息不受光照影响，在低光和明亮环境下表现一致，当前深度图分辨率为320x240。

Conclusion: LiDAR技术有望成为视频和摄影应用中背景移除的主流方法，前提是深度图分辨率能提升到与彩色图像匹配的水平。

Abstract: Light Detection and Ranging (LiDAR) technology in consumer-grade mobile devices can be used as a replacement for traditional background removal and compositing techniques. Unlike approaches such as chroma keying and trained AI models, LiDAR's depth information is independent of subject lighting, and performs equally well in low-light and well-lit environments. We integrate the LiDAR and color cameras on the iPhone 15 Pro Max with GPU-based image processing. We use Apple's SwiftUI and Swift frameworks for user interface and backend development, and Metal Shader Language (MSL) for realtime image enhancement at the standard iPhone streaming frame rate of 60 frames per second. The only meaningful limitations of the technology are the streaming bandwidth of the depth data, which currently reduces the depth map resolution to 320x240, and any pre-existing limitations of the LiDAR IR laser to reflect accurate depth from some materials. If the LiDAR resolution on a mobile device like the iPhone can be improved to match the color image resolution, LiDAR could feasibly become the preeminent method of background removal for video applications and photography.

</details>


### [27] [GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac MRI Reconstruction](https://arxiv.org/abs/2508.20600)
*Kian Anvari Hamedani,Narges Razizadeh,Shahabedin Nabavi,Mohsen Ebrahimi Moghaddam*

Main category: eess.IV

TL;DR: GENRE-CMR是一种基于GAN的生成对抗网络架构，采用残差深度展开重建框架，通过边缘感知区域损失和统计分布对齐损失，显著提升心脏磁共振图像重建质量和泛化能力


<details>
  <summary>Details</summary>
Motivation: 解决加速心脏磁共振成像中扫描时间与图像质量之间的权衡问题，特别是在不同采集设置下的泛化能力挑战

Method: 提出GAN架构的残差深度展开重建框架，集成边缘感知区域损失(EAR)和统计分布对齐损失(SDA)，通过对称KL散度正则化特征空间

Result: 在未见数据上达到0.9552 SSIM和38.90 dB PSNR，超越现有最优方法，在各种加速因子和采样轨迹下表现优异

Conclusion: 该框架为高质量CMR重建提供了统一且鲁棒的解决方案，为临床异构采集协议的适应性部署铺平道路

Abstract: Accelerated Cardiovascular Magnetic Resonance (CMR) image reconstruction remains a critical challenge due to the trade-off between scan time and image quality, particularly when generalizing across diverse acquisition settings. We propose GENRE-CMR, a generative adversarial network (GAN)-based architecture employing a residual deep unrolled reconstruction framework to enhance reconstruction fidelity and generalization. The architecture unrolls iterative optimization into a cascade of convolutional subnetworks, enriched with residual connections to enable progressive feature propagation from shallow to deeper stages. To further improve performance, we integrate two loss functions: (1) an Edge-Aware Region (EAR) loss, which guides the network to focus on structurally informative regions and helps prevent common reconstruction blurriness; and (2) a Statistical Distribution Alignment (SDA) loss, which regularizes the feature space across diverse data distributions via a symmetric KL divergence formulation. Extensive experiments confirm that GENRE-CMR surpasses state-of-the-art methods on training and unseen data, achieving 0.9552 SSIM and 38.90 dB PSNR on unseen distributions across various acceleration factors and sampling trajectories. Ablation studies confirm the contribution of each proposed component to reconstruction quality and generalization. Our framework presents a unified and robust solution for high-quality CMR reconstruction, paving the way for clinically adaptable deployment across heterogeneous acquisition protocols.

</details>


### [28] [Efficient Fine-Tuning of DINOv3 Pretrained on Natural Images for Atypical Mitotic Figure Classification in MIDOG 2025](https://arxiv.org/abs/2508.21041)
*Guillaume Balezo,Raphaël Bourgade,Thomas Walter*

Main category: eess.IV

TL;DR: DINOv3-H+视觉变换器通过LoRA微调和数据增强，在MIDOG 2025挑战赛中实现了0.8871的平衡准确率，为异常有丝分裂分类提供了强大基线


<details>
  <summary>Details</summary>
Motivation: 异常有丝分裂图像(AMFs)是预后不良的标志，但由于低发生率、形态细微和观察者间差异，检测困难。MIDOG 2025挑战赛旨在建立跨域AMF分类基准

Method: 使用在自然图像上预训练的DINOv3-H+视觉变换器，通过低秩适应(LoRA，65万可训练参数)和广泛数据增强进行微调

Result: 尽管存在领域差距，DINOv3在组织病理学图像上有效迁移，在初步测试集上达到0.8871的平衡准确率

Conclusion: DINOv3预训练具有强大鲁棒性，结合参数高效微调方法，为MIDOG 2025的异常有丝分裂分类提供了强有力的基准模型

Abstract: Atypical mitotic figures (AMFs) are markers of abnormal cell division associated with poor prognosis, yet their detection remains difficult due to low prevalence, subtle morphology, and inter-observer variability. The MIDOG 2025 challenge introduces a benchmark for AMF classification across multiple domains. In this work, we evaluate the recently published DINOv3-H+ vision transformer, pretrained on natural images, which we fine-tuned using low-rank adaptation (LoRA, 650k trainable parameters) and extensive augmentation. Despite the domain gap, DINOv3 transfers effectively to histopathology, achieving a balanced accuracy of 0.8871 on the preliminary test set. These results highlight the robustness of DINOv3 pretraining and show that, when combined with parameter-efficient fine-tuning, it provides a strong baseline for atypical mitosis classification in MIDOG 2025.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [29] [SPGrasp: Spatiotemporal Prompt-driven Grasp Synthesis in Dynamic Scenes](https://arxiv.org/abs/2508.20547)
*Yunpeng Mei,Hongjie Cao,Yinqiu Xia,Wei Xiao,Zhaohan Feng,Gang Wang,Jie Chen*

Main category: cs.RO

TL;DR: SPGrasp是一个实时动态抓取合成框架，通过集成SAMv2模型实现低延迟（59ms）的交互式抓取，在多个基准测试中达到90%+的准确率，比现有方法延迟降低58.5%。


<details>
  <summary>Details</summary>
Motivation: 解决现有动态物体抓取方法无法同时实现低延迟推理和提示驱动交互的问题，填补实时交互抓取合成领域的空白。

Method: 基于SAMv2模型扩展，集成用户提示和时空上下文信息，实现端到端的视频流抓取估计，确保动态物体的时间一致性。

Result: 在OCID上达到90.6%的实例级抓取准确率，Jacquard上93.8%，GraspNet-1Billion连续跟踪下92.0%准确率，延迟73.1ms，比RoG-SAM降低58.5%。真实世界实验中13个移动物体达到94.8%的成功率。

Conclusion: SPGrasp有效解决了动态抓取合成中的延迟-交互性权衡问题，实现了实时交互式抓取，代码已开源。

Abstract: Real-time interactive grasp synthesis for dynamic objects remains challenging as existing methods fail to achieve low-latency inference while maintaining promptability. To bridge this gap, we propose SPGrasp (spatiotemporal prompt-driven dynamic grasp synthesis), a novel framework extending segment anything model v2 (SAMv2) for video stream grasp estimation. Our core innovation integrates user prompts with spatiotemporal context, enabling real-time interaction with end-to-end latency as low as 59 ms while ensuring temporal consistency for dynamic objects. In benchmark evaluations, SPGrasp achieves instance-level grasp accuracies of 90.6% on OCID and 93.8% on Jacquard. On the challenging GraspNet-1Billion dataset under continuous tracking, SPGrasp achieves 92.0% accuracy with 73.1 ms per-frame latency, representing a 58.5% reduction compared to the prior state-of-the-art promptable method RoG-SAM while maintaining competitive accuracy. Real-world experiments involving 13 moving objects demonstrate a 94.8% success rate in interactive grasping scenarios. These results confirm SPGrasp effectively resolves the latency-interactivity trade-off in dynamic grasp synthesis. Code is available at https://github.com/sejmoonwei/SPGrasp.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [30] [Unleashing Uncertainty: Efficient Machine Unlearning for Generative AI](https://arxiv.org/abs/2508.20773)
*Christoforos N. Spartalis,Theodoros Semertzidis,Petros Daras,Efstratios Gavves*

Main category: cs.LG

TL;DR: SAFEMax是一种基于信息论原理的扩散模型机器遗忘方法，通过最大化生成图像的熵，使模型在条件于不可允许类别时生成高斯噪声并停止去噪过程。


<details>
  <summary>Details</summary>
Motivation: 为了解决扩散模型中特定类别内容的遗忘问题，同时保持对其他类别的记忆，需要一种高效的机器遗忘方法。

Method: 基于信息论原理，通过最大化生成图像的熵，在条件于不可允许类别时生成高斯噪声并停止去噪过程。通过选择性关注早期扩散步骤来控制遗忘与保留的平衡。

Result: SAFEMax表现出有效性，并在效率方面显著优于现有最先进方法。

Conclusion: SAFEMax提供了一种有效且高效的扩散模型机器遗忘解决方案，通过信息论方法实现了选择性遗忘。

Abstract: We introduce SAFEMax, a novel method for Machine Unlearning in diffusion models. Grounded in information-theoretic principles, SAFEMax maximizes the entropy in generated images, causing the model to generate Gaussian noise when conditioned on impermissible classes by ultimately halting its denoising process. Also, our method controls the balance between forgetting and retention by selectively focusing on the early diffusion steps, where class-specific information is prominent. Our results demonstrate the effectiveness of SAFEMax and highlight its substantial efficiency gains over state-of-the-art methods.

</details>
