{"id": "2510.06802", "pdf": "https://arxiv.org/pdf/2510.06802", "abs": "https://arxiv.org/abs/2510.06802", "authors": ["Islomjon Shukhratov", "Sergey Gorinsky"], "title": "Capture and Interact: Rapid 3D Object Acquisition and Rendering with Gaussian Splatting in Unity", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Capturing and rendering three-dimensional (3D) objects in real time remain a significant challenge, yet hold substantial potential for applications in augmented reality, digital twin systems, remote collaboration and prototyping. We present an end-to-end pipeline that leverages 3D Gaussian Splatting (3D GS) to enable rapid acquisition and interactive rendering of real-world objects using a mobile device, cloud processing and a local computer. Users scan an object with a smartphone video, upload it for automated 3D reconstruction, and visualize it interactively in Unity at an average of 150 frames per second (fps) on a laptop. The system integrates mobile capture, cloud-based 3D GS and Unity rendering to support real-time telepresence. Our experiments show that the pipeline processes scans in approximately 10 minutes on a graphics processing unit (GPU) achieving real-time rendering on the laptop.", "AI": {"tldr": "\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u7aef\u5230\u7aef\u5b9e\u65f63D\u5bf9\u8c61\u91c7\u96c6\u4e0e\u6e32\u67d3\u7cfb\u7edf\uff0c\u4f7f\u7528\u624b\u673a\u89c6\u9891\u626b\u63cf\u3001\u4e91\u7aef\u5904\u7406\u548c\u672c\u5730Unity\u6e32\u67d3\uff0c\u5b9e\u73b0150fps\u7684\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u3002", "motivation": "\u89e3\u51b3\u5b9e\u65f63D\u5bf9\u8c61\u6355\u83b7\u548c\u6e32\u67d3\u7684\u6311\u6218\uff0c\u4e3a\u589e\u5f3a\u73b0\u5b9e\u3001\u6570\u5b57\u5b6a\u751f\u3001\u8fdc\u7a0b\u534f\u4f5c\u7b49\u5e94\u7528\u63d0\u4f9b\u6f5c\u529b\u3002", "method": "\u96c6\u6210\u79fb\u52a8\u8bbe\u5907\u6355\u83b7\u3001\u4e91\u7aef3D\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u548cUnity\u6e32\u67d3\u7684\u7aef\u5230\u7aef\u6d41\u6c34\u7ebf\uff0c\u7528\u6237\u901a\u8fc7\u624b\u673a\u89c6\u9891\u626b\u63cf\u5bf9\u8c61\u5e76\u4e0a\u4f20\u4e91\u7aef\u5904\u7406\u3002", "result": "\u7cfb\u7edf\u5728GPU\u4e0a\u7ea610\u5206\u949f\u5b8c\u6210\u626b\u63cf\u5904\u7406\uff0c\u5728\u7b14\u8bb0\u672c\u7535\u8111\u4e0a\u5b9e\u73b0\u5e73\u5747150fps\u7684\u5b9e\u65f6\u6e32\u67d3\u6027\u80fd\u3002", "conclusion": "\u8be5\u6d41\u6c34\u7ebf\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u79fb\u52a8\u6355\u6349\u5230\u4ea4\u4e92\u5f0f\u6e32\u67d3\u7684\u5b9e\u65f63D\u5bf9\u8c61\u5904\u7406\uff0c\u652f\u6301\u5b9e\u65f6\u8fdc\u7a0b\u5448\u73b0\u5e94\u7528\u3002"}}
{"id": "2510.06281", "pdf": "https://arxiv.org/pdf/2510.06281", "abs": "https://arxiv.org/abs/2510.06281", "authors": ["Chenyang Li", "Qin Li", "Haimin Wang", "Bo Shen"], "title": "Improving the Spatial Resolution of GONG Solar Images to GST Quality Using Deep Learning", "categories": ["cs.CV", "cs.AI"], "comment": "5 pages; accepted as a workshop paper in ICDM 2025", "summary": "High-resolution (HR) solar imaging is crucial for capturing fine-scale dynamic features such as filaments and fibrils. However, the spatial resolution of the full-disk H$\\alpha$ images is limited and insufficient to resolve these small-scale structures. To address this, we propose a GAN-based superresolution approach to enhance low-resolution (LR) full-disk H$\\alpha$ images from the Global Oscillation Network Group (GONG) to a quality comparable with HR observations from the Big Bear Solar Observatory/Goode Solar Telescope (BBSO/GST). We employ Real-ESRGAN with Residual-in-Residual Dense Blocks and a relativistic discriminator. We carefully aligned GONG-GST pairs. The model effectively recovers fine details within sunspot penumbrae and resolves fine details in filaments and fibrils, achieving an average mean squared error (MSE) of 467.15, root mean squared error (RMSE) of 21.59, and cross-correlation (CC) of 0.7794. Slight misalignments between image pairs limit quantitative performance, which we plan to address in future work alongside dataset expansion to further improve reconstruction quality.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eGAN\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u5c06GONG\u7684\u4f4e\u5206\u8fa8\u7387H\u03b1\u592a\u9633\u56fe\u50cf\u589e\u5f3a\u81f3\u63a5\u8fd1BBSO/GST\u7684\u9ad8\u5206\u8fa8\u7387\u8d28\u91cf\uff0c\u6709\u6548\u6062\u590d\u592a\u9633\u9ed1\u5b50\u534a\u5f71\u548c\u7ec6\u4e1d\u7ed3\u6784\u7684\u7ec6\u8282\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u592a\u9633\u6210\u50cf\u5bf9\u6355\u6349\u7ec6\u4e1d\u548c\u7ea4\u7ef4\u7b49\u7cbe\u7ec6\u52a8\u6001\u7279\u5f81\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5168\u76d8H\u03b1\u56fe\u50cf\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\u6709\u9650\uff0c\u65e0\u6cd5\u89e3\u6790\u8fd9\u4e9b\u5fae\u5c0f\u7ed3\u6784\u3002", "method": "\u4f7f\u7528Real-ESRGAN\u6a21\u578b\uff0c\u5305\u542b\u6b8b\u5dee\u4e2d\u7684\u6b8b\u5dee\u5bc6\u96c6\u5757\u548c\u76f8\u5bf9\u5224\u522b\u5668\uff0c\u5bf9GONG-GST\u56fe\u50cf\u5bf9\u8fdb\u884c\u7cbe\u5fc3\u5bf9\u9f50\u5904\u7406\u3002", "result": "\u6a21\u578b\u6709\u6548\u6062\u590d\u4e86\u592a\u9633\u9ed1\u5b50\u534a\u5f71\u5185\u7684\u7cbe\u7ec6\u7ec6\u8282\uff0c\u5e76\u89e3\u6790\u4e86\u7ec6\u4e1d\u548c\u7ea4\u7ef4\u7684\u7ec6\u5fae\u7ed3\u6784\uff0c\u5e73\u5747MSE\u4e3a467.15\uff0cRMSE\u4e3a21.59\uff0c\u4ea4\u53c9\u76f8\u5173\u6027\u4e3a0.7794\u3002", "conclusion": "\u56fe\u50cf\u5bf9\u7684\u8f7b\u5fae\u4e0d\u5bf9\u9f50\u9650\u5236\u4e86\u5b9a\u91cf\u6027\u80fd\uff0c\u672a\u6765\u8ba1\u5212\u89e3\u51b3\u5bf9\u9f50\u95ee\u9898\u5e76\u6269\u5c55\u6570\u636e\u96c6\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2510.06295", "pdf": "https://arxiv.org/pdf/2510.06295", "abs": "https://arxiv.org/abs/2510.06295", "authors": ["Young D. Kwon", "Abhinav Mehrotra", "Malcolm Chadwick", "Alberto Gil Ramos", "Sourav Bhattacharya"], "title": "Efficient High-Resolution Image Editing with Hallucination-Aware Loss and Adaptive Tiling", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Preprint. Under review", "summary": "High-resolution (4K) image-to-image synthesis has become increasingly important for mobile applications. Existing diffusion models for image editing face significant challenges, in terms of memory and image quality, when deployed on resource-constrained devices. In this paper, we present MobilePicasso, a novel system that enables efficient image editing at high resolutions, while minimising computational cost and memory usage. MobilePicasso comprises three stages: (i) performing image editing at a standard resolution with hallucination-aware loss, (ii) applying latent projection to overcome going to the pixel space, and (iii) upscaling the edited image latent to a higher resolution with adaptive context-preserving tiling. Our user study with 46 participants reveals that MobilePicasso not only improves image quality by 18-48% but reduces hallucinations by 14-51% over existing methods. MobilePicasso demonstrates significantly lower latency, e.g., up to 55.8$\\times$ speed-up, yet with a small increase in runtime memory, e.g., a mere 9% increase over prior work. Surprisingly, the on-device runtime of MobilePicasso is observed to be faster than a server-based high-resolution image editing model running on an A100 GPU.", "AI": {"tldr": "MobilePicasso\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7f16\u8f91\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u65b9\u6cd5\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b04K\u56fe\u50cf\u7f16\u8f91\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u8fdb\u884c\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7f16\u8f91\u65f6\u9762\u4e34\u5185\u5b58\u548c\u56fe\u50cf\u8d28\u91cf\u7684\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a\u6807\u51c6\u5206\u8fa8\u7387\u7f16\u8f91\uff08\u542b\u5e7b\u89c9\u611f\u77e5\u635f\u5931\uff09\u3001\u6f5c\u5728\u6295\u5f71\u907f\u514d\u50cf\u7d20\u7a7a\u95f4\u8f6c\u6362\u3001\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u4fdd\u6301\u5206\u5757\u4e0a\u91c7\u6837\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\u56fe\u50cf\u8d28\u91cf\u63d0\u534718-48%\uff0c\u5e7b\u89c9\u51cf\u5c1114-51%\uff0c\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe55.8\u500d\uff0c\u5185\u5b58\u4ec5\u589e\u52a09%\uff0c\u8bbe\u5907\u7aef\u8fd0\u884c\u901f\u5ea6\u751a\u81f3\u8d85\u8fc7A100 GPU\u4e0a\u7684\u670d\u52a1\u5668\u6a21\u578b\u3002", "conclusion": "MobilePicasso\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u9ad8\u6548\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7f16\u8f91\uff0c\u5728\u4fdd\u6301\u4f4e\u5185\u5b58\u5360\u7528\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u548c\u5904\u7406\u901f\u5ea6\u3002"}}
{"id": "2510.06298", "pdf": "https://arxiv.org/pdf/2510.06298", "abs": "https://arxiv.org/abs/2510.06298", "authors": ["Tobias J. Bauer"], "title": "RGBD Gaze Tracking Using Transformer for Feature Fusion", "categories": ["cs.CV", "cs.AI"], "comment": "Master Thesis with 125 pages, 59 figures, 17 tables", "summary": "Subject of this thesis is the implementation of an AI-based Gaze Tracking system using RGBD images that contain both color (RGB) and depth (D) information. To fuse the features extracted from the images, a module based on the Transformer architecture is used. The combination of RGBD input images and Transformers was chosen because it has not yet been investigated. Furthermore, a new dataset is created for training the AI models as existing datasets either do not contain depth information or only contain labels for Gaze Point Estimation that are not suitable for the task of Gaze Angle Estimation. Various model configurations are trained, validated and evaluated on a total of three different datasets. The trained models are then to be used in a real-time pipeline to estimate the gaze direction and thus the gaze point of a person in front of a computer screen. The AI model architecture used in this thesis is based on an earlier work by Lian et al. It uses a Generative Adversarial Network (GAN) to simultaneously remove depth map artifacts and extract head pose features. Lian et al. achieve a mean Euclidean error of 38.7mm on their own dataset ShanghaiTechGaze+. In this thesis, a model architecture with a Transformer module for feature fusion achieves a mean Euclidean error of 55.3mm on the same dataset, but we show that using no pre-trained GAN module leads to a mean Euclidean error of 30.1mm. Replacing the Transformer module with a Multilayer Perceptron (MLP) improves the error to 26.9mm. These results are coherent with the ones on the other two datasets. On the ETH-XGaze dataset, the model with Transformer module achieves a mean angular error of 3.59{\\deg} and without Transformer module 3.26{\\deg}, whereas the fundamentally different model architecture used by the dataset authors Zhang et al. achieves a mean angular error of 2.04{\\deg}. On the OTH-Gaze-Estimation dataset created for...", "AI": {"tldr": "\u57fa\u4e8eRGBD\u56fe\u50cf\u7684AI\u89c6\u7ebf\u8ffd\u8e2a\u7cfb\u7edf\u5b9e\u73b0\uff0c\u4f7f\u7528Transformer\u878d\u5408\u7279\u5f81\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6027\u80fd\uff0c\u53d1\u73b0\u4e0d\u4f7f\u7528\u9884\u8bad\u7ec3GAN\u6a21\u5757\u548c\u7528MLP\u66ff\u4ee3Transformer\u80fd\u83b7\u5f97\u66f4\u597d\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u8981\u4e48\u7f3a\u5c11\u6df1\u5ea6\u4fe1\u606f\uff0c\u8981\u4e48\u53ea\u9002\u5408\u89c6\u7ebf\u70b9\u4f30\u8ba1\u800c\u4e0d\u9002\u5408\u89c6\u7ebf\u89d2\u5ea6\u4f30\u8ba1\uff0c\u56e0\u6b64\u9700\u8981\u521b\u5efa\u65b0\u6570\u636e\u96c6\u5e76\u7814\u7a76RGBD\u56fe\u50cf\u4e0eTransformer\u7ed3\u5408\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528RGBD\u56fe\u50cf\u8f93\u5165\uff0c\u57fa\u4e8eTransformer\u67b6\u6784\u7684\u7279\u5f81\u878d\u5408\u6a21\u5757\uff0c\u91c7\u7528GAN\u53bb\u9664\u6df1\u5ea6\u56fe\u4f2a\u5f71\u5e76\u63d0\u53d6\u5934\u90e8\u59ff\u6001\u7279\u5f81\uff0c\u5728\u4e09\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u548c\u8bc4\u4f30\u591a\u79cd\u6a21\u578b\u914d\u7f6e\u3002", "result": "\u5728ShanghaiTechGaze+\u6570\u636e\u96c6\u4e0a\uff0c\u5e26Transformer\u6a21\u5757\u7684\u6a21\u578b\u5e73\u5747\u6b27\u51e0\u91cc\u5f97\u8bef\u5dee\u4e3a55.3mm\uff0c\u4e0d\u4f7f\u7528\u9884\u8bad\u7ec3GAN\u6a21\u5757\u964d\u81f330.1mm\uff0c\u7528MLP\u66ff\u4ee3Transformer\u8fdb\u4e00\u6b65\u964d\u81f326.9mm\u3002\u5728ETH-XGaze\u6570\u636e\u96c6\u4e0a\uff0c\u5e26Transformer\u6a21\u5757\u7684\u5e73\u5747\u89d2\u5ea6\u8bef\u5dee\u4e3a3.59\u5ea6\uff0c\u4e0d\u5e26\u4e3a3.26\u5ea6\u3002", "conclusion": "RGBD\u56fe\u50cf\u4e0eTransformer\u7ed3\u5408\u7684\u65b9\u6cd5\u53ef\u884c\uff0c\u4f46\u4f7f\u7528\u66f4\u7b80\u5355\u7684MLP\u66ff\u4ee3Transformer\u80fd\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\uff0c\u4e14\u4e0d\u4f7f\u7528\u9884\u8bad\u7ec3GAN\u6a21\u5757\u4e5f\u80fd\u6539\u5584\u7ed3\u679c\u3002"}}
{"id": "2510.06308", "pdf": "https://arxiv.org/pdf/2510.06308", "abs": "https://arxiv.org/abs/2510.06308", "authors": ["Yi Xin", "Qi Qin", "Siqi Luo", "Kaiwen Zhu", "Juncheng Yan", "Yan Tai", "Jiayi Lei", "Yuewen Cao", "Keqi Wang", "Yibin Wang", "Jinbin Bai", "Qian Yu", "Dengyang Jiang", "Yuandong Pu", "Haoxing Chen", "Le Zhuo", "Junjun He", "Gen Luo", "Tianbin Li", "Ming Hu", "Jin Ye", "Shenglong Ye", "Bo Zhang", "Chang Xu", "Wenhai Wang", "Hongsheng Li", "Guangtao Zhai", "Tianfan Xue", "Bin Fu", "Xiaohong Liu", "Yu Qiao", "Yihao Liu"], "title": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding", "categories": ["cs.CV"], "comment": "33 pages, 13 figures, 10 tables", "summary": "We introduce Lumina-DiMOO, an open-source foundational model for seamless multi-modal generation and understanding. Lumina-DiMOO sets itself apart from prior unified models by utilizing a fully discrete diffusion modeling to handle inputs and outputs across various modalities. This innovative approach allows Lumina-DiMOO to achieve higher sampling efficiency compared to previous autoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support a broad spectrum of multi-modal tasks, including text-to-image generation, image-to-image generation (e.g., image editing, subject-driven generation, and image inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves state-of-the-art performance on multiple benchmarks, surpassing existing open-source unified multi-modal models. To foster further advancements in multi-modal and discrete diffusion model research, we release our code and checkpoints to the community. Project Page: https://synbol.github.io/Lumina-DiMOO.", "AI": {"tldr": "Lumina-DiMOO\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528\u5b8c\u5168\u79bb\u6563\u6269\u6563\u5efa\u6a21\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\u8f93\u51fa\uff0c\u5728\u91c7\u6837\u6548\u7387\u548c\u4efb\u52a1\u652f\u6301\u8303\u56f4\u4e0a\u4f18\u4e8e\u4e4b\u524d\u7684\u81ea\u56de\u5f52\u6216\u6df7\u5408\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u5728\u91c7\u6837\u6548\u7387\u548c\u4efb\u52a1\u652f\u6301\u8303\u56f4\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5904\u7406\u6587\u672c\u3001\u56fe\u50cf\u7b49\u591a\u79cd\u6a21\u6001\u7684\u751f\u6210\u548c\u7406\u89e3\u4efb\u52a1\u3002", "method": "\u4f7f\u7528\u5b8c\u5168\u79bb\u6563\u6269\u6563\u5efa\u6a21\u6765\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\u8f93\u51fa\uff0c\u76f8\u6bd4\u81ea\u56de\u5f52\u6216\u6df7\u5408\u81ea\u56de\u5f52-\u6269\u6563\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u91c7\u6837\u6548\u7387\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u5f00\u6e90\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u3002", "conclusion": "Lumina-DiMOO\u901a\u8fc7\u79bb\u6563\u6269\u6563\u5efa\u6a21\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u6a21\u6001\u751f\u6210\u548c\u7406\u89e3\uff0c\u4e3a\u591a\u6a21\u6001\u548c\u79bb\u6563\u6269\u6563\u6a21\u578b\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5f00\u6e90\u5de5\u5177\u3002"}}
{"id": "2510.06460", "pdf": "https://arxiv.org/pdf/2510.06460", "abs": "https://arxiv.org/abs/2510.06460", "authors": ["Piyush Dashpute", "Niki Nezakati", "Wolfgang Heidrich", "Vishwanath Saragadam"], "title": "TDiff: Thermal Plug-And-Play Prior with Patch-Based Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Thermal images from low-cost cameras often suffer from low resolution, fixed pattern noise, and other localized degradations. Available datasets for thermal imaging are also limited in both size and diversity. To address these challenges, we propose a patch-based diffusion framework (TDiff) that leverages the local nature of these distortions by training on small thermal patches. In this approach, full-resolution images are restored by denoising overlapping patches and blending them using smooth spatial windowing. To our knowledge, this is the first patch-based diffusion framework that models a learned prior for thermal image restoration across multiple tasks. Experiments on denoising, super-resolution, and deblurring demonstrate strong results on both simulated and real thermal data, establishing our method as a unified restoration pipeline.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8epatch\u7684\u6269\u6563\u6846\u67b6TDiff\uff0c\u901a\u8fc7\u5728\u5c0f\u70ed\u56fe\u50cfpatch\u4e0a\u8bad\u7ec3\u6765\u89e3\u51b3\u4f4e\u5206\u8fa8\u7387\u70ed\u56fe\u50cf\u7684\u6062\u590d\u95ee\u9898\uff0c\u5728\u53bb\u566a\u3001\u8d85\u5206\u8fa8\u7387\u548c\u53bb\u6a21\u7cca\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f4e\u6210\u672c\u70ed\u6210\u50cf\u76f8\u673a\u5b58\u5728\u4f4e\u5206\u8fa8\u7387\u3001\u56fa\u5b9a\u6a21\u5f0f\u566a\u58f0\u7b49\u5c40\u90e8\u9000\u5316\u95ee\u9898\uff0c\u4e14\u73b0\u6709\u70ed\u6210\u50cf\u6570\u636e\u96c6\u5728\u89c4\u6a21\u548c\u591a\u6837\u6027\u65b9\u9762\u90fd\u6709\u9650\u3002", "method": "\u57fa\u4e8epatch\u7684\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u5c0f\u70ed\u56fe\u50cfpatch\u4e0a\u8bad\u7ec3\u5b66\u4e60\u5148\u9a8c\u77e5\u8bc6\uff0c\u4f7f\u7528\u91cd\u53e0patch\u53bb\u566a\u548c\u5e73\u6ed1\u7a7a\u95f4\u7a97\u53e3\u878d\u5408\u6765\u6062\u590d\u5168\u5206\u8fa8\u7387\u56fe\u50cf\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u70ed\u6570\u636e\u4e0a\u7684\u53bb\u566a\u3001\u8d85\u5206\u8fa8\u7387\u548c\u53bb\u6a21\u7cca\u5b9e\u9a8c\u90fd\u53d6\u5f97\u4e86\u5f3a\u52b2\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u4e3a\u70ed\u56fe\u50cf\u6062\u590d\u7684\u7edf\u4e00\u6d41\u7a0b\uff0c\u662f\u9996\u4e2a\u57fa\u4e8epatch\u7684\u6269\u6563\u6846\u67b6\uff0c\u80fd\u591f\u8de8\u591a\u4e2a\u4efb\u52a1\u5efa\u6a21\u5b66\u4e60\u5148\u9a8c\u3002"}}
{"id": "2510.06469", "pdf": "https://arxiv.org/pdf/2510.06469", "abs": "https://arxiv.org/abs/2510.06469", "authors": ["Oindrila Saha", "Vojtech Krs", "Radomir Mech", "Subhransu Maji", "Kevin Blackburn-Matzen", "Matheus Gadelha"], "title": "SIGMA-GEN: Structure and Identity Guided Multi-subject Assembly for Image Generation", "categories": ["cs.CV"], "comment": "Webpage: https://oindrilasaha.github.io/SIGMA-Gen/", "summary": "We present SIGMA-GEN, a unified framework for multi-identity preserving image generation. Unlike prior approaches, SIGMA-GEN is the first to enable single-pass multi-subject identity-preserved generation guided by both structural and spatial constraints. A key strength of our method is its ability to support user guidance at various levels of precision -- from coarse 2D or 3D boxes to pixel-level segmentations and depth -- with a single model. To enable this, we introduce SIGMA-SET27K, a novel synthetic dataset that provides identity, structure, and spatial information for over 100k unique subjects across 27k images. Through extensive evaluation we demonstrate that SIGMA-GEN achieves state-of-the-art performance in identity preservation, image generation quality, and speed. Code and visualizations at https://oindrilasaha.github.io/SIGMA-Gen/", "AI": {"tldr": "SIGMA-GEN\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u8eab\u4efd\u4fdd\u6301\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8e\u7ed3\u6784\u548c\u7a7a\u95f4\u7ea6\u675f\u7684\u5355\u6b21\u591a\u4e3b\u4f53\u8eab\u4efd\u4fdd\u6301\u751f\u6210\uff0c\u652f\u6301\u4ece\u7c97\u7c92\u5ea6\u5230\u50cf\u7d20\u7ea7\u7cbe\u5ea6\u7684\u7528\u6237\u5f15\u5bfc\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u4e3b\u4f53\u8eab\u4efd\u4fdd\u6301\u751f\u6210\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u7528\u6237\u5f15\u5bfc\u80fd\u529b\u3002", "method": "\u5f15\u5165SIGMA-SET27K\u5408\u6210\u6570\u636e\u96c6\uff0c\u5305\u542b10\u4e07+\u72ec\u7279\u4e3b\u4f53\u548c2.7\u4e07\u5f20\u56fe\u50cf\u7684\u8eab\u4efd\u3001\u7ed3\u6784\u548c\u7a7a\u95f4\u4fe1\u606f\uff1b\u5f00\u53d1\u7edf\u4e00\u6846\u67b6\u652f\u6301\u591a\u79cd\u7cbe\u5ea6\u7ea7\u522b\u7684\u7528\u6237\u5f15\u5bfc\u3002", "result": "\u5728\u8eab\u4efd\u4fdd\u6301\u3001\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u548c\u901f\u5ea6\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "SIGMA-GEN\u4e3a\u591a\u8eab\u4efd\u4fdd\u6301\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u4f18\u5f02\u7684\u6027\u80fd\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2510.06529", "pdf": "https://arxiv.org/pdf/2510.06529", "abs": "https://arxiv.org/abs/2510.06529", "authors": ["Xiangyi Chen", "Th\u00e9ophane Vallaeys", "Maha Elbayad", "John Nguyen", "Jakob Verbeek"], "title": "VUGEN: Visual Understanding priors for GENeration", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in Vision-Language Models (VLMs) have enabled unified understanding across text and images, yet equipping these models with robust image generation capabilities remains challenging. Existing approaches often rely on reconstruction-oriented autoencoders or complex bridging mechanisms, leading to misalignment between understanding and generation representations, or architectural complexity. In this work, we propose VUGEN, a novel framework that explicitly leverages VLM's pretrained visual understanding priors for efficient and high-quality image generation. Our approach first transforms the high-dimensional latent space of the VLM's native vision encoder into a lower-dimensional, tractable distribution that maximally preserves visual information. The VLM is then trained to sample within this reduced latent space, ensuring alignment with its visual understanding capabilities. Finally, a dedicated pixel decoder maps these generated latents back to the image space. We find that a VAE-free pixel diffusion decoder to be on par or better than commonly used complex latent diffusion decoders that internally rely on VAE latents. Extensive experiments demonstrate that VUGEN achieves superior image generation performance, improving DPG Bench from 71.17 to 74.32 and FID from 11.86 to 9.06 on COCO, while fully preserving the VLM's original understanding capabilities.", "AI": {"tldr": "VUGEN\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u7406\u89e3\u5148\u9a8c\u6765\u5b9e\u73b0\u9ad8\u6548\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u751f\u6210\uff0c\u65e0\u9700\u590d\u6742\u7684\u81ea\u7f16\u7801\u5668\u6216\u6865\u63a5\u673a\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u901a\u5e38\u4f9d\u8d56\u4e8e\u91cd\u5efa\u5bfc\u5411\u7684\u81ea\u7f16\u7801\u5668\u6216\u590d\u6742\u7684\u6865\u63a5\u673a\u5236\uff0c\u5bfc\u81f4\u7406\u89e3\u4e0e\u751f\u6210\u8868\u793a\u4e0d\u5bf9\u9f50\u6216\u67b6\u6784\u590d\u6742\u3002", "method": "\u9996\u5148\u5c06VLM\u89c6\u89c9\u7f16\u7801\u5668\u7684\u9ad8\u7ef4\u6f5c\u5728\u7a7a\u95f4\u8f6c\u6362\u4e3a\u4f4e\u7ef4\u53ef\u5904\u7406\u5206\u5e03\uff0c\u7136\u540e\u8bad\u7ec3VLM\u5728\u8be5\u964d\u7ef4\u6f5c\u5728\u7a7a\u95f4\u4e2d\u91c7\u6837\uff0c\u6700\u540e\u4f7f\u7528\u4e13\u95e8\u7684\u50cf\u7d20\u89e3\u7801\u5668\u5c06\u751f\u6210\u7684\u6f5c\u5728\u6620\u5c04\u56de\u56fe\u50cf\u7a7a\u95f4\u3002", "result": "VUGEN\u5728\u56fe\u50cf\u751f\u6210\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c06DPG Bench\u4ece71.17\u63d0\u5347\u523074.32\uff0cCOCO\u4e0a\u7684FID\u4ece11.86\u6539\u5584\u52309.06\uff0c\u540c\u65f6\u5b8c\u5168\u4fdd\u7559\u4e86VLM\u539f\u6709\u7684\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "VUGEN\u6846\u67b6\u6210\u529f\u5730\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u4e0e\u56fe\u50cf\u751f\u6210\u80fd\u529b\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u751f\u6210\uff0c\u4e14\u65e0\u9700\u590d\u6742\u7684\u6f5c\u5728\u6269\u6563\u89e3\u7801\u5668\u3002"}}
{"id": "2510.06564", "pdf": "https://arxiv.org/pdf/2510.06564", "abs": "https://arxiv.org/abs/2510.06564", "authors": ["Qiongyang Hu", "Wenyang Liu", "Wenbin Zou", "Yuejiao Su", "Lap-Pui Chau", "Yi Wang"], "title": "HSNet: Heterogeneous Subgraph Network for Single Image Super-resolution", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing deep learning approaches for image super-resolution, particularly those based on CNNs and attention mechanisms, often suffer from structural inflexibility. Although graph-based methods offer greater representational adaptability, they are frequently impeded by excessive computational complexity. To overcome these limitations, this paper proposes the Heterogeneous Subgraph Network (HSNet), a novel framework that efficiently leverages graph modeling while maintaining computational feasibility. The core idea of HSNet is to decompose the global graph into manageable sub-components. First, we introduce the Constructive Subgraph Set Block (CSSB), which generates a diverse set of complementary subgraphs. Rather than relying on a single monolithic graph, CSSB captures heterogeneous characteristics of the image by modeling different relational patterns and feature interactions, producing a rich ensemble of both local and global graph structures. Subsequently, the Subgraph Aggregation Block (SAB) integrates the representations embedded across these subgraphs. Through adaptive weighting and fusion of multi-graph features, SAB constructs a comprehensive and discriminative representation that captures intricate interdependencies. Furthermore, a Node Sampling Strategy (NSS) is designed to selectively retain the most salient features, thereby enhancing accuracy while reducing computational overhead. Extensive experiments demonstrate that HSNet achieves state-of-the-art performance, effectively balancing reconstruction quality with computational efficiency. The code will be made publicly available.", "AI": {"tldr": "\u63d0\u51faHSNet\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u5168\u5c40\u56fe\u4e3a\u53ef\u7ba1\u7406\u7684\u5b50\u56fe\u7ec4\u4ef6\u6765\u89e3\u51b3\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\u7ed3\u6784\u4e0d\u7075\u6d3b\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u8fc7\u9ad8\u7684\u95ee\u9898", "motivation": "\u73b0\u6709\u57fa\u4e8eCNN\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\u5b58\u5728\u7ed3\u6784\u4e0d\u7075\u6d3b\u95ee\u9898\uff0c\u800c\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\u867d\u7136\u8868\u793a\u80fd\u529b\u66f4\u5f3a\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u8fc7\u9ad8", "method": "\u4f7f\u7528\u5f02\u6784\u5b50\u56fe\u7f51\u7edc(HSNet)\uff0c\u5305\u542b\u6784\u9020\u6027\u5b50\u56fe\u96c6\u5757(CSSB)\u751f\u6210\u4e92\u8865\u5b50\u56fe\uff0c\u5b50\u56fe\u805a\u5408\u5757(SAB)\u96c6\u6210\u591a\u56fe\u7279\u5f81\uff0c\u4ee5\u53ca\u8282\u70b9\u91c7\u6837\u7b56\u7565(NSS)\u9009\u62e9\u91cd\u8981\u7279\u5f81", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eHSNet\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861", "conclusion": "HSNet\u901a\u8fc7\u6709\u6548\u7684\u56fe\u5efa\u6a21\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\u7684\u7ed3\u6784\u4e0d\u7075\u6d3b\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u8fc7\u9ad8\u95ee\u9898"}}
{"id": "2510.06601", "pdf": "https://arxiv.org/pdf/2510.06601", "abs": "https://arxiv.org/abs/2510.06601", "authors": ["Feiran Li", "Jiacheng Li", "Marcos V. Conde", "Beril Besbinar", "Vlad Hosu", "Daisuke Iso", "Radu Timofte"], "title": "AIM 2025 Challenge on Real-World RAW Image Denoising", "categories": ["cs.CV"], "comment": null, "summary": "We introduce the AIM 2025 Real-World RAW Image Denoising Challenge, aiming to advance efficient and effective denoising techniques grounded in data synthesis. The competition is built upon a newly established evaluation benchmark featuring challenging low-light noisy images captured in the wild using five different DSLR cameras. Participants are tasked with developing novel noise synthesis pipelines, network architectures, and training methodologies to achieve high performance across different camera models. Winners are determined based on a combination of performance metrics, including full-reference measures (PSNR, SSIM, LPIPS), and non-reference ones (ARNIQA, TOPIQ). By pushing the boundaries of camera-agnostic low-light RAW image denoising trained on synthetic data, the competition promotes the development of robust and practical models aligned with the rapid progress in digital photography. We expect the competition outcomes to influence multiple domains, from image restoration to night-time autonomous driving.", "AI": {"tldr": "AIM 2025\u771f\u5b9e\u4e16\u754cRAW\u56fe\u50cf\u53bb\u566a\u6311\u6218\u8d5b\uff0c\u65e8\u5728\u901a\u8fc7\u6570\u636e\u5408\u6210\u63a8\u8fdb\u9ad8\u6548\u53bb\u566a\u6280\u672f\uff0c\u4f7f\u7528\u4e94\u79cd\u4e0d\u540cDSLR\u76f8\u673a\u5728\u771f\u5b9e\u4f4e\u5149\u73af\u5883\u4e0b\u62cd\u6444\u7684\u56fe\u50cf\u4f5c\u4e3a\u8bc4\u6d4b\u57fa\u51c6\u3002", "motivation": "\u63a8\u52a8\u57fa\u4e8e\u5408\u6210\u6570\u636e\u7684\u76f8\u673a\u65e0\u5173\u4f4e\u5149RAW\u56fe\u50cf\u53bb\u566a\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4fc3\u8fdb\u4e0e\u6570\u5b57\u6444\u5f71\u5feb\u901f\u8fdb\u6b65\u76f8\u4e00\u81f4\u7684\u9c81\u68d2\u5b9e\u7528\u6a21\u578b\u5f00\u53d1\u3002", "method": "\u53c2\u4e0e\u8005\u9700\u8981\u5f00\u53d1\u65b0\u9896\u7684\u566a\u58f0\u5408\u6210\u6d41\u7a0b\u3001\u7f51\u7edc\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u76f8\u673a\u578b\u53f7\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002", "result": "\u4f18\u80dc\u8005\u57fa\u4e8e\u5168\u53c2\u8003\u6307\u6807\uff08PSNR\u3001SSIM\u3001LPIPS\uff09\u548c\u65e0\u53c2\u8003\u6307\u6807\uff08ARNIQA\u3001TOPIQ\uff09\u7684\u7efc\u5408\u8868\u73b0\u786e\u5b9a\u3002", "conclusion": "\u8be5\u7ade\u8d5b\u6210\u679c\u9884\u8ba1\u5c06\u5f71\u54cd\u4ece\u56fe\u50cf\u6062\u590d\u5230\u591c\u95f4\u81ea\u52a8\u9a7e\u9a76\u7b49\u591a\u4e2a\u9886\u57df\u3002"}}
{"id": "2510.06694", "pdf": "https://arxiv.org/pdf/2510.06694", "abs": "https://arxiv.org/abs/2510.06694", "authors": ["Jipeng Lyu", "Jiahua Dong", "Yu-Xiong Wang"], "title": "SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis", "categories": ["cs.CV"], "comment": "Published in Transactions on Machine Learning Research (06/2025)", "summary": "Persistent dynamic scene modeling for tracking and novel-view synthesis remains challenging due to the difficulty of capturing accurate deformations while maintaining computational efficiency. We propose SCas4D, a cascaded optimization framework that leverages structural patterns in 3D Gaussian Splatting for dynamic scenes. The key idea is that real-world deformations often exhibit hierarchical patterns, where groups of Gaussians share similar transformations. By progressively refining deformations from coarse part-level to fine point-level, SCas4D achieves convergence within 100 iterations per time frame and produces results comparable to existing methods with only one-twentieth of the training iterations. The approach also demonstrates effectiveness in self-supervised articulated object segmentation, novel view synthesis, and dense point tracking tasks.", "AI": {"tldr": "SCas4D\u662f\u4e00\u4e2a\u7ea7\u8054\u4f18\u5316\u6846\u67b6\uff0c\u5229\u75283D\u9ad8\u65af\u6cfc\u6e85\u4e2d\u7684\u7ed3\u6784\u6a21\u5f0f\u6765\u5efa\u6a21\u52a8\u6001\u573a\u666f\uff0c\u901a\u8fc7\u4ece\u7c97\u5230\u7ec6\u7684\u53d8\u5f62\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u548c\u9ad8\u8d28\u91cf\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u573a\u666f\u5efa\u6a21\u4e2d\u51c6\u786e\u6355\u6349\u53d8\u5f62\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u6311\u6218\uff0c\u5229\u7528\u73b0\u5b9e\u4e16\u754c\u53d8\u5f62\u901a\u5e38\u5448\u73b0\u5c42\u6b21\u5316\u6a21\u5f0f\u7684\u7279\u70b9\u3002", "method": "\u91c7\u7528\u7ea7\u8054\u4f18\u5316\u6846\u67b6\uff0c\u4ece\u7c97\u7c92\u5ea6\u7684\u90e8\u4ef6\u7ea7\u53d8\u5f62\u9010\u6b65\u7ec6\u5316\u5230\u7ec6\u7c92\u5ea6\u7684\u70b9\u7ea7\u53d8\u5f62\uff0c\u5229\u7528\u9ad8\u65af\u7fa4\u7ec4\u5171\u4eab\u76f8\u4f3c\u53d8\u6362\u7684\u7279\u6027\u3002", "result": "\u6bcf\u4e2a\u65f6\u95f4\u5e27\u5728100\u6b21\u8fed\u4ee3\u5185\u6536\u655b\uff0c\u4ec5\u9700\u73b0\u6709\u65b9\u6cd5\u4e8c\u5341\u5206\u4e4b\u4e00\u7684\u8bad\u7ec3\u8fed\u4ee3\u6b21\u6570\u5373\u53ef\u8fbe\u5230\u53ef\u6bd4\u7ed3\u679c\uff0c\u5728\u81ea\u76d1\u7763\u5173\u8282\u7269\u4f53\u5206\u5272\u3001\u65b0\u89c6\u89d2\u5408\u6210\u548c\u5bc6\u96c6\u70b9\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u6548\u3002", "conclusion": "SCas4D\u901a\u8fc7\u5c42\u6b21\u5316\u53d8\u5f62\u5efa\u6a21\u5b9e\u73b0\u4e86\u52a8\u6001\u573a\u666f\u7684\u9ad8\u6548\u9ad8\u8d28\u91cf\u5efa\u6a21\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.06746", "pdf": "https://arxiv.org/pdf/2510.06746", "abs": "https://arxiv.org/abs/2510.06746", "authors": ["Zhiliang Zhu", "Tao Zeng", "Tao Yang", "Guoliang Luo", "Jiyong Zeng"], "title": "DeRainMamba: A Frequency-Aware State Space Model with Detail Enhancement for Image Deraining", "categories": ["cs.CV"], "comment": "accepted by IEEE SPL", "summary": "Image deraining is crucial for improving visual quality and supporting reliable downstream vision tasks. Although Mamba-based models provide efficient sequence modeling, their limited ability to capture fine-grained details and lack of frequency-domain awareness restrict further improvements. To address these issues, we propose DeRainMamba, which integrates a Frequency-Aware State-Space Module (FASSM) and Multi-Directional Perception Convolution (MDPConv). FASSM leverages Fourier transform to distinguish rain streaks from high-frequency image details, balancing rain removal and detail preservation. MDPConv further restores local structures by capturing anisotropic gradient features and efficiently fusing multiple convolution branches. Extensive experiments on four public benchmarks demonstrate that DeRainMamba consistently outperforms state-of-the-art methods in PSNR and SSIM, while requiring fewer parameters and lower computational costs. These results validate the effectiveness of combining frequency-domain modeling and spatial detail enhancement within a state-space framework for single image deraining.", "AI": {"tldr": "\u63d0\u51faDeRainMamba\u6a21\u578b\uff0c\u7ed3\u5408\u9891\u7387\u611f\u77e5\u72b6\u6001\u7a7a\u95f4\u6a21\u5757\u548c\u591a\u5411\u611f\u77e5\u5377\u79ef\uff0c\u5728\u56fe\u50cf\u53bb\u96e8\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u7ec6\u8282\u4fdd\u7559\u548c\u96e8\u7eb9\u53bb\u9664\u6548\u679c\u3002", "motivation": "\u73b0\u6709Mamba\u6a21\u578b\u5728\u6355\u6349\u7ec6\u7c92\u5ea6\u7ec6\u8282\u548c\u9891\u7387\u57df\u611f\u77e5\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u9650\u5236\u4e86\u56fe\u50cf\u53bb\u96e8\u6027\u80fd\u7684\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "method": "\u96c6\u6210\u9891\u7387\u611f\u77e5\u72b6\u6001\u7a7a\u95f4\u6a21\u5757(FASSM)\u548c\u591a\u5411\u611f\u77e5\u5377\u79ef(MDPConv)\uff0cFASSM\u5229\u7528\u5085\u91cc\u53f6\u53d8\u6362\u533a\u5206\u96e8\u7eb9\u548c\u56fe\u50cf\u9ad8\u9891\u7ec6\u8282\uff0cMDPConv\u901a\u8fc7\u6355\u83b7\u5404\u5411\u5f02\u6027\u68af\u5ea6\u7279\u5f81\u548c\u591a\u5206\u652f\u878d\u5408\u6765\u6062\u590d\u5c40\u90e8\u7ed3\u6784\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDeRainMamba\u5728PSNR\u548cSSIM\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u9700\u8981\u66f4\u5c11\u7684\u53c2\u6570\u548c\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u5728\u72b6\u6001\u7a7a\u95f4\u6846\u67b6\u4e2d\u7ed3\u5408\u9891\u7387\u57df\u5efa\u6a21\u548c\u7a7a\u95f4\u7ec6\u8282\u589e\u5f3a\u5bf9\u4e8e\u5355\u56fe\u50cf\u53bb\u96e8\u662f\u6709\u6548\u7684\u3002"}}
{"id": "2510.06751", "pdf": "https://arxiv.org/pdf/2510.06751", "abs": "https://arxiv.org/abs/2510.06751", "authors": ["Junhan Zhu", "Hesong Wang", "Mingluo Su", "Zefang Wang", "Huan Wang"], "title": "OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot", "categories": ["cs.CV"], "comment": null, "summary": "Large-scale text-to-image diffusion models, while powerful, suffer from prohibitive computational cost. Existing one-shot network pruning methods can hardly be directly applied to them due to the iterative denoising nature of diffusion models. To bridge the gap, this paper presents OBS-Diff, a novel one-shot pruning framework that enables accurate and training-free compression of large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff revitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex architectures of modern diffusion models and supporting diverse pruning granularity, including unstructured, N:M semi-structured, and structured (MHA heads and FFN neurons) sparsity; (ii) To align the pruning criteria with the iterative dynamics of the diffusion process, by examining the problem from an error-accumulation perspective, we propose a novel timestep-aware Hessian construction that incorporates a logarithmic-decrease weighting scheme, assigning greater importance to earlier timesteps to mitigate potential error accumulation; (iii) Furthermore, a computationally efficient group-wise sequential pruning strategy is proposed to amortize the expensive calibration process. Extensive experiments show that OBS-Diff achieves state-of-the-art one-shot pruning for diffusion models, delivering inference acceleration with minimal degradation in visual quality.", "AI": {"tldr": "OBS-Diff\u662f\u4e00\u79cd\u65b0\u9896\u7684\u4e00\u6b21\u6027\u526a\u679d\u6846\u67b6\uff0c\u7528\u4e8e\u51c6\u786e\u4e14\u65e0\u9700\u8bad\u7ec3\u5730\u538b\u7f29\u5927\u89c4\u6a21\u6587\u751f\u56fe\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u6539\u8fdb\u7684OBS\u7b97\u6cd5\u3001\u65f6\u95f4\u6b65\u611f\u77e5Hessian\u6784\u5efa\u548c\u5206\u7ec4\u987a\u5e8f\u526a\u679d\u7b56\u7565\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\u3002", "motivation": "\u5927\u89c4\u6a21\u6587\u751f\u56fe\u6269\u6563\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u73b0\u6709\u7684\u4e00\u6b21\u6027\u7f51\u7edc\u526a\u679d\u65b9\u6cd5\u7531\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8fed\u4ee3\u53bb\u566a\u7279\u6027\u800c\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u3002", "method": "\u6539\u8fdb\u7ecf\u5178OBS\u7b97\u6cd5\u9002\u5e94\u73b0\u4ee3\u6269\u6563\u6a21\u578b\u67b6\u6784\uff1b\u63d0\u51fa\u65f6\u95f4\u6b65\u611f\u77e5Hessian\u6784\u5efa\uff0c\u91c7\u7528\u5bf9\u6570\u9012\u51cf\u52a0\u6743\u65b9\u6848\uff1b\u8bbe\u8ba1\u8ba1\u7b97\u9ad8\u6548\u7684\u5206\u7ec4\u987a\u5e8f\u526a\u679d\u7b56\u7565\u3002", "result": "OBS-Diff\u5728\u6269\u6563\u6a21\u578b\u7684\u4e00\u6b21\u6027\u526a\u679d\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5728\u89c6\u89c9\u8d28\u91cf\u6700\u5c0f\u9000\u5316\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u63a8\u7406\u52a0\u901f\u3002", "conclusion": "OBS-Diff\u6210\u529f\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u6269\u6563\u6a21\u578b\u7684\u9ad8\u6548\u538b\u7f29\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.06757", "pdf": "https://arxiv.org/pdf/2510.06757", "abs": "https://arxiv.org/abs/2510.06757", "authors": ["Sheng Fu", "Junchao Zhang", "Kailun Yang"], "title": "Transforming Noise Distributions with Histogram Matching: Towards a Single Denoiser for All", "categories": ["cs.CV"], "comment": "12 pages", "summary": "Supervised Gaussian denoisers exhibit limited generalization when confronted with out-of-distribution noise, due to the diverse distributional characteristics of different noise types. To bridge this gap, we propose a histogram matching approach that transforms arbitrary noise towards a target Gaussian distribution with known intensity. Moreover, a mutually reinforcing cycle is established between noise transformation and subsequent denoising. This cycle progressively refines the noise to be converted, making it approximate the real noise, thereby enhancing the noise transformation effect and further improving the denoising performance. We tackle specific noise complexities: local histogram matching handles signal-dependent noise, intrapatch permutation processes channel-related noise, and frequency-domain histogram matching coupled with pixel-shuffle down-sampling breaks spatial correlation. By applying these transformations, a single Gaussian denoiser gains remarkable capability to handle various out-of-distribution noises, including synthetic noises such as Poisson, salt-and-pepper and repeating pattern noises, as well as complex real-world noises. Extensive experiments demonstrate the superior generalization and effectiveness of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u76f4\u65b9\u56fe\u5339\u914d\u65b9\u6cd5\uff0c\u5c06\u4efb\u610f\u566a\u58f0\u8f6c\u6362\u4e3a\u76ee\u6807\u9ad8\u65af\u5206\u5e03\uff0c\u901a\u8fc7\u566a\u58f0\u8f6c\u6362\u4e0e\u53bb\u566a\u7684\u76f8\u4e92\u589e\u5f3a\u5faa\u73af\uff0c\u4f7f\u5355\u4e2a\u9ad8\u65af\u53bb\u566a\u5668\u80fd\u591f\u5904\u7406\u5404\u79cd\u5206\u5e03\u5916\u566a\u58f0\u3002", "motivation": "\u76d1\u7763\u9ad8\u65af\u53bb\u566a\u5668\u5728\u5904\u7406\u5206\u5e03\u5916\u566a\u58f0\u65f6\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u56e0\u4e3a\u4e0d\u540c\u566a\u58f0\u7c7b\u578b\u5177\u6709\u4e0d\u540c\u7684\u5206\u5e03\u7279\u6027\u3002", "method": "\u4f7f\u7528\u76f4\u65b9\u56fe\u5339\u914d\u5c06\u4efb\u610f\u566a\u58f0\u8f6c\u6362\u4e3a\u5df2\u77e5\u5f3a\u5ea6\u7684\u76ee\u6807\u9ad8\u65af\u5206\u5e03\uff0c\u5efa\u7acb\u566a\u58f0\u8f6c\u6362\u4e0e\u53bb\u566a\u7684\u76f8\u4e92\u589e\u5f3a\u5faa\u73af\uff0c\u5e76\u9488\u5bf9\u7279\u5b9a\u566a\u58f0\u590d\u6742\u6027\u91c7\u7528\u5c40\u90e8\u76f4\u65b9\u56fe\u5339\u914d\u3001\u7247\u5185\u7f6e\u6362\u548c\u9891\u57df\u76f4\u65b9\u56fe\u5339\u914d\u7b49\u65b9\u6cd5\u3002", "result": "\u5355\u4e2a\u9ad8\u65af\u53bb\u566a\u5668\u83b7\u5f97\u4e86\u663e\u8457\u5904\u7406\u5404\u79cd\u5206\u5e03\u5916\u566a\u58f0\u7684\u80fd\u529b\uff0c\u5305\u62ec\u6cca\u677e\u566a\u58f0\u3001\u6912\u76d0\u566a\u58f0\u3001\u91cd\u590d\u6a21\u5f0f\u566a\u58f0\u4ee5\u53ca\u590d\u6742\u7684\u771f\u5b9e\u4e16\u754c\u566a\u58f0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2510.06827", "pdf": "https://arxiv.org/pdf/2510.06827", "abs": "https://arxiv.org/abs/2510.06827", "authors": ["Jaeseok Jeong", "Junho Kim", "Gayoung Lee", "Yunjey Choi", "Youngjung Uh"], "title": "StyleKeeper: Prevent Content Leakage using Negative Visual Query Guidance", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025; CVPRW AI4CC 2024 (Best Paper + Oral)", "summary": "In the domain of text-to-image generation, diffusion models have emerged as powerful tools. Recently, studies on visual prompting, where images are used as prompts, have enabled more precise control over style and content. However, existing methods often suffer from content leakage, where undesired elements of the visual style prompt are transferred along with the intended style. To address this issue, we 1) extend classifier-free guidance (CFG) to utilize swapping self-attention and propose 2) negative visual query guidance (NVQG) to reduce the transfer of unwanted contents. NVQG employs negative score by intentionally simulating content leakage scenarios that swap queries instead of key and values of self-attention layers from visual style prompts. This simple yet effective method significantly reduces content leakage. Furthermore, we provide careful solutions for using a real image as visual style prompts. Through extensive evaluation across various styles and text prompts, our method demonstrates superiority over existing approaches, reflecting the style of the references, and ensuring that resulting images match the text prompts. Our code is available \\href{https://github.com/naver-ai/StyleKeeper}{here}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8d1f\u89c6\u89c9\u67e5\u8be2\u5f15\u5bfc\uff08NVQG\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u5e76\u4ea4\u6362\u81ea\u6ce8\u610f\u529b\u5c42\u7684\u67e5\u8be2\u6765\u51cf\u5c11\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5185\u5bb9\u6cc4\u6f0f\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u63d0\u793a\u65b9\u6cd5\u5728\u63a7\u5236\u98ce\u683c\u65f6\u7ecf\u5e38\u51fa\u73b0\u5185\u5bb9\u6cc4\u6f0f\u95ee\u9898\uff0c\u5373\u89c6\u89c9\u98ce\u683c\u63d0\u793a\u4e2d\u4e0d\u9700\u8981\u7684\u5185\u5bb9\u5143\u7d20\u4f1a\u4e0e\u9884\u671f\u98ce\u683c\u4e00\u8d77\u88ab\u8f6c\u79fb\u3002", "method": "1) \u6269\u5c55\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\uff08CFG\uff09\u4ee5\u5229\u7528\u4ea4\u6362\u81ea\u6ce8\u610f\u529b\uff1b2) \u63d0\u51fa\u8d1f\u89c6\u89c9\u67e5\u8be2\u5f15\u5bfc\uff08NVQG\uff09\uff0c\u901a\u8fc7\u6545\u610f\u6a21\u62df\u5185\u5bb9\u6cc4\u6f0f\u573a\u666f\u6765\u4ea4\u6362\u81ea\u6ce8\u610f\u529b\u5c42\u7684\u67e5\u8be2\u800c\u975e\u952e\u548c\u503c\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u98ce\u683c\u548c\u6587\u672c\u63d0\u793a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u80fd\u51c6\u786e\u53cd\u6620\u53c2\u8003\u56fe\u50cf\u7684\u98ce\u683c\uff0c\u540c\u65f6\u786e\u4fdd\u751f\u6210\u56fe\u50cf\u4e0e\u6587\u672c\u63d0\u793a\u5339\u914d\u3002", "conclusion": "NVQG\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u51cf\u5c11\u5185\u5bb9\u6cc4\u6f0f\u95ee\u9898\uff0c\u5e76\u4e3a\u4f7f\u7528\u771f\u5b9e\u56fe\u50cf\u4f5c\u4e3a\u89c6\u89c9\u98ce\u683c\u63d0\u793a\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.06928", "pdf": "https://arxiv.org/pdf/2510.06928", "abs": "https://arxiv.org/abs/2510.06928", "authors": ["Ran Yi", "Teng Hu", "Zihan Su", "Lizhuang Ma"], "title": "IAR2: Improving Autoregressive Visual Generation with Semantic-Detail Associated Token Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Autoregressive models have emerged as a powerful paradigm for visual content creation, but often overlook the intrinsic structural properties of visual data. Our prior work, IAR, initiated a direction to address this by reorganizing the visual codebook based on embedding similarity, thereby improving generation robustness. However, it is constrained by the rigidity of pre-trained codebooks and the inaccuracies of hard, uniform clustering. To overcome these limitations, we propose IAR2, an advanced autoregressive framework that enables a hierarchical semantic-detail synthesis process. At the core of IAR2 is a novel Semantic-Detail Associated Dual Codebook, which decouples image representations into a semantic codebook for global semantic information and a detail codebook for fine-grained refinements. It expands the quantization capacity from a linear to a polynomial scale, significantly enhancing expressiveness. To accommodate this dual representation, we propose a Semantic-Detail Autoregressive Prediction scheme coupled with a Local-Context Enhanced Autoregressive Head, which performs hierarchical prediction-first the semantic token, then the detail token-while leveraging a local context window to enhance spatial coherence. Furthermore, for conditional generation, we introduce a Progressive Attention-Guided Adaptive CFG mechanism that dynamically modulates the guidance scale for each token based on its relevance to the condition and its temporal position in the generation sequence, improving conditional alignment without sacrificing realism. Extensive experiments demonstrate that IAR2 sets a new state-of-the-art for autoregressive image generation, achieving a FID of 1.50 on ImageNet. Our model not only surpasses previous methods in performance but also demonstrates superior computational efficiency, highlighting the effectiveness of our structured, coarse-to-fine generation strategy.", "AI": {"tldr": "IAR2\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u8bed\u4e49-\u7ec6\u8282\u5408\u6210\u7684\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49-\u7ec6\u8282\u5173\u8054\u53cc\u7801\u672c\u5c06\u56fe\u50cf\u8868\u793a\u89e3\u8026\u4e3a\u8bed\u4e49\u7801\u672c\u548c\u7ec6\u8282\u7801\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u81ea\u56de\u5f52\u6a21\u578b\u5ffd\u7565\u4e86\u89c6\u89c9\u6570\u636e\u7684\u5185\u5728\u7ed3\u6784\u7279\u6027\uff0c\u4e14\u53d7\u9650\u4e8e\u9884\u8bad\u7ec3\u7801\u672c\u7684\u521a\u6027\u548c\u786c\u805a\u7c7b\u7684\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u5206\u5c42\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u8bed\u4e49-\u7ec6\u8282\u5173\u8054\u53cc\u7801\u672c\u3001\u8bed\u4e49-\u7ec6\u8282\u81ea\u56de\u5f52\u9884\u6d4b\u65b9\u6848\u3001\u5c40\u90e8\u4e0a\u4e0b\u6587\u589e\u5f3a\u81ea\u56de\u5f52\u5934\uff0c\u4ee5\u53ca\u6e10\u8fdb\u6ce8\u610f\u529b\u5f15\u5bfc\u81ea\u9002\u5e94CFG\u673a\u5236\u3002", "result": "\u5728ImageNet\u4e0a\u8fbe\u5230FID 1.50\u7684\u65b0SOTA\uff0c\u4e0d\u4ec5\u6027\u80fd\u8d85\u8d8a\u5148\u524d\u65b9\u6cd5\uff0c\u8fd8\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "IAR2\u901a\u8fc7\u7ed3\u6784\u5316\u3001\u4ece\u7c97\u5230\u7ec6\u7684\u751f\u6210\u7b56\u7565\uff0c\u8bc1\u660e\u4e86\u5206\u5c42\u8bed\u4e49-\u7ec6\u8282\u5408\u6210\u5728\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.06967", "pdf": "https://arxiv.org/pdf/2510.06967", "abs": "https://arxiv.org/abs/2510.06967", "authors": ["Huanning Dong", "Fan Li", "Ping Kuang", "Jianwen Min"], "title": "Generating Surface for Text-to-3D using 2D Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in Text-to-3D modeling have shown significant potential for the creation of 3D content. However, due to the complex geometric shapes of objects in the natural world, generating 3D content remains a challenging task. Current methods either leverage 2D diffusion priors to recover 3D geometry, or train the model directly based on specific 3D representations. In this paper, we propose a novel method named DirectGaussian, which focuses on generating the surfaces of 3D objects represented by surfels. In DirectGaussian, we utilize conditional text generation models and the surface of a 3D object is rendered by 2D Gaussian splatting with multi-view normal and texture priors. For multi-view geometric consistency problems, DirectGaussian incorporates curvature constraints on the generated surface during optimization process. Through extensive experiments, we demonstrate that our framework is capable of achieving diverse and high-fidelity 3D content creation.", "AI": {"tldr": "\u63d0\u51faDirectGaussian\u65b9\u6cd5\uff0c\u901a\u8fc7\u6761\u4ef6\u6587\u672c\u751f\u6210\u6a21\u578b\u548c2D\u9ad8\u65af\u6cfc\u6e85\uff0c\u7ed3\u5408\u591a\u89c6\u89d2\u6cd5\u7ebf\u548c\u7eb9\u7406\u5148\u9a8c\uff0c\u751f\u6210\u57fa\u4e8esurfels\u76843D\u7269\u4f53\u8868\u9762\uff0c\u5e76\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u52a0\u5165\u66f2\u7387\u7ea6\u675f\u89e3\u51b3\u51e0\u4f55\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u81ea\u7136\u4e16\u754c\u4e2d\u7269\u4f53\u51e0\u4f55\u5f62\u72b6\u590d\u6742\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f7f\u75282D\u6269\u6563\u5148\u9a8c\u6062\u590d3D\u51e0\u4f55\uff0c\u8981\u4e48\u57fa\u4e8e\u7279\u5b9a3D\u8868\u793a\u76f4\u63a5\u8bad\u7ec3\u6a21\u578b\uff0c3D\u5185\u5bb9\u751f\u6210\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u6587\u672c\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc72D\u9ad8\u65af\u6cfc\u6e85\u6e32\u67d33D\u7269\u4f53\u8868\u9762\uff0c\u7ed3\u5408\u591a\u89c6\u89d2\u6cd5\u7ebf\u548c\u7eb9\u7406\u5148\u9a8c\uff0c\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u52a0\u5165\u66f2\u7387\u7ea6\u675f\u786e\u4fdd\u591a\u89c6\u89d2\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u591a\u6837\u5316\u548c\u9ad8\u4fdd\u771f\u5ea6\u76843D\u5185\u5bb9\u521b\u5efa\u3002", "conclusion": "DirectGaussian\u65b9\u6cd5\u57283D\u5185\u5bb9\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u5e76\u4fdd\u6301\u9ad8\u8d28\u91cf\u8f93\u51fa\u3002"}}
{"id": "2510.06973", "pdf": "https://arxiv.org/pdf/2510.06973", "abs": "https://arxiv.org/abs/2510.06973", "authors": ["Zhantao Yang", "Huangji Wang", "Ruili Feng", "Han Zhang", "Yuting Hu", "Shangwen Zhu", "Junyan Li", "Yu Liu", "Fan Cheng"], "title": "Addressing the ID-Matching Challenge in Long Video Captioning", "categories": ["cs.CV"], "comment": null, "summary": "Generating captions for long and complex videos is both critical and challenging, with significant implications for the growing fields of text-to-video generation and multi-modal understanding. One key challenge in long video captioning is accurately recognizing the same individuals who appear in different frames, which we refer to as the ID-Matching problem. Few prior works have focused on this important issue. Those that have, usually suffer from limited generalization and depend on point-wise matching, which limits their overall effectiveness. In this paper, unlike previous approaches, we build upon LVLMs to leverage their powerful priors. We aim to unlock the inherent ID-Matching capabilities within LVLMs themselves to enhance the ID-Matching performance of captions. Specifically, we first introduce a new benchmark for assessing the ID-Matching capabilities of video captions. Using this benchmark, we investigate LVLMs containing GPT-4o, revealing key insights that the performance of ID-Matching can be improved through two methods: 1) enhancing the usage of image information and 2) increasing the quantity of information of individual descriptions. Based on these insights, we propose a novel video captioning method called Recognizing Identities for Captioning Effectively (RICE). Extensive experiments including assessments of caption quality and ID-Matching performance, demonstrate the superiority of our approach. Notably, when implemented on GPT-4o, our RICE improves the precision of ID-Matching from 50% to 90% and improves the recall of ID-Matching from 15% to 80% compared to baseline. RICE makes it possible to continuously track different individuals in the captions of long videos.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86RICE\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5728ID\u5339\u914d\u80fd\u529b\u6765\u89e3\u51b3\u957f\u89c6\u9891\u5b57\u5e55\u4e2d\u7684\u8eab\u4efd\u8bc6\u522b\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86ID\u5339\u914d\u7684\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u3002", "motivation": "\u957f\u89c6\u9891\u5b57\u5e55\u751f\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\u662f\u51c6\u786e\u8bc6\u522b\u4e0d\u540c\u5e27\u4e2d\u51fa\u73b0\u7684\u540c\u4e00\u4eba\u7269\uff08ID\u5339\u914d\u95ee\u9898\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u6709\u9650\u4e14\u4f9d\u8d56\u70b9\u5bf9\u70b9\u5339\u914d\uff0c\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u57fa\u4e8eLVLMs\u6784\u5efaRICE\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u56fe\u50cf\u4fe1\u606f\u5229\u7528\u548c\u589e\u52a0\u4e2a\u4f53\u63cf\u8ff0\u4fe1\u606f\u91cf\u4e24\u79cd\u65b9\u5f0f\u6765\u63d0\u5347ID\u5339\u914d\u6027\u80fd\uff0c\u5e76\u5efa\u7acb\u4e86\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "\u5728GPT-4o\u4e0a\u5b9e\u73b0\uff0cRICE\u5c06ID\u5339\u914d\u7684\u7cbe\u786e\u5ea6\u4ece50%\u63d0\u5347\u523090%\uff0c\u53ec\u56de\u7387\u4ece15%\u63d0\u5347\u523080%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "RICE\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8ddf\u8e2a\u957f\u89c6\u9891\u5b57\u5e55\u4e2d\u4e0d\u540c\u4e2a\u4f53\u7684\u8fde\u7eed\u51fa\u73b0\uff0c\u4e3a\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u548c\u591a\u6a21\u6001\u7406\u89e3\u9886\u57df\u63d0\u4f9b\u4e86\u91cd\u8981\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.06988", "pdf": "https://arxiv.org/pdf/2510.06988", "abs": "https://arxiv.org/abs/2510.06988", "authors": ["Girolamo Macaluso", "Lorenzo Mandelli", "Mirko Bicchierai", "Stefano Berretti", "Andrew D. Bagdanov"], "title": "No MoCap Needed: Post-Training Motion Diffusion Models with Reinforcement Learning using Only Textual Prompts", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have recently advanced human motion generation, producing realistic and diverse animations from textual prompts. However, adapting these models to unseen actions or styles typically requires additional motion capture data and full retraining, which is costly and difficult to scale. We propose a post-training framework based on Reinforcement Learning that fine-tunes pretrained motion diffusion models using only textual prompts, without requiring any motion ground truth. Our approach employs a pretrained text-motion retrieval network as a reward signal and optimizes the diffusion policy with Denoising Diffusion Policy Optimization, effectively shifting the model's generative distribution toward the target domain without relying on paired motion data. We evaluate our method on cross-dataset adaptation and leave-one-out motion experiments using the HumanML3D and KIT-ML datasets across both latent- and joint-space diffusion architectures. Results from quantitative metrics and user studies show that our approach consistently improves the quality and diversity of generated motions, while preserving performance on the original distribution. Our approach is a flexible, data-efficient, and privacy-preserving solution for motion adaptation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u6587\u672c\u63d0\u793a\u5fae\u8c03\u9884\u8bad\u7ec3\u8fd0\u52a8\u6269\u6563\u6a21\u578b\uff0c\u65e0\u9700\u8fd0\u52a8\u771f\u503c\u6570\u636e\uff0c\u5b9e\u73b0\u8de8\u6570\u636e\u96c6\u548c\u7559\u4e00\u8fd0\u52a8\u9002\u5e94\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u9002\u5e94\u672a\u89c1\u52a8\u4f5c\u6216\u98ce\u683c\u901a\u5e38\u9700\u8981\u989d\u5916\u8fd0\u52a8\u6355\u6349\u6570\u636e\u548c\u5b8c\u6574\u91cd\u8bad\u7ec3\uff0c\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u6587\u672c-\u8fd0\u52a8\u68c0\u7d22\u7f51\u7edc\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\uff0c\u901a\u8fc7Denoising Diffusion Policy Optimization\u4f18\u5316\u6269\u6563\u7b56\u7565\uff0c\u65e0\u9700\u914d\u5bf9\u8fd0\u52a8\u6570\u636e\u5373\u53ef\u5c06\u751f\u6210\u5206\u5e03\u8f6c\u5411\u76ee\u6807\u57df\u3002", "result": "\u5728HumanML3D\u548cKIT-ML\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u6307\u6807\u548c\u7528\u6237\u7814\u7a76\u4e2d\u4e00\u81f4\u63d0\u9ad8\u4e86\u751f\u6210\u8fd0\u52a8\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u5206\u5e03\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8fd0\u52a8\u9002\u5e94\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u6570\u636e\u9ad8\u6548\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.07008", "pdf": "https://arxiv.org/pdf/2510.07008", "abs": "https://arxiv.org/abs/2510.07008", "authors": ["Gianmarco Perantoni", "Giulio Weikmann", "Lorenzo Bruzzone"], "title": "Bayesian Modelling of Multi-Year Crop Type Classification Using Deep Neural Networks and Hidden Markov Models", "categories": ["cs.CV"], "comment": "5 pages, 1 figure, accepted conference paper at IEEE International   Geoscience and Remote Sensing Symposium, 7-12 July 2024, Athens, Greece", "summary": "The temporal consistency of yearly land-cover maps is of great importance to model the evolution and change of the land cover over the years. In this paper, we focus the attention on a novel approach to classification of yearly satellite image time series (SITS) that combines deep learning with Bayesian modelling, using Hidden Markov Models (HMMs) integrated with Transformer Encoder (TE) based DNNs. The proposed approach aims to capture both i) intricate temporal correlations in yearly SITS and ii) specific patterns in multiyear crop type sequences. It leverages the cascade classification of an HMM layer built on top of the TE, discerning consistent yearly crop-type sequences. Validation on a multiyear crop type classification dataset spanning 47 crop types and six years of Sentinel-2 acquisitions demonstrates the importance of modelling temporal consistency in the predicted labels. HMMs enhance the overall performance and F1 scores, emphasising the effectiveness of the proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u8d1d\u53f6\u65af\u5efa\u6a21\u7684\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u4e0eTransformer\u7f16\u7801\u5668\u8fdb\u884c\u5e74\u5ea6\u536b\u661f\u56fe\u50cf\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\uff0c\u65e8\u5728\u6355\u6349\u590d\u6742\u7684\u65f6\u95f4\u76f8\u5173\u6027\u548c\u591a\u5e74\u4f5c\u7269\u7c7b\u578b\u5e8f\u5217\u6a21\u5f0f\u3002", "motivation": "\u5e74\u5ea6\u571f\u5730\u8986\u76d6\u5730\u56fe\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u5bf9\u4e8e\u5efa\u6a21\u571f\u5730\u8986\u76d6\u7684\u6f14\u53d8\u548c\u53d8\u5316\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u89e3\u51b3\u5e74\u5ea6\u536b\u661f\u56fe\u50cf\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4e2d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "\u5c06\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u5c42\u6784\u5efa\u5728Transformer\u7f16\u7801\u5668\u4e4b\u4e0a\uff0c\u5229\u7528HMM\u8fdb\u884c\u7ea7\u8054\u5206\u7c7b\uff0c\u8bc6\u522b\u4e00\u81f4\u7684\u5e74\u5ea6\u4f5c\u7269\u7c7b\u578b\u5e8f\u5217\u3002", "result": "\u5728\u5305\u542b47\u79cd\u4f5c\u7269\u7c7b\u578b\u548c6\u5e74Sentinel-2\u91c7\u96c6\u6570\u636e\u7684\u591a\u5e74\u4f5c\u7269\u7c7b\u578b\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cHMM\u63d0\u9ad8\u4e86\u6574\u4f53\u6027\u80fd\u548cF1\u5206\u6570\u3002", "conclusion": "\u5efa\u6a21\u9884\u6d4b\u6807\u7b7e\u4e2d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u975e\u5e38\u91cd\u8981\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002"}}
{"id": "2510.07129", "pdf": "https://arxiv.org/pdf/2510.07129", "abs": "https://arxiv.org/abs/2510.07129", "authors": ["Sarah Cechnicka", "Matthew Baugh", "Weitong Zhang", "Mischa Dombrowski", "Zhe Li", "Johannes C. Paetzold", "Candice Roufosse", "Bernhard Kainz"], "title": "Graph Conditioned Diffusion for Controllable Histopathology Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in Diffusion Probabilistic Models (DPMs) have set new standards in high-quality image synthesis. Yet, controlled generation remains challenging, particularly in sensitive areas such as medical imaging. Medical images feature inherent structure such as consistent spatial arrangement, shape or texture, all of which are critical for diagnosis. However, existing DPMs operate in noisy latent spaces that lack semantic structure and strong priors, making it difficult to ensure meaningful control over generated content. To address this, we propose graph-based object-level representations for Graph-Conditioned-Diffusion. Our approach generates graph nodes corresponding to each major structure in the image, encapsulating their individual features and relationships. These graph representations are processed by a transformer module and integrated into a diffusion model via the text-conditioning mechanism, enabling fine-grained control over generation. We evaluate this approach using a real-world histopathology use case, demonstrating that our generated data can reliably substitute for annotated patient data in downstream segmentation tasks. The code is available here.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u7684\u5bf9\u8c61\u7ea7\u8868\u793a\u65b9\u6cd5Graph-Conditioned-Diffusion\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u751f\u6210\uff0c\u901a\u8fc7\u56fe\u8282\u70b9\u8868\u793a\u4e3b\u8981\u7ed3\u6784\u7279\u5f81\u548c\u5173\u7cfb\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6982\u7387\u6a21\u578b\u5728\u566a\u58f0\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7f3a\u4e4f\u8bed\u4e49\u7ed3\u6784\u548c\u5f3a\u5148\u9a8c\uff0c\u96be\u4ee5\u5728\u533b\u5b66\u56fe\u50cf\u7b49\u654f\u611f\u9886\u57df\u5b9e\u73b0\u6709\u610f\u4e49\u7684\u63a7\u5236\u751f\u6210\u3002\u533b\u5b66\u56fe\u50cf\u5177\u6709\u56fa\u6709\u7684\u7ed3\u6784\u7279\u5f81\uff0c\u5bf9\u8bca\u65ad\u81f3\u5173\u91cd\u8981\u3002", "method": "\u751f\u6210\u5bf9\u5e94\u56fe\u50cf\u4e2d\u6bcf\u4e2a\u4e3b\u8981\u7ed3\u6784\u7684\u56fe\u8282\u70b9\uff0c\u5c01\u88c5\u4e2a\u4f53\u7279\u5f81\u548c\u5173\u7cfb\uff1b\u4f7f\u7528transformer\u6a21\u5757\u5904\u7406\u56fe\u8868\u793a\uff0c\u901a\u8fc7\u6587\u672c\u6761\u4ef6\u673a\u5236\u96c6\u6210\u5230\u6269\u6563\u6a21\u578b\u4e2d\u3002", "result": "\u5728\u771f\u5b9e\u7ec4\u7ec7\u75c5\u7406\u5b66\u7528\u4f8b\u4e2d\u8bc4\u4f30\uff0c\u751f\u6210\u7684\u6570\u636e\u53ef\u4ee5\u53ef\u9760\u5730\u66ff\u4ee3\u6807\u6ce8\u7684\u60a3\u8005\u6570\u636e\u7528\u4e8e\u4e0b\u6e38\u5206\u5272\u4efb\u52a1\u3002", "conclusion": "\u56fe\u6761\u4ef6\u6269\u6563\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u533b\u5b66\u56fe\u50cf\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u751f\u6210\uff0c\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u53ef\u6709\u6548\u66ff\u4ee3\u771f\u5b9e\u6807\u6ce8\u6570\u636e\u3002"}}
{"id": "2510.07190", "pdf": "https://arxiv.org/pdf/2510.07190", "abs": "https://arxiv.org/abs/2510.07190", "authors": ["Yihao Zhi", "Chenghong Li", "Hongjie Liao", "Xihe Yang", "Zhengwentai Sun", "Jiahao Chang", "Xiaodong Cun", "Wensen Feng", "Xiaoguang Han"], "title": "MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis", "categories": ["cs.CV"], "comment": "Accepted by SIGGRAPH Asia 2025 conference track", "summary": "Recent breakthroughs in video generation, powered by large-scale datasets and diffusion techniques, have shown that video diffusion models can function as implicit 4D novel view synthesizers. Nevertheless, current methods primarily concentrate on redirecting camera trajectory within the front view while struggling to generate 360-degree viewpoint changes. In this paper, we focus on human-centric subdomain and present MV-Performer, an innovative framework for creating synchronized novel view videos from monocular full-body captures. To achieve a 360-degree synthesis, we extensively leverage the MVHumanNet dataset and incorporate an informative condition signal. Specifically, we use the camera-dependent normal maps rendered from oriented partial point clouds, which effectively alleviate the ambiguity between seen and unseen observations. To maintain synchronization in the generated videos, we propose a multi-view human-centric video diffusion model that fuses information from the reference video, partial rendering, and different viewpoints. Additionally, we provide a robust inference procedure for in-the-wild video cases, which greatly mitigates the artifacts induced by imperfect monocular depth estimation. Extensive experiments on three datasets demonstrate our MV-Performer's state-of-the-art effectiveness and robustness, setting a strong model for human-centric 4D novel view synthesis.", "AI": {"tldr": "MV-Performer\u662f\u4e00\u4e2a\u7528\u4e8e\u4ece\u5355\u76ee\u5168\u8eab\u6355\u6349\u751f\u6210\u540c\u6b65\u591a\u89c6\u89d2\u89c6\u9891\u7684\u521b\u65b0\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u4eba\u4f53\u4e2d\u5fc3\u7684\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u80fd\u591f\u5b9e\u73b0360\u5ea6\u89c6\u89d2\u53d8\u5316\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u524d\u89c6\u89d2\u7684\u76f8\u673a\u8f68\u8ff9\u91cd\u5b9a\u5411\uff0c\u96be\u4ee5\u751f\u6210360\u5ea6\u89c6\u89d2\u53d8\u5316\uff0c\u7279\u522b\u662f\u5728\u4eba\u4f53\u4e2d\u5fc3\u9886\u57df\u3002", "method": "\u4f7f\u7528MVHumanNet\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u76f8\u673a\u76f8\u5173\u6cd5\u7ebf\u56fe\u4f5c\u4e3a\u6761\u4ef6\u4fe1\u53f7\uff0c\u63d0\u51fa\u591a\u89c6\u89d2\u4eba\u4f53\u4e2d\u5fc3\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u878d\u5408\u53c2\u8003\u89c6\u9891\u3001\u90e8\u5206\u6e32\u67d3\u548c\u4e0d\u540c\u89c6\u89d2\u7684\u4fe1\u606f\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMV-Performer\u5728\u4eba\u4f53\u4e2d\u5fc34D\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u9762\u5177\u6709\u6700\u5148\u8fdb\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "MV-Performer\u4e3a\u4eba\u4f53\u4e2d\u5fc34D\u65b0\u89c6\u89d2\u5408\u6210\u5efa\u7acb\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6a21\u578b\u57fa\u51c6\u3002"}}
{"id": "2510.07206", "pdf": "https://arxiv.org/pdf/2510.07206", "abs": "https://arxiv.org/abs/2510.07206", "authors": ["Shirin Shoushtari", "Yi Wang", "Xiao Shi", "M. Salman Asif", "Ulugbek S. Kamilov"], "title": "EigenScore: OOD Detection using Covariance in Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Out-of-distribution (OOD) detection is critical for the safe deployment of machine learning systems in safety-sensitive domains. Diffusion models have recently emerged as powerful generative models, capable of capturing complex data distributions through iterative denoising. Building on this progress, recent work has explored their potential for OOD detection. We propose EigenScore, a new OOD detection method that leverages the eigenvalue spectrum of the posterior covariance induced by a diffusion model. We argue that posterior covariance provides a consistent signal of distribution shift, leading to larger trace and leading eigenvalues on OOD inputs, yielding a clear spectral signature. We further provide analysis explicitly linking posterior covariance to distribution mismatch, establishing it as a reliable signal for OOD detection. To ensure tractability, we adopt a Jacobian-free subspace iteration method to estimate the leading eigenvalues using only forward evaluations of the denoiser. Empirically, EigenScore achieves SOTA performance, with up to 5% AUROC improvement over the best baseline. Notably, it remains robust in near-OOD settings such as CIFAR-10 vs CIFAR-100, where existing diffusion-based methods often fail.", "AI": {"tldr": "EigenScore\u662f\u4e00\u79cd\u65b0\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u8bf1\u5bfc\u7684\u540e\u9a8c\u534f\u65b9\u5dee\u77e9\u9635\u7684\u7279\u5f81\u503c\u8c31\u6765\u68c0\u6d4b\u5206\u5e03\u5916\u6570\u636e\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u53ef\u884c\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "OOD\u68c0\u6d4b\u5bf9\u4e8e\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u5728\u5b89\u5168\u654f\u611f\u9886\u57df\u7684\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u5f3a\u5927\u7684\u751f\u6210\u6a21\u578b\uff0c\u80fd\u591f\u6355\u6349\u590d\u6742\u7684\u6570\u636e\u5206\u5e03\uff0c\u4f46\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\u5728\u8fd1OOD\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faEigenScore\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u540e\u9a8c\u534f\u65b9\u5dee\u77e9\u9635\u7684\u7279\u5f81\u503c\u8c31\u4f5c\u4e3a\u5206\u5e03\u504f\u79fb\u7684\u6307\u6807\u3002\u91c7\u7528\u65e0\u96c5\u53ef\u6bd4\u77e9\u9635\u7684\u5b50\u7a7a\u95f4\u8fed\u4ee3\u65b9\u6cd5\u4ec5\u901a\u8fc7\u53bb\u566a\u5668\u7684\u524d\u5411\u8bc4\u4f30\u6765\u4f30\u8ba1\u4e3b\u5bfc\u7279\u5f81\u503c\uff0c\u786e\u4fdd\u8ba1\u7b97\u53ef\u884c\u6027\u3002", "result": "EigenScore\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0cAUROC\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u9ad8\u4e865%\u3002\u5728CIFAR-10 vs CIFAR-100\u7b49\u8fd1OOD\u573a\u666f\u4e2d\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u800c\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u5f80\u5f80\u5931\u8d25\u3002", "conclusion": "\u540e\u9a8c\u534f\u65b9\u5dee\u63d0\u4f9b\u4e86\u5206\u5e03\u504f\u79fb\u7684\u4e00\u81f4\u4fe1\u53f7\uff0cEigenScore\u901a\u8fc7\u5229\u7528\u8fd9\u4e00\u4fe1\u53f7\u5b9e\u73b0\u4e86\u6709\u6548\u7684OOD\u68c0\u6d4b\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u8fd1OOD\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.07217", "pdf": "https://arxiv.org/pdf/2510.07217", "abs": "https://arxiv.org/abs/2510.07217", "authors": ["Wen Ye", "Zhaocheng Liu", "Yuwei Gui", "Tingyu Yuan", "Yunyue Su", "Bowen Fang", "Chaoyang Zhao", "Qiang Liu", "Liang Wang"], "title": "GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": "30 pages, 21 figures, accepted to EMNLP 2025 findings", "summary": "Text-to-image synthesis has made remarkable progress, yet accurately interpreting complex and lengthy prompts remains challenging, often resulting in semantic inconsistencies and missing details. Existing solutions, such as fine-tuning, are model-specific and require training, while prior automatic prompt optimization (APO) approaches typically lack systematic error analysis and refinement strategies, resulting in limited reliability and effectiveness. Meanwhile, test-time scaling methods operate on fixed prompts and on noise or sample numbers, limiting their interpretability and adaptability. To solve these, we introduce a flexible and efficient test-time prompt optimization strategy that operates directly on the input text. We propose a plug-and-play multi-agent system called GenPilot, integrating error analysis, clustering-based adaptive exploration, fine-grained verification, and a memory module for iterative optimization. Our approach is model-agnostic, interpretable, and well-suited for handling long and complex prompts. Simultaneously, we summarize the common patterns of errors and the refinement strategy, offering more experience and encouraging further exploration. Experiments on DPG-bench and Geneval with improvements of up to 16.9% and 5.7% demonstrate the strong capability of our methods in enhancing the text and image consistency and structural coherence of generated images, revealing the effectiveness of our test-time prompt optimization strategy. The code is available at https://github.com/27yw/GenPilot.", "AI": {"tldr": "\u63d0\u51faGenPilot\uff0c\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u6d4b\u8bd5\u65f6\u63d0\u793a\u4f18\u5316\uff0c\u901a\u8fc7\u9519\u8bef\u5206\u6790\u3001\u805a\u7c7b\u81ea\u9002\u5e94\u63a2\u7d22\u3001\u7ec6\u7c92\u5ea6\u9a8c\u8bc1\u548c\u5185\u5b58\u6a21\u5757\u8fed\u4ee3\u4f18\u5316\uff0c\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u4e00\u81f4\u6027\u548c\u7ed3\u6784\u8fde\u8d2f\u6027\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u5728\u5904\u7406\u590d\u6742\u957f\u63d0\u793a\u65f6\u5b58\u5728\u8bed\u4e49\u4e0d\u4e00\u81f4\u548c\u7ec6\u8282\u7f3a\u5931\u95ee\u9898\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5982\u5fae\u8c03\u9700\u8981\u8bad\u7ec3\u4e14\u6a21\u578b\u7279\u5b9a\uff0c\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u9519\u8bef\u5206\u6790\u548c\u4f18\u5316\u7b56\u7565\uff0c\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\u5728\u56fa\u5b9a\u63d0\u793a\u4e0a\u64cd\u4f5c\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faGenPilot\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u5305\u542b\u9519\u8bef\u5206\u6790\u3001\u805a\u7c7b\u81ea\u9002\u5e94\u63a2\u7d22\u3001\u7ec6\u7c92\u5ea6\u9a8c\u8bc1\u548c\u5185\u5b58\u6a21\u5757\uff0c\u76f4\u63a5\u5728\u8f93\u5165\u6587\u672c\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u65f6\u63d0\u793a\u4f18\u5316\uff0c\u65e0\u9700\u8bad\u7ec3\u4e14\u6a21\u578b\u65e0\u5173\u3002", "result": "\u5728DPG-bench\u548cGeneval\u6570\u636e\u96c6\u4e0a\u5206\u522b\u63d0\u534716.9%\u548c5.7%\uff0c\u663e\u8457\u589e\u5f3a\u751f\u6210\u56fe\u50cf\u7684\u6587\u672c\u4e00\u81f4\u6027\u548c\u7ed3\u6784\u8fde\u8d2f\u6027\u3002", "conclusion": "GenPilot\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u63d0\u793a\u4f18\u5316\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u957f\u63d0\u793a\uff0c\u603b\u7ed3\u9519\u8bef\u6a21\u5f0f\u548c\u4f18\u5316\u7b56\u7565\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u7ecf\u9a8c\u3002"}}
{"id": "2510.07249", "pdf": "https://arxiv.org/pdf/2510.07249", "abs": "https://arxiv.org/abs/2510.07249", "authors": ["Jiaben Chen", "Zixin Wang", "Ailing Zeng", "Yang Fu", "Xueyang Yu", "Siyuan Cen", "Julian Tanke", "Yihang Chen", "Koichi Saito", "Yuki Mitsufuji", "Chuang Gan"], "title": "TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video Generation", "categories": ["cs.CV"], "comment": "Project page: https://talkcuts.github.io/", "summary": "In this work, we present TalkCuts, a large-scale dataset designed to facilitate the study of multi-shot human speech video generation. Unlike existing datasets that focus on single-shot, static viewpoints, TalkCuts offers 164k clips totaling over 500 hours of high-quality human speech videos with diverse camera shots, including close-up, half-body, and full-body views. The dataset includes detailed textual descriptions, 2D keypoints and 3D SMPL-X motion annotations, covering over 10k identities, enabling multimodal learning and evaluation. As a first attempt to showcase the value of the dataset, we present Orator, an LLM-guided multi-modal generation framework as a simple baseline, where the language model functions as a multi-faceted director, orchestrating detailed specifications for camera transitions, speaker gesticulations, and vocal modulation. This architecture enables the synthesis of coherent long-form videos through our integrated multi-modal video generation module. Extensive experiments in both pose-guided and audio-driven settings show that training on TalkCuts significantly enhances the cinematographic coherence and visual appeal of generated multi-shot speech videos. We believe TalkCuts provides a strong foundation for future work in controllable, multi-shot speech video generation and broader multimodal learning.", "AI": {"tldr": "TalkCuts\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u955c\u5934\u4eba\u7c7b\u8bed\u97f3\u89c6\u9891\u6570\u636e\u96c6\uff0c\u5305\u542b164k\u4e2a\u7247\u6bb5\u3001500+\u5c0f\u65f6\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u6db5\u76d6\u591a\u79cd\u62cd\u6444\u89d2\u5ea6\u548c\u8be6\u7ec6\u6807\u6ce8\u3002\u4f5c\u8005\u8fd8\u63d0\u51fa\u4e86Orator\u6846\u67b6\u4f5c\u4e3a\u57fa\u7ebf\uff0c\u5c55\u793a\u8be5\u6570\u636e\u96c6\u5728\u591a\u955c\u5934\u8bed\u97f3\u89c6\u9891\u751f\u6210\u4e2d\u7684\u4ef7\u503c\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u5355\u955c\u5934\u9759\u6001\u89c6\u89d2\uff0c\u7f3a\u4e4f\u591a\u955c\u5934\u3001\u52a8\u6001\u89c6\u89d2\u7684\u8bed\u97f3\u89c6\u9891\u6570\u636e\uff0c\u9650\u5236\u4e86\u591a\u955c\u5934\u8bed\u97f3\u89c6\u9891\u751f\u6210\u7684\u7814\u7a76\u3002", "method": "\u6784\u5efaTalkCuts\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51faOrator\u6846\u67b6\u2014\u2014\u4e00\u4e2aLLM\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u751f\u6210\u6846\u67b6\uff0c\u5176\u4e2d\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5bfc\u6f14\u534f\u8c03\u6444\u50cf\u673a\u8f6c\u6362\u3001\u624b\u52bf\u548c\u8bed\u97f3\u8c03\u5236\u3002", "result": "\u5728\u59ff\u6001\u5f15\u5bfc\u548c\u97f3\u9891\u9a71\u52a8\u8bbe\u7f6e\u4e0b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u5728TalkCuts\u4e0a\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u591a\u955c\u5934\u8bed\u97f3\u89c6\u9891\u7684\u7535\u5f71\u8fde\u8d2f\u6027\u548c\u89c6\u89c9\u5438\u5f15\u529b\u3002", "conclusion": "TalkCuts\u4e3a\u53ef\u63a7\u591a\u955c\u5934\u8bed\u97f3\u89c6\u9891\u751f\u6210\u548c\u66f4\u5e7f\u6cdb\u7684\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2510.07310", "pdf": "https://arxiv.org/pdf/2510.07310", "abs": "https://arxiv.org/abs/2510.07310", "authors": ["Siyoon Jin", "Seongchan Kim", "Dahyun Chung", "Jaeho Lee", "Hyunwook Choi", "Jisu Nam", "Jiyoung Kim", "Seungryong Kim"], "title": "MATRIX: Mask Track Alignment for Interaction-aware Video Generation", "categories": ["cs.CV"], "comment": "Project Page is available at: https://cvlab-kaist.github.io/MATRIX/", "summary": "Video DiTs have advanced video generation, yet they still struggle to model multi-instance or subject-object interactions. This raises a key question: How do these models internally represent interactions? To answer this, we curate MATRIX-11K, a video dataset with interaction-aware captions and multi-instance mask tracks. Using this dataset, we conduct a systematic analysis that formalizes two perspectives of video DiTs: semantic grounding, via video-to-text attention, which evaluates whether noun and verb tokens capture instances and their relations; and semantic propagation, via video-to-video attention, which assesses whether instance bindings persist across frames. We find both effects concentrate in a small subset of interaction-dominant layers. Motivated by this, we introduce MATRIX, a simple and effective regularization that aligns attention in specific layers of video DiTs with multi-instance mask tracks from the MATRIX-11K dataset, enhancing both grounding and propagation. We further propose InterGenEval, an evaluation protocol for interaction-aware video generation. In experiments, MATRIX improves both interaction fidelity and semantic alignment while reducing drift and hallucination. Extensive ablations validate our design choices. Codes and weights will be released.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u89c6\u9891DiTs\u5728\u5efa\u6a21\u591a\u5b9e\u4f8b\u548c\u4e3b\u4f53-\u5ba2\u4f53\u4ea4\u4e92\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86MATRIX-11K\u6570\u636e\u96c6\u548cMATRIX\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5bf9\u9f50\u589e\u5f3a\u4ea4\u4e92\u4fdd\u771f\u5ea6\u548c\u8bed\u4e49\u5bf9\u9f50\u3002", "motivation": "\u5f53\u524d\u89c6\u9891DiTs\u5728\u591a\u5b9e\u4f8b\u6216\u4e3b\u4f53-\u5ba2\u4f53\u4ea4\u4e92\u5efa\u6a21\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u7406\u89e3\u8fd9\u4e9b\u6a21\u578b\u5185\u90e8\u5982\u4f55\u8868\u793a\u4ea4\u4e92\u5173\u7cfb\u3002", "method": "\u6784\u5efaMATRIX-11K\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u5206\u6790\u89c6\u9891DiTs\u7684\u8bed\u4e49\u57fa\u7840\u548c\u8bed\u4e49\u4f20\u64ad\u7279\u6027\uff0c\u63d0\u51faMATRIX\u6b63\u5219\u5316\u65b9\u6cd5\u5bf9\u9f50\u6ce8\u610f\u529b\u4e0e\u591a\u5b9e\u4f8b\u63a9\u7801\u8f68\u8ff9\u3002", "result": "MATRIX\u65b9\u6cd5\u63d0\u9ad8\u4e86\u4ea4\u4e92\u4fdd\u771f\u5ea6\u548c\u8bed\u4e49\u5bf9\u9f50\uff0c\u51cf\u5c11\u4e86\u6f02\u79fb\u548c\u5e7b\u89c9\u73b0\u8c61\u3002", "conclusion": "\u901a\u8fc7\u6ce8\u610f\u529b\u5bf9\u9f50\u7684\u7b80\u5355\u6b63\u5219\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u89c6\u9891DiTs\u7684\u4ea4\u4e92\u5efa\u6a21\u80fd\u529b\uff0c\u4e3a\u4ea4\u4e92\u611f\u77e5\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc4\u4f30\u534f\u8bae\u3002"}}
{"id": "2510.07313", "pdf": "https://arxiv.org/pdf/2510.07313", "abs": "https://arxiv.org/abs/2510.07313", "authors": ["Zezhong Qian", "Xiaowei Chi", "Yuming Li", "Shizun Wang", "Zhiyuan Qin", "Xiaozhu Ju", "Sirui Han", "Shanghang Zhang"], "title": "WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Wrist-view observations are crucial for VLA models as they capture fine-grained hand-object interactions that directly enhance manipulation performance. Yet large-scale datasets rarely include such recordings, resulting in a substantial gap between abundant anchor views and scarce wrist views. Existing world models cannot bridge this gap, as they require a wrist-view first frame and thus fail to generate wrist-view videos from anchor views alone. Amid this gap, recent visual geometry models such as VGGT emerge with geometric and cross-view priors that make it possible to address extreme viewpoint shifts. Inspired by these insights, we propose WristWorld, the first 4D world model that generates wrist-view videos solely from anchor views. WristWorld operates in two stages: (i) Reconstruction, which extends VGGT and incorporates our Spatial Projection Consistency (SPC) Loss to estimate geometrically consistent wrist-view poses and 4D point clouds; (ii) Generation, which employs our video generation model to synthesize temporally coherent wrist-view videos from the reconstructed perspective. Experiments on Droid, Calvin, and Franka Panda demonstrate state-of-the-art video generation with superior spatial consistency, while also improving VLA performance, raising the average task completion length on Calvin by 3.81% and closing 42.4% of the anchor-wrist view gap.", "AI": {"tldr": "\u63d0\u51fa\u4e86WristWorld\uff0c\u9996\u4e2a\u4ec5\u4ece\u951a\u70b9\u89c6\u89d2\u751f\u6210\u8155\u90e8\u89c6\u89d2\u89c6\u9891\u76844D\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u51e0\u4f55\u91cd\u5efa\u548c\u89c6\u9891\u751f\u6210\u4e24\u9636\u6bb5\u65b9\u6cd5\u89e3\u51b3\u951a\u70b9-\u8155\u90e8\u89c6\u89d2\u5dee\u8ddd\u95ee\u9898\u3002", "motivation": "\u8155\u90e8\u89c6\u89d2\u89c2\u5bdf\u5bf9VLA\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5927\u89c4\u6a21\u6570\u636e\u96c6\u5f88\u5c11\u5305\u542b\u6b64\u7c7b\u8bb0\u5f55\uff0c\u5bfc\u81f4\u951a\u70b9\u89c6\u89d2\u4e30\u5bcc\u800c\u8155\u90e8\u89c6\u89d2\u7a00\u7f3a\u7684\u663e\u8457\u5dee\u8ddd\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u91cd\u5efa\u9636\u6bb5\u6269\u5c55VGGT\u5e76\u5f15\u5165\u7a7a\u95f4\u6295\u5f71\u4e00\u81f4\u6027\u635f\u5931\u6765\u4f30\u8ba1\u51e0\u4f55\u4e00\u81f4\u7684\u8155\u90e8\u89c6\u89d2\u59ff\u6001\u548c4D\u70b9\u4e91\uff1b\u751f\u6210\u9636\u6bb5\u4f7f\u7528\u89c6\u9891\u751f\u6210\u6a21\u578b\u4ece\u91cd\u5efa\u89c6\u89d2\u5408\u6210\u65f6\u5e8f\u8fde\u8d2f\u7684\u8155\u90e8\u89c6\u89d2\u89c6\u9891\u3002", "result": "\u5728Droid\u3001Calvin\u548cFranka Panda\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u89c6\u9891\u751f\u6210\u6548\u679c\uff0c\u5177\u6709\u4f18\u8d8a\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86VLA\u6027\u80fd\uff0c\u5728Calvin\u4e0a\u5e73\u5747\u4efb\u52a1\u5b8c\u6210\u957f\u5ea6\u63d0\u53473.81%\uff0c\u586b\u8865\u4e8642.4%\u7684\u951a\u70b9-\u8155\u90e8\u89c6\u89d2\u5dee\u8ddd\u3002", "conclusion": "WristWorld\u6210\u529f\u89e3\u51b3\u4e86\u951a\u70b9\u89c6\u89d2\u5230\u8155\u90e8\u89c6\u89d2\u7684\u8f6c\u6362\u95ee\u9898\uff0c\u4e3aVLA\u6a21\u578b\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u8155\u90e8\u89c6\u89d2\u89c2\u5bdf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u64cd\u4f5c\u6027\u80fd\u3002"}}
{"id": "2510.07316", "pdf": "https://arxiv.org/pdf/2510.07316", "abs": "https://arxiv.org/abs/2510.07316", "authors": ["Gangwei Xu", "Haotong Lin", "Hongcheng Luo", "Xianqi Wang", "Jingfeng Yao", "Lianghui Zhu", "Yuechuan Pu", "Cheng Chi", "Haiyang Sun", "Bing Wang", "Guang Chen", "Hangjun Ye", "Sida Peng", "Xin Yang"], "title": "Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers", "categories": ["cs.CV"], "comment": "NeurIPS 2025. Project page: https://pixel-perfect-depth.github.io/", "summary": "This paper presents Pixel-Perfect Depth, a monocular depth estimation model based on pixel-space diffusion generation that produces high-quality, flying-pixel-free point clouds from estimated depth maps. Current generative depth estimation models fine-tune Stable Diffusion and achieve impressive performance. However, they require a VAE to compress depth maps into latent space, which inevitably introduces \\textit{flying pixels} at edges and details. Our model addresses this challenge by directly performing diffusion generation in the pixel space, avoiding VAE-induced artifacts. To overcome the high complexity associated with pixel-space generation, we introduce two novel designs: 1) Semantics-Prompted Diffusion Transformers (SP-DiT), which incorporate semantic representations from vision foundation models into DiT to prompt the diffusion process, thereby preserving global semantic consistency while enhancing fine-grained visual details; and 2) Cascade DiT Design that progressively increases the number of tokens to further enhance efficiency and accuracy. Our model achieves the best performance among all published generative models across five benchmarks, and significantly outperforms all other models in edge-aware point cloud evaluation.", "AI": {"tldr": "\u63d0\u51faPixel-Perfect Depth\u6a21\u578b\uff0c\u901a\u8fc7\u50cf\u7d20\u7a7a\u95f4\u6269\u6563\u751f\u6210\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u65e0\u98de\u884c\u50cf\u7d20\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eStable Diffusion\u7684\u751f\u6210\u5f0f\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u9700\u8981VAE\u5c06\u6df1\u5ea6\u56fe\u538b\u7f29\u5230\u6f5c\u5728\u7a7a\u95f4\uff0c\u8fd9\u4f1a\u5728\u8fb9\u7f18\u548c\u7ec6\u8282\u5904\u5f15\u5165\u98de\u884c\u50cf\u7d20\u4f2a\u5f71\u3002", "method": "1) \u76f4\u63a5\u5728\u50cf\u7d20\u7a7a\u95f4\u8fdb\u884c\u6269\u6563\u751f\u6210\u4ee5\u907f\u514dVAE\u4f2a\u5f71\uff1b2) \u5f15\u5165\u8bed\u4e49\u63d0\u793a\u6269\u6563\u53d8\u6362\u5668(SP-DiT)\u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u8bed\u4e49\u8868\u793a\u878d\u5165DiT\uff1b3) \u91c7\u7528\u7ea7\u8054DiT\u8bbe\u8ba1\u9010\u6b65\u589e\u52a0token\u6570\u91cf\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6240\u6709\u5df2\u53d1\u5e03\u751f\u6210\u6a21\u578b\u7684\u6700\u4f73\u6027\u80fd\uff0c\u5728\u8fb9\u7f18\u611f\u77e5\u70b9\u4e91\u8bc4\u4f30\u4e2d\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u6240\u6709\u6a21\u578b\u3002", "conclusion": "\u50cf\u7d20\u7a7a\u95f4\u6269\u6563\u751f\u6210\u80fd\u591f\u6709\u6548\u89e3\u51b3VAE\u5f15\u5165\u7684\u98de\u884c\u50cf\u7d20\u95ee\u9898\uff0c\u7ed3\u5408\u8bed\u4e49\u63d0\u793a\u548c\u7ea7\u8054\u8bbe\u8ba1\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u65e0\u4f2a\u5f71\u7684\u6df1\u5ea6\u4f30\u8ba1\u3002"}}
{"id": "2510.06283", "pdf": "https://arxiv.org/pdf/2510.06283", "abs": "https://arxiv.org/abs/2510.06283", "authors": ["Sashank Makanaboyina"], "title": "SER-Diff: Synthetic Error Replay Diffusion for Incremental Brain Tumor Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Incremental brain tumor segmentation is critical for models that must adapt to evolving clinical datasets without retraining on all prior data. However, catastrophic forgetting, where models lose previously acquired knowledge, remains a major obstacle. Recent incremental learning frameworks with knowledge distillation partially mitigate forgetting but rely heavily on generative replay or auxiliary storage. Meanwhile, diffusion models have proven effective for refining tumor segmentations, but have not been explored in incremental learning contexts. We propose Synthetic Error Replay Diffusion (SER-Diff), the first framework that unifies diffusion-based refinement with incremental learning. SER-Diff leverages a frozen teacher diffusion model to generate synthetic error maps from past tasks, which are replayed during training on new tasks. A dual-loss formulation combining Dice loss for new data and knowledge distillation loss for replayed errors ensures both adaptability and retention. Experiments on BraTS2020, BraTS2021, and BraTS2023 demonstrate that SER-Diff consistently outperforms prior methods. It achieves the highest Dice scores of 95.8\\%, 94.9\\%, and 94.6\\%, along with the lowest HD95 values of 4.4 mm, 4.7 mm, and 4.9 mm, respectively. These results indicate that SER-Diff not only mitigates catastrophic forgetting but also delivers more accurate and anatomically coherent segmentations across evolving datasets.", "AI": {"tldr": "SER-Diff\u662f\u4e00\u4e2a\u5c06\u6269\u6563\u6a21\u578b\u4e0e\u589e\u91cf\u5b66\u4e60\u7edf\u4e00\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u8bef\u5dee\u56fe\u6765\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5728\u8111\u80bf\u7624\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u8111\u80bf\u7624\u5206\u5272\u6a21\u578b\u9700\u8981\u9002\u5e94\u4e0d\u65ad\u6f14\u5316\u7684\u4e34\u5e8a\u6570\u636e\uff0c\u4f46\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u963b\u788d\u4e86\u589e\u91cf\u5b66\u4e60\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u751f\u6210\u91cd\u653e\u6216\u8f85\u52a9\u5b58\u50a8\uff0c\u800c\u6269\u6563\u6a21\u578b\u5728\u5206\u5272\u7ec6\u5316\u4e2d\u6709\u6548\u4f46\u672a\u5728\u589e\u91cf\u5b66\u4e60\u4e2d\u88ab\u63a2\u7d22\u3002", "method": "\u63d0\u51faSER-Diff\u6846\u67b6\uff0c\u5229\u7528\u51bb\u7ed3\u7684\u6559\u5e08\u6269\u6563\u6a21\u578b\u751f\u6210\u8fc7\u53bb\u4efb\u52a1\u7684\u5408\u6210\u8bef\u5dee\u56fe\uff0c\u5728\u65b0\u4efb\u52a1\u8bad\u7ec3\u65f6\u91cd\u653e\u8fd9\u4e9b\u8bef\u5dee\u56fe\u3002\u91c7\u7528\u7ed3\u5408Dice\u635f\u5931\u548c\u77e5\u8bc6\u84b8\u998f\u635f\u5931\u7684\u53cc\u635f\u5931\u51fd\u6570\uff0c\u786e\u4fdd\u9002\u5e94\u6027\u548c\u77e5\u8bc6\u4fdd\u7559\u3002", "result": "\u5728BraTS2020\u3001BraTS2021\u548cBraTS2023\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSER-Diff\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u5206\u522b\u83b7\u5f9795.8%\u300194.9%\u548c94.6%\u7684\u6700\u9ad8Dice\u5206\u6570\uff0c\u4ee5\u53ca4.4mm\u30014.7mm\u548c4.9mm\u7684\u6700\u4f4eHD95\u503c\u3002", "conclusion": "SER-Diff\u4e0d\u4ec5\u6709\u6548\u7f13\u89e3\u4e86\u707e\u96be\u6027\u9057\u5fd8\uff0c\u8fd8\u5728\u6f14\u5316\u6570\u636e\u96c6\u4e0a\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u548c\u89e3\u5256\u5b66\u4e0a\u66f4\u4e00\u81f4\u7684\u5206\u5272\u7ed3\u679c\u3002"}}
{"id": "2510.06335", "pdf": "https://arxiv.org/pdf/2510.06335", "abs": "https://arxiv.org/abs/2510.06335", "authors": ["Mohammed Alsubaie", "Wenxi Liu", "Linxia Gu", "Ovidiu C. Andronesi", "Sirani M. Perera", "Xianqi Li"], "title": "Conditional Denoising Diffusion Model-Based Robust MR Image Reconstruction from Highly Undersampled Data", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Magnetic Resonance Imaging (MRI) is a critical tool in modern medical diagnostics, yet its prolonged acquisition time remains a critical limitation, especially in time-sensitive clinical scenarios. While undersampling strategies can accelerate image acquisition, they often result in image artifacts and degraded quality. Recent diffusion models have shown promise for reconstructing high-fidelity images from undersampled data by learning powerful image priors; however, most existing approaches either (i) rely on unsupervised score functions without paired supervision or (ii) apply data consistency only as a post-processing step. In this work, we introduce a conditional denoising diffusion framework with iterative data-consistency correction, which differs from prior methods by embedding the measurement model directly into every reverse diffusion step and training the model on paired undersampled-ground truth data. This hybrid design bridges generative flexibility with explicit enforcement of MRI physics. Experiments on the fastMRI dataset demonstrate that our framework consistently outperforms recent state-of-the-art deep learning and diffusion-based methods in SSIM, PSNR, and LPIPS, with LPIPS capturing perceptual improvements more faithfully. These results demonstrate that integrating conditional supervision with iterative consistency updates yields substantial improvements in both pixel-level fidelity and perceptual realism, establishing a principled and practical advance toward robust, accelerated MRI reconstruction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6761\u4ef6\u53bb\u566a\u6269\u6563\u6a21\u578b\u548c\u8fed\u4ee3\u6570\u636e\u4e00\u81f4\u6027\u6821\u6b63\u7684MRI\u91cd\u5efa\u65b9\u6cd5\uff0c\u5728\u6bcf\u6b21\u53cd\u5411\u6269\u6563\u6b65\u9aa4\u4e2d\u5d4c\u5165\u6d4b\u91cf\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u56fe\u50cf\u7684\u8d28\u91cf\u548c\u611f\u77e5\u771f\u5b9e\u6027\u3002", "motivation": "MRI\u91c7\u96c6\u65f6\u95f4\u8fc7\u957f\u662f\u4e34\u5e8a\u8bca\u65ad\u7684\u4e3b\u8981\u9650\u5236\uff0c\u73b0\u6709\u6b20\u91c7\u6837\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u56fe\u50cf\u4f2a\u5f71\u548c\u8d28\u91cf\u4e0b\u964d\u3002\u867d\u7136\u6269\u6563\u6a21\u578b\u5728\u91cd\u5efa\u9ad8\u4fdd\u771f\u56fe\u50cf\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u7f3a\u4e4f\u914d\u5bf9\u76d1\u7763\uff0c\u8981\u4e48\u4ec5\u5728\u540e\u671f\u5904\u7406\u4e2d\u5e94\u7528\u6570\u636e\u4e00\u81f4\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u6761\u4ef6\u53bb\u566a\u6269\u6563\u6846\u67b6\uff0c\u5728\u6bcf\u4e2a\u53cd\u5411\u6269\u6563\u6b65\u9aa4\u4e2d\u76f4\u63a5\u5d4c\u5165\u6d4b\u91cf\u6a21\u578b\uff0c\u5e76\u5728\u914d\u5bf9\u7684\u6b20\u91c7\u6837-\u771f\u5b9e\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5c06\u751f\u6210\u7075\u6d3b\u6027\u4e0eMRI\u7269\u7406\u7ea6\u675f\u76f8\u7ed3\u5408\u3002", "result": "\u5728fastMRI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728SSIM\u3001PSNR\u548cLPIPS\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u6700\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u548c\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\uff0cLPIPS\u66f4\u80fd\u51c6\u786e\u6355\u6349\u611f\u77e5\u6539\u8fdb\u3002", "conclusion": "\u5c06\u6761\u4ef6\u76d1\u7763\u4e0e\u8fed\u4ee3\u4e00\u81f4\u6027\u66f4\u65b0\u76f8\u7ed3\u5408\uff0c\u5728\u50cf\u7d20\u7ea7\u4fdd\u771f\u5ea6\u548c\u611f\u77e5\u771f\u5b9e\u6027\u65b9\u9762\u90fd\u5e26\u6765\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u4e3a\u7a33\u5065\u7684\u52a0\u901fMRI\u91cd\u5efa\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u548c\u5b9e\u7528\u7684\u8fdb\u5c55\u3002"}}
{"id": "2510.06646", "pdf": "https://arxiv.org/pdf/2510.06646", "abs": "https://arxiv.org/abs/2510.06646", "authors": ["Mansi Sakarvadia", "Kareem Hegazy", "Amin Totounferoush", "Kyle Chard", "Yaoqing Yang", "Ian Foster", "Michael W. Mahoney"], "title": "The False Promise of Zero-Shot Super-Resolution in Machine-Learned Operators", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "A core challenge in scientific machine learning, and scientific computing more generally, is modeling continuous phenomena which (in practice) are represented discretely. Machine-learned operators (MLOs) have been introduced as a means to achieve this modeling goal, as this class of architecture can perform inference at arbitrary resolution. In this work, we evaluate whether this architectural innovation is sufficient to perform \"zero-shot super-resolution,\" namely to enable a model to serve inference on higher-resolution data than that on which it was originally trained. We comprehensively evaluate both zero-shot sub-resolution and super-resolution (i.e., multi-resolution) inference in MLOs. We decouple multi-resolution inference into two key behaviors: 1) extrapolation to varying frequency information; and 2) interpolating across varying resolutions. We empirically demonstrate that MLOs fail to do both of these tasks in a zero-shot manner. Consequently, we find MLOs are not able to perform accurate inference at resolutions different from those on which they were trained, and instead they are brittle and susceptible to aliasing. To address these failure modes, we propose a simple, computationally-efficient, and data-driven multi-resolution training protocol that overcomes aliasing and that provides robust multi-resolution generalization.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u673a\u5668\u5b66\u4e60\u7b97\u5b50(MLOs)\u5728\u96f6\u6837\u672c\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709MLOs\u65e0\u6cd5\u5728\u672a\u8bad\u7ec3\u7684\u5206\u8fa8\u7387\u4e0a\u51c6\u786e\u63a8\u7406\uff0c\u5b58\u5728\u6df7\u53e0\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u591a\u5206\u8fa8\u7387\u8bad\u7ec3\u534f\u8bae\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "motivation": "\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4e2d\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\u662f\u5efa\u6a21\u8fde\u7eed\u73b0\u8c61\uff0c\u673a\u5668\u5b66\u4e60\u7b97\u5b50(MLOs)\u88ab\u5f15\u5165\u4f5c\u4e3a\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u7684\u624b\u6bb5\uff0c\u4f46\u9700\u8981\u8bc4\u4f30\u5176\u662f\u5426\u80fd\u591f\u5b9e\u73b0\u96f6\u6837\u672c\u8d85\u5206\u8fa8\u7387\u63a8\u7406\u3002", "method": "\u5c06\u591a\u5206\u8fa8\u7387\u63a8\u7406\u89e3\u8026\u4e3a\u4e24\u4e2a\u5173\u952e\u884c\u4e3a\uff1a1)\u5bf9\u4e0d\u540c\u9891\u7387\u4fe1\u606f\u7684\u63a8\u65ad\uff1b2)\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u95f4\u7684\u63d2\u503c\u3002\u901a\u8fc7\u5b9e\u8bc1\u8bc4\u4f30MLOs\u7684\u96f6\u6837\u672c\u591a\u5206\u8fa8\u7387\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u6570\u636e\u9a71\u52a8\u7684\u591a\u5206\u8fa8\u7387\u8bad\u7ec3\u534f\u8bae\u3002", "result": "\u5b9e\u8bc1\u7814\u7a76\u8868\u660eMLOs\u65e0\u6cd5\u4ee5\u96f6\u6837\u672c\u65b9\u5f0f\u5b8c\u6210\u9891\u7387\u63a8\u65ad\u548c\u5206\u8fa8\u7387\u63d2\u503c\u4efb\u52a1\uff0c\u5728\u672a\u8bad\u7ec3\u7684\u5206\u8fa8\u7387\u4e0a\u63a8\u7406\u4e0d\u51c6\u786e\uff0c\u5bb9\u6613\u53d7\u5230\u6df7\u53e0\u5f71\u54cd\u3002\u63d0\u51fa\u7684\u591a\u5206\u8fa8\u7387\u8bad\u7ec3\u534f\u8bae\u80fd\u591f\u514b\u670d\u6df7\u53e0\u95ee\u9898\u5e76\u63d0\u4f9b\u7a33\u5065\u7684\u591a\u5206\u8fa8\u7387\u6cdb\u5316\u3002", "conclusion": "\u5355\u7eaf\u7684MLOs\u67b6\u6784\u521b\u65b0\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u96f6\u6837\u672c\u8d85\u5206\u8fa8\u7387\uff0c\u9700\u8981\u4e13\u95e8\u7684\u591a\u5206\u8fa8\u7387\u8bad\u7ec3\u7b56\u7565\u6765\u786e\u4fdd\u6a21\u578b\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u4e0a\u7684\u7a33\u5065\u6027\u80fd\u3002"}}
