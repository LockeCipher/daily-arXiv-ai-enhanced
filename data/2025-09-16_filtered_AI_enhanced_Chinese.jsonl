{"id": "2509.10678", "pdf": "https://arxiv.org/pdf/2509.10678", "abs": "https://arxiv.org/abs/2509.10678", "authors": ["Jiahao Luo", "Chaoyang Wang", "Michael Vasilkovsky", "Vladislav Shakhrai", "Di Liu", "Peiye Zhuang", "Sergey Tulyakov", "Peter Wonka", "Hsin-Ying Lee", "James Davis", "Jian Wang"], "title": "T2Bs: Text-to-Character Blendshapes via Video Generation", "categories": ["cs.GR"], "comment": null, "summary": "We present T2Bs, a framework for generating high-quality, animatable character head morphable models from text by combining static text-to-3D generation with video diffusion. Text-to-3D models produce detailed static geometry but lack motion synthesis, while video diffusion models generate motion with temporal and multi-view geometric inconsistencies. T2Bs bridges this gap by leveraging deformable 3D Gaussian splatting to align static 3D assets with video outputs. By constraining motion with static geometry and employing a view-dependent deformation MLP, T2Bs (i) outperforms existing 4D generation methods in accuracy and expressiveness while reducing video artifacts and view inconsistencies, and (ii) reconstructs smooth, coherent, fully registered 3D geometries designed to scale for building morphable models with diverse, realistic facial motions. This enables synthesizing expressive, animatable character heads that surpass current 4D generation techniques.", "AI": {"tldr": "T2Bs\u662f\u4e00\u4e2a\u4ece\u6587\u672c\u751f\u6210\u9ad8\u8d28\u91cf\u53ef\u52a8\u753b\u89d2\u8272\u5934\u90e8\u5f62\u53d8\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u9759\u6001\u6587\u672c\u52303D\u751f\u6210\u548c\u89c6\u9891\u6269\u6563\u6280\u672f\uff0c\u901a\u8fc7\u53ef\u53d8\u5f623D\u9ad8\u65af\u6cfc\u6e85\u5bf9\u9f50\u9759\u60013D\u8d44\u4ea7\u4e0e\u89c6\u9891\u8f93\u51fa\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u52303D\u6a21\u578b\u7f3a\u4e4f\u8fd0\u52a8\u5408\u6210\u80fd\u529b\uff0c\u4ee5\u53ca\u89c6\u9891\u6269\u6563\u6a21\u578b\u5b58\u5728\u65f6\u95f4\u548c\u591a\u89c6\u89d2\u51e0\u4f55\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u53ef\u52a8\u753b\u89d2\u8272\u5934\u90e8\u7684\u751f\u6210\u3002", "method": "\u5229\u7528\u53ef\u53d8\u5f623D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u901a\u8fc7\u9759\u6001\u51e0\u4f55\u7ea6\u675f\u8fd0\u52a8\uff0c\u5e76\u4f7f\u7528\u89c6\u89d2\u4f9d\u8d56\u53d8\u5f62MLP\u6765\u5bf9\u9f50\u9759\u60013D\u8d44\u4ea7\u4e0e\u89c6\u9891\u8f93\u51fa\u3002", "result": "\u5728\u51c6\u786e\u6027\u548c\u8868\u73b0\u529b\u65b9\u9762\u4f18\u4e8e\u73b0\u67094D\u751f\u6210\u65b9\u6cd5\uff0c\u51cf\u5c11\u89c6\u9891\u4f2a\u5f71\u548c\u89c6\u89d2\u4e0d\u4e00\u81f4\uff0c\u91cd\u5efa\u5e73\u6ed1\u3001\u8fde\u8d2f\u3001\u5b8c\u5168\u914d\u51c6\u76843D\u51e0\u4f55\u4f53\u3002", "conclusion": "\u80fd\u591f\u5408\u6210\u8d85\u8d8a\u5f53\u524d4D\u751f\u6210\u6280\u672f\u7684\u5bcc\u6709\u8868\u73b0\u529b\u7684\u53ef\u52a8\u753b\u89d2\u8272\u5934\u90e8\uff0c\u4e3a\u6784\u5efa\u5177\u6709\u591a\u6837\u5316\u903c\u771f\u9762\u90e8\u8fd0\u52a8\u7684\u5f62\u53d8\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.11003", "pdf": "https://arxiv.org/pdf/2509.11003", "abs": "https://arxiv.org/abs/2509.11003", "authors": ["Gurutva Patle", "Nilay Girgaonkar", "Nagabhushan Somraj", "Rajiv Soundararajan"], "title": "AD-GS: Alternating Densification for Sparse-Input 3D Gaussian Splatting", "categories": ["cs.GR", "cs.CV"], "comment": "SIGGRAPH Asia 2025", "summary": "3D Gaussian Splatting (3DGS) has shown impressive results in real-time novel view synthesis. However, it often struggles under sparse-view settings, producing undesirable artifacts such as floaters, inaccurate geometry, and overfitting due to limited observations. We find that a key contributing factor is uncontrolled densification, where adding Gaussian primitives rapidly without guidance can harm geometry and cause artifacts. We propose AD-GS, a novel alternating densification framework that interleaves high and low densification phases. During high densification, the model densifies aggressively, followed by photometric loss based training to capture fine-grained scene details. Low densification then primarily involves aggressive opacity pruning of Gaussians followed by regularizing their geometry through pseudo-view consistency and edge-aware depth smoothness. This alternating approach helps reduce overfitting by carefully controlling model capacity growth while progressively refining the scene representation. Extensive experiments on challenging datasets demonstrate that AD-GS significantly improves rendering quality and geometric consistency compared to existing methods.", "AI": {"tldr": "AD-GS\u662f\u4e00\u79cd\u4ea4\u66ff\u5bc6\u5ea6\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u4f4e\u5bc6\u5ea6\u5316\u9636\u6bb5\u4ea4\u66ff\u6765\u63a7\u52363D\u9ad8\u65af\u5206\u5e03\u7684\u5bc6\u5ea6\u589e\u957f\uff0c\u89e3\u51b3\u7a00\u758f\u89c6\u89d2\u4e0b\u7684\u4f2a\u5f71\u548c\u8fc7\u62df\u5408\u95ee\u9898", "motivation": "3D\u9ad8\u65af\u5206\u5e03\u6280\u672f\u5728\u7a00\u758f\u89c6\u89d2\u8bbe\u7f6e\u4e0b\u5bb9\u6613\u4ea7\u751f\u6d6e\u52a8\u7269\u4f53\u3001\u51e0\u4f55\u4e0d\u51c6\u786e\u548c\u8fc7\u62df\u5408\u7b49\u4f2a\u5f71\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u65e0\u63a7\u5236\u7684\u5bc6\u5ea6\u5316\u8fc7\u7a0b", "method": "\u63d0\u51fa\u4ea4\u66ff\u5bc6\u5ea6\u5316\u6846\u67b6\uff1a\u9ad8\u5bc6\u5ea6\u5316\u9636\u6bb5\u6fc0\u8fdb\u6dfb\u52a0\u9ad8\u65af\u57fa\u5143\u5e76\u57fa\u4e8e\u5149\u5ea6\u635f\u5931\u8bad\u7ec3\uff1b\u4f4e\u5bc6\u5ea6\u5316\u9636\u6bb5\u6fc0\u8fdb\u4fee\u526a\u4e0d\u900f\u660e\u5ea6\u5e76\u901a\u8fc7\u4f2a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u8fb9\u7f18\u611f\u77e5\u6df1\u5ea6\u5e73\u6ed1\u6b63\u5219\u5316\u51e0\u4f55", "result": "\u5728\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAD-GS\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u8d28\u91cf\u548c\u51e0\u4f55\u4e00\u81f4\u6027", "conclusion": "\u4ea4\u66ff\u5bc6\u5ea6\u5316\u65b9\u6cd5\u901a\u8fc7\u7cbe\u5fc3\u63a7\u5236\u6a21\u578b\u5bb9\u91cf\u589e\u957f\uff0c\u6709\u6548\u51cf\u5c11\u8fc7\u62df\u5408\u5e76\u9010\u6b65\u4f18\u5316\u573a\u666f\u8868\u793a"}}
{"id": "2509.10466", "pdf": "https://arxiv.org/pdf/2509.10466", "abs": "https://arxiv.org/abs/2509.10466", "authors": ["Christian Fane"], "title": "A Real-Time Diminished Reality Approach to Privacy in MR Collaboration", "categories": ["cs.CV", "cs.HC", "H.5.1; H.5.2; I.4.6; I.4.4; I.2.10; I.4.9; K.4.1"], "comment": "50 pages, 12 figures | Demo video: https://youtu.be/udBxj35GEKI?t=499   | Code: https://github.com/c1h1r1i1s1 (multiple repositories)", "summary": "Diminished reality (DR) refers to the digital removal of real-world objects by compositing background content in their place. This thesis presents a real-time, inpainting-based DR system designed to enable privacy control in shared-space mixed reality (MR) meetings. The system allows a primary headset user to selectively remove personal or sensitive items from their environment, ensuring that those objects are no longer visible to other participants. Removal is achieved through semantic segmentation and precise object selection, followed by real-time inpainting from the viewpoint of a secondary observer, implemented using a mobile ZED 2i depth camera. The solution is designed to be portable and robust, requiring neither a fixed secondary viewpoint nor prior 3D scanning of the environment. The system utilises YOLOv11 for object detection and a modified Decoupled Spatial-Temporal Transformer (DSTT) model for high-quality video inpainting. At 720p resolution, the pipeline sustains frame rates exceeding 20 fps, demonstrating the feasibility of real-time diminished reality for practical privacy-preserving MR applications.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u6062\u590d\u7684\u5b9e\u65f6\u865a\u51cf\u73b0\u5b9e\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bed\u4e49\u5206\u5272\u548c\u5b9e\u65f6\u6062\u590d\u6280\u672f\u5728\u6df7\u5408\u73b0\u5b9e\u4f1a\u8bae\u4e2d\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\uff0c\u53ef\u4ee5\u572820fps\u7684\u901f\u5ea6\u79fb\u9664\u654f\u611f\u7269\u54c1\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6df7\u5408\u73b0\u5b9e\u4f1a\u8bae\u4e2d\u7684\u9690\u79c1\u6cc4\u6f0f\u95ee\u9898\uff0c\u5141\u8bb8\u4e3b\u8981\u7528\u6237\u9009\u62e9\u6027\u79fb\u9664\u73af\u5883\u4e2d\u7684\u4e2a\u4eba\u6216\u654f\u611f\u7269\u54c1\uff0c\u786e\u4fdd\u5176\u4ed6\u53c2\u4e0e\u8005\u65e0\u6cd5\u770b\u5230\u8fd9\u4e9b\u7269\u4f53\u3002", "method": "\u7cfb\u7edf\u91c7\u7528YOLOv11\u8fdb\u884c\u7269\u4f53\u68c0\u6d4b\uff0c\u901a\u8fc7\u8bed\u4e49\u5206\u5272\u548c\u7cbe\u786e\u7269\u4f53\u9009\u62e9\uff0c\u4f7f\u7528\u6539\u8fdb\u7684Decoupled Spatial-Temporal Transformer (DSTT)\u6a21\u578b\u8fdb\u884c\u9ad8\u8d28\u91cf\u89c6\u9891\u6062\u590d\uff0c\u57fa\u4e8emobile ZED 2i\u6df1\u5ea6\u6444\u50cf\u5934\u5b9e\u73b0\u5b9e\u65f6\u5904\u7406\u3002", "result": "\u5728720p\u5206\u8fa8\u7387\u4e0b\uff0c\u7cfb\u7edf\u80fd\u591f\u7ef4\u6301\u8d85\u8fc720fps\u7684\u684c\u9762\u7387\uff0c\u8bc1\u660e\u4e86\u5b9e\u65f6\u865a\u51cf\u73b0\u5b9e\u5728\u5b9e\u9645\u9690\u79c1\u4fdd\u62a4\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b26\u5408\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u7684\u53ef\u79fb\u52a8\u3001\u9ad8\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\uff0c\u65e0\u9700\u56fa\u5b9a\u89c2\u5bdf\u70b9\u6216\u73af\u58833D\u626b\u63cf\uff0c\u4e3aMR\u4f1a\u8bae\u9690\u79c1\u63a7\u5236\u5f00\u542f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.11377", "pdf": "https://arxiv.org/pdf/2509.11377", "abs": "https://arxiv.org/abs/2509.11377", "authors": ["Isha Sharma", "Dieter Schmalstieg"], "title": "3D Gaussian Modeling and Ray Marching of OpenVDB datasets for Scientific Visualization", "categories": ["cs.GR"], "comment": null, "summary": "3D Gaussians are currently being heavily investigated for their scene modeling and compression abilities. In 3D volumes, their use is being explored for representing dense volumes as sparsely as possible. However, most of these methods begin with a memory inefficient data format. Specially in Scientific Visualization(SciVis), where most popular formats are dense-grid data structures that store every grid cell, irrespective of its contribution. OpenVDB library and data format were introduced for representing sparse volumetric data specifically for visual effects use cases such as clouds, fire, fluids etc. It avoids storing empty cells by masking them during storage. It presents an opportunity for use in SciVis, specifically as a modeling framework for conversion to 3D Gaussian particles for further compression and for a unified modeling approach for different scientific volume types. This compression head-start is non-trivial and this paper would like to present this with a rendering algorithm based on line integration implemented in OptiX8.1 for calculating 3D Gaussians contribution along a ray for optical-depth accumulation. For comparing the rendering results of our ray marching Gaussians renderer, we also implement a SciVis style primary-ray only NanoVDB HDDA based ray marcher for OpenVDB voxel grids. Finally, this paper also explores application of this Gaussian model to formats of volumes other than regular grids, such as AMR volumes and point clouds, using internal representation of OpenVDB grid class types for data hierarchy and subdivision structure.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u7d22\u4e86\u4f7f\u7528OpenVDB\u7a00\u758f\u4f53\u79ef\u6570\u636e\u683c\u5f0f\u4f5c\u4e3a3D\u9ad8\u65af\u7c92\u5b50\u7684\u524d\u7f6e\u6a21\u578b\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u5149\u7ebf\u79ef\u5206\u7684\u6e32\u67d3\u7b97\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u79d1\u5b66\u53ef\u89c6\u5316\u6570\u636e\u538b\u7f29\u548c\u7edf\u4e00\u6a21\u578b\u8868\u793a\u3002", "motivation": "\u79d1\u5b66\u53ef\u89c6\u5316\u9886\u57df\u4e2d\u5e38\u89c1\u7684\u5bc6\u96c6\u683c\u70b9\u6570\u636e\u683c\u5f0f\u5185\u5b58\u6548\u7387\u4f4e\uff0c\u800cOpenVDB\u7a00\u758f\u4f53\u79ef\u6570\u636e\u683c\u5f0f\u80fd\u591f\u907f\u514d\u5b58\u50a8\u7a7a\u5355\u5143\u683c\uff0c\u4e3a\u8f6c\u6362\u52303D\u9ad8\u65af\u7c92\u5b50\u63d0\u4f9b\u4e86\u538b\u7f29\u4f18\u52bf\u3002", "method": "\u4f7f\u7528OpenVDB\u4f5c\u4e3a\u524d\u7f6e\u6a21\u578b\u6846\u67b6\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8e\u5149\u7ebf\u79ef\u5206\u7684\u6e32\u67d3\u7b97\u6cd5\uff0c\u5728OptiX8.1\u4e2d\u5b9e\u73b0\u4e86\u8ba13D\u9ad8\u65af\u7c92\u5b50\u6cbf\u5149\u7ebf\u8d21\u732e\u7684\u8ba1\u7b97\u3002\u540c\u65f6\u5b9e\u73b0\u4e86\u57fa\u4e8eNanoVDB HDDA\u7684\u5bf9\u7167\u6e32\u67d3\u5668\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5c06\u7a00\u758f\u4f53\u79ef\u6570\u636e\u8f6c\u6362\u4e3a3D\u9ad8\u65af\u7c92\u5b50\u8868\u793a\uff0c\u5b9e\u73b0\u6570\u636e\u538b\u7f29\u3002\u6e32\u67d3\u7b97\u6cd5\u80fd\u591f\u51c6\u786e\u8ba1\u7b97\u9ad8\u65af\u7c92\u5b50\u7684\u5149\u5b66\u6df1\u5ea6\u79ef\u7d2f\u6548\u679c\u3002", "conclusion": "\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u79d1\u5b66\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6a21\u578b\u8868\u793a\u65b9\u5f0f\uff0c\u80fd\u591f\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u7684\u4f53\u79ef\u6570\u636e\uff08\u5305\u62ec\u6b63\u5219\u683c\u70b9\u3001AMR\u4f53\u79ef\u548c\u70b9\u4e91\uff09\uff0c\u5177\u6709\u826f\u597d\u7684\u538b\u7f29\u6548\u679c\u548c\u6e32\u67d3\u6027\u80fd\u3002"}}
{"id": "2509.11411", "pdf": "https://arxiv.org/pdf/2509.11411", "abs": "https://arxiv.org/abs/2509.11411", "authors": ["Nikolaos Zioulis", "Nikolaos Kotarelas", "Georgios Albanis", "Spyridon Thermos", "Anargyros Chatzitofis"], "title": "On the Skinning of Gaussian Avatars", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Radiance field-based methods have recently been used to reconstruct human avatars, showing that we can significantly downscale the systems needed for creating animated human avatars. Although this progress has been initiated by neural radiance fields, their slow rendering and backward mapping from the observation space to the canonical space have been the main challenges. With Gaussian splatting overcoming both challenges, a new family of approaches has emerged that are faster to train and render, while also straightforward to implement using forward skinning from the canonical to the observation space. However, the linear blend skinning required for the deformation of the Gaussians does not provide valid results for their non-linear rotation properties. To address such artifacts, recent works use mesh properties to rotate the non-linear Gaussian properties or train models to predict corrective offsets. Instead, we propose a weighted rotation blending approach that leverages quaternion averaging. This leads to simpler vertex-based Gaussians that can be efficiently animated and integrated in any engine by only modifying the linear blend skinning technique, and using any Gaussian rasterizer.", "AI": {"tldr": "\u57fa\u4e8e\u5411\u91cf\u6743\u91cd\u65cb\u8f6c\u6df7\u5408\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4f2a\u68b5\u6563\u70b9\u5728\u4eba\u4f53\u52a8\u753b\u4e2d\u7684\u65cb\u8f6c\u95ee\u9898\uff0c\u65e0\u9700\u7f51\u683c\u6216\u9884\u6d4b\u6a21\u578b", "motivation": "\u4f2a\u68b5\u6563\u70b9\u6280\u672f\u867d\u7136\u63d0\u9ad8\u4e86\u8bad\u7ec3\u548c\u6e32\u67d3\u901f\u5ea6\uff0c\u4f46\u7ebf\u6027\u6df7\u5408\u76ae\u80a4\u65b9\u6cd5\u65e0\u6cd5\u6b63\u786e\u5904\u7406\u9ad8\u65af\u5206\u5e03\u7684\u975e\u7ebf\u6027\u65cb\u8f6c\u7279\u6027\uff0c\u5bfc\u81f4\u52a8\u753b\u6548\u679c\u4e0d\u7406\u60f3", "method": "\u63d0\u51fa\u57fa\u4e8e\u56db\u5143\u6570\u5e73\u5747\u7684\u6743\u91cd\u65cb\u8f6c\u6df7\u5408\u65b9\u6cd5\uff0c\u53ea\u9700\u4fee\u6539\u7ebf\u6027\u6df7\u5408\u76ae\u80a4\u6280\u672f\u5373\u53ef\u5b9e\u73b0", "result": "\u5b9e\u73b0\u4e86\u66f4\u7b80\u5355\u7684\u9876\u70b9\u57fa\u4f2a\u68b5\u6563\u70b9\u6a21\u578b\uff0c\u80fd\u591f\u9ad8\u6548\u52a8\u753b\u5e76\u96c6\u6210\u5230\u4efb\u4f55\u6e32\u67d3\u5f15\u64ce\u4e2d", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f2a\u68b5\u6563\u70b9\u5728\u4eba\u4f53\u52a8\u753b\u4e2d\u7684\u65cb\u8f6c\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u4eba\u4f53\u865a\u62df\u5316\u8eab\u521b\u5efa\u63d0\u4f9b\u4e86\u66f4\u7b80\u5355\u6613\u7528\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.10687", "pdf": "https://arxiv.org/pdf/2509.10687", "abs": "https://arxiv.org/abs/2509.10687", "authors": ["Hao Zhang", "Chun-Han Yao", "Simon Donn\u00e9", "Narendra Ahuja", "Varun Jampani"], "title": "Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation", "categories": ["cs.CV"], "comment": "Page: https://stablepartdiffusion4d.github.io/", "summary": "We present Stable Part Diffusion 4D (SP4D), a framework for generating paired RGB and kinematic part videos from monocular inputs. Unlike conventional part segmentation methods that rely on appearance-based semantic cues, SP4D learns to produce kinematic parts - structural components aligned with object articulation and consistent across views and time. SP4D adopts a dual-branch diffusion model that jointly synthesizes RGB frames and corresponding part segmentation maps. To simplify the architecture and flexibly enable different part counts, we introduce a spatial color encoding scheme that maps part masks to continuous RGB-like images. This encoding allows the segmentation branch to share the latent VAE from the RGB branch, while enabling part segmentation to be recovered via straightforward post-processing. A Bidirectional Diffusion Fusion (BiDiFuse) module enhances cross-branch consistency, supported by a contrastive part consistency loss to promote spatial and temporal alignment of part predictions. We demonstrate that the generated 2D part maps can be lifted to 3D to derive skeletal structures and harmonic skinning weights with few manual adjustments. To train and evaluate SP4D, we construct KinematicParts20K, a curated dataset of over 20K rigged objects selected and processed from Objaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part video sequences. Experiments show that SP4D generalizes strongly to diverse scenarios, including real-world videos, novel generated objects, and rare articulated poses, producing kinematic-aware outputs suitable for downstream animation and motion-related tasks.", "AI": {"tldr": "SP4D\u662f\u4e00\u4e2a\u4ece\u5355\u76ee\u8f93\u5165\u751f\u6210\u914d\u5bf9RGB\u548c\u8fd0\u52a8\u5b66\u90e8\u4ef6\u89c6\u9891\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u6269\u6563\u6a21\u578b\u8054\u5408\u5408\u6210RGB\u5e27\u548c\u90e8\u4ef6\u5206\u5272\u56fe\uff0c\u91c7\u7528\u7a7a\u95f4\u989c\u8272\u7f16\u7801\u7b80\u5316\u67b6\u6784\uff0c\u5e76\u901a\u8fc7\u53cc\u5411\u6269\u6563\u878d\u5408\u6a21\u5757\u589e\u5f3a\u8de8\u5206\u652f\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u90e8\u4ef6\u5206\u5272\u65b9\u6cd5\u4f9d\u8d56\u57fa\u4e8e\u5916\u89c2\u7684\u8bed\u4e49\u7ebf\u7d22\uff0c\u800cSP4D\u65e8\u5728\u5b66\u4e60\u751f\u6210\u4e0e\u7269\u4f53\u5173\u8282\u5bf9\u9f50\u3001\u5728\u89c6\u89d2\u548c\u65f6\u95f4\u4e0a\u4fdd\u6301\u4e00\u81f4\u7684\u52a8\u6001\u8fd0\u52a8\u5b66\u90e8\u4ef6\uff0c\u4e3a\u4e0b\u6e38\u52a8\u753b\u548c\u8fd0\u52a8\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u6269\u6563\u6a21\u578b\u8054\u5408\u5408\u6210RGB\u548c\u90e8\u4ef6\u5206\u5272\uff1b\u5f15\u5165\u7a7a\u95f4\u989c\u8272\u7f16\u7801\u5c06\u90e8\u4ef6\u63a9\u7801\u6620\u5c04\u4e3a\u8fde\u7eedRGB\u56fe\u50cf\uff1b\u4f7f\u7528\u53cc\u5411\u6269\u6563\u878d\u5408\u6a21\u5757\u589e\u5f3a\u4e00\u81f4\u6027\uff1b\u91c7\u7528\u5bf9\u6bd4\u90e8\u4ef6\u4e00\u81f4\u6027\u635f\u5931\u4fc3\u8fdb\u65f6\u7a7a\u5bf9\u9f50\u3002", "result": "SP4D\u80fd\u591f\u6cdb\u5316\u5230\u591a\u6837\u5316\u573a\u666f\uff0c\u5305\u62ec\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u3001\u65b0\u751f\u6210\u7269\u4f53\u548c\u7f55\u89c1\u5173\u8282\u59ff\u6001\uff0c\u751f\u6210\u76842D\u90e8\u4ef6\u56fe\u53ef\u63d0\u5347\u52303D\u4ee5\u63a8\u5bfc\u9aa8\u9abc\u7ed3\u6784\u548c\u76ae\u80a4\u6743\u91cd\u3002", "conclusion": "SP4D\u6846\u67b6\u80fd\u591f\u751f\u6210\u8fd0\u52a8\u5b66\u611f\u77e5\u7684\u8f93\u51fa\uff0c\u9002\u7528\u4e8e\u52a8\u753b\u548c\u8fd0\u52a8\u76f8\u5173\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7KinematicParts20K\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.10759", "pdf": "https://arxiv.org/pdf/2509.10759", "abs": "https://arxiv.org/abs/2509.10759", "authors": ["Yi-Ruei Liu", "You-Zhe Xie", "Yu-Hsiang Hsu", "I-Sheng Fang", "Yu-Lun Liu", "Jun-Cheng Chen"], "title": "Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing for Physics-based Camera Effect Data Generation", "categories": ["cs.CV"], "comment": null, "summary": "Common computer vision systems typically assume ideal pinhole cameras but fail when facing real-world camera effects such as fisheye distortion and rolling shutter, mainly due to the lack of learning from training data with camera effects. Existing data generation approaches suffer from either high costs, sim-to-real gaps or fail to accurately model camera effects. To address this bottleneck, we propose 4D Gaussian Ray Tracing (4D-GRT), a novel two-stage pipeline that combines 4D Gaussian Splatting with physically-based ray tracing for camera effect simulation. Given multi-view videos, 4D-GRT first reconstructs dynamic scenes, then applies ray tracing to generate videos with controllable, physically accurate camera effects. 4D-GRT achieves the fastest rendering speed while performing better or comparable rendering quality compared to existing baselines. Additionally, we construct eight synthetic dynamic scenes in indoor environments across four camera effects as a benchmark to evaluate generated videos with camera effects.", "AI": {"tldr": "4D-GRT\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6d41\u6c34\u7ebf\uff0c\u7ed3\u54084D\u9ad8\u65af\u6e85\u5c04\u548c\u57fa\u4e8e\u7269\u7406\u7684\u5149\u7ebf\u8ffd\u8e2a\uff0c\u7528\u4e8e\u6a21\u62df\u76f8\u673a\u6548\u679c\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u4eff\u771f\u5230\u771f\u5b9e\u5dee\u8ddd\u5927\u6216\u65e0\u6cd5\u51c6\u786e\u5efa\u6a21\u76f8\u673a\u6548\u679c\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u5047\u8bbe\u7406\u60f3\u9488\u5b54\u76f8\u673a\uff0c\u4f46\u5728\u9762\u5bf9\u9c7c\u773c\u7578\u53d8\u3001\u6eda\u52a8\u5feb\u95e8\u7b49\u771f\u5b9e\u76f8\u673a\u6548\u679c\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u5305\u542b\u76f8\u673a\u6548\u679c\u7684\u8bad\u7ec3\u6570\u636e\u3002\u73b0\u6709\u6570\u636e\u751f\u6210\u65b9\u6cd5\u8981\u4e48\u6210\u672c\u9ad8\uff0c\u8981\u4e48\u5b58\u5728\u4eff\u771f\u5230\u771f\u5b9e\u5dee\u8ddd\uff0c\u6216\u65e0\u6cd5\u51c6\u786e\u5efa\u6a21\u76f8\u673a\u6548\u679c\u3002", "method": "\u63d0\u51fa4D\u9ad8\u65af\u5149\u7ebf\u8ffd\u8e2a(4D-GRT)\u4e24\u9636\u6bb5\u6d41\u6c34\u7ebf\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u75284D\u9ad8\u65af\u6e85\u5c04\u91cd\u5efa\u52a8\u6001\u573a\u666f\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5e94\u7528\u57fa\u4e8e\u7269\u7406\u7684\u5149\u7ebf\u8ffd\u8e2a\u751f\u6210\u5177\u6709\u53ef\u63a7\u3001\u7269\u7406\u51c6\u786e\u7684\u76f8\u673a\u6548\u679c\u7684\u89c6\u9891\u3002", "result": "4D-GRT\u5b9e\u73b0\u4e86\u6700\u5feb\u7684\u6e32\u67d3\u901f\u5ea6\uff0c\u540c\u65f6\u5728\u6e32\u67d3\u8d28\u91cf\u4e0a\u4f18\u4e8e\u6216\u4e0e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5f53\u3002\u6784\u5efa\u4e86\u5305\u542b8\u4e2a\u5408\u6210\u52a8\u6001\u573a\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u6db5\u76d64\u79cd\u76f8\u673a\u6548\u679c\u3002", "conclusion": "4D-GRT\u80fd\u591f\u6709\u6548\u751f\u6210\u5177\u6709\u7269\u7406\u51c6\u786e\u76f8\u673a\u6548\u679c\u7684\u89c6\u9891\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u5728\u771f\u5b9e\u76f8\u673a\u6548\u679c\u4e0b\u7684\u6027\u80fd\u63d0\u5347\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6570\u636e\u751f\u6210\u65b9\u6848\u3002"}}
{"id": "2509.10980", "pdf": "https://arxiv.org/pdf/2509.10980", "abs": "https://arxiv.org/abs/2509.10980", "authors": ["Haoming Lu"], "title": "TrueSkin: Towards Fair and Accurate Skin Tone Recognition and Generation", "categories": ["cs.CV"], "comment": null, "summary": "Skin tone recognition and generation play important roles in model fairness, healthcare, and generative AI, yet they remain challenging due to the lack of comprehensive datasets and robust methodologies. Compared to other human image analysis tasks, state-of-the-art large multimodal models (LMMs) and image generation models struggle to recognize and synthesize skin tones accurately. To address this, we introduce TrueSkin, a dataset with 7299 images systematically categorized into 6 classes, collected under diverse lighting conditions, camera angles, and capture settings. Using TrueSkin, we benchmark existing recognition and generation approaches, revealing substantial biases: LMMs tend to misclassify intermediate skin tones as lighter ones, whereas generative models struggle to accurately produce specified skin tones when influenced by inherent biases from unrelated attributes in the prompts, such as hairstyle or environmental context. We further demonstrate that training a recognition model on TrueSkin improves classification accuracy by more than 20\\% compared to LMMs and conventional approaches, and fine-tuning with TrueSkin significantly improves skin tone fidelity in image generation models. Our findings highlight the need for comprehensive datasets like TrueSkin, which not only serves as a benchmark for evaluating existing models but also provides a valuable training resource to enhance fairness and accuracy in skin tone recognition and generation tasks.", "AI": {"tldr": "TrueSkin\u662f\u4e00\u4e2a\u5305\u542b7299\u5f20\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u5730\u5c06\u76ae\u80a4\u8272\u8c03\u5206\u4e3a6\u7c7b\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u76ae\u80a4\u8272\u8c03\u8bc6\u522b\u4e0e\u751f\u6210\u6a21\u578b\u7684\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u76ae\u80a4\u8272\u8c03\u8bc6\u522b\u548c\u751f\u6210\u5728\u6a21\u578b\u516c\u5e73\u6027\u3001\u533b\u7597\u5065\u5eb7\u548c\u751f\u6210\u5f0fAI\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u5168\u9762\u6570\u636e\u96c6\u548c\u7a33\u5065\u65b9\u6cd5\u800c\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u548c\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u51c6\u786e\u8bc6\u522b\u548c\u5408\u6210\u76ae\u80a4\u8272\u8c03\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "\u5f15\u5165TrueSkin\u6570\u636e\u96c6\uff0c\u5305\u542b7299\u5f20\u5728\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u3001\u76f8\u673a\u89d2\u5ea6\u548c\u62cd\u6444\u8bbe\u7f6e\u4e0b\u6536\u96c6\u7684\u56fe\u50cf\uff0c\u7cfb\u7edf\u5206\u4e3a6\u4e2a\u7c7b\u522b\u3002\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u5bf9\u73b0\u6709\u8bc6\u522b\u548c\u751f\u6210\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u8bad\u7ec3\u4e13\u95e8\u7684\u8bc6\u522b\u6a21\u578b\u548c\u5fae\u8c03\u751f\u6210\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5b58\u5728\u663e\u8457\u504f\u89c1\uff1aLMMs\u503e\u5411\u4e8e\u5c06\u4e2d\u95f4\u76ae\u80a4\u8272\u8c03\u8bef\u5206\u7c7b\u4e3a\u8f83\u6d45\u8272\u8c03\uff0c\u751f\u6210\u6a21\u578b\u5728\u53d7\u63d0\u793a\u4e2d\u65e0\u5173\u5c5e\u6027\uff08\u5982\u53d1\u578b\u6216\u73af\u5883\u80cc\u666f\uff09\u5f71\u54cd\u65f6\u96be\u4ee5\u51c6\u786e\u751f\u6210\u6307\u5b9a\u76ae\u80a4\u8272\u8c03\u3002\u5728TrueSkin\u4e0a\u8bad\u7ec3\u7684\u8bc6\u522b\u6a21\u578b\u6bd4LMMs\u548c\u4f20\u7edf\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u9ad820%\u4ee5\u4e0a\uff0c\u5fae\u8c03\u540e\u751f\u6210\u6a21\u578b\u7684\u76ae\u80a4\u8272\u8c03\u4fdd\u771f\u5ea6\u663e\u8457\u6539\u5584\u3002", "conclusion": "TrueSkin\u6570\u636e\u96c6\u4e0d\u4ec5\u53ef\u4f5c\u4e3a\u8bc4\u4f30\u73b0\u6709\u6a21\u578b\u7684\u57fa\u51c6\uff0c\u8fd8\u80fd\u4f5c\u4e3a\u6709\u4ef7\u503c\u7684\u8bad\u7ec3\u8d44\u6e90\uff0c\u63d0\u9ad8\u76ae\u80a4\u8272\u8c03\u8bc6\u522b\u548c\u751f\u6210\u4efb\u52a1\u7684\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\uff0c\u5f3a\u8c03\u4e86\u6b64\u7c7b\u5168\u9762\u6570\u636e\u96c6\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.11071", "pdf": "https://arxiv.org/pdf/2509.11071", "abs": "https://arxiv.org/abs/2509.11071", "authors": ["Jinghan Peng", "Jingwen Wang", "Xing Yu", "Dehui Du"], "title": "The System Description of CPS Team for Track on Driving with Language of CVPR 2024 Autonomous Grand Challenge", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "This report outlines our approach using vision language model systems for the Driving with Language track of the CVPR 2024 Autonomous Grand Challenge. We have exclusively utilized the DriveLM-nuScenes dataset for training our models. Our systems are built on the LLaVA models, which we enhanced through fine-tuning with the LoRA and DoRA methods. Additionally, we have integrated depth information from open-source depth estimation models to enrich the training and inference processes. For inference, particularly with multiple-choice and yes/no questions, we adopted a Chain-of-Thought reasoning approach to improve the accuracy of the results. This comprehensive methodology enabled us to achieve a top score of 0.7799 on the validation set leaderboard, ranking 1st on the leaderboard.", "AI": {"tldr": "\u57fa\u4e8eLLaVA\u6a21\u578b\u7684\u89c6\u89c9\u8bed\u8a00\u7cfb\u7edf\uff0c\u901a\u8fc7LoRA\u548cDoRA\u5fae\u8c03\u65b9\u6cd5\uff0c\u96c6\u6210\u6df1\u5ea6\u4fe1\u606f\uff0c\u91c7\u7528Chain-of-Thought\u63a8\u7406\uff0c\u5728CVPR 2024\u81ea\u52a8\u9a7e\u9a76\u6311\u6218\u8d5b\u4e2d\u53d6\u5f97\u7b2c\u4e00\u540d", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u8bed\u8a00\u7406\u89e3\u548c\u89c6\u89c9\u63a8\u7406\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u9a7e\u9a76\u73af\u5883\u4e2d\u7684\u95ee\u7b54\u80fd\u529b", "method": "\u4f7f\u7528LLaVA\u6a21\u578b\u4e3a\u57fa\u7840\uff0c\u91c7\u7528LoRA\u548cDoRA\u8fdb\u884c\u5fae\u8c03\uff0c\u96c6\u6210\u5f00\u6e90\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u7684\u6df1\u5ea6\u4fe1\u606f\uff0c\u5728\u63a8\u7406\u65f6\u4f7f\u7528Chain-of-Thought\u65b9\u6cd5\u5904\u7406\u9009\u62e9\u9898\u548c\u662f\u975e\u9898", "result": "\u5728\u9a8c\u8bc1\u96c6\u6392\u884c\u699c\u4e0a\u83b7\u5f970.7799\u7684\u6700\u9ad8\u5206\uff0c\u6392\u540d\u7b2c\u4e00", "conclusion": "\u901a\u8fc7\u7efc\u5408\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u65b9\u6cd5\uff0c\u5305\u62ec\u6df1\u5ea6\u4fe1\u606f\u96c6\u6210\u548c\u63a8\u7406\u7b56\u7565\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e0b\u7684\u8bed\u8a00\u7406\u89e3\u6027\u80fd"}}
{"id": "2509.11092", "pdf": "https://arxiv.org/pdf/2509.11092", "abs": "https://arxiv.org/abs/2509.11092", "authors": ["Zeyu Dong", "Yuyang Yin", "Yuqi Li", "Eric Li", "Hao-Xiang Guo", "Yikai Wang"], "title": "PanoLora: Bridging Perspective and Panoramic Video Generation with LoRA Adaptation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Generating high-quality 360{\\deg} panoramic videos remains a significant challenge due to the fundamental differences between panoramic and traditional perspective-view projections. While perspective videos rely on a single viewpoint with a limited field of view, panoramic content requires rendering the full surrounding environment, making it difficult for standard video generation models to adapt. Existing solutions often introduce complex architectures or large-scale training, leading to inefficiency and suboptimal results. Motivated by the success of Low-Rank Adaptation (LoRA) in style transfer tasks, we propose treating panoramic video generation as an adaptation problem from perspective views. Through theoretical analysis, we demonstrate that LoRA can effectively model the transformation between these projections when its rank exceeds the degrees of freedom in the task. Our approach efficiently fine-tunes a pretrained video diffusion model using only approximately 1,000 videos while achieving high-quality panoramic generation. Experimental results demonstrate that our method maintains proper projection geometry and surpasses previous state-of-the-art approaches in visual quality, left-right consistency, and motion diversity.", "AI": {"tldr": "\u901a\u8fc7LoRA\u9002\u914d\u6280\u672f\uff0c\u5c06\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u8f7b\u91cf\u7ea7\u8c03\u6574\u4e3a\u9ad8\u8d28\u91cf360\u5ea6\u5168\u666f\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u4ec5\u9700\u7ea61,000\u4e2a\u89c6\u9891\u8fdb\u884c\u8bad\u7ec3\u5373\u53ef\u8fbe\u5230\u72ec\u7279\u6548\u679c\u3002", "motivation": "\u89e3\u51b3360\u5ea6\u5168\u666f\u89c6\u9891\u751f\u6210\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6848\u901a\u5e38\u9700\u8981\u590d\u6742\u67b6\u6784\u6216\u5927\u89c4\u6a21\u8bad\u7ec3\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u5c06\u5168\u666f\u89c6\u9891\u751f\u6210\u89c6\u4e3a\u4ece\u900f\u89c6\u89c6\u56fe\u5230\u5168\u666f\u6295\u5f71\u7684\u9002\u914d\u95ee\u9898\uff0c\u91c7\u7528Low-Rank Adaptation (LoRA)\u6280\u672f\u5bf9\u9884\u8bad\u7ec3\u89c6\u9891\u6a21\u578b\u8fdb\u884c\u7cbe\u7ec6\u8c03\u6574\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u7ef4\u6301\u6b63\u786e\u7684\u6295\u5f71\u51e0\u4f55\u7ed3\u6784\uff0c\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u5de6\u53f3\u4e00\u81f4\u6027\u548c\u8fd0\u52a8\u591a\u6837\u6027\u65b9\u9762\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "LoRA\u6280\u672f\u53ef\u4ee5\u9ad8\u6548\u5730\u6a21\u578b\u4e0d\u540c\u6295\u5f71\u4e4b\u95f4\u7684\u53d8\u6362\u5173\u7cfb\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8c03\u6574\u65b9\u6848\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684360\u5ea6\u5168\u666f\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2509.11093", "pdf": "https://arxiv.org/pdf/2509.11093", "abs": "https://arxiv.org/abs/2509.11093", "authors": ["Ruiying Li", "Bin Pan", "Qiaoying Qu", "Xia Xu", "Zhenwei Shi"], "title": "SMILE: A Super-resolution Guided Multi-task Learning Method for Hyperspectral Unmixing", "categories": ["cs.CV"], "comment": "12 pages, 7 figures", "summary": "The performance of hyperspectral unmixing may be constrained by low spatial resolution, which can be enhanced using super-resolution in a multitask learning way. However, integrating super-resolution and unmixing directly may suffer two challenges: Task affinity is not verified, and the convergence of unmixing is not guaranteed. To address the above issues, in this paper, we provide theoretical analysis and propose super-resolution guided multi-task learning method for hyperspectral unmixing (SMILE). The provided theoretical analysis validates feasibility of multitask learning way and verifies task affinity, which consists of relationship and existence theorems by proving the positive guidance of super-resolution. The proposed framework generalizes positive information from super-resolution to unmixing by learning both shared and specific representations. Moreover, to guarantee the convergence, we provide the accessibility theorem by proving the optimal solution of unmixing. The major contributions of SMILE include providing progressive theoretical support, and designing a new framework for unmixing under the guidance of super-resolution. Our experiments on both synthetic and real datasets have substantiate the usefulness of our work.", "AI": {"tldr": "SMILE\u65b9\u6cd5\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u8d85\u5206\u8fa8\u7387\u4e0e\u9ad8\u5149\u8c31\u89e3\u6df7\u7ed3\u5408\uff0c\u9a8c\u8bc1\u4efb\u52a1\u4eb2\u548c\u6027\u5e76\u4fdd\u8bc1\u89e3\u6df7\u6536\u655b\u6027", "motivation": "\u9ad8\u5149\u8c31\u89e3\u6df7\u6027\u80fd\u53d7\u9650\u4e8e\u4f4e\u7a7a\u95f4\u5206\u8fa8\u7387\uff0c\u76f4\u63a5\u6574\u5408\u8d85\u5206\u8fa8\u7387\u548c\u89e3\u6df7\u5b58\u5728\u4efb\u52a1\u4eb2\u548c\u6027\u672a\u9a8c\u8bc1\u548c\u89e3\u6df7\u6536\u655b\u6027\u65e0\u6cd5\u4fdd\u8bc1\u7684\u95ee\u9898", "method": "\u63d0\u51fa\u8d85\u5206\u8fa8\u7387\u5f15\u5bfc\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6SMILE\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u9a8c\u8bc1\u4efb\u52a1\u4eb2\u548c\u6027\uff0c\u5b66\u4e60\u5171\u4eab\u548c\u7279\u5b9a\u8868\u793a\u6765\u4f20\u9012\u6b63\u5411\u4fe1\u606f\uff0c\u5e76\u63d0\u4f9b\u53ef\u8fbe\u6027\u5b9a\u7406\u4fdd\u8bc1\u89e3\u6df7\u6536\u655b", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "SMILE\u63d0\u4f9b\u4e86\u6e10\u8fdb\u5f0f\u7406\u8bba\u652f\u6301\uff0c\u8bbe\u8ba1\u4e86\u8d85\u5206\u8fa8\u7387\u5f15\u5bfc\u4e0b\u7684\u89e3\u6df7\u65b0\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u6311\u6218"}}
{"id": "2509.11102", "pdf": "https://arxiv.org/pdf/2509.11102", "abs": "https://arxiv.org/abs/2509.11102", "authors": ["Nhi Kieu", "Kien Nguyen", "Arnold Wiliem", "Clinton Fookes", "Sridha Sridharan"], "title": "Filling the Gaps: A Multitask Hybrid Multiscale Generative Framework for Missing Modality in Remote Sensing Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted to DICTA 2025", "summary": "Multimodal learning has shown significant performance boost compared to ordinary unimodal models across various domains. However, in real-world scenarios, multimodal signals are susceptible to missing because of sensor failures and adverse weather conditions, which drastically deteriorates models' operation and performance. Generative models such as AutoEncoder (AE) and Generative Adversarial Network (GAN) are intuitive solutions aiming to reconstruct missing modality from available ones. Yet, their efficacy in remote sensing semantic segmentation remains underexplored. In this paper, we first examine the limitations of existing generative approaches in handling the heterogeneity of multimodal remote sensing data. They inadequately capture semantic context in complex scenes with large intra-class and small inter-class variation. In addition, traditional generative models are susceptible to heavy dependence on the dominant modality, introducing bias that affects model robustness under missing modality conditions. To tackle these limitations, we propose a novel Generative-Enhanced MultiModal learning Network (GEMMNet) with three key components: (1) Hybrid Feature Extractor (HyFEx) to effectively learn modality-specific representations, (2) Hybrid Fusion with Multiscale Awareness (HyFMA) to capture modality-synergistic semantic context across scales and (3) Complementary Loss (CoLoss) scheme to alleviate the inherent bias by encouraging consistency across modalities and tasks. Our method, GEMMNet, outperforms both generative baselines AE, cGAN (conditional GAN), and state-of-the-art non-generative approaches - mmformer and shaspec - on two challenging semantic segmentation remote sensing datasets (Vaihingen and Potsdam). Source code is made available.", "AI": {"tldr": "\u63d0\u51faGEMMNet\u7f51\u7edc\u89e3\u51b3\u591a\u6a21\u6001\u9065\u611f\u6570\u636e\u7f3a\u5931\u95ee\u9898\uff0c\u901a\u8fc7\u6df7\u5408\u7279\u5f81\u63d0\u53d6\u3001\u591a\u5c3a\u5ea6\u878d\u5408\u548c\u4e92\u8865\u635f\u5931\u51fd\u6570\uff0c\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u751f\u6210\u5f0f\u548c\u975e\u751f\u6210\u5f0f\u65b9\u6cd5", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u591a\u6a21\u6001\u4fe1\u53f7\u5bb9\u6613\u56e0\u4f20\u611f\u5668\u6545\u969c\u548c\u6076\u52a3\u5929\u6c14\u800c\u7f3a\u5931\uff0c\u4f20\u7edf\u751f\u6210\u6a21\u578b\uff08AE\u3001GAN\uff09\u5728\u5904\u7406\u591a\u6a21\u6001\u9065\u611f\u6570\u636e\u5f02\u8d28\u6027\u548c\u8bed\u4e49\u4e0a\u4e0b\u6587\u6355\u83b7\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u5bb9\u6613\u8fc7\u5ea6\u4f9d\u8d56\u4e3b\u5bfc\u6a21\u6001", "method": "\u63d0\u51faGEMMNet\u7f51\u7edc\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) HyFEx\u6df7\u5408\u7279\u5f81\u63d0\u53d6\u5668\u5b66\u4e60\u6a21\u6001\u7279\u5b9a\u8868\u793a\uff1b2) HyFMA\u591a\u5c3a\u5ea6\u611f\u77e5\u6df7\u5408\u878d\u5408\u6355\u83b7\u8de8\u5c3a\u5ea6\u6a21\u6001\u534f\u540c\u8bed\u4e49\u4e0a\u4e0b\u6587\uff1b3) CoLoss\u4e92\u8865\u635f\u5931\u65b9\u6848\u901a\u8fc7\u9f13\u52b1\u8de8\u6a21\u6001\u548c\u4efb\u52a1\u4e00\u81f4\u6027\u6765\u51cf\u8f7b\u56fa\u6709\u504f\u5dee", "result": "\u5728\u4e24\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u9065\u611f\u8bed\u4e49\u5206\u5272\u6570\u636e\u96c6\uff08Vaihingen\u548cPotsdam\uff09\u4e0a\uff0cGEMMNet\u8d85\u8d8a\u4e86\u751f\u6210\u5f0f\u57fa\u7ebf\u65b9\u6cd5\uff08AE\u3001cGAN\uff09\u548c\u6700\u5148\u8fdb\u7684\u975e\u751f\u6210\u5f0f\u65b9\u6cd5\uff08mmformer\u3001shaspec\uff09", "conclusion": "GEMMNet\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u9065\u611f\u6570\u636e\u7f3a\u5931\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u7f51\u7edc\u8bbe\u8ba1\u548c\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u8bed\u4e49\u5206\u5272\u6027\u80fd\uff0c\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.11116", "pdf": "https://arxiv.org/pdf/2509.11116", "abs": "https://arxiv.org/abs/2509.11116", "authors": ["Ashkan Taghipour", "Vahid Naghshin", "Benjamin Southwell", "Farid Boussaid", "Hamid Laga", "Mohammed Bennamoun"], "title": "SVR-GS: Spatially Variant Regularization for Probabilistic Masks in 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) enables fast, high-quality novel view synthesis but typically relies on densification followed by pruning to optimize the number of Gaussians. Existing mask-based pruning, such as MaskGS, regularizes the global mean of the mask, which is misaligned with the local per-pixel (per-ray) reconstruction loss that determines image quality along individual camera rays. This paper introduces SVR-GS, a spatially variant regularizer that renders a per-pixel spatial mask from each Gaussian's effective contribution along the ray, thereby applying sparsity pressure where it matters: on low-importance Gaussians. We explore three spatial-mask aggregation strategies, implement them in CUDA, and conduct a gradient analysis to motivate our final design. Extensive experiments on Tanks\\&Temples, Deep Blending, and Mip-NeRF360 datasets demonstrate that, on average across the three datasets, the proposed SVR-GS reduces the number of Gaussians by 1.79\\(\\times\\) compared to MaskGS and 5.63\\(\\times\\) compared to 3DGS, while incurring only 0.50 dB and 0.40 dB PSNR drops, respectively. These gains translate into significantly smaller, faster, and more memory-efficient models, making them well-suited for real-time applications such as robotics, AR/VR, and mobile perception.", "AI": {"tldr": "SVR-GS\u662f\u4e00\u79cd\u7a7a\u95f4\u53d8\u4f53\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u6bcf\u4e2a\u9ad8\u65af\u6cbf\u5149\u7ebf\u6709\u6548\u8d21\u732e\u7684\u9010\u50cf\u7d20\u7a7a\u95f4\u63a9\u7801\uff0c\u663e\u8457\u51cf\u5c113D\u9ad8\u65af\u6570\u91cf\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63a9\u7801\u7684\u526a\u679d\u65b9\u6cd5\uff08\u5982MaskGS\uff09\u4f7f\u7528\u5168\u5c40\u5747\u503c\u6b63\u5219\u5316\uff0c\u8fd9\u4e0e\u51b3\u5b9a\u56fe\u50cf\u8d28\u91cf\u7684\u9010\u50cf\u7d20\u91cd\u5efa\u635f\u5931\u4e0d\u5339\u914d\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u7a7a\u95f4\u611f\u77e5\u6b63\u5219\u5316", "method": "\u63d0\u51fa\u7a7a\u95f4\u53d8\u4f53\u6b63\u5219\u5316\u5668\uff0c\u6e32\u67d3\u6bcf\u4e2a\u9ad8\u65af\u7684\u9010\u50cf\u7d20\u7a7a\u95f4\u63a9\u7801\uff0c\u63a2\u7d22\u4e09\u79cd\u7a7a\u95f4\u63a9\u7801\u805a\u5408\u7b56\u7565\uff0c\u5e76\u5728CUDA\u4e2d\u5b9e\u73b0\uff0c\u901a\u8fc7\u68af\u5ea6\u5206\u6790\u786e\u5b9a\u6700\u7ec8\u8bbe\u8ba1", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u51cf\u5c11\u9ad8\u65af\u6570\u91cf\uff1a\u76f8\u6bd4MaskGS\u51cf\u5c111.79\u500d\uff0c\u76f8\u6bd43DGS\u51cf\u5c115.63\u500d\uff0cPSNR\u4ec5\u4e0b\u964d0.50dB\u548c0.40dB", "conclusion": "SVR-GS\u4ea7\u751f\u66f4\u5c0f\u3001\u66f4\u5feb\u3001\u5185\u5b58\u6548\u7387\u66f4\u9ad8\u7684\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u3001AR/VR\u548c\u79fb\u52a8\u611f\u77e5\u7b49\u5b9e\u65f6\u5e94\u7528"}}
{"id": "2509.11165", "pdf": "https://arxiv.org/pdf/2509.11165", "abs": "https://arxiv.org/abs/2509.11165", "authors": ["Waikit Xiu", "Qiang Lu", "Xiying Li", "Chen Hu", "Shengbo Sun"], "title": "Traffic-MLLM: A Spatio-Temporal MLLM with Retrieval-Augmented Generation for Causal Inference in Traffic", "categories": ["cs.CV"], "comment": null, "summary": "As intelligent transportation systems advance, traffic video understanding plays an increasingly pivotal role in comprehensive scene perception and causal analysis. Yet, existing approaches face notable challenges in accurately modeling spatiotemporal causality and integrating domain-specific knowledge, limiting their effectiveness in complex scenarios. To address these limitations, we propose Traffic-MLLM, a multimodal large language model tailored for fine-grained traffic analysis. Built on the Qwen2.5-VL backbone, our model leverages high-quality traffic-specific multimodal datasets and uses Low-Rank Adaptation (LoRA) for lightweight fine-tuning, significantly enhancing its capacity to model continuous spatiotemporal features in video sequences. Furthermore, we introduce an innovative knowledge prompting module fusing Chain-of-Thought (CoT) reasoning with Retrieval-Augmented Generation (RAG), enabling precise injection of detailed traffic regulations and domain knowledge into the inference process. This design markedly boosts the model's logical reasoning and knowledge adaptation capabilities. Experimental results on TrafficQA and DriveQA benchmarks show Traffic-MLLM achieves state-of-the-art performance, validating its superior ability to process multimodal traffic data. It also exhibits remarkable zero-shot reasoning and cross-scenario generalization capabilities.", "AI": {"tldr": "Traffic-MLLM\u662f\u57fa\u4e8eQwen2.5-VL\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7LoRA\u5fae\u8c03\u548c\u77e5\u8bc6\u63d0\u793a\u6a21\u5757\uff0c\u5728\u4ea4\u901a\u89c6\u9891\u5206\u6790\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4ea4\u901a\u89c6\u9891\u7406\u89e3\u65b9\u6cd5\u5728\u5efa\u6a21\u65f6\u7a7a\u56e0\u679c\u5173\u7cfb\u548c\u6574\u5408\u9886\u57df\u77e5\u8bc6\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u57fa\u4e8eQwen2.5-VL\u67b6\u6784\uff0c\u4f7f\u7528\u9ad8\u8d28\u91cf\u4ea4\u901a\u591a\u6a21\u6001\u6570\u636e\u96c6\u548cLoRA\u8f7b\u91cf\u7ea7\u5fae\u8c03\uff0c\u5f15\u5165\u7ed3\u5408CoT\u63a8\u7406\u548cRAG\u7684\u77e5\u8bc6\u63d0\u793a\u6a21\u5757\u3002", "result": "\u5728TrafficQA\u548cDriveQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5c55\u73b0\u51fa\u4f18\u79c0\u7684\u96f6\u6837\u672c\u63a8\u7406\u548c\u8de8\u573a\u666f\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Traffic-MLLM\u901a\u8fc7\u521b\u65b0\u7684\u77e5\u8bc6\u878d\u5408\u548c\u5fae\u8c03\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u901a\u89c6\u9891\u5206\u6790\u7684\u65f6\u7a7a\u5efa\u6a21\u548c\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2509.11169", "pdf": "https://arxiv.org/pdf/2509.11169", "abs": "https://arxiv.org/abs/2509.11169", "authors": ["Hong Zhang", "Fei Guo", "Zihan Xie", "Dizhao Yao"], "title": "Multispectral-NeRF:a multispectral modeling approach based on neural radiance fields", "categories": ["cs.CV"], "comment": null, "summary": "3D reconstruction technology generates three-dimensional representations of real-world objects, scenes, or environments using sensor data such as 2D images, with extensive applications in robotics, autonomous vehicles, and virtual reality systems. Traditional 3D reconstruction techniques based on 2D images typically relies on RGB spectral information. With advances in sensor technology, additional spectral bands beyond RGB have been increasingly incorporated into 3D reconstruction workflows. Existing methods that integrate these expanded spectral data often suffer from expensive scheme prices, low accuracy and poor geometric features. Three - dimensional reconstruction based on NeRF can effectively address the various issues in current multispectral 3D reconstruction methods, producing high - precision and high - quality reconstruction results. However, currently, NeRF and some improved models such as NeRFacto are trained on three - band data and cannot take into account the multi - band information. To address this problem, we propose Multispectral-NeRF, an enhanced neural architecture derived from NeRF that can effectively integrates multispectral information. Our technical contributions comprise threefold modifications: Expanding hidden layer dimensionality to accommodate 6-band spectral inputs; Redesigning residual functions to optimize spectral discrepancy calculations between reconstructed and reference images; Adapting data compression modules to address the increased bit-depth requirements of multispectral imagery. Experimental results confirm that Multispectral-NeRF successfully processes multi-band spectral features while accurately preserving the original scenes' spectral characteristics.", "AI": {"tldr": "\u63d0\u51fa\u4e86Multispectral-NeRF\uff0c\u4e00\u79cd\u57fa\u4e8eNeRF\u7684\u6539\u8fdb\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u5149\u8c31\u4fe1\u606f\u8fdb\u884c3D\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u5229\u7528\u591a\u6ce2\u6bb5\u4fe1\u606f\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf3D\u91cd\u5efa\u6280\u672f\u4e3b\u8981\u4f9d\u8d56RGB\u5149\u8c31\u4fe1\u606f\uff0c\u800c\u73b0\u6709\u7684\u591a\u5149\u8c313D\u91cd\u5efa\u65b9\u6cd5\u5b58\u5728\u6210\u672c\u9ad8\u3001\u7cbe\u5ea6\u4f4e\u548c\u51e0\u4f55\u7279\u5f81\u5dee\u7684\u95ee\u9898\u3002NeRF\u53ca\u5176\u6539\u8fdb\u6a21\u578b\u53ea\u80fd\u5904\u7406\u4e09\u6ce2\u6bb5\u6570\u636e\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u591a\u6ce2\u6bb5\u4fe1\u606f\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u6280\u672f\u6539\u8fdb\uff1a\u6269\u5c55\u9690\u85cf\u5c42\u7ef4\u5ea6\u4ee5\u5bb9\u7eb36\u6ce2\u6bb5\u5149\u8c31\u8f93\u5165\uff1b\u91cd\u65b0\u8bbe\u8ba1\u6b8b\u5dee\u51fd\u6570\u4ee5\u4f18\u5316\u91cd\u5efa\u56fe\u50cf\u4e0e\u53c2\u8003\u56fe\u50cf\u4e4b\u95f4\u7684\u5149\u8c31\u5dee\u5f02\u8ba1\u7b97\uff1b\u8c03\u6574\u6570\u636e\u538b\u7f29\u6a21\u5757\u4ee5\u9002\u5e94\u591a\u5149\u8c31\u56fe\u50cf\u589e\u52a0\u7684\u4f4d\u6df1\u5ea6\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eMultispectral-NeRF\u6210\u529f\u5904\u7406\u4e86\u591a\u6ce2\u6bb5\u5149\u8c31\u7279\u5f81\uff0c\u540c\u65f6\u51c6\u786e\u4fdd\u7559\u4e86\u539f\u59cb\u573a\u666f\u7684\u5149\u8c31\u7279\u6027\u3002", "conclusion": "Multispectral-NeRF\u6709\u6548\u89e3\u51b3\u4e86\u5f53\u524d\u591a\u5149\u8c313D\u91cd\u5efa\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u80fd\u591f\u4ea7\u751f\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u8d28\u91cf\u76843D\u91cd\u5efa\u7ed3\u679c\uff0c\u4e3a\u591a\u5149\u8c313D\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.11171", "pdf": "https://arxiv.org/pdf/2509.11171", "abs": "https://arxiv.org/abs/2509.11171", "authors": ["Zhiwen Yang", "Yuxin Peng"], "title": "SPHERE: Semantic-PHysical Engaged REpresentation for 3D Semantic Scene Completion", "categories": ["cs.CV"], "comment": "10 pages, 6 figures", "summary": "Camera-based 3D Semantic Scene Completion (SSC) is a critical task in autonomous driving systems, assessing voxel-level geometry and semantics for holistic scene perception. While existing voxel-based and plane-based SSC methods have achieved considerable progress, they struggle to capture physical regularities for realistic geometric details. On the other hand, neural reconstruction methods like NeRF and 3DGS demonstrate superior physical awareness, but suffer from high computational cost and slow convergence when handling large-scale, complex autonomous driving scenes, leading to inferior semantic accuracy. To address these issues, we propose the Semantic-PHysical Engaged REpresentation (SPHERE) for camera-based SSC, which integrates voxel and Gaussian representations for joint exploitation of semantic and physical information. First, the Semantic-guided Gaussian Initialization (SGI) module leverages dual-branch 3D scene representations to locate focal voxels as anchors to guide efficient Gaussian initialization. Then, the Physical-aware Harmonics Enhancement (PHE) module incorporates semantic spherical harmonics to model physical-aware contextual details and promote semantic-geometry consistency through focal distribution alignment, generating SSC results with realistic details. Extensive experiments and analyses on the popular SemanticKITTI and SSCBench-KITTI-360 benchmarks validate the effectiveness of SPHERE. The code is available at https://github.com/PKU-ICST-MIPL/SPHERE_ACMMM2025.", "AI": {"tldr": "SPHERE\u662f\u4e00\u79cd\u65b0\u9896\u7684\u76f8\u673a3D\u8bed\u4e49\u573a\u666f\u8865\u5168\u65b9\u6cd5\uff0c\u7ed3\u5408\u4f53\u7d20\u548c\u9ad8\u65af\u8868\u793a\uff0c\u901a\u8fc7\u8bed\u4e49\u5f15\u5bfc\u7684\u9ad8\u65af\u521d\u59cb\u5316\u548c\u7269\u7406\u611f\u77e5\u8c10\u6ce2\u589e\u5f3a\u6a21\u5757\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u903c\u771f\u7684\u51e0\u4f55\u7ec6\u8282\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u4f53\u7d20\u548c\u5e73\u9762\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7269\u7406\u89c4\u5f8b\u5b9e\u73b0\u903c\u771f\u51e0\u4f55\u7ec6\u8282\uff0c\u800c\u795e\u7ecf\u91cd\u5efa\u65b9\u6cd5\u5982NeRF\u548c3DGS\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u6536\u655b\u6162\uff0c\u8bed\u4e49\u51c6\u786e\u6027\u5dee\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u5904\u7406\u8bed\u4e49\u548c\u7269\u7406\u4fe1\u606f\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSPHERE\u6846\u67b6\uff1a1\uff09\u8bed\u4e49\u5f15\u5bfc\u9ad8\u65af\u521d\u59cb\u5316\u6a21\u5757\u5229\u7528\u53cc\u5206\u652f3D\u573a\u666f\u8868\u793a\u5b9a\u4f4d\u7126\u70b9\u4f53\u7d20\u4f5c\u4e3a\u951a\u70b9\uff1b2\uff09\u7269\u7406\u611f\u77e5\u8c10\u6ce2\u589e\u5f3a\u6a21\u5757\u901a\u8fc7\u8bed\u4e49\u7403\u8c10\u51fd\u6570\u5efa\u6a21\u7269\u7406\u611f\u77e5\u4e0a\u4e0b\u6587\u7ec6\u8282\uff0c\u901a\u8fc7\u7126\u70b9\u5206\u5e03\u5bf9\u9f50\u4fc3\u8fdb\u8bed\u4e49-\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "result": "\u5728SemanticKITTI\u548cSSCBench-KITTI-360\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u65b9\u6cd5\u6709\u6548\u3002", "conclusion": "SPHERE\u6210\u529f\u6574\u5408\u4e86\u4f53\u7d20\u548c\u9ad8\u65af\u8868\u793a\u7684\u4f18\u52bf\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u903c\u771f\u7684\u51e0\u4f55\u7ec6\u8282\u91cd\u5efa\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u611f\u77e5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.11213", "pdf": "https://arxiv.org/pdf/2509.11213", "abs": "https://arxiv.org/abs/2509.11213", "authors": ["Yufei Tang", "Daiheng Gao", "Pingyu Wu", "Wenbo Zhou", "Bang Zhang", "Weiming Zhang"], "title": "Beyond Sliders: Mastering the Art of Diffusion-based Image Manipulation", "categories": ["cs.CV"], "comment": "6 pages, 6 figures", "summary": "In the realm of image generation, the quest for realism and customization has never been more pressing. While existing methods like concept sliders have made strides, they often falter when it comes to no-AIGC images, particularly images captured in real world settings. To bridge this gap, we introduce Beyond Sliders, an innovative framework that integrates GANs and diffusion models to facilitate sophisticated image manipulation across diverse image categories. Improved upon concept sliders, our method refines the image through fine grained guidance both textual and visual in an adversarial manner, leading to a marked enhancement in image quality and realism. Extensive experimental validation confirms the robustness and versatility of Beyond Sliders across a spectrum of applications.", "AI": {"tldr": "Beyond Sliders\u662f\u4e00\u4e2a\u521b\u65b0\u6846\u67b6\uff0c\u7ed3\u5408GAN\u548c\u6269\u6563\u6a21\u578b\uff0c\u5728\u6982\u5ff5\u6ed1\u5757\u57fa\u7840\u4e0a\u6539\u8fdb\uff0c\u901a\u8fc7\u6587\u672c\u548c\u89c6\u89c9\u7684\u7ec6\u7c92\u5ea6\u5bf9\u6297\u6027\u6307\u5bfc\u6765\u589e\u5f3a\u56fe\u50cf\u8d28\u91cf\u548c\u771f\u5b9e\u611f\uff0c\u7279\u522b\u9488\u5bf9\u975eAIGC\u56fe\u50cf\u5904\u7406\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982\u6982\u5ff5\u6ed1\u5757\u5728\u5904\u7406\u975eAIGC\u56fe\u50cf\uff08\u7279\u522b\u662f\u771f\u5b9e\u4e16\u754c\u62cd\u6444\u7684\u56fe\u50cf\uff09\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8de8\u8d8a\u4e0d\u540c\u56fe\u50cf\u7c7b\u522b\u8fdb\u884c\u590d\u6742\u56fe\u50cf\u64cd\u4f5c\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u96c6\u6210GAN\u548c\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u6587\u672c\u548c\u89c6\u89c9\u7684\u7ec6\u7c92\u5ea6\u5bf9\u6297\u6027\u6307\u5bfc\u6765\u7cbe\u70bc\u56fe\u50cf\uff0c\u6539\u8fdb\u6982\u5ff5\u6ed1\u5757\u65b9\u6cd5\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8bc1\u5b9e\u4e86Beyond Sliders\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u591a\u529f\u80fd\u6027\uff0c\u56fe\u50cf\u8d28\u91cf\u548c\u771f\u5b9e\u611f\u663e\u8457\u63d0\u5347\u3002", "conclusion": "Beyond Sliders\u6210\u529f\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u975eAIGC\u56fe\u50cf\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u8de8\u7c7b\u522b\u56fe\u50cf\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.11220", "pdf": "https://arxiv.org/pdf/2509.11220", "abs": "https://arxiv.org/abs/2509.11220", "authors": ["Gao Yu Lee", "Tanmoy Dam", "Md Meftahul Ferdaus", "Daniel Puiu Poenar", "Vu N. Duong"], "title": "ANROT-HELANet: Adverserially and Naturally Robust Attention-Based Aggregation Network via The Hellinger Distance for Few-Shot Classification", "categories": ["cs.CV"], "comment": "Preprint version. The manuscript has been submitted to a journal. All   changes will be transferred to the final version if accepted. Also an   erratum: In Figure 10 and 11, the $\\epsilon = 0.005$ value should be   $\\epsilon = 0.05$", "summary": "Few-Shot Learning (FSL), which involves learning to generalize using only a few data samples, has demonstrated promising and superior performances to ordinary CNN methods. While Bayesian based estimation approaches using Kullback-Leibler (KL) divergence have shown improvements, they remain vulnerable to adversarial attacks and natural noises. We introduce ANROT-HELANet, an Adversarially and Naturally RObusT Hellinger Aggregation Network that significantly advances the state-of-the-art in FSL robustness and performance. Our approach implements an adversarially and naturally robust Hellinger distance-based feature class aggregation scheme, demonstrating resilience to adversarial perturbations up to $\\epsilon=0.30$ and Gaussian noise up to $\\sigma=0.30$. The network achieves substantial improvements across benchmark datasets, including gains of 1.20\\% and 1.40\\% for 1-shot and 5-shot scenarios on miniImageNet respectively. We introduce a novel Hellinger Similarity contrastive loss function that generalizes cosine similarity contrastive loss for variational few-shot inference scenarios. Our approach also achieves superior image reconstruction quality with a FID score of 2.75, outperforming traditional VAE (3.43) and WAE (3.38) approaches. Extensive experiments conducted on four few-shot benchmarked datasets verify that ANROT-HELANet's combination of Hellinger distance-based feature aggregation, attention mechanisms, and our novel loss function establishes new state-of-the-art performance while maintaining robustness against both adversarial and natural perturbations. Our code repository will be available at https://github.com/GreedYLearner1146/ANROT-HELANet/tree/main.", "AI": {"tldr": "ANROT-HELANet\u662f\u4e00\u4e2a\u57fa\u4e8eHellinger\u8ddd\u79bb\u7684\u5bf9\u6297\u6027\u548c\u81ea\u7136\u9c81\u68d2\u7684\u5c0f\u6837\u672c\u5b66\u4e60\u7f51\u7edc\uff0c\u5728\u5bf9\u6297\u6270\u52a8\u548c\u566a\u58f0\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eKL\u6563\u5ea6\u7684\u8d1d\u53f6\u65af\u4f30\u8ba1\u65b9\u6cd5\u867d\u7136\u5728\u5c0f\u6837\u672c\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u4e8e\u666e\u901aCNN\u65b9\u6cd5\uff0c\u4f46\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u653b\u51fb\u548c\u81ea\u7136\u566a\u58f0\u7684\u5f71\u54cd\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86ANROT-HELANet\u7f51\u7edc\uff0c\u91c7\u7528\u57fa\u4e8eHellinger\u8ddd\u79bb\u7684\u7279\u5f81\u7c7b\u522b\u805a\u5408\u65b9\u6848\uff0c\u5f15\u5165\u65b0\u9896\u7684Hellinger\u76f8\u4f3c\u6027\u5bf9\u6bd4\u635f\u5931\u51fd\u6570\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u5bf9\u5bf9\u6297\u6270\u52a8\u548c\u81ea\u7136\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5bf9\u6297\u6270\u52a8\u03b5=0.30\u548c\u9ad8\u65af\u566a\u58f0\u03c3=0.30\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\uff1b\u5728miniImageNet\u4e0a1-shot\u548c5-shot\u573a\u666f\u5206\u522b\u63d0\u53471.20%\u548c1.40%\uff1b\u56fe\u50cf\u91cd\u5efa\u8d28\u91cfFID\u5f97\u52062.75\uff0c\u4f18\u4e8e\u4f20\u7edfVAE\u548cWAE\u65b9\u6cd5\u3002", "conclusion": "ANROT-HELANet\u901a\u8fc7Hellinger\u8ddd\u79bb\u7279\u5f81\u805a\u5408\u3001\u6ce8\u610f\u529b\u673a\u5236\u548c\u65b0\u9896\u635f\u5931\u51fd\u6570\u7684\u7ec4\u5408\uff0c\u5728\u4fdd\u6301\u5bf9\u6297\u6027\u548c\u81ea\u7136\u9c81\u68d2\u6027\u7684\u540c\u65f6\uff0c\u5728\u5c0f\u6837\u672c\u5b66\u4e60\u4efb\u52a1\u4e2d\u786e\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002"}}
{"id": "2509.11275", "pdf": "https://arxiv.org/pdf/2509.11275", "abs": "https://arxiv.org/abs/2509.11275", "authors": ["Lianjun Liao", "Chunhui Zhang", "Tong Wu", "Henglei Lv", "Bailin Deng", "Lin Gao"], "title": "ROSGS: Relightable Outdoor Scenes With Gaussian Splatting", "categories": ["cs.CV", "I.2.10; I.3"], "comment": null, "summary": "Image data captured outdoors often exhibit unbounded scenes and unconstrained, varying lighting conditions, making it challenging to decompose them into geometry, reflectance, and illumination. Recent works have focused on achieving this decomposition using Neural Radiance Fields (NeRF) or the 3D Gaussian Splatting (3DGS) representation but remain hindered by two key limitations: the high computational overhead associated with neural networks of NeRF and the use of low-frequency lighting representations, which often result in inefficient rendering and suboptimal relighting accuracy. We propose ROSGS, a two-stage pipeline designed to efficiently reconstruct relightable outdoor scenes using the Gaussian Splatting representation. By leveraging monocular normal priors, ROSGS first reconstructs the scene's geometry with the compact 2D Gaussian Splatting (2DGS) representation, providing an efficient and accurate geometric foundation. Building upon this reconstructed geometry, ROSGS then decomposes the scene's texture and lighting through a hybrid lighting model. This model effectively represents typical outdoor lighting by employing a spherical Gaussian function to capture the directional, high-frequency components of sunlight, while learning a radiance transfer function via Spherical Harmonic coefficients to model the remaining low-frequency skylight comprehensively. Both quantitative metrics and qualitative comparisons demonstrate that ROSGS achieves state-of-the-art performance in relighting outdoor scenes and highlight its ability to deliver superior relighting accuracy and rendering efficiency.", "AI": {"tldr": "ROSGS\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u9ad8\u6548\u5ba4\u5916\u573a\u666f\u91cd\u5149\u7167\u91cd\u5efa\u65b9\u6cd5\uff0c\u4f7f\u7528\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\uff0c\u7ed3\u5408\u5355\u76ee\u6cd5\u7ebf\u5148\u9a8c\u548c\u6df7\u5408\u5149\u7167\u6a21\u578b\uff0c\u5728\u91cd\u5149\u7167\u7cbe\u5ea6\u548c\u6e32\u67d3\u6548\u7387\u65b9\u9762\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u5ba4\u5916\u56fe\u50cf\u6570\u636e\u901a\u5e38\u5305\u542b\u65e0\u754c\u573a\u666f\u548c\u53d8\u5316\u7684\u7167\u660e\u6761\u4ef6\uff0c\u73b0\u6709\u65b9\u6cd5\u5982NeRF\u548c3DGS\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u548c\u4f4e\u9891\u5149\u7167\u8868\u793a\u5bfc\u81f4\u6e32\u67d3\u6548\u7387\u4f4e\u3001\u91cd\u5149\u7167\u7cbe\u5ea6\u5dee\u7684\u95ee\u9898", "method": "\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a1) \u4f7f\u7528\u5355\u76ee\u6cd5\u7ebf\u5148\u9a8c\u548c2D\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u573a\u666f\u51e0\u4f55\uff1b2) \u901a\u8fc7\u6df7\u5408\u5149\u7167\u6a21\u578b\u5206\u89e3\u7eb9\u7406\u548c\u5149\u7167\uff0c\u4f7f\u7528\u7403\u9762\u9ad8\u65af\u51fd\u6570\u6355\u6349\u9ad8\u9891\u9633\u5149\u5206\u91cf\uff0c\u7403\u8c10\u7cfb\u6570\u5b66\u4e60\u4f4e\u9891\u5929\u5149", "result": "\u5b9a\u91cf\u6307\u6807\u548c\u5b9a\u6027\u6bd4\u8f83\u8868\u660e\uff0cROSGS\u5728\u5ba4\u5916\u573a\u666f\u91cd\u5149\u7167\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5177\u6709\u4f18\u8d8a\u7684\u91cd\u5149\u7167\u7cbe\u5ea6\u548c\u6e32\u67d3\u6548\u7387", "conclusion": "ROSGS\u901a\u8fc7\u521b\u65b0\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u548c\u6df7\u5408\u5149\u7167\u8868\u793a\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5ba4\u5916\u573a\u666f\u91cd\u5149\u7167\u7684\u6311\u6218\uff0c\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272"}}
{"id": "2509.11385", "pdf": "https://arxiv.org/pdf/2509.11385", "abs": "https://arxiv.org/abs/2509.11385", "authors": ["Akhil Padmanabha", "Arpit Agarwal", "Catherine Li", "Austin Williams", "Dinesh K. Patel", "Sankalp Chopkar", "Achu Wilson", "Ahmet Ozkan", "Wenzhen Yuan", "Sonal Choudhary", "Arash Mostaghimi", "Zackory Erickson", "Carmel Majidi"], "title": "In-Vivo Skin 3-D Surface Reconstruction and Wrinkle Depth Estimation using Handheld High Resolution Tactile Sensing", "categories": ["cs.CV"], "comment": null, "summary": "Three-dimensional (3-D) skin surface reconstruction offers promise for objective and quantitative dermatological assessment, but no portable, high-resolution device exists that has been validated and used for depth reconstruction across various body locations. We present a compact 3-D skin reconstruction probe based on GelSight tactile imaging with a custom elastic gel and a learning-based reconstruction algorithm for micron-level wrinkle height estimation. Our probe, integrated into a handheld probe with force sensing for consistent contact, achieves a mean absolute error of 12.55 micron on wrinkle-like test objects. In a study with 15 participants without skin disorders, we provide the first validated wrinkle depth metrics across multiple body regions. We further demonstrate statistically significant reductions in wrinkle height at three locations following over-the-counter moisturizer application. Our work offers a validated tool for clinical and cosmetic skin analysis, with potential applications in diagnosis, treatment monitoring, and skincare efficacy evaluation.", "AI": {"tldr": "\u4e00\u79cd\u57fa\u4e8eGelSight\u89e6\u89c9\u6210\u50cf\u6280\u672f\u7684\u624b\u6301\u5f0f3D\u76ae\u80a4\u91cd\u5efa\u63a2\u5934\uff0c\u901a\u8fc7\u5b66\u4e60\u7b97\u6cd5\u5b9e\u73b0\u5fae\u7c73\u7ea7\u7ec6\u7eb9\u6df1\u5ea6\u4f30\u8ba1\uff0c\u5728\u591a\u4e2a\u8eab\u4f53\u533a\u57df\u63d0\u4f9b\u7b2c\u4e00\u4e2a\u7ecf\u9a8c\u8bc1\u7684\u7ec6\u7eb9\u6df1\u5ea6\u8bc4\u4f30\u5de5\u5177\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u7cbe\u786e\u3001\u53ef\u4f9b\u5b9e\u9645\u5e94\u7528\u7684\u4e09\u7ef4\u76ae\u80a4\u8868\u9762\u91cd\u5efa\u8bbe\u5907\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u4ee5\u5728\u5404\u79cd\u8eab\u4f53\u4f4d\u7f6e\u8fdb\u884c\u9ad8\u5206\u8fa8\u7387\u6df1\u5ea6\u91cd\u5efa\u7684\u7b26\u5408\u4ea4\u4ed8\u8981\u6c42\u7684\u624b\u6301\u5f0f\u8bbe\u5907\u3002", "method": "\u57fa\u4e8eGelSight\u89e6\u89c9\u6210\u50cf\u6280\u672f\uff0c\u4f7f\u7528\u81ea\u5b9a\u5236\u5f39\u6027\u51dd\u80f6\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u91cd\u5efa\u7b97\u6cd5\uff0c\u96c6\u6210\u5230\u5177\u6709\u529b\u611f\u6d4b\u91cf\u529f\u80fd\u7684\u624b\u6301\u5f0f\u63a2\u5934\u4e2d\uff0c\u4ee5\u4fdd\u8bc1\u4e00\u81f4\u7684\u63a5\u89e6\u6761\u4ef6\u3002", "result": "\u5728\u7ec6\u7eb9\u6a21\u62df\u6d4b\u8bd5\u7269\u4f53\u4e0a\u5b9e\u73b0\u4e8612.55\u5fae\u7c73\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff0c\u572815\u540d\u5065\u5eb7\u53c2\u4e0e\u8005\u7684\u591a\u4e2a\u8eab\u4f53\u533a\u57df\u63d0\u4f9b\u4e86\u7b2c\u4e00\u4e2a\u7ecf\u9a8c\u8bc1\u7684\u7ec6\u7eb9\u6df1\u5ea6\u6307\u6807\uff0c\u5e76\u8bc1\u660e\u4e86\u4f7f\u7528\u8fc7\u8ba2\u62a4\u80a4\u971c\u540e\u4e09\u4e2a\u4f4d\u7f6e\u7684\u7ec6\u7eb9\u9ad8\u5ea6\u51cf\u5c11\u5177\u6709\u7edf\u8ba1\u5b66\u663e\u8457\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u4e34\u5e8a\u548c\u5316\u5986\u76ae\u80a4\u5206\u6790\uff0c\u5728\u8bca\u65ad\u3001\u6cbb\u7597\u76d1\u6d4b\u548c\u62a4\u80a4\u6548\u679c\u8bc4\u4f30\u65b9\u9762\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.11589", "pdf": "https://arxiv.org/pdf/2509.11589", "abs": "https://arxiv.org/abs/2509.11589", "authors": ["Yanyun Pu", "Kehan Li", "Zeyi Huang", "Zhijie Zhong", "Kaixiang Yang"], "title": "MVQA-68K: A Multi-dimensional and Causally-annotated Dataset with Quality Interpretability for Video Assessment", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of video generation models such as Sora, video quality assessment (VQA) is becoming increasingly crucial for selecting high-quality videos from large-scale datasets used in pre-training. Traditional VQA methods, typically producing single numerical scores, often lack comprehensiveness and interpretability. To address these challenges, we introduce MVQA-68K, a novel multi-dimensional VQA dataset comprising over 68,000 carefully annotated videos, covering seven essential quality dimensions: overall aesthetics, camera movement, dynamic degree, texture detail, composition, visual quality, and factual consistency. Each annotation includes detailed chain-of-thought reasoning to facilitate interpretability and comprehensive understanding. Extensive experiments demonstrate that MVQA-68K significantly enhances the performance of various multimodal large language models (MLLMs) on the VQA task, achieving state-of-the-art results not only on our internal test set (Fig.1) but also on public benchmarks including LSVQ-test, LSVQ-1080p, and LIVE-VQC. Meantime, incorporating explicit reasoning process during VQA training substantially boosts the zero-shot generalization. Code and dataset will be available at github: https://github.com/Controller01-ai/MVQA-68K", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51faMVQA-68K\u591a\u7ef4\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u5305\u542b68,000\u4e2a\u89c6\u9891\u548c7\u4e2a\u8d28\u91cf\u7ef4\u5ea6\u7684\u8be6\u7ec6\u6ce8\u91ca\uff0c\u901a\u8fc7\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u63d0\u9ad8\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684VQA\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u4ea7\u751f\u7684\u5355\u4e00\u6570\u503c\u5206\u6570\u7f3a\u4e4f\u5168\u9762\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u65e0\u6cd5\u6ee1\u8db3Sora\u7b49\u89c6\u9891\u751f\u6210\u6a21\u578b\u5bf9\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u89c6\u9891\u6570\u636e\u96c6\u7684\u9700\u6c42\u3002", "method": "\u6784\u5efaMVQA-68K\u6570\u636e\u96c6\uff0c\u5305\u542b68,000\u4e2a\u8be6\u7ec6\u6ce8\u91ca\u7684\u89c6\u9891\uff0c\u8986\u76d67\u4e2a\u5173\u952e\u8d28\u91cf\u7ef4\u5ea6\uff1a\u6574\u4f53\u7f8e\u5b66\u3001\u6444\u50cf\u673a\u8fd0\u52a8\u3001\u52a8\u6001\u7a0b\u5ea6\u3001\u7eb9\u7406\u7ec6\u8282\u3001\u6784\u56fe\u3001\u89c6\u89c9\u8d28\u91cf\u548c\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002\u6bcf\u4e2a\u6ce8\u91ca\u5305\u542b\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aMVQA-68K\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728VQA\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5728\u5185\u90e8\u6d4b\u8bd5\u96c6\u548c\u516c\u5f00\u6d4b\u8bd5\u96c6\uff08LSVQ-test\u3001LSVQ-1080p\u3001LIVE-VQC\uff09\u4e0a\u8fbe\u5230\u6700\u9ad8\u6c34\u5e73\u3002\u540c\u65f6\uff0c\u5728VQA\u8bad\u7ec3\u4e2d\u878d\u5165\u663e\u5f0f\u63a8\u7406\u8fc7\u7a0b\u663e\u8457\u63d0\u9ad8\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MVQA-68K\u6570\u636e\u96c6\u901a\u8fc7\u591a\u7ef4\u5ea6\u8d28\u91cf\u6ce8\u91ca\u548c\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfVQA\u65b9\u6cd5\u7684\u7f3a\u9677\uff0c\u4e3a\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u3001\u53ef\u89e3\u91ca\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5bf9\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u8bad\u7ec3\u6570\u636e\u9009\u62e9\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2509.11624", "pdf": "https://arxiv.org/pdf/2509.11624", "abs": "https://arxiv.org/abs/2509.11624", "authors": ["Wending Liu", "Siyun Liang", "Huy H. Nguyen", "Isao Echizen"], "title": "A Controllable 3D Deepfake Generation Framework with Gaussian Splatting", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We propose a novel 3D deepfake generation framework based on 3D Gaussian Splatting that enables realistic, identity-preserving face swapping and reenactment in a fully controllable 3D space. Compared to conventional 2D deepfake approaches that suffer from geometric inconsistencies and limited generalization to novel view, our method combines a parametric head model with dynamic Gaussian representations to support multi-view consistent rendering, precise expression control, and seamless background integration. To address editing challenges in point-based representations, we explicitly separate the head and background Gaussians and use pre-trained 2D guidance to optimize the facial region across views. We further introduce a repair module to enhance visual consistency under extreme poses and expressions. Experiments on NeRSemble and additional evaluation videos demonstrate that our method achieves comparable performance to state-of-the-art 2D approaches in identity preservation, as well as pose and expression consistency, while significantly outperforming them in multi-view rendering quality and 3D consistency. Our approach bridges the gap between 3D modeling and deepfake synthesis, enabling new directions for scene-aware, controllable, and immersive visual forgeries, revealing the threat that emerging 3D Gaussian Splatting technique could be used for manipulation attacks.", "AI": {"tldr": "\u57fa\u4e8e3D\u9ad8\u65af\u62d6\u5c3e\u7684\u65b0\u98983D\u6df1\u5ea6\u4f2a\u9020\u751f\u6210\u6846\u67b6\uff0c\u652f\u6301\u591a\u89c6\u89d2\u4e00\u81f4\u7684\u9762\u90e8\u6362\u8138\u548c\u91cd\u73b0\uff0c\u5145\u5206\u5229\u75293D\u9ad8\u65af\u8868\u793a\u7684\u4f18\u52bf", "motivation": "\u89e3\u51b3\u4f20\u7edf2D\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u5728\u51e0\u4f55\u4e0d\u4e00\u81f4\u6027\u3001\u65b0\u89c6\u89d2\u6cbf\u751f\u548c\u63a7\u5236\u6027\u65b9\u9762\u7684\u9650\u5236\uff0c\u5c1d\u8bd5\u5c063D\u5efa\u6a21\u4e0e\u6df1\u5ea6\u4f2a\u9020\u5408\u6210\u76f8\u7ed3\u5408", "method": "\u7ed3\u5408\u53c2\u6570\u5316\u5934\u90e8\u6a21\u578b\u4e0e\u52a8\u6001\u9ad8\u65af\u8868\u793a\uff0c\u660e\u786e\u5206\u79bb\u5934\u90e8\u548c\u80cc\u666f\u9ad8\u65af\uff0c\u4f7f\u7528\u9884\u8bad\u7ec32D\u6307\u5bfc\u8fdb\u884c\u591a\u89c6\u89d2\u4f18\u5316\uff0c\u5e76\u4e3b\u52a8\u4fee\u590d\u6781\u7aef\u59ff\u52bf\u548c\u8868\u60c5\u4e0b\u7684\u89c6\u89c9\u4e00\u81f4\u6027", "result": "\u5728NeRSemble\u6570\u636e\u96c6\u4e0a\u8bc6\u522b\u4fdd\u6301\u3001\u59ff\u52bf\u8868\u60c5\u4e00\u81f4\u6027\u65b9\u9762\u4e0e\u6700\u4f732D\u65b9\u6cd5\u76f8\u5f53\uff0c\u5728\u591a\u89c6\u89d2\u6e32\u67d3\u8d28\u91cf\u548c3D\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5bf92D\u65b9\u6cd5", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u573a\u666f\u611f\u77e5\u3001\u53ef\u63a7\u5236\u548c\u6c89\u6d78\u5f0f\u89c6\u89c9\u4f2a\u9020\u5f00\u542f\u4e86\u65b0\u65b9\u5411\uff0c\u540c\u65f6\u4e5f\u544a\u8bc9\u4e863D\u9ad8\u65af\u62d6\u5c3e\u6280\u672f\u53ef\u80fd\u88ab\u7528\u4e8e\u64cd\u7eb7\u653b\u51fb\u7684\u98ce\u9669"}}
{"id": "2509.11638", "pdf": "https://arxiv.org/pdf/2509.11638", "abs": "https://arxiv.org/abs/2509.11638", "authors": ["Yongzhe Lyu", "Yu Wu", "Yutian Lin", "Bo Du"], "title": "IS-Diff: Improving Diffusion-Based Inpainting with Better Initial Seed", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have shown promising results in free-form inpainting. Recent studies based on refined diffusion samplers or novel architectural designs led to realistic results and high data consistency. However, random initialization seed (noise) adopted in vanilla diffusion process may introduce mismatched semantic information in masked regions, leading to biased inpainting results, e.g., low consistency and low coherence with the other unmasked area. To address this issue, we propose the Initial Seed refined Diffusion Model (IS-Diff), a completely training-free approach incorporating distributional harmonious seeds to produce harmonious results. Specifically, IS-Diff employs initial seeds sampled from unmasked areas to imitate the masked data distribution, thereby setting a promising direction for the diffusion procedure. Moreover, a dynamic selective refinement mechanism is proposed to detect severe unharmonious inpaintings in intermediate latent and adjust the strength of our initialization prior dynamically. We validate our method on both standard and large-mask inpainting tasks using the CelebA-HQ, ImageNet, and Places2 datasets, demonstrating its effectiveness across all metrics compared to state-of-the-art inpainting methods.", "AI": {"tldr": "IS-Diff\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u4fee\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u975e\u63a9\u7801\u533a\u57df\u91c7\u6837\u521d\u59cb\u5316\u79cd\u5b50\u6765\u751f\u6210\u8bed\u4e49\u4e00\u81f4\u7684\u4fee\u590d\u7ed3\u679c\uff0c\u907f\u514d\u4f20\u7edf\u968f\u673a\u521d\u59cb\u5316\u5bfc\u81f4\u7684\u8bed\u4e49\u4e0d\u5339\u914d\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6269\u6563\u6a21\u578b\u5728\u4fee\u590d\u4efb\u52a1\u4e2d\u4f7f\u7528\u968f\u673a\u521d\u59cb\u5316\u79cd\u5b50\uff0c\u53ef\u80fd\u5bfc\u81f4\u63a9\u7801\u533a\u57df\u4e0e\u975e\u63a9\u7801\u533a\u57df\u7684\u8bed\u4e49\u4fe1\u606f\u4e0d\u5339\u914d\uff0c\u4ea7\u751f\u4f4e\u4e00\u81f4\u6027\u548c\u4f4e\u8fde\u8d2f\u6027\u7684\u4fee\u590d\u7ed3\u679c\u3002", "method": "\u63d0\u51faIS-Diff\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u4ece\u975e\u63a9\u7801\u533a\u57df\u91c7\u6837\u7684\u5206\u5e03\u548c\u8c10\u79cd\u5b50\u4f5c\u4e3a\u521d\u59cb\u5316\uff1b2\uff09\u8bbe\u8ba1\u52a8\u6001\u9009\u62e9\u6027\u7cbe\u70bc\u673a\u5236\uff0c\u68c0\u6d4b\u4e2d\u95f4\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u4e0d\u548c\u8c10\u4fee\u590d\u5e76\u52a8\u6001\u8c03\u6574\u521d\u59cb\u5316\u5148\u9a8c\u5f3a\u5ea6\u3002", "result": "\u5728CelebA-HQ\u3001ImageNet\u548cPlaces2\u6570\u636e\u96c6\u7684\u6807\u51c6\u548c\u5927\u63a9\u7801\u4fee\u590d\u4efb\u52a1\u4e2d\uff0cIS-Diff\u5728\u6240\u6709\u6307\u6807\u4e0a\u90fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4fee\u590d\u65b9\u6cd5\u3002", "conclusion": "IS-Diff\u901a\u8fc7\u5206\u5e03\u548c\u8c10\u7684\u521d\u59cb\u5316\u79cd\u5b50\u548c\u52a8\u6001\u7cbe\u70bc\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u4fee\u590d\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u8fde\u8d2f\u6027\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002"}}
{"id": "2509.11642", "pdf": "https://arxiv.org/pdf/2509.11642", "abs": "https://arxiv.org/abs/2509.11642", "authors": ["Qiyuan Guan", "Qianfeng Yang", "Xiang Chen", "Tianyu Song", "Guiyue Jin", "Jiyu Jin"], "title": "WeatherBench: A Real-World Benchmark Dataset for All-in-One Adverse Weather Image Restoration", "categories": ["cs.CV"], "comment": "Accepted by ACMMM 2025 Datasets Track", "summary": "Existing all-in-one image restoration approaches, which aim to handle multiple weather degradations within a single framework, are predominantly trained and evaluated using mixed single-weather synthetic datasets. However, these datasets often differ significantly in resolution, style, and domain characteristics, leading to substantial domain gaps that hinder the development and fair evaluation of unified models. Furthermore, the lack of a large-scale, real-world all-in-one weather restoration dataset remains a critical bottleneck in advancing this field. To address these limitations, we present a real-world all-in-one adverse weather image restoration benchmark dataset, which contains image pairs captured under various weather conditions, including rain, snow, and haze, as well as diverse outdoor scenes and illumination settings. The resulting dataset provides precisely aligned degraded and clean images, enabling supervised learning and rigorous evaluation. We conduct comprehensive experiments by benchmarking a variety of task-specific, task-general, and all-in-one restoration methods on our dataset. Our dataset offers a valuable foundation for advancing robust and practical all-in-one image restoration in real-world scenarios. The dataset has been publicly released and is available at https://github.com/guanqiyuan/WeatherBench.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u5168\u80fd\u5929\u6c14\u56fe\u50cf\u6062\u590d\u6570\u636e\u96c6WeatherBench\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u9884\u8bad\u7ec3\u6a21\u578b\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\u5b58\u5728\u7684\u9886\u57df\u5dee\u5f02\u95ee\u9898\uff0c\u4e3a\u5168\u80fd\u5929\u6c14\u6062\u590d\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u7684\u5168\u80fd\u5929\u6c14\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u96c6\u5728\u5206\u8fa8\u7387\u3001\u98ce\u683c\u548c\u9886\u57df\u7279\u5f81\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5bfc\u81f4\u4e86\u5927\u7684\u9886\u57df\u95f4\u9694\uff0c\u963b\u788d\u4e86\u7edf\u4e00\u6a21\u578b\u7684\u53d1\u5c55\u548c\u516c\u5e73\u8bc4\u4f30\u3002\u540c\u65f6\uff0c\u7f3a\u4e4f\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u7684\u5168\u80fd\u5929\u6c14\u6062\u590d\u6570\u636e\u96c6\u662f\u8be5\u9886\u57df\u53d1\u5c55\u7684\u5173\u952e\u74f6\u9888\u3002", "method": "\u7814\u7a76\u4eba\u5458\u6784\u5efa\u4e86\u4e00\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u5168\u80fd\u5929\u6c14\u6076\u52a3\u5929\u6c14\u56fe\u50cf\u6062\u590d\u6570\u636e\u96c6\u57fa\u51c6\uff0c\u5305\u542b\u5728\u591a\u79cd\u5929\u6c14\u6761\u4ef6\uff08\u5982\u96e8\u3001\u96ea\u3001\u971c\uff09\u4e0b\u6355\u83b7\u7684\u56fe\u50cf\u5bf9\uff0c\u4ee5\u53ca\u591a\u6837\u7684\u6237\u5916\u573a\u666f\u548c\u7167\u660e\u8bbe\u7f6e\u3002\u8be5\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u7cbe\u786e\u5bf9\u9f50\u7684\u9000\u5316\u548c\u6e05\u6f84\u56fe\u50cf\uff0c\u652f\u6301\u76d1\u7763\u5b66\u4e60\u548c\u4e25\u683c\u7684\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u4eba\u5458\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u5bf9\u5404\u79cd\u4efb\u52a1\u7279\u5b9a\u3001\u4efb\u52a1\u901a\u7528\u548c\u5168\u80fd\u5929\u6c14\u6062\u590d\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u9a8c\u5bf9\u6bd4\u8bc4\u6d4b\u3002\u8be5\u6570\u636e\u96c6\u4e3a\u63a8\u52a8\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e0b\u5065\u58ee\u548c\u5b9e\u7528\u7684\u5168\u80fd\u5929\u6c14\u56fe\u50cf\u6062\u590d\u6280\u672f\u63d0\u4f9b\u4e86\u4ef7\u503c\u8f83\u9ad8\u7684\u57fa\u7840\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6784\u5efaWeatherBench\u6570\u636e\u96c6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f53\u524d\u5168\u80fd\u5929\u6c14\u56fe\u50cf\u6062\u590d\u9884\u8bad\u7ec3\u6a21\u578b\u9762\u4e34\u7684\u6570\u636e\u96c6\u95ee\u9898\u3002\u8be5\u6570\u636e\u96c6\u5df2\u7ecf\u516c\u5f00\u53d1\u5e03\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8d44\u6e90\u652f\u6301\u3002"}}
{"id": "2509.11661", "pdf": "https://arxiv.org/pdf/2509.11661", "abs": "https://arxiv.org/abs/2509.11661", "authors": ["Lifei Hao", "Yue Cheng", "Baoqi Huang", "Bing Jia", "Xuandong Zhao"], "title": "DTGen: Generative Diffusion-Based Few-Shot Data Augmentation for Fine-Grained Dirty Tableware Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Intelligent tableware cleaning is a critical application in food safety and smart homes, but existing methods are limited by coarse-grained classification and scarcity of few-shot data, making it difficult to meet industrialization requirements. We propose DTGen, a few-shot data augmentation scheme based on generative diffusion models, specifically designed for fine-grained dirty tableware recognition. DTGen achieves efficient domain specialization through LoRA, generates diverse dirty images via structured prompts, and ensures data quality through CLIP-based cross-modal filtering. Under extremely limited real few-shot conditions, DTGen can synthesize virtually unlimited high-quality samples, significantly improving classifier performance and supporting fine-grained dirty tableware recognition. We further elaborate on lightweight deployment strategies, promising to transfer DTGen's benefits to embedded dishwashers and integrate with cleaning programs to intelligently regulate energy consumption and detergent usage. Research results demonstrate that DTGen not only validates the value of generative AI in few-shot industrial vision but also provides a feasible deployment path for automated tableware cleaning and food safety monitoring.", "AI": {"tldr": "DTGen\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5c11\u6837\u672c\u6570\u636e\u589e\u5f3a\u65b9\u6848\uff0c\u4e13\u95e8\u7528\u4e8e\u7ec6\u7c92\u5ea6\u810f\u9910\u5177\u8bc6\u522b\uff0c\u901a\u8fc7LoRA\u5b9e\u73b0\u9ad8\u6548\u9886\u57df\u4e13\u4e1a\u5316\uff0c\u751f\u6210\u591a\u6837\u5316\u810f\u6c61\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7CLIP\u8de8\u6a21\u6001\u8fc7\u6ee4\u786e\u4fdd\u6570\u636e\u8d28\u91cf\u3002", "motivation": "\u667a\u80fd\u9910\u5177\u6e05\u6d01\u5728\u98df\u54c1\u5b89\u5168\u548c\u667a\u80fd\u5bb6\u5c45\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u7c97\u7c92\u5ea6\u5206\u7c7b\u548c\u5c11\u6837\u672c\u6570\u636e\u7a00\u7f3a\uff0c\u96be\u4ee5\u6ee1\u8db3\u5de5\u4e1a\u5316\u9700\u6c42\u3002", "method": "\u57fa\u4e8e\u751f\u6210\u6269\u6563\u6a21\u578b\u7684\u5c11\u6837\u672c\u6570\u636e\u589e\u5f3a\u65b9\u6848\uff0c\u4f7f\u7528LoRA\u5b9e\u73b0\u9ad8\u6548\u9886\u57df\u4e13\u4e1a\u5316\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u751f\u6210\u591a\u6837\u5316\u810f\u6c61\u56fe\u50cf\uff0c\u91c7\u7528CLIP\u8de8\u6a21\u6001\u8fc7\u6ee4\u786e\u4fdd\u6570\u636e\u8d28\u91cf\u3002", "result": "\u5728\u6781\u6709\u9650\u7684\u771f\u5b9e\u5c11\u6837\u672c\u6761\u4ef6\u4e0b\uff0cDTGen\u80fd\u5408\u6210\u51e0\u4e4e\u65e0\u9650\u7684\u9ad8\u8d28\u91cf\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u5206\u7c7b\u5668\u6027\u80fd\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u810f\u9910\u5177\u8bc6\u522b\u3002", "conclusion": "DTGen\u4e0d\u4ec5\u9a8c\u8bc1\u4e86\u751f\u6210\u5f0fAI\u5728\u5c11\u6837\u672c\u5de5\u4e1a\u89c6\u89c9\u4e2d\u7684\u4ef7\u503c\uff0c\u8fd8\u4e3a\u81ea\u52a8\u5316\u9910\u5177\u6e05\u6d01\u548c\u98df\u54c1\u5b89\u5168\u76d1\u63a7\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u90e8\u7f72\u8def\u5f84\u3002"}}
{"id": "2509.11853", "pdf": "https://arxiv.org/pdf/2509.11853", "abs": "https://arxiv.org/abs/2509.11853", "authors": ["Yi-Hsin Li", "Thomas Sikora", "Sebastian Knorr", "M\u00e5arten Sj\u00f6str\u00f6m"], "title": "Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Sparse-view synthesis remains a challenging problem due to the difficulty of recovering accurate geometry and appearance from limited observations. While recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time rendering with competitive quality, existing pipelines often rely on Structure-from-Motion (SfM) for camera pose estimation, an approach that struggles in genuinely sparse-view settings. Moreover, several SfM-free methods replace SfM with multi-view stereo (MVS) models, but generate massive numbers of 3D Gaussians by back-projecting every pixel into 3D space, leading to high memory costs. We propose Segmentation-Driven Initialization for Gaussian Splatting (SDI-GS), a method that mitigates inefficiency by leveraging region-based segmentation to identify and retain only structurally significant regions. This enables selective downsampling of the dense point cloud, preserving scene fidelity while substantially reducing Gaussian count. Experiments across diverse benchmarks show that SDI-GS reduces Gaussian count by up to 50% and achieves comparable or superior rendering quality in PSNR and SSIM, with only marginal degradation in LPIPS. It further enables faster training and lower memory footprint, advancing the practicality of 3DGS for constrained-view scenarios.", "AI": {"tldr": "SDI-GS\u901a\u8fc7\u533a\u57df\u5206\u5272\u9a71\u52a8\u521d\u59cb\u5316\uff0c\u5728\u7a00\u758f\u89c6\u56fe\u5408\u6210\u4e2d\u51cf\u5c1150%\u9ad8\u65af\u70b9\u6570\u91cf\uff0c\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\u7684\u540c\u65f6\u964d\u4f4e\u5185\u5b58\u548c\u8bad\u7ec3\u65f6\u95f4", "motivation": "\u89e3\u51b3\u7a00\u758f\u89c6\u56fe\u5408\u6210\u4e2d\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56SfM\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u56f0\u96be\uff0c\u4ee5\u53ca\u57fa\u4e8eMVS\u7684\u65b9\u6cd5\u751f\u6210\u8fc7\u591a\u9ad8\u65af\u70b9\u5bfc\u81f4\u5185\u5b58\u6210\u672c\u9ad8\u7684\u95ee\u9898", "method": "\u5229\u7528\u533a\u57df\u5206\u5272\u8bc6\u522b\u7ed3\u6784\u91cd\u8981\u533a\u57df\uff0c\u5bf9\u5bc6\u96c6\u70b9\u4e91\u8fdb\u884c\u9009\u62e9\u6027\u4e0b\u91c7\u6837\uff0c\u4fdd\u7559\u573a\u666f\u4fdd\u771f\u5ea6\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u9ad8\u65af\u70b9\u6570\u91cf", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u51cf\u5c11\u9ad8\u8fbe50%\u7684\u9ad8\u65af\u70b9\u6570\u91cf\uff0cPSNR\u548cSSIM\u6307\u6807\u76f8\u5f53\u6216\u66f4\u4f18\uff0cLPIPS\u4ec5\u6709\u8f7b\u5fae\u4e0b\u964d\uff0c\u8bad\u7ec3\u66f4\u5feb\u4e14\u5185\u5b58\u5360\u7528\u66f4\u4f4e", "conclusion": "SDI-GS\u63d0\u9ad8\u4e863D\u9ad8\u65af\u6e85\u5c04\u5728\u53d7\u9650\u89c6\u56fe\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u4e3a\u7a00\u758f\u89c6\u56fe\u5408\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.11878", "pdf": "https://arxiv.org/pdf/2509.11878", "abs": "https://arxiv.org/abs/2509.11878", "authors": ["Sofia Jamil", "Kotla Sai Charan", "Sriparna Saha", "Koustava Goswami", "K J Joseph"], "title": "Do It Yourself (DIY): Modifying Images for Poems in a Zero-Shot Setting Using Weighted Prompt Manipulation", "categories": ["cs.CV"], "comment": null, "summary": "Poetry is an expressive form of art that invites multiple interpretations, as readers often bring their own emotions, experiences, and cultural backgrounds into their understanding of a poem. Recognizing this, we aim to generate images for poems and improve these images in a zero-shot setting, enabling audiences to modify images as per their requirements. To achieve this, we introduce a novel Weighted Prompt Manipulation (WPM) technique, which systematically modifies attention weights and text embeddings within diffusion models. By dynamically adjusting the importance of specific words, WPM enhances or suppresses their influence in the final generated image, leading to semantically richer and more contextually accurate visualizations. Our approach exploits diffusion models and large language models (LLMs) such as GPT in conjunction with existing poetry datasets, ensuring a comprehensive and structured methodology for improved image generation in the literary domain. To the best of our knowledge, this is the first attempt at integrating weighted prompt manipulation for enhancing imagery in poetic language.", "AI": {"tldr": "\u57fa\u4e8e\u6743\u91cd\u63d0\u793a\u64cd\u63a7(WPM)\u6280\u672f\u7684\u96f6\u6837\u672c\u8bd7\u6b4c\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6ce8\u610f\u529b\u6743\u91cd\u548c\u6587\u672c\u5d4c\u5165\u6765\u6539\u5584\u8bd7\u6b4c\u7684\u89c6\u89c9\u5316\u6548\u679c", "motivation": "\u8bd7\u6b4c\u4f5c\u4e3a\u4e00\u79cd\u8868\u8fbe\u6027\u827a\u672f\u5f62\u5f0f\uff0c\u5e7b\u8bf7\u591a\u91cd\u89e3\u91ca\uff0c\u9700\u8981\u4e3a\u8bd7\u6b4c\u751f\u6210\u56fe\u50cf\u5e76\u652f\u6301\u9605\u8bfb\u8005\u6839\u636e\u81ea\u8eab\u9700\u6c42\u4fee\u6539\u56fe\u50cf", "method": "\u63d0\u51fa\u6743\u91cd\u63d0\u793a\u64cd\u63a7(WPM)\u6280\u672f\uff0c\u7cfb\u7edf\u6027\u4fee\u6539\u6a21\u578b\u5185\u7684\u6ce8\u610f\u529b\u6743\u91cd\u548c\u6587\u672c\u5d4c\u5165\uff0c\u52a8\u6001\u8c03\u6574\u5177\u4f53\u5355\u8bcd\u7684\u91cd\u8981\u6027\uff0c\u5229\u7528\u53cc\u5411\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u73b0\u6709\u8bd7\u6b4c\u6570\u636e\u96c6", "result": "\u5b9e\u73b0\u4e86\u8bed\u4e49\u66f4\u4e30\u5bcc\u3001\u4e0a\u4e0b\u6587\u66f4\u51c6\u786e\u7684\u89c6\u89c9\u5316\u6548\u679c\uff0c\u652f\u6301\u9605\u8bfb\u8005\u6839\u636e\u4e2a\u4eba\u9700\u6c42\u4fee\u6539\u751f\u6210\u7684\u56fe\u50cf", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5c06\u6743\u91cd\u63d0\u793a\u64cd\u63a7\u6280\u672f\u5e94\u7528\u4e8e\u63d0\u5347\u8bd7\u6b4c\u8bed\u8a00\u56fe\u50cf\u751f\u6210\u7684\u7814\u7a76\uff0c\u4e3a\u6587\u5b66\u9886\u57df\u7684\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u65b9\u6cd5"}}
{"id": "2509.11948", "pdf": "https://arxiv.org/pdf/2509.11948", "abs": "https://arxiv.org/abs/2509.11948", "authors": ["Mahmoud Z. A. Wahba", "Sara Baldoni", "Federica Battisti"], "title": "Sphere-GAN: a GAN-based Approach for Saliency Estimation in 360\u00b0 Videos", "categories": ["cs.CV", "cs.MM", "eess.IV"], "comment": null, "summary": "The recent success of immersive applications is pushing the research community to define new approaches to process 360{\\deg} images and videos and optimize their transmission. Among these, saliency estimation provides a powerful tool that can be used to identify visually relevant areas and, consequently, adapt processing algorithms. Although saliency estimation has been widely investigated for 2D content, very few algorithms have been proposed for 360{\\deg} saliency estimation. Towards this goal, we introduce Sphere-GAN, a saliency detection model for 360{\\deg} videos that leverages a Generative Adversarial Network with spherical convolutions. Extensive experiments were conducted using a public 360{\\deg} video saliency dataset, and the results demonstrate that Sphere-GAN outperforms state-of-the-art models in accurately predicting saliency maps.", "AI": {"tldr": "Sphere-GAN\u662f\u4e00\u4e2a\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u548c\u7403\u9762\u5377\u79ef\u7684360\u5ea6\u89c6\u9891\u663e\u8457\u6027\u68c0\u6d4b\u6a21\u578b\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u6a21\u578b", "motivation": "\u968f\u7740\u6c89\u6d78\u5f0f\u5e94\u7528\u7684\u6210\u529f\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u5904\u7406360\u5ea6\u56fe\u50cf\u89c6\u9891\u5e76\u4f18\u5316\u4f20\u8f93\u3002\u663e\u8457\u6027\u4f30\u8ba1\u53ef\u4ee5\u8bc6\u522b\u89c6\u89c9\u76f8\u5173\u533a\u57df\uff0c\u4f46\u73b0\u6709\u7b97\u6cd5\u5927\u591a\u9488\u5bf92D\u5185\u5bb9\uff0c360\u5ea6\u663e\u8457\u6027\u4f30\u8ba1\u7814\u7a76\u8f83\u5c11", "method": "\u4f7f\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u7ed3\u5408\u7403\u9762\u5377\u79ef\u6765\u6784\u5efa360\u5ea6\u89c6\u9891\u663e\u8457\u6027\u68c0\u6d4b\u6a21\u578b", "result": "\u5728\u516c\u5f00\u7684360\u5ea6\u89c6\u9891\u663e\u8457\u6027\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793aSphere-GAN\u5728\u51c6\u786e\u9884\u6d4b\u663e\u8457\u6027\u56fe\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6a21\u578b", "conclusion": "Sphere-GAN\u4e3a360\u5ea6\u89c6\u9891\u663e\u8457\u6027\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5"}}
{"id": "2509.11959", "pdf": "https://arxiv.org/pdf/2509.11959", "abs": "https://arxiv.org/abs/2509.11959", "authors": ["Ao Liang", "Youquan Liu", "Yu Yang", "Dongyue Lu", "Linfeng Li", "Lingdong Kong", "Huaici Zhao", "Wei Tsang Ooi"], "title": "Learning to Generate 4D LiDAR Sequences", "categories": ["cs.CV", "cs.RO"], "comment": "Abstract Paper (Non-Archival) @ ICCV 2025 Wild3D Workshop; GitHub   Repo at https://lidarcrafter.github.io/", "summary": "While generative world models have advanced video and occupancy-based data synthesis, LiDAR generation remains underexplored despite its importance for accurate 3D perception. Extending generation to 4D LiDAR data introduces challenges in controllability, temporal stability, and evaluation. We present LiDARCrafter, a unified framework that converts free-form language into editable LiDAR sequences. Instructions are parsed into ego-centric scene graphs, which a tri-branch diffusion model transforms into object layouts, trajectories, and shapes. A range-image diffusion model generates the initial scan, and an autoregressive module extends it into a temporally coherent sequence. The explicit layout design further supports object-level editing, such as insertion or relocation. To enable fair assessment, we provide EvalSuite, a benchmark spanning scene-, object-, and sequence-level metrics. On nuScenes, LiDARCrafter achieves state-of-the-art fidelity, controllability, and temporal consistency, offering a foundation for LiDAR-based simulation and data augmentation.", "AI": {"tldr": "LiDARCrafter\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u5c06\u81ea\u7531\u5f62\u5f0f\u7684\u8bed\u8a00\u8f6c\u6362\u4e3a\u53ef\u7f16\u8f91\u7684LiDAR\u5e8f\u5217\uff0c\u89e3\u51b3\u4e864D LiDAR\u6570\u636e\u751f\u6210\u5728\u53ef\u63a7\u6027\u3001\u65f6\u95f4\u7a33\u5b9a\u6027\u548c\u8bc4\u4f30\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u5c3d\u7ba1\u751f\u6210\u5f0f\u4e16\u754c\u6a21\u578b\u5728\u89c6\u9891\u548c\u57fa\u4e8e\u5360\u7528\u7684\u6570\u636e\u5408\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46LiDAR\u751f\u6210\u4ecd\u7136\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u5c3d\u7ba1\u5b83\u5bf9\u4e8e\u51c6\u786e\u76843D\u611f\u77e5\u81f3\u5173\u91cd\u8981\u3002\u5c06\u751f\u6210\u6269\u5c55\u52304D LiDAR\u6570\u636e\u5e26\u6765\u4e86\u53ef\u63a7\u6027\u3001\u65f6\u95f4\u7a33\u5b9a\u6027\u548c\u8bc4\u4f30\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faLiDARCrafter\u6846\u67b6\uff1a1\uff09\u5c06\u6307\u4ee4\u89e3\u6790\u4e3a\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u573a\u666f\u56fe\uff1b2\uff09\u4f7f\u7528\u4e09\u5206\u652f\u6269\u6563\u6a21\u578b\u8f6c\u6362\u4e3a\u5bf9\u8c61\u5e03\u5c40\u3001\u8f68\u8ff9\u548c\u5f62\u72b6\uff1b3\uff09\u8303\u56f4\u56fe\u50cf\u6269\u6563\u6a21\u578b\u751f\u6210\u521d\u59cb\u626b\u63cf\uff1b4\uff09\u81ea\u56de\u5f52\u6a21\u5757\u6269\u5c55\u4e3a\u65f6\u95f4\u76f8\u5e72\u5e8f\u5217\u3002\u652f\u6301\u5bf9\u8c61\u7ea7\u7f16\u8f91\u5982\u63d2\u5165\u6216\u91cd\u5b9a\u4f4d\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0cLiDARCrafter\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u4fdd\u771f\u5ea6\u3001\u53ef\u63a7\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u63d0\u4f9b\u4e86EvalSuite\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u6db5\u76d6\u573a\u666f\u3001\u5bf9\u8c61\u548c\u5e8f\u5217\u7ea7\u522b\u7684\u5ea6\u91cf\u6807\u51c6\u3002", "conclusion": "LiDARCrafter\u4e3a\u57fa\u4e8eLiDAR\u7684\u4eff\u771f\u548c\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5728LiDAR\u5e8f\u5217\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u7834\u6027\u8fdb\u5c55\uff0c\u89e3\u51b3\u4e864D LiDAR\u6570\u636e\u751f\u6210\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2509.12024", "pdf": "https://arxiv.org/pdf/2509.12024", "abs": "https://arxiv.org/abs/2509.12024", "authors": ["Zixuan Fu", "Yan Ren", "Finn Carter", "Chenyue Wen", "Le Ku", "Daheng Yu", "Emily Davis", "Bo Zhang"], "title": "Robust Concept Erasure in Diffusion Models: A Theoretical Perspective on Security and Robustness", "categories": ["cs.CV"], "comment": "Camera ready version", "summary": "Diffusion models have achieved unprecedented success in image generation but pose increasing risks in terms of privacy, fairness, and security. A growing demand exists to \\emph{erase} sensitive or harmful concepts (e.g., NSFW content, private individuals, artistic styles) from these models while preserving their overall generative capabilities. We introduce \\textbf{SCORE} (Secure and Concept-Oriented Robust Erasure), a novel framework for robust concept removal in diffusion models. SCORE formulates concept erasure as an \\emph{adversarial independence} problem, theoretically guaranteeing that the model's outputs become statistically independent of the erased concept. Unlike prior heuristic methods, SCORE minimizes the mutual information between a target concept and generated outputs, yielding provable erasure guarantees. We provide formal proofs establishing convergence properties and derive upper bounds on residual concept leakage. Empirically, we evaluate SCORE on Stable Diffusion and FLUX across four challenging benchmarks: object erasure, NSFW removal, celebrity face suppression, and artistic style unlearning. SCORE consistently outperforms state-of-the-art methods including EraseAnything, ANT, MACE, ESD, and UCE, achieving up to \\textbf{12.5\\%} higher erasure efficacy while maintaining comparable or superior image quality. By integrating adversarial optimization, trajectory consistency, and saliency-driven fine-tuning, SCORE sets a new standard for secure and robust concept erasure in diffusion models.", "AI": {"tldr": "SCORE\u662f\u4e00\u4e2a\u7528\u4e8e\u6269\u6563\u6a21\u578b\u6982\u5ff5\u64e6\u9664\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u72ec\u7acb\u6027\u7406\u8bba\u4fdd\u8bc1\u6a21\u578b\u8f93\u51fa\u4e0e\u64e6\u9664\u6982\u5ff5\u7684\u7edf\u8ba1\u72ec\u7acb\u6027\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\uff0c\u4f46\u5728\u9690\u79c1\u3001\u516c\u5e73\u6027\u548c\u5b89\u5168\u6027\u65b9\u9762\u5b58\u5728\u98ce\u9669\uff0c\u9700\u8981\u80fd\u591f\u64e6\u9664\u654f\u611f\u6216\u6709\u5bb3\u6982\u5ff5\uff08\u5982NSFW\u5185\u5bb9\u3001\u79c1\u4eba\u4fe1\u606f\u3001\u827a\u672f\u98ce\u683c\uff09\u540c\u65f6\u4fdd\u6301\u6574\u4f53\u751f\u6210\u80fd\u529b", "method": "SCORE\u5c06\u6982\u5ff5\u64e6\u9664\u8868\u8ff0\u4e3a\u5bf9\u6297\u6027\u72ec\u7acb\u6027\u95ee\u9898\uff0c\u6700\u5c0f\u5316\u76ee\u6807\u6982\u5ff5\u4e0e\u751f\u6210\u8f93\u51fa\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\uff0c\u7ed3\u5408\u5bf9\u6297\u4f18\u5316\u3001\u8f68\u8ff9\u4e00\u81f4\u6027\u548c\u663e\u8457\u6027\u9a71\u52a8\u7684\u5fae\u8c03", "result": "\u5728Stable Diffusion\u548cFLUX\u4e0a\u7684\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSCORE\u6bd4\u73b0\u6709\u65b9\u6cd5\uff08EraseAnything\u3001ANT\u3001MACE\u3001ESD\u3001UCE\uff09\u64e6\u9664\u6548\u679c\u63d0\u5347\u9ad8\u8fbe12.5%\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u6216\u66f4\u597d\u7684\u56fe\u50cf\u8d28\u91cf", "conclusion": "SCORE\u901a\u8fc7\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u5b89\u5168\u548c\u9c81\u68d2\u6982\u5ff5\u64e6\u9664\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6"}}
{"id": "2509.12039", "pdf": "https://arxiv.org/pdf/2509.12039", "abs": "https://arxiv.org/abs/2509.12039", "authors": ["Zilong Zhang", "Chujie Qin", "Chunle Guo", "Yong Zhang", "Chao Xue", "Ming-Ming Cheng", "Chongyi Li"], "title": "RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration", "categories": ["cs.CV"], "comment": "18 pages, 22 figures", "summary": "This work presents Robust Representation Learning via Adaptive Mask (RAM++), a two-stage framework for all-in-one image restoration. RAM++ integrates high-level semantic understanding with low-level texture generation to achieve content-oriented robust restoration. It addresses the limitations of existing degradation-oriented methods in extreme scenarios (e.g., degradations strongly coupled with image structures). RAM++ also mitigates common challenges such as unbalanced performance across tasks, overfitting to seen degradations, and weak generalization to unseen ones through three key designs: 1) Adaptive Semantic-Aware Mask (AdaSAM): a pretraining strategy that applies pixel-level masks to semantically rich and textured regions. This design enables the network to learn both generative priors and image content priors from various degradations. 2) Mask Attribute Conductance (MAC): a selective fine-tuning strategy that adjusts the layers with higher contributions to bridge the integrity gap between masked pretraining and full-image fine-tuning while retaining learned priors. 3) Robust Feature Regularization (RFR): a strategy that leverages DINOv2's semantically consistent and degradation-invariant representations, together with efficient feature fusion, to achieve faithful and semantically coherent restoration. With these designs, RAM++ achieves robust, well-balanced, and state-of-the-art performance across seen, unseen, extreme, and mixed degradations. Our code and model will be released at https://github.com/DragonisCV/RAM", "AI": {"tldr": "RAM++\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u56fe\u50cf\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8bed\u4e49\u611f\u77e5\u63a9\u7801\u3001\u63a9\u7801\u5c5e\u6027\u4f20\u5bfc\u548c\u9c81\u68d2\u7279\u5f81\u6b63\u5219\u5316\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u5bf9\u53ef\u89c1\u3001\u4e0d\u53ef\u89c1\u3001\u6781\u7aef\u548c\u6df7\u5408\u9000\u5316\u7684\u9c81\u68d2\u3001\u5e73\u8861\u548c\u6700\u5148\u8fdb\u7684\u4fee\u590d\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u9000\u5316\u5bfc\u5411\u65b9\u6cd5\u5728\u6781\u7aef\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\uff08\u5982\u9000\u5316\u4e0e\u56fe\u50cf\u7ed3\u6784\u5f3a\u8026\u5408\uff09\uff0c\u4ee5\u53ca\u8de8\u4efb\u52a1\u6027\u80fd\u4e0d\u5e73\u8861\u3001\u5bf9\u5df2\u89c1\u9000\u5316\u8fc7\u62df\u5408\u3001\u5bf9\u672a\u89c1\u9000\u5316\u6cdb\u5316\u80fd\u529b\u5f31\u7b49\u5e38\u89c1\u6311\u6218\u3002", "method": "1) \u81ea\u9002\u5e94\u8bed\u4e49\u611f\u77e5\u63a9\u7801(AdaSAM)\uff1a\u5728\u8bed\u4e49\u4e30\u5bcc\u548c\u7eb9\u7406\u533a\u57df\u5e94\u7528\u50cf\u7d20\u7ea7\u63a9\u7801\u7684\u9884\u8bad\u7ec3\u7b56\u7565\uff1b2) \u63a9\u7801\u5c5e\u6027\u4f20\u5bfc(MAC)\uff1a\u9009\u62e9\u6027\u5fae\u8c03\u7b56\u7565\uff0c\u8c03\u6574\u8d21\u732e\u5ea6\u9ad8\u7684\u5c42\uff1b3) \u9c81\u68d2\u7279\u5f81\u6b63\u5219\u5316(RFR)\uff1a\u5229\u7528DINOv2\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u9000\u5316\u4e0d\u53d8\u6027\u8868\u793a\u3002", "result": "RAM++\u5728\u53ef\u89c1\u3001\u4e0d\u53ef\u89c1\u3001\u6781\u7aef\u548c\u6df7\u5408\u9000\u5316\u60c5\u51b5\u4e0b\u90fd\u5b9e\u73b0\u4e86\u9c81\u68d2\u3001\u5e73\u8861\u548c\u6700\u5148\u8fdb\u7684\u4fee\u590d\u6027\u80fd\u3002", "conclusion": "RAM++\u901a\u8fc7\u96c6\u6210\u9ad8\u7ea7\u8bed\u4e49\u7406\u89e3\u548c\u4f4e\u7ea7\u7eb9\u7406\u751f\u6210\uff0c\u5b9e\u73b0\u4e86\u5185\u5bb9\u5bfc\u5411\u7684\u9c81\u68d2\u56fe\u50cf\u4fee\u590d\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u591a\u4e2a\u5173\u952e\u5c40\u9650\u6027\u3002"}}
{"id": "2509.12040", "pdf": "https://arxiv.org/pdf/2509.12040", "abs": "https://arxiv.org/abs/2509.12040", "authors": ["Bingyu Li", "Haocheng Dong", "Da Zhang", "Zhiyuan Zhao", "Junyu Gao", "Xuelong Li"], "title": "Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Open-Vocabulary Remote Sensing Image Segmentation (OVRSIS), an emerging task that adapts Open-Vocabulary Segmentation (OVS) to the remote sensing (RS) domain, remains underexplored due to the absence of a unified evaluation benchmark and the domain gap between natural and RS images. To bridge these gaps, we first establish a standardized OVRSIS benchmark (\\textbf{OVRSISBench}) based on widely-used RS segmentation datasets, enabling consistent evaluation across methods. Using this benchmark, we comprehensively evaluate several representative OVS/OVRSIS models and reveal their limitations when directly applied to remote sensing scenarios. Building on these insights, we propose \\textbf{RSKT-Seg}, a novel open-vocabulary segmentation framework tailored for remote sensing. RSKT-Seg integrates three key components: (1) a Multi-Directional Cost Map Aggregation (RS-CMA) module that captures rotation-invariant visual cues by computing vision-language cosine similarities across multiple directions; (2) an Efficient Cost Map Fusion (RS-Fusion) transformer, which jointly models spatial and semantic dependencies with a lightweight dimensionality reduction strategy; and (3) a Remote Sensing Knowledge Transfer (RS-Transfer) module that injects pre-trained knowledge and facilitates domain adaptation via enhanced upsampling. Extensive experiments on the benchmark show that RSKT-Seg consistently outperforms strong OVS baselines by +3.8 mIoU and +5.9 mACC, while achieving 2x faster inference through efficient aggregation. Our code is \\href{https://github.com/LiBingyu01/RSKT-Seg}{\\textcolor{blue}{here}}.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86RSKT-Seg\u6846\u67b6\uff0c\u4e13\u95e8\u89e3\u51b3\u8fdc\u7a0b\u611f\u77e5\u56fe\u50cf\u7684\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u95ee\u9898\uff0c\u901a\u8fc7\u65b0\u7684\u8bc4\u6d4b\u57fa\u51c6\u548c\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff0c\u5728\u6027\u80fd\u548c\u901f\u5ea6\u4e0a\u90fd\u8d85\u8fc7\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8fdc\u7a0b\u611f\u77e5\u56fe\u50cf\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u4efb\u52a1(OVRSIS)\u7f3a\u4e4f\u7edf\u4e00\u8bc4\u6d4b\u57fa\u51c6\uff0c\u4e14\u81ea\u7136\u56fe\u50cf\u4e0e\u8fdc\u611f\u56fe\u50cf\u5b58\u5728\u57df\u95f4\u8ddd\uff0c\u9700\u8981\u4e13\u95e8\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u9996\u5148\u5efa\u7acbOVRSISBench\u8bc4\u6d4b\u57fa\u51c6\uff0c\u7136\u540e\u63d0\u51faRSKT-Seg\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u591a\u65b9\u5411\u6210\u672c\u5730\u56fe\u805a\u5408\u6a21\u5757(RS-CMA)\u6350\u6355\u65cb\u8f6c\u4e0d\u53d8\u7279\u5f81\uff1b2\uff09\u9ad8\u6548\u6210\u672c\u5730\u56fe\u878d\u5408\u53d8\u6362\u5668(RS-Fusion)\uff1b3\uff09\u8fdc\u611f\u77e5\u8bc6\u8f6c\u79fb\u6a21\u5757(RS-Transfer)\u6ce8\u5165\u9886\u57df\u77e5\u8bc6\u3002", "result": "\u5728\u57fa\u51c6\u4e0a\u8bc4\u6d4b\u663e\u793a\uff0cRSKT-Seg\u6bd4\u5f3a\u57fa\u7ebf\u63d0\u5347+3.8 mIoU\u548c+5.9 mACC\uff0c\u540c\u65f6\u63a8\u7406\u901f\u5ea6\u63d0\u53472\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8fdc\u611f\u56fe\u50cf\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u7684\u6311\u6218\uff0c\u4e3a\u8be5\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u8bc4\u6d4b\u6807\u51c6\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.12046", "pdf": "https://arxiv.org/pdf/2509.12046", "abs": "https://arxiv.org/abs/2509.12046", "authors": ["Zirui Zheng", "Takashi Isobe", "Tong Shen", "Xu Jia", "Jianbin Zhao", "Xiaomin Li", "Mengmeng Ge", "Baolu Li", "Qinghe Wang", "Dong Li", "Dong Zhou", "Yunzhi Zhuge", "Huchuan Lu", "Emad Barsoum"], "title": "Layout-Conditioned Autoregressive Text-to-Image Generation via Structured Masking", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 3 figures", "summary": "While autoregressive (AR) models have demonstrated remarkable success in image generation, extending them to layout-conditioned generation remains challenging due to the sparse nature of layout conditions and the risk of feature entanglement. We present Structured Masking for AR-based Layout-to-Image (SMARLI), a novel framework for layoutto-image generation that effectively integrates spatial layout constraints into AR-based image generation. To equip AR model with layout control, a specially designed structured masking strategy is applied to attention computation to govern the interaction among the global prompt, layout, and image tokens. This design prevents mis-association between different regions and their descriptions while enabling sufficient injection of layout constraints into the generation process. To further enhance generation quality and layout accuracy, we incorporate Group Relative Policy Optimization (GRPO) based post-training scheme with specially designed layout reward functions for next-set-based AR models. Experimental results demonstrate that SMARLI is able to seamlessly integrate layout tokens with text and image tokens without compromising generation quality. It achieves superior layoutaware control while maintaining the structural simplicity and generation efficiency of AR models.", "AI": {"tldr": "SMARLI\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u5e03\u5c40\u5230\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a9\u7801\u7b56\u7565\u5c06\u7a7a\u95f4\u5e03\u5c40\u7ea6\u675f\u6709\u6548\u6574\u5408\u5230\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u4e2d\uff0c\u89e3\u51b3\u4e86\u5e03\u5c40\u6761\u4ef6\u7a00\u758f\u548c\u7279\u5f81\u7ea0\u7f20\u7684\u95ee\u9898\u3002", "motivation": "\u81ea\u56de\u5f52\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u6269\u5c55\u5230\u5e03\u5c40\u6761\u4ef6\u751f\u6210\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5e03\u5c40\u6761\u4ef6\u7684\u7a00\u758f\u6027\u548c\u7279\u5f81\u7ea0\u7f20\u98ce\u9669\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u63a9\u7801\u7b56\u7565\u6765\u63a7\u5236\u5168\u5c40\u63d0\u793a\u3001\u5e03\u5c40\u548c\u56fe\u50cftoken\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u9632\u6b62\u4e0d\u540c\u533a\u57df\u4e0e\u5176\u63cf\u8ff0\u4e4b\u95f4\u7684\u9519\u8bef\u5173\u8054\uff1b\u540c\u65f6\u4f7f\u7528\u57fa\u4e8e\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u7684\u540e\u8bad\u7ec3\u65b9\u6848\u548c\u4e13\u95e8\u8bbe\u8ba1\u7684\u5e03\u5c40\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eSMARLI\u80fd\u591f\u65e0\u7f1d\u6574\u5408\u5e03\u5c40token\u4e0e\u6587\u672c\u548c\u56fe\u50cftoken\uff0c\u5728\u4e0d\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5353\u8d8a\u7684\u5e03\u5c40\u611f\u77e5\u63a7\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u81ea\u56de\u5f52\u6a21\u578b\u7684\u7ed3\u6784\u7b80\u5355\u6027\u548c\u751f\u6210\u6548\u7387\u3002", "conclusion": "SMARLI\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5e03\u5c40\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u7684\u6311\u6218\uff0c\u4e3a\u81ea\u56de\u5f52\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5e03\u5c40\u63a7\u5236\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u7b80\u6d01\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5e03\u5c40\u611f\u77e5\u56fe\u50cf\u751f\u6210\u3002"}}
{"id": "2509.12052", "pdf": "https://arxiv.org/pdf/2509.12052", "abs": "https://arxiv.org/abs/2509.12052", "authors": ["Yuchen Deng", "Xiuyang Wu", "Hai-Tao Zheng", "Suiyang Zhang", "Yi He", "Yuxing Han"], "title": "AvatarSync: Rethinking Talking-Head Animation through Autoregressive Perspective", "categories": ["cs.CV"], "comment": null, "summary": "Existing talking-head animation approaches based on Generative Adversarial Networks (GANs) or diffusion models often suffer from inter-frame flicker, identity drift, and slow inference. These limitations inherent to their video generation pipelines restrict their suitability for applications. To address this, we introduce AvatarSync, an autoregressive framework on phoneme representations that generates realistic and controllable talking-head animations from a single reference image, driven directly text or audio input. In addition, AvatarSync adopts a two-stage generation strategy, decoupling semantic modeling from visual dynamics, which is a deliberate \"Divide and Conquer\" design. The first stage, Facial Keyframe Generation (FKG), focuses on phoneme-level semantic representation by leveraging the many-to-one mapping from text or audio to phonemes. A Phoneme-to-Visual Mapping is constructed to anchor abstract phonemes to character-level units. Combined with a customized Text-Frame Causal Attention Mask, the keyframes are generated. The second stage, inter-frame interpolation, emphasizes temporal coherence and visual smoothness. We introduce a timestamp-aware adaptive strategy based on a selective state space model, enabling efficient bidirectional context reasoning. To support deployment, we optimize the inference pipeline to reduce latency without compromising visual fidelity. Extensive experiments show that AvatarSync outperforms existing talking-head animation methods in visual fidelity, temporal consistency, and computational efficiency, providing a scalable and controllable solution.", "AI": {"tldr": "AvatarSync\u662f\u4e00\u4e2a\u57fa\u4e8e\u97f3\u7d20\u8868\u793a\u7684\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u751f\u6210\u7b56\u7565\uff08\u5173\u952e\u5e27\u751f\u6210\u548c\u5e27\u95f4\u63d2\u503c\uff09\u4ece\u5355\u5f20\u53c2\u8003\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u63a7\u7684\u8bf4\u8bdd\u5934\u90e8\u52a8\u753b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u95ea\u70c1\u3001\u8eab\u4efd\u6f02\u79fb\u548c\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8eGAN\u548c\u6269\u6563\u6a21\u578b\u7684\u8bf4\u8bdd\u5934\u90e8\u52a8\u753b\u65b9\u6cd5\u5b58\u5728\u7684\u5e27\u95f4\u95ea\u70c1\u3001\u8eab\u4efd\u6f02\u79fb\u548c\u63a8\u7406\u901f\u5ea6\u6162\u7b49\u9650\u5236\uff0c\u63d0\u4f9b\u66f4\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u7684\u9ad8\u8d28\u91cf\u52a8\u753b\u751f\u6210\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u751f\u6210\u7b56\u7565\uff1a1\uff09\u9762\u90e8\u5173\u952e\u5e27\u751f\u6210\u9636\u6bb5\uff0c\u901a\u8fc7\u97f3\u7d20\u5230\u89c6\u89c9\u6620\u5c04\u548c\u6587\u672c-\u5e27\u56e0\u679c\u6ce8\u610f\u529b\u63a9\u7801\u751f\u6210\u8bed\u4e49\u5173\u952e\u5e27\uff1b2\uff09\u5e27\u95f4\u63d2\u503c\u9636\u6bb5\uff0c\u4f7f\u7528\u65f6\u5e8f\u611f\u77e5\u81ea\u9002\u5e94\u7b56\u7565\u548c\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u786e\u4fdd\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u5e73\u6ed1\u5ea6\u3002", "result": "\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u3001\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u8bf4\u8bdd\u5934\u90e8\u52a8\u753b\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "AvatarSync\u901a\u8fc7\u521b\u65b0\u7684\u81ea\u56de\u5f52\u6846\u67b6\u548c\u4e24\u9636\u6bb5\u751f\u6210\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8bf4\u8bdd\u5934\u90e8\u52a8\u753b\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u751f\u6210\u65b9\u6848\u3002"}}
{"id": "2509.12105", "pdf": "https://arxiv.org/pdf/2509.12105", "abs": "https://arxiv.org/abs/2509.12105", "authors": ["Bernardo Forni", "Gabriele Lombardi", "Federico Pozzi", "Mirco Planamente"], "title": "FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation", "categories": ["cs.CV"], "comment": "Accepted at ICIAP 2025", "summary": "Few-shot semantic segmentation has recently attracted great attention. The goal is to develop a model capable of segmenting unseen classes using only a few annotated samples. Most existing approaches adapt a pre-trained model by training from scratch an additional module. Achieving optimal performance with these approaches requires extensive training on large-scale datasets. The Segment Anything Model 2 (SAM2) is a foundational model for zero-shot image and video segmentation with a modular design. In this paper, we propose a Few-Shot segmentation method based on SAM2 (FS-SAM2), where SAM2's video capabilities are directly repurposed for the few-shot task. Moreover, we apply a Low-Rank Adaptation (LoRA) to the original modules in order to handle the diverse images typically found in standard datasets, unlike the temporally connected frames used in SAM2's pre-training. With this approach, only a small number of parameters is meta-trained, which effectively adapts SAM2 while benefiting from its impressive segmentation performance. Our method supports any K-shot configuration. We evaluate FS-SAM2 on the PASCAL-5$^i$, COCO-20$^i$ and FSS-1000 datasets, achieving remarkable results and demonstrating excellent computational efficiency during inference. Code is available at https://github.com/fornib/FS-SAM2", "AI": {"tldr": "FS-SAM2\u662f\u4e00\u4e2a\u57fa\u4e8eSegment Anything Model 2\u7684\u5c11\u6837\u672c\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7LoRA\u5fae\u8c03\u548c\u91cd\u7528\u9014\u89c6\u9891\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5c11\u6837\u672c\u5206\u5272\u65b9\u6cd5\u9700\u8981\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u4ece\u5934\u8bad\u7ec3\u989d\u5916\u6a21\u5757\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002SAM2\u4f5c\u4e3a\u57fa\u7840\u5206\u5272\u6a21\u578b\u5177\u6709\u5f3a\u5927\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u4f46\u9700\u8981\u9002\u5e94\u5c11\u6837\u672c\u4efb\u52a1\u3002", "method": "\u5c06SAM2\u7684\u89c6\u9891\u80fd\u529b\u76f4\u63a5\u91cd\u7528\u4e8e\u5c11\u6837\u672c\u4efb\u52a1\uff0c\u5e94\u7528\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u5904\u7406\u6807\u51c6\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\u56fe\u50cf\uff0c\u4ec5\u5fae\u8c03\u5c11\u91cf\u53c2\u6570\u8fdb\u884c\u5143\u8bad\u7ec3\u3002", "result": "\u5728PASCAL-5i\u3001COCO-20i\u548cFSS-1000\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7ed3\u679c\uff0c\u63a8\u7406\u65f6\u5177\u6709\u4f18\u79c0\u7684\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "FS-SAM2\u6709\u6548\u5229\u7528\u4e86SAM2\u7684\u5f3a\u5927\u5206\u5272\u80fd\u529b\uff0c\u901a\u8fc7\u5c11\u91cf\u53c2\u6570\u5fae\u8c03\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u5c11\u6837\u672c\u5206\u5272\u6027\u80fd\uff0c\u652f\u6301\u4efb\u610fK-shot\u914d\u7f6e\u3002"}}
{"id": "2509.12155", "pdf": "https://arxiv.org/pdf/2509.12155", "abs": "https://arxiv.org/abs/2509.12155", "authors": ["M. Bolhassani", "B. Veasey", "E. Daugherty", "S. Keltner", "N. Kumar", "N. Dunlap", "A. Amini"], "title": "LoRA-fine-tuned Large Vision Models for Automated Assessment of Post-SBRT Lung Injury", "categories": ["cs.CV"], "comment": "5 pages, 5 figures", "summary": "This study investigates the efficacy of Low-Rank Adaptation (LoRA) for fine-tuning large Vision Models, DinoV2 and SwinV2, to diagnose Radiation-Induced Lung Injury (RILI) from X-ray CT scans following Stereotactic Body Radiation Therapy (SBRT). To evaluate the robustness and efficiency of this approach, we compare LoRA with traditional full fine-tuning and inference-only (no fine-tuning) methods. Cropped images of two sizes (50 mm3 and 75 mm3), centered at the treatment isocenter, in addition to different adaptation techniques for adapting the 2D LVMs for 3D data were used to determine the sensitivity of the models to spatial context. Experimental results show that LoRA achieves comparable or superior performance to traditional fine-tuning while significantly reducing computational costs and training times by requiring fewer trainable parameters.", "AI": {"tldr": "LoRA\u5fae\u8c03\u65b9\u6cd5\u5728DinoV2\u548cSwinV2\u89c6\u89c9\u6a21\u578b\u4e0a\u8bca\u65ad\u653e\u5c04\u6027\u80ba\u635f\u4f24\uff0c\u76f8\u6bd4\u5168\u5fae\u8c03\u548c\u65e0\u5fae\u8c03\u65b9\u6cd5\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u65b9\u9762\u8868\u73b0\u66f4\u4f18", "motivation": "\u7814\u7a76\u4f4e\u79e9\u9002\u5e94(LoRA)\u65b9\u6cd5\u5728\u5927\u578b\u89c6\u89c9\u6a21\u578b\u5fae\u8c03\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u8bca\u65adSBRT\u6cbb\u7597\u540e\u653e\u5c04\u6027\u80ba\u635f\u4f24\u7684\u533b\u5b66\u5f71\u50cf\u5206\u6790\u4efb\u52a1\u4e2d", "method": "\u4f7f\u7528DinoV2\u548cSwinV2\u6a21\u578b\uff0c\u6bd4\u8f83LoRA\u3001\u4f20\u7edf\u5168\u5fae\u8c03\u548c\u4ec5\u63a8\u7406\u4e09\u79cd\u65b9\u6cd5\u3002\u91c7\u752850mm\u00b3\u548c75mm\u00b3\u4e24\u79cd\u5c3a\u5bf8\u7684\u88c1\u526a\u56fe\u50cf\uff0c\u5e76\u63a2\u7d222D\u6a21\u578b\u9002\u5e943D\u6570\u636e\u7684\u4e0d\u540c\u6280\u672f", "result": "LoRA\u65b9\u6cd5\u5728\u8fbe\u5230\u4e0e\u4f20\u7edf\u5fae\u8c03\u76f8\u5f53\u6216\u66f4\u4f18\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u6210\u672c\u548c\u8bad\u7ec3\u65f6\u95f4\uff0c\u6240\u9700\u53ef\u8bad\u7ec3\u53c2\u6570\u66f4\u5c11", "conclusion": "LoRA\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u533b\u5b66\u5f71\u50cf\u5206\u6790\u4efb\u52a1\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42"}}
{"id": "2509.12201", "pdf": "https://arxiv.org/pdf/2509.12201", "abs": "https://arxiv.org/abs/2509.12201", "authors": ["Yang Zhou", "Yifan Wang", "Jianjun Zhou", "Wenzheng Chang", "Haoyu Guo", "Zizun Li", "Kaijing Ma", "Xinyue Li", "Yating Wang", "Haoyi Zhu", "Mingyu Liu", "Dingning Liu", "Jiange Yang", "Zhoujie Fu", "Junyi Chen", "Chunhua Shen", "Jiangmiao Pang", "Kaipeng Zhang", "Tong He"], "title": "OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling", "categories": ["cs.CV"], "comment": "https://yangzhou24.github.io/OmniWorld/", "summary": "The field of 4D world modeling - aiming to jointly capture spatial geometry and temporal dynamics - has witnessed remarkable progress in recent years, driven by advances in large-scale generative models and multimodal learning. However, the development of truly general 4D world models remains fundamentally constrained by the availability of high-quality data. Existing datasets and benchmarks often lack the dynamic complexity, multi-domain diversity, and spatial-temporal annotations required to support key tasks such as 4D geometric reconstruction, future prediction, and camera-control video generation. To address this gap, we introduce OmniWorld, a large-scale, multi-domain, multi-modal dataset specifically designed for 4D world modeling. OmniWorld consists of a newly collected OmniWorld-Game dataset and several curated public datasets spanning diverse domains. Compared with existing synthetic datasets, OmniWorld-Game provides richer modality coverage, larger scale, and more realistic dynamic interactions. Based on this dataset, we establish a challenging benchmark that exposes the limitations of current state-of-the-art (SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning existing SOTA methods on OmniWorld leads to significant performance gains across 4D reconstruction and video generation tasks, strongly validating OmniWorld as a powerful resource for training and evaluation. We envision OmniWorld as a catalyst for accelerating the development of general-purpose 4D world models, ultimately advancing machines' holistic understanding of the physical world.", "AI": {"tldr": "OmniWorld\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u60014D\u4e16\u754c\u5efa\u6a21\u6570\u636e\u96c6\uff0c\u5305\u542b\u65b0\u6536\u96c6\u7684OmniWorld-Game\u6570\u636e\u548c\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6570\u636e\u5728\u52a8\u6001\u590d\u6742\u6027\u3001\u591a\u9886\u57df\u591a\u6837\u6027\u548c\u65f6\u7a7a\u6807\u6ce8\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a4D\u91cd\u5efa\u548c\u89c6\u9891\u751f\u6210\u7b49\u4efb\u52a1\u63d0\u4f9b\u8bad\u7ec3\u548c\u8bc4\u4f30\u8d44\u6e90\u3002", "motivation": "\u73b0\u67094D\u4e16\u754c\u5efa\u6a21\u9886\u57df\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u73b0\u6709\u6570\u636e\u96c6\u5728\u52a8\u6001\u590d\u6742\u6027\u3001\u591a\u9886\u57df\u591a\u6837\u6027\u548c\u65f6\u7a7a\u6807\u6ce8\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u901a\u75284D\u4e16\u754c\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u5f15\u5165OmniWorld\u6570\u636e\u96c6\uff0c\u5305\u62ec\u65b0\u6536\u96c6\u7684OmniWorld-Game\u6570\u636e\u96c6\uff08\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u6a21\u6001\u8986\u76d6\u3001\u66f4\u5927\u89c4\u6a21\u548c\u66f4\u771f\u5b9e\u7684\u52a8\u6001\u4ea4\u4e92\uff09\u548c\u591a\u4e2a\u7cbe\u9009\u7684\u516c\u5171\u6570\u636e\u96c6\uff0c\u5e76\u57fa\u4e8e\u6b64\u5efa\u7acb\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u66b4\u9732\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u5728\u5efa\u6a21\u590d\u67424D\u73af\u5883\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5728OmniWorld\u4e0a\u5fae\u8c03\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e864D\u91cd\u5efa\u548c\u89c6\u9891\u751f\u6210\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "OmniWorld\u4f5c\u4e3a\u5f3a\u5927\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u8d44\u6e90\uff0c\u5c06\u52a0\u901f\u901a\u75284D\u4e16\u754c\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u63a8\u52a8\u673a\u5668\u5bf9\u7269\u7406\u4e16\u754c\u7684\u6574\u4f53\u7406\u89e3\u3002"}}
{"id": "2509.12203", "pdf": "https://arxiv.org/pdf/2509.12203", "abs": "https://arxiv.org/abs/2509.12203", "authors": ["Zixin Yin", "Xili Dai", "Duomin Wang", "Xianfang Zeng", "Lionel M. Ni", "Gang Yu", "Heung-Yeung Shum"], "title": "LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence", "categories": ["cs.CV"], "comment": null, "summary": "The reliance on implicit point matching via attention has become a core bottleneck in drag-based editing, resulting in a fundamental compromise on weakened inversion strength and costly test-time optimization (TTO). This compromise severely limits the generative capabilities of diffusion models, suppressing high-fidelity inpainting and text-guided creation. In this paper, we introduce LazyDrag, the first drag-based image editing method for Multi-Modal Diffusion Transformers, which directly eliminates the reliance on implicit point matching. In concrete terms, our method generates an explicit correspondence map from user drag inputs as a reliable reference to boost the attention control. This reliable reference opens the potential for a stable full-strength inversion process, which is the first in the drag-based editing task. It obviates the necessity for TTO and unlocks the generative capability of models. Therefore, LazyDrag naturally unifies precise geometric control with text guidance, enabling complex edits that were previously out of reach: opening the mouth of a dog and inpainting its interior, generating new objects like a ``tennis ball'', or for ambiguous drags, making context-aware changes like moving a hand into a pocket. Additionally, LazyDrag supports multi-round workflows with simultaneous move and scale operations. Evaluated on the DragBench, our method outperforms baselines in drag accuracy and perceptual quality, as validated by VIEScore and human evaluation. LazyDrag not only establishes new state-of-the-art performance, but also paves a new way to editing paradigms.", "AI": {"tldr": "LazyDrag\u662f\u9996\u4e2a\u57fa\u4e8e\u62d6\u62fd\u7684\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u5bf9\u5e94\u6620\u5c04\u66ff\u4ee3\u9690\u5f0f\u70b9\u5339\u914d\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u5168\u5f3a\u5ea6\u53cd\u6f14\uff0c\u65e0\u9700\u6d4b\u8bd5\u65f6\u4f18\u5316\uff0c\u7edf\u4e00\u4e86\u51e0\u4f55\u63a7\u5236\u4e0e\u6587\u672c\u5f15\u5bfc\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u62d6\u62fd\u7684\u7f16\u8f91\u65b9\u6cd5\u4f9d\u8d56\u6ce8\u610f\u529b\u673a\u5236\u7684\u9690\u5f0f\u70b9\u5339\u914d\uff0c\u5bfc\u81f4\u53cd\u6f14\u5f3a\u5ea6\u51cf\u5f31\u548c\u6602\u8d35\u7684\u6d4b\u8bd5\u65f6\u4f18\u5316\uff0c\u9650\u5236\u4e86\u6269\u6563\u6a21\u578b\u7684\u9ad8\u4fdd\u771f\u4fee\u590d\u548c\u6587\u672c\u5f15\u5bfc\u521b\u4f5c\u80fd\u529b\u3002", "method": "\u4ece\u7528\u6237\u62d6\u62fd\u8f93\u5165\u751f\u6210\u663e\u5f0f\u5bf9\u5e94\u6620\u5c04\u4f5c\u4e3a\u53ef\u9760\u53c2\u8003\u6765\u589e\u5f3a\u6ce8\u610f\u529b\u63a7\u5236\uff0c\u5b9e\u73b0\u7a33\u5b9a\u7684\u5168\u5f3a\u5ea6\u53cd\u6f14\u8fc7\u7a0b\uff0c\u6d88\u9664\u6d4b\u8bd5\u65f6\u4f18\u5316\u9700\u6c42\u3002", "result": "\u5728DragBench\u8bc4\u4f30\u4e2d\uff0cLazyDrag\u5728\u62d6\u62fd\u7cbe\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u8f6e\u5de5\u4f5c\u6d41\u548c\u540c\u65f6\u79fb\u52a8\u7f29\u653e\u64cd\u4f5c\uff0c\u80fd\u591f\u5b9e\u73b0\u590d\u6742\u7f16\u8f91\u5982\u5f00\u53e3\u4fee\u590d\u3001\u751f\u6210\u65b0\u7269\u4f53\u7b49\u3002", "conclusion": "LazyDrag\u4e0d\u4ec5\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8fd8\u4e3a\u7f16\u8f91\u8303\u5f0f\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u7edf\u4e00\u4e86\u51e0\u4f55\u7cbe\u786e\u63a7\u5236\u4e0e\u6587\u672c\u5f15\u5bfc\u80fd\u529b\u3002"}}
{"id": "2509.09719", "pdf": "https://arxiv.org/pdf/2509.09719", "abs": "https://arxiv.org/abs/2509.09719", "authors": ["Hemanth Chandravamsi", "Dhanush V. Shenoy", "Itay Zinn", "Shimon Pisnoy", "Steven H. Frankel"], "title": "Spectral Bottleneck in Deep Neural Networks: Noise is All You Need", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.SD"], "comment": null, "summary": "Deep neural networks are known to exhibit a spectral learning bias, wherein low-frequency components are learned early in training, while high-frequency modes emerge more gradually in later epochs. However, when the target signal lacks low-frequency components and is dominated by broadband high frequencies, training suffers from a 'spectral bottleneck', and the model fails to reconstruct the entire signal, including the frequency components that lie within the network's representational capacity. We examine such a scenario in the context of implicit neural representations (INRs) with sinusoidal representation networks (SIRENs), focusing on the challenge of fitting high-frequency-dominant signals that are susceptible to spectral bottleneck. To effectively fit any target signal irrespective of it's frequency content, we propose a generalized target-aware 'weight perturbation scheme' (WINNER - weight initialization with noise for neural representations) for network initialization. The scheme perturbs uniformly initialized weights with Gaussian noise, where the noise scales are adaptively determined by the spectral centroid of the target signal. We show that the noise scales can provide control over the spectra of network activations and the eigenbasis of the empirical neural tangent kernel. This method not only addresses the spectral bottleneck but also yields faster convergence and with improved representation accuracy, outperforming state-of-the-art approaches in audio fitting and achieving notable gains in image fitting and denoising tasks. Beyond signal reconstruction, our approach opens new directions for adaptive weight initialization strategies in computer vision and scientific machine learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86WINNER\u6743\u91cd\u6270\u52a8\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9ad8\u65af\u566a\u58f0\u6270\u52a8\u89e3\u51b3SIREN\u7f51\u7edc\u5728\u5904\u7406\u9ad8\u9891\u4e3b\u5bfc\u4fe1\u53f7\u65f6\u7684\u9891\u8c31\u74f6\u9888\u95ee\u9898", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5b58\u5728\u9891\u8c31\u5b66\u4e60\u504f\u5dee\uff0c\u4f4e\u9891\u6210\u5206\u5148\u88ab\u5b66\u4e60\uff0c\u9ad8\u9891\u6210\u5206\u5b66\u4e60\u7f13\u6162\u3002\u5f53\u76ee\u6807\u4fe1\u53f7\u7f3a\u4e4f\u4f4e\u9891\u6210\u5206\u4e14\u4ee5\u9ad8\u9891\u4e3a\u4e3b\u65f6\uff0c\u4f1a\u51fa\u73b0\u9891\u8c31\u74f6\u9888\uff0c\u5bfc\u81f4\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u91cd\u5efa\u4fe1\u53f7", "method": "\u63d0\u51faWINNER\u6743\u91cd\u6270\u52a8\u65b9\u6848\uff0c\u5728\u5747\u5300\u521d\u59cb\u5316\u6743\u91cd\u57fa\u7840\u4e0a\u6dfb\u52a0\u9ad8\u65af\u566a\u58f0\uff0c\u566a\u58f0\u5c3a\u5ea6\u6839\u636e\u76ee\u6807\u4fe1\u53f7\u7684\u9891\u8c31\u8d28\u5fc3\u81ea\u9002\u5e94\u786e\u5b9a\uff0c\u4ece\u800c\u63a7\u5236\u7f51\u7edc\u6fc0\u6d3b\u8c31\u548c\u795e\u7ecf\u6b63\u5207\u6838\u7279\u5f81\u57fa", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u89e3\u51b3\u4e86\u9891\u8c31\u74f6\u9888\u95ee\u9898\uff0c\u8fd8\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u8868\u793a\u7cbe\u5ea6\uff0c\u5728\u97f3\u9891\u62df\u5408\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u56fe\u50cf\u62df\u5408\u548c\u53bb\u566a\u4efb\u52a1\u4e2d\u4e5f\u6709\u663e\u8457\u63d0\u5347", "conclusion": "WINNER\u65b9\u6cd5\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u81ea\u9002\u5e94\u6743\u91cd\u521d\u59cb\u5316\u7b56\u7565\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u4efb\u610f\u9891\u7387\u5185\u5bb9\u7684\u76ee\u6807\u4fe1\u53f7"}}
{"id": "2509.10704", "pdf": "https://arxiv.org/pdf/2509.10704", "abs": "https://arxiv.org/abs/2509.10704", "authors": ["Xingchen Wan", "Han Zhou", "Ruoxi Sun", "Hootan Nakhost", "Ke Jiang", "Rajarishi Sinha", "Sercan \u00d6. Ar\u0131k"], "title": "Maestro: Self-Improving Text-to-Image Generation via Agent Orchestration", "categories": ["cs.AI", "cs.CV"], "comment": "15 pages, 7 figures, 2 tables (22 pages, 9 figures and 3 tables   including references and appendices)", "summary": "Text-to-image (T2I) models, while offering immense creative potential, are highly reliant on human intervention, posing significant usability challenges that often necessitate manual, iterative prompt engineering over often underspecified prompts. This paper introduces Maestro, a novel self-evolving image generation system that enables T2I models to autonomously self-improve generated images through iterative evolution of prompts, using only an initial prompt. Maestro incorporates two key innovations: 1) self-critique, where specialized multimodal LLM (MLLM) agents act as 'critics' to identify weaknesses in generated images, correct for under-specification, and provide interpretable edit signals, which are then integrated by a 'verifier' agent while preserving user intent; and 2) self-evolution, utilizing MLLM-as-a-judge for head-to-head comparisons between iteratively generated images, eschewing problematic images, and evolving creative prompt candidates that align with user intents. Extensive experiments on complex T2I tasks using black-box models demonstrate that Maestro significantly improves image quality over initial prompts and state-of-the-art automated methods, with effectiveness scaling with more advanced MLLM components. This work presents a robust, interpretable, and effective pathway towards self-improving T2I generation.", "AI": {"tldr": "Maestro\u662f\u4e00\u4e2a\u81ea\u6f14\u5316\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u6a21\u6001LLM\u4ee3\u7406\u8fdb\u884c\u81ea\u6211\u6279\u5224\u548c\u6f14\u5316\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u5373\u53ef\u81ea\u52a8\u6539\u8fdb\u751f\u6210\u56fe\u50cf\u8d28\u91cf", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u9ad8\u5ea6\u4f9d\u8d56\u4eba\u5de5\u5e72\u9884\u548c\u8fed\u4ee3\u63d0\u793a\u5de5\u7a0b\uff0c\u5b58\u5728\u53ef\u7528\u6027\u6311\u6218\uff0c\u9700\u8981\u624b\u52a8\u5904\u7406\u4e0d\u660e\u786e\u7684\u63d0\u793a", "method": "\u91c7\u7528\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u81ea\u6211\u6279\u5224 - \u591a\u6a21\u6001LLM\u4ee3\u7406\u4f5c\u4e3a\u8bc4\u8bba\u5bb6\u8bc6\u522b\u56fe\u50cf\u5f31\u70b9\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u7f16\u8f91\u4fe1\u53f7\uff1b2) \u81ea\u6211\u6f14\u5316 - \u4f7f\u7528MLLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u8fdb\u884c\u8fed\u4ee3\u56fe\u50cf\u6bd4\u8f83\uff0c\u6f14\u5316\u521b\u610f\u63d0\u793a", "result": "\u5728\u590d\u6742\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMaestro\u663e\u8457\u63d0\u9ad8\u4e86\u56fe\u50cf\u8d28\u91cf\uff0c\u4f18\u4e8e\u521d\u59cb\u63d0\u793a\u548c\u6700\u5148\u8fdb\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u4e14\u6548\u679c\u968fMLLM\u7ec4\u4ef6\u8fdb\u6b65\u800c\u63d0\u5347", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u3001\u53ef\u89e3\u91ca\u4e14\u6709\u6548\u7684\u81ea\u6539\u8fdb\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u9014\u5f84"}}
{"id": "2509.10913", "pdf": "https://arxiv.org/pdf/2509.10913", "abs": "https://arxiv.org/abs/2509.10913", "authors": ["Ali Hedayatnia", "Mostafa Tavassolipour", "Babak Nadjar Araabi", "Abdol-Hossein Vahabie"], "title": "Robustifying Diffusion-Denoised Smoothing Against Covariate Shift", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Randomized smoothing is a well-established method for achieving certified robustness against l2-adversarial perturbations. By incorporating a denoiser before the base classifier, pretrained classifiers can be seamlessly integrated into randomized smoothing without significant performance degradation. Among existing methods, Diffusion Denoised Smoothing - where a pretrained denoising diffusion model serves as the denoiser - has produced state-of-the-art results. However, we show that employing a denoising diffusion model introduces a covariate shift via misestimation of the added noise, ultimately degrading the smoothed classifier's performance. To address this issue, we propose a novel adversarial objective function focused on the added noise of the denoising diffusion model. This approach is inspired by our understanding of the origin of the covariate shift. Our goal is to train the base classifier to ensure it is robust against the covariate shift introduced by the denoiser. Our method significantly improves certified accuracy across three standard classification benchmarks - MNIST, CIFAR-10, and ImageNet - achieving new state-of-the-art performance in l2-adversarial perturbations. Our implementation is publicly available at https://github.com/ahedayat/Robustifying-DDS-Against-Covariate-Shift", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u6269\u6563\u53bb\u566a\u5e73\u6ed1\u4e2d\u7684\u534f\u53d8\u91cf\u504f\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u5728\u53bb\u566a\u6269\u6563\u6a21\u578b\u7684\u6dfb\u52a0\u566a\u58f0\u4e0a\u4f7f\u7528\u5bf9\u6297\u76ee\u6807\u51fd\u6570\u6765\u8bad\u7ec3\u57fa\u7840\u5206\u7c7b\u5668\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba4\u8bc1\u7cbe\u5ea6\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e2d\u4f7f\u7528\u9884\u8bad\u7ec3\u53bb\u566a\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u53bb\u566a\u5668\u4f1a\u5f15\u5165\u534f\u53d8\u91cf\u504f\u79fb\uff0c\u8fd9\u662f\u7531\u4e8e\u5bf9\u6dfb\u52a0\u566a\u58f0\u7684\u9519\u8bef\u4f30\u8ba1\u5bfc\u81f4\u7684\uff0c\u8fd9\u4f1a\u964d\u4f4e\u5e73\u6ed1\u5206\u7c7b\u5668\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u6297\u76ee\u6807\u51fd\u6570\uff0c\u4e13\u6ce8\u4e8e\u53bb\u566a\u6269\u6563\u6a21\u578b\u7684\u6dfb\u52a0\u566a\u58f0\uff0c\u8bad\u7ec3\u57fa\u7840\u5206\u7c7b\u5668\u4f7f\u5176\u5bf9\u53bb\u566a\u5668\u5f15\u5165\u7684\u534f\u53d8\u91cf\u504f\u79fb\u5177\u6709\u9c81\u68d2\u6027\u3002", "result": "\u5728MNIST\u3001CIFAR-10\u548cImageNet\u4e09\u4e2a\u6807\u51c6\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u8ba4\u8bc1\u7cbe\u5ea6\uff0c\u5b9e\u73b0\u4e86l2\u5bf9\u6297\u6270\u52a8\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u9488\u5bf9\u53bb\u566a\u6269\u6563\u6a21\u578b\u6dfb\u52a0\u566a\u58f0\u7684\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u534f\u53d8\u91cf\u504f\u79fb\u95ee\u9898\uff0c\u4e3a\u968f\u673a\u5e73\u6ed1\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.11197", "pdf": "https://arxiv.org/pdf/2509.11197", "abs": "https://arxiv.org/abs/2509.11197", "authors": ["Yunheng Wang", "Yuetong Fang", "Taowen Wang", "Yixiao Feng", "Yawen Tan", "Shuning Zhang", "Peiran Liu", "Yiding Ji", "Renjing Xu"], "title": "DreamNav: A Trajectory-Based Imaginative Framework for Zero-Shot Vision-and-Language Navigation", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Vision-and-Language Navigation in Continuous Environments (VLN-CE), which links language instructions to perception and control in the real world, is a core capability of embodied robots. Recently, large-scale pretrained foundation models have been leveraged as shared priors for perception, reasoning, and action, enabling zero-shot VLN without task-specific training. However, existing zero-shot VLN methods depend on costly perception and passive scene understanding, collapsing control to point-level choices. As a result, they are expensive to deploy, misaligned in action semantics, and short-sighted in planning. To address these issues, we present DreamNav that focuses on the following three aspects: (1) for reducing sensory cost, our EgoView Corrector aligns viewpoints and stabilizes egocentric perception; (2) instead of point-level actions, our Trajectory Predictor favors global trajectory-level planning to better align with instruction semantics; and (3) to enable anticipatory and long-horizon planning, we propose an Imagination Predictor to endow the agent with proactive thinking capability. On VLN-CE and real-world tests, DreamNav sets a new zero-shot state-of-the-art (SOTA), outperforming the strongest egocentric baseline with extra information by up to 7.49\\% and 18.15\\% in terms of SR and SPL metrics. To our knowledge, this is the first zero-shot VLN method to unify trajectory-level planning and active imagination while using only egocentric inputs.", "AI": {"tldr": "DreamNav\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u89d2\u6821\u6b63\u3001\u8f68\u8ff9\u7ea7\u89c4\u5212\u548c\u4e3b\u52a8\u60f3\u8c61\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u9ad8\u6210\u672c\u3001\u52a8\u4f5c\u8bed\u4e49\u4e0d\u5bf9\u9f50\u548c\u77ed\u89c6\u89c4\u5212\u95ee\u9898\uff0c\u5728VLN-CE\u548c\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u65b0\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u96f6\u6837\u672cVLN\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u611f\u77e5\u548c\u88ab\u52a8\u573a\u666f\u7406\u89e3\uff0c\u5c06\u63a7\u5236\u7b80\u5316\u4e3a\u70b9\u7ea7\u9009\u62e9\uff0c\u5bfc\u81f4\u90e8\u7f72\u6210\u672c\u9ad8\u3001\u52a8\u4f5c\u8bed\u4e49\u4e0d\u5bf9\u9f50\u548c\u89c4\u5212\u77ed\u89c6\u3002", "method": "\u63d0\u51faDreamNav\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1)EgoView Corrector\u5bf9\u9f50\u89c6\u89d2\u5e76\u7a33\u5b9a\u81ea\u6211\u4e2d\u5fc3\u611f\u77e5\uff1b(2)Trajectory Predictor\u8fdb\u884c\u5168\u5c40\u8f68\u8ff9\u7ea7\u89c4\u5212\uff1b(3)Imagination Predictor\u8d4b\u4e88\u667a\u80fd\u4f53\u4e3b\u52a8\u601d\u8003\u80fd\u529b\u3002", "result": "\u5728VLN-CE\u548c\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u4e2d\uff0cDreamNav\u5728SR\u548cSPL\u6307\u6807\u4e0a\u5206\u522b\u6bd4\u6700\u5f3a\u7684\u81ea\u6211\u4e2d\u5fc3\u57fa\u7ebf\u9ad8\u51fa7.49%\u548c18.15%\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u96f6\u6837\u672cSOTA\u3002", "conclusion": "\u8fd9\u662f\u7b2c\u4e00\u4e2a\u7edf\u4e00\u8f68\u8ff9\u7ea7\u89c4\u5212\u548c\u4e3b\u52a8\u60f3\u8c61\u7684\u96f6\u6837\u672cVLN\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u81ea\u6211\u4e2d\u5fc3\u8f93\u5165\uff0c\u4e3a\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u5bfc\u822a\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2509.11628", "pdf": "https://arxiv.org/pdf/2509.11628", "abs": "https://arxiv.org/abs/2509.11628", "authors": ["Jiacheng Liu", "Chang Zou", "Yuanhuiyi Lyu", "Fei Ren", "Shaobo Wang", "Kaixin Li", "Linfeng Zhang"], "title": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature Caching", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "15 pages, 9 figures, ACM Multimedia 2025", "summary": "Diffusion models have revolutionized high-fidelity image and video synthesis, yet their computational demands remain prohibitive for real-time applications. These models face two fundamental challenges: strict temporal dependencies preventing parallelization, and computationally intensive forward passes required at each denoising step. Drawing inspiration from speculative decoding in large language models, we present SpeCa, a novel 'Forecast-then-verify' acceleration framework that effectively addresses both limitations. SpeCa's core innovation lies in introducing Speculative Sampling to diffusion models, predicting intermediate features for subsequent timesteps based on fully computed reference timesteps. Our approach implements a parameter-free verification mechanism that efficiently evaluates prediction reliability, enabling real-time decisions to accept or reject each prediction while incurring negligible computational overhead. Furthermore, SpeCa introduces sample-adaptive computation allocation that dynamically modulates resources based on generation complexity, allocating reduced computation for simpler samples while preserving intensive processing for complex instances. Experiments demonstrate 6.34x acceleration on FLUX with minimal quality degradation (5.5% drop), 7.3x speedup on DiT while preserving generation fidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The verification mechanism incurs minimal overhead (1.67%-3.5% of full inference costs), establishing a new paradigm for efficient diffusion model inference while maintaining generation quality even at aggressive acceleration ratios. Our codes have been released in Github: \\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}", "AI": {"tldr": "SpeCa\u662f\u4e00\u4e2a\u57fa\u4e8e\u63a8\u6d4b\u91c7\u6837\u7684\u6269\u6563\u6a21\u578b\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u4e2d\u95f4\u7279\u5f81\u548c\u9a8c\u8bc1\u673a\u5236\u5b9e\u73b06-7\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf", "motivation": "\u6269\u6563\u6a21\u578b\u8ba1\u7b97\u9700\u6c42\u5927\uff0c\u5b58\u5728\u4e25\u683c\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u8ba1\u7b97\u5bc6\u96c6\u578b\u524d\u5411\u4f20\u9012\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u5e94\u7528", "method": "\u5f15\u5165\u63a8\u6d4b\u91c7\u6837\u9884\u6d4b\u540e\u7eed\u65f6\u95f4\u6b65\u7279\u5f81\uff0c\u91c7\u7528\u53c2\u6570\u65e0\u5173\u9a8c\u8bc1\u673a\u5236\u8bc4\u4f30\u9884\u6d4b\u53ef\u9760\u6027\uff0c\u5e76\u5b9e\u73b0\u6837\u672c\u81ea\u9002\u5e94\u8ba1\u7b97\u5206\u914d", "result": "\u5728FLUX\u4e0a\u5b9e\u73b06.34\u500d\u52a0\u901f\uff08\u8d28\u91cf\u4e0b\u964d5.5%\uff09\uff0cDiT\u4e0a7.3\u500d\u52a0\u901f\u4fdd\u6301\u4fdd\u771f\u5ea6\uff0cHunyuanVideo\u4e0a6.1\u500d\u52a0\u901f\u83b7\u5f9779.84% VBench\u5206\u6570", "conclusion": "SpeCa\u4e3a\u6269\u6563\u6a21\u578b\u63a8\u7406\u5efa\u7acb\u4e86\u65b0\u7684\u9ad8\u6548\u8303\u5f0f\uff0c\u5728\u6fc0\u8fdb\u52a0\u901f\u6bd4\u4e0b\u4ecd\u80fd\u4fdd\u6301\u751f\u6210\u8d28\u91cf\uff0c\u9a8c\u8bc1\u673a\u5236\u5f00\u9500\u6781\u5c0f"}}
{"id": "2509.11724", "pdf": "https://arxiv.org/pdf/2509.11724", "abs": "https://arxiv.org/abs/2509.11724", "authors": ["Wa-Kin Lei", "Jun-Cheng Chen", "Shang-Tse Chen"], "title": "DRAG: Data Reconstruction Attack using Guided Diffusion", "categories": ["cs.LG", "cs.CV"], "comment": "ICML 2025", "summary": "With the rise of large foundation models, split inference (SI) has emerged as a popular computational paradigm for deploying models across lightweight edge devices and cloud servers, addressing data privacy and computational cost concerns. However, most existing data reconstruction attacks have focused on smaller CNN classification models, leaving the privacy risks of foundation models in SI settings largely unexplored. To address this gap, we propose a novel data reconstruction attack based on guided diffusion, which leverages the rich prior knowledge embedded in a latent diffusion model (LDM) pre-trained on a large-scale dataset. Our method performs iterative reconstruction on the LDM's learned image prior, effectively generating high-fidelity images resembling the original data from their intermediate representations (IR). Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art methods, both qualitatively and quantitatively, in reconstructing data from deep-layer IRs of the vision foundation model. The results highlight the urgent need for more robust privacy protection mechanisms for large models in SI scenarios. Code is available at: https://github.com/ntuaislab/DRAG.", "AI": {"tldr": "\u57fa\u4e8e\u5bfc\u5411\u6269\u6563\u6a21\u578b\u7684\u6570\u636e\u91cd\u6784\u653b\u51fb\u65b9\u6cd5\uff0c\u5229\u7528\u9886\u5148\u77e5\u8bc6\u4ece\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u4e2d\u95f4\u8868\u5f81\u91cd\u6784\u9ad8\u4fdd\u771f\u56fe\u50cf\uff0c\u663e\u793a\u5206\u5f00\u63a8\u7406\u4e2d\u7684\u4e25\u91cd\u9690\u79c1\u98ce\u9669", "motivation": "\u968f\u7740\u5927\u578b\u57fa\u7840\u6a21\u578b\u7684\u5174\u8d77\uff0c\u5206\u5f00\u63a8\u7406\u6210\u4e3a\u666e\u904d\u8ba1\u7b97\u8303\u5f0f\uff0c\u4f46\u73b0\u6709\u7684\u6570\u636e\u91cd\u6784\u653b\u51fb\u4e3b\u8981\u96c6\u4e2d\u5728\u5c0f\u578bCNN\u5206\u7c7b\u6a21\u578b\u4e0a\uff0c\u5927\u578b\u57fa\u7840\u6a21\u578b\u5728\u5206\u5f00\u63a8\u7406\u573a\u666f\u4e2d\u7684\u9690\u79c1\u98ce\u9669\u5f88\u5c11\u88ab\u63a2\u7d22", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bfc\u5411\u6269\u6563\u6a21\u578b\u7684\u65b0\u9898\u6570\u636e\u91cd\u6784\u653b\u51fb\u65b9\u6cd5\uff0c\u5229\u7528\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u9886\u5148\u8bad\u7ec3\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b(LDM)\u4e2d\u5d4c\u5165\u7684\u4e30\u5bcc\u9886\u5148\u77e5\u8bc6\uff0c\u5728LDM\u5b66\u4e60\u5230\u7684\u56fe\u50cf\u9886\u5148\u4e0a\u8fdb\u884c\u8fed\u4ee3\u91cd\u6784", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u4ece\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u6df1\u5c42\u4e2d\u95f4\u8868\u5f81\u91cd\u6784\u6570\u636e\u65f6\uff0c\u5728\u8d28\u91cf\u548c\u6570\u91cf\u4e0a\u90fd\u663e\u8457\u8d85\u8fc7\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u56fe\u50cf", "conclusion": "\u8be5\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728\u5206\u5f00\u63a8\u7406\u573a\u666f\u4e2d\u4e3a\u5927\u578b\u6a21\u578b\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u9690\u79c1\u4fdd\u62a4\u673a\u5236\u7684\u7d27\u8feb\u6027"}}
{"id": "2509.12001", "pdf": "https://arxiv.org/pdf/2509.12001", "abs": "https://arxiv.org/abs/2509.12001", "authors": ["Marcus Lin", "Jennifer Lai"], "title": "Data-driven Smile Design: Personalized Dental Aesthetics Outcomes Using Deep Learning", "categories": ["eess.IV", "cs.CV", "I.2.6; I.2.10; J.3"], "comment": "6 pages, 2 figures", "summary": "A healthy smile plays a significant role in functional as well as esthetic considerations, improving confidence. It is difficult for dental professionals to strike a balance between esthetic requirements and functional requirements. Traditional smile design has had heavy reliance on dentist expertise and used plaster models and hand drawings, raising questions about the outcome for patients. Digital technology, led by Dr. Christian Coachman in 2007, allows photographic and videographic assessments, enabling improved intercommunication among specialists and patients. Advances in artificial intelligence (AI) and big data have supported analysis of facial features and development of personalized smile designs in the last few years. Outputs are, however, susceptible to practitioner bias or limitations of training data, and may be suboptimal for individual users. The study presented here suggests a comprehensive system integrating AI, big data, and recognition technologies to automate the smile design process so that both experienced and inexperienced dentists can generate pleasing aesthetics with ease. The system has a Facial Feature Extraction Module and an Image Generation Module, serving diverse practitioner and patient needs. User data can be incorporated in future research for design optimization and testing of virtual and augmented reality for real-time previewing. Data gathered can also be employed in aesthetic preference analyses, which can enhance our knowledge of smile design in dental practice.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6574\u5408AI\u3001\u5927\u6570\u636e\u548c\u8bc6\u522b\u6280\u672f\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u7b11\u5bb9\u8bbe\u8ba1\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4f20\u7edf\u624b\u5de5\u65b9\u6cd5\u5bf9\u533b\u751f\u4e13\u4e1a\u77e5\u8bc6\u7684\u4f9d\u8d56\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7b11\u5bb9\u8bbe\u8ba1\u4f9d\u8d56\u7259\u533b\u4e13\u4e1a\u77e5\u8bc6\u548c\u624b\u5de5\u64cd\u4f5c\uff0c\u5b58\u5728\u6548\u679c\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u800c\u73b0\u6709\u6570\u5b57\u6280\u672f\u4ecd\u53d7\u5230\u5b9e\u65bd\u8005\u504f\u89c1\u548c\u8bad\u7ec3\u6570\u636e\u9650\u5236\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u7cfb\u7edf\uff0c\u5305\u542b\u9762\u90e8\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u548c\u56fe\u50cf\u751f\u6210\u6a21\u5757\uff0c\u901a\u8fc7AI\u3001\u5927\u6570\u636e\u548c\u8bc6\u522b\u6280\u672f\u81ea\u52a8\u5316\u7b11\u5bb9\u8bbe\u8ba1\u8fc7\u7a0b\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u8ba9\u7ecf\u9a8c\u4e30\u5bcc\u548c\u7ecf\u9a8c\u4e0d\u8db3\u7684\u7259\u533b\u90fd\u80fd\u8f7b\u677e\u751f\u6210\u7f8e\u89c2\u7684\u7b11\u5bb9\u8bbe\u8ba1\uff0c\u6700\u5927\u9650\u5ea6\u51cf\u5c11\u4e86\u4e3b\u89c2\u504f\u89c1\u7684\u5f71\u54cd\u3002", "conclusion": "\u8fd9\u79cd\u6570\u5b57\u5316\u7b11\u5bb9\u8bbe\u8ba1\u7cfb\u7edf\u6709\u671b\u63d0\u9ad8\u7259\u79d1\u5b9e\u8df5\u4e2d\u7684\u7f8e\u5b66\u6548\u679c\uff0c\u672a\u6765\u53ef\u901a\u8fc7\u865a\u62df\u73b0\u5b9e/\u589e\u5f3a\u73b0\u5b9e\u6280\u672f\u8fdb\u884c\u5b9e\u65f6\u9884\u89c8\uff0c\u6240\u6536\u96c6\u7684\u6570\u636e\u4e5f\u53ef\u7528\u4e8e\u7f8e\u5b66\u504f\u597d\u5206\u6790\u3002"}}
{"id": "2509.12074", "pdf": "https://arxiv.org/pdf/2509.12074", "abs": "https://arxiv.org/abs/2509.12074", "authors": ["Mohammadreza Narimani", "Alireza Pourreza", "Ali Moghimi", "Parastoo Farajpoor", "Hamid Jafarbiglu", "Mohsen B. Mesgaran"], "title": "Early Detection of Branched Broomrape (Phelipanche ramosa) Infestation in Tomato Crops Using Leaf Spectral Analysis and Machine Learning", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.SP", "68T07, 68T45, 68U10", "I.5.4; I.4.6; I.2.6"], "comment": "Author-accepted version. Accepted and presented at AGRICONTROL 2025   (8th IFAC Conference on Sensing, Control and Automation Technologies for   Agriculture), UC Davis, USA. To appear in IFAC-PapersOnLine (Elsevier)", "summary": "Branched broomrape (Phelipanche ramosa) is a chlorophyll-deficient parasitic weed that threatens tomato production by extracting nutrients from the host. We investigate early detection using leaf-level spectral reflectance (400-2500 nm) and ensemble machine learning. In a field experiment in Woodland, California, we tracked 300 tomato plants across growth stages defined by growing degree days (GDD). Leaf reflectance was acquired with a portable spectrometer and preprocessed (band denoising, 1 nm interpolation, Savitzky-Golay smoothing, correlation-based band reduction). Clear class differences were observed near 1500 nm and 2000 nm water absorption features, consistent with reduced leaf water content in infected plants at early stages. An ensemble combining Random Forest, XGBoost, SVM with RBF kernel, and Naive Bayes achieved 89% accuracy at 585 GDD, with recalls of 0.86 (infected) and 0.93 (noninfected). Accuracy declined at later stages (e.g., 69% at 1568 GDD), likely due to senescence and weed interference. Despite the small number of infected plants and environmental confounders, results show that proximal sensing with ensemble learning enables timely detection of broomrape before canopy symptoms are visible, supporting targeted interventions and reduced yield losses.", "AI": {"tldr": "\u901a\u8fc7\u53f6\u9762\u5149\u8c31\u53cd\u5c04\u548c\u96c6\u6210\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728585\u589e\u6e29\u5ea6\u79ef\u6e29\u65f6\u5b9e\u73b0\u4e86\u65e9\u671f\u68c0\u6d4b\u6a31\u6843\u4f2f\u547c\u85cf\u83ab\u85cf\u7684\u9ad8\u51c6\u786e\u7387\uff0c\u4e3a\u9488\u5bf9\u6027\u5e72\u9884\u63d0\u4f9b\u652f\u6301", "motivation": "\u5206\u652f\u6021\u624b\u68d5\u662f\u4e00\u79cd\u4e25\u91cd\u5a01\u80c1\u756a\u8304\u751f\u4ea7\u7684\u5bc4\u751f\u6742\u8349\uff0c\u9700\u8981\u5728\u53ef\u89c1\u75c7\u72b6\u51fa\u73b0\u524d\u5b9e\u73b0\u65e9\u671f\u68c0\u6d4b\uff0c\u4ee5\u51cf\u5c11\u4ea7\u91cf\u635f\u5931", "method": "\u91c7\u7528\u53f6\u9762\u5149\u8c31\u53cd\u5c04\u6280\u672f(400-2500nm)\u548c\u96c6\u6210\u673a\u5668\u5b66\u4e60\u7b97\u6cd5(Random Forest\u3001XGBoost\u3001SVM\u3001Naive Bayes)\uff0c\u901a\u8fc7\u589e\u6e29\u5ea6\u79ef\u6e29\u5b9a\u4e49\u690d\u6848\u751f\u957f\u9636\u6bb5", "result": "\u5728585\u589e\u6e29\u5ea6\u79ef\u6e29\u65f6\u8fbe\u523089%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u611f\u67d3\u548c\u975e\u611f\u67d3\u690d\u6848\u7684\u53ec\u56de\u7387\u5206\u522b\u4e3a0.86\u548c0.93\uff0c\u540e\u671f\u51c6\u786e\u7387\u4e0b\u964d\u81f369%", "conclusion": "\u8fd1\u7aef\u611f\u77e5\u6280\u672f\u7ed3\u5408\u96c6\u6210\u5b66\u4e60\u80fd\u591f\u5728\u53ef\u89c1\u75c7\u72b6\u51fa\u73b0\u524d\u53ca\u65f6\u68c0\u6d4b\u5206\u652f\u6021\u624b\u68d5\uff0c\u4e3a\u9488\u5bf9\u6027\u5e72\u9884\u63d0\u4f9b\u4e86\u6709\u6548\u624b\u6bb5"}}
