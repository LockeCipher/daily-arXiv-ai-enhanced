<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 32]
- [cs.LG](#cs.LG) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [DualPhys-GS: Dual Physically-Guided 3D Gaussian Splatting for Underwater Scene Reconstruction](https://arxiv.org/abs/2508.09610)
*Jiachen Li,Guangzhi Han,Jin Wan,Yuan Gao,Delong Han*

Main category: cs.GR

TL;DR: 提出DualPhys-GS框架，通过双路径优化机制解决水下场景3D重建中的颜色失真和几何伪影问题。


<details>
  <summary>Details</summary>
Motivation: 传统大气光学模型无法有效处理水下光波长选择性衰减和悬浮颗粒散射，导致重建质量下降。

Method: 采用双特征引导的衰减-散射建模机制，结合RGB特征和深度信息优化衰减，多尺度深度感知散射模型捕捉散射效应，并设计多种特殊损失函数。

Result: 在悬浮物密集区域和远距离场景中，重建质量显著优于现有方法。

Conclusion: DualPhys-GS框架通过物理一致性和自适应机制，显著提升了水下3D重建的精度和鲁棒性。

Abstract: In 3D reconstruction of underwater scenes, traditional methods based on atmospheric optical models cannot effectively deal with the selective attenuation of light wavelengths and the effect of suspended particle scattering, which are unique to the water medium, and lead to color distortion, geometric artifacts, and collapsing phenomena at long distances. We propose the DualPhys-GS framework to achieve high-quality underwater reconstruction through a dual-path optimization mechanism. Our approach further develops a dual feature-guided attenuation-scattering modeling mechanism, the RGB-guided attenuation optimization model combines RGB features and depth information and can handle edge and structural details. In contrast, the multi-scale depth-aware scattering model captures scattering effects at different scales using a feature pyramid network and an attention mechanism. Meanwhile, we design several special loss functions. The attenuation scattering consistency loss ensures physical consistency. The water body type adaptive loss dynamically adjusts the weighting coefficients. The edge-aware scattering loss is used to maintain the sharpness of structural edges. The multi-scale feature loss helps to capture global and local structural information. In addition, we design a scene adaptive mechanism that can automatically identify the water-body-type characteristics (e.g., clear coral reef waters or turbid coastal waters) and dynamically adjust the scattering and attenuation parameters and optimization strategies. Experimental results show that our method outperforms existing methods in several metrics, especially in suspended matter-dense regions and long-distance scenes, and the reconstruction quality is significantly improved.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [Synthetic Data Generation for Emotional Depth Faces: Optimizing Conditional DCGANs via Genetic Algorithms in the Latent Space and Stabilizing Training with Knowledge Distillation](https://arxiv.org/abs/2508.09188)
*Seyed Muhammad Hossein Mousavi,S. Younes Mirinezhad*

Main category: cs.CV

TL;DR: 提出了一种基于优化GAN和知识蒸馏的合成深度人脸生成框架，结合遗传算法提升多样性和质量，在情感分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决情感计算中高质量、多样化深度面部数据集的缺乏问题。

Method: 使用优化的GAN和知识蒸馏（EMA教师模型）稳定训练，结合遗传算法优化潜在向量，提取多种特征进行分类。

Result: 在多样性和质量上优于GAN、VAE、GMM和KDE，分类准确率达94%和96%。

Conclusion: 该方法在生成质量和分类性能上均优于现有技术。

Abstract: Affective computing faces a major challenge: the lack of high-quality, diverse depth facial datasets for recognizing subtle emotional expressions. We propose a framework for synthetic depth face generation using an optimized GAN with Knowledge Distillation (EMA teacher models) to stabilize training, improve quality, and prevent mode collapse. We also apply Genetic Algorithms to evolve GAN latent vectors based on image statistics, boosting diversity and visual quality for target emotions. The approach outperforms GAN, VAE, GMM, and KDE in both diversity and quality. For classification, we extract and concatenate LBP, HOG, Sobel edge, and intensity histogram features, achieving 94% and 96% accuracy with XGBoost. Evaluation using FID, IS, SSIM, and PSNR shows consistent improvement over state-of-the-art methods.

</details>


### [3] [Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method](https://arxiv.org/abs/2508.09202)
*Masoumeh Sharafi,Soufiane Belharbi,Houssem Ben Salem,Ali Etemad,Alessandro Lameiras Koerich,Marco Pedersoli,Simon Bacon,Eric Granger*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的个性化特征翻译（PFT）方法，用于源数据不可用的无监督域适应（SFDA），仅利用未标记的中性表情目标数据，避免了图像合成的复杂性和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的SFDA方法通常无法适应仅包含单一类别（如中性表情）的目标数据，且基于图像生成的方法不稳定且计算量大。

Method: 提出PFT方法，在潜在空间中进行特征翻译，通过预训练翻译器在源数据上转换主体风格特征，并在目标数据上适应，无需源数据或图像合成。

Result: PFT避免了表情生成的噪声和复杂性，生成优化的分类嵌入，计算效率高。

Conclusion: PFT是一种高效且轻量级的SFDA方法，适用于仅含中性表情的目标数据场景。

Abstract: Facial expression recognition (FER) models are employed in many video-based affective computing applications, such as human-computer interaction and healthcare monitoring. However, deep FER models often struggle with subtle expressions and high inter-subject variability, limiting their performance in real-world applications. To improve their performance, source-free domain adaptation (SFDA) methods have been proposed to personalize a pretrained source model using only unlabeled target domain data, thereby avoiding data privacy, storage, and transmission constraints. This paper addresses a challenging scenario where source data is unavailable for adaptation, and only unlabeled target data consisting solely of neutral expressions is available. SFDA methods are not typically designed to adapt using target data from only a single class. Further, using models to generate facial images with non-neutral expressions can be unstable and computationally intensive. In this paper, personalized feature translation (PFT) is proposed for SFDA. Unlike current image translation methods for SFDA, our lightweight method operates in the latent space. We first pre-train the translator on the source domain data to transform the subject-specific style features from one source subject into another. Expression information is preserved by optimizing a combination of expression consistency and style-aware objectives. Then, the translator is adapted on neutral target data, without using source data or image synthesis. By translating in the latent space, PFT avoids the complexity and noise of face expression generation, producing discriminative embeddings optimized for classification. Using PFT eliminates the need for image synthesis, reduces computational overhead (using a lightweight translator), and only adapts part of the model, making the method efficient compared to image-based translation.

</details>


### [4] [GANime: Generating Anime and Manga Character Drawings from Sketches with Deep Learning](https://arxiv.org/abs/2508.09207)
*Tai Vu,Robert Yang*

Main category: cs.CV

TL;DR: 研究比较了多种图像到图像转换模型，发现C-GAN在动漫角色草图着色任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决动漫行业中草图着色过程的高成本和低效率问题。

Method: 评估了Neural Style Transfer、C-GAN和CycleGAN等模型。

Result: C-GAN能生成接近人类创作的高质量、高分辨率图像。

Conclusion: C-GAN是动漫草图着色任务中最有效的模型。

Abstract: The process of generating fully colorized drawings from sketches is a large, usually costly bottleneck in the manga and anime industry. In this study, we examine multiple models for image-to-image translation between anime characters and their sketches, including Neural Style Transfer, C-GAN, and CycleGAN. By assessing them qualitatively and quantitatively, we find that C-GAN is the most effective model that is able to produce high-quality and high-resolution images close to those created by humans.

</details>


### [5] [Gradient-Direction-Aware Density Control for 3D Gaussian Splatting](https://arxiv.org/abs/2508.09239)
*Zheng Zhou,Yu-Jie Xiong,Chun-Ming Xia,Jia-Chen Zhang,Hong-Jian Zhan*

Main category: cs.CV

TL;DR: GDAGS提出了一种基于梯度方向感知的自适应密度控制框架，解决了3D高斯溅射中的过重建和过密集问题，显著提升了渲染质量并减少了内存消耗。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯溅射方法在复杂场景中存在过重建和过密集问题，导致内存开销增加和渲染质量下降。

Method: GDAGS通过梯度一致性比率（GCR）和非线性动态加权机制，实现梯度方向感知的密度控制，优化高斯分布。

Result: GDAGS在多种真实场景基准测试中表现出色，渲染质量提升，内存消耗减少50%。

Conclusion: GDAGS有效解决了3D高斯溅射的关键问题，为实时逼真渲染提供了更高效的解决方案。

Abstract: The emergence of 3D Gaussian Splatting (3DGS) has significantly advanced novel view synthesis through explicit scene representation, enabling real-time photorealistic rendering. However, existing approaches manifest two critical limitations in complex scenarios: (1) Over-reconstruction occurs when persistent large Gaussians cannot meet adaptive splitting thresholds during density control. This is exacerbated by conflicting gradient directions that prevent effective splitting of these Gaussians; (2) Over-densification of Gaussians occurs in regions with aligned gradient aggregation, leading to redundant component proliferation. This redundancy significantly increases memory overhead due to unnecessary data retention. We present Gradient-Direction-Aware Gaussian Splatting (GDAGS), a gradient-direction-aware adaptive density control framework to address these challenges. Our key innovations: the gradient coherence ratio (GCR), computed through normalized gradient vector norms, which explicitly discriminates Gaussians with concordant versus conflicting gradient directions; and a nonlinear dynamic weighting mechanism leverages the GCR to enable gradient-direction-aware density control. Specifically, GDAGS prioritizes conflicting-gradient Gaussians during splitting operations to enhance geometric details while suppressing redundant concordant-direction Gaussians. Conversely, in cloning processes, GDAGS promotes concordant-direction Gaussian densification for structural completion while preventing conflicting-direction Gaussian overpopulation. Comprehensive evaluations across diverse real-world benchmarks demonstrate that GDAGS achieves superior rendering quality while effectively mitigating over-reconstruction, suppressing over-densification, and constructing compact scene representations with 50\% reduced memory consumption through optimized Gaussians utilization.

</details>


### [6] [Lung-DDPM+: Efficient Thoracic CT Image Synthesis using Diffusion Probabilistic Model](https://arxiv.org/abs/2508.09327)
*Yifan Jiang,Ahmad Shariftabrizi,Venkata SK. Manem*

Main category: cs.CV

TL;DR: 论文提出了一种改进的生成模型Lung-DDPM+，用于高效生成高保真度的肺部CT图像，解决了现有模型效率低和解剖不精确的问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在肺部癌症诊断中效率低且解剖精度不足，限制了临床应用。

Method: 采用基于结节语义布局的去噪扩散概率模型（DDPM），并通过肺部DPM-solver加速，优化采样效率与质量。

Result: 在LIDC-IDRI数据集上，模型减少了8倍FLOPs、6.8倍GPU内存消耗，采样速度提升14倍，同时保持高质量样本生成能力。

Conclusion: Lung-DDPM+能高效生成高质量肺部CT图像，具有广泛临床应用潜力。

Abstract: Generative artificial intelligence (AI) has been playing an important role in various domains. Leveraging its high capability to generate high-fidelity and diverse synthetic data, generative AI is widely applied in diagnostic tasks, such as lung cancer diagnosis using computed tomography (CT). However, existing generative models for lung cancer diagnosis suffer from low efficiency and anatomical imprecision, which limit their clinical applicability. To address these drawbacks, we propose Lung-DDPM+, an improved version of our previous model, Lung-DDPM. This novel approach is a denoising diffusion probabilistic model (DDPM) guided by nodule semantic layouts and accelerated by a pulmonary DPM-solver, enabling the method to focus on lesion areas while achieving a better trade-off between sampling efficiency and quality. Evaluation results on the public LIDC-IDRI dataset suggest that the proposed method achieves 8$\times$ fewer FLOPs (floating point operations per second), 6.8$\times$ lower GPU memory consumption, and 14$\times$ faster sampling compared to Lung-DDPM. Moreover, it maintains comparable sample quality to both Lung-DDPM and other state-of-the-art (SOTA) generative models in two downstream segmentation tasks. We also conducted a Visual Turing Test by an experienced radiologist, showing the advanced quality and fidelity of synthetic samples generated by the proposed method. These experimental results demonstrate that Lung-DDPM+ can effectively generate high-quality thoracic CT images with lung nodules, highlighting its potential for broader applications, such as general tumor synthesis and lesion generation in medical imaging. The code and pretrained models are available at https://github.com/Manem-Lab/Lung-DDPM-PLUS.

</details>


### [7] [DenoDet V2: Phase-Amplitude Cross Denoising for SAR Object Detection](https://arxiv.org/abs/2508.09392)
*Kang Ni,Minrui Zou,Yuxuan Li,Xiang Li,Kehua Guo,Ming-Ming Cheng,Yimian Dai*

Main category: cs.CV

TL;DR: DenoDet V2提出了一种基于变换域特征解调和调制的注意力架构，通过振幅和相位信息的互补性实现去噪，性能优于V1且模型复杂度减半。


<details>
  <summary>Details</summary>
Motivation: 解决合成孔径雷达（SAR）目标检测中相干噪声的普遍影响，现有方法多基于空间域特征分析或增强，而DenoDet V2探索了变换域的新视角。

Method: 设计了注意力架构，通过波段互调制机制利用振幅和相位信息的互补性，实现相互增强。

Result: 在多个SAR数据集上表现优异，SARDet-100K上比V1提升0.8%，模型复杂度减半。

Conclusion: DenoDet V2通过变换域特征调制实现了高效去噪，性能显著提升且模型更轻量。

Abstract: One of the primary challenges in Synthetic Aperture Radar (SAR) object detection lies in the pervasive influence of coherent noise. As a common practice, most existing methods, whether handcrafted approaches or deep learning-based methods, employ the analysis or enhancement of object spatial-domain characteristics to achieve implicit denoising. In this paper, we propose DenoDet V2, which explores a completely novel and different perspective to deconstruct and modulate the features in the transform domain via a carefully designed attention architecture. Compared to DenoDet V1, DenoDet V2 is a major advancement that exploits the complementary nature of amplitude and phase information through a band-wise mutual modulation mechanism, which enables a reciprocal enhancement between phase and amplitude spectra. Extensive experiments on various SAR datasets demonstrate the state-of-the-art performance of DenoDet V2. Notably, DenoDet V2 achieves a significant 0.8\% improvement on SARDet-100K dataset compared to DenoDet V1, while reducing the model complexity by half. The code is available at https://github.com/GrokCV/GrokSAR.

</details>


### [8] [RASR: Retrieval-Augmented Super Resolution for Practical Reference-based Image Restoration](https://arxiv.org/abs/2508.09449)
*Jiaqi Yan,Shuning Xu,Xiangyu Chen,Dell Zhang,Jie Tang,Gangshan Wu,Jie Liu*

Main category: cs.CV

TL;DR: 论文提出了一种新的参考超分辨率方法RASR，通过自动检索相关高分辨率参考图像，解决了传统RefSR依赖手动配对的限制。


<details>
  <summary>Details</summary>
Motivation: 现有RefSR方法依赖手动配对的参考图像，限制了实际应用。RASR旨在通过自动检索参考图像，提升实用性和灵活性。

Method: 提出RASR框架，结合语义检索器和扩散生成器，自动检索并利用相关参考图像进行超分辨率重建。

Result: 在RASR-Flickr30数据集上，RASRNet比SISR基线提升0.38 dB PSNR和-0.0131 LPIPS，生成更真实的纹理。

Conclusion: RASR通过检索增强，为RefSR的实际应用提供了新方向。

Abstract: Reference-based Super Resolution (RefSR) improves upon Single Image Super Resolution (SISR) by leveraging high-quality reference images to enhance texture fidelity and visual realism. However, a critical limitation of existing RefSR approaches is their reliance on manually curated target-reference image pairs, which severely constrains their practicality in real-world scenarios. To overcome this, we introduce Retrieval-Augmented Super Resolution (RASR), a new and practical RefSR paradigm that automatically retrieves semantically relevant high-resolution images from a reference database given only a low-quality input. This enables scalable and flexible RefSR in realistic use cases, such as enhancing mobile photos taken in environments like zoos or museums, where category-specific reference data (e.g., animals, artworks) can be readily collected or pre-curated. To facilitate research in this direction, we construct RASR-Flickr30, the first benchmark dataset designed for RASR. Unlike prior datasets with fixed target-reference pairs, RASR-Flickr30 provides per-category reference databases to support open-world retrieval. We further propose RASRNet, a strong baseline that combines a semantic reference retriever with a diffusion-based RefSR generator. It retrieves relevant references based on semantic similarity and employs a diffusion-based generator enhanced with semantic conditioning. Experiments on RASR-Flickr30 demonstrate that RASRNet consistently improves over SISR baselines, achieving +0.38 dB PSNR and -0.0131 LPIPS, while generating more realistic textures. These findings highlight retrieval augmentation as a promising direction to bridge the gap between academic RefSR research and real-world applicability.

</details>


### [9] [IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding](https://arxiv.org/abs/2508.09456)
*Junxian Li,Beining Xu,Di Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新型输入感知后门攻击方法IAG，针对视觉语言模型（VLMs）的视觉定位任务，通过自适应触发器生成器将攻击目标的语义信息嵌入图像，实现攻击目标的无视用户查询的定位。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在视觉定位任务中表现优异，但其在视觉定位任务中的安全问题，尤其是后门攻击，尚未得到充分研究。

Method: 提出IAG方法，利用文本条件U-Net生成自适应触发器，将攻击目标的语义信息嵌入图像，并通过重构损失确保攻击的隐蔽性。

Result: IAG在InternVL-2.5-8B上的ASR@0.5超过65%，且在Ferret-7B和LlaVA-1.5-7B上表现出色，对干净样本的准确性影响极小。

Conclusion: IAG展示了强大的攻击效果和隐蔽性，为视觉语言模型的安全性研究提供了新视角。

Abstract: Vision-language models (VLMs) have shown significant advancements in tasks such as visual grounding, where they localize specific objects in images based on natural language queries and images. However, security issues in visual grounding tasks for VLMs remain underexplored, especially in the context of backdoor attacks. In this paper, we introduce a novel input-aware backdoor attack method, IAG, designed to manipulate the grounding behavior of VLMs. This attack forces the model to ground a specific target object in the input image, regardless of the user's query. We propose an adaptive trigger generator that embeds the semantic information of the attack target's description into the original image using a text-conditional U-Net, thereby overcoming the open-vocabulary attack challenge. To ensure the attack's stealthiness, we utilize a reconstruction loss to minimize visual discrepancies between poisoned and clean images. Additionally, we introduce a unified method for generating attack data. IAG is evaluated theoretically and empirically, demonstrating its feasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches over 65\% on various testing sets. IAG also shows promising potential on manipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on clean samples. Extensive specific experiments, such as ablation study and potential defense, also indicate the robustness and transferability of our attack.

</details>


### [10] [From Large Angles to Consistent Faces: Identity-Preserving Video Generation via Mixture of Facial Experts](https://arxiv.org/abs/2508.09476)
*Yuji Wang,Moran Li,Xiaobin Hu,Ran Yi,Jiangning Zhang,Chengming Xu,Weijian Cao,Yabiao Wang,Chengjie Wang,Lizhuang Ma*

Main category: cs.CV

TL;DR: 论文提出了一种动态混合面部专家（MoFE）方法和定制数据处理流程，解决了视频生成模型中大角度面部身份保持的难题，并创建了LFA数据集。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在大角度面部时难以保持身份一致性，主要因缺乏有效机制将身份特征融入DiT结构，以及开源数据集中大角度面部覆盖不足。

Method: 引入MoFE，结合三种专家（身份、语义、细节）动态捕捉面部特征；设计数据处理流程（面部约束和身份一致性）以优化数据集。

Result: 在LFA基准测试中，方法在面部相似度、FID和CLIP语义对齐上显著优于现有技术。

Conclusion: MoFE和LFA数据集有效提升了大角度面部视频生成的身份保持能力，代码和数据集将开源。

Abstract: Current video generation models struggle with identity preservation under large facial angles, primarily facing two challenges: the difficulty in exploring an effective mechanism to integrate identity features into DiT structure, and the lack of targeted coverage of large facial angles in existing open-source video datasets. To address these, we present two key innovations. First, we introduce a Mixture of Facial Experts (MoFE) that dynamically combines complementary cues from three specialized experts, each designed to capture distinct but mutually reinforcing aspects of facial attributes. The identity expert captures cross-pose identity-sensitive features, the semantic expert extracts high-level visual semantxics, and the detail expert preserves pixel-level features (e.g., skin texture, color gradients). Furthermore, to mitigate dataset limitations, we have tailored a data processing pipeline centered on two key aspects: Face Constraints and Identity Consistency. Face Constraints ensure facial angle diversity and a high proportion of facial regions, while Identity Consistency preserves coherent person-specific features across temporal sequences, collectively addressing the scarcity of large facial angles and identity-stable training data in existing datasets. Leveraging this pipeline, we have curated and refined a Large Face Angles (LFA) Dataset from existing open-source human video datasets, comprising 460K video clips with annotated facial angles. Experimental results on the LFA benchmark demonstrate that our method, empowered by the LFA dataset, significantly outperforms prior SOTA methods in face similarity, face FID, and CLIP semantic alignment. The code and dataset will be made publicly available at https://github.com/rain152/LFA-Video-Generation.

</details>


### [11] [SkySplat: Generalizable 3D Gaussian Splatting from Multi-Temporal Sparse Satellite Images](https://arxiv.org/abs/2508.09479)
*Xuejun Huang,Xinyi Liu,Yi Wan,Zhi Zheng,Bin Zhang,Mingtao Xiong,Yingying Pei,Yongjun Zhang*

Main category: cs.CV

TL;DR: SkySplat是一种自监督框架，通过整合RPC模型到3DGS流程中，显著提升了稀疏卫星图像的三维重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法不适用于卫星图像，且多时相稀疏卫星图像的重建面临几何约束不足、瞬态物体和辐射不一致等挑战。

Method: 提出SkySplat框架，结合RPC模型和3DGS，引入CSCM模块和多视角一致性聚合策略，仅依赖RGB图像和相对高度监督。

Result: SkySplat比EOGS快86倍且更准确，在DFC19数据集上MAE从13.18米降至1.80米，并在MVS3D基准测试中表现优异。

Conclusion: SkySplat通过自监督和几何约束优化，显著提升了稀疏卫星图像的三维重建效率和精度。

Abstract: Three-dimensional scene reconstruction from sparse-view satellite images is a long-standing and challenging task. While 3D Gaussian Splatting (3DGS) and its variants have recently attracted attention for its high efficiency, existing methods remain unsuitable for satellite images due to incompatibility with rational polynomial coefficient (RPC) models and limited generalization capability. Recent advances in generalizable 3DGS approaches show potential, but they perform poorly on multi-temporal sparse satellite images due to limited geometric constraints, transient objects, and radiometric inconsistencies. To address these limitations, we propose SkySplat, a novel self-supervised framework that integrates the RPC model into the generalizable 3DGS pipeline, enabling more effective use of sparse geometric cues for improved reconstruction. SkySplat relies only on RGB images and radiometric-robust relative height supervision, thereby eliminating the need for ground-truth height maps. Key components include a Cross-Self Consistency Module (CSCM), which mitigates transient object interference via consistency-based masking, and a multi-view consistency aggregation strategy that refines reconstruction results. Compared to per-scene optimization methods, SkySplat achieves an 86 times speedup over EOGS with higher accuracy. It also outperforms generalizable 3DGS baselines, reducing MAE from 13.18 m to 1.80 m on the DFC19 dataset significantly, and demonstrates strong cross-dataset generalization on the MVS3D benchmark.

</details>


### [12] [Generation of Indian Sign Language Letters, Numbers, and Words](https://arxiv.org/abs/2508.09522)
*Ajeet Kumar Yadav,Nishant Kumar,Rathna G N*

Main category: cs.CV

TL;DR: 论文提出了一种结合ProGAN和SAGAN的GAN变体，用于生成高质量、高分辨率的印度手语图像，并在IS和FID指标上优于传统ProGAN。


<details>
  <summary>Details</summary>
Motivation: 手语是听力障碍人士的重要交流工具，但目前手语生成技术仍有待探索。

Method: 结合ProGAN和SAGAN的优点，开发了一种改进的基于注意力的模型，用于生成印度手语的字母、数字和单词图像。

Result: 新模型在IS和FID指标上分别提升了3.2和30.12，并发布了一个包含高质量印度手语图像的大型数据集。

Conclusion: 该研究为手语生成提供了更高效的方法，并贡献了一个宝贵的数据集。

Abstract: Sign language, which contains hand movements, facial expressions and bodily gestures, is a significant medium for communicating with hard-of-hearing people. A well-trained sign language community communicates easily, but those who don't know sign language face significant challenges. Recognition and generation are basic communication methods between hearing and hard-of-hearing individuals. Despite progress in recognition, sign language generation still needs to be explored. The Progressive Growing of Generative Adversarial Network (ProGAN) excels at producing high-quality images, while the Self-Attention Generative Adversarial Network (SAGAN) generates feature-rich images at medium resolutions. Balancing resolution and detail is crucial for sign language image generation. We are developing a Generative Adversarial Network (GAN) variant that combines both models to generate feature-rich, high-resolution, and class-conditional sign language images. Our modified Attention-based model generates high-quality images of Indian Sign Language letters, numbers, and words, outperforming the traditional ProGAN in Inception Score (IS) and Fr\'echet Inception Distance (FID), with improvements of 3.2 and 30.12, respectively. Additionally, we are publishing a large dataset incorporating high-quality images of Indian Sign Language alphabets, numbers, and 129 words.

</details>


### [13] [WEC-DG: Multi-Exposure Wavelet Correction Method Guided by Degradation Description](https://arxiv.org/abs/2508.09565)
*Ming Zhao,Pingping Liu,Tongshun Zhang,Zhe Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于小波变换的多曝光校正方法（WEC-DG），通过引入退化描述符和小波变换的光-细节解耦特性，显著提升了复杂成像条件下的校正效果。


<details>
  <summary>Details</summary>
Motivation: 当前多曝光校正方法在处理单曝光图像时，难以应对由多样光照条件和环境因素引起的类内变异性，导致曝光异常校正不准确。

Method: 提出WEC-DG方法，包括曝光一致性对齐模块（ECAM）和曝光恢复与细节重建模块（EDRM），利用小波变换处理低高频信息。

Result: 在多个公开数据集上的实验表明，该方法性能优于现有算法，显著提升了校正效果。

Conclusion: WEC-DG方法在复杂成像条件下具有高效性和实用性，为多曝光校正提供了新思路。

Abstract: Multi-exposure correction technology is essential for restoring images affected by insufficient or excessive lighting, enhancing the visual experience by improving brightness, contrast, and detail richness. However, current multi-exposure correction methods often encounter challenges in addressing intra-class variability caused by diverse lighting conditions, shooting environments, and weather factors, particularly when processing images captured at a single exposure level. To enhance the adaptability of these models under complex imaging conditions, this paper proposes a Wavelet-based Exposure Correction method with Degradation Guidance (WEC-DG). Specifically, we introduce a degradation descriptor within the Exposure Consistency Alignment Module (ECAM) at both ends of the processing pipeline to ensure exposure consistency and achieve final alignment. This mechanism effectively addresses miscorrected exposure anomalies caused by existing methods' failure to recognize 'blurred' exposure degradation. Additionally, we investigate the light-detail decoupling properties of the wavelet transform to design the Exposure Restoration and Detail Reconstruction Module (EDRM), which processes low-frequency information related to exposure enhancement before utilizing high-frequency information as a prior guide for reconstructing spatial domain details. This serial processing strategy guarantees precise light correction and enhances detail recovery. Extensive experiments conducted on multiple public datasets demonstrate that the proposed method outperforms existing algorithms, achieving significant performance improvements and validating its effectiveness and practical applicability.

</details>


### [14] [SVG-Head: Hybrid Surface-Volumetric Gaussians for High-Fidelity Head Reconstruction and Real-Time Editing](https://arxiv.org/abs/2508.09597)
*Heyi Sun,Cong Wang,Tian-Xing Xu,Jingwei Huang,Di Kang,Chunchao Guo,Song-Hai Zhang*

Main category: cs.CV

TL;DR: SVG-Head提出了一种混合表示方法，结合3D高斯和FLAME网格，实现了高保真、可实时编辑的头像建模。


<details>
  <summary>Details</summary>
Motivation: 解决头像建模中几何与全局外观的隐式表示和纠缠问题，提升AR/VR应用中的实时编辑能力。

Method: 使用表面高斯和体积高斯分别建模外观和非朗伯区域，结合网格感知的UV映射和分层优化策略。

Result: 在NeRSemble数据集上实现了高保真渲染，并首次支持实时外观编辑。

Conclusion: SVG-Head为头像建模提供了高保真和实时编辑的解决方案。

Abstract: Creating high-fidelity and editable head avatars is a pivotal challenge in computer vision and graphics, boosting many AR/VR applications. While recent advancements have achieved photorealistic renderings and plausible animation, head editing, especially real-time appearance editing, remains challenging due to the implicit representation and entangled modeling of the geometry and global appearance. To address this, we propose Surface-Volumetric Gaussian Head Avatar (SVG-Head), a novel hybrid representation that explicitly models the geometry with 3D Gaussians bound on a FLAME mesh and leverages disentangled texture images to capture the global appearance. Technically, it contains two types of Gaussians, in which surface Gaussians explicitly model the appearance of head avatars using learnable texture images, facilitating real-time texture editing, while volumetric Gaussians enhance the reconstruction quality of non-Lambertian regions (e.g., lips and hair). To model the correspondence between 3D world and texture space, we provide a mesh-aware Gaussian UV mapping method, which leverages UV coordinates given by the FLAME mesh to obtain sharp texture images and real-time rendering speed. A hierarchical optimization strategy is further designed to pursue the optimal performance in both reconstruction quality and editing flexibility. Experiments on the NeRSemble dataset show that SVG-Head not only generates high-fidelity rendering results, but also is the first method to obtain explicit texture images for Gaussian head avatars and support real-time appearance editing.

</details>


### [15] [Images Speak Louder Than Scores: Failure Mode Escape for Enhancing Generative Quality](https://arxiv.org/abs/2508.09598)
*Jie Shao,Ke Zhu,Minghao Fu,Guo-hua Wang,Jianxin Wu*

Main category: cs.CV

TL;DR: FaME是一种无需训练、高效推理的方法，通过图像质量评估模型识别低质量生成样本，并利用其采样轨迹作为负向引导，提升生成图像的感知质量。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在类到图像生成方面取得了显著进展，但现有模型在某些类别中仍会生成失真或低质量的图像，而FID等指标无法充分评估单个样本的感知质量。

Method: 提出FaME方法，利用图像质量评估模型识别低质量生成样本，并存储其采样轨迹作为负向引导，避免未来采样进入低质量区域。

Result: 在ImageNet上的实验表明，FaME在不影响FID的情况下，显著提升了生成图像的视觉质量。

Conclusion: FaME是一种有效的训练无关方法，能够提升扩散模型的感知质量，并有望扩展到文本到图像生成任务。

Abstract: Diffusion models have achieved remarkable progress in class-to-image generation. However, we observe that despite impressive FID scores, state-of-the-art models often generate distorted or low-quality images, especially in certain classes. This gap arises because FID evaluates global distribution alignment, while ignoring the perceptual quality of individual samples. We further examine the role of CFG, a common technique used to enhance generation quality. While effective in improving metrics and suppressing outliers, CFG can introduce distribution shift and visual artifacts due to its misalignment with both training objectives and user expectations. In this work, we propose FaME, a training-free and inference-efficient method for improving perceptual quality. FaME uses an image quality assessment model to identify low-quality generations and stores their sampling trajectories. These failure modes are then used as negative guidance to steer future sampling away from poor-quality regions. Experiments on ImageNet demonstrate that FaME brings consistent improvements in visual quality without compromising FID. FaME also shows the potential to be extended to improve text-to-image generation.

</details>


### [16] [MInDI-3D: Iterative Deep Learning in 3D for Sparse-view Cone Beam Computed Tomography](https://arxiv.org/abs/2508.09616)
*Daniel Barco,Marc Stadelmann,Martin Oswald,Ivo Herzig,Lukas Lichtensteiger,Pascal Paysan,Igor Peterlik,Michal Walczak,Bjoern Menze,Frank-Peter Schilling*

Main category: cs.CV

TL;DR: MInDI-3D是一种基于3D条件扩散的模型，用于稀疏视图CBCT伪影去除，显著降低辐射暴露。


<details>
  <summary>Details</summary>
Motivation: 减少医学成像中的辐射暴露，同时提升稀疏视图CBCT图像质量。

Method: 将2D InDI扩展到3D，通过迭代去噪直接从稀疏视图输入优化CBCT体积，并使用大型伪CBCT数据集训练模型。

Result: 在CT-RATE测试集上，PSNR提升12.96 dB，辐射暴露减少8倍，且与3D U-Net在真实扫描中表现相当。

Conclusion: MInDI-3D在临床评估中表现优异，适用于多种解剖部位，并能泛化到新的CBCT扫描仪几何结构。

Abstract: We present MInDI-3D (Medical Inversion by Direct Iteration in 3D), the first 3D conditional diffusion-based model for real-world sparse-view Cone Beam Computed Tomography (CBCT) artefact removal, aiming to reduce imaging radiation exposure. A key contribution is extending the "InDI" concept from 2D to a full 3D volumetric approach for medical images, implementing an iterative denoising process that refines the CBCT volume directly from sparse-view input. A further contribution is the generation of a large pseudo-CBCT dataset (16,182) from chest CT volumes of the CT-RATE public dataset to robustly train MInDI-3D. We performed a comprehensive evaluation, including quantitative metrics, scalability analysis, generalisation tests, and a clinical assessment by 11 clinicians. Our results show MInDI-3D's effectiveness, achieving a 12.96 (6.10) dB PSNR gain over uncorrected scans with only 50 projections on the CT-RATE pseudo-CBCT (independent real-world) test set and enabling an 8x reduction in imaging radiation exposure. We demonstrate its scalability by showing that performance improves with more training data. Importantly, MInDI-3D matches the performance of a 3D U-Net on real-world scans from 16 cancer patients across distortion and task-based metrics. It also generalises to new CBCT scanner geometries. Clinicians rated our model as sufficient for patient positioning across all anatomical sites and found it preserved lung tumour boundaries well.

</details>


### [17] [Preacher: Paper-to-Video Agentic System](https://arxiv.org/abs/2508.09632)
*Jingwei Liu,Ling Yang,Hao Luo,Fan Wang Hongyan Li,Mengdi Wang*

Main category: cs.CV

TL;DR: 论文提出Preacher系统，将研究论文转化为结构化视频摘要，解决现有视频生成模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型受限于上下文窗口、视频时长、风格多样性和领域知识表示不足，需改进。

Method: 采用自上而下分解、总结和重构论文，再自下而上生成视频，结合P-CoT进行迭代规划。

Result: 在五个研究领域成功生成高质量视频摘要，表现优于现有模型。

Conclusion: Preacher系统展示了跨模态表示对齐的潜力，代码将开源。

Abstract: The paper-to-video task converts a research paper into a structured video abstract, distilling key concepts, methods, and conclusions into an accessible, well-organized format. While state-of-the-art video generation models demonstrate potential, they are constrained by limited context windows, rigid video duration constraints, limited stylistic diversity, and an inability to represent domain-specific knowledge. To address these limitations, we introduce Preacher, the first paper-to-video agentic system. Preacher employs a top-down approach to decompose, summarize, and reformulate the paper, followed by bottom-up video generation, synthesizing diverse video segments into a coherent abstract. To align cross-modal representations, we define key scenes and introduce a Progressive Chain of Thought (P-CoT) for granular, iterative planning. Preacher successfully generates high-quality video abstracts across five research fields, demonstrating expertise beyond current video generation models. Code will be released at: https://github.com/GenVerse/Paper2Video

</details>


### [18] [GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors](https://arxiv.org/abs/2508.09667)
*Xingyilang Yin,Qi Zhang,Jiahao Chang,Ying Feng,Qingnan Fan,Xi Yang,Chi-Man Pun,Huaqi Zhang,Xiaodong Cun*

Main category: cs.CV

TL;DR: GSFixer是一个新框架，通过参考引导的视频修复模型提升稀疏视图下3D高斯溅射（3DGS）重建的质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图下3DGS重建因信息不足导致明显伪影，现有方法难以生成与输入一致的补全内容。

Method: 基于DiT的视频扩散模型，结合2D语义和3D几何特征，增强修复视图的语义一致性和3D一致性。

Result: 实验表明GSFixer在3DGS伪影修复和稀疏视图3D重建上优于现有方法。

Conclusion: GSFixer通过多特征整合显著提升了稀疏视图3DGS重建的质量。

Abstract: Reconstructing 3D scenes using 3D Gaussian Splatting (3DGS) from sparse views is an ill-posed problem due to insufficient information, often resulting in noticeable artifacts. While recent approaches have sought to leverage generative priors to complete information for under-constrained regions, they struggle to generate content that remains consistent with input observations. To address this challenge, we propose GSFixer, a novel framework designed to improve the quality of 3DGS representations reconstructed from sparse inputs. The core of our approach is the reference-guided video restoration model, built upon a DiT-based video diffusion model trained on paired artifact 3DGS renders and clean frames with additional reference-based conditions. Considering the input sparse views as references, our model integrates both 2D semantic features and 3D geometric features of reference views extracted from the visual geometry foundation model, enhancing the semantic coherence and 3D consistency when fixing artifact novel views. Furthermore, considering the lack of suitable benchmarks for 3DGS artifact restoration evaluation, we present DL3DV-Res which contains artifact frames rendered using low-quality 3DGS. Extensive experiments demonstrate our GSFixer outperforms current state-of-the-art methods in 3DGS artifact restoration and sparse-view 3D reconstruction. Project page: https://github.com/GVCLab/GSFixer.

</details>


### [19] [Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in surgical vision](https://arxiv.org/abs/2508.09681)
*Gerardo Loza,Junlei Hu,Dominic Jones,Sharib Ali,Pietro Valdastri*

Main category: cs.CV

TL;DR: 提出了一种基于NeRF架构的新型测试时优化（TTO）方法，用于长期3D点跟踪，显著提升了2D和3D跟踪的精度和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前点跟踪方法在长期一致性或3D运动方面表现不佳，需要一种能够结合多方法优势并适应手术场景的解决方案。

Method: 采用可逆神经辐射场（InvNeRF）架构，结合多尺度HexPlanes和高效像素采样算法，通过渲染监督实现双向可变形-规范映射。

Result: 在STIR和SCARE数据集上，2D点跟踪的平均精度提升近50%，3D跟踪首次超越前馈方法。

Conclusion: 该方法在2D和3D点跟踪中均表现出色，为手术场景提供了高效的解决方案。

Abstract: We proposed a novel test-time optimisation (TTO) approach framed by a NeRF-based architecture for long-term 3D point tracking. Most current methods in point tracking struggle to obtain consistent motion or are limited to 2D motion. TTO approaches frame the solution for long-term tracking as optimising a function that aggregates correspondences from other specialised state-of-the-art methods. Unlike the state-of-the-art on TTO, we propose parametrising such a function with our new invertible Neural Radiance Field (InvNeRF) architecture to perform both 2D and 3D tracking in surgical scenarios. Our approach allows us to exploit the advantages of a rendering-based approach by supervising the reprojection of pixel correspondences. It adapts strategies from recent rendering-based methods to obtain a bidirectional deformable-canonical mapping, to efficiently handle a defined workspace, and to guide the rays' density. It also presents our multi-scale HexPlanes for fast inference and a new algorithm for efficient pixel sampling and convergence criteria. We present results in the STIR and SCARE datasets, for evaluating point tracking and testing the integration of kinematic data in our pipeline, respectively. In 2D point tracking, our approach surpasses the precision and accuracy of the TTO state-of-the-art methods by nearly 50% on average precision, while competing with other approaches. In 3D point tracking, this is the first TTO approach, surpassing feed-forward methods while incorporating the benefits of a deformable NeRF-based reconstruction.

</details>


### [20] [Region-to-Region: Enhancing Generative Image Harmonization with Adaptive Regional Injection](https://arxiv.org/abs/2508.09746)
*Zhiqiu Zhang,Dongqi Fan,Mingjie Wang,Qiang Tang,Jian Yang,Zili Yi*

Main category: cs.CV

TL;DR: 论文提出了一种基于区域到区域变换的图像和谐化方法R2R，通过Clear-VAE和Harmony Controller提升细节保留与和谐化能力，并构建了新的合成数据集RPHarmony。


<details>
  <summary>Details</summary>
Motivation: 现有LDM和谐化方法在细节保留和复杂光照条件下表现不足，且合成数据集缺乏多样性。

Method: 提出Region-to-Region变换，结合Clear-VAE和MACA控制器，并采用Random Poisson Blending生成多样化数据集。

Result: 实验表明R2R在定量指标和视觉和谐性上优于其他方法，且新数据集提升了模型在真实场景中的表现。

Conclusion: R2R方法有效解决了现有和谐化技术的局限性，并公开了代码、数据集和模型权重。

Abstract: The goal of image harmonization is to adjust the foreground in a composite image to achieve visual consistency with the background. Recently, latent diffusion model (LDM) are applied for harmonization, achieving remarkable results. However, LDM-based harmonization faces challenges in detail preservation and limited harmonization ability. Additionally, current synthetic datasets rely on color transfer, which lacks local variations and fails to capture complex real-world lighting conditions. To enhance harmonization capabilities, we propose the Region-to-Region transformation. By injecting information from appropriate regions into the foreground, this approach preserves original details while achieving image harmonization or, conversely, generating new composite data. From this perspective, We propose a novel model R2R. Specifically, we design Clear-VAE to preserve high-frequency details in the foreground using Adaptive Filter while eliminating disharmonious elements. To further enhance harmonization, we introduce the Harmony Controller with Mask-aware Adaptive Channel Attention (MACA), which dynamically adjusts the foreground based on the channel importance of both foreground and background regions. To address the limitation of existing datasets, we propose Random Poisson Blending, which transfers color and lighting information from a suitable region to the foreground, thereby generating more diverse and challenging synthetic images. Using this method, we construct a new synthetic dataset, RPHarmony. Experiments demonstrate the superiority of our method over other methods in both quantitative metrics and visual harmony. Moreover, our dataset helps the model generate more realistic images in real examples. Our code, dataset, and model weights have all been released for open access.

</details>


### [21] [MUJICA: Reforming SISR Models for PBR Material Super-Resolution via Cross-Map Attention](https://arxiv.org/abs/2508.09802)
*Xin Du,Maoyuan Xu,Zhi Ying*

Main category: cs.CV

TL;DR: MUJICA是一种基于跨图注意力的多模态上采样方法，用于提升PBR材料的超分辨率性能，解决了现有SISR方法的跨图不一致性和泛化能力不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有SISR方法在PBR材料超分辨率中存在跨图不一致、模态特征建模不足和数据分布偏移导致的泛化能力有限问题。

Method: 提出MUJICA，一种灵活的适配器，基于预训练的Swin-transformer SISR模型，通过跨图注意力融合特征，保持预训练模型的优秀重建能力。

Result: MUJICA在SwinIR、DRCT和HMANet等SISR模型上提升了PSNR、SSIM和LPIPS分数，同时保持了跨图一致性。

Conclusion: MUJICA在有限资源下高效训练，并在PBR材料数据集上实现了最先进的性能。

Abstract: Physically Based Rendering (PBR) materials are typically characterized by multiple 2D texture maps such as basecolor, normal, metallic, and roughness which encode spatially-varying bi-directional reflectance distribution function (SVBRDF) parameters to model surface reflectance properties and microfacet interactions. Upscaling SVBRDF material is valuable for modern 3D graphics applications. However, existing Single Image Super-Resolution (SISR) methods struggle with cross-map inconsistency, inadequate modeling of modality-specific features, and limited generalization due to data distribution shifts. In this work, we propose Multi-modal Upscaling Joint Inference via Cross-map Attention (MUJICA), a flexible adapter that reforms pre-trained Swin-transformer-based SISR models for PBR material super-resolution. MUJICA is seamlessly attached after the pre-trained and frozen SISR backbone. It leverages cross-map attention to fuse features while preserving remarkable reconstruction ability of the pre-trained SISR model. Applied to SISR models such as SwinIR, DRCT, and HMANet, MUJICA improves PSNR, SSIM, and LPIPS scores while preserving cross-map consistency. Experiments demonstrate that MUJICA enables efficient training even with limited resources and delivers state-of-the-art performance on PBR material datasets.

</details>


### [22] [Physical Autoregressive Model for Robotic Manipulation without Action Pretraining](https://arxiv.org/abs/2508.09822)
*Zijian Song,Sihan Qin,Tianshui Chen,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: 本文提出了一种物理自回归模型（PAR），利用预训练视频生成模型解决机器人操作数据稀缺问题，通过结合帧和动作的物理令牌表示机器人与环境的联合演化，实现了高精度的视频预测和一致的动作轨迹。


<details>
  <summary>Details</summary>
Motivation: 机器人操作数据稀缺，促使研究者利用其他模态的预训练大模型。本文旨在通过视频预训练中的世界知识理解物理动力学，无需动作预训练。

Method: 提出PAR模型，结合帧和动作的物理令牌，采用DiT基的去令牌器建模连续令牌，减少量化误差。引入因果掩码、逆运动学、并行训练和KV缓存机制提升性能。

Result: 在ManiSkill基准测试中，PAR在PushCube任务上达到100%成功率，与其他任务的动作预训练基线性能相当，并能准确预测未来视频和动作轨迹。

Conclusion: PAR展示了通过视频预训练迁移世界知识在机器人操作中的潜力，为未来研究提供了新方向。

Abstract: The scarcity of manipulation data has motivated the use of pretrained large models from other modalities in robotics. In this work, we build upon autoregressive video generation models to propose a Physical Autoregressive Model (PAR), where physical tokens combine frames and actions to represent the joint evolution of the robot and its environment. PAR leverages the world knowledge embedded in video pretraining to understand physical dynamics without requiring action pretraining, enabling accurate video prediction and consistent action trajectories. It also adopts a DiT-based de-tokenizer to model frames and actions as continuous tokens, mitigating quantization errors and facilitating mutual enhancement. Furthermore, we incorporate a causal mask with inverse kinematics, parallel training, and the KV-cache mechanism to further improve performance and efficiency. Experiments on the ManiSkill benchmark show that PAR achieves a 100\% success rate on the PushCube task, matches the performance of action-pretrained baselines on other tasks, and accurately predicts future videos with tightly aligned action trajectories. These findings underscore a promising direction for robotic manipulation by transferring world knowledge from autoregressive video pretraining.

</details>


### [23] [KonfAI: A Modular and Fully Configurable Framework for Deep Learning in Medical Imaging](https://arxiv.org/abs/2508.09823)
*Valentin Boussot,Jean-Louis Dillenseger*

Main category: cs.CV

TL;DR: KonfAI是一个模块化、可扩展且完全可配置的深度学习框架，专为医学影像任务设计，通过YAML配置文件实现工作流定义，无需修改代码。


<details>
  <summary>Details</summary>
Motivation: 提高医学影像任务的复现性、透明性和实验可追溯性，同时减少开发时间。

Method: 使用YAML配置文件定义训练、推理和评估工作流，支持高级策略如基于补丁的学习、测试时增强、模型集成和深度监督。

Result: 成功应用于分割、配准和图像合成任务，并在多个国际医学影像挑战中取得领先成绩。

Conclusion: KonfAI是一个高效、灵活的框架，适用于复杂的医学影像任务，且开源可用。

Abstract: KonfAI is a modular, extensible, and fully configurable deep learning framework specifically designed for medical imaging tasks. It enables users to define complete training, inference, and evaluation workflows through structured YAML configuration files, without modifying the underlying code. This declarative approach enhances reproducibility, transparency, and experimental traceability while reducing development time. Beyond the capabilities of standard pipelines, KonfAI provides native abstractions for advanced strategies including patch-based learning, test-time augmentation, model ensembling, and direct access to intermediate feature representations for deep supervision. It also supports complex multi-model training setups such as generative adversarial architectures. Thanks to its modular and extensible architecture, KonfAI can easily accommodate custom models, loss functions, and data processing components. The framework has been successfully applied to segmentation, registration, and image synthesis tasks, and has contributed to top-ranking results in several international medical imaging challenges. KonfAI is open source and available at \href{https://github.com/vboussot/KonfAI}{https://github.com/vboussot/KonfAI}.

</details>


### [24] [Reverse Convolution and Its Applications to Image Restoration](https://arxiv.org/abs/2508.09824)
*Xuhong Huang,Shiqi Liu,Kai Zhang,Ying Tai,Jian Yang,Hui Zeng,Lei Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种新颖的深度反向卷积算子，用于有效反转深度卷积，并通过构建反向卷积块改进现有架构。


<details>
  <summary>Details</summary>
Motivation: 由于转置卷积并非卷积的真正逆运算，目前神经网络中缺乏标准的反向卷积算子，因此需要开发一种有效的反向卷积方法。

Method: 通过正则化最小二乘优化问题设计和实现深度反向卷积算子，并结合层归一化、1×1卷积和GELU激活构建反向卷积块。

Result: 实验表明，提出的反向卷积算子作为基础模块在图像去噪、超分辨率和去模糊任务中表现有效。

Conclusion: 该研究为深度学习模型设计中的新算子开发提供了可能，并展示了其实际应用潜力。

Abstract: Convolution and transposed convolution are fundamental operators widely used in neural networks. However, transposed convolution (a.k.a. deconvolution) does not serve as a true inverse of convolution due to inherent differences in their mathematical formulations. To date, no reverse convolution operator has been established as a standard component in neural architectures. In this paper, we propose a novel depthwise reverse convolution operator as an initial attempt to effectively reverse depthwise convolution by formulating and solving a regularized least-squares optimization problem. We thoroughly investigate its kernel initialization, padding strategies, and other critical aspects to ensure its effective implementation. Building upon this operator, we further construct a reverse convolution block by combining it with layer normalization, 1$\times$1 convolution, and GELU activation, forming a Transformer-like structure. The proposed operator and block can directly replace conventional convolution and transposed convolution layers in existing architectures, leading to the development of ConverseNet. Corresponding to typical image restoration models such as DnCNN, SRResNet and USRNet, we train three variants of ConverseNet for Gaussian denoising, super-resolution and deblurring, respectively. Extensive experiments demonstrate the effectiveness of the proposed reverse convolution operator as a basic building module. We hope this work could pave the way for developing new operators in deep model design and applications.

</details>


### [25] [Enhancing Diffusion Face Generation with Contrastive Embeddings and SegFormer Guidance](https://arxiv.org/abs/2508.09847)
*Dhruvraj Singh Rawat,Enggen Sherpa,Rishikesan Kirupanantha,Tin Hoang*

Main category: cs.CV

TL;DR: 论文提出了一种在小规模CelebAMask-HQ数据集上评估扩散模型生成人脸的基准，比较了UNet和DiT架构的无条件生成，并探索了基于LoRA的预训练Stable Diffusion模型微调。通过改进属性嵌入和分割编码，提升了语义对齐和可控性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估扩散模型在小规模数据集上的表现，并探索如何通过改进属性嵌入和分割编码提升生成人脸的可控性和语义对齐。

Method: 比较了UNet和DiT架构的无条件生成，采用LoRA微调预训练模型，并引入InfoNCE损失和SegFormer分割编码器。

Result: 结果表明，对比嵌入学习和先进分割编码在小规模数据下能有效提升可控人脸生成的效果。

Conclusion: 通过改进属性嵌入和分割编码，论文在小规模数据集上实现了更优的语义对齐和可控性，为受限数据环境下的可控生成提供了有效方法。

Abstract: We present a benchmark of diffusion models for human face generation on a small-scale CelebAMask-HQ dataset, evaluating both unconditional and conditional pipelines. Our study compares UNet and DiT architectures for unconditional generation and explores LoRA-based fine-tuning of pretrained Stable Diffusion models as a separate experiment. Building on the multi-conditioning approach of Giambi and Lisanti, which uses both attribute vectors and segmentation masks, our main contribution is the integration of an InfoNCE loss for attribute embedding and the adoption of a SegFormer-based segmentation encoder. These enhancements improve the semantic alignment and controllability of attribute-guided synthesis. Our results highlight the effectiveness of contrastive embedding learning and advanced segmentation encoding for controlled face generation in limited data settings.

</details>


### [26] [OneVAE: Joint Discrete and Continuous Optimization Helps Discrete Video VAE Train Better](https://arxiv.org/abs/2508.09857)
*Yupeng Zhou,Zhen Li,Ziheng Ouyang,Yuming Chen,Ruoyi Du,Daquan Zhou,Bin Fu,Yihao Liu,Peng Gao,Ming-Ming Cheng,Qibin Hou*

Main category: cs.CV

TL;DR: 论文提出了一种名为OneVAE的方法，通过结合连续和离散视频VAE的优势，提升了离散视频VAE的训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 离散视频VAE在训练稳定性、时长和重建质量上存在问题，而连续VAE表现更优。论文旨在通过结合两者优势改进离散VAE。

Method: 利用FSQ保留连续VAE的先验知识，提出多令牌量化机制和强化首帧重建，并设计联合离散-连续优化方案。

Result: 方法收敛速度更快，性能更优，PSNR提升近1 dB，且在单网络中同时支持连续和离散表示。

Conclusion: OneVAE成功结合了连续和离散VAE的优势，为多模态LLM提供了高效的视频表示方法。

Abstract: Encoding videos into discrete tokens could align with text tokens to facilitate concise and unified multi-modal LLMs, yet introducing significant spatiotemporal compression compared to continuous video representation. Previous discrete video VAEs experienced unstable training, long training time, and degraded reconstruction quality. Given the easier training and superior performance of continuous VAEs, an intuitive idea is to enhance discrete video VAEs by leveraging continuous VAEs. After rethinking the intrinsic link between discrete and continuous representations, we found that FSQ could effectively preserve pre-trained continuous VAE priors compared to other quantization methods. By leveraging continuous VAE priors, it converges several times faster than training from scratch and achieves superior performance at convergence. Meanwhile, two structural improvements are proposed. First, inspired by how continuous VAEs enhance reconstruction via enlarged latent dimensions, we introduce a multi-token quantization mechanism, which achieves nearly a 1 dB improvement in PSNR without compromising the token compression ratio. Second, to tackle reconstruction challenges in high-compression video VAEs, we strengthen first-frame reconstruction, enabling the causal VAE to leverage this information in subsequent frames and markedly improving the performance of 4 x 16 x 16 discrete VAEs. Furthermore, we propose a joint discrete-continuous optimization scheme that unifies the two paradigms and, for the first time, achieves competitive performance on both continuous and discrete representations within a single network. We name our method OneVAE to reflect this connection.

</details>


### [27] [HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics](https://arxiv.org/abs/2508.09858)
*Weiqi Li,Zehao Zhang,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: HumanGenesis框架通过四个协作代理解决合成人类动态中的几何不一致和运动泛化问题，实现了高质量的3D重建和视频合成。


<details>
  <summary>Details</summary>
Motivation: 当前方法在3D建模和细节保留上存在几何不一致和粗糙重建问题，且生成能力有限导致运动泛化和场景不协调。

Method: HumanGenesis整合几何和生成建模，包括Reconstructor（3D高斯溅射）、Critique Agent（多轮MLLM反射）、Pose Guider（时间感知参数编码器）和Video Harmonizer（混合渲染）。

Result: 在文本引导合成、视频重演和新姿势泛化任务中达到最先进性能，显著提升表现力、几何保真度和场景整合。

Conclusion: HumanGenesis通过多代理协作有效解决了现有挑战，为合成人类动态提供了高质量解决方案。

Abstract: \textbf{Synthetic human dynamics} aims to generate photorealistic videos of human subjects performing expressive, intention-driven motions. However, current approaches face two core challenges: (1) \emph{geometric inconsistency} and \emph{coarse reconstruction}, due to limited 3D modeling and detail preservation; and (2) \emph{motion generalization limitations} and \emph{scene inharmonization}, stemming from weak generative capabilities. To address these, we present \textbf{HumanGenesis}, a framework that integrates geometric and generative modeling through four collaborative agents: (1) \textbf{Reconstructor} builds 3D-consistent human-scene representations from monocular video using 3D Gaussian Splatting and deformation decomposition. (2) \textbf{Critique Agent} enhances reconstruction fidelity by identifying and refining poor regions via multi-round MLLM-based reflection. (3) \textbf{Pose Guider} enables motion generalization by generating expressive pose sequences using time-aware parametric encoders. (4) \textbf{Video Harmonizer} synthesizes photorealistic, coherent video via a hybrid rendering pipeline with diffusion, refining the Reconstructor through a Back-to-4D feedback loop. HumanGenesis achieves state-of-the-art performance on tasks including text-guided synthesis, video reenactment, and novel-pose generalization, significantly improving expressiveness, geometric fidelity, and scene integration.

</details>


### [28] [E-4DGS: High-Fidelity Dynamic Reconstruction from the Multi-view Event Cameras](https://arxiv.org/abs/2508.09912)
*Chaoran Feng,Zhenyu Tang,Wangbo Yu,Yatian Pang,Yian Zhao,Jianbin Zhao,Li Yuan,Yonghong Tian*

Main category: cs.CV

TL;DR: 论文提出E-4DGS，一种基于事件相机的动态高斯泼溅方法，用于多视角事件流的新视角合成，解决了高速运动和低光场景下的重建问题。


<details>
  <summary>Details</summary>
Motivation: 传统RGB相机在光照不足、运动模糊和动态范围有限等方面存在局限性，事件相机因其低功耗、高时间分辨率和高动态范围特性，为高速运动和低光场景的重建提供了新思路。

Method: 提出事件驱动的初始化方案和事件自适应切片泼溅技术，结合强度重要性剪枝和自适应对比度阈值优化，实现时间感知重建。

Result: E-4DGS在合成多视角事件流数据集上表现优于纯事件和事件-RGB融合基线方法。

Conclusion: 该方法为多视角事件流重建提供了新途径，推动了快速场景捕捉的探索。

Abstract: Novel view synthesis and 4D reconstruction techniques predominantly rely on RGB cameras, thereby inheriting inherent limitations such as the dependence on adequate lighting, susceptibility to motion blur, and a limited dynamic range. Event cameras, offering advantages of low power, high temporal resolution and high dynamic range, have brought a new perspective to addressing the scene reconstruction challenges in high-speed motion and low-light scenes. To this end, we propose E-4DGS, the first event-driven dynamic Gaussian Splatting approach, for novel view synthesis from multi-view event streams with fast-moving cameras. Specifically, we introduce an event-based initialization scheme to ensure stable training and propose event-adaptive slicing splatting for time-aware reconstruction. Additionally, we employ intensity importance pruning to eliminate floating artifacts and enhance 3D consistency, while incorporating an adaptive contrast threshold for more precise optimization. We design a synthetic multi-view camera setup with six moving event cameras surrounding the object in a 360-degree configuration and provide a benchmark multi-view event stream dataset that captures challenging motion scenarios. Our approach outperforms both event-only and event-RGB fusion baselines and paves the way for the exploration of multi-view event-based reconstruction as a novel approach for rapid scene capture.

</details>


### [29] [AST-n: A Fast Sampling Approach for Low-Dose CT Reconstruction using Diffusion Models](https://arxiv.org/abs/2508.09943)
*Tomás de la Sotta,José M. Saavedra,Héctor Henríquez,Violeta Chang,Aline Xavier*

Main category: cs.CV

TL;DR: AST-n框架通过从中间噪声水平启动反向扩散并结合高阶ODE求解器，显著加速低剂量CT（LDCT）去噪推理，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT（LDCT）协议减少辐射但增加图像噪声，影响诊断信心。扩散生成模型通过学习图像先验和迭代优化，为LDCT去噪提供潜力。

Method: 提出AST-n框架，从中间噪声水平启动反向扩散，并集成高阶ODE求解器以减少采样步骤。评估两种加速范式：AST-n采样和标准调度结合高阶求解器。

Result: AST-25（仅25步）在峰值信噪比（PSNR）和结构相似性指数（SSIM）上接近标准基线，同时将推理时间从16秒缩短至1秒以下。无条件采样质量显著下降。

Conclusion: AST-n结合高阶采样器可实现快速LDCT重建，无明显图像质量损失，推动扩散方法在临床工作流程中的可行性。

Abstract: Low-dose CT (LDCT) protocols reduce radiation exposure but increase image noise, compromising diagnostic confidence. Diffusion-based generative models have shown promise for LDCT denoising by learning image priors and performing iterative refinement. In this work, we introduce AST-n, an accelerated inference framework that initiates reverse diffusion from intermediate noise levels, and integrate high-order ODE solvers within conditioned models to further reduce sampling steps. We evaluate two acceleration paradigms--AST-n sampling and standard scheduling with high-order solvers -- on the Low Dose CT Grand Challenge dataset, covering head, abdominal, and chest scans at 10-25 % of standard dose. Conditioned models using only 25 steps (AST-25) achieve peak signal-to-noise ratio (PSNR) above 38 dB and structural similarity index (SSIM) above 0.95, closely matching standard baselines while cutting inference time from ~16 seg to under 1 seg per slice. Unconditional sampling suffers substantial quality loss, underscoring the necessity of conditioning. We also assess DDIM inversion, which yields marginal PSNR gains at the cost of doubling inference time, limiting its clinical practicality. Our results demonstrate that AST-n with high-order samplers enables rapid LDCT reconstruction without significant loss of image fidelity, advancing the feasibility of diffusion-based methods in clinical workflows.

</details>


### [30] [Stable Diffusion Models are Secretly Good at Visual In-Context Learning](https://arxiv.org/abs/2508.09949)
*Trevine Oorloff,Vishwanath Sindagi,Wele Gedara Chaminda Bandara,Ali Shafahi,Amin Ghiasi,Charan Prakash,Reza Ardekani*

Main category: cs.CV

TL;DR: 本文提出了一种利用现成的Stable Diffusion模型进行视觉上下文学习（V-ICL）的方法，无需额外微调即可适应多种视觉任务。


<details>
  <summary>Details</summary>
Motivation: 探索如何简化视觉上下文学习过程，避免复杂的训练和额外数据需求，提高通用性。

Method: 在Stable Diffusion的自注意力层中重新计算注意力，显式结合查询和示例提示的上下文。

Result: 该方法在六个视觉任务上表现优异，例如在Pascal-5i数据集上，前景分割任务的mIoU分别比Visual Prompting和IMProv提高了8.9%和3.2%。

Conclusion: 研究表明，现成的Stable Diffusion模型可通过简单的注意力重计算实现高效的视觉上下文学习，且能通过集成进一步提升性能。

Abstract: Large language models (LLM) in natural language processing (NLP) have demonstrated great potential for in-context learning (ICL) -- the ability to leverage a few sets of example prompts to adapt to various tasks without having to explicitly update the model weights. ICL has recently been explored for computer vision tasks with promising early outcomes. These approaches involve specialized training and/or additional data that complicate the process and limit its generalizability. In this work, we show that off-the-shelf Stable Diffusion models can be repurposed for visual in-context learning (V-ICL). Specifically, we formulate an in-place attention re-computation within the self-attention layers of the Stable Diffusion architecture that explicitly incorporates context between the query and example prompts. Without any additional fine-tuning, we show that this repurposed Stable Diffusion model is able to adapt to six different tasks: foreground segmentation, single object detection, semantic segmentation, keypoint detection, edge detection, and colorization. For example, the proposed approach improves the mean intersection over union (mIoU) for the foreground segmentation task on Pascal-5i dataset by 8.9% and 3.2% over recent methods such as Visual Prompting and IMProv, respectively. Additionally, we show that the proposed method is able to effectively leverage multiple prompts through ensembling to infer the task better and further improve the performance.

</details>


### [31] [PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations from a Single Image](https://arxiv.org/abs/2508.09973)
*Geonhee Sim,Gyeongsik Moon*

Main category: cs.CV

TL;DR: PERSONA框架结合3D和扩散方法，从单张图像生成个性化3D人体化身，解决了身份保持和姿态变形问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D方法需要大量姿态丰富的视频，而扩散方法难以保持身份一致性。PERSONA旨在结合两者优势。

Method: 利用扩散方法从单张图像生成姿态丰富的视频，并通过平衡采样和几何加权优化优化3D化身。

Result: 实现了从单张图像生成高质量、身份一致的3D人体化身，支持姿态驱动变形。

Conclusion: PERSONA成功结合两种方法，解决了身份保持和姿态变形的挑战。

Abstract: Two major approaches exist for creating animatable human avatars. The first, a 3D-based approach, optimizes a NeRF- or 3DGS-based avatar from videos of a single person, achieving personalization through a disentangled identity representation. However, modeling pose-driven deformations, such as non-rigid cloth deformations, requires numerous pose-rich videos, which are costly and impractical to capture in daily life. The second, a diffusion-based approach, learns pose-driven deformations from large-scale in-the-wild videos but struggles with identity preservation and pose-dependent identity entanglement. We present PERSONA, a framework that combines the strengths of both approaches to obtain a personalized 3D human avatar with pose-driven deformations from a single image. PERSONA leverages a diffusion-based approach to generate pose-rich videos from the input image and optimizes a 3D avatar based on them. To ensure high authenticity and sharp renderings across diverse poses, we introduce balanced sampling and geometry-weighted optimization. Balanced sampling oversamples the input image to mitigate identity shifts in diffusion-generated training videos. Geometry-weighted optimization prioritizes geometry constraints over image loss, preserving rendering quality in diverse poses.

</details>


### [32] [A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation](https://arxiv.org/abs/2508.09977)
*Shuting He,Peilin Ji,Yitong Yang,Changshuo Wang,Jiayi Ji,Yinglin Wang,Henghui Ding*

Main category: cs.CV

TL;DR: 3D高斯泼溅（3DGS）作为NeRF的替代方案，提供高保真实时渲染，并支持多种下游应用。本文综述了3DGS在分割、编辑、生成等任务中的进展，总结了方法、数据集和评估协议。


<details>
  <summary>Details</summary>
Motivation: 探索3DGS在3D场景表示中的潜力，尤其是在几何和语义理解方面的应用。

Method: 综述了3DGS的应用，包括2D基础模型、NeRF方法的借鉴，以及分类讨论分割、编辑、生成等功能任务。

Result: 总结了代表性方法、监督策略和学习范式，并比较了公开基准上的性能。

Conclusion: 3DGS在多种应用中表现出色，未来研究可通过持续更新的资源库进一步推动发展。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a powerful alternative to Neural Radiance Fields (NeRF) for 3D scene representation, offering high-fidelity photorealistic rendering with real-time performance. Beyond novel view synthesis, the explicit and compact nature of 3DGS enables a wide range of downstream applications that require geometric and semantic understanding. This survey provides a comprehensive overview of recent progress in 3DGS applications. It first introduces 2D foundation models that support semantic understanding and control in 3DGS applications, followed by a review of NeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGS applications into segmentation, editing, generation, and other functional tasks. For each, we summarize representative methods, supervision strategies, and learning paradigms, highlighting shared design principles and emerging trends. Commonly used datasets and evaluation protocols are also summarized, along with comparative analyses of recent methods across public benchmarks. To support ongoing research and development, a continually updated repository of papers, code, and resources is maintained at https://github.com/heshuting555/Awesome-3DGS-Applications.

</details>


### [33] [Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation](https://arxiv.org/abs/2508.09987)
*Junyan Ye,Dongzhi Jiang,Zihao Wang,Leqi Zhu,Zhenghao Hu,Zilong Huang,Jun He,Zhiyuan Yan,Jinghua Yu,Hongsheng Li,Conghui He,Weijia Li*

Main category: cs.CV

TL;DR: 论文探讨了GPT-4o生成的合成图像数据在补充现实数据集不足和提供干净监督信号方面的优势，并提出了Echo-4o-Image数据集和两个新评估基准。


<details>
  <summary>Details</summary>
Motivation: 现实世界图像数据集存在稀有场景覆盖不足和噪声问题，而GPT-4o生成的合成数据可以弥补这些缺陷。

Method: 利用GPT-4o生成180K规模的合成数据集Echo-4o-Image，并基于此微调Bagel模型得到Echo-4o。同时提出GenEval++和Imagine-Bench两个新评估基准。

Result: Echo-4o在标准基准测试中表现优异，且Echo-4o-Image数据集对其他基础模型也有显著性能提升。

Conclusion: 合成图像数据在补充现实数据集和提升模型性能方面具有重要价值，未来可进一步探索其潜力。

Abstract: Recently, GPT-4o has garnered significant attention for its strong performance in image generation, yet open-source models still lag behind. Several studies have explored distilling image data from GPT-4o to enhance open-source models, achieving notable progress. However, a key question remains: given that real-world image datasets already constitute a natural source of high-quality data, why should we use GPT-4o-generated synthetic data? In this work, we identify two key advantages of synthetic images. First, they can complement rare scenarios in real-world datasets, such as surreal fantasy or multi-reference image generation, which frequently occur in user queries. Second, they provide clean and controllable supervision. Real-world data often contains complex background noise and inherent misalignment between text descriptions and image content, whereas synthetic images offer pure backgrounds and long-tailed supervision signals, facilitating more accurate text-to-image alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale synthetic dataset generated by GPT-4o, harnessing the power of synthetic image data to address blind spots in real-world coverage. Using this dataset, we fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o. In addition, we propose two new evaluation benchmarks for a more accurate and challenging assessment of image generation capabilities: GenEval++, which increases instruction complexity to mitigate score saturation, and Imagine-Bench, which focuses on evaluating both the understanding and generation of imaginative content. Echo-4o demonstrates strong performance across standard benchmarks. Moreover, applying Echo-4o-Image to other foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains across multiple metrics, highlighting the datasets strong transferability.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [34] [MoLAN: A Unified Modality-Aware Noise Dynamic Editing Framework for Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.09145)
*Xingle Xu,Yongkang Liu,Dexian Cai,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.LG

TL;DR: MoLAN是一个多模态感知的动态噪声编辑框架，通过分块处理和动态去噪强度分配，有效抑制噪声并保留关键信息。MoLAN+在此基础上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态情感分析中因无关或误导性视觉和听觉信息导致的问题，避免传统方法因整体处理而丢失关键信息。

Method: 提出MoLAN框架，将每个模态特征分块，根据噪声水平和语义相关性动态分配去噪强度。MoLAN+是基于此框架的新方法。

Result: 在五个模型和四个数据集上验证了MoLAN的广泛有效性，MoLAN+实现了最先进的性能。

Conclusion: MoLAN框架灵活统一，可集成到多种多模态模型中，显著提升了多模态情感分析的效果。

Abstract: Multimodal Sentiment Analysis aims to integrate information from various modalities, such as audio, visual, and text, to make complementary predictions. However, it often struggles with irrelevant or misleading visual and auditory information. Most existing approaches typically treat the entire modality information (e.g., a whole image, audio segment, or text paragraph) as an independent unit for feature enhancement or denoising. They often suppress the redundant and noise information at the risk of losing critical information. To address this challenge, we propose MoLAN, a unified ModaLity-aware noise dynAmic editiNg framework. Specifically, MoLAN performs modality-aware blocking by dividing the features of each modality into multiple blocks. Each block is then dynamically assigned a distinct denoising strength based on its noise level and semantic relevance, enabling fine-grained noise suppression while preserving essential multimodal information. Notably, MoLAN is a unified and flexible framework that can be seamlessly integrated into a wide range of multimodal models. Building upon this framework, we further introduce MoLAN+, a new multimodal sentiment analysis approach. Experiments across five models and four datasets demonstrate the broad effectiveness of the MoLAN framework. Extensive evaluations show that MoLAN+ achieves the state-of-the-art performance. The code is publicly available at https://github.com/betterfly123/MoLAN-Framework.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [35] [Generative Artificial Intelligence in Medical Imaging: Foundations, Progress, and Clinical Translation](https://arxiv.org/abs/2508.09177)
*Xuanru Zhou,Cheng Li,Shuqiang Wang,Ye Li,Tao Tan,Hairong Zheng,Shanshan Wang*

Main category: eess.IV

TL;DR: 本文综述了生成式AI在医学影像中的进展，包括GANs、VAEs、扩散模型等，并探讨了其在临床工作流中的应用、挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在医学影像中的潜力巨大，可解决数据稀缺、标准化和多模态整合等长期挑战。

Method: 系统分析了生成式AI在影像工作流各阶段的应用，并提出三层次评估框架（像素级、特征级、任务级）。

Result: 生成式AI在数据合成、图像增强、模态转换等方面表现优异，但仍面临泛化性、幻觉风险、隐私和监管等挑战。

Conclusion: 生成式AI与大型基础模型的结合有望推动下一代可扩展、可靠且临床集成的影像系统发展。

Abstract: Generative artificial intelligence (AI) is rapidly transforming medical imaging by enabling capabilities such as data synthesis, image enhancement, modality translation, and spatiotemporal modeling. This review presents a comprehensive and forward-looking synthesis of recent advances in generative modeling including generative adversarial networks (GANs), variational autoencoders (VAEs), diffusion models, and emerging multimodal foundation architectures and evaluates their expanding roles across the clinical imaging continuum. We systematically examine how generative AI contributes to key stages of the imaging workflow, from acquisition and reconstruction to cross-modality synthesis, diagnostic support, and treatment planning. Emphasis is placed on both retrospective and prospective clinical scenarios, where generative models help address longstanding challenges such as data scarcity, standardization, and integration across modalities. To promote rigorous benchmarking and translational readiness, we propose a three-tiered evaluation framework encompassing pixel-level fidelity, feature-level realism, and task-level clinical relevance. We also identify critical obstacles to real-world deployment, including generalization under domain shift, hallucination risk, data privacy concerns, and regulatory hurdles. Finally, we explore the convergence of generative AI with large-scale foundation models, highlighting how this synergy may enable the next generation of scalable, reliable, and clinically integrated imaging systems. By charting technical progress and translational pathways, this review aims to guide future research and foster interdisciplinary collaboration at the intersection of AI, medicine, and biomedical engineering.

</details>


### [36] [AMRG: Extend Vision Language Models for Automatic Mammography Report Generation](https://arxiv.org/abs/2508.09225)
*Nak-Jun Sung,Donghyun Lee,Bo Hwa Choi,Chae Jung Park*

Main category: eess.IV

TL;DR: AMRG是首个基于大视觉语言模型（VLM）的端到端框架，用于生成乳腺X光检查报告，通过参数高效微调（PEFT）和LoRA技术实现轻量级适应，并在公开数据集DMID上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 乳腺X光检查报告生成是医学AI中重要但未充分探索的任务，面临多视图图像推理、高分辨率视觉线索和非结构化放射学语言等挑战。

Method: 基于MedGemma-4B-it模型，采用LoRA进行参数高效微调，训练和评估在DMID数据集上进行，探索了多种VLM主干和LoRA超参数配置。

Result: 在语言生成和临床指标上表现优异，ROUGE-L为0.5691，METEOR为0.6152，CIDEr为0.5818，BI-RADS准确率为0.5582，诊断一致性和幻觉减少。

Conclusion: AMRG为放射学报告生成提供了可扩展和适应性强的框架，推动了多模态医学AI的未来研究。

Abstract: Mammography report generation is a critical yet underexplored task in medical AI, characterized by challenges such as multiview image reasoning, high-resolution visual cues, and unstructured radiologic language. In this work, we introduce AMRG (Automatic Mammography Report Generation), the first end-to-end framework for generating narrative mammography reports using large vision-language models (VLMs). Building upon MedGemma-4B-it-a domain-specialized, instruction-tuned VLM-we employ a parameter-efficient fine-tuning (PEFT) strategy via Low-Rank Adaptation (LoRA), enabling lightweight adaptation with minimal computational overhead. We train and evaluate AMRG on DMID, a publicly available dataset of paired high-resolution mammograms and diagnostic reports. This work establishes the first reproducible benchmark for mammography report generation, addressing a longstanding gap in multimodal clinical AI. We systematically explore LoRA hyperparameter configurations and conduct comparative experiments across multiple VLM backbones, including both domain-specific and general-purpose models under a unified tuning protocol. Our framework demonstrates strong performance across both language generation and clinical metrics, achieving a ROUGE-L score of 0.5691, METEOR of 0.6152, CIDEr of 0.5818, and BI-RADS accuracy of 0.5582. Qualitative analysis further highlights improved diagnostic consistency and reduced hallucinations. AMRG offers a scalable and adaptable foundation for radiology report generation and paves the way for future research in multimodal medical AI.

</details>


### [37] [T-CACE: A Time-Conditioned Autoregressive Contrast Enhancement Multi-Task Framework for Contrast-Free Liver MRI Synthesis, Segmentation, and Diagnosis](https://arxiv.org/abs/2508.09919)
*Xiaojiao Xiao,Jianfeng Zhao,Qinmin Vivian Hu,Guanghui Wang*

Main category: eess.IV

TL;DR: T-CACE框架通过合成多期对比增强MRI（CEMRI）来替代传统对比剂增强MRI，解决了传统MRI的局限性，包括对比剂风险、手动评估耗时和标注数据有限。


<details>
  <summary>Details</summary>
Motivation: 传统MRI在肝癌诊断中存在对比剂风险、耗时和数据集有限的挑战，需要一种更安全、高效的方法。

Method: T-CACE框架结合条件令牌编码（CTE）、动态时间感知注意力掩码（DTAM）和时间分类一致性约束（TCC），直接从非对比MRI合成多期CEMRI。

Result: 在两个独立肝脏MRI数据集上的实验表明，T-CACE在图像合成、分割和病灶分类上优于现有方法。

Conclusion: T-CACE为肝脏病变评估提供了一种临床相关且高效的替代方案，提高了安全性、诊断效率和可靠性。

Abstract: Magnetic resonance imaging (MRI) is a leading modality for the diagnosis of liver cancer, significantly improving the classification of the lesion and patient outcomes. However, traditional MRI faces challenges including risks from contrast agent (CA) administration, time-consuming manual assessment, and limited annotated datasets. To address these limitations, we propose a Time-Conditioned Autoregressive Contrast Enhancement (T-CACE) framework for synthesizing multi-phase contrast-enhanced MRI (CEMRI) directly from non-contrast MRI (NCMRI). T-CACE introduces three core innovations: a conditional token encoding (CTE) mechanism that unifies anatomical priors and temporal phase information into latent representations; and a dynamic time-aware attention mask (DTAM) that adaptively modulates inter-phase information flow using a Gaussian-decayed attention mechanism, ensuring smooth and physiologically plausible transitions across phases. Furthermore, a constraint for temporal classification consistency (TCC) aligns the lesion classification output with the evolution of the physiological signal, further enhancing diagnostic reliability. Extensive experiments on two independent liver MRI datasets demonstrate that T-CACE outperforms state-of-the-art methods in image synthesis, segmentation, and lesion classification. This framework offers a clinically relevant and efficient alternative to traditional contrast-enhanced imaging, improving safety, diagnostic efficiency, and reliability for the assessment of liver lesion. The implementation of T-CACE is publicly available at: https://github.com/xiaojiao929/T-CACE.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [38] [Toward Human-Robot Teaming: Learning Handover Behaviors from 3D Scenes](https://arxiv.org/abs/2508.09855)
*Yuekun Wu,Yik Lung Pang,Andrea Cavallaro,Changjae Oh*

Main category: cs.RO

TL;DR: 提出一种仅从RGB图像训练人机协作策略的方法，无需真实机器人数据，利用高斯泼溅重建生成演示，实现稳定抓取和避碰。


<details>
  <summary>Details</summary>
Motivation: 解决人机协作中真实机器人数据收集成本高及仿真与真实视觉域差距的问题。

Method: 利用稀疏视图高斯泼溅重建人机交接场景，生成图像-动作对演示，模拟相机姿态变化转换为夹爪姿态变化。

Result: 在高斯泼溅重建场景和真实人机交接实验中验证了方法的有效性和鲁棒性。

Conclusion: 该方法为人机交接任务提供了新的有效表示，促进了更无缝和鲁棒的人机协作。

Abstract: Human-robot teaming (HRT) systems often rely on large-scale datasets of human and robot interactions, especially for close-proximity collaboration tasks such as human-robot handovers. Learning robot manipulation policies from raw, real-world image data requires a large number of robot-action trials in the physical environment. Although simulation training offers a cost-effective alternative, the visual domain gap between simulation and robot workspace remains a major limitation. We introduce a method for training HRT policies, focusing on human-to-robot handovers, solely from RGB images without the need for real-robot training or real-robot data collection. The goal is to enable the robot to reliably receive objects from a human with stable grasping while avoiding collisions with the human hand. The proposed policy learner leverages sparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes to generate robot demonstrations containing image-action pairs captured with a camera mounted on the robot gripper. As a result, the simulated camera pose changes in the reconstructed scene can be directly translated into gripper pose changes. Experiments in both Gaussian Splatting reconstructed scene and real-world human-to-robot handover experiments demonstrate that our method serves as a new and effective representation for the human-to-robot handover task, contributing to more seamless and robust HRT.

</details>
