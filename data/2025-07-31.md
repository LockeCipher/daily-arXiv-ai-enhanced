<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 17]
- [cs.CR](#cs.CR) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Trade-offs in Image Generation: How Do Different Dimensions Interact?](https://arxiv.org/abs/2507.22100)
*Sicheng Zhang,Binzhu Xie,Zhonghao Yan,Yuli Zhang,Donghao Zhou,Xiaofei Chen,Shi Qiu,Jiaqi Liu,Guoyang Xie,Zhichao Lu*

Main category: cs.CV

TL;DR: 论文提出了TRIG-Bench和TRIGScore，用于量化图像生成模型在多个维度上的权衡，并通过实验展示了如何利用这些工具优化模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对图像生成模型在多维度（如质量、对齐、多样性等）上复杂权衡的量化工具和数据集。

Method: 引入TRIG-Bench数据集和TRIGScore评估指标，开发Relation Recognition System生成Dimension Trade-off Map（DTM），评估14种模型。

Result: 实验表明DTM能全面理解模型在不同维度上的权衡，并通过微调优化模型性能。

Conclusion: TRIG-Bench和TRIGScore为图像生成模型的评估和优化提供了新工具，展示了多维度权衡的重要性。

Abstract: Model performance in text-to-image (T2I) and image-to-image (I2I) generation often depends on multiple aspects, including quality, alignment, diversity, and robustness. However, models' complex trade-offs among these dimensions have rarely been explored due to (1) the lack of datasets that allow fine-grained quantification of these trade-offs, and (2) the use of a single metric for multiple dimensions. To bridge this gap, we introduce TRIG-Bench (Trade-offs in Image Generation), which spans 10 dimensions (Realism, Originality, Aesthetics, Content, Relation, Style, Knowledge, Ambiguity, Toxicity, and Bias), contains 40,200 samples, and covers 132 pairwise dimensional subsets. Furthermore, we develop TRIGScore, a VLM-as-judge metric that automatically adapts to various dimensions. Based on TRIG-Bench and TRIGScore, we evaluate 14 models across T2I and I2I tasks. In addition, we propose the Relation Recognition System to generate the Dimension Trade-off Map (DTM) that visualizes the trade-offs among model-specific capabilities. Our experiments demonstrate that DTM consistently provides a comprehensive understanding of the trade-offs between dimensions for each type of generative model. Notably, we show that the model's dimension-specific weaknesses can be mitigated through fine-tuning on DTM to enhance overall performance. Code is available at: https://github.com/fesvhtr/TRIG

</details>


### [2] [Temporally Consistent Unsupervised Segmentation for Mobile Robot Perception](https://arxiv.org/abs/2507.22194)
*Christian Ellis,Maggie Wigness,Craig Lennon,Lance Fiondella*

Main category: cs.CV

TL;DR: Frontier-Seg是一种无需监督的方法，通过聚类超像素特征并保持时间一致性，实现地形分割。


<details>
  <summary>Details</summary>
Motivation: 现有监督方法依赖昂贵的数据标注，且难以适应无标签的复杂环境，而零样本方法缺乏时间一致性。

Method: 利用DINOv2提取超像素特征，聚类并强制时间一致性以识别地形边界。

Result: 在RUGD和RELLIS-3D等数据集上验证了其有效性。

Conclusion: Frontier-Seg在无监督地形分割中表现出色，适用于复杂环境。

Abstract: Rapid progress in terrain-aware autonomous ground navigation has been driven by advances in supervised semantic segmentation. However, these methods rely on costly data collection and labor-intensive ground truth labeling to train deep models. Furthermore, autonomous systems are increasingly deployed in unrehearsed, unstructured environments where no labeled data exists and semantic categories may be ambiguous or domain-specific. Recent zero-shot approaches to unsupervised segmentation have shown promise in such settings but typically operate on individual frames, lacking temporal consistency-a critical property for robust perception in unstructured environments. To address this gap we introduce Frontier-Seg, a method for temporally consistent unsupervised segmentation of terrain from mobile robot video streams. Frontier-Seg clusters superpixel-level features extracted from foundation model backbones-specifically DINOv2-and enforces temporal consistency across frames to identify persistent terrain boundaries or frontiers without human supervision. We evaluate Frontier-Seg on a diverse set of benchmark datasets-including RUGD and RELLIS-3D-demonstrating its ability to perform unsupervised segmentation across unstructured off-road environments.

</details>


### [3] [UFV-Splatter: Pose-Free Feed-Forward 3D Gaussian Splatting Adapted to Unfavorable Views](https://arxiv.org/abs/2507.22342)
*Yuki Fujimura,Takahiro Kushida,Kazuya Kitano,Takuya Funatomi,Yasuhiro Mukaigawa*

Main category: cs.CV

TL;DR: 提出一种无需姿态、前馈的3D高斯泼溅框架，用于处理不利输入视角，通过低秩适应层和几何一致性增强模块提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖有利视角训练，限制了在未知相机姿态场景中的应用，需解决不利视角下的渲染问题。

Method: 结合低秩适应层（LoRA）和高斯适配模块，利用有利图像先验，提出高斯对齐方法和新训练策略。

Result: 在合成和真实数据集上验证了方法处理不利视角的有效性。

Conclusion: 提出的框架显著提升了模型在不利视角下的渲染能力，适用于更广泛的现实场景。

Abstract: This paper presents a pose-free, feed-forward 3D Gaussian Splatting (3DGS) framework designed to handle unfavorable input views. A common rendering setup for training feed-forward approaches places a 3D object at the world origin and renders it from cameras pointed toward the origin -- i.e., from favorable views, limiting the applicability of these models to real-world scenarios involving varying and unknown camera poses. To overcome this limitation, we introduce a novel adaptation framework that enables pretrained pose-free feed-forward 3DGS models to handle unfavorable views. We leverage priors learned from favorable images by feeding recentered images into a pretrained model augmented with low-rank adaptation (LoRA) layers. We further propose a Gaussian adapter module to enhance the geometric consistency of the Gaussians derived from the recentered inputs, along with a Gaussian alignment method to render accurate target views for training. Additionally, we introduce a new training strategy that utilizes an off-the-shelf dataset composed solely of favorable images. Experimental results on both synthetic images from the Google Scanned Objects dataset and real images from the OmniObject3D dataset validate the effectiveness of our method in handling unfavorable input views.

</details>


### [4] [GVD: Guiding Video Diffusion Model for Scalable Video Distillation](https://arxiv.org/abs/2507.22360)
*Kunyang Li,Jeffrey A Chan Santiago,Sarinda Dhanesh Samarasinghe,Gaowen Liu,Mubarak Shah*

Main category: cs.CV

TL;DR: GVD是一种基于扩散的视频数据集蒸馏方法，显著减少计算和存储需求，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型视频数据集的计算和存储需求问题，通过蒸馏方法保留时空信息。

Method: 提出GVD方法，联合蒸馏空间和时间特征，确保高保真视频生成和运动信息捕捉。

Result: 在MiniUCF和HMDB51数据集上显著优于现有方法，性能接近原始数据集，同时大幅减少帧数。

Conclusion: GVD在性能和效率上均达到最佳水平，支持高分辨率和多实例生成。

Abstract: To address the larger computation and storage requirements associated with large video datasets, video dataset distillation aims to capture spatial and temporal information in a significantly smaller dataset, such that training on the distilled data has comparable performance to training on all of the data. We propose GVD: Guiding Video Diffusion, the first diffusion-based video distillation method. GVD jointly distills spatial and temporal features, ensuring high-fidelity video generation across diverse actions while capturing essential motion information. Our method's diverse yet representative distillations significantly outperform previous state-of-the-art approaches on the MiniUCF and HMDB51 datasets across 5, 10, and 20 Instances Per Class (IPC). Specifically, our method achieves 78.29 percent of the original dataset's performance using only 1.98 percent of the total number of frames in MiniUCF. Additionally, it reaches 73.83 percent of the performance with just 3.30 percent of the frames in HMDB51. Experimental results across benchmark video datasets demonstrate that GVD not only achieves state-of-the-art performance but can also generate higher resolution videos and higher IPC without significantly increasing computational cost.

</details>


### [5] [TopoLiDM: Topology-Aware LiDAR Diffusion Models for Interpretable and Realistic LiDAR Point Cloud Generation](https://arxiv.org/abs/2507.22454)
*Jiuming Liu,Zheng Huang,Mengmeng Liu,Tianchen Deng,Francesco Nex,Hao Cheng,Hesheng Wang*

Main category: cs.CV

TL;DR: TopoLiDM结合图神经网络与扩散模型，通过拓扑正则化实现高保真LiDAR场景生成，显著提升几何真实性与全局拓扑一致性。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR生成方法难以捕捉几何真实性和全局拓扑一致性，限制了感知任务的鲁棒性。

Method: 提出TopoLiDM框架，结合拓扑保持的VAE和潜在扩散模型，引入0维持久同调约束。

Result: 在KITTI-360数据集上，TopoLiDM在FRID和MMD指标上分别提升22.6%和9.2%，生成速度为1.68样本/秒。

Conclusion: TopoLiDM在LiDAR生成任务中表现出色，兼具高效性和可扩展性。

Abstract: LiDAR scene generation is critical for mitigating real-world LiDAR data collection costs and enhancing the robustness of downstream perception tasks in autonomous driving. However, existing methods commonly struggle to capture geometric realism and global topological consistency. Recent LiDAR Diffusion Models (LiDMs) predominantly embed LiDAR points into the latent space for improved generation efficiency, which limits their interpretable ability to model detailed geometric structures and preserve global topological consistency. To address these challenges, we propose TopoLiDM, a novel framework that integrates graph neural networks (GNNs) with diffusion models under topological regularization for high-fidelity LiDAR generation. Our approach first trains a topological-preserving VAE to extract latent graph representations by graph construction and multiple graph convolutional layers. Then we freeze the VAE and generate novel latent topological graphs through the latent diffusion models. We also introduce 0-dimensional persistent homology (PH) constraints, ensuring the generated LiDAR scenes adhere to real-world global topological structures. Extensive experiments on the KITTI-360 dataset demonstrate TopoLiDM's superiority over state-of-the-art methods, achieving improvements of 22.6% lower Frechet Range Image Distance (FRID) and 9.2% lower Minimum Matching Distance (MMD). Notably, our model also enables fast generation speed with an average inference time of 1.68 samples/s, showcasing its scalability for real-world applications. We will release the related codes at https://github.com/IRMVLab/TopoLiDM.

</details>


### [6] [Exploiting Diffusion Prior for Task-driven Image Restoration](https://arxiv.org/abs/2507.22459)
*Jaeha Kim,Junghun Oh,Kyoung Mu Lee*

Main category: cs.CV

TL;DR: 论文提出EDTR方法，利用扩散先验恢复任务相关细节，显著提升多复杂退化场景下的任务性能和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有任务驱动图像恢复方法在多复杂退化场景下表现不佳的问题，利用扩散先验恢复任务相关细节。

Method: 提出EDTR方法，通过基于像素误差的预恢复图像生成和少量去噪步骤，直接利用低质量图像中的有用线索。

Result: EDTR方法在多复杂退化任务中显著提升了任务性能和视觉质量。

Conclusion: EDTR有效利用扩散先验，为任务驱动图像恢复提供了新思路。

Abstract: Task-driven image restoration (TDIR) has recently emerged to address performance drops in high-level vision tasks caused by low-quality (LQ) inputs. Previous TDIR methods struggle to handle practical scenarios in which images are degraded by multiple complex factors, leaving minimal clues for restoration. This motivates us to leverage the diffusion prior, one of the most powerful natural image priors. However, while the diffusion prior can help generate visually plausible results, using it to restore task-relevant details remains challenging, even when combined with recent TDIR methods. To address this, we propose EDTR, which effectively harnesses the power of diffusion prior to restore task-relevant details. Specifically, we propose directly leveraging useful clues from LQ images in the diffusion process by generating from pixel-error-based pre-restored LQ images with mild noise added. Moreover, we employ a small number of denoising steps to prevent the generation of redundant details that dilute crucial task-related information. We demonstrate that our method effectively utilizes diffusion prior for TDIR, significantly enhancing task performance and visual quality across diverse tasks with multiple complex degradations.

</details>


### [7] [Visual Language Models as Zero-Shot Deepfake Detectors](https://arxiv.org/abs/2507.22469)
*Viacheslav Pirogov*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉语言模型（VLM）的零样本方法用于深度伪造检测，并在高质量数据集上展示了其优于传统方法的性能。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术（如GAN或扩散模型）对数字媒体和身份验证等系统构成威胁，现有检测方法仅专注于图像域而缺乏辅助任务增强鲁棒性。

Method: 利用视觉语言模型的零样本能力，提出新的VLM方法，并在包含60,000张图像的高质量深度伪造数据集上进行评估。

Result: 零样本模型表现优于现有方法，最佳架构InstructBLIP在DFDC-P数据集上的零样本和微调场景中均优于传统分类器。

Conclusion: 视觉语言模型在深度伪造检测中展现出优于传统分类器的潜力。

Abstract: The contemporary phenomenon of deepfakes, utilizing GAN or diffusion models for face swapping, presents a substantial and evolving threat in digital media, identity verification, and a multitude of other systems. The majority of existing methods for detecting deepfakes rely on training specialized classifiers to distinguish between genuine and manipulated images, focusing only on the image domain without incorporating any auxiliary tasks that could enhance robustness. In this paper, inspired by the zero-shot capabilities of Vision Language Models, we propose a novel VLM-based approach to image classification and then evaluate it for deepfake detection. Specifically, we utilize a new high-quality deepfake dataset comprising 60,000 images, on which our zero-shot models demonstrate superior performance to almost all existing methods. Subsequently, we compare the performance of the best-performing architecture, InstructBLIP, on the popular deepfake dataset DFDC-P against traditional methods in two scenarios: zero-shot and in-domain fine-tuning. Our results demonstrate the superiority of VLMs over traditional classifiers.

</details>


### [8] [Robust Adverse Weather Removal via Spectral-based Spatial Grouping](https://arxiv.org/abs/2507.22498)
*Yuhwan Jeong,Yunseo Yang,Youngjo Yoon,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: 提出了一种基于频谱分解和分组注意力的多天气图像恢复方法SSGformer，通过分解图像为高低频特征并利用分组掩码和注意力机制，有效处理复杂天气退化。


<details>
  <summary>Details</summary>
Motivation: 现有All-in-One模型难以捕捉多样化和局部化的天气退化模式，需要一种更有效的方法来处理这些复杂问题。

Method: SSGformer通过频谱分解（边缘检测和奇异值分解）提取高低频特征，利用分组掩码和组注意力机制进行特征融合和恢复。

Result: 实验证明SSGformer在多种复杂天气退化条件下表现优异。

Conclusion: SSGformer通过频谱分解和分组注意力机制，显著提升了多天气图像恢复的性能。

Abstract: Adverse weather conditions cause diverse and complex degradation patterns, driving the development of All-in-One (AiO) models. However, recent AiO solutions still struggle to capture diverse degradations, since global filtering methods like direct operations on the frequency domain fail to handle highly variable and localized distortions. To address these issue, we propose Spectral-based Spatial Grouping Transformer (SSGformer), a novel approach that leverages spectral decomposition and group-wise attention for multi-weather image restoration. SSGformer decomposes images into high-frequency edge features using conventional edge detection and low-frequency information via Singular Value Decomposition. We utilize multi-head linear attention to effectively model the relationship between these features. The fused features are integrated with the input to generate a grouping-mask that clusters regions based on the spatial similarity and image texture. To fully leverage this mask, we introduce a group-wise attention mechanism, enabling robust adverse weather removal and ensuring consistent performance across diverse weather conditions. We also propose a Spatial Grouping Transformer Block that uses both channel attention and spatial attention, effectively balancing feature-wise relationships and spatial dependencies. Extensive experiments show the superiority of our approach, validating its effectiveness in handling the varied and intricate adverse weather degradations.

</details>


### [9] [DACA-Net: A Degradation-Aware Conditional Diffusion Network for Underwater Image Enhancement](https://arxiv.org/abs/2507.22501)
*Chang Huang,Jiahang Cao,Jun Ma,Kieren Yu,Cong Li,Huayong Yang,Kaishun Wu*

Main category: cs.CV

TL;DR: 提出了一种基于退化感知的条件扩散模型，用于自适应且鲁棒地增强水下图像，通过预测退化分数和引入物理先验，显著提升了图像质量。


<details>
  <summary>Details</summary>
Motivation: 水下图像常因散射和吸收等光学效应导致色彩失真、能见度低和结构模糊，现有方法难以自适应处理多样化退化条件且未能有效利用水下物理先验。

Method: 使用轻量级双流卷积网络预测退化分数，结合Swin UNet的条件扩散网络实现自适应噪声调度和分层特征优化，并引入退化引导的特征融合模块和混合损失函数。

Result: 在基准数据集上验证，该方法在色彩保真度、感知质量和结构细节方面显著优于现有方法。

Conclusion: 提出的框架在定量指标和定性视觉评估上均取得显著提升，为水下图像增强提供了有效解决方案。

Abstract: Underwater images typically suffer from severe colour distortions, low visibility, and reduced structural clarity due to complex optical effects such as scattering and absorption, which greatly degrade their visual quality and limit the performance of downstream visual perception tasks. Existing enhancement methods often struggle to adaptively handle diverse degradation conditions and fail to leverage underwater-specific physical priors effectively. In this paper, we propose a degradation-aware conditional diffusion model to enhance underwater images adaptively and robustly. Given a degraded underwater image as input, we first predict its degradation level using a lightweight dual-stream convolutional network, generating a continuous degradation score as semantic guidance. Based on this score, we introduce a novel conditional diffusion-based restoration network with a Swin UNet backbone, enabling adaptive noise scheduling and hierarchical feature refinement. To incorporate underwater-specific physical priors, we further propose a degradation-guided adaptive feature fusion module and a hybrid loss function that combines perceptual consistency, histogram matching, and feature-level contrast. Comprehensive experiments on benchmark datasets demonstrate that our method effectively restores underwater images with superior colour fidelity, perceptual quality, and structural details. Compared with SOTA approaches, our framework achieves significant improvements in both quantitative metrics and qualitative visual assessments.

</details>


### [10] [ShortFT: Diffusion Model Alignment via Shortcut-based Fine-Tuning](https://arxiv.org/abs/2507.22604)
*Xiefan Guo,Miaomiao Cui,Liefeng Bo,Di Huang*

Main category: cs.CV

TL;DR: 论文提出了一种基于短链去噪的高效微调策略ShortFT，解决了传统反向传播方法因长链去噪导致的梯度爆炸和计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于反向传播的方法在长链去噪过程中存在梯度爆炸和计算成本高的问题，导致结果不理想。

Method: 利用轨迹保留的少步扩散模型构建短链去噪路径，优化微调效率。

Result: ShortFT显著提升了模型对齐性能，优于现有方法。

Conclusion: ShortFT是一种高效且有效的微调策略，适用于多种奖励函数。

Abstract: Backpropagation-based approaches aim to align diffusion models with reward functions through end-to-end backpropagation of the reward gradient within the denoising chain, offering a promising perspective. However, due to the computational costs and the risk of gradient explosion associated with the lengthy denoising chain, existing approaches struggle to achieve complete gradient backpropagation, leading to suboptimal results. In this paper, we introduce Shortcut-based Fine-Tuning (ShortFT), an efficient fine-tuning strategy that utilizes the shorter denoising chain. More specifically, we employ the recently researched trajectory-preserving few-step diffusion model, which enables a shortcut over the original denoising chain, and construct a shortcut-based denoising chain of shorter length. The optimization on this chain notably enhances the efficiency and effectiveness of fine-tuning the foundational model. Our method has been rigorously tested and can be effectively applied to various reward functions, significantly improving alignment performance and surpassing state-of-the-art alternatives.

</details>


### [11] [Generative Active Learning for Long-tail Trajectory Prediction via Controllable Diffusion Model](https://arxiv.org/abs/2507.22615)
*Daehee Park,Monu Surana,Pranav Desai,Ashish Mehta,Reuben MV John,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: GALTraj通过生成式主动学习改进轨迹预测，专注于长尾场景，无需修改模型结构。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的轨迹预测在长尾场景中表现不佳，传统方法通过修改模型架构解决，但本研究提出通过优化训练过程来释放模型潜力。

Method: 提出GALTraj方法，利用生成式主动学习识别模型失败的稀有样本，并通过可控扩散模型增强这些样本。设计了一种尾部感知生成方法，确保生成场景多样、真实且保留尾部特征。

Result: 在多个轨迹数据集（WOMD、Argoverse2）和流行模型（QCNet、MTR）上验证，GALTraj显著提升尾部样本性能，同时提高头部样本准确性。

Conclusion: GALTraj是首个成功将生成式主动学习应用于轨迹预测的方法，有效解决了长尾学习问题，且无需改变模型结构。

Abstract: While data-driven trajectory prediction has enhanced the reliability of autonomous driving systems, it still struggles with rarely observed long-tail scenarios. Prior works addressed this by modifying model architectures, such as using hypernetworks. In contrast, we propose refining the training process to unlock each model's potential without altering its structure. We introduce Generative Active Learning for Trajectory prediction (GALTraj), the first method to successfully deploy generative active learning into trajectory prediction. It actively identifies rare tail samples where the model fails and augments these samples with a controllable diffusion model during training. In our framework, generating scenarios that are diverse, realistic, and preserve tail-case characteristics is paramount. Accordingly, we design a tail-aware generation method that applies tailored diffusion guidance to generate trajectories that both capture rare behaviors and respect traffic rules. Unlike prior simulation methods focused solely on scenario diversity, GALTraj is the first to show how simulator-driven augmentation benefits long-tail learning in trajectory prediction. Experiments on multiple trajectory datasets (WOMD, Argoverse2) with popular backbones (QCNet, MTR) confirm that our method significantly boosts performance on tail samples and also enhances accuracy on head samples.

</details>


### [12] [LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text Pairing](https://arxiv.org/abs/2507.22627)
*Federico Girella,Davide Talon,Ziyue Liu,Zanxi Ruan,Yiming Wang,Marco Cristani*

Main category: cs.CV

TL;DR: LOTS是一种基于局部草图+文本的时尚图像生成方法，通过全局描述和局部条件结合，实现高度定制化的设计生成。


<details>
  <summary>Details</summary>
Motivation: 时尚设计需要结合视觉和文本表达，传统方法难以同时处理全局和局部信息，LOTS旨在解决这一问题。

Method: LOTS采用模块化对中心表示和扩散对引导策略，将草图和文本编码到共享潜在空间，并通过扩散模型的多步去噪过程整合条件。

Result: LOTS在全局和局部指标上均达到最佳性能，并通过定性示例和人类评估验证了其设计定制化的能力。

Conclusion: LOTS为时尚图像生成提供了新的解决方案，能够高效结合草图与文本信息，实现高度定制化的设计输出。

Abstract: Fashion design is a complex creative process that blends visual and textual expressions. Designers convey ideas through sketches, which define spatial structure and design elements, and textual descriptions, capturing material, texture, and stylistic details. In this paper, we present LOcalized Text and Sketch for fashion image generation (LOTS), an approach for compositional sketch-text based generation of complete fashion outlooks. LOTS leverages a global description with paired localized sketch + text information for conditioning and introduces a novel step-based merging strategy for diffusion adaptation. First, a Modularized Pair-Centric representation encodes sketches and text into a shared latent space while preserving independent localized features; then, a Diffusion Pair Guidance phase integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we build on Fashionpedia to release Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Quantitative results show LOTS achieves state-of-the-art image generation performance on both global and localized metrics, while qualitative examples and a human evaluation study highlight its unprecedented level of design customization.

</details>


### [13] [Zero-Shot Image Anomaly Detection Using Generative Foundation Models](https://arxiv.org/abs/2507.22692)
*Lemar Abdi,Amaan Valiuddin,Francisco Caetano,Christiaan Viviers,Fons van der Sommen*

Main category: cs.CV

TL;DR: 该论文提出了一种基于扩散模型的OOD检测方法，利用DDMs的去噪轨迹和SSIM指标分析Stein分数误差，无需重新训练即可识别异常样本。


<details>
  <summary>Details</summary>
Motivation: 在开放世界中部署安全的视觉系统需要有效的OOD检测方法。本文探索扩散模型作为通用感知模板的潜力，而非生成器。

Method: 利用Denoising Diffusion Models（DDMs）的去噪轨迹提取纹理和语义信息，结合SSIM指标分析Stein分数误差，提出一种无需重新训练的OOD检测方法。

Result: 在CelebA数据集上训练的单一模型表现优异，甚至优于ImageNet，部分基准测试接近完美性能。

Conclusion: 生成基础模型在异常检测中具有强大潜力，未来可进一步优化。

Abstract: Detecting out-of-distribution (OOD) inputs is pivotal for deploying safe vision systems in open-world environments. We revisit diffusion models, not as generators, but as universal perceptual templates for OOD detection. This research explores the use of score-based generative models as foundational tools for semantic anomaly detection across unseen datasets. Specifically, we leverage the denoising trajectories of Denoising Diffusion Models (DDMs) as a rich source of texture and semantic information. By analyzing Stein score errors, amplified through the Structural Similarity Index Metric (SSIM), we introduce a novel method for identifying anomalous samples without requiring re-training on each target dataset. Our approach improves over state-of-the-art and relies on training a single model on one dataset -- CelebA -- which we find to be an effective base distribution, even outperforming more commonly used datasets like ImageNet in several settings. Experimental results show near-perfect performance on some benchmarks, with notable headroom on others, highlighting both the strength and future potential of generative foundation models in anomaly detection.

</details>


### [14] [Social-Pose: Enhancing Trajectory Prediction with Human Body Pose](https://arxiv.org/abs/2507.22742)
*Yang Gao,Saeed Saadatnejad,Alexandre Alahi*

Main category: cs.CV

TL;DR: 论文提出了一种基于人体姿态的轨迹预测方法Social-pose，通过注意力机制捕捉场景中所有人的姿态及其社交关系，显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有模型未能充分利用人类在空间中导航时潜意识传递的视觉线索（如姿态），影响了轨迹预测的准确性。

Method: 提出Social-pose，一种基于注意力的姿态编码器，可整合到多种轨迹预测架构中，并比较了2D与3D姿态的效果。

Result: 在多个数据集（合成和真实）上验证了方法的有效性，显著优于基于LSTM、GAN、MLP和Transformer的现有模型。

Conclusion: 利用人体姿态信息可显著提升轨迹预测性能，尤其在机器人导航场景中具有潜在应用价值。

Abstract: Accurate human trajectory prediction is one of the most crucial tasks for autonomous driving, ensuring its safety. Yet, existing models often fail to fully leverage the visual cues that humans subconsciously communicate when navigating the space. In this work, we study the benefits of predicting human trajectories using human body poses instead of solely their Cartesian space locations in time. We propose `Social-pose', an attention-based pose encoder that effectively captures the poses of all humans in a scene and their social relations. Our method can be integrated into various trajectory prediction architectures. We have conducted extensive experiments on state-of-the-art models (based on LSTM, GAN, MLP, and Transformer), and showed improvements over all of them on synthetic (Joint Track Auto) and real (Human3.6M, Pedestrians and Cyclists in Road Traffic, and JRDB) datasets. We also explored the advantages of using 2D versus 3D poses, as well as the effect of noisy poses and the application of our pose-based predictor in robot navigation scenarios.

</details>


### [15] [Segment Anything for Video: A Comprehensive Review of Video Object Segmentation and Tracking from Past to Future](https://arxiv.org/abs/2507.22792)
*Guoping Xu,Jayaram K. Udupa,Yajun Yu,Hua-Chieh Shao,Songlin Zhao,Wei Liu,You Zhang*

Main category: cs.CV

TL;DR: 本文综述了基于SAM/SAM2的视频对象分割与跟踪（VOST）方法，分析了其在过去、现在和未来三个时间维度的策略，并探讨了当前挑战与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统VOST方法在领域泛化、时间一致性和计算效率方面存在不足，而SAM/SAM2等基础模型的出现为解决这些问题提供了新思路。

Method: 通过回顾SAM/SAM2在VOST中的应用，分析其在历史信息保留（过去）、当前帧特征提取（现在）和未来动态预测（未来）的方法。

Result: 总结了从早期内存架构到SAM2实时分割的演进，并讨论了运动感知内存选择和轨迹引导提示等创新技术。

Conclusion: 指出了内存冗余、错误累积和提示效率等挑战，并提出了未来研究方向，为VOST领域的研究者和实践者提供了指导。

Abstract: Video Object Segmentation and Tracking (VOST) presents a complex yet critical challenge in computer vision, requiring robust integration of segmentation and tracking across temporally dynamic frames. Traditional methods have struggled with domain generalization, temporal consistency, and computational efficiency. The emergence of foundation models like the Segment Anything Model (SAM) and its successor, SAM2, has introduced a paradigm shift, enabling prompt-driven segmentation with strong generalization capabilities. Building upon these advances, this survey provides a comprehensive review of SAM/SAM2-based methods for VOST, structured along three temporal dimensions: past, present, and future. We examine strategies for retaining and updating historical information (past), approaches for extracting and optimizing discriminative features from the current frame (present), and motion prediction and trajectory estimation mechanisms for anticipating object dynamics in subsequent frames (future). In doing so, we highlight the evolution from early memory-based architectures to the streaming memory and real-time segmentation capabilities of SAM2. We also discuss recent innovations such as motion-aware memory selection and trajectory-guided prompting, which aim to enhance both accuracy and efficiency. Finally, we identify remaining challenges including memory redundancy, error accumulation, and prompt inefficiency, and suggest promising directions for future research. This survey offers a timely and structured overview of the field, aiming to guide researchers and practitioners in advancing the state of VOST through the lens of foundation models.

</details>


### [16] [Advancing Fetal Ultrasound Image Quality Assessment in Low-Resource Settings](https://arxiv.org/abs/2507.22802)
*Dongli He,Hu Wang,Mohammad Yaqub*

Main category: cs.CV

TL;DR: 利用FetalCLIP模型进行胎儿超声图像质量评估，提出FetalCLIP$_{CLS}$模型，在资源有限地区提升产前护理。


<details>
  <summary>Details</summary>
Motivation: 解决低收入国家因缺乏专业超声技师导致的胎儿超声图像质量评估困难问题。

Method: 基于FetalCLIP预训练模型，采用LoRA进行参数高效微调，提出FetalCLIP$_{CLS}$模型，并与CNN和Transformer基线模型对比。

Result: FetalCLIP$_{CLS}$在ACOUSLIC-AI数据集上F1得分0.757，改进后的分割模型F1得分0.771。

Conclusion: 参数高效微调胎儿超声基础模型可实现任务特定适配，推动资源有限地区的产前护理。

Abstract: Accurate fetal biometric measurements, such as abdominal circumference, play a vital role in prenatal care. However, obtaining high-quality ultrasound images for these measurements heavily depends on the expertise of sonographers, posing a significant challenge in low-income countries due to the scarcity of trained personnel. To address this issue, we leverage FetalCLIP, a vision-language model pretrained on a curated dataset of over 210,000 fetal ultrasound image-caption pairs, to perform automated fetal ultrasound image quality assessment (IQA) on blind-sweep ultrasound data. We introduce FetalCLIP$_{CLS}$, an IQA model adapted from FetalCLIP using Low-Rank Adaptation (LoRA), and evaluate it on the ACOUSLIC-AI dataset against six CNN and Transformer baselines. FetalCLIP$_{CLS}$ achieves the highest F1 score of 0.757. Moreover, we show that an adapted segmentation model, when repurposed for classification, further improves performance, achieving an F1 score of 0.771. Our work demonstrates how parameter-efficient fine-tuning of fetal ultrasound foundation models can enable task-specific adaptations, advancing prenatal care in resource-limited settings. The experimental code is available at: https://github.com/donglihe-hub/FetalCLIP-IQA.

</details>


### [17] [LCS: An AI-based Low-Complexity Scaler for Power-Efficient Super-Resolution of Game Content](https://arxiv.org/abs/2507.22873)
*Simon Pochinda,Momen K. Tageldeen,Mark Thompson,Tony Rinaldi,Troy Giorshev,Keith Lee,Jie Zhou,Frederick Walls*

Main category: cs.CV

TL;DR: 论文提出了一种基于AI的低复杂度缩放器（LCS），用于减轻GPU的工作负载，通过对抗训练和模型优化技术，在资源受限设备上实现高质量图像放大。


<details>
  <summary>Details</summary>
Motivation: 现代游戏中内容渲染的复杂性增加导致GPU工作负载显著增长，需要一种能在低功耗设备（如NPU）上运行的高效缩放解决方案。

Method: LCS基于高效超分辨率（ESR）模型，通过对抗训练、重参数化和量化技术降低模型复杂度和大小，使用GameIR图像对进行训练。

Result: LCS在五项指标上优于AMD的硬件缩放方案（EASF和FSR1），具有更好的感知质量。

Conclusion: LCS展示了ESR模型在资源受限设备上进行图像放大的潜力。

Abstract: The increasing complexity of content rendering in modern games has led to a problematic growth in the workload of the GPU. In this paper, we propose an AI-based low-complexity scaler (LCS) inspired by state-of-the-art efficient super-resolution (ESR) models which could offload the workload on the GPU to a low-power device such as a neural processing unit (NPU). The LCS is trained on GameIR image pairs natively rendered at low and high resolution. We utilize adversarial training to encourage reconstruction of perceptually important details, and apply reparameterization and quantization techniques to reduce model complexity and size. In our comparative analysis we evaluate the LCS alongside the publicly available AMD hardware-based Edge Adaptive Scaling Function (EASF) and AMD FidelityFX Super Resolution 1 (FSR1) on five different metrics, and find that the LCS achieves better perceptual quality, demonstrating the potential of ESR models for upscaling on resource-constrained devices.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [18] [Hate in Plain Sight: On the Risks of Moderating AI-Generated Hateful Illusions](https://arxiv.org/abs/2507.22617)
*Yiting Qu,Ziqing Yang,Yihan Ma,Michael Backes,Savvas Zannettou,Yang Zhang*

Main category: cs.CR

TL;DR: 该论文研究了基于文本到图像扩散模型生成的光学幻觉（含仇恨信息）对内容审核模型的潜在威胁，并评估了现有模型的检测能力。


<details>
  <summary>Details</summary>
Motivation: 探讨光学幻觉技术被滥用于生成仇恨内容的可能性，以及现有内容审核模型在此类内容上的脆弱性。

Method: 使用Stable Diffusion和ControlNet生成1,860个光学幻觉（含62种仇恨信息），构建Hateful Illusion数据集，并测试6种审核分类器和9种视觉语言模型的检测性能。

Result: 现有审核模型的检测准确率极低（分类器<0.245，VLM<0.102），主要因视觉编码器忽略隐藏信息层。

Conclusion: 需改进审核模型以识别隐藏仇恨信息，初步探索了图像变换和训练策略的缓解措施。

Abstract: Recent advances in text-to-image diffusion models have enabled the creation of a new form of digital art: optical illusions--visual tricks that create different perceptions of reality. However, adversaries may misuse such techniques to generate hateful illusions, which embed specific hate messages into harmless scenes and disseminate them across web communities. In this work, we take the first step toward investigating the risks of scalable hateful illusion generation and the potential for bypassing current content moderation models. Specifically, we generate 1,860 optical illusions using Stable Diffusion and ControlNet, conditioned on 62 hate messages. Of these, 1,571 are hateful illusions that successfully embed hate messages, either overtly or subtly, forming the Hateful Illusion dataset. Using this dataset, we evaluate the performance of six moderation classifiers and nine vision language models (VLMs) in identifying hateful illusions. Experimental results reveal significant vulnerabilities in existing moderation models: the detection accuracy falls below 0.245 for moderation classifiers and below 0.102 for VLMs. We further identify a critical limitation in their vision encoders, which mainly focus on surface-level image details while overlooking the secondary layer of information, i.e., hidden messages. To address this risk, we explore preliminary mitigation measures and identify the most effective approaches from the perspectives of image transformations and training-level strategies.

</details>
