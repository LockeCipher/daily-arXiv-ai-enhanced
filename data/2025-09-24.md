<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 3]
- [cs.CV](#cs.CV) [Total: 31]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Differentiable Light Transport with Gaussian Surfels via Adapted Radiosity for Efficient Relighting and Geometry Reconstruction](https://arxiv.org/abs/2509.18497)
*Kaiwen Jiang,Jia-Mu Sun,Zilu Li,Dan Wang,Tzu-Mao Li,Ravi Ramamoorthi*

Main category: cs.GR

TL;DR: 本文提出了一种基于高斯面元的高效可微分光传输框架，结合辐射度理论，在球谐系数空间中实现全局光照，支持漫反射和镜面材质，实现了视图无关渲染和实时全局光照效果。


<details>
  <summary>Details</summary>
Motivation: 传统辐射场方法牺牲了材质反射特性和光照条件的建模，导致几何歧义和无法进行重光照。现有方法在优化过程中加入全局光照成本过高，通常采用简化方案但精度不足。

Method: 采用高斯面元作为基元，在球谐系数空间中构建可微分光传输框架，扩展经典辐射度理论支持非二元可见性和半透明基元，提出高效求解器和梯度优化方法。

Result: 实现了视图无关渲染，在视角变化时无需重新计算光传输，达到数百FPS的全局光照效果，包括视图相关的反射。在几何重建、视图合成和重光照方面优于现有基线方法。

Conclusion: 该方法在稀疏数据集和已知/未知光照条件下，均表现出优于现有逆向渲染基线和数据驱动基线的性能，为高效的全局光照建模提供了新思路。

Abstract: Radiance fields have gained tremendous success with applications ranging from novel view synthesis to geometry reconstruction, especially with the advent of Gaussian splatting. However, they sacrifice modeling of material reflective properties and lighting conditions, leading to significant geometric ambiguities and the inability to easily perform relighting. One way to address these limitations is to incorporate physically-based rendering, but it has been prohibitively expensive to include full global illumination within the inner loop of the optimization. Therefore, previous works adopt simplifications that make the whole optimization with global illumination effects efficient but less accurate. In this work, we adopt Gaussian surfels as the primitives and build an efficient framework for differentiable light transport, inspired from the classic radiosity theory. The whole framework operates in the coefficient space of spherical harmonics, enabling both diffuse and specular materials. We extend the classic radiosity into non-binary visibility and semi-opaque primitives, propose novel solvers to efficiently solve the light transport, and derive the backward pass for gradient optimizations, which is more efficient than auto-differentiation. During inference, we achieve view-independent rendering where light transport need not be recomputed under viewpoint changes, enabling hundreds of FPS for global illumination effects, including view-dependent reflections using a spherical harmonics representation. Through extensive qualitative and quantitative experiments, we demonstrate superior geometry reconstruction, view synthesis and relighting than previous inverse rendering baselines, or data-driven baselines given relatively sparse datasets with known or unknown lighting conditions.

</details>


### [2] [Text Slider: Efficient and Plug-and-Play Continuous Concept Control for Image/Video Synthesis via LoRA Adapters](https://arxiv.org/abs/2509.18831)
*Pin-Yen Chiu,I-Sheng Fang,Jun-Cheng Chen*

Main category: cs.GR

TL;DR: Text Slider是一个轻量级、高效的即插即用框架，通过在预训练文本编码器中识别低秩方向，实现对视觉概念的连续控制，显著减少训练时间、GPU内存消耗和可训练参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有的概念控制方法需要大量训练时间和GPU内存来学习滑块或嵌入，并且需要为不同的扩散主干网络重新训练，限制了其可扩展性和适应性。

Method: 在预训练文本编码器中识别低秩方向，实现连续概念控制，支持多概念组合和连续控制。

Result: Text Slider实现了平滑连续的概念属性调制，同时保持输入的空间布局和结构，训练速度比Concept Slider快5倍，比Attribute Control快47倍，GPU内存使用分别减少近2倍和4倍。

Conclusion: Text Slider提供了一个高效、轻量级的解决方案，解决了现有概念控制方法的可扩展性和适应性限制。

Abstract: Recent advances in diffusion models have significantly improved image and video synthesis. In addition, several concept control methods have been proposed to enable fine-grained, continuous, and flexible control over free-form text prompts. However, these methods not only require intensive training time and GPU memory usage to learn the sliders or embeddings but also need to be retrained for different diffusion backbones, limiting their scalability and adaptability. To address these limitations, we introduce Text Slider, a lightweight, efficient and plug-and-play framework that identifies low-rank directions within a pre-trained text encoder, enabling continuous control of visual concepts while significantly reducing training time, GPU memory consumption, and the number of trainable parameters. Furthermore, Text Slider supports multi-concept composition and continuous control, enabling fine-grained and flexible manipulation in both image and video synthesis. We show that Text Slider enables smooth and continuous modulation of specific attributes while preserving the original spatial layout and structure of the input. Text Slider achieves significantly better efficiency: 5$\times$ faster training than Concept Slider and 47$\times$ faster than Attribute Control, while reducing GPU memory usage by nearly 2$\times$ and 4$\times$, respectively.

</details>


### [3] [One-shot Embroidery Customization via Contrastive LoRA Modulation](https://arxiv.org/abs/2509.18948)
*Jun Ma,Qian He,Gaofeng He,Huang Chen,Chen Liu,Xiaogang Jin,Huamin Wang*

Main category: cs.GR

TL;DR: 提出了一种基于对比学习的框架，用于从单张参考图像中解耦细粒度风格和内容特征，特别针对刺绣等复杂纹理的定制化需求，并在多个领域展示了优越性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像处理方面取得显著进展，但现有风格迁移方法难以处理刺绣等具有复杂针法图案和材质特性的细粒度视觉特征迁移需求。

Method: 采用对比学习框架，构建图像对定义目标风格，利用预训练扩散模型的解耦表示进行风格-内容分离，提出两阶段对比LoRA调制技术，包括迭代更新整个LoRA和选定风格块，以及通过自知识蒸馏进一步解耦风格和内容。

Result: 在刺绣定制化基准测试中超越先前方法，并在艺术风格迁移、素描上色和外观迁移三个额外领域展现出强大的泛化能力。

Conclusion: 该方法能够有效处理细粒度风格迁移任务，为复杂纹理特征的定制化提供了有效解决方案，具有广泛的应用前景。

Abstract: Diffusion models have significantly advanced image manipulation techniques, and their ability to generate photorealistic images is beginning to transform retail workflows, particularly in presale visualization. Beyond artistic style transfer, the capability to perform fine-grained visual feature transfer is becoming increasingly important. Embroidery is a textile art form characterized by intricate interplay of diverse stitch patterns and material properties, which poses unique challenges for existing style transfer methods. To explore the customization for such fine-grained features, we propose a novel contrastive learning framework that disentangles fine-grained style and content features with a single reference image, building on the classic concept of image analogy. We first construct an image pair to define the target style, and then adopt a similarity metric based on the decoupled representations of pretrained diffusion models for style-content separation. Subsequently, we propose a two-stage contrastive LoRA modulation technique to capture fine-grained style features. In the first stage, we iteratively update the whole LoRA and the selected style blocks to initially separate style from content. In the second stage, we design a contrastive learning strategy to further decouple style and content through self-knowledge distillation. Finally, we build an inference pipeline to handle image or text inputs with only the style blocks. To evaluate our method on fine-grained style transfer, we build a benchmark for embroidery customization. Our approach surpasses prior methods on this task and further demonstrates strong generalization to three additional domains: artistic style transfer, sketch colorization, and appearance transfer.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation](https://arxiv.org/abs/2509.19296)
*Sherwin Bahmani,Tianchang Shen,Jiawei Ren,Jiahui Huang,Yifeng Jiang,Haithem Turki,Andrea Tagliasacchi,David B. Lindell,Zan Gojcic,Sanja Fidler,Huan Ling,Jun Gao,Xuanchi Ren*

Main category: cs.CV

TL;DR: 提出了一种自蒸馏框架，将视频扩散模型中的隐式3D知识蒸馏到显式的3D高斯泼溅表示中，无需多视图训练数据即可生成3D场景


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的3D重建方法依赖真实世界多视图数据，但这些数据并不总是可用。视频扩散模型具有出色的想象力，但其2D特性限制了在需要与环境交互的模拟应用中的使用

Method: 在RGB解码器基础上增加3DGS解码器，通过RGB解码器的输出进行监督训练。3DGS解码器可以完全使用视频扩散模型生成的合成数据进行训练

Result: 实验结果表明，该框架在静态和动态3D场景生成方面达到了最先进的性能

Conclusion: 该框架能够从文本提示或单张图像合成3D场景进行实时渲染，并扩展到从单目输入视频生成动态3D场景

Abstract: The ability to generate virtual environments is crucial for applications ranging from gaming to physical AI domains such as robotics, autonomous driving, and industrial AI. Current learning-based 3D reconstruction methods rely on the availability of captured real-world multi-view data, which is not always readily available. Recent advancements in video diffusion models have shown remarkable imagination capabilities, yet their 2D nature limits the applications to simulation where a robot needs to navigate and interact with the environment. In this paper, we propose a self-distillation framework that aims to distill the implicit 3D knowledge in the video diffusion models into an explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for multi-view training data. Specifically, we augment the typical RGB decoder with a 3DGS decoder, which is supervised by the output of the RGB decoder. In this approach, the 3DGS decoder can be purely trained with synthetic data generated by video diffusion models. At inference time, our model can synthesize 3D scenes from either a text prompt or a single image for real-time rendering. Our framework further extends to dynamic 3D scene generation from a monocular input video. Experimental results show that our framework achieves state-of-the-art performance in static and dynamic 3D scene generation.

</details>


### [5] [The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image Generation Outcomes](https://arxiv.org/abs/2509.18179)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.CV

TL;DR: 该论文通过实证分析量化了视觉-语言-视觉管道中描述-生成瓶颈造成的信息损失，发现99.3%的样本存在显著感知退化，91.5%存在结构信息损失。


<details>
  <summary>Details</summary>
Motivation: 随着多模态AI系统在创意工作流中的集成增加，理解视觉-语言-视觉管道中的信息损失对于评估系统局限性变得重要，但目前通过文本中介传递视觉内容时的退化程度尚未得到充分量化。

Method: 生成了150对图像通过描述-生成管道，并应用现有指标（LPIPS、SSIM和颜色距离）来测量感知、结构和色彩维度上的信息保存情况。

Result: 评估显示99.3%的样本表现出显著的感知退化，91.5%的样本显示出显著的结构信息损失。

Conclusion: 描述-生成瓶颈代表了当代多模态系统中可测量且一致的限制，为系统改进提供了实证基础。

Abstract: With the increasing integration of multimodal AI systems in creative workflows, understanding information loss in vision-language-vision pipelines has become important for evaluating system limitations. However, the degradation that occurs when visual content passes through textual intermediation remains poorly quantified. In this work, we provide empirical analysis of the describe-then-generate bottleneck, where natural language serves as an intermediate representation for visual information. We generated 150 image pairs through the describe-then-generate pipeline and applied existing metrics (LPIPS, SSIM, and color distance) to measure information preservation across perceptual, structural, and chromatic dimensions. Our evaluation reveals that 99.3% of samples exhibit substantial perceptual degradation and 91.5% demonstrate significant structural information loss, providing empirical evidence that the describe-then-generate bottleneck represents a measurable and consistent limitation in contemporary multimodal systems.

</details>


### [6] [HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing](https://arxiv.org/abs/2509.18190)
*Junseong Shin,Seungwoo Chung,Yunjeong Yang,Tae Hyun Kim*

Main category: cs.CV

TL;DR: HazeFlow是一个基于常微分方程的去雾框架，将大气散射模型重新表述为ODE，通过单步推理实现真实世界的去雾，并利用马尔可夫链布朗运动生成非均匀雾霾数据来解决训练数据不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法因缺乏配对的真实世界训练数据和领域差距，在真实场景中泛化能力不足。传统基于大气散射模型的物理方法难以处理真实世界的复杂性和多样化雾霾模式。

Method: 提出HazeFlow框架，将大气散射模型重新表述为ODE，受Rectified Flow启发学习从雾霾图像到清晰图像的最优ODE轨迹。引入基于马尔可夫链布朗运动的非均匀雾霾生成方法模拟真实雾霾模式。

Result: 在多个真实世界去雾基准数据集上，HazeFlow实现了最先进的性能表现。

Conclusion: HazeFlow通过ODE框架和真实雾霾模拟方法，有效解决了真实世界去雾的挑战，在泛化性和性能方面都表现出色。

Abstract: Dehazing involves removing haze or fog from images to restore clarity and improve visibility by estimating atmospheric scattering effects. While deep learning methods show promise, the lack of paired real-world training data and the resulting domain gap hinder generalization to real-world scenarios. In this context, physics-grounded learning becomes crucial; however, traditional methods based on the Atmospheric Scattering Model (ASM) often fall short in handling real-world complexities and diverse haze patterns. To solve this problem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM as an ordinary differential equation (ODE). Inspired by Rectified Flow (RF), HazeFlow learns an optimal ODE trajectory to map hazy images to clean ones, enhancing real-world dehazing performance with only a single inference step. Additionally, we introduce a non-homogeneous haze generation method using Markov Chain Brownian Motion (MCBM) to address the scarcity of paired real-world data. By simulating realistic haze patterns through MCBM, we enhance the adaptability of HazeFlow to diverse real-world scenarios. Through extensive experiments, we demonstrate that HazeFlow achieves state-of-the-art performance across various real-world dehazing benchmark datasets.

</details>


### [7] [MoCrop: Training Free Motion Guided Cropping for Efficient Video Action Recognition](https://arxiv.org/abs/2509.18473)
*Binhua Huang,Wendong Yao,Shaowu Chen,Guoxin Wang,Qingyuan Wang,Soumyabrata Dev*

Main category: cs.CV

TL;DR: MoCrop是一种基于压缩域的运动感知自适应裁剪模块，用于高效视频动作识别，利用H.264视频中的运动向量定位运动密集区域，无需训练即可提升识别精度或降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有视频动作识别方法计算成本高，特别是在压缩域处理中。作者希望开发一种无需训练、参数零增加的方法，利用压缩视频中已有的运动向量信息来提高识别效率。

Method: MoCrop包含三个轻量级组件：去噪与合并(DM)、蒙特卡洛采样(MCS)和基于运动密度子矩阵搜索的自适应裁剪(AC)。该方法在推理时对所有I帧应用单一剪辑级裁剪。

Result: 在UCF101数据集上，MoCrop在保持相同FLOPs时提升ResNet-50准确率3.5%，或在减少26.5%FLOPs时提升2.4%准确率。应用于CoViAR时，在原始计算成本下达到89.2%准确率，或在减少计算量(11.6→8.5 GFLOPs)时保持88.5%准确率。

Conclusion: MoCrop具有强泛化性，在多种骨干网络上均表现一致，为压缩域实时部署提供了实用解决方案。

Abstract: We introduce MoCrop, a motion-aware adaptive cropping module for efficient video action recognition in the compressed domain. MoCrop uses motion vectors that are available in H.264 video to locate motion-dense regions and produces a single clip-level crop that is applied to all I-frames at inference. The module is training free, adds no parameters, and can be plugged into diverse backbones. A lightweight pipeline that includes denoising & merge (DM), Monte Carlo sampling (MCS), and adaptive cropping (AC) via a motion-density submatrix search yields robust crops with negligible overhead. On UCF101, MoCrop improves accuracy or reduces compute. With ResNet-50, it delivers +3.5% Top-1 accuracy at equal FLOPs (attention setting), or +2.4% Top-1 accuracy with 26.5% fewer FLOPs (efficiency setting). Applied to CoViAR, it reaches 89.2% Top-1 accuracy at the original cost and 88.5% Top-1 accuracy while reducing compute from 11.6 to 8.5 GFLOPs. Consistent gains on MobileNet-V3, EfficientNet-B1, and Swin-B indicate strong generality and make MoCrop practical for real-time deployment in the compressed domain. Our code and models are available at https://github.com/microa/MoCrop.

</details>


### [8] [BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting for Deformable Intraoperative Surgical Navigation](https://arxiv.org/abs/2509.18501)
*Maximilian Fehrentz,Alexander Winkler,Thomas Heiliger,Nazim Haouchine,Christian Heiliger,Nassir Navab*

Main category: cs.CV

TL;DR: BridgeSplat是一种新颖的可变形手术导航方法，通过将术中3D重建与术前CT数据耦合，在手术视频和体积患者数据之间建立桥梁。该方法将3D高斯函数绑定到CT网格上，通过光度监督联合优化高斯参数和网格变形。


<details>
  <summary>Details</summary>
Motivation: 解决手术导航中手术视频与术前CT数据之间的差距问题，实现术中实时变形跟踪和CT数据更新。

Method: 将3D高斯函数参数化相对于其父网格三角形，通过联合优化高斯参数和网格变形，强制执行高斯与网格之间的对齐，并将变形传播回CT进行更新。

Result: 在猪内脏手术和人类肝脏合成数据上验证了方法的有效性，展示了在单目RGB数据上对术前CT的合理变形。

Conclusion: BridgeSplat能够有效实现手术导航中的实时变形跟踪，为手术提供更准确的空间定位和变形感知。

Abstract: We introduce BridgeSplat, a novel approach for deformable surgical navigation that couples intraoperative 3D reconstruction with preoperative CT data to bridge the gap between surgical video and volumetric patient data. Our method rigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian parameters and mesh deformation through photometric supervision. By parametrizing each Gaussian relative to its parent mesh triangle, we enforce alignment between Gaussians and mesh and obtain deformations that can be propagated back to update the CT. We demonstrate BridgeSplat's effectiveness on visceral pig surgeries and synthetic data of a human liver under simulation, showing sensible deformations of the preoperative CT on monocular RGB data. Code, data, and additional resources can be found at https://maxfehrentz.github.io/ct-informed-splatting/ .

</details>


### [9] [Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment](https://arxiv.org/abs/2509.18502)
*Wenjie Liu,Hongmin Liu,Lixin Zhang,Bin Fan*

Main category: cs.CV

TL;DR: 提出了一种名为DGLE的伪标签优化框架，用于解决源数据不可访问的语义分割领域自适应问题。该方法从少量高质量伪标签出发，利用扩散模型传播生成完整的高质量伪标签集。


<details>
  <summary>Details</summary>
Motivation: 在源数据不可访问的实际场景中，源自由领域自适应（SFDA）研究有限。现有自训练方法需要高质量伪标签，但伪标签集通常包含大量噪声，同时优化所有标签具有挑战性。

Method: 1）基于置信度过滤和超分辨率增强的伪标签融合方法，获得少量高质量初始种子标签；2）利用扩散模型传播不完整种子伪标签，生成完整高质量伪标签。

Result: 该方法有效避免了直接优化完整伪标签集的困难，显著提高了伪标签质量，从而提升了模型在目标域的性能。

Conclusion: DGLE框架通过扩散模型引导的标签丰富化，为源自由领域自适应的语义分割提供了一种有效的伪标签优化解决方案。

Abstract: Research on unsupervised domain adaptation (UDA) for semantic segmentation of remote sensing images has been extensively conducted. However, research on how to achieve domain adaptation in practical scenarios where source domain data is inaccessible namely, source-free domain adaptation (SFDA) remains limited. Self-training has been widely used in SFDA, which requires obtaining as many high-quality pseudo-labels as possible to train models on target domain data. Most existing methods optimize the entire pseudo-label set to obtain more supervisory information. However, as pseudo-label sets often contain substantial noise, simultaneously optimizing all labels is challenging. This limitation undermines the effectiveness of optimization approaches and thus restricts the performance of self-training. To address this, we propose a novel pseudo-label optimization framework called Diffusion-Guided Label Enrichment (DGLE), which starts from a few easily obtained high-quality pseudo-labels and propagates them to a complete set of pseudo-labels while ensuring the quality of newly generated labels. Firstly, a pseudo-label fusion method based on confidence filtering and super-resolution enhancement is proposed, which utilizes cross-validation of details and contextual information to obtain a small number of high-quality pseudo-labels as initial seeds. Then, we leverage the diffusion model to propagate incomplete seed pseudo-labels with irregular distributions due to its strong denoising capability for randomly distributed noise and powerful modeling capacity for complex distributions, thereby generating complete and high-quality pseudo-labels. This method effectively avoids the difficulty of directly optimizing the complete set of pseudo-labels, significantly improves the quality of pseudo-labels, and thus enhances the model's performance in the target domain.

</details>


### [10] [Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction](https://arxiv.org/abs/2509.18566)
*Xiaoting Yin,Hao Shi,Kailun Yang,Jiajun Zhai,Shangwei Guo,Lin Wang,Kaiwei Wang*

Main category: cs.CV

TL;DR: 提出了一种基于事件相机和3D高斯泼溅的单目视频动态人体与静态场景联合重建框架，通过事件引导的损失函数解决快速运动下的运动模糊问题


<details>
  <summary>Details</summary>
Motivation: 从单目视频重建动态人体和静态场景存在困难，特别是在快速运动时RGB帧会出现运动模糊。事件相机具有微秒级时间分辨率的优势，更适合动态人体重建

Method: 使用统一的3D高斯集合，其中包含可学习的语义属性；只有被分类为人体的高斯会进行变形动画，场景高斯保持静态。提出事件引导的损失函数，匹配连续渲染之间的模拟亮度变化与事件流

Result: 在两个基准数据集ZJU-MoCap-Blur和MMHPSD-Blur上实现了最先进的人体-场景重建效果，在PSNR/SSIM指标上显著优于强基线，LPIPS指标降低，特别对高速运动主体效果明显

Conclusion: 该方法无需外部人体掩码，简化了单独高斯集合的管理，能够有效解决快速运动下的重建问题

Abstract: Reconstructing dynamic humans together with static scenes from monocular videos remains difficult, especially under fast motion, where RGB frames suffer from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond temporal resolution, making them a superior sensing choice for dynamic human reconstruction. Accordingly, we present a novel event-guided human-scene reconstruction framework that jointly models human and scene from a single monocular event camera via 3D Gaussian Splatting. Specifically, a unified set of 3D Gaussians carries a learnable semantic attribute; only Gaussians classified as human undergo deformation for animation, while scene Gaussians stay static. To combat blur, we propose an event-guided loss that matches simulated brightness changes between consecutive renderings with the event stream, improving local fidelity in fast-moving regions. Our approach removes the need for external human masks and simplifies managing separate Gaussian sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers state-of-the-art human-scene reconstruction, with notable gains over strong baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.

</details>


### [11] [SSCM: A Spatial-Semantic Consistent Model for Multi-Contrast MRI Super-Resolution](https://arxiv.org/abs/2509.18593)
*Xiaoman Wu,Lubin Gan,Siying Wu,Jing Zhang,Yunwei Ou,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 提出SSCM模型解决多对比度MRI超分辨率问题，通过动态空间扭曲、语义感知令牌聚合和空间频率融合模块，实现空间语义一致性重建


<details>
  <summary>Details</summary>
Motivation: 传统方法在空间语义一致性建模不足，未能充分利用频域信息，导致细粒度对齐差和高频细节恢复不足

Method: SSCM包含三个核心模块：动态空间扭曲模块实现跨对比度空间对齐，语义感知令牌聚合块保持长程语义一致性，空间频率融合块恢复精细结构

Result: 在公共和私有数据集上的实验表明，SSCM以更少参数实现最先进性能，确保空间和语义一致的重建

Conclusion: SSCM模型有效解决了多对比度MRI超分辨率中的空间语义一致性问题，显著提升了重建质量

Abstract: Multi-contrast Magnetic Resonance Imaging super-resolution (MC-MRI SR) aims to enhance low-resolution (LR) contrasts leveraging high-resolution (HR) references, shortening acquisition time and improving imaging efficiency while preserving anatomical details. The main challenge lies in maintaining spatial-semantic consistency, ensuring anatomical structures remain well-aligned and coherent despite structural discrepancies and motion between the target and reference images. Conventional methods insufficiently model spatial-semantic consistency and underuse frequency-domain information, which leads to poor fine-grained alignment and inadequate recovery of high-frequency details. In this paper, we propose the Spatial-Semantic Consistent Model (SSCM), which integrates a Dynamic Spatial Warping Module for inter-contrast spatial alignment, a Semantic-Aware Token Aggregation Block for long-range semantic consistency, and a Spatial-Frequency Fusion Block for fine structure restoration. Experiments on public and private datasets show that SSCM achieves state-of-the-art performance with fewer parameters while ensuring spatially and semantically consistent reconstructions.

</details>


### [12] [Training-Free Multi-Style Fusion Through Reference-Based Adaptive Modulation](https://arxiv.org/abs/2509.18602)
*Xu Liu,Yibo Lu,Xinxian Wang,Xinyu Wu*

Main category: cs.CV

TL;DR: AMSF是一个无需训练的参考式框架，能够在扩散模型中实现多参考风格的可控融合，解决了现有方法只能处理单一风格且缺乏平衡机制的问题。


<details>
  <summary>Details</summary>
Motivation: 现有参考式方法存在两个主要限制：(a) 只能接受单一风格图像，无法实现混合美学和扩展到更多风格；(b) 缺乏平衡多种风格影响的机制。

Method: 通过语义标记分解模块编码所有风格图像和文本提示，自适应注入到冻结扩散模型的每个交叉注意力层，然后通过相似度感知重加权模块在每个去噪步骤重新校准对每个风格组件的注意力分配。

Result: 定性和定量评估显示，AMSF生成的多风格融合结果持续优于最先进方法，且其融合设计可无缝扩展到两种或更多风格。

Conclusion: AMSF是向扩散模型中表达性多风格生成迈出的实用一步，实现了无需微调或外部适配器的平衡且用户可控的风格混合。

Abstract: We propose Adaptive Multi-Style Fusion (AMSF), a reference-based training-free framework that enables controllable fusion of multiple reference styles in diffusion models. Most of the existing reference-based methods are limited by (a) acceptance of only one style image, thus prohibiting hybrid aesthetics and scalability to more styles, and (b) lack of a principled mechanism to balance several stylistic influences. AMSF mitigates these challenges by encoding all style images and textual hints with a semantic token decomposition module that is adaptively injected into every cross-attention layer of an frozen diffusion model. A similarity-aware re-weighting module then recalibrates, at each denoising step, the attention allocated to every style component, yielding balanced and user-controllable blends without any fine-tuning or external adapters. Both qualitative and quantitative evaluations show that AMSF produces multi-style fusion results that consistently outperform the state-of-the-art approaches, while its fusion design scales seamlessly to two or more styles. These capabilities position AMSF as a practical step toward expressive multi-style generation in diffusion models.

</details>


### [13] [Prompt-Guided Dual Latent Steering for Inversion Problems](https://arxiv.org/abs/2509.18619)
*Yichen Wu,Xu Liu,Chenxuan Zhao,Xinyu Wu*

Main category: cs.CV

TL;DR: PDLS是一种基于Rectified Flow模型的无需训练框架，通过双潜在流（结构路径和语义路径）解决扩散模型潜在空间图像反演中的语义漂移问题，使用LQR控制器动态引导生成轨迹。


<details>
  <summary>Details</summary>
Motivation: 当前方法将图像编码为单个潜在向量时，难以平衡结构保真度和语义准确性，导致重建图像出现语义漂移（如细节模糊或属性错误）。

Method: PDLS将反演过程分解为两个互补流：结构路径保持源图像完整性，语义路径由提示词引导。通过最优控制问题建模，使用线性二次调节器（LQR）获得闭式解，动态引导生成轨迹。

Result: 在FFHQ-1K和ImageNet-1K上的广泛实验表明，PDLS在各种反演任务（高斯去模糊、运动去模糊、超分辨率和自由形式修复）中，比单潜在基线方法产生更忠实于原始图像且语义信息更对齐的重建结果。

Conclusion: PDLS框架有效解决了扩散模型潜在空间图像反演中的语义漂移问题，无需昂贵的每图像优化，在保持细节的同时确保语义准确性。

Abstract: Inverting corrupted images into the latent space of diffusion models is challenging. Current methods, which encode an image into a single latent vector, struggle to balance structural fidelity with semantic accuracy, leading to reconstructions with semantic drift, such as blurred details or incorrect attributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering (PDLS), a novel, training-free framework built upon Rectified Flow models for their stable inversion paths. PDLS decomposes the inversion process into two complementary streams: a structural path to preserve source integrity and a semantic path guided by a prompt. We formulate this dual guidance as an optimal control problem and derive a closed-form solution via a Linear Quadratic Regulator (LQR). This controller dynamically steers the generative trajectory at each step, preventing semantic drift while ensuring the preservation of fine detail without costly, per-image optimization. Extensive experiments on FFHQ-1K and ImageNet-1K under various inversion tasks, including Gaussian deblurring, motion deblurring, super-resolution and freeform inpainting, demonstrate that PDLS produces reconstructions that are both more faithful to the original image and better aligned with the semantic information than single-latent baselines.

</details>


### [14] [Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation](https://arxiv.org/abs/2509.18639)
*Yuanhuiyi Lyu,Chi Kit Wong,Chenfei Liao,Lutao Jiang,Xu Zheng,Zexin Lu,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: 提出了一种名为Understanding-in-Generation (UiG)的新型推理框架，通过将理解能力融入生成过程来增强统一模型的文本到图像生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有的推理方法将理解和生成过程分离，限制了它们指导统一模型解决生成能力不足的能力。

Method: 引入"图像编辑"作为桥梁，在推理过程中通过强大的理解能力来整合生成指导。首先验证生成的图像并将模型的理解融入编辑指令中，然后逐步增强生成的图像。

Result: 在TIIF基准测试的长提示设置上实现了3.92%的性能提升，显著优于现有的文本到图像推理方法。

Conclusion: UiG框架通过将理解能力融入生成过程，有效缓解了生成能力的局限性，为统一模型的文本到图像生成提供了新的解决方案。

Abstract: Recent works have made notable advancements in enhancing unified models for text-to-image generation through the Chain-of-Thought (CoT). However, these reasoning methods separate the processes of understanding and generation, which limits their ability to guide the reasoning of unified models in addressing the deficiencies of their generative capabilities. To this end, we propose a novel reasoning framework for unified models, Understanding-in-Generation (UiG), which harnesses the robust understanding capabilities of unified models to reinforce their performance in image generation. The core insight of our UiG is to integrate generative guidance by the strong understanding capabilities during the reasoning process, thereby mitigating the limitations of generative abilities. To achieve this, we introduce "Image Editing" as a bridge to infuse understanding into the generation process. Initially, we verify the generated image and incorporate the understanding of unified models into the editing instructions. Subsequently, we enhance the generated image step by step, gradually infusing the understanding into the generation process. Our UiG framework demonstrates a significant performance improvement in text-to-image generation over existing text-to-image reasoning methods, e.g., a 3.92% gain on the long prompt setting of the TIIF benchmark. The project code: https://github.com/QC-LY/UiG

</details>


### [15] [RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images](https://arxiv.org/abs/2509.18711)
*Ke Li,Di Wang,Ting Wang,Fuyu Dong,Yiming Zhang,Luyao Zhang,Xiangyu Wang,Shaofeng Li,Quan Wang*

Main category: cs.CV

TL;DR: RSVG-ZeroOV是一个无需训练的零样本开放词汇遥感视觉定位框架，利用冻结的通用基础模型实现开放世界场景下的目标定位


<details>
  <summary>Details</summary>
Motivation: 现有遥感视觉定位方法受限于封闭词汇表，而基于基础模型的方法需要昂贵的高质量数据集和耗时微调，需要一种更高效、可扩展的解决方案

Method: 三阶段框架：1）利用视觉语言模型获取文本查询与视觉区域的语义关联；2）利用扩散模型的细粒度建模能力补充结构形状信息；3）通过注意力进化模块抑制无关激活，生成纯净分割掩码

Result: 在广泛实验中，该框架一致优于现有的弱监督和零样本方法

Conclusion: RSVG-ZeroOV提供了一个无需繁琐任务特定训练的高效可扩展解决方案，展示了冻结通用基础模型在零样本开放词汇遥感视觉定位中的潜力

Abstract: Remote sensing visual grounding (RSVG) aims to localize objects in remote sensing images based on free-form natural language expressions. Existing approaches are typically constrained to closed-set vocabularies, limiting their applicability in open-world scenarios. While recent attempts to leverage generic foundation models for open-vocabulary RSVG, they overly rely on expensive high-quality datasets and time-consuming fine-tuning. To address these limitations, we propose \textbf{RSVG-ZeroOV}, a training-free framework that aims to explore the potential of frozen generic foundation models for zero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key stages: (i) Overview: We utilize a vision-language model (VLM) to obtain cross-attention\footnote[1]{In this paper, although decoder-only VLMs use self-attention over all tokens, we refer to the image-text interaction part as cross-attention to distinguish it from pure visual self-attention.}maps that capture semantic correlations between text queries and visual regions. (ii) Focus: By leveraging the fine-grained modeling priors of a diffusion model (DM), we fill in gaps in structural and shape information of objects, which are often overlooked by VLM. (iii) Evolve: A simple yet effective attention evolution module is introduced to suppress irrelevant activations, yielding purified segmentation masks over the referred objects. Without cumbersome task-specific training, RSVG-ZeroOV offers an efficient and scalable solution. Extensive experiments demonstrate that the proposed framework consistently outperforms existing weakly-supervised and zero-shot methods.

</details>


### [16] [HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal Salient Object Detection](https://arxiv.org/abs/2509.18738)
*Ruichao Hou,Xingyuan Li,Tongwei Ren,Dongming Zhou,Gangshan Wu,Jinde Cao*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的混合提示驱动的Segment Anything模型（HyPSAM），用于RGB-热显著目标检测。通过动态融合网络生成高质量初始显著图作为视觉提示，并结合文本、掩码和框提示来指导SAM模型优化显著目标检测结果。


<details>
  <summary>Details</summary>
Motivation: RGB-热显著目标检测面临特征融合不足和数据稀缺的挑战，需要更有效的方法来学习精确边界和完整目标。

Method: 1. 提出动态融合网络（DFNet）使用动态卷积和多分支解码实现自适应跨模态交互；2. 设计即插即用优化网络（P2RNet）作为通用优化策略，利用混合提示（文本、掩码、框）指导SAM模型优化显著图。

Result: 在三个公开数据集上的实验表明，该方法达到了最先进的性能，并且具有显著的多功能性，可以与不同的RGB-T SOD方法无缝集成实现性能提升。

Conclusion: HyPSAM展示了提示工程在RGB-T SOD领域的潜力，通过利用SAM的零样本泛化能力，有效解决了多模态特征融合和边界精确定位的问题。

Abstract: RGB-thermal salient object detection (RGB-T SOD) aims to identify prominent objects by integrating complementary information from RGB and thermal modalities. However, learning the precise boundaries and complete objects remains challenging due to the intrinsic insufficient feature fusion and the extrinsic limitations of data scarcity. In this paper, we propose a novel hybrid prompt-driven segment anything model (HyPSAM), which leverages the zero-shot generalization capabilities of the segment anything model (SAM) for RGB-T SOD. Specifically, we first propose a dynamic fusion network (DFNet) that generates high-quality initial saliency maps as visual prompts. DFNet employs dynamic convolution and multi-branch decoding to facilitate adaptive cross-modality interaction, overcoming the limitations of fixed-parameter kernels and enhancing multi-modal feature representation. Moreover, we propose a plug-and-play refinement network (P2RNet), which serves as a general optimization strategy to guide SAM in refining saliency maps by using hybrid prompts. The text prompt ensures reliable modality input, while the mask and box prompts enable precise salient object localization. Extensive experiments on three public datasets demonstrate that our method achieves state-of-the-art performance. Notably, HyPSAM has remarkable versatility, seamlessly integrating with different RGB-T SOD methods to achieve significant performance gains, thereby highlighting the potential of prompt engineering in this field. The code and results of our method are available at: https://github.com/milotic233/HyPSAM.

</details>


### [17] [TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing](https://arxiv.org/abs/2509.18743)
*Susmit Neogi*

Main category: cs.CV

TL;DR: TriFusion-AE是一种多模态交叉注意力自编码器，通过整合文本先验、多视图图像的单目深度图和LiDAR点云，提高LiDAR感知在噪声和对抗性攻击下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LiDAR点云对噪声、遮挡和对抗性攻击高度敏感，现有自编码器在真实世界挑战性条件下性能下降，需要更鲁棒的感知方法。

Method: 提出TriFusion-AE，通过跨模态注意力机制对齐文本语义线索、图像几何特征和LiDAR空间结构，学习对噪声和对抗性攻击具有弹性的表示。

Result: 在nuScenes-mini数据集上评估，模型在强对抗性攻击和重度噪声下表现出显著鲁棒性，而传统CNN自编码器在这些条件下崩溃。

Conclusion: 多模态融合框架具有模型无关性，可与任何基于CNN的点云自编码器无缝集成，为自动驾驶和机器人技术提供更可靠的感知解决方案。

Abstract: LiDAR-based perception is central to autonomous driving and robotics, yet raw point clouds remain highly vulnerable to noise, occlusion, and adversarial corruptions. Autoencoders offer a natural framework for denoising and reconstruction, but their performance degrades under challenging real-world conditions. In this work, we propose TriFusion-AE, a multimodal cross-attention autoencoder that integrates textual priors, monocular depth maps from multi-view images, and LiDAR point clouds to improve robustness. By aligning semantic cues from text, geometric (depth) features from images, and spatial structure from LiDAR, TriFusion-AE learns representations that are resilient to stochastic noise and adversarial perturbations. Interestingly, while showing limited gains under mild perturbations, our model achieves significantly more robust reconstruction under strong adversarial attacks and heavy noise, where CNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset to reflect realistic low-data deployment scenarios. Our multimodal fusion framework is designed to be model-agnostic, enabling seamless integration with any CNN-based point cloud autoencoder for joint representation learning.

</details>


### [18] [FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score Distillation](https://arxiv.org/abs/2509.18759)
*Zhaorui Wang,Yi Gu,Deming Zhou,Renjing Xu*

Main category: cs.CV

TL;DR: FixingGS是一种无需训练的方法，利用现有扩散模型增强稀疏视角3D高斯溅射重建，通过更准确的跨视图一致性扩散先验和自适应渐进增强方案，有效去除伪影并补全缺失内容。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角下的3D场景重建由于视觉信息不足会产生明显伪影，现有基于生成先验的方法难以保证多视图一致性，导致结构模糊和细节不真实。

Method: 提出蒸馏方法提供更准确和跨视图一致的扩散先验，结合自适应渐进增强方案，在训练约束不足区域进一步优化重建效果。

Result: 大量实验表明FixingGS在视觉质量和重建性能上优于现有最先进方法。

Conclusion: FixingGS通过充分利用扩散模型能力，成功解决了稀疏视角3D高斯溅射重建中的伪影和多视图一致性问题。

Abstract: Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in 3D reconstruction and novel view synthesis. However, reconstructing 3D scenes from sparse viewpoints remains highly challenging due to insufficient visual information, which results in noticeable artifacts persisting across the 3D representation. To address this limitation, recent methods have resorted to generative priors to remove artifacts and complete missing content in under-constrained areas. Despite their effectiveness, these approaches struggle to ensure multi-view consistency, resulting in blurred structures and implausible details. In this work, we propose FixingGS, a training-free method that fully exploits the capabilities of the existing diffusion model for sparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our distillation approach, which delivers more accurate and cross-view coherent diffusion priors, thereby enabling effective artifact removal and inpainting. In addition, we propose an adaptive progressive enhancement scheme that further refines reconstructions in under-constrained regions. Extensive experiments demonstrate that FixingGS surpasses existing state-of-the-art methods with superior visual quality and reconstruction performance. Our code will be released publicly.

</details>


### [19] [Towards Application Aligned Synthetic Surgical Image Synthesis](https://arxiv.org/abs/2509.18796)
*Danush Kumar Venkatesh,Stefanie Speidel*

Main category: cs.CV

TL;DR: SAADi框架通过将扩散模型与下游模型偏好的样本对齐，解决手术数据稀缺问题，避免数据记忆化导致的样本不一致或多样性不足。


<details>
  <summary>Details</summary>
Motivation: 手术数据标注稀缺限制了深度学习系统在计算机辅助干预中的发展，现有扩散模型存在数据记忆化问题，生成的样本可能不一致或缺乏多样性，甚至损害下游性能。

Method: 构建偏好和非偏好合成图像对，通过轻量级微调扩散模型，使图像生成过程与下游目标明确对齐。

Result: 在三个手术数据集上的实验显示，分类任务提升7-9%，分割任务提升2-10%，对代表性不足的类别改善显著。迭代细化合成样本可进一步提升性能4-10%。

Conclusion: SAADi方法克服了样本退化问题，确立了任务感知对齐作为缓解数据稀缺和推进手术视觉应用的关键原则。

Abstract: The scarcity of annotated surgical data poses a significant challenge for developing deep learning systems in computer-assisted interventions. While diffusion models can synthesize realistic images, they often suffer from data memorization, resulting in inconsistent or non-diverse samples that may fail to improve, or even harm, downstream performance. We introduce \emph{Surgical Application-Aligned Diffusion} (SAADi), a new framework that aligns diffusion models with samples preferred by downstream models. Our method constructs pairs of \emph{preferred} and \emph{non-preferred} synthetic images and employs lightweight fine-tuning of diffusion models to align the image generation process with downstream objectives explicitly. Experiments on three surgical datasets demonstrate consistent gains of $7$--$9\%$ in classification and $2$--$10\%$ in segmentation tasks, with the considerable improvements observed for underrepresented classes. Iterative refinement of synthetic samples further boosts performance by $4$--$10\%$. Unlike baseline approaches, our method overcomes sample degradation and establishes task-aware alignment as a key principle for mitigating data scarcity and advancing surgical vision applications.

</details>


### [20] [A Kernel Space-based Multidimensional Sparse Model for Dynamic PET Image Denoising](https://arxiv.org/abs/2509.18801)
*Kuang Xiaodong,Li Bingxuan,Li Yuan,Rao Fan,Ma Gege,Xie Qingguo,Mok Greta S P,Liu Huafeng,Zhu Wentao*

Main category: cs.CV

TL;DR: 提出了一种基于模型神经网络的动态PET图像去噪方法KMDS-Net，利用帧间空间相关性和帧内结构一致性建立多维稀疏模型，通过神经网络自适应优化参数，在模拟和真实数据上表现出优于基线方法的去噪性能。


<details>
  <summary>Details</summary>
Motivation: 动态PET中短时间帧的图像质量因统计量有限而难以保证，深度学习在医学图像去噪中展现出潜力，但需要专门针对动态PET特性的方法。

Method: 建立基于核空间的多维稀疏(KMDS)模型，利用动态PET的帧间空间相关性和帧内结构一致性，然后用神经网络替代参数估计过程，形成端到端的KMDS-Net网络。

Result: 在模拟和真实数据上的广泛实验表明，KMDS-Net在动态PET去噪方面表现出强大的性能，优于之前的基线方法。

Conclusion: 该方法可有效实现动态PET的高时间和空间分辨率，为动态PET图像质量提升提供了有效解决方案。

Abstract: Achieving high image quality for temporal frames in dynamic positron emission tomography (PET) is challenging due to the limited statistic especially for the short frames. Recent studies have shown that deep learning (DL) is useful in a wide range of medical image denoising tasks. In this paper, we propose a model-based neural network for dynamic PET image denoising. The inter-frame spatial correlation and intra-frame structural consistency in dynamic PET are used to establish the kernel space-based multidimensional sparse (KMDS) model. We then substitute the inherent forms of the parameter estimation with neural networks to enable adaptive parameters optimization, forming the end-to-end neural KMDS-Net. Extensive experimental results from simulated and real data demonstrate that the neural KMDS-Net exhibits strong denoising performance for dynamic PET, outperforming previous baseline methods. The proposed method may be used to effectively achieve high temporal and spatial resolution for dynamic PET. Our source code is available at https://github.com/Kuangxd/Neural-KMDS-Net/tree/main.

</details>


### [21] [Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation](https://arxiv.org/abs/2509.18824)
*Yanzuo Lu,Xin Xia,Manlin Zhang,Huafeng Kuang,Jianbin Zheng,Yuxi Ren,Xuefeng Xiao*

Main category: cs.CV

TL;DR: Hyper-Bagel是一个统一的多模态加速框架，通过推测解码和多阶段蒸馏技术，在保持高质量输出的同时显著加速多模态理解和生成任务。


<details>
  <summary>Details</summary>
Motivation: 随着多模态上下文包含越来越多的交织标记，扩散去噪和自回归解码的迭代过程带来了巨大的计算开销，需要高效的加速解决方案。

Method: 采用分治策略，使用推测解码进行下一标记预测，并通过多阶段蒸馏过程加速扩散去噪。结合对抗蒸馏和人类反馈学习开发高效模型。

Result: 在多模态理解任务上实现超过2倍加速；文本到图像生成实现16.67倍加速；图像编辑实现22倍加速；开发出近乎实时的1-NFE模型。

Conclusion: Hyper-Bagel框架通过先进的加速技术实现了成本效益和响应性的显著提升，使复杂的多模态交互变得无缝且即时。

Abstract: Unified multimodal models have recently attracted considerable attention for their remarkable abilities in jointly understanding and generating diverse content. However, as contexts integrate increasingly numerous interleaved multimodal tokens, the iterative processes of diffusion denoising and autoregressive decoding impose significant computational overhead. To address this, we propose Hyper-Bagel, a unified acceleration framework designed to simultaneously speed up both multimodal understanding and generation tasks. Our approach uses a divide-and-conquer strategy, employing speculative decoding for next-token prediction and a multi-stage distillation process for diffusion denoising. The framework delivers substantial performance gains, achieving over a 2x speedup in multimodal understanding. For generative tasks, our resulting lossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a 22x speedup in image editing, all while preserving the high-quality output of the original model. We further develop a highly efficient 1-NFE model that enables near real-time interactive editing and generation. By combining advanced adversarial distillation with human feedback learning, this model achieves ultimate cost-effectiveness and responsiveness, making complex multimodal interactions seamless and instantaneous.

</details>


### [22] [RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing](https://arxiv.org/abs/2509.18897)
*Jiayu Wang,Ruizhi Wang,Jie Song,Haofei Zhang,Mingli Song,Zunlei Feng,Li Sun*

Main category: cs.CV

TL;DR: 本文提出了RS3DBench基准数据集，包含54,951对遥感图像与像素级对齐的深度图，旨在推动遥感领域通用大规模3D视觉模型的发展。


<details>
  <summary>Details</summary>
Motivation: 现有遥感数据集缺乏全面的深度信息或深度数据与遥感图像之间的精确对齐，限制了3D视觉感知模型在遥感领域的发展。

Method: 构建包含54,951对遥感图像和深度图的数据集，并基于稳定扩散模型开发遥感深度估计模型，利用其多模态融合能力。

Result: 提出的深度估计模型在RS3DBench数据集上实现了最先进的性能。

Conclusion: RS3DBench基准将为遥感领域3D视觉感知模型的发展和地理人工智能的进步做出重要贡献。

Abstract: In this paper, we introduce a novel benchmark designed to propel the advancement of general-purpose, large-scale 3D vision models for remote sensing imagery. While several datasets have been proposed within the realm of remote sensing, many existing collections either lack comprehensive depth information or fail to establish precise alignment between depth data and remote sensing images. To address this deficiency, we present a visual Benchmark for 3D understanding of Remotely Sensed images, dubbed RS3DBench. This dataset encompasses 54,951 pairs of remote sensing images and pixel-level aligned depth maps, accompanied by corresponding textual descriptions, spanning a broad array of geographical contexts. It serves as a tool for training and assessing 3D visual perception models within remote sensing image spatial understanding tasks. Furthermore, we introduce a remotely sensed depth estimation model derived from stable diffusion, harnessing its multimodal fusion capabilities, thereby delivering state-of-the-art performance on our dataset. Our endeavor seeks to make a profound contribution to the evolution of 3D visual perception models and the advancement of geographic artificial intelligence within the remote sensing domain. The dataset, models and code will be accessed on the https://rs3dbench.github.io.

</details>


### [23] [DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring](https://arxiv.org/abs/2509.18898)
*Pengteng Li,Yunfan Lu,Pinhao Song,Weiyu Guo,Huizai Yao,F. Richard Yu,Hui Xiong*

Main category: cs.CV

TL;DR: DeblurSplat是首个无需Structure-from-Motion的事件相机去模糊3D高斯泼溅方法，通过DUSt3R直接获取初始点云并利用事件流进行精细监督，实现高效高保真度的去模糊渲染。


<details>
  <summary>Details</summary>
Motivation: 解决传统运动去模糊方法中相机位姿估计累积误差问题，以及利用事件相机对动态变化的高敏感性来提升去模糊效果。

Method: 1) 使用DUSt3R密集立体模块直接从模糊图像获取准确初始点云，避免相机位姿估计误差传递；2) 引入事件流解码潜在清晰图像，为场景重建优化提供精细监督信号。

Result: 在多个场景上的实验表明，DeblurSplat不仅能生成高保真度的新视角图像，而且在去模糊3D-GS方面相比现有最优方法实现了显著的渲染效率提升。

Conclusion: 该方法成功实现了无需SfM的高效去模糊3D重建，结合事件相机的优势为动态场景重建提供了新的解决方案。

Abstract: In this paper, we propose the first Structure-from-Motion (SfM)-free deblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat. We address the motion-deblurring problem in two ways. First, we leverage the pretrained capability of the dense stereo module (DUSt3R) to directly obtain accurate initial point clouds from blurred images. Without calculating camera poses as an intermediate result, we avoid the cumulative errors transfer from inaccurate camera poses to the initial point clouds' positions. Second, we introduce the event stream into the deblur pipeline for its high sensitivity to dynamic change. By decoding the latent sharp images from the event stream and blurred images, we can provide a fine-grained supervision signal for scene reconstruction optimization. Extensive experiments across a range of scenes demonstrate that DeblurSplat not only excels in generating high-fidelity novel views but also achieves significant rendering efficiency compared to the SOTAs in deblur 3D-GS.

</details>


### [24] [LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2509.18917)
*Amirhesam Aghanouri,Cristina Olaverri-Monreal*

Main category: cs.CV

TL;DR: 本文提出了一种基于去噪扩散概率模型（DDPM）的方法，通过改进的噪声调度和时间步嵌入技术生成高质量合成LiDAR数据，用于增强自动驾驶车辆的3D视觉系统性能。


<details>
  <summary>Details</summary>
Motivation: 真实LiDAR数据采集耗时且易受噪声和稀疏性影响（如恶劣天气或传感器限制），需要合成数据来增强自动驾驶感知系统的鲁棒性。

Method: 采用改进的DDPM模型，引入新颖的噪声调度和时间步嵌入技术，优化去噪过程和时间感知能力，生成更真实的点云数据。

Result: 在IAMCV和KITTI-360数据集上的评估显示，该方法在四项性能指标上优于大多数现有基线方法，能有效缓解噪声和稀疏LiDAR数据的影响。

Conclusion: 所提出的方法能够生成具有丰富空间关系和结构细节的多样化点云，显著提升自动驾驶感知任务的性能。

Abstract: Autonomous vehicles (AVs) are expected to revolutionize transportation by improving efficiency and safety. Their success relies on 3D vision systems that effectively sense the environment and detect traffic agents. Among sensors AVs use to create a comprehensive view of surroundings, LiDAR provides high-resolution depth data enabling accurate object detection, safe navigation, and collision avoidance. However, collecting real-world LiDAR data is time-consuming and often affected by noise and sparsity due to adverse weather or sensor limitations. This work applies a denoising diffusion probabilistic model (DDPM), enhanced with novel noise scheduling and time-step embedding techniques to generate high-quality synthetic data for augmentation, thereby improving performance across a range of computer vision tasks, particularly in AV perception. These modifications impact the denoising process and the model's temporal awareness, allowing it to produce more realistic point clouds based on the projection. The proposed method was extensively evaluated under various configurations using the IAMCV and KITTI-360 datasets, with four performance metrics compared against state-of-the-art (SOTA) methods. The results demonstrate the model's superior performance over most existing baselines and its effectiveness in mitigating the effects of noisy and sparse LiDAR data, producing diverse point clouds with rich spatial relationships and structural detail.

</details>


### [25] [Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting](https://arxiv.org/abs/2509.18956)
*Zijing Guo,Yunyang Zhao,Lin Wang*

Main category: cs.CV

TL;DR: 本文提出了MirrorScene3D数据集和ReflectiveGS方法，用于解决含镜面环境中的3D重建和新视角合成问题，通过利用镜面反射作为补充视角来提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D重建方法（如NeRF和3DGS）在含镜面环境中性能下降，传统方法主要关注镜面表面的对称映射，但忽略了镜面反射携带的丰富信息，这些反射可以提供补充视角来填补缺失细节。

Method: 提出了ReflectiveGS方法，这是3D高斯泼溅的扩展，将镜面反射作为补充视角而非简单的对称伪影，从而增强场景几何并恢复缺失细节。

Result: 在MirrorScene3D数据集上的实验表明，ReflectiveGS在SSIM、PSNR、LPIPS指标和训练速度上均优于现有方法。

Conclusion: ReflectiveGS为镜面丰富环境中的3D重建设定了新的基准，证明了利用镜面反射作为补充视角的有效性。

Abstract: Mirror-containing environments pose unique challenges for 3D reconstruction and novel view synthesis (NVS), as reflective surfaces introduce view-dependent distortions and inconsistencies. While cutting-edge methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical scenes, their performance deteriorates in the presence of mirrors. Existing solutions mainly focus on handling mirror surfaces through symmetry mapping but often overlook the rich information carried by mirror reflections. These reflections offer complementary perspectives that can fill in absent details and significantly enhance reconstruction quality. To advance 3D reconstruction in mirror-rich environments, we present MirrorScene3D, a comprehensive dataset featuring diverse indoor scenes, 1256 high-quality images, and annotated mirror masks, providing a benchmark for evaluating reconstruction methods in reflective settings. Building on this, we propose ReflectiveGS, an extension of 3D Gaussian Splatting that utilizes mirror reflections as complementary viewpoints rather than simple symmetry artifacts, enhancing scene geometry and recovering absent details. Experiments on MirrorScene3D show that ReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and training speed, setting a new benchmark for 3D reconstruction in mirror-rich environments.

</details>


### [26] [Generative data augmentation for biliary tract detection on intraoperative images](https://arxiv.org/abs/2509.18958)
*Cristina Iacono,Mariarosaria Meola,Federica Conte,Laura Mecozzi,Umberto Bracale,Pietro Falco,Fanny Ficuciello*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的术中胆道定位方法，使用Yolo检测算法和GAN生成合成训练数据，旨在降低腹腔镜胆囊切除术中胆管损伤的风险。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜胆囊切除术虽然恢复快、美容效果好，但胆管损伤风险较高，严重影响患者生活质量和生存率。为改善术中胆道可视化，需要开发可靠的胆道定位技术。

Method: 构建并标注图像数据库训练Yolo检测算法，采用经典数据增强技术，并提出使用生成对抗网络(GAN)生成部分合成训练数据集。

Result: 实验结果表明该方法能够有效定位胆道结构，但具体性能指标未在摘要中详细说明。

Conclusion: 深度学习方法有望改善腹腔镜胆囊切除术中的胆道可视化，降低胆管损伤风险，同时需要关注相关的伦理考量。

Abstract: Cholecystectomy is one of the most frequently performed procedures in gastrointestinal surgery, and the laparoscopic approach is the gold standard for symptomatic cholecystolithiasis and acute cholecystitis. In addition to the advantages of a significantly faster recovery and better cosmetic results, the laparoscopic approach bears a higher risk of bile duct injury, which has a significant impact on quality of life and survival. To avoid bile duct injury, it is essential to improve the intraoperative visualization of the bile duct. This work aims to address this problem by leveraging a deep-learning approach for the localization of the biliary tract from white-light images acquired during the surgical procedures. To this end, the construction and annotation of an image database to train the Yolo detection algorithm has been employed. Besides classical data augmentation techniques, the paper proposes Generative Adversarial Network (GAN) for the generation of a synthetic portion of the training dataset. Experimental results have been discussed along with ethical considerations.

</details>


### [27] [A DyL-Unet framework based on dynamic learning for Temporally Consistent Echocardiographic Segmentation](https://arxiv.org/abs/2509.19052)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: DyL-UNet是一种基于动态学习的时间一致性U-Net分割架构，旨在实现超声心动图分割的时间稳定性和精确性，通过构建回声动态图和使用心脏相位动态注意力机制来增强时间一致性。


<details>
  <summary>Details</summary>
Motivation: 超声心动图容易变形和产生斑点噪声，导致帧间分割抖动，即使单帧分割精度高，时间不稳定性也会削弱功能估计并影响临床解释性。

Method: DyL-UNet通过动态学习构建回声动态图提取视频动态信息，采用多个基于Swin-Transformer的编码器-解码器分支处理单帧图像，并在跳跃连接处引入心脏相位动态注意力机制，利用动态特征和心脏相位线索强制时间一致性。

Result: 在CAMUS和EchoNet-Dynamic数据集上的实验表明，DyL-UNet在保持与现有方法相当的分割精度的同时，实现了更优的时间一致性。

Conclusion: DyL-UNet为自动化临床超声心动图提供了可靠的解决方案，能够实现时间稳定且精确的分割。

Abstract: Accurate segmentation of cardiac anatomy in echocardiography is essential for cardiovascular diagnosis and treatment. Yet echocardiography is prone to deformation and speckle noise, causing frame-to-frame segmentation jitter. Even with high accuracy in single-frame segmentation, temporal instability can weaken functional estimates and impair clinical interpretability. To address these issues, we propose DyL-UNet, a dynamic learning-based temporal consistency U-Net segmentation architecture designed to achieve temporally stable and precise echocardiographic segmentation. The framework constructs an Echo-Dynamics Graph (EDG) through dynamic learning to extract dynamic information from videos. DyL-UNet incorporates multiple Swin-Transformer-based encoder-decoder branches for processing single-frame images. It further introduces Cardiac Phase-Dynamics Attention (CPDA) at the skip connections, which uses EDG-encoded dynamic features and cardiac-phase cues to enforce temporal consistency during segmentation. Extensive experiments on the CAMUS and EchoNet-Dynamic datasets demonstrate that DyL-UNet maintains segmentation accuracy comparable to existing methods while achieving superior temporal consistency, providing a reliable solution for automated clinical echocardiography.

</details>


### [28] [WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction](https://arxiv.org/abs/2509.19073)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: WaveletGaussian是一个用于稀疏视图3D高斯对象重建的高效框架，通过将扩散过程转移到小波域，仅在低分辨率LL子带应用扩散，高频子带使用轻量网络优化，显著减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射在稀疏视图设置下性能急剧下降，现有方法使用扩散模型修复损坏的渲染图像作为伪地面真值，但计算成本高昂。

Method: 提出小波域扩散方法：在低分辨率LL子带应用扩散，高频子带使用轻量网络优化；采用高效的在线随机掩码策略替代低效的留一策略来构建训练对。

Result: 在Mip-NeRF 360和OmniObject3D两个基准数据集上的实验表明，WaveletGaussian在保持竞争性渲染质量的同时大幅减少了训练时间。

Conclusion: WaveletGaussian框架通过小波域的高效扩散策略，成功解决了稀疏视图3D高斯重建的计算效率问题，实现了质量与效率的良好平衡。

Abstract: 3D Gaussian Splatting (3DGS) has become a powerful representation for image-based object reconstruction, yet its performance drops sharply in sparse-view settings. Prior works address this limitation by employing diffusion models to repair corrupted renders, subsequently using them as pseudo ground truths for later optimization. While effective, such approaches incur heavy computation from the diffusion fine-tuning and repair steps. We present WaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object reconstruction. Our key idea is to shift diffusion into the wavelet domain: diffusion is applied only to the low-resolution LL subband, while high-frequency subbands are refined with a lightweight network. We further propose an efficient online random masking strategy to curate training pairs for diffusion fine-tuning, replacing the commonly used, but inefficient, leave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360 and OmniObject3D, show WaveletGaussian achieves competitive rendering quality while substantially reducing training time.

</details>


### [29] [Enabling Plant Phenotyping in Weedy Environments using Multi-Modal Imagery via Synthetic and Generated Training Data](https://arxiv.org/abs/2509.19208)
*Earl Ranario,Ismael Mayanja,Heesup Yun,Brian N. Bailey,J. Mason Earles*

Main category: cs.CV

TL;DR: 提出了一种利用合成RGB图像、少量真实标注和GAN跨模态对齐来增强热图像语义分割的框架，在复杂田间环境中显著提升了植物分割性能。


<details>
  <summary>Details</summary>
Motivation: 热图像中的植物分割在室外高通量表型分析中面临挑战，主要是植物与杂草对比度低和频繁遮挡导致性能不佳。

Method: 使用1,128张合成图像训练模型生成作物和杂草分割掩码，结合少量真实标注图像，采用CycleGAN-turbo进行RGB到热图像的跨模态转换。

Result: 结合所有合成图像和少量真实图像，杂草类和植物类的分割性能相比全真实数据基线分别提升了22%和17%。

Conclusion: 合成数据与有限手动标注以及生成模型的跨域转换相结合，可以显著提升复杂田间环境中多模态图像的语义分割性能。

Abstract: Accurate plant segmentation in thermal imagery remains a significant challenge for high throughput field phenotyping, particularly in outdoor environments where low contrast between plants and weeds and frequent occlusions hinder performance. To address this, we present a framework that leverages synthetic RGB imagery, a limited set of real annotations, and GAN-based cross-modality alignment to enhance semantic segmentation in thermal images. We trained models on 1,128 synthetic images containing complex mixtures of crop and weed plants in order to generate image segmentation masks for crop and weed plants. We additionally evaluated the benefit of integrating as few as five real, manually segmented field images within the training process using various sampling strategies. When combining all the synthetic images with a few labeled real images, we observed a maximum relative improvement of 22% for the weed class and 17% for the plant class compared to the full real-data baseline. Cross-modal alignment was enabled by translating RGB to thermal using CycleGAN-turbo, allowing robust template matching without calibration. Results demonstrated that combining synthetic data with limited manual annotations and cross-domain translation via generative models can significantly boost segmentation performance in complex field environments for multi-model imagery.

</details>


### [30] [DevFD: Developmental Face Forgery Detection by Learning Shared and Orthogonal LoRA Subspaces](https://arxiv.org/abs/2509.19230)
*Tianshuo Zhang,Li Gao,Siran Peng,Xiangyu Zhu,Zhen Lei*

Main category: cs.CV

TL;DR: 该论文提出了一种基于持续学习的数字人脸伪造检测方法，通过发展性混合专家架构来应对不断演变的伪造技术，防止模型遗忘已学习的伪造类型。


<details>
  <summary>Details</summary>
Motivation: 随着数字人脸生成和操纵技术的快速发展，现有的检测模型难以跟上技术迭代速度。需要让模型能够快速适应新领域，同时避免遗忘已学习的伪造类型。

Method: 采用发展性混合专家架构，使用LoRA模型作为专家，分为Real-LoRA学习真实人脸知识，多个Fake-LoRA捕获不同伪造类型的增量信息。通过正交梯度和正交损失防止梯度干扰和灾难性遗忘。

Result: 在数据集和操纵类型增量协议下的实验结果表明该方法具有有效性。

Conclusion: 该方法成功地将人脸伪造检测构建为持续学习问题，能够有效应对不断演变的伪造技术挑战。

Abstract: The rise of realistic digital face generation and manipulation poses significant social risks. The primary challenge lies in the rapid and diverse evolution of generation techniques, which often outstrip the detection capabilities of existing models. To defend against the ever-evolving new types of forgery, we need to enable our model to quickly adapt to new domains with limited computation and data while avoiding forgetting previously learned forgery types. In this work, we posit that genuine facial samples are abundant and relatively stable in acquisition methods, while forgery faces continuously evolve with the iteration of manipulation techniques. Given the practical infeasibility of exhaustively collecting all forgery variants, we frame face forgery detection as a continual learning problem and allow the model to develop as new forgery types emerge. Specifically, we employ a Developmental Mixture of Experts (MoE) architecture that uses LoRA models as its individual experts. These experts are organized into two groups: a Real-LoRA to learn and refine knowledge of real faces, and multiple Fake-LoRAs to capture incremental information from different forgery types. To prevent catastrophic forgetting, we ensure that the learning direction of Fake-LoRAs is orthogonal to the established subspace. Moreover, we integrate orthogonal gradients into the orthogonal loss of Fake-LoRAs, preventing gradient interference throughout the training process of each task. Experimental results under both the datasets and manipulation types incremental protocols demonstrate the effectiveness of our method.

</details>


### [31] [Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2509.19244)
*Shufan Li,Jiuxiang Gu,Kangning Liu,Zhe Lin,Zijun Wei,Aditya Grover,Jason Kuen*

Main category: cs.CV

TL;DR: Lavida-O是一个统一的多模态掩码扩散模型，支持图像理解和生成任务，具备物体定位、图像编辑和高分辨率图像合成等新能力


<details>
  <summary>Details</summary>
Motivation: 现有多模态扩散语言模型仅支持简单的图像级理解任务和低分辨率图像生成，无法满足复杂任务需求

Method: 采用弹性混合Transformer架构、通用文本条件化和分层采样等新技术，通过规划和迭代自反思利用理解能力改进生成结果

Result: 在RefCOCO物体定位、GenEval文本到图像生成和ImgEdit图像编辑等基准测试中达到最先进性能，超越Qwen2.5-VL和FluxKontext-dev等模型

Conclusion: Lavida-O是首个统一的掩码扩散模型，通过理解能力提升生成质量，同时在推理时提供显著加速

Abstract: We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM) capable of image understanding and generation tasks. Unlike existing multimodal diffsion language models such as MMaDa and Muddit which only support simple image-level understanding tasks and low-resolution image generation, Lavida-O exhibits many new capabilities such as object grounding, image-editing, and high-resolution (1024px) image synthesis. It is also the first unified MDM that uses its understanding capabilities to improve image generation and editing results through planning and iterative self-reflection. To allow effective and efficient training and sampling, Lavida-O ntroduces many novel techniques such as Elastic Mixture-of-Transformer architecture, universal text conditioning, and stratified sampling. \ours~achieves state-of-the-art performance on a wide range of benchmarks such as RefCOCO object grounding, GenEval text-to-image generation, and ImgEdit image editing, outperforming existing autoregressive and continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while offering considerable speedup at inference.

</details>


### [32] [Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps](https://arxiv.org/abs/2509.19252)
*Gabriel Maldonado,Narges Rashvand,Armin Danesh Pazho,Ghazal Alinezhad Noghre,Vinit Katariya,Hamed Tabkhi*

Main category: cs.CV

TL;DR: 提出了一种基于对抗性精炼的VQ-GAN框架，通过密集运动标记化技术压缩时空热图，有效解决人体运动理解中的高维度和冗余问题。


<details>
  <summary>Details</summary>
Motivation: 连续人体运动理解在计算机视觉中面临高维度和内在冗余的挑战，需要高效的压缩和表示方法来分析复杂的运动动态。

Method: 结合密集运动标记化和对抗性精炼的VQ-GAN框架，消除重建伪影如运动模糊和时间错位。

Result: 在CMU Panoptic数据集上，方法比dVAE基线在SSIM指标上提升9.31%，时间不稳定性降低37.1%。发现2D运动可用128个标记词汇表表示，3D运动需要1024个标记代码本。

Conclusion: 该方法为各种运动分析应用提供了实际部署的可行性，证明了密集标记化策略在运动复杂度分析中的有效性。

Abstract: Continuous human motion understanding remains a core challenge in computer vision due to its high dimensionality and inherent redundancy. Efficient compression and representation are crucial for analyzing complex motion dynamics. In this work, we introduce an adversarially-refined VQ-GAN framework with dense motion tokenization for compressing spatio-temporal heatmaps while preserving the fine-grained traces of human motion. Our approach combines dense motion tokenization with adversarial refinement, which eliminates reconstruction artifacts like motion smearing and temporal misalignment observed in non-adversarial baselines. Our experiments on the CMU Panoptic dataset provide conclusive evidence of our method's superiority, outperforming the dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%. Furthermore, our dense tokenization strategy enables a novel analysis of motion complexity, revealing that 2D motion can be optimally represented with a compact 128-token vocabulary, while 3D motion's complexity demands a much larger 1024-token codebook for faithful reconstruction. These results establish practical deployment feasibility across diverse motion analysis applications. The code base for this work is available at https://github.com/TeCSAR-UNCC/Pose-Quantization.

</details>


### [33] [OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps](https://arxiv.org/abs/2509.19282)
*Bingnan Li,Chen-Yu Wang,Haiyang Xu,Xiang Zhang,Ethan Armand,Divyansh Srivastava,Xiaojun Shan,Zeyuan Chen,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: 该论文提出了OverLayScore指标和OverLayBench基准，用于评估和处理布局到图像生成中边界框重叠的挑战性问题。


<details>
  <summary>Details</summary>
Motivation: 当前布局到图像生成方法在处理边界框显著重叠的布局时表现不佳，现有基准偏向于简单重叠情况，无法有效评估模型在复杂重叠场景下的性能。

Method: 1）提出OverLayScore指标量化边界框重叠复杂度；2）构建OverLayBench基准，包含高质量标注和平衡的OverLayScore分布；3）提出CreatiLayout-AM模型，在amodal掩码数据集上进行微调。

Result: 分析表明现有基准存在偏向低OverLayScore值的偏差，限制了评估模型在挑战性条件下的有效性。提出的OverLayBench提供了更全面的评估框架。

Conclusion: 该研究为在现实和挑战性场景下实现更稳健的布局到图像生成奠定了基础，通过新指标和基准推动了该领域的发展。

Abstract: Despite steady progress in layout-to-image generation, current methods still struggle with layouts containing significant overlap between bounding boxes. We identify two primary challenges: (1) large overlapping regions and (2) overlapping instances with minimal semantic distinction. Through both qualitative examples and quantitative analysis, we demonstrate how these factors degrade generation quality. To systematically assess this issue, we introduce OverLayScore, a novel metric that quantifies the complexity of overlapping bounding boxes. Our analysis reveals that existing benchmarks are biased toward simpler cases with low OverLayScore values, limiting their effectiveness in evaluating model performance under more challenging conditions. To bridge this gap, we present OverLayBench, a new benchmark featuring high-quality annotations and a balanced distribution across different levels of OverLayScore. As an initial step toward improving performance on complex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a curated amodal mask dataset. Together, our contributions lay the groundwork for more robust layout-to-image generation under realistic and challenging scenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.

</details>


### [34] [VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction](https://arxiv.org/abs/2509.19297)
*Weijie Wang,Yeqing Chen,Zeyu Zhang,Hengyu Liu,Haoxiao Wang,Zhiyuan Feng,Wenkang Qin,Zheng Zhu,Donny Y. Chen,Bohan Zhuang*

Main category: cs.CV

TL;DR: VolSplat提出了一种新的多视角前馈3D高斯溅射方法，用体素对齐的高斯预测取代像素对齐方法，解决了现有方法对输入视角数量的依赖、视角偏差密度分布和对齐误差等问题。


<details>
  <summary>Details</summary>
Motivation: 现有的像素对齐3D高斯预测方法存在对输入视角数量的依赖、视角偏差密度分布、以及对遮挡和低纹理区域的对齐误差等固有局限性。

Method: VolSplat采用体素对齐的高斯预测范式，直接从预测的3D体素网格预测高斯分布，避免了基于2D特征匹配的像素对齐方法的问题。

Result: 在RealEstate10K和ScanNet等基准测试中，VolSplat实现了最先进的性能，产生更可信和视角一致的3D高斯重建，并提高了新视角渲染质量。

Conclusion: VolSplat为前馈3D重建建立了一个更可扩展的框架，提供了更密集和更鲁棒的表示，为更广泛社区的研究铺平了道路。

Abstract: Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective solution for novel view synthesis. Existing methods predominantly rely on a pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a 3D Gaussian. We rethink this widely adopted formulation and identify several inherent limitations: it renders the reconstructed 3D models heavily dependent on the number of input views, leads to view-biased density distributions, and introduces alignment errors, particularly when source views contain occlusions or low texture. To address these challenges, we introduce VolSplat, a new multi-view feed-forward paradigm that replaces pixel alignment with voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature matching, ensuring robust multi-view consistency. Furthermore, it enables adaptive control over Gaussian density based on 3D scene complexity, yielding more faithful Gaussian point clouds, improved geometric consistency, and enhanced novel-view rendering quality. Experiments on widely used benchmarks including RealEstate10K and ScanNet demonstrate that VolSplat achieves state-of-the-art performance while producing more plausible and view-consistent Gaussian reconstructions. In addition to superior results, our approach establishes a more scalable framework for feed-forward 3D reconstruction with denser and more robust representations, paving the way for further research in wider communities. The video results, code and trained models are available on our project page: https://lhmd.top/volsplat.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [35] [Latent Danger Zone: Distilling Unified Attention for Cross-Architecture Black-box Attacks](https://arxiv.org/abs/2509.19044)
*Yang Li,Chenyu Wang,Tingrui Wang,Yongwei Wang,Haonan Li,Zhunga Liu,Quan Pan*

Main category: cs.LG

TL;DR: JAD是一个基于潜在扩散模型的黑盒对抗攻击框架，通过联合注意力蒸馏策略实现跨架构的对抗样本生成，具有更好的泛化性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有黑盒对抗攻击方法依赖特定网络架构、查询次数多、跨架构迁移性差的问题。

Method: 使用潜在扩散模型，通过从CNN和ViT模型中蒸馏注意力图来指导对抗样本生成，聚焦于跨架构敏感的图像区域。

Result: JAD在攻击泛化性、生成效率和跨架构迁移性方面优于现有方法。

Conclusion: JAD为黑盒对抗攻击提供了一个有前景的有效范式，具有架构无关性和高生成效率。

Abstract: Black-box adversarial attacks remain challenging due to limited access to model internals. Existing methods often depend on specific network architectures or require numerous queries, resulting in limited cross-architecture transferability and high query costs. To address these limitations, we propose JAD, a latent diffusion model framework for black-box adversarial attacks. JAD generates adversarial examples by leveraging a latent diffusion model guided by attention maps distilled from both a convolutional neural network (CNN) and a Vision Transformer (ViT) models. By focusing on image regions that are commonly sensitive across architectures, this approach crafts adversarial perturbations that transfer effectively between different model types. This joint attention distillation strategy enables JAD to be architecture-agnostic, achieving superior attack generalization across diverse models. Moreover, the generative nature of the diffusion framework yields high adversarial sample generation efficiency by reducing reliance on iterative queries. Experiments demonstrate that JAD offers improved attack generalization, generation efficiency, and cross-architecture transferability compared to existing methods, providing a promising and effective paradigm for black-box adversarial attacks.

</details>
