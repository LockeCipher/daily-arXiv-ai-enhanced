{"id": "2507.08285", "pdf": "https://arxiv.org/pdf/2507.08285", "abs": "https://arxiv.org/abs/2507.08285", "authors": ["Gwanhyeong Koo", "Sunjae Yoon", "Younghwan Lee", "Ji Woo Hong", "Chang D. Yoo"], "title": "FlowDrag: 3D-aware Drag-based Image Editing with Mesh-guided Deformation Vector Flow Fields", "categories": ["cs.GR", "cs.CV"], "comment": "ICML 2025 Spotlight", "summary": "Drag-based editing allows precise object manipulation through point-based control, offering user convenience. However, current methods often suffer from a geometric inconsistency problem by focusing exclusively on matching user-defined points, neglecting the broader geometry and leading to artifacts or unstable edits. We propose FlowDrag, which leverages geometric information for more accurate and coherent transformations. Our approach constructs a 3D mesh from the image, using an energy function to guide mesh deformation based on user-defined drag points. The resulting mesh displacements are projected into 2D and incorporated into a UNet denoising process, enabling precise handle-to-target point alignment while preserving structural integrity. Additionally, existing drag-editing benchmarks provide no ground truth, making it difficult to assess how accurately the edits match the intended transformations. To address this, we present VFD (VidFrameDrag) benchmark dataset, which provides ground-truth frames using consecutive shots in a video dataset. FlowDrag outperforms existing drag-based editing methods on both VFD Bench and DragBench.", "AI": {"tldr": "FlowDrag\u5229\u7528\u51e0\u4f55\u4fe1\u606f\u6539\u8fdb\u62d6\u62fd\u7f16\u8f91\uff0c\u901a\u8fc73D\u7f51\u683c\u53d8\u5f62\u548cUNet\u53bb\u566a\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u7f16\u8f91\uff0c\u5e76\u63d0\u51fa\u4e86\u5e26\u771f\u5b9e\u6570\u636e\u7684VFD\u57fa\u51c6\u3002", "motivation": "\u5f53\u524d\u62d6\u62fd\u7f16\u8f91\u65b9\u6cd5\u56e0\u4ec5\u5173\u6ce8\u7528\u6237\u5b9a\u4e49\u70b9\u800c\u5ffd\u7565\u6574\u4f53\u51e0\u4f55\uff0c\u5bfc\u81f4\u4e0d\u4e00\u81f4\u6027\u548c\u4f2a\u5f71\u3002", "method": "\u6784\u5efa3D\u7f51\u683c\uff0c\u901a\u8fc7\u80fd\u91cf\u51fd\u6570\u6307\u5bfc\u53d8\u5f62\uff0c\u7ed3\u5408UNet\u53bb\u566a\u5b9e\u73b02D\u6295\u5f71\u3002", "result": "FlowDrag\u5728VFD Bench\u548cDragBench\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FlowDrag\u901a\u8fc7\u51e0\u4f55\u4fe1\u606f\u548c\u771f\u5b9e\u57fa\u51c6\u63d0\u5347\u4e86\u62d6\u62fd\u7f16\u8f91\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2507.08513", "pdf": "https://arxiv.org/pdf/2507.08513", "abs": "https://arxiv.org/abs/2507.08513", "authors": ["Liu He", "Xiao Zeng", "Yizhi Song", "Albert Y. C. Chen", "Lu Xia", "Shashwat Verma", "Sankalp Dayal", "Min Sun", "Cheng-Hao Kuo", "Daniel Aliaga"], "title": "Advancing Multimodal LLMs by Large-Scale 3D Visual Instruction Dataset Generation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) struggle with accurately capturing camera-object relations, especially for object orientation, camera viewpoint, and camera shots. This stems from the fact that existing MLLMs are trained on images with limited diverse camera-object relations and corresponding textual descriptions. To address this, we propose a synthetic generation pipeline to create large-scale 3D visual instruction datasets. Our framework takes 3D assets as input and uses rendering and diffusion-based image generation models to create photorealistic images preserving precise camera-object relations. Additionally, large language models (LLMs) are used to generate text prompts for guiding visual instruction tuning and controlling image generation. We create Ultimate3D, a dataset of 240K VQAs with precise camera-object annotations, and corresponding benchmark. MLLMs fine-tuned on our proposed dataset outperform commercial models by a large margin, achieving an average accuracy improvement of 33.4% on camera-object relation recognition tasks. Our code, dataset, and benchmark will contribute to broad MLLM applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5408\u6210\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u521b\u5efa\u5927\u89c4\u6a213D\u89c6\u89c9\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u4ee5\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u6355\u6349\u76f8\u673a-\u7269\u4f53\u5173\u7cfb\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u76f8\u673a-\u7269\u4f53\u5173\u7cfb\uff08\u5982\u7269\u4f53\u65b9\u5411\u3001\u76f8\u673a\u89c6\u89d2\u548c\u955c\u5934\uff09\u7684\u51c6\u786e\u6355\u6349\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u4e3a\u8bad\u7ec3\u6570\u636e\u7684\u591a\u6837\u6027\u548c\u6587\u672c\u63cf\u8ff0\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5408\u6210\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u5229\u75283D\u8d44\u4ea7\u3001\u6e32\u67d3\u548c\u6269\u6563\u6a21\u578b\u751f\u6210\u903c\u771f\u56fe\u50cf\uff0c\u5e76\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u6587\u672c\u63d0\u793a\uff0c\u6784\u5efa\u4e86Ultimate3D\u6570\u636e\u96c6\uff08240K VQAs\uff09\u3002", "result": "\u5728\u76f8\u673a-\u7269\u4f53\u5173\u7cfb\u8bc6\u522b\u4efb\u52a1\u4e0a\uff0c\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\u5fae\u8c03\u7684MLLMs\u6bd4\u5546\u4e1a\u6a21\u578b\u5e73\u5747\u51c6\u786e\u7387\u63d0\u9ad8\u4e8633.4%\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86MLLMs\u7684\u6027\u80fd\uff0c\u5176\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548c\u57fa\u51c6\u5c06\u4e3a\u5e7f\u6cdbMLLM\u5e94\u7528\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2507.08039", "pdf": "https://arxiv.org/pdf/2507.08039", "abs": "https://arxiv.org/abs/2507.08039", "authors": ["Sujith Vemishetty", "Advitiya Arora", "Anupama Sharma"], "title": "Towards Evaluating Robustness of Prompt Adherence in Text to Image Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The advancements in the domain of LLMs in recent years have surprised many, showcasing their remarkable capabilities and diverse applications. Their potential applications in various real-world scenarios have led to significant research on their reliability and effectiveness. On the other hand, multimodal LLMs and Text-to-Image models have only recently gained prominence, especially when compared to text-only LLMs. Their reliability remains constrained due to insufficient research on assessing their performance and robustness. This paper aims to establish a comprehensive evaluation framework for Text-to-Image models, concentrating particularly on their adherence to prompts. We created a novel dataset that aimed to assess the robustness of these models in generating images that conform to the specified factors of variation in the input text prompts. Our evaluation studies present findings on three variants of Stable Diffusion models: Stable Diffusion 3 Medium, Stable Diffusion 3.5 Large, and Stable Diffusion 3.5 Large Turbo, and two variants of Janus models: Janus Pro 1B and Janus Pro 7B. We introduce a pipeline that leverages text descriptions generated by the gpt-4o model for our ground-truth images, which are then used to generate artificial images by passing these descriptions to the Text-to-Image models. We then pass these generated images again through gpt-4o using the same system prompt and compare the variation between the two descriptions. Our results reveal that these models struggle to create simple binary images with only two factors of variation: a simple geometric shape and its location. We also show, using pre-trained VAEs on our dataset, that they fail to generate images that follow our input dataset distribution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u5bf9\u63d0\u793a\u7684\u9075\u5faa\u80fd\u529b\uff0c\u5e76\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u751f\u6210\u7b80\u5355\u4e8c\u8fdb\u5236\u56fe\u50cf\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u591a\u6a21\u6001LLM\u548c\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u7814\u7a76\u76f8\u5bf9\u4e0d\u8db3\uff0c\u5176\u53ef\u9760\u6027\u548c\u6027\u80fd\u8bc4\u4f30\u7f3a\u4e4f\u7cfb\u7edf\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u521b\u5efa\u65b0\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u7ba1\u9053\uff0c\u5229\u7528gpt-4o\u751f\u6210\u6587\u672c\u63cf\u8ff0\u4f5c\u4e3a\u57fa\u51c6\uff0c\u518d\u901a\u8fc7\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u751f\u6210\u56fe\u50cf\uff0c\u6700\u540e\u6bd4\u8f83\u63cf\u8ff0\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u5728\u751f\u6210\u4ec5\u5305\u542b\u4e24\u4e2a\u53d8\u5316\u56e0\u7d20\u7684\u7b80\u5355\u4e8c\u8fdb\u5236\u56fe\u50cf\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u65e0\u6cd5\u9075\u5faa\u8f93\u5165\u6570\u636e\u96c6\u7684\u5206\u5e03\u3002", "conclusion": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u9075\u5faa\u63d0\u793a\u548c\u751f\u6210\u7279\u5b9a\u5206\u5e03\u56fe\u50cf\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2507.08044", "pdf": "https://arxiv.org/pdf/2507.08044", "abs": "https://arxiv.org/abs/2507.08044", "authors": ["Debasmit Das", "Hyoungwoo Park", "Munawar Hayat", "Seokeon Choi", "Sungrack Yun", "Fatih Porikli"], "title": "ConsNoTrainLoRA: Data-driven Weight Initialization of Low-rank Adapters using Constraints", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV 2025", "summary": "Foundation models are pre-trained on large-scale datasets and subsequently fine-tuned on small-scale datasets using parameter-efficient fine-tuning (PEFT) techniques like low-rank adapters (LoRA). In most previous works, LoRA weight matrices are randomly initialized with a fixed rank across all attachment points. In this paper, we improve convergence and final performance of LoRA fine-tuning, using our proposed data-driven weight initialization method, ConsNoTrainLoRA (CNTLoRA). We express LoRA initialization as a domain shift problem where we use multiple constraints relating the pre-training and fine-tuning activations. By reformulating these constraints, we obtain a closed-form estimate of LoRA weights that depends on pre-training weights and fine-tuning activation vectors and hence requires no training during initialization. This weight estimate is decomposed to initialize the up and down matrices with proposed flexibility of variable ranks. With the proposed initialization method, we fine-tune on downstream tasks such as image generation, image classification and image understanding. Both quantitative and qualitative results demonstrate that CNTLoRA outperforms standard and data-driven weight initialization methods. Extensive analyses and ablations further elucidate the design choices of our framework, providing an optimal recipe for faster convergence and enhanced performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCNTLoRA\u7684\u6570\u636e\u9a71\u52a8\u6743\u91cd\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdbLoRA\u5fae\u8c03\u7684\u6536\u655b\u6027\u548c\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u9884\u8bad\u7ec3\u6743\u91cd\u548c\u5fae\u8c03\u6fc0\u6d3b\u5411\u91cf\u7684\u95ed\u5f0f\u4f30\u8ba1\u521d\u59cb\u5316LoRA\u6743\u91cd\uff0c\u65e0\u9700\u8bad\u7ec3\uff0c\u4e14\u652f\u6301\u53ef\u53d8\u79e9\u77e9\u9635\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6807\u51c6\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfLoRA\u6743\u91cd\u77e9\u9635\u968f\u673a\u521d\u59cb\u5316\u4e14\u56fa\u5b9a\u79e9\uff0c\u9650\u5236\u4e86\u5fae\u8c03\u7684\u6027\u80fd\u548c\u6536\u655b\u901f\u5ea6\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06LoRA\u521d\u59cb\u5316\u89c6\u4e3a\u57df\u504f\u79fb\u95ee\u9898\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u6fc0\u6d3b\u4e4b\u95f4\u7684\u7ea6\u675f\u5173\u7cfb\uff0c\u63a8\u5bfc\u51fa\u95ed\u5f0f\u6743\u91cd\u4f30\u8ba1\uff0c\u5e76\u652f\u6301\u53ef\u53d8\u79e9\u77e9\u9635\u521d\u59cb\u5316\u3002", "result": "\u5728\u56fe\u50cf\u751f\u6210\u3001\u5206\u7c7b\u548c\u7406\u89e3\u7b49\u4efb\u52a1\u4e2d\uff0cCNTLoRA\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u4e0a\u5747\u4f18\u4e8e\u6807\u51c6\u53ca\u6570\u636e\u9a71\u52a8\u521d\u59cb\u5316\u65b9\u6cd5\u3002", "conclusion": "CNTLoRA\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u521d\u59cb\u5316\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86LoRA\u5fae\u8c03\u7684\u6027\u80fd\u548c\u6536\u655b\u901f\u5ea6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.08059", "pdf": "https://arxiv.org/pdf/2507.08059", "abs": "https://arxiv.org/abs/2507.08059", "authors": ["F. Alberto Gr\u00fcnbaum", "Tondgi Xu"], "title": "The relative importance of being Gaussian", "categories": ["cs.CV", "math.PR", "68T05, 68T45, 60J60, 82C22, 82C31"], "comment": null, "summary": "The remarkable results for denoising in computer vision using diffusion models given in \\cite{SDWMG,HJA,HHG} yield a robust mathematical justification for algorithms based on crucial properties of a sequence of Gaussian independent $N(0,1)$ random variables. In particular the derivations use the fact that a Gaussian distribution is determined by its mean and variance and that the sum of two Gaussians is another Gaussian.   \\bigskip   The issue raised in this short note is the following: suppose we use the algorithm without any changes but replace the nature of the noise and use, for instance, uniformly distributed noise or noise with a Beta distribution, or noise which is a random superposition of two Gaussians with very different variances. One could, of course, try to modify the algorithm keeping in mind the nature of the noise, but this is not what we do. Instead we study the performance of the algorithm when used with noise that is very far in nature from the Gaussian case, where it is designed to work well.   Usually these algorithms are implemented on very powerful computers. Our experiments are all carried out on a small laptop and for the smallest possible image size. Exploring how our observations are confirmed or changed when dealing in different situations remains an interesting challenge.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u975e\u9ad8\u65af\u566a\u58f0\u6761\u4ef6\u4e0b\u4f7f\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u53bb\u566a\u7684\u6027\u80fd\u8868\u73b0\uff0c\u800c\u975e\u4fee\u6539\u7b97\u6cd5\u4ee5\u9002\u5e94\u566a\u58f0\u7c7b\u578b\u3002", "motivation": "\u7814\u7a76\u6269\u6563\u6a21\u578b\u5728\u975e\u9ad8\u65af\u566a\u58f0\uff08\u5982\u5747\u5300\u5206\u5e03\u6216Beta\u5206\u5e03\uff09\u4e0b\u7684\u8868\u73b0\uff0c\u800c\u975e\u9488\u5bf9\u566a\u58f0\u7c7b\u578b\u8c03\u6574\u7b97\u6cd5\u3002", "method": "\u4f7f\u7528\u539f\u59cb\u7b97\u6cd5\uff0c\u4f46\u66ff\u6362\u9ad8\u65af\u566a\u58f0\u4e3a\u5176\u4ed6\u7c7b\u578b\uff08\u5982\u5747\u5300\u5206\u5e03\u3001Beta\u5206\u5e03\u6216\u6df7\u5408\u9ad8\u65af\u566a\u58f0\uff09\uff0c\u5e76\u5728\u5c0f\u578b\u8bbe\u5907\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u7b97\u6cd5\u5728\u975e\u9ad8\u65af\u566a\u58f0\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4f46\u672a\u5177\u4f53\u8bf4\u660e\u7ed3\u679c\u4f18\u52a3\u3002", "conclusion": "\u63a2\u7d22\u4e0d\u540c\u566a\u58f0\u6761\u4ef6\u4e0b\u7b97\u6cd5\u7684\u8868\u73b0\u662f\u4e00\u4e2a\u6709\u8da3\u7684\u6311\u6218\uff0c\u672a\u6765\u53ef\u5728\u4e0d\u540c\u60c5\u5883\u4e0b\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002"}}
{"id": "2507.08096", "pdf": "https://arxiv.org/pdf/2507.08096", "abs": "https://arxiv.org/abs/2507.08096", "authors": ["Babak Memar", "Luigi Russo", "Silvia Liberata Ullo", "Paolo Gamba"], "title": "An Object-Based Deep Learning Approach for Building Height Estimation from Single SAR Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate estimation of building heights using very high resolution (VHR) synthetic aperture radar (SAR) imagery is crucial for various urban applications. This paper introduces a Deep Learning (DL)-based methodology for automated building height estimation from single VHR COSMO-SkyMed images: an object-based regression approach based on bounding box detection followed by height estimation. This model was trained and evaluated on a unique multi-continental dataset comprising eight geographically diverse cities across Europe, North and South America, and Asia, employing a cross-validation strategy to explicitly assess out-of-distribution (OOD) generalization. The results demonstrate highly promising performance, particularly on European cities where the model achieves a Mean Absolute Error (MAE) of approximately one building story (2.20 m in Munich), significantly outperforming recent state-of-the-art methods in similar OOD scenarios. Despite the increased variability observed when generalizing to cities in other continents, particularly in Asia with its distinct urban typologies and prevalence of high-rise structures, this study underscores the significant potential of DL for robust cross-city and cross-continental transfer learning in building height estimation from single VHR SAR data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u5355\u5f20\u8d85\u9ad8\u5206\u8fa8\u7387SAR\u56fe\u50cf\u4f30\u7b97\u5efa\u7b51\u7269\u9ad8\u5ea6\uff0c\u5e76\u5728\u591a\u5927\u9646\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u51c6\u786e\u7684\u5efa\u7b51\u7269\u9ad8\u5ea6\u4f30\u7b97\u5bf9\u57ce\u5e02\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u57ce\u5e02\u548c\u8de8\u5927\u9646\u7684\u6cdb\u5316\u80fd\u529b\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8fb9\u754c\u6846\u68c0\u6d4b\u548c\u76ee\u6807\u56de\u5f52\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5229\u7528\u5355\u5f20VHR COSMO-SkyMed\u56fe\u50cf\u8fdb\u884c\u9ad8\u5ea6\u4f30\u7b97\u3002", "result": "\u6a21\u578b\u5728\u6b27\u6d32\u57ce\u5e02\u8868\u73b0\u4f18\u5f02\uff08MAE\u7ea62.20\u7c73\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4f46\u5728\u4e9a\u6d32\u57ce\u5e02\u56e0\u9ad8\u697c\u548c\u57ce\u5e02\u7c7b\u578b\u5dee\u5f02\u6cdb\u5316\u80fd\u529b\u6709\u6240\u4e0b\u964d\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u5728\u5355\u5f20VHR SAR\u6570\u636e\u7684\u8de8\u57ce\u5e02\u548c\u8de8\u5927\u9646\u5efa\u7b51\u7269\u9ad8\u5ea6\u4f30\u7b97\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\u3002"}}
{"id": "2507.08136", "pdf": "https://arxiv.org/pdf/2507.08136", "abs": "https://arxiv.org/abs/2507.08136", "authors": ["Chong Cheng", "Yu Hu", "Sicheng Yu", "Beizhen Zhao", "Zijian Wang", "Hao Wang"], "title": "RegGS: Unposed Sparse Views Gaussian Splatting with 3DGS Registration", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "3D Gaussian Splatting (3DGS) has demonstrated its potential in reconstructing scenes from unposed images. However, optimization-based 3DGS methods struggle with sparse views due to limited prior knowledge. Meanwhile, feed-forward Gaussian approaches are constrained by input formats, making it challenging to incorporate more input views. To address these challenges, we propose RegGS, a 3D Gaussian registration-based framework for reconstructing unposed sparse views. RegGS aligns local 3D Gaussians generated by a feed-forward network into a globally consistent 3D Gaussian representation. Technically, we implement an entropy-regularized Sinkhorn algorithm to efficiently solve the optimal transport Mixture 2-Wasserstein $(\\text{MW}_2)$ distance, which serves as an alignment metric for Gaussian mixture models (GMMs) in $\\mathrm{Sim}(3)$ space. Furthermore, we design a joint 3DGS registration module that integrates the $\\text{MW}_2$ distance, photometric consistency, and depth geometry. This enables a coarse-to-fine registration process while accurately estimating camera poses and aligning the scene. Experiments on the RE10K and ACID datasets demonstrate that RegGS effectively registers local Gaussians with high fidelity, achieving precise pose estimation and high-quality novel-view synthesis. Project page: https://3dagentworld.github.io/reggs/.", "AI": {"tldr": "RegGS\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6ce8\u518c\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u91cd\u5efa\u65e0\u59ff\u6001\u7a00\u758f\u89c6\u56fe\uff0c\u901a\u8fc7\u71b5\u6b63\u5219\u5316\u7684Sinkhorn\u7b97\u6cd5\u548c\u8054\u54083DGS\u6ce8\u518c\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u7684\u5c40\u90e8\u9ad8\u65af\u5bf9\u9f50\u548c\u9ad8\u8d28\u91cf\u65b0\u89c6\u89d2\u5408\u6210\u3002", "motivation": "\u89e3\u51b3\u4f18\u5316\u578b3DGS\u65b9\u6cd5\u5728\u7a00\u758f\u89c6\u56fe\u4e0b\u56e0\u5148\u9a8c\u77e5\u8bc6\u4e0d\u8db3\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u4ee5\u53ca\u524d\u9988\u9ad8\u65af\u65b9\u6cd5\u56e0\u8f93\u5165\u683c\u5f0f\u9650\u5236\u96be\u4ee5\u6269\u5c55\u66f4\u591a\u89c6\u56fe\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u71b5\u6b63\u5219\u5316\u7684Sinkhorn\u7b97\u6cd5\u8ba1\u7b97\u6700\u4f18\u4f20\u8f93Mixture 2-Wasserstein\u8ddd\u79bb\uff0c\u4f5c\u4e3aGMM\u5bf9\u9f50\u5ea6\u91cf\uff0c\u5e76\u7ed3\u5408\u5149\u5ea6\u4e00\u81f4\u6027\u548c\u6df1\u5ea6\u51e0\u4f55\u8bbe\u8ba1\u8054\u54083DGS\u6ce8\u518c\u6a21\u5757\u3002", "result": "\u5728RE10K\u548cACID\u6570\u636e\u96c6\u4e0a\uff0cRegGS\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u7684\u5c40\u90e8\u9ad8\u65af\u5bf9\u9f50\uff0c\u7cbe\u786e\u7684\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c\u9ad8\u8d28\u91cf\u7684\u65b0\u89c6\u89d2\u5408\u6210\u3002", "conclusion": "RegGS\u901a\u8fc7\u5168\u5c40\u4e00\u81f4\u76843D\u9ad8\u65af\u8868\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u7684\u6311\u6218\uff0c\u4e3a\u65e0\u59ff\u6001\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.08137", "pdf": "https://arxiv.org/pdf/2507.08137", "abs": "https://arxiv.org/abs/2507.08137", "authors": ["Hyungjun Doh", "Dong In Lee", "Seunggeun Chi", "Pin-Hao Huang", "Kwonjoon Lee", "Sangpil Kim", "Karthik Ramani"], "title": "Temporally Consistent Amodal Completion for 3D Human-Object Interaction Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce a novel framework for reconstructing dynamic human-object interactions from monocular video that overcomes challenges associated with occlusions and temporal inconsistencies. Traditional 3D reconstruction methods typically assume static objects or full visibility of dynamic subjects, leading to degraded performance when these assumptions are violated-particularly in scenarios where mutual occlusions occur. To address this, our framework leverages amodal completion to infer the complete structure of partially obscured regions. Unlike conventional approaches that operate on individual frames, our method integrates temporal context, enforcing coherence across video sequences to incrementally refine and stabilize reconstructions. This template-free strategy adapts to varying conditions without relying on predefined models, significantly enhancing the recovery of intricate details in dynamic scenes. We validate our approach using 3D Gaussian Splatting on challenging monocular videos, demonstrating superior precision in handling occlusions and maintaining temporal stability compared to existing techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5355\u76ee\u89c6\u9891\u91cd\u5efa\u52a8\u6001\u4eba-\u7269\u4ea4\u4e92\u7684\u65b0\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u906e\u6321\u548c\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf3D\u91cd\u5efa\u65b9\u6cd5\u5047\u8bbe\u9759\u6001\u7269\u4f53\u6216\u52a8\u6001\u4e3b\u4f53\u5b8c\u5168\u53ef\u89c1\uff0c\u5728\u906e\u6321\u573a\u666f\u4e0b\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5229\u7528\u6a21\u6001\u8865\u5168\u63a8\u65ad\u906e\u6321\u533a\u57df\u7684\u5b8c\u6574\u7ed3\u6784\uff0c\u7ed3\u5408\u65f6\u95f4\u4e0a\u4e0b\u6587\u589e\u5f3a\u89c6\u9891\u5e8f\u5217\u7684\u4e00\u81f4\u6027\uff0c\u65e0\u9700\u6a21\u677f\u3002", "result": "\u5728\u5355\u76ee\u89c6\u9891\u4e0a\u9a8c\u8bc1\uff0c\u5904\u7406\u906e\u6321\u548c\u65f6\u95f4\u7a33\u5b9a\u6027\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u573a\u666f\u4e2d\u590d\u6742\u7ec6\u8282\u7684\u6062\u590d\u80fd\u529b\u3002"}}
{"id": "2507.08163", "pdf": "https://arxiv.org/pdf/2507.08163", "abs": "https://arxiv.org/abs/2507.08163", "authors": ["Frederick Shpilevskiy", "Saiyue Lyu", "Krishnamurthy Dj Dvijotham", "Mathias L\u00e9cuyer", "Pierre-Andr\u00e9 No\u00ebl"], "title": "Adaptive Diffusion Denoised Smoothing : Certified Robustness via Randomized Smoothing with Differentially Private Guided Denoising Diffusion", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": null, "summary": "We propose Adaptive Diffusion Denoised Smoothing, a method for certifying the predictions of a vision model against adversarial examples, while adapting to the input. Our key insight is to reinterpret a guided denoising diffusion model as a long sequence of adaptive Gaussian Differentially Private (GDP) mechanisms refining a pure noise sample into an image. We show that these adaptive mechanisms can be composed through a GDP privacy filter to analyze the end-to-end robustness of the guided denoising process, yielding a provable certification that extends the adaptive randomized smoothing analysis. We demonstrate that our design, under a specific guiding strategy, can improve both certified accuracy and standard accuracy on ImageNet for an $\\ell_2$ threat model.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.08205", "pdf": "https://arxiv.org/pdf/2507.08205", "abs": "https://arxiv.org/abs/2507.08205", "authors": ["Ken C. L. Wong", "Hongzhi Wang", "Tanveer Syeda-Mahmood"], "title": "HNOSeg-XS: Extremely Small Hartley Neural Operator for Efficient and Resolution-Robust 3D Image Segmentation", "categories": ["cs.CV"], "comment": "This paper was accepted by IEEE TMI 2025", "summary": "In medical image segmentation, convolutional neural networks (CNNs) and transformers are dominant. For CNNs, given the local receptive fields of convolutional layers, long-range spatial correlations are captured through consecutive convolutions and pooling. However, as the computational cost and memory footprint can be prohibitively large, 3D models can only afford fewer layers than 2D models with reduced receptive fields and abstract levels. For transformers, although long-range correlations can be captured by multi-head attention, its quadratic complexity with respect to input size is computationally demanding. Therefore, either model may require input size reduction to allow more filters and layers for better segmentation. Nevertheless, given their discrete nature, models trained with patch-wise training or image downsampling may produce suboptimal results when applied on higher resolutions. To address this issue, here we propose the resolution-robust HNOSeg-XS architecture. We model image segmentation by learnable partial differential equations through the Fourier neural operator which has the zero-shot super-resolution property. By replacing the Fourier transform by the Hartley transform and reformulating the problem in the frequency domain, we created the HNOSeg-XS model, which is resolution robust, fast, memory efficient, and extremely parameter efficient. When tested on the BraTS'23, KiTS'23, and MVSeg'23 datasets with a Tesla V100 GPU, HNOSeg-XS showed its superior resolution robustness with fewer than 34.7k model parameters. It also achieved the overall best inference time (< 0.24 s) and memory efficiency (< 1.8 GiB) compared to the tested CNN and transformer models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHartley\u53d8\u6362\u7684HNOSeg-XS\u67b6\u6784\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u5177\u6709\u5206\u8fa8\u7387\u9c81\u68d2\u6027\u3001\u9ad8\u6548\u6027\u548c\u53c2\u6570\u6548\u7387\u3002", "motivation": "\u89e3\u51b3CNN\u548cTransformer\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u56e0\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u5360\u7528\u9ad8\u800c\u5bfc\u81f4\u7684\u8f93\u5165\u5c3a\u5bf8\u9650\u5236\u95ee\u9898\uff0c\u4ee5\u53ca\u79bb\u6563\u8bad\u7ec3\u5e26\u6765\u7684\u6b21\u4f18\u7ed3\u679c\u3002", "method": "\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u504f\u5fae\u5206\u65b9\u7a0b\u5efa\u6a21\u56fe\u50cf\u5206\u5272\uff0c\u5229\u7528Fourier\u795e\u7ecf\u7b97\u5b50\u7684\u96f6\u6837\u672c\u8d85\u5206\u8fa8\u7387\u7279\u6027\uff0c\u5e76\u7528Hartley\u53d8\u6362\u66ff\u4ee3Fourier\u53d8\u6362\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cHNOSeg-XS\u8868\u73b0\u51fa\u8272\uff0c\u53c2\u6570\u5c11\u4e8e34.7k\uff0c\u63a8\u7406\u65f6\u95f4<0.24\u79d2\uff0c\u5185\u5b58\u5360\u7528<1.8 GiB\u3002", "conclusion": "HNOSeg-XS\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u5206\u8fa8\u7387\u9c81\u68d2\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u4f18\u4e8e\u4f20\u7edfCNN\u548cTransformer\u6a21\u578b\u3002"}}
{"id": "2507.08307", "pdf": "https://arxiv.org/pdf/2507.08307", "abs": "https://arxiv.org/abs/2507.08307", "authors": ["Kui Jiang", "Shiyu Liu", "Junjun Jiang", "Xin Yang", "Hongxun Yang", "Xiaopeng Fan"], "title": "M2DAO-Talker: Harmonizing Multi-granular Motion Decoupling and Alternating Optimization for Talking-head Generation", "categories": ["cs.CV"], "comment": null, "summary": "Audio-driven talking head generation holds significant potential for film production. While existing 3D methods have advanced motion modeling and content synthesis, they often produce rendering artifacts, such as motion blur, temporal jitter, and local penetration, due to limitations in representing stable, fine-grained motion fields. Through systematic analysis, we reformulate talking head generation into a unified framework comprising three steps: video preprocessing, motion representation, and rendering reconstruction. This framework underpins our proposed M2DAO-Talker, which addresses current limitations via multi-granular motion decoupling and alternating optimization.Specifically, we devise a novel 2D portrait preprocessing pipeline to extract frame-wise deformation control conditions (motion region segmentation masks, and camera parameters) to facilitate motion representation. To ameliorate motion modeling, we elaborate a multi-granular motion decoupling strategy, which independently models non-rigid (oral and facial) and rigid (head) motions for improved reconstruction accuracy.Meanwhile, a motion consistency constraint is developed to ensure head-torso kinematic consistency, thereby mitigating penetration artifacts caused by motion aliasing. In addition, an alternating optimization strategy is designed to iteratively refine facial and oral motion parameters, enabling more realistic video generation.Experiments across multiple datasets show that M2DAO-Talker achieves state-of-the-art performance, with the 2.43 dB PSNR improvement in generation quality and 0.64 gain in user-evaluated video realness versus TalkingGaussian while with 150 FPS inference speed. Our project homepage is https://m2dao-talker.github.io/M2DAO-Talk.github.io", "AI": {"tldr": "M2DAO-Talker\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u97f3\u9891\u9a71\u52a8\u8bf4\u8bdd\u5934\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u8fd0\u52a8\u89e3\u8026\u548c\u4ea4\u66ff\u4f18\u5316\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6e32\u67d3\u4f2a\u5f71\u95ee\u9898\u3002", "motivation": "\u73b0\u67093D\u65b9\u6cd5\u5728\u8fd0\u52a8\u5efa\u6a21\u548c\u5185\u5bb9\u5408\u6210\u65b9\u9762\u5b58\u5728\u6e32\u67d3\u4f2a\u5f71\uff08\u5982\u8fd0\u52a8\u6a21\u7cca\u3001\u65f6\u95f4\u6296\u52a8\u548c\u5c40\u90e8\u7a7f\u900f\uff09\uff0c\u9650\u5236\u4e86\u751f\u6210\u89c6\u9891\u7684\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e09\u6b65\u6846\u67b6\uff1a\u89c6\u9891\u9884\u5904\u7406\u3001\u8fd0\u52a8\u8868\u793a\u548c\u6e32\u67d3\u91cd\u5efa\u3002\u5177\u4f53\u5305\u62ec2D\u8096\u50cf\u9884\u5904\u7406\u3001\u591a\u7c92\u5ea6\u8fd0\u52a8\u89e3\u8026\u7b56\u7565\u548c\u4ea4\u66ff\u4f18\u5316\u7b56\u7565\u3002", "result": "M2DAO-Talker\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u751f\u6210\u8d28\u91cf\u63d0\u53472.43 dB PSNR\uff0c\u7528\u6237\u8bc4\u4ef7\u89c6\u9891\u771f\u5b9e\u611f\u63d0\u53470.64\uff0c\u63a8\u7406\u901f\u5ea6\u8fbe150 FPS\u3002", "conclusion": "M2DAO-Talker\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u548c\u4f18\u5316\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u8bf4\u8bdd\u5934\u751f\u6210\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u771f\u5b9e\u611f\u3002"}}
{"id": "2507.08375", "pdf": "https://arxiv.org/pdf/2507.08375", "abs": "https://arxiv.org/abs/2507.08375", "authors": ["Alexandra Malyugina", "Yini Li", "Joanne Lin", "Nantheera Anantrasirichai"], "title": "Unsupervised Methods for Video Quality Improvement: A Survey of Restoration and Enhancement Techniques", "categories": ["cs.CV"], "comment": null, "summary": "Video restoration and enhancement are critical not only for improving visual quality, but also as essential pre-processing steps to boost the performance of a wide range of downstream computer vision tasks. This survey presents a comprehensive review of video restoration and enhancement techniques with a particular focus on unsupervised approaches. We begin by outlining the most common video degradations and their underlying causes, followed by a review of early conventional and deep learning methods-based, highlighting their strengths and limitations. We then present an in-depth overview of unsupervised methods, categorise by their fundamental approaches, including domain translation, self-supervision signal design and blind spot or noise-based methods. We also provide a categorization of loss functions employed in unsupervised video restoration and enhancement, and discuss the role of paired synthetic datasets in enabling objective evaluation. Finally, we identify key challenges and outline promising directions for future research in this field.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u89c6\u9891\u4fee\u590d\u4e0e\u589e\u5f3a\u6280\u672f\uff0c\u91cd\u70b9\u63a2\u8ba8\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u6db5\u76d6\u5e38\u89c1\u9000\u5316\u7c7b\u578b\u3001\u4f20\u7edf\u4e0e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3001\u65e0\u76d1\u7763\u65b9\u6cd5\u5206\u7c7b\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u89c6\u9891\u4fee\u590d\u4e0e\u589e\u5f3a\u5bf9\u63d0\u5347\u89c6\u89c9\u8d28\u91cf\u548c\u4e0b\u6e38\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u65e0\u76d1\u7763\u65b9\u6cd5\u56e0\u5176\u65e0\u9700\u6807\u6ce8\u6570\u636e\u800c\u5907\u53d7\u5173\u6ce8\u3002", "method": "\u7efc\u8ff0\u5305\u62ec\u5e38\u89c1\u9000\u5316\u5206\u6790\u3001\u4f20\u7edf\u4e0e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u56de\u987e\u3001\u65e0\u76d1\u7763\u65b9\u6cd5\u5206\u7c7b\uff08\u5982\u57df\u8f6c\u6362\u3001\u81ea\u76d1\u7763\u4fe1\u53f7\u8bbe\u8ba1\u7b49\uff09\u53ca\u635f\u5931\u51fd\u6570\u5206\u7c7b\u3002", "result": "\u603b\u7ed3\u4e86\u65e0\u76d1\u7763\u65b9\u6cd5\u7684\u4f18\u52bf\u4e0e\u5c40\u9650\uff0c\u5e76\u8ba8\u8bba\u4e86\u5408\u6210\u6570\u636e\u96c6\u5728\u5ba2\u89c2\u8bc4\u4f30\u4e2d\u7684\u4f5c\u7528\u3002", "conclusion": "\u6307\u51fa\u4e86\u8be5\u9886\u57df\u7684\u5173\u952e\u6311\u6218\u4e0e\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5f3a\u8c03\u65e0\u76d1\u7763\u65b9\u6cd5\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.08380", "pdf": "https://arxiv.org/pdf/2507.08380", "abs": "https://arxiv.org/abs/2507.08380", "authors": ["Sen Wang", "Shao Zeng", "Tianjun Gu", "Zhizhong Zhang", "Ruixin Zhang", "Shouhong Ding", "Jingyun Zhang", "Jun Wang", "Xin Tan", "Yuan Xie", "Lizhuang Ma"], "title": "From Enhancement to Understanding: Build a Generalized Bridge for Low-light Vision via Semantically Consistent Unsupervised Fine-tuning", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Low-level enhancement and high-level visual understanding in low-light vision have traditionally been treated separately. Low-light enhancement improves image quality for downstream tasks, but existing methods rely on physical or geometric priors, limiting generalization. Evaluation mainly focuses on visual quality rather than downstream performance. Low-light visual understanding, constrained by scarce labeled data, primarily uses task-specific domain adaptation, which lacks scalability. To address these challenges, we build a generalized bridge between low-light enhancement and low-light understanding, which we term Generalized Enhancement For Understanding (GEFU). This paradigm improves both generalization and scalability. To address the diverse causes of low-light degradation, we leverage pretrained generative diffusion models to optimize images, achieving zero-shot generalization performance. Building on this, we propose Semantically Consistent Unsupervised Fine-tuning (SCUF). Specifically, to overcome text prompt limitations, we introduce an illumination-aware image prompt to explicitly guide image generation and propose a cycle-attention adapter to maximize its semantic potential. To mitigate semantic degradation in unsupervised training, we propose caption and reflectance consistency to learn high-level semantics and image-level spatial semantics. Extensive experiments demonstrate that our proposed method outperforms current state-of-the-art methods in traditional image quality and GEFU tasks including classification, detection, and semantic segmentation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGEFU\u7684\u901a\u7528\u589e\u5f3a\u4e0e\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u6269\u6563\u6a21\u578b\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u65e0\u76d1\u7763\u5fae\u8c03\uff08SCUF\uff09\u89e3\u51b3\u4f4e\u5149\u89c6\u89c9\u4e2d\u7684\u589e\u5f3a\u4e0e\u7406\u89e3\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u4f4e\u5149\u589e\u5f3a\u4e0e\u89c6\u89c9\u7406\u89e3\u5206\u5f00\u5904\u7406\uff0c\u524d\u8005\u4f9d\u8d56\u5148\u9a8c\u9650\u5236\u6cdb\u5316\u80fd\u529b\uff0c\u540e\u8005\u56e0\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u800c\u7f3a\u4e4f\u6269\u5c55\u6027\u3002GEFU\u65e8\u5728\u6784\u5efa\u4e24\u8005\u95f4\u7684\u901a\u7528\u6865\u6881\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u751f\u6210\u6269\u6563\u6a21\u578b\u4f18\u5316\u56fe\u50cf\uff0c\u63d0\u51faSCUF\u6846\u67b6\uff0c\u5305\u62ec\u5149\u7167\u611f\u77e5\u56fe\u50cf\u63d0\u793a\u548c\u5faa\u73af\u6ce8\u610f\u529b\u9002\u914d\u5668\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u4e00\u81f4\u6027\u7ea6\u675f\u907f\u514d\u8bad\u7ec3\u4e2d\u7684\u8bed\u4e49\u9000\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u5206\u7c7b\u3001\u68c0\u6d4b\u3001\u5206\u5272\u7b49\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GEFU\u901a\u8fc7\u7edf\u4e00\u589e\u5f3a\u4e0e\u7406\u89e3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u6269\u5c55\u6027\uff0c\u4e3a\u4f4e\u5149\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.08396", "pdf": "https://arxiv.org/pdf/2507.08396", "abs": "https://arxiv.org/abs/2507.08396", "authors": ["Zhanxin Gao", "Beier Zhu", "Liang Yao", "Jian Yang", "Ying Tai"], "title": "Subject-Consistent and Pose-Diverse Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Subject-consistent generation (SCG)-aiming to maintain a consistent subject identity across diverse scenes-remains a challenge for text-to-image (T2I) models. Existing training-free SCG methods often achieve consistency at the cost of layout and pose diversity, hindering expressive visual storytelling. To address the limitation, we propose subject-Consistent and pose-Diverse T2I framework, dubbed as CoDi, that enables consistent subject generation with diverse pose and layout. Motivated by the progressive nature of diffusion, where coarse structures emerge early and fine details are refined later, CoDi adopts a two-stage strategy: Identity Transport (IT) and Identity Refinement (IR). IT operates in the early denoising steps, using optimal transport to transfer identity features to each target image in a pose-aware manner. This promotes subject consistency while preserving pose diversity. IR is applied in the later denoising steps, selecting the most salient identity features to further refine subject details. Extensive qualitative and quantitative results on subject consistency, pose diversity, and prompt fidelity demonstrate that CoDi achieves both better visual perception and stronger performance across all metrics. The code is provided in https://github.com/NJU-PCALab/CoDi.", "AI": {"tldr": "CoDi\u662f\u4e00\u4e2a\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u7b56\u7565\uff08\u8eab\u4efd\u4f20\u8f93\u548c\u8eab\u4efd\u7ec6\u5316\uff09\u5b9e\u73b0\u4e3b\u9898\u4e00\u81f4\u6027\u548c\u59ff\u52bf\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u4e3b\u9898\u4e00\u81f4\u6027\u65f6\u4f1a\u727a\u7272\u5e03\u5c40\u548c\u59ff\u52bf\u591a\u6837\u6027\uff0c\u9650\u5236\u4e86\u89c6\u89c9\u53d9\u4e8b\u7684\u8868\u8fbe\u3002", "method": "CoDi\u91c7\u7528\u4e24\u9636\u6bb5\u7b56\u7565\uff1a\u8eab\u4efd\u4f20\u8f93\uff08IT\uff09\u548c\u8eab\u4efd\u7ec6\u5316\uff08IR\uff09\u3002IT\u5728\u65e9\u671f\u53bb\u566a\u6b65\u9aa4\u4e2d\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u4f20\u9012\u8eab\u4efd\u7279\u5f81\uff0cIR\u5728\u540e\u671f\u7ec6\u5316\u8eab\u4efd\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCoDi\u5728\u4e3b\u9898\u4e00\u81f4\u6027\u3001\u59ff\u52bf\u591a\u6837\u6027\u548c\u63d0\u793a\u4fdd\u771f\u5ea6\u65b9\u9762\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "CoDi\u5728\u89c6\u89c9\u611f\u77e5\u548c\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.08422", "pdf": "https://arxiv.org/pdf/2507.08422", "abs": "https://arxiv.org/abs/2507.08422", "authors": ["Wongi Jeong", "Kyungryeol Lee", "Hoigi Seo", "Se Young Chun"], "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated Diffusion Transformers", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Diffusion transformers have emerged as an alternative to U-net-based diffusion models for high-fidelity image and video generation, offering superior scalability. However, their heavy computation remains a major obstacle to real-world deployment. Existing acceleration methods primarily exploit the temporal dimension such as reusing cached features across diffusion timesteps. Here, we propose Region-Adaptive Latent Upsampling (RALU), a training-free framework that accelerates inference along spatial dimension. RALU performs mixed-resolution sampling across three stages: 1) low-resolution denoising latent diffusion to efficiently capture global semantic structure, 2) region-adaptive upsampling on specific regions prone to artifacts at full-resolution, and 3) all latent upsampling at full-resolution for detail refinement. To stabilize generations across resolution transitions, we leverage noise-timestep rescheduling to adapt the noise level across varying resolutions. Our method significantly reduces computation while preserving image quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$ on Stable Diffusion 3 with minimal degradation. Furthermore, RALU is complementary to existing temporal accelerations such as caching methods, thus can be seamlessly integrated to further reduce inference latency without compromising generation quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRALU\u7684\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u7ef4\u5ea6\u52a0\u901f\u6269\u6563\u53d8\u6362\u5668\u7684\u63a8\u7406\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u53d8\u6362\u5668\u867d\u7136\u5728\u9ad8\u4fdd\u771f\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8ba1\u7b97\u91cf\u5927\uff0c\u963b\u788d\u5b9e\u9645\u5e94\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5229\u7528\u65f6\u95f4\u7ef4\u5ea6\u52a0\u901f\uff0c\u800cRALU\u5219\u4e13\u6ce8\u4e8e\u7a7a\u95f4\u7ef4\u5ea6\u4f18\u5316\u3002", "method": "RALU\u91c7\u7528\u4e09\u9636\u6bb5\u6df7\u5408\u5206\u8fa8\u7387\u91c7\u6837\uff1a1\uff09\u4f4e\u5206\u8fa8\u7387\u53bb\u566a\u6355\u6349\u5168\u5c40\u8bed\u4e49\u7ed3\u6784\uff0c2\uff09\u5bf9\u6613\u4ea7\u751f\u4f2a\u5f71\u7684\u533a\u57df\u8fdb\u884c\u533a\u57df\u81ea\u9002\u5e94\u4e0a\u91c7\u6837\uff0c3\uff09\u5168\u5206\u8fa8\u7387\u4e0a\u91c7\u6837\u7ec6\u5316\u7ec6\u8282\u3002\u901a\u8fc7\u566a\u58f0\u65f6\u95f4\u6b65\u91cd\u8c03\u5ea6\u7a33\u5b9a\u5206\u8fa8\u7387\u8f6c\u6362\u3002", "result": "\u5728FLUX\u4e0a\u5b9e\u73b07.0\u500d\u52a0\u901f\uff0cStable Diffusion 3\u4e0a\u5b9e\u73b03.0\u500d\u52a0\u901f\uff0c\u4e14\u56fe\u50cf\u8d28\u91cf\u635f\u5931\u6700\u5c0f\u3002", "conclusion": "RALU\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u7a7a\u95f4\u7ef4\u5ea6\u52a0\u901f\u65b9\u6cd5\uff0c\u53ef\u4e0e\u73b0\u6709\u65f6\u95f4\u7ef4\u5ea6\u52a0\u901f\u6280\u672f\u4e92\u8865\uff0c\u8fdb\u4e00\u6b65\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u800c\u4e0d\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2507.08434", "pdf": "https://arxiv.org/pdf/2507.08434", "abs": "https://arxiv.org/abs/2507.08434", "authors": ["Ji Hyun Seo", "Byounhyun Yoo", "Gerard Jounghyun Kim"], "title": "RePaintGS: Reference-Guided Gaussian Splatting for Realistic and View-Consistent 3D Scene Inpainting", "categories": ["cs.CV"], "comment": null, "summary": "Radiance field methods, such as Neural Radiance Field or 3D Gaussian Splatting, have emerged as seminal 3D representations for synthesizing realistic novel views. For practical applications, there is ongoing research on flexible scene editing techniques, among which object removal is a representative task. However, removing objects exposes occluded regions, often leading to unnatural appearances. Thus, studies have employed image inpainting techniques to replace such regions with plausible content - a task referred to as 3D scene inpainting. However, image inpainting methods produce one of many plausible completions for each view, leading to inconsistencies between viewpoints. A widely adopted approach leverages perceptual cues to blend inpainted views smoothly. However, it is prone to detail loss and can fail when there are perceptual inconsistencies across views. In this paper, we propose a novel 3D scene inpainting method that reliably produces realistic and perceptually consistent results even for complex scenes by leveraging a reference view. Given the inpainted reference view, we estimate the inpainting similarity of the other views to adjust their contribution in constructing an accurate geometry tailored to the reference. This geometry is then used to warp the reference inpainting to other views as pseudo-ground truth, guiding the optimization to match the reference appearance. Comparative evaluation studies have shown that our approach improves both the geometric fidelity and appearance consistency of inpainted scenes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76843D\u573a\u666f\u4fee\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u53c2\u8003\u89c6\u56fe\u751f\u6210\u51e0\u4f55\u548c\u5916\u89c2\u4e00\u81f4\u7684\u4fee\u590d\u7ed3\u679c\u3002", "motivation": "\u73b0\u67093D\u573a\u666f\u4fee\u590d\u65b9\u6cd5\u56e0\u89c6\u89d2\u95f4\u4e0d\u4e00\u81f4\u6027\u5bfc\u81f4\u4fee\u590d\u7ed3\u679c\u4e0d\u81ea\u7136\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u4fdd\u6301\u51e0\u4f55\u548c\u5916\u89c2\u4e00\u81f4\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u53c2\u8003\u89c6\u56fe\u4f30\u8ba1\u5176\u4ed6\u89c6\u56fe\u7684\u4fee\u590d\u76f8\u4f3c\u6027\uff0c\u8c03\u6574\u5176\u8d21\u732e\u4ee5\u6784\u5efa\u51c6\u786e\u51e0\u4f55\uff0c\u5e76\u901a\u8fc7\u53c2\u8003\u89c6\u56fe\u5f15\u5bfc\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51e0\u4f55\u4fdd\u771f\u5ea6\u548c\u5916\u89c2\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u53c2\u8003\u89c6\u56fe\u6709\u6548\u89e3\u51b3\u4e863D\u573a\u666f\u4fee\u590d\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u3002"}}
{"id": "2507.08441", "pdf": "https://arxiv.org/pdf/2507.08441", "abs": "https://arxiv.org/abs/2507.08441", "authors": ["Anlin Zheng", "Xin Wen", "Xuanyang Zhang", "Chuofan Ma", "Tiancai Wang", "Gang Yu", "Xiangyu Zhang", "Xiaojuan Qi"], "title": "Vision Foundation Models as Effective Visual Tokenizers for Autoregressive Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages, 4 figures", "summary": "Leveraging the powerful representations of pre-trained vision foundation models -- traditionally used for visual comprehension -- we explore a novel direction: building an image tokenizer directly atop such models, a largely underexplored area. Specifically, we employ a frozen vision foundation model as the encoder of our tokenizer. To enhance its effectiveness, we introduce two key components: (1) a region-adaptive quantization framework that reduces redundancy in the pre-trained features on regular 2D grids, and (2) a semantic reconstruction objective that aligns the tokenizer's outputs with the foundation model's representations to preserve semantic fidelity. Based on these designs, our proposed image tokenizer, VFMTok, achieves substantial improvements in image reconstruction and generation quality, while also enhancing token efficiency. It further boosts autoregressive (AR) generation -- achieving a gFID of 2.07 on ImageNet benchmarks, while accelerating model convergence by three times, and enabling high-fidelity class-conditional synthesis without the need for classifier-free guidance (CFG). The code will be released publicly to benefit the community.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u56fe\u50cf\u5206\u8bcd\u5668VFMTok\uff0c\u901a\u8fc7\u533a\u57df\u81ea\u9002\u5e94\u91cf\u5316\u6846\u67b6\u548c\u8bed\u4e49\u91cd\u5efa\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u91cd\u5efa\u4e0e\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u63a2\u7d22\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u6784\u5efa\u56fe\u50cf\u5206\u8bcd\u5668\uff0c\u586b\u8865\u8be5\u9886\u57df\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u51bb\u7ed3\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u7f16\u7801\u5668\uff0c\u5f15\u5165\u533a\u57df\u81ea\u9002\u5e94\u91cf\u5316\u6846\u67b6\u548c\u8bed\u4e49\u91cd\u5efa\u76ee\u6807\u3002", "result": "VFMTok\u5728\u56fe\u50cf\u91cd\u5efa\u548c\u751f\u6210\u8d28\u91cf\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0cgFID\u8fbe2.07\uff0c\u52a0\u901f\u6a21\u578b\u6536\u655b\u4e09\u500d\uff0c\u65e0\u9700\u5206\u7c7b\u5668\u81ea\u7531\u6307\u5bfc\u5373\u53ef\u5b9e\u73b0\u9ad8\u4fdd\u771f\u7c7b\u522b\u6761\u4ef6\u5408\u6210\u3002", "conclusion": "VFMTok\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u793e\u533a\u53d1\u5c55\u3002"}}
{"id": "2507.08548", "pdf": "https://arxiv.org/pdf/2507.08548", "abs": "https://arxiv.org/abs/2507.08548", "authors": ["Alen Adamyan", "Tom\u00e1\u0161 \u010c\u00ed\u017eek", "Matej Straka", "Klara Janouskova", "Martin Schmid"], "title": "SAM2RL: Towards Reinforcement Learning Memory Control in Segment Anything Model 2", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks and has become the state-of-the-art for visual object tracking. The model stores information from previous frames in a memory bank, enabling temporal consistency across video sequences. Recent methods augment SAM 2 with hand-crafted update rules to better handle distractors, occlusions, and object motion. We propose a fundamentally different approach using reinforcement learning for optimizing memory updates in SAM 2 by framing memory control as a sequential decision-making problem. In an overfitting setup with a separate agent per video, our method achieves a relative improvement over SAM 2 that exceeds by more than three times the gains of existing heuristics. These results reveal the untapped potential of the memory bank and highlight reinforcement learning as a powerful alternative to hand-crafted update rules for memory control in visual object tracking.", "AI": {"tldr": "SAM 2\u5728\u76ee\u6807\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u4e3a\u89c6\u89c9\u76ee\u6807\u8ddf\u8e2a\u7684SOTA\u3002\u672c\u6587\u63d0\u51fa\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5176\u5185\u5b58\u66f4\u65b0\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u89c4\u5219\u66f4\u65b0\u5185\u5b58\uff0c\u96be\u4ee5\u5904\u7406\u5e72\u6270\u3001\u906e\u6321\u548c\u8fd0\u52a8\u3002\u672c\u6587\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\uff0c\u4ee5\u4f18\u5316\u5185\u5b58\u63a7\u5236\u3002", "method": "\u5c06\u5185\u5b58\u63a7\u5236\u5efa\u6a21\u4e3a\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316SAM 2\u7684\u5185\u5b58\u66f4\u65b0\u7b56\u7565\u3002", "result": "\u5728\u8fc7\u62df\u5408\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u5bf9SAM 2\u7684\u6539\u8fdb\u8d85\u8fc7\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u4e09\u500d\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u662f\u4f18\u5316\u5185\u5b58\u63a7\u5236\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u63ed\u793a\u4e86\u5185\u5b58\u94f6\u884c\u7684\u672a\u5f00\u53d1\u6f5c\u529b\u3002"}}
{"id": "2507.08554", "pdf": "https://arxiv.org/pdf/2507.08554", "abs": "https://arxiv.org/abs/2507.08554", "authors": ["Cristina Mata", "Michael S. Ryoo", "Henrik Turbell"], "title": "Image Translation with Kernel Prediction Networks for Semantic Segmentation", "categories": ["cs.CV"], "comment": "OOD-CV Workshop at ECCV 2024", "summary": "Semantic segmentation relies on many dense pixel-wise annotations to achieve the best performance, but owing to the difficulty of obtaining accurate annotations for real world data, practitioners train on large-scale synthetic datasets. Unpaired image translation is one method used to address the ensuing domain gap by generating more realistic training data in low-data regimes. Current methods for unpaired image translation train generative adversarial networks (GANs) to perform the translation and enforce pixel-level semantic matching through cycle consistency. These methods do not guarantee that the semantic matching holds, posing a problem for semantic segmentation where performance is sensitive to noisy pixel labels. We propose a novel image translation method, Domain Adversarial Kernel Prediction Network (DA-KPN), that guarantees semantic matching between the synthetic label and translation. DA-KPN estimates pixel-wise input transformation parameters of a lightweight and simple translation function. To ensure the pixel-wise transformation is realistic, DA-KPN uses multi-scale discriminators to distinguish between translated and target samples. We show DA-KPN outperforms previous GAN-based methods on syn2real benchmarks for semantic segmentation with limited access to real image labels and achieves comparable performance on face parsing.", "AI": {"tldr": "DA-KPN\u662f\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7ffb\u8bd1\u51fd\u6570\u548c\u8bed\u4e49\u5339\u914d\u4fdd\u8bc1\uff0c\u5728\u4f4e\u6570\u636e\u91cf\u4e0b\u63d0\u5347\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709GAN\u65b9\u6cd5\u5728\u8bed\u4e49\u5206\u5272\u4e2d\u65e0\u6cd5\u4fdd\u8bc1\u8bed\u4e49\u5339\u914d\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4f4e\u6570\u636e\u91cf\u4e0b\u3002", "method": "\u4f7f\u7528Domain Adversarial Kernel Prediction Network\uff08DA-KPN\uff09\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7ffb\u8bd1\u51fd\u6570\u548c\u591a\u5c3a\u5ea6\u5224\u522b\u5668\u5b9e\u73b0\u50cf\u7d20\u7ea7\u8bed\u4e49\u5339\u914d\u3002", "result": "\u5728syn2real\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u57fa\u4e8eGAN\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u4eba\u8138\u89e3\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "DA-KPN\u5728\u4f4e\u6570\u636e\u91cf\u4e0b\u6709\u6548\u63d0\u5347\u8bed\u4e49\u5206\u5272\u6027\u80fd\uff0c\u5e76\u4fdd\u8bc1\u8bed\u4e49\u5339\u914d\u3002"}}
{"id": "2507.08644", "pdf": "https://arxiv.org/pdf/2507.08644", "abs": "https://arxiv.org/abs/2507.08644", "authors": ["Junho Koh", "Youngwoo Lee", "Jungho Kim", "Dongyoung Lee", "Jun Won Choi"], "title": "OnlineBEV: Recurrent Temporal Fusion in Bird's Eye View Representations for Multi-Camera 3D Perception", "categories": ["cs.CV"], "comment": "Accepted to Transactions on Intelligent Transportation Systems", "summary": "Multi-view camera-based 3D perception can be conducted using bird's eye view (BEV) features obtained through perspective view-to-BEV transformations. Several studies have shown that the performance of these 3D perception methods can be further enhanced by combining sequential BEV features obtained from multiple camera frames. However, even after compensating for the ego-motion of an autonomous agent, the performance gain from temporal aggregation is limited when combining a large number of image frames. This limitation arises due to dynamic changes in BEV features over time caused by object motion. In this paper, we introduce a novel temporal 3D perception method called OnlineBEV, which combines BEV features over time using a recurrent structure. This structure increases the effective number of combined features with minimal memory usage. However, it is critical to spatially align the features over time to maintain strong performance. OnlineBEV employs the Motion-guided BEV Fusion Network (MBFNet) to achieve temporal feature alignment. MBFNet extracts motion features from consecutive BEV frames and dynamically aligns historical BEV features with current ones using these motion features. To enforce temporal feature alignment explicitly, we use Temporal Consistency Learning Loss, which captures discrepancies between historical and target BEV features. Experiments conducted on the nuScenes benchmark demonstrate that OnlineBEV achieves significant performance gains over the current best method, SOLOFusion. OnlineBEV achieves 63.9% NDS on the nuScenes test set, recording state-of-the-art performance in the camera-only 3D object detection task.", "AI": {"tldr": "OnlineBEV\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65f6\u5e8f3D\u611f\u77e5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5faa\u73af\u7ed3\u6784\u7ed3\u5408BEV\u7279\u5f81\uff0c\u5229\u7528MBFNet\u5b9e\u73b0\u65f6\u5e8f\u7279\u5f81\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u65f6\u5e8fBEV\u7279\u5f81\u7ed3\u5408\u80fd\u63d0\u53473D\u611f\u77e5\u6027\u80fd\uff0c\u4f46\u52a8\u6001\u7269\u4f53\u8fd0\u52a8\u5bfc\u81f4\u7279\u5f81\u53d8\u5316\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002OnlineBEV\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5faa\u73af\u7ed3\u6784\u7ed3\u5408BEV\u7279\u5f81\uff0c\u4f7f\u7528MBFNet\u63d0\u53d6\u8fd0\u52a8\u7279\u5f81\u5e76\u52a8\u6001\u5bf9\u9f50\u5386\u53f2\u4e0e\u5f53\u524d\u7279\u5f81\uff0c\u901a\u8fc7\u65f6\u5e8f\u4e00\u81f4\u6027\u5b66\u4e60\u635f\u5931\u663e\u5f0f\u5bf9\u9f50\u7279\u5f81\u3002", "result": "\u5728nuScenes\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523063.9% NDS\uff0c\u6027\u80fd\u4f18\u4e8e\u5f53\u524d\u6700\u4f73\u65b9\u6cd5SOLOFusion\u3002", "conclusion": "OnlineBEV\u901a\u8fc7\u65f6\u5e8f\u7279\u5f81\u5bf9\u9f50\u663e\u8457\u63d0\u5347\u4e863D\u611f\u77e5\u6027\u80fd\uff0c\u6210\u4e3a\u76f8\u673a\u4ec53D\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u7684SOTA\u65b9\u6cd5\u3002"}}
{"id": "2507.08710", "pdf": "https://arxiv.org/pdf/2507.08710", "abs": "https://arxiv.org/abs/2507.08710", "authors": ["Li Li", "Yingzhe Peng", "Xu Yang", "Ruoxi Cheng", "Haiyang Xu", "Ming Yan", "Fei Huang"], "title": "L-CLIPScore: a Lightweight Embedding-based Captioning Metric for Evaluating and Training", "categories": ["cs.CV"], "comment": "10 pages, 4 figures", "summary": "We propose a novel embedding-based captioning metric termed as L-CLIPScore that can be used for efficiently evaluating caption quality and training captioning model. L-CLIPScore is calculated from a lightweight CLIP (L-CLIP), which is a dual-encoder architecture compressed and distilled from CLIP. To compress, we apply two powerful techniques which are weight multiplexing and matrix decomposition for reducing the parameters of encoders and word embedding matrix, respectively. To distill, we design a novel multi-modal Similarity Regulator (SR) loss to transfer more vision-language alignment knowledge. Specifically, SR loss amplifies the multi-modal embedding similarity if the given image-text pair is matched and diminishes the similarity if the pair is non-matched. By compressing and distilling by this novel SR loss, our L-CLIP achieves comparable multi-modal alignment ability to the original CLIP while it requires fewer computation resources and running time. We carry out exhaustive experiments to validate the efficiency and effectiveness of L-CLIPScore when using it as the judge to evaluate caption quality. We also discover that when using L-CLIPScore as the supervisor to train the captioning model, it should be mixed up by an n-gram-based metric and meanwhile analyze why using L-CLIPScore only will cause fail training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f7b\u91cf\u7ea7CLIP\uff08L-CLIP\uff09\u7684\u5d4c\u5165\u5f0f\u5b57\u5e55\u8bc4\u4f30\u6307\u6807L-CLIPScore\uff0c\u7528\u4e8e\u9ad8\u6548\u8bc4\u4f30\u5b57\u5e55\u8d28\u91cf\u548c\u8bad\u7ec3\u5b57\u5e55\u6a21\u578b\u3002", "motivation": "\u73b0\u6709CLIP\u6a21\u578b\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\uff0c\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u5316\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u6743\u91cd\u590d\u7528\u548c\u77e9\u9635\u5206\u89e3\u538b\u7f29\u6a21\u578b\u53c2\u6570\uff0c\u8bbe\u8ba1\u591a\u6a21\u6001\u76f8\u4f3c\u6027\u8c03\u8282\u5668\uff08SR\uff09\u635f\u5931\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u3002", "result": "L-CLIP\u5728\u4fdd\u6301\u591a\u6a21\u6001\u5bf9\u9f50\u80fd\u529b\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u548c\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "L-CLIPScore\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\u9ad8\u6548\u6709\u6548\uff0c\u4f46\u9700\u4e0en-gram\u6307\u6807\u6df7\u5408\u4f7f\u7528\u4ee5\u907f\u514d\u8bad\u7ec3\u5931\u8d25\u3002"}}
{"id": "2507.08729", "pdf": "https://arxiv.org/pdf/2507.08729", "abs": "https://arxiv.org/abs/2507.08729", "authors": ["Yuqiang Lin", "Sam Lockyer", "Mingxuan Sui", "Li Gan", "Florian Stanek", "Markus Zarbock", "Wenbin Li", "Adrian Evans", "Nic Zhang"], "title": "RoundaboutHD: High-Resolution Real-World Urban Environment Benchmark for Multi-Camera Vehicle Tracking", "categories": ["cs.CV"], "comment": null, "summary": "The multi-camera vehicle tracking (MCVT) framework holds significant potential for smart city applications, including anomaly detection, traffic density estimation, and suspect vehicle tracking. However, current publicly available datasets exhibit limitations, such as overly simplistic scenarios, low-resolution footage, and insufficiently diverse conditions, creating a considerable gap between academic research and real-world scenario. To fill this gap, we introduce RoundaboutHD, a comprehensive, high-resolution multi-camera vehicle tracking benchmark dataset specifically designed to represent real-world roundabout scenarios. RoundaboutHD provides a total of 40 minutes of labelled video footage captured by four non-overlapping, high-resolution (4K resolution, 15 fps) cameras. In total, 512 unique vehicle identities are annotated across different camera views, offering rich cross-camera association data. RoundaboutHD offers temporal consistency video footage and enhanced challenges, including increased occlusions and nonlinear movement inside the roundabout. In addition to the full MCVT dataset, several subsets are also available for object detection, single camera tracking, and image-based vehicle re-identification (ReID) tasks. Vehicle model information and camera modelling/ geometry information are also included to support further analysis. We provide baseline results for vehicle detection, single-camera tracking, image-based vehicle re-identification, and multi-camera tracking. The dataset and the evaluation code are publicly available at: https://github.com/siri-rouser/RoundaboutHD.git", "AI": {"tldr": "\u4ecb\u7ecd\u4e86RoundaboutHD\uff0c\u4e00\u4e2a\u9ad8\u5206\u8fa8\u7387\u591a\u6444\u50cf\u5934\u8f66\u8f86\u8ddf\u8e2a\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u4e0d\u8db3\uff0c\u652f\u6301\u591a\u79cd\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u516c\u5f00\u6570\u636e\u96c6\u5728\u573a\u666f\u590d\u6742\u6027\u3001\u5206\u8fa8\u7387\u548c\u591a\u6837\u6027\u65b9\u9762\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6ee1\u8db3\u771f\u5b9e\u4e16\u754c\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u56db\u4e2a\u975e\u91cd\u53e0\u9ad8\u5206\u8fa8\u7387\u6444\u50cf\u5934\u91c7\u96c640\u5206\u949f\u89c6\u9891\u6570\u636e\uff0c\u6807\u6ce8512\u8f86\u8f66\u7684\u8eab\u4efd\u548c\u8de8\u6444\u50cf\u5934\u5173\u8054\u6570\u636e\u3002", "result": "\u63d0\u4f9b\u4e86\u8f66\u8f86\u68c0\u6d4b\u3001\u5355\u6444\u50cf\u5934\u8ddf\u8e2a\u3001\u8f66\u8f86\u91cd\u8bc6\u522b\u548c\u591a\u6444\u50cf\u5934\u8ddf\u8e2a\u7684\u57fa\u7ebf\u7ed3\u679c\u3002", "conclusion": "RoundaboutHD\u4e3a\u591a\u6444\u50cf\u5934\u8f66\u8f86\u8ddf\u8e2a\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u63a5\u8fd1\u771f\u5b9e\u573a\u666f\u7684\u6570\u636e\u548c\u6311\u6218\u3002"}}
{"id": "2507.08772", "pdf": "https://arxiv.org/pdf/2507.08772", "abs": "https://arxiv.org/abs/2507.08772", "authors": ["Shaocong Dong", "Lihe Ding", "Xiao Chen", "Yaokun Li", "Yuxin Wang", "Yucheng Wang", "Qi Wang", "Jaehyeok Kim", "Chenjian Gao", "Zhanpeng Huang", "Zibin Wang", "Tianfan Xue", "Dan Xu"], "title": "From One to More: Contextual Part Latents for 3D Generation", "categories": ["cs.CV"], "comment": "Project page: https://hkdsc.github.io/project/copart", "summary": "Recent advances in 3D generation have transitioned from multi-view 2D rendering approaches to 3D-native latent diffusion frameworks that exploit geometric priors in ground truth data. Despite progress, three key limitations persist: (1) Single-latent representations fail to capture complex multi-part geometries, causing detail degradation; (2) Holistic latent coding neglects part independence and interrelationships critical for compositional design; (3) Global conditioning mechanisms lack fine-grained controllability. Inspired by human 3D design workflows, we propose CoPart - a part-aware diffusion framework that decomposes 3D objects into contextual part latents for coherent multi-part generation. This paradigm offers three advantages: i) Reduces encoding complexity through part decomposition; ii) Enables explicit part relationship modeling; iii) Supports part-level conditioning. We further develop a mutual guidance strategy to fine-tune pre-trained diffusion models for joint part latent denoising, ensuring both geometric coherence and foundation model priors. To enable large-scale training, we construct Partverse - a novel 3D part dataset derived from Objaverse through automated mesh segmentation and human-verified annotations. Extensive experiments demonstrate CoPart's superior capabilities in part-level editing, articulated object generation, and scene composition with unprecedented controllability.", "AI": {"tldr": "CoPart\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u90e8\u5206\u611f\u77e5\u7684\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e33D\u5bf9\u8c61\u4e3a\u4e0a\u4e0b\u6587\u90e8\u5206\u6f5c\u5728\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u73b0\u67093D\u751f\u6210\u65b9\u6cd5\u5728\u7ec6\u8282\u3001\u90e8\u5206\u72ec\u7acb\u6027\u548c\u53ef\u63a7\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u67093D\u751f\u6210\u65b9\u6cd5\u5728\u590d\u6742\u591a\u90e8\u5206\u51e0\u4f55\u3001\u90e8\u5206\u72ec\u7acb\u6027\u548c\u7ec6\u7c92\u5ea6\u63a7\u5236\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cCoPart\u53d7\u4eba\u7c7b3D\u8bbe\u8ba1\u5de5\u4f5c\u6d41\u7a0b\u542f\u53d1\uff0c\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "CoPart\u91c7\u7528\u90e8\u5206\u611f\u77e5\u6269\u6563\u6846\u67b6\uff0c\u5206\u89e33D\u5bf9\u8c61\u4e3a\u90e8\u5206\u6f5c\u5728\u8868\u793a\uff0c\u5e76\u5f00\u53d1\u4e86\u4e92\u6307\u5bfc\u7b56\u7565\u8054\u5408\u53bb\u566a\uff0c\u540c\u65f6\u6784\u5efa\u4e86Partverse\u6570\u636e\u96c6\u652f\u6301\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCoPart\u5728\u90e8\u5206\u7ea7\u522b\u7f16\u8f91\u3001\u5173\u8282\u5bf9\u8c61\u751f\u6210\u548c\u573a\u666f\u7ec4\u5408\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u524d\u6240\u672a\u6709\u7684\u53ef\u63a7\u6027\u3002", "conclusion": "CoPart\u901a\u8fc7\u90e8\u5206\u5206\u89e3\u548c\u5173\u7cfb\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u751f\u6210\u7684\u7ec6\u8282\u548c\u53ef\u63a7\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.08801", "pdf": "https://arxiv.org/pdf/2507.08801", "abs": "https://arxiv.org/abs/2507.08801", "authors": ["Hangjie Yuan", "Weihua Chen", "Jun Cen", "Hu Yu", "Jingyun Liang", "Shuning Chang", "Zhihui Lin", "Tao Feng", "Pengwei Liu", "Jiazheng Xing", "Hao Luo", "Jiasheng Tang", "Fan Wang", "Yi Yang"], "title": "Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Code and Models: https://github.com/alibaba-damo-academy/Lumos", "summary": "Autoregressive large language models (LLMs) have unified a vast range of language tasks, inspiring preliminary efforts in autoregressive video generation. Existing autoregressive video generators either diverge from standard LLM architectures, depend on bulky external text encoders, or incur prohibitive latency due to next-token decoding. In this paper, we introduce Lumos-1, an autoregressive video generator that retains the LLM architecture with minimal architectural modifications. To inject spatiotemporal correlations in LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its imbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE scheme that preserves the original textual RoPE while providing comprehensive frequency spectra and scaled 3D positions for modeling multimodal spatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy that obeys intra-frame bidirectionality and inter-frame temporal causality. Based on this dependency strategy, we identify the issue of frame-wise loss imbalance caused by spatial information redundancy and solve it by proposing Autoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal tube masking during training with a compatible inference-time masking policy to avoid quality degradation. By using memory-efficient training techniques, we pre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on GenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code and models are available at https://github.com/alibaba-damo-academy/Lumos.", "AI": {"tldr": "Lumos-1\u662f\u4e00\u79cd\u57fa\u4e8eLLM\u67b6\u6784\u7684\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u5668\uff0c\u901a\u8fc7\u6539\u8fdb\u76843D RoPE\u548cAR-DF\u7b56\u7565\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5ef6\u8fdf\u548c\u67b6\u6784\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u5668\u8981\u4e48\u504f\u79bb\u6807\u51c6LLM\u67b6\u6784\uff0c\u8981\u4e48\u4f9d\u8d56\u7b28\u91cd\u7684\u5916\u90e8\u6587\u672c\u7f16\u7801\u5668\uff0c\u6216\u5b58\u5728\u9ad8\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u63d0\u51faMM-RoPE\u65b9\u6848\uff0c\u7ed3\u54083D RoPE\u548c\u6587\u672cRoPE\uff0c\u5e76\u5f15\u5165AR-DF\u7b56\u7565\u89e3\u51b3\u5e27\u95f4\u635f\u5931\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u572848\u4e2aGPU\u4e0a\u9884\u8bad\u7ec3\uff0c\u6027\u80fd\u4e0eEMU3\u3001COSMOS-Video2World\u548cOpenSoraPlan\u76f8\u5f53\u3002", "conclusion": "Lumos-1\u901a\u8fc7\u6700\u5c0f\u67b6\u6784\u4fee\u6539\u548c\u9ad8\u6548\u8bad\u7ec3\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2507.08726", "pdf": "https://arxiv.org/pdf/2507.08726", "abs": "https://arxiv.org/abs/2507.08726", "authors": ["Yuekun Wu", "Yik Lung Pang", "Andrea Cavallaro", "Changjae Oh"], "title": "Learning human-to-robot handovers through 3D scene reconstruction", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, 6 figures, 2 table", "summary": "Learning robot manipulation policies from raw, real-world image data requires a large number of robot-action trials in the physical environment. Although training using simulations offers a cost-effective alternative, the visual domain gap between simulation and robot workspace remains a major limitation. Gaussian Splatting visual reconstruction methods have recently provided new directions for robot manipulation by generating realistic environments. In this paper, we propose the first method for learning supervised-based robot handovers solely from RGB images without the need of real-robot training or real-robot data collection. The proposed policy learner, Human-to-Robot Handover using Sparse-View Gaussian Splatting (H2RH-SGS), leverages sparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes to generate robot demonstrations containing image-action pairs captured with a camera mounted on the robot gripper. As a result, the simulated camera pose changes in the reconstructed scene can be directly translated into gripper pose changes. We train a robot policy on demonstrations collected with 16 household objects and {\\em directly} deploy this policy in the real environment. Experiments in both Gaussian Splatting reconstructed scene and real-world human-to-robot handover experiments demonstrate that H2RH-SGS serves as a new and effective representation for the human-to-robot handover task.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4eceRGB\u56fe\u50cf\u5b66\u4e60\u673a\u5668\u4eba\u4ea4\u63a5\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u771f\u5b9e\u673a\u5668\u4eba\u8bad\u7ec3\u6216\u6570\u636e\u6536\u96c6\uff0c\u5229\u7528\u7a00\u758f\u89c6\u56fe\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u573a\u666f\u751f\u6210\u673a\u5668\u4eba\u6f14\u793a\u3002", "motivation": "\u89e3\u51b3\u4ece\u6a21\u62df\u5230\u771f\u5b9e\u73af\u5883\u7684\u89c6\u89c9\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u51cf\u5c11\u5bf9\u771f\u5b9e\u673a\u5668\u4eba\u8bad\u7ec3\u548c\u6570\u636e\u6536\u96c6\u7684\u4f9d\u8d56\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u89c6\u56fe\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u4eba\u7c7b\u5230\u673a\u5668\u4eba\u4ea4\u63a5\u573a\u666f\uff0c\u751f\u6210\u56fe\u50cf-\u52a8\u4f5c\u5bf9\u6f14\u793a\uff0c\u5e76\u76f4\u63a5\u8bad\u7ec3\u673a\u5668\u4eba\u7b56\u7565\u3002", "result": "\u572816\u79cd\u5bb6\u5ead\u7269\u54c1\u4e0a\u6d4b\u8bd5\uff0c\u7b56\u7565\u53ef\u76f4\u63a5\u90e8\u7f72\u5230\u771f\u5b9e\u73af\u5883\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "conclusion": "H2RH-SGS\u4e3a\u4eba\u7c7b\u5230\u673a\u5668\u4eba\u4ea4\u63a5\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u8868\u793a\u65b9\u6cd5\u3002"}}
