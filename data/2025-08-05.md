<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 42]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [eess.IV](#eess.IV) [Total: 6]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Uncertainty Estimation for Novel Views in Gaussian Splatting from Primitive-Based Representations of Error and Visibility](https://arxiv.org/abs/2508.02443)
*Thomas Gottwald,Edgar Heinert,Matthias Rottmann*

Main category: cs.GR

TL;DR: 提出了一种新的高斯泼溅不确定性估计方法，通过训练误差和可见性的原始表示生成不确定性特征图，并通过回归模型聚合，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高斯泼溅在机器人和医学等关键应用中需要不确定性估计，但现有方法仅估计高斯基元的方差，缺乏更全面的不确定性信息。

Method: 通过训练误差和可见性的投影建立原始表示，生成不确定性特征图，并通过回归模型聚合。

Result: 实验表明该方法与真实误差高度相关，尤其在前景对象上优于现有方法，且回归模型具有泛化能力。

Conclusion: 该方法为高斯泼溅提供了更有效的不确定性估计，适用于新场景且无需额外数据。

Abstract: In this work, we present a novel method for uncertainty estimation (UE) in Gaussian Splatting. UE is crucial for using Gaussian Splatting in critical applications such as robotics and medicine. Previous methods typically estimate the variance of Gaussian primitives and use the rendering process to obtain pixel-wise uncertainties. Our method establishes primitive representations of error and visibility of trainings views, which carries meaningful uncertainty information. This representation is obtained by projection of training error and visibility onto the primitives. Uncertainties of novel views are obtained by rendering the primitive representations of uncertainty for those novel views, yielding uncertainty feature maps. To aggregate these uncertainty feature maps of novel views, we perform a pixel-wise regression on holdout data. In our experiments, we analyze the different components of our method, investigating various combinations of uncertainty feature maps and regression models. Furthermore, we considered the effect of separating splatting into foreground and background. Our UEs show high correlations to true errors, outperforming state-of-the-art methods, especially on foreground objects. The trained regression models show generalization capabilities to new scenes, allowing uncertainty estimation without the need for holdout data.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [Phase-fraction guided denoising diffusion model for augmenting multiphase steel microstructure segmentation via micrograph image-mask pair synthesis](https://arxiv.org/abs/2508.00896)
*Hoang Hai Nam Nguyen,Minh Tien Tran,Hoheok Kim,Ho Won Lee*

Main category: cs.CV

TL;DR: PF-DiffSeg是一种基于扩散框架的单阶段生成方法，用于同时合成显微结构图像和分割掩码，显著提高了分割精度，尤其在少数类别上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决金属合金显微结构分割中因缺乏人工标注掩码（特别是稀有或复杂形态）而导致的机器学习效果受限问题。

Method: 提出PF-DiffSeg，一种基于全局相分数控制的单阶段去噪扩散框架，通过生成合成数据增强多样性和训练效率。

Result: 在MetalDAM基准测试中，该方法显著提升了分割精度（尤其是少数类别），并优于两阶段掩码引导扩散和GAN基线方法。

Conclusion: PF-DiffSeg将生成与条件整合为统一框架，为金相应用提供了一种可扩展的数据增强解决方案。

Abstract: The effectiveness of machine learning in metallographic microstructure segmentation is often constrained by the lack of human-annotated phase masks, particularly for rare or compositionally complex morphologies within the metal alloy. We introduce PF-DiffSeg, a phase-fraction controlled, one-stage denoising diffusion framework that jointly synthesizes microstructure images and their corresponding segmentation masks in a single generative trajectory to further improve segmentation accuracy. By conditioning on global phase-fraction vectors, augmented to represent real data distribution and emphasize minority classes, our model generates compositionally valid and structurally coherent microstructure image and mask samples that improve both data diversity and training efficiency. Evaluated on the MetalDAM benchmark for additively manufactured multiphase steel, our synthetic augmentation method yields notable improvements in segmentation accuracy compared to standard augmentation strategies especially in minority classes and further outperforms a two-stage mask-guided diffusion and generative adversarial network (GAN) baselines, while also reducing inference time compared to conventional approach. The method integrates generation and conditioning into a unified framework, offering a scalable solution for data augmentation in metallographic applications.

</details>


### [3] [Benefits of Feature Extraction and Temporal Sequence Analysis for Video Frame Prediction: An Evaluation of Hybrid Deep Learning Models](https://arxiv.org/abs/2508.00898)
*Jose M. Sánchez Velázquez,Mingbo Cai,Andrew Coney,Álvaro J. García- Tejedor,Alberto Nogales*

Main category: cs.CV

TL;DR: 本文探讨了结合自编码器与RNN、3D CNN等架构的混合深度学习方法在视频帧预测中的应用，结果表明3D CNN和ConvLSTM的混合模型效果最佳。


<details>
  <summary>Details</summary>
Motivation: 视频帧预测在天气预报和自动驾驶等领域有重要应用，但现有模型仍有改进空间。

Method: 结合自编码器的特征提取能力与RNN、3D CNN等架构进行时序建模。

Result: 在三个数据集上的实验显示，SSIM指标从0.69提升至0.82，3D CNN和ConvLSTM的混合模型表现最优。

Conclusion: 混合模型在视频帧预测中效果显著，尤其是3D CNN和ConvLSTM的组合，且灰度视频的预测更容易。

Abstract: In recent years, advances in Artificial Intelligence have significantly impacted computer science, particularly in the field of computer vision, enabling solutions to complex problems such as video frame prediction. Video frame prediction has critical applications in weather forecasting or autonomous systems and can provide technical improvements, such as video compression and streaming. Among Artificial Intelligence methods, Deep Learning has emerged as highly effective for solving vision-related tasks, although current frame prediction models still have room for enhancement. This paper evaluates several hybrid deep learning approaches that combine the feature extraction capabilities of autoencoders with temporal sequence modelling using Recurrent Neural Networks (RNNs), 3D Convolutional Neural Networks (3D CNNs), and related architectures. The proposed solutions were rigorously evaluated on three datasets that differ in terms of synthetic versus real-world scenarios and grayscale versus color imagery. Results demonstrate that the approaches perform well, with SSIM metrics increasing from 0.69 to 0.82, indicating that hybrid models utilizing 3DCNNs and ConvLSTMs are the most effective, and greyscale videos with real data are the easiest to predict.

</details>


### [4] [Latent Diffusion Based Face Enhancement under Degraded Conditions for Forensic Face Recognition](https://arxiv.org/abs/2508.00941)
*Hassan Ugail,Hamad Mansour Alawar,AbdulNasser Abbas Zehi,Ahmed Mohammad Alkendi,Ismail Lujain Jaleel*

Main category: cs.CV

TL;DR: 论文提出了一种基于潜在扩散模型的增强方法，显著提升了低质量法医图像的人脸识别准确率。


<details>
  <summary>Details</summary>
Motivation: 法医图像质量低导致人脸识别系统性能下降，需要一种有效的增强方法。

Method: 使用Flux.1 Kontext Dev管道和Facezoom LoRA适应，测试了7种退化类型，包括压缩伪影、模糊和噪声。

Result: 识别准确率从29.1%提升至84.5%，效果显著。

Conclusion: 扩散增强技术在法医人脸识别中具有实际应用潜力。

Abstract: Face recognition systems experience severe performance degradation when processing low-quality forensic evidence imagery. This paper presents an evaluation of latent diffusion-based enhancement for improving face recognition under forensically relevant degradations. Using a dataset of 3,000 individuals from LFW with 24,000 recognition attempts, we implement the Flux.1 Kontext Dev pipeline with Facezoom LoRA adaptation to test against seven degradation categories, including compression artefacts, blur effects, and noise contamination. Our approach demonstrates substantial improvements, increasing overall recognition accuracy from 29.1% to 84.5% (55.4 percentage point improvement, 95% CI: [54.1, 56.7]). Statistical analysis reveals significant performance gains across all degradation types, with effect sizes exceeding conventional thresholds for practical significance. These findings establish the potential of sophisticated diffusion based enhancement in forensic face recognition applications.

</details>


### [5] [ROVI: A VLM-LLM Re-Captioned Dataset for Open-Vocabulary Instance-Grounded Text-to-Image Generation](https://arxiv.org/abs/2508.01008)
*Cihang Peng,Qiming Hou,Zhong Ren,Kun Zhou*

Main category: cs.CV

TL;DR: ROVI是一个高质量合成数据集，用于实例接地文本到图像生成，通过标注100万张精选网络图像创建。其创新在于重新标注策略，结合VLM和LLM生成视觉描述和类别列表，提升检测效果。ROVI在质量和类别数量上优于现有数据集，训练后的GLIGEN模型表现卓越。


<details>
  <summary>Details</summary>
Motivation: 现有检测数据集在图像质量、分辨率和类别数量上有限，ROVI旨在解决这些问题，提供更全面的实例接地文本到图像生成支持。

Method: 采用重新标注策略，结合VLM生成视觉描述和LLM提取类别列表，用于OVD检测，生成全局提示并捕获次要视觉元素。

Result: ROVI在图像质量、分辨率和类别数量上显著优于现有数据集，训练后的GLIGEN模型在实例接地准确性、提示保真度和美学质量上表现优异。

Conclusion: ROVI为实例接地文本到图像生成提供了高质量数据集和可复现流程，显著提升了模型性能。

Abstract: We present ROVI, a high-quality synthetic dataset for instance-grounded text-to-image generation, created by labeling 1M curated web images. Our key innovation is a strategy called re-captioning, focusing on the pre-detection stage, where a VLM (Vision-Language Model) generates comprehensive visual descriptions that are then processed by an LLM (Large Language Model) to extract a flat list of potential categories for OVDs (Open-Vocabulary Detectors) to detect. This approach yields a global prompt inherently linked to instance annotations while capturing secondary visual elements humans typically overlook. Evaluations show that ROVI exceeds existing detection datasets in image quality and resolution while containing two orders of magnitude more categories with an open-vocabulary nature. For demonstrative purposes, a text-to-image model GLIGEN trained on ROVI significantly outperforms state-of-the-art alternatives in instance grounding accuracy, prompt fidelity, and aesthetic quality. Our dataset and reproducible pipeline are available at https://github.com/CihangPeng/ROVI.

</details>


### [6] [Trans-Adapter: A Plug-and-Play Framework for Transparent Image Inpainting](https://arxiv.org/abs/2508.01098)
*Yuekun Dai,Haitian Li,Shangchen Zhou,Chen Change Loy*

Main category: cs.CV

TL;DR: Trans-Adapter是一种即插即用的适配器，使基于扩散的图像修复模型能够直接处理透明图像，解决了传统方法中透明一致性和边缘锯齿问题。


<details>
  <summary>Details</summary>
Motivation: 现有图像修复方法仅适用于RGB图像，传统透明图像修复方法存在透明一致性差和边缘锯齿问题。

Method: 提出Trans-Adapter适配器，支持ControlNet可控编辑，并引入LayerBench和新的透明度边缘质量评估指标。

Result: 实验证明Trans-Adapter在透明图像修复中表现优异。

Conclusion: Trans-Adapter为透明图像修复提供了高效且可控的解决方案。

Abstract: RGBA images, with the additional alpha channel, are crucial for any application that needs blending, masking, or transparency effects, making them more versatile than standard RGB images. Nevertheless, existing image inpainting methods are designed exclusively for RGB images. Conventional approaches to transparent image inpainting typically involve placing a background underneath RGBA images and employing a two-stage process: image inpainting followed by image matting. This pipeline, however, struggles to preserve transparency consistency in edited regions, and matting can introduce jagged edges along transparency boundaries. To address these challenges, we propose Trans-Adapter, a plug-and-play adapter that enables diffusion-based inpainting models to process transparent images directly. Trans-Adapter also supports controllable editing via ControlNet and can be seamlessly integrated into various community models. To evaluate our method, we introduce LayerBench, along with a novel non-reference alpha edge quality evaluation metric for assessing transparency edge quality. We conduct extensive experiments on LayerBench to demonstrate the effectiveness of our approach.

</details>


### [7] [UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction, Forecasting, and Generation](https://arxiv.org/abs/2508.01126)
*Chaitanya Patel,Hiroki Nakamura,Yuta Kyuragi,Kazuki Kozuka,Juan Carlos Niebles,Ehsan Adeli*

Main category: cs.CV

TL;DR: 该论文提出了Egocentric Motion Generation和Egocentric Motion Forecasting两个新任务，并开发了UniEgoMotion模型，利用第一人称图像进行场景感知运动合成，无需依赖显式3D场景。


<details>
  <summary>Details</summary>
Motivation: 提升AR/VR体验、改善人机交互、推动辅助技术和自适应医疗解决方案，需要从第一人称视角准确预测和模拟运动。现有方法多关注第三人称运动合成，难以适应第一人称视角的限制。

Method: 提出UniEgoMotion，一种统一的基于条件扩散的模型，采用头部中心运动表示，支持从第一人称视觉输入中重建、预测和生成运动。

Result: UniEgoMotion在运动重建中表现优异，首次实现从单张第一人称图像生成运动，并建立了新的基准。

Conclusion: UniEgoMotion为第一人称运动建模设定了新标准，拓展了相关应用的可能性。

Abstract: Egocentric human motion generation and forecasting with scene-context is crucial for enhancing AR/VR experiences, improving human-robot interaction, advancing assistive technologies, and enabling adaptive healthcare solutions by accurately predicting and simulating movement from a first-person perspective. However, existing methods primarily focus on third-person motion synthesis with structured 3D scene contexts, limiting their effectiveness in real-world egocentric settings where limited field of view, frequent occlusions, and dynamic cameras hinder scene perception. To bridge this gap, we introduce Egocentric Motion Generation and Egocentric Motion Forecasting, two novel tasks that utilize first-person images for scene-aware motion synthesis without relying on explicit 3D scene. We propose UniEgoMotion, a unified conditional motion diffusion model with a novel head-centric motion representation tailored for egocentric devices. UniEgoMotion's simple yet effective design supports egocentric motion reconstruction, forecasting, and generation from first-person visual inputs in a unified framework. Unlike previous works that overlook scene semantics, our model effectively extracts image-based scene context to infer plausible 3D motion. To facilitate training, we introduce EE4D-Motion, a large-scale dataset derived from EgoExo4D, augmented with pseudo-ground-truth 3D motion annotations. UniEgoMotion achieves state-of-the-art performance in egocentric motion reconstruction and is the first to generate motion from a single egocentric image. Extensive evaluations demonstrate the effectiveness of our unified framework, setting a new benchmark for egocentric motion modeling and unlocking new possibilities for egocentric applications.

</details>


### [8] [Dataset Condensation with Color Compensation](https://arxiv.org/abs/2508.01139)
*Huyu Wu,Duo Su,Junjie Hou,Guang Li*

Main category: cs.CV

TL;DR: 论文提出DC3框架，通过颜色补偿解决数据集压缩中的性能与保真度问题，利用潜在扩散模型增强颜色多样性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在数据集压缩中存在效率低或语义失真的问题，忽视了颜色作为信息载体和语义单元的双重作用。

Method: 提出DC3框架，结合校准选择策略和潜在扩散模型，增强图像颜色多样性而非生成全新图像。

Result: 实验表明DC3在多个基准测试中优于现有方法，且能避免模型崩溃或性能下降。

Conclusion: DC3是首个利用预训练扩散模型微调压缩数据集的研究，展示了高质量数据集训练的可行性。

Abstract: Dataset condensation always faces a constitutive trade-off: balancing performance and fidelity under extreme compression. Existing methods struggle with two bottlenecks: image-level selection methods (Coreset Selection, Dataset Quantization) suffer from inefficiency condensation, while pixel-level optimization (Dataset Distillation) introduces semantic distortion due to over-parameterization. With empirical observations, we find that a critical problem in dataset condensation is the oversight of color's dual role as an information carrier and a basic semantic representation unit. We argue that improving the colorfulness of condensed images is beneficial for representation learning. Motivated by this, we propose DC3: a Dataset Condensation framework with Color Compensation. After a calibrated selection strategy, DC3 utilizes the latent diffusion model to enhance the color diversity of an image rather than creating a brand-new one. Extensive experiments demonstrate the superior performance and generalization of DC3 that outperforms SOTA methods across multiple benchmarks. To the best of our knowledge, besides focusing on downstream tasks, DC3 is the first research to fine-tune pre-trained diffusion models with condensed datasets. The FID results prove that training networks with our high-quality datasets is feasible without model collapse or other degradation issues. Code and generated data will be released soon.

</details>


### [9] [OpenGS-Fusion: Open-Vocabulary Dense Mapping with Hybrid 3D Gaussian Splatting for Refined Object-Level Understanding](https://arxiv.org/abs/2508.01150)
*Dianyi Yang,Xihan Wang,Yu Gao,Shiyang Liu,Bohan Ren,Yufeng Yue,Yi Yang*

Main category: cs.CV

TL;DR: OpenGS-Fusion提出了一种创新的开放词汇密集映射框架，结合3D高斯表示和截断符号距离场，提升语义建模和对象级理解。


<details>
  <summary>Details</summary>
Motivation: 现有方法在开放词汇查询下存在离线流程僵化和对象级理解不精确的问题。

Method: 结合3D高斯表示与截断符号距离场，提出多模态语言引导的自适应阈值方法。

Result: 在3D mIoU上比固定阈值策略提升17%，在对象理解和场景重建质量上优于现有方法。

Conclusion: OpenGS-Fusion在语言引导场景交互中表现出色，代码已开源。

Abstract: Recent advancements in 3D scene understanding have made significant strides in enabling interaction with scenes using open-vocabulary queries, particularly for VR/AR and robotic applications. Nevertheless, existing methods are hindered by rigid offline pipelines and the inability to provide precise 3D object-level understanding given open-ended queries. In this paper, we present OpenGS-Fusion, an innovative open-vocabulary dense mapping framework that improves semantic modeling and refines object-level understanding. OpenGS-Fusion combines 3D Gaussian representation with a Truncated Signed Distance Field to facilitate lossless fusion of semantic features on-the-fly. Furthermore, we introduce a novel multimodal language-guided approach named MLLM-Assisted Adaptive Thresholding, which refines the segmentation of 3D objects by adaptively adjusting similarity thresholds, achieving an improvement 17\% in 3D mIoU compared to the fixed threshold strategy. Extensive experiments demonstrate that our method outperforms existing methods in 3D object understanding and scene reconstruction quality, as well as showcasing its effectiveness in language-guided scene interaction. The code is available at https://young-bit.github.io/opengs-fusion.github.io/ .

</details>


### [10] [LawDIS: Language-Window-based Controllable Dichotomous Image Segmentation](https://arxiv.org/abs/2508.01152)
*Xinyu Yan,Meijun Sun,Ge-Peng Ji,Fahad Shahbaz Khan,Salman Khan,Deng-Ping Fan*

Main category: cs.CV

TL;DR: LawDIS是一个基于语言窗口的可控二分图像分割框架，通过语言提示和窗口细化策略生成高质量对象掩码。


<details>
  <summary>Details</summary>
Motivation: 解决现有二分图像分割方法在用户控制和个性化应用中的不足。

Method: 采用语言控制的初始掩码生成（LS）和窗口控制的细化（WR）策略，结合潜在扩散模型。

Result: 在DIS5K基准测试中显著优于11种先进方法，Fβω提升4.6%。

Conclusion: LawDIS框架在用户控制和精度方面表现出色，适用于高精度个性化应用。

Abstract: We present LawDIS, a language-window-based controllable dichotomous image segmentation (DIS) framework that produces high-quality object masks. Our framework recasts DIS as an image-conditioned mask generation task within a latent diffusion model, enabling seamless integration of user controls. LawDIS is enhanced with macro-to-micro control modes. Specifically, in macro mode, we introduce a language-controlled segmentation strategy (LS) to generate an initial mask based on user-provided language prompts. In micro mode, a window-controlled refinement strategy (WR) allows flexible refinement of user-defined regions (i.e., size-adjustable windows) within the initial mask. Coordinated by a mode switcher, these modes can operate independently or jointly, making the framework well-suited for high-accuracy, personalised applications. Extensive experiments on the DIS5K benchmark reveal that our LawDIS significantly outperforms 11 cutting-edge methods across all metrics. Notably, compared to the second-best model MVANet, we achieve $F_\beta^\omega$ gains of 4.6\% with both the LS and WR strategies and 3.6\% gains with only the LS strategy on DIS-TE. Codes will be made available at https://github.com/XinyuYanTJU/LawDIS.

</details>


### [11] [No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views](https://arxiv.org/abs/2508.01171)
*Ranran Huang,Krystian Mikolajczyk*

Main category: cs.CV

TL;DR: SPFSplat是一个无需真实姿态的3D高斯溅射框架，通过共享特征提取和单步前馈设计，实现高效的新视角合成和姿态估计。


<details>
  <summary>Details</summary>
Motivation: 解决多视角稀疏图像下3D高斯溅射的效率和姿态依赖问题，提出无需真实姿态的训练和推理框架。

Method: 使用共享特征提取主干，单步前馈预测3D高斯基元和相机姿态，结合渲染损失和重投影损失优化几何约束。

Result: 在无姿态监督下，SPFSplat在新视角合成和相对姿态估计中达到SOTA性能。

Conclusion: SPFSplat的免姿态训练和高效设计使其适用于实际应用，性能优于现有方法。

Abstract: We introduce SPFSplat, an efficient framework for 3D Gaussian splatting from sparse multi-view images, requiring no ground-truth poses during training or inference. It employs a shared feature extraction backbone, enabling simultaneous prediction of 3D Gaussian primitives and camera poses in a canonical space from unposed inputs within a single feed-forward step. Alongside the rendering loss based on estimated novel-view poses, a reprojection loss is integrated to enforce the learning of pixel-aligned Gaussian primitives for enhanced geometric constraints. This pose-free training paradigm and efficient one-step feed-forward design make SPFSplat well-suited for practical applications. Remarkably, despite the absence of pose supervision, SPFSplat achieves state-of-the-art performance in novel view synthesis even under significant viewpoint changes and limited image overlap. It also surpasses recent methods trained with geometry priors in relative pose estimation. Code and trained models are available on our project page: https://ranrhuang.github.io/spfsplat/.

</details>


### [12] [StyDeco: Unsupervised Style Transfer with Distilling Priors and Semantic Decoupling](https://arxiv.org/abs/2508.01215)
*Yuanlin Yang,Quanjian Song,Zhexian Gao,Ge Wang,Shanshan Li,Xiaoyan Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种名为StyDeco的无监督框架，通过专门为风格迁移任务学习文本表示，解决了扩散模型中文本描述作为统一指导的核心限制。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在风格迁移中将文本描述视为统一的指导，忽略了文本描述的非空间性与视觉风格的空间属性之间的语义差距，导致语义结构和细节丢失。

Method: 框架采用Prior-Guided Data Distillation (PGD)策略无监督地提取风格知识，并引入Contrastive Semantic Decoupling (CSD)目标，通过领域特定权重调整文本编码器。

Result: 在三个经典基准测试中，StyDeco在风格保真度和结构保留方面优于现有方法，并支持独特的去风格化过程。

Conclusion: StyDeco框架有效解决了文本驱动风格迁移中的语义保留问题，并展示了其扩展性。

Abstract: Diffusion models have emerged as the dominant paradigm for style transfer, but their text-driven mechanism is hindered by a core limitation: it treats textual descriptions as uniform, monolithic guidance. This limitation overlooks the semantic gap between the non-spatial nature of textual descriptions and the spatially-aware attributes of visual style, often leading to the loss of semantic structure and fine-grained details during stylization. In this paper, we propose StyDeco, an unsupervised framework that resolves this limitation by learning text representations specifically tailored for the style transfer task. Our framework first employs Prior-Guided Data Distillation (PGD), a strategy designed to distill stylistic knowledge without human supervision. It leverages a powerful frozen generative model to automatically synthesize pseudo-paired data. Subsequently, we introduce Contrastive Semantic Decoupling (CSD), a task-specific objective that adapts a text encoder using domain-specific weights. CSD performs a two-class clustering in the semantic space, encouraging source and target representations to form distinct clusters. Extensive experiments on three classic benchmarks demonstrate that our framework outperforms several existing approaches in both stylistic fidelity and structural preservation, highlighting its effectiveness in style transfer with semantic preservation. In addition, our framework supports a unique de-stylization process, further demonstrating its extensibility. Our code is vailable at https://github.com/QuanjianSong/StyDeco.

</details>


### [13] [OCSplats: Observation Completeness Quantification and Label Noise Separation in 3DGS](https://arxiv.org/abs/2508.01239)
*Han Ling,Xian Xu,Yinghui Sun,Quansen Sun*

Main category: cs.CV

TL;DR: 3DGS在3D重建中前景广阔，但现实场景中的标签噪声（如移动物体、非朗伯表面和阴影）会导致重建错误。现有抗噪声方法效果不佳或需场景调参。本文提出OCSplats框架，结合混合噪声评估和基于观察的认知校正，显著提升噪声分类精度，并通过动态锚点实现多场景应用。实验证明其性能领先。


<details>
  <summary>Details</summary>
Motivation: 解决3DGS在现实场景中因标签噪声导致的重建错误问题，现有方法效果有限或需调参。

Method: 提出OCSplats框架，结合混合噪声评估、观察认知校正和动态锚点分类技术。

Result: 显著提升噪声分类精度，适用于不同噪声比例场景，实验性能领先。

Conclusion: OCSplats有效解决3DGS抗噪声重建问题，无需调参且性能优越。

Abstract: 3D Gaussian Splatting (3DGS) has become one of the most promising 3D reconstruction technologies. However, label noise in real-world scenarios-such as moving objects, non-Lambertian surfaces, and shadows-often leads to reconstruction errors. Existing 3DGS-Bsed anti-noise reconstruction methods either fail to separate noise effectively or require scene-specific fine-tuning of hyperparameters, making them difficult to apply in practice. This paper re-examines the problem of anti-noise reconstruction from the perspective of epistemic uncertainty, proposing a novel framework, OCSplats. By combining key technologies such as hybrid noise assessment and observation-based cognitive correction, the accuracy of noise classification in areas with cognitive differences has been significantly improved. Moreover, to address the issue of varying noise proportions in different scenarios, we have designed a label noise classification pipeline based on dynamic anchor points. This pipeline enables OCSplats to be applied simultaneously to scenarios with vastly different noise proportions without adjusting parameters. Extensive experiments demonstrate that OCSplats always achieve leading reconstruction performance and precise label noise classification in scenes of different complexity levels.

</details>


### [14] [NS-Net: Decoupling CLIP Semantic Information through NULL-Space for Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2508.01248)
*Jiazhen Yan,Fan Wang,Weiwei Jiang,Ziqiang Li,Zhangjie Fu*

Main category: cs.CV

TL;DR: 论文提出NS-Net，通过NULL-Space投影和对比学习改进AI生成图像检测，显著提升检测精度。


<details>
  <summary>Details</summary>
Motivation: 生成模型（如GAN和扩散模型）生成的图像越来越逼真，现有检测器在未知模型上泛化能力不足，尤其是语义内容相近时。

Method: 提出NS-Net，利用NULL-Space投影解耦CLIP视觉特征中的语义信息，结合对比学习捕捉真实与生成图像的分布差异，并设计Patch Selection策略保留细粒度伪影。

Result: 在包含40种生成模型的开放世界基准测试中，NS-Net检测精度提升7.4%，优于现有方法。

Conclusion: NS-Net在GAN和扩散模型生成的图像检测中表现出强泛化能力。

Abstract: The rapid progress of generative models, such as GANs and diffusion models, has facilitated the creation of highly realistic images, raising growing concerns over their misuse in security-sensitive domains. While existing detectors perform well under known generative settings, they often fail to generalize to unknown generative models, especially when semantic content between real and fake images is closely aligned. In this paper, we revisit the use of CLIP features for AI-generated image detection and uncover a critical limitation: the high-level semantic information embedded in CLIP's visual features hinders effective discrimination. To address this, we propose NS-Net, a novel detection framework that leverages NULL-Space projection to decouple semantic information from CLIP's visual features, followed by contrastive learning to capture intrinsic distributional differences between real and generated images. Furthermore, we design a Patch Selection strategy to preserve fine-grained artifacts by mitigating semantic bias caused by global image structures. Extensive experiments on an open-world benchmark comprising images generated by 40 diverse generative models show that NS-Net outperforms existing state-of-the-art methods, achieving a 7.4\% improvement in detection accuracy, thereby demonstrating strong generalization across both GAN- and diffusion-based image generation techniques.

</details>


### [15] [SpatioTemporal Difference Network for Video Depth Super-Resolution](https://arxiv.org/abs/2508.01259)
*Zhengxue Wang,Yuan Wu,Xiang Li,Zhiqiang Yan,Jian Yang*

Main category: cs.CV

TL;DR: 论文提出了一种名为STDNet的时空差异网络，通过空间和时间差异分支解决视频深度超分辨率中的长尾分布问题，显著提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 视频深度超分辨率在空间非平滑区域和时间变化区域存在明显的长尾分布问题，影响了重建质量。

Method: STDNet包含空间差异分支和时间差异分支。空间差异分支通过动态对齐RGB特征和学习空间差异表示来校准深度；时间差异分支通过传播相邻帧的时空差异信息实现精确运动补偿。

Result: 在多个数据集上的实验表明，STDNet优于现有方法。

Conclusion: STDNet有效解决了视频深度超分辨率中的长尾分布问题，提升了重建质量。

Abstract: Depth super-resolution has achieved impressive performance, and the incorporation of multi-frame information further enhances reconstruction quality. Nevertheless, statistical analyses reveal that video depth super-resolution remains affected by pronounced long-tailed distributions, with the long-tailed effects primarily manifesting in spatial non-smooth regions and temporal variation zones. To address these challenges, we propose a novel SpatioTemporal Difference Network (STDNet) comprising two core branches: a spatial difference branch and a temporal difference branch. In the spatial difference branch, we introduce a spatial difference mechanism to mitigate the long-tailed issues in spatial non-smooth regions. This mechanism dynamically aligns RGB features with learned spatial difference representations, enabling intra-frame RGB-D aggregation for depth calibration. In the temporal difference branch, we further design a temporal difference strategy that preferentially propagates temporal variation information from adjacent RGB and depth frames to the current depth frame, leveraging temporal difference representations to achieve precise motion compensation in temporal long-tailed areas. Extensive experimental results across multiple datasets demonstrate the effectiveness of our STDNet, outperforming existing approaches.

</details>


### [16] [PromptSafe: Gated Prompt Tuning for Safe Text-to-Image Generation](https://arxiv.org/abs/2508.01272)
*Zonglei Jing,Xiao Yang,Xiaoqian Li,Siyuan Liang,Aishan Liu,Mingchuan Zhang,Xianglong Liu*

Main category: cs.CV

TL;DR: PromptSafe是一个基于门控提示调优的框架，用于防止文本到图像（T2I）模型生成不安全内容，同时保持良性图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大规模图像-文本数据集，计算成本高且适应性差，无法满足多样化的安全需求。

Method: 通过LLM重写不安全提示，构建文本训练语料库，优化通用软提示，并引入门控机制动态调整防御强度。

Result: 在多个基准测试中，PromptSafe将不安全生成率降至2.36%，同时保持高良性保真度。

Conclusion: PromptSafe在安全性、泛化性和适应性方面表现出色，具有实际部署价值。

Abstract: Text-to-image (T2I) models have demonstrated remarkable generative capabilities but remain vulnerable to producing not-safe-for-work (NSFW) content, such as violent or explicit imagery. While recent moderation efforts have introduced soft prompt-guided tuning by appending defensive tokens to the input, these approaches often rely on large-scale curated image-text datasets and apply static, one-size-fits-all defenses at inference time. However, this results not only in high computational cost and degraded benign image quality, but also in limited adaptability to the diverse and nuanced safety requirements of real-world prompts. To address these challenges, we propose PromptSafe, a gated prompt tuning framework that combines a lightweight, text-only supervised soft embedding with an inference-time gated control network. Instead of training on expensive image-text datasets, we first rewrite unsafe prompts into semantically aligned but safe alternatives using an LLM, constructing an efficient text-only training corpus. Based on this, we optimize a universal soft prompt that repels unsafe and attracts safe embeddings during the diffusion denoising process. To avoid over-suppressing benign prompts, we introduce a gated mechanism that adaptively adjusts the defensive strength based on estimated prompt toxicity, thereby aligning defense intensity with prompt risk and ensuring strong protection for harmful inputs while preserving benign generation quality. Extensive experiments across multiple benchmarks and T2I models show that PromptSafe achieves a SOTA unsafe generation rate (2.36%), while preserving high benign fidelity. Furthermore, PromptSafe demonstrates strong generalization to unseen harmful categories, robust transferability across diffusion model architectures, and resilience under adaptive adversarial attacks, highlighting its practical value for safe and scalable deployment.

</details>


### [17] [Construction of Digital Terrain Maps from Multi-view Satellite Imagery using Neural Volume Rendering](https://arxiv.org/abs/2508.01386)
*Josef X. Biberstein,Guilherme Cavalheiro,Juyeop Han,Sertac Karaman*

Main category: cs.CV

TL;DR: 本文提出了一种名为神经地形图（NTM）的方法，通过神经体积渲染技术直接从卫星图像学习地形图，避免了传统多视图立体流程的繁琐预处理。


<details>
  <summary>Details</summary>
Motivation: 随着机器人探索任务的复杂性增加，高质量数字地形图（DTM）的需求日益增长，但现有方法需要大量手动预处理，效率低下。

Method: 采用神经体积渲染技术，仅需每个图像像素的轨迹信息，无需深度或其他结构先验，直接从卫星图像学习地形图。

Result: 在合成和真实卫星数据（地球和火星）上验证，覆盖面积约100平方公里，地形预测精度接近卫星图像分辨率。

Conclusion: NTM方法在相机参数不完美的情况下仍能提供高精度地形图，展示了其潜力。

Abstract: Digital terrain maps (DTMs) are an important part of planetary exploration, enabling operations such as terrain relative navigation during entry, descent, and landing for spacecraft and aiding in navigation on the ground. As robotic exploration missions become more ambitious, the need for high quality DTMs will only increase. However, producing DTMs via multi-view stereo pipelines for satellite imagery, the current state-of-the-art, can be cumbersome and require significant manual image preprocessing to produce satisfactory results. In this work, we seek to address these shortcomings by adapting neural volume rendering techniques to learn textured digital terrain maps directly from satellite imagery. Our method, neural terrain maps (NTM), only requires the locus for each image pixel and does not rely on depth or any other structural priors. We demonstrate our method on both synthetic and real satellite data from Earth and Mars encompassing scenes on the order of $100 \textrm{km}^2$. We evaluate the accuracy of our output terrain maps by comparing with existing high-quality DTMs produced using traditional multi-view stereo pipelines. Our method shows promising results, with the precision of terrain prediction almost equal to the resolution of the satellite images even in the presence of imperfect camera intrinsics and extrinsics.

</details>


### [18] [Can3Tok: Canonical 3D Tokenization and Latent Modeling of Scene-Level 3D Gaussians](https://arxiv.org/abs/2508.01464)
*Quankai Gao,Iliyan Georgiev,Tuanfeng Y. Wang,Krishna Kumar Singh,Ulrich Neumann,Jae Shin Yoon*

Main category: cs.CV

TL;DR: Can3Tok是首个3D场景级变分自编码器（VAE），能够将大量高斯基元编码为低维潜在嵌入，解决了3D场景生成中的尺度不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成技术主要针对对象级，场景级生成因缺乏统一潜在表示学习模型而鲜有探索。

Method: 提出Can3Tok模型及通用3D场景数据处理流程，解决尺度不一致问题。

Result: 在DL3DV-10K数据集上验证，Can3Tok能泛化到新场景，而其他方法无法收敛。

Conclusion: Can3Tok成功应用于图像到3DGS和文本到3DGS生成任务，展示了其在下游生成任务中的潜力。

Abstract: 3D generation has made significant progress, however, it still largely remains at the object-level. Feedforward 3D scene-level generation has been rarely explored due to the lack of models capable of scaling-up latent representation learning on 3D scene-level data. Unlike object-level generative models, which are trained on well-labeled 3D data in a bounded canonical space, scene-level generations with 3D scenes represented by 3D Gaussian Splatting (3DGS) are unbounded and exhibit scale inconsistency across different scenes, making unified latent representation learning for generative purposes extremely challenging. In this paper, we introduce Can3Tok, the first 3D scene-level variational autoencoder (VAE) capable of encoding a large number of Gaussian primitives into a low-dimensional latent embedding, which effectively captures both semantic and spatial information of the inputs. Beyond model design, we propose a general pipeline for 3D scene data processing to address scale inconsistency issue. We validate our method on the recent scene-level 3D dataset DL3DV-10K, where we found that only Can3Tok successfully generalizes to novel 3D scenes, while compared methods fail to converge on even a few hundred scene inputs during training and exhibit zero generalization ability during inference. Finally, we demonstrate image-to-3DGS and text-to-3DGS generation as our applications to demonstrate its ability to facilitate downstream generation tasks.

</details>


### [19] [ReasonAct: Progressive Training for Fine-Grained Video Reasoning in Small Models](https://arxiv.org/abs/2508.01533)
*Jiaxin Liu,Zhaolu Kang*

Main category: cs.CV

TL;DR: ReasonAct通过三阶段训练提升小型模型在视频理解中的细粒度时序推理能力，结合T-GRPO与时序一致性建模，提出生物力学启发的子动作分解机制，在多个数据集上显著超越基线。


<details>
  <summary>Details</summary>
Motivation: 解决小型多模态模型在视频理解中细粒度时序推理的不足。

Method: 三阶段训练：文本推理基础训练、视频微调、时序感知强化学习优化；结合T-GRPO与时序一致性建模；提出子动作分解机制。

Result: 在HMDB51、UCF-101和Kinetics-400上分别达到67.2%、94.1%和78.9%的准确率，显著优于基线。

Conclusion: 渐进式训练方法使小型模型在保持计算效率的同时实现竞争性视频推理性能。

Abstract: While recent multimodal models have shown progress in vision-language tasks, small-scale variants still struggle with the fine-grained temporal reasoning required for video understanding. We introduce ReasonAct, a method that enhances video reasoning in smaller models through a three-stage training process: first building a foundation with text-only reasoning, then fine-tuning on video, and finally refining with temporal-aware reinforcement learning. We build upon Temporal Group Relative Policy Optimization (T-GRPO) by incorporating temporal consistency modeling into policy optimization. We also propose a biomechanically-motivated sub-action decomposition mechanism that provides graduated rewards for constituent action phases. Through experiments on HMDB51, UCF-101, and Kinetics-400, our 3B-parameter model achieves 67.2%, 94.1%, and 78.9% accuracy respectively, demonstrating improvements of 17.9, 15.8, and 12.3 points over baselines. Ablation studies validate that our progressive training methodology enables smaller models to achieve competitive video reasoning performance while maintaining computational efficiency.

</details>


### [20] [LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical Image Understanding](https://arxiv.org/abs/2508.01617)
*Xuanzhao Dong,Wenhui Zhu,Xiwen Chen,Zhipeng Wang,Peijie Qiu,Shao Tang,Xin Li,Yalin Wang*

Main category: cs.CV

TL;DR: LLaDA-MedV是一种基于扩散模型的大型语言模型，专为生物医学图像理解设计，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 填补扩散模型在生物医学视觉语言模型领域的空白，提升性能。

Method: 通过视觉指令调优，结合初始化权重选择、微调策略和采样步骤优化。

Result: 在开放和封闭任务中均取得最佳性能，生成更长且信息更丰富的回答。

Conclusion: LLaDA-MedV在生物医学领域表现出色，为扩散模型的应用提供了新方向。

Abstract: Autoregressive models (ARMs) have long dominated the landscape of biomedical vision-language models (VLMs). Recently, masked diffusion models such as LLaDA have emerged as promising alternatives, yet their application in the biomedical domain remains largely underexplored. To bridge this gap, we introduce \textbf{LLaDA-MedV}, the first large language diffusion model tailored for biomedical image understanding through vision instruction tuning. LLaDA-MedV achieves relative performance gains of 7.855\% over LLaVA-Med and 1.867\% over LLaDA-V in the open-ended biomedical visual conversation task, and sets new state-of-the-art accuracy on the closed-form subset of three VQA benchmarks: 84.93\% on VQA-RAD, 92.31\% on SLAKE, and 95.15\% on PathVQA. Furthermore, a detailed comparison with LLaVA-Med suggests that LLaDA-MedV is capable of generating reasonably longer responses by explicitly controlling response length, which can lead to more informative outputs. We also conduct an in-depth analysis of both the training and inference stages, highlighting the critical roles of initialization weight selection, fine-tuning strategies, and the interplay between sampling steps and response repetition. The code and model weight is released at https://github.com/LLM-VLM-GSL/LLaDA-MedV.

</details>


### [21] [StrandDesigner: Towards Practical Strand Generation with Sketch Guidance](https://arxiv.org/abs/2508.01650)
*Na Zhang,Moran Li,Chengming Xu,Han Feng,Xiaobin Hu,Jiangning Zhang,Weijian Cao,Chengjie Wang,Yanwei Fu*

Main category: cs.CV

TL;DR: 提出首个基于草图的发丝生成模型，通过可学习的发丝上采样和多尺度自适应条件机制提升生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本或图像的扩散模型在发丝生成中缺乏精确性和用户友好性，草图输入能提供更精细的控制。

Method: 采用可学习的发丝上采样策略和多尺度自适应条件机制（基于Transformer和扩散头），解决发丝交互和草图多样性的挑战。

Result: 在多个基准数据集上表现优于现有方法，生成效果更真实和精确。

Conclusion: 该方法在发丝生成中实现了更高的控制性和用户友好性，代码将开源。

Abstract: Realistic hair strand generation is crucial for applications like computer graphics and virtual reality. While diffusion models can generate hairstyles from text or images, these inputs lack precision and user-friendliness. Instead, we propose the first sketch-based strand generation model, which offers finer control while remaining user-friendly. Our framework tackles key challenges, such as modeling complex strand interactions and diverse sketch patterns, through two main innovations: a learnable strand upsampling strategy that encodes 3D strands into multi-scale latent spaces, and a multi-scale adaptive conditioning mechanism using a transformer with diffusion heads to ensure consistency across granularity levels. Experiments on several benchmark datasets show our method outperforms existing approaches in realism and precision. Qualitative results further confirm its effectiveness. Code will be released at [GitHub](https://github.com/fighting-Zhang/StrandDesigner).

</details>


### [22] [DAG: Unleash the Potential of Diffusion Model for Open-Vocabulary 3D Affordance Grounding](https://arxiv.org/abs/2508.01651)
*Hanqing Wang,Zhenhao Zhang,Kaiyang Ji,Mingyu Liu,Wenti Yin,Yuchao Chen,Zhirui Liu,Xiangyu Zeng,Tianxiang Gui,Hangxing Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的3D物体可供性（affordance）定位方法DAG，通过利用文本到图像扩散模型的内部表示空间，解决了现有方法泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过学习演示图像预测3D物体的可触摸区域，但无法捕捉图像中的通用可供性知识，导致泛化能力不足。

Method: 提出DAG框架，利用文本到图像扩散模型的冻结内部表示空间，结合可供性模块和多源可供性解码器，实现3D密集可供性预测。

Result: 实验表明，DAG在性能上优于现有方法，并展现出开放世界的泛化能力。

Conclusion: DAG通过扩散模型的内部表示空间成功提取了通用可供性知识，为3D可供性定位提供了新的解决方案。

Abstract: 3D object affordance grounding aims to predict the touchable regions on a 3d object, which is crucial for human-object interaction, human-robot interaction, embodied perception, and robot learning. Recent advances tackle this problem via learning from demonstration images. However, these methods fail to capture the general affordance knowledge within the image, leading to poor generalization. To address this issue, we propose to use text-to-image diffusion models to extract the general affordance knowledge because we find that such models can generate semantically valid HOI images, which demonstrate that their internal representation space is highly correlated with real-world affordance concepts. Specifically, we introduce the DAG, a diffusion-based 3d affordance grounding framework, which leverages the frozen internal representations of the text-to-image diffusion model and unlocks affordance knowledge within the diffusion model to perform 3D affordance grounding. We further introduce an affordance block and a multi-source affordance decoder to endow 3D dense affordance prediction. Extensive experimental evaluations show that our model excels over well-established methods and exhibits open-world generalization.

</details>


### [23] [DisCo3D: Distilling Multi-View Consistency for 3D Scene Editing](https://arxiv.org/abs/2508.01684)
*Yufeng Chi,Huimin Ma,Kafeng Wang,Jianmin Li*

Main category: cs.CV

TL;DR: DisCo3D提出了一种新框架，通过将3D一致性先验知识蒸馏到2D编辑器中，解决了3D编辑中的多视角一致性问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在2D图像生成和编辑中表现出色，但扩展到3D编辑时面临多视角一致性的挑战。现有方法存在收敛慢、模糊伪影和细粒度不一致等问题。

Method: DisCo3D首先通过多视角输入微调3D生成器以适应场景，然后通过一致性蒸馏训练2D编辑器，最后通过高斯泼溅将编辑后的多视角输出优化为3D表示。

Result: 实验结果表明，DisCo3D在多视角一致性方面表现稳定，编辑质量优于现有方法。

Conclusion: DisCo3D通过一致性蒸馏和高斯泼溅优化，有效解决了3D编辑中的多视角一致性问题，提升了编辑质量。

Abstract: While diffusion models have demonstrated remarkable progress in 2D image generation and editing, extending these capabilities to 3D editing remains challenging, particularly in maintaining multi-view consistency. Classical approaches typically update 3D representations through iterative refinement based on a single editing view. However, these methods often suffer from slow convergence and blurry artifacts caused by cross-view inconsistencies. Recent methods improve efficiency by propagating 2D editing attention features, yet still exhibit fine-grained inconsistencies and failure modes in complex scenes due to insufficient constraints. To address this, we propose \textbf{DisCo3D}, a novel framework that distills 3D consistency priors into a 2D editor. Our method first fine-tunes a 3D generator using multi-view inputs for scene adaptation, then trains a 2D editor through consistency distillation. The edited multi-view outputs are finally optimized into 3D representations via Gaussian Splatting. Experimental results show DisCo3D achieves stable multi-view consistency and outperforms state-of-the-art methods in editing quality.

</details>


### [24] [Versatile Transition Generation with Image-to-Video Diffusion](https://arxiv.org/abs/2508.01698)
*Zuhao Yang,Jiahui Zhang,Yingchen Yu,Shijian Lu,Song Bai*

Main category: cs.CV

TL;DR: VTG是一个多功能过渡视频生成框架，通过插值初始化和双方向运动微调等技术，生成平滑、高保真且语义连贯的视频过渡。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的视频生成方法在给定首尾帧和文本提示时，生成平滑合理的过渡视频仍未被充分探索。

Method: VTG采用插值初始化、双方向运动微调和表示对齐正则化技术。

Result: VTG在TransitBench基准测试中表现出色，在概念混合和场景过渡任务中均取得优异性能。

Conclusion: VTG为过渡视频生成提供了高效解决方案，并推动了统一过渡生成研究的进展。

Abstract: Leveraging text, images, structure maps, or motion trajectories as conditional guidance, diffusion models have achieved great success in automated and high-quality video generation. However, generating smooth and rational transition videos given the first and last video frames as well as descriptive text prompts is far underexplored. We present VTG, a Versatile Transition video Generation framework that can generate smooth, high-fidelity, and semantically coherent video transitions. VTG introduces interpolation-based initialization that helps preserve object identity and handle abrupt content changes effectively. In addition, it incorporates dual-directional motion fine-tuning and representation alignment regularization to mitigate the limitations of pre-trained image-to-video diffusion models in motion smoothness and generation fidelity, respectively. To evaluate VTG and facilitate future studies on unified transition generation, we collected TransitBench, a comprehensive benchmark for transition generation covering two representative transition tasks: concept blending and scene transition. Extensive experiments show that VTG achieves superior transition performance consistently across all four tasks.

</details>


### [25] [LT-Gaussian: Long-Term Map Update Using 3D Gaussian Splatting for Autonomous Driving](https://arxiv.org/abs/2508.01704)
*Luqi Cheng,Zhangshuo Qi,Zijie Zhou,Chao Lu,Guangming Xiong*

Main category: cs.CV

TL;DR: 提出LT-Gaussian方法，用于高效更新基于3D高斯泼溅的地图，解决自动驾驶场景中地图更新的挑战。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅（3D-GS）在自动驾驶地图构建中潜力显著，但生成和更新高斯场景的时间和计算成本高，需解决地图更新问题。

Method: LT-Gaussian包含多模态高斯泼溅、结构变化检测模块和高斯地图更新模块，通过对比旧地图与当前LiDAR数据实现针对性更新。

Result: 实验表明，LT-Gaussian能高效更新地图，处理常见环境变化，且重建质量优于从头构建的策略。

Conclusion: LT-Gaussian为自动驾驶地图更新提供了一种高效且高质量的解决方案。

Abstract: Maps play an important role in autonomous driving systems. The recently proposed 3D Gaussian Splatting (3D-GS) produces rendering-quality explicit scene reconstruction results, demonstrating the potential for map construction in autonomous driving scenarios. However, because of the time and computational costs involved in generating Gaussian scenes, how to update the map becomes a significant challenge. In this paper, we propose LT-Gaussian, a map update method for 3D-GS-based maps. LT-Gaussian consists of three main components: Multimodal Gaussian Splatting, Structural Change Detection Module, and Gaussian-Map Update Module. Firstly, the Gaussian map of the old scene is generated using our proposed Multimodal Gaussian Splatting. Subsequently, during the map update process, we compare the outdated Gaussian map with the current LiDAR data stream to identify structural changes. Finally, we perform targeted updates to the Gaussian-map to generate an up-to-date map. We establish a benchmark for map updating on the nuScenes dataset to quantitatively evaluate our method. The experimental results show that LT-Gaussian can effectively and efficiently update the Gaussian-map, handling common environmental changes in autonomous driving scenarios. Furthermore, by taking full advantage of information from both new and old scenes, LT-Gaussian is able to produce higher quality reconstruction results compared to map update strategies that reconstruct maps from scratch. Our open-source code is available at https://github.com/ChengLuqi/LT-gaussian.

</details>


### [26] [AG$^2$aussian: Anchor-Graph Structured Gaussian Splatting for Instance-Level 3D Scene Understanding and Editing](https://arxiv.org/abs/2508.01740)
*Zhaonan Wang,Manyi Li,Changhe Tu*

Main category: cs.CV

TL;DR: AG$^2$aussian框架通过锚点图结构组织语义特征，优化3D高斯表示，实现更清晰的实例级分割和编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D高斯表示中附加语义特征时导致噪声分割和高斯选择混乱，需要更有效的解决方案。

Method: 利用锚点图结构组织语义特征并规范高斯基元，促进紧凑且实例感知的高斯分布。

Result: 在四种应用中验证了方法的优势，包括交互式查询、文本驱动查询、对象移除编辑和物理模拟。

Conclusion: AG$^2$aussian通过锚点图结构显著提升了语义感知3D高斯的准确性和应用效果。

Abstract: 3D Gaussian Splatting (3DGS) has witnessed exponential adoption across diverse applications, driving a critical need for semantic-aware 3D Gaussian representations to enable scene understanding and editing tasks. Existing approaches typically attach semantic features to a collection of free Gaussians and distill the features via differentiable rendering, leading to noisy segmentation and a messy selection of Gaussians. In this paper, we introduce AG$^2$aussian, a novel framework that leverages an anchor-graph structure to organize semantic features and regulate Gaussian primitives. Our anchor-graph structure not only promotes compact and instance-aware Gaussian distributions, but also facilitates graph-based propagation, achieving a clean and accurate instance-level Gaussian selection. Extensive validation across four applications, i.e. interactive click-based query, open-vocabulary text-driven query, object removal editing, and physics simulation, demonstrates the advantages of our approach and its benefits to various applications. The experiments and ablation studies further evaluate the effectiveness of the key designs of our approach.

</details>


### [27] [Diffusion-based 3D Hand Motion Recovery with Intuitive Physics](https://arxiv.org/abs/2508.01835)
*Yufei Zhang,Zijun Cui,Jeffrey O. Kephart,Qiang Ji*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型和物理增强的运动优化框架，用于从单目视频中恢复3D手部运动，显著提升了重建的准确性和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管从单目图像进行3D手部重建已取得进展，但在视频中生成准确且时间一致的运动估计仍具挑战性，尤其是在手与物体交互时。

Method: 通过扩散模型和物理增强的运动优化模型，利用运动捕捉数据（无需图像）训练模型，并结合物理知识（如关键运动状态及其约束）优化运动序列。

Result: 实验表明，该方法显著提升了多种帧级重建方法的性能，并在现有基准测试中达到最优（SOTA）水平。

Conclusion: 该框架通过结合扩散模型和物理知识，有效提升了3D手部运动重建的准确性和一致性。

Abstract: While 3D hand reconstruction from monocular images has made significant progress, generating accurate and temporally coherent motion estimates from videos remains challenging, particularly during hand-object interactions. In this paper, we present a novel 3D hand motion recovery framework that enhances image-based reconstructions through a diffusion-based and physics-augmented motion refinement model. Our model captures the distribution of refined motion estimates conditioned on initial ones, generating improved sequences through an iterative denoising process. Instead of relying on scarce annotated video data, we train our model only using motion capture data without images. We identify valuable intuitive physics knowledge during hand-object interactions, including key motion states and their associated motion constraints. We effectively integrate these physical insights into our diffusion model to improve its performance. Extensive experiments demonstrate that our approach significantly improves various frame-wise reconstruction methods, achieving state-of-the-art (SOTA) performance on existing benchmarks.

</details>


### [28] [Beyond Vulnerabilities: A Survey of Adversarial Attacks as Both Threats and Defenses in Computer Vision Systems](https://arxiv.org/abs/2508.01845)
*Zhongliang Guo,Yifei Qian,Yanli Li,Weiye Li,Chun Tong Lei,Shuai Zhao,Lei Fang,Ognjen Arandjelović,Chun Pong Lau*

Main category: cs.CV

TL;DR: 本文综述了对抗性攻击在计算机视觉系统中的双重角色，分析了像素空间、物理可实现和潜在空间攻击方法，并探讨了其在防御和漏洞评估中的应用。


<details>
  <summary>Details</summary>
Motivation: 对抗性攻击揭示了神经网络鲁棒性和安全性的根本问题，研究其方法和应用有助于提升系统安全性。

Method: 系统分析了三种对抗性攻击方法：像素空间攻击、物理可实现攻击和潜在空间攻击，并追踪了从梯度基础方法到高级优化技术的演变。

Result: 研究发现对抗性攻击不仅是安全威胁，还可用于防御和漏洞评估，同时揭示了神经风格转移保护和计算效率的研究空白。

Conclusion: 本文提供了对抗性攻击的全面分类和未来研究方向，旨在推动更鲁棒和可信的计算机视觉系统发展。

Abstract: Adversarial attacks against computer vision systems have emerged as a critical research area that challenges the fundamental assumptions about neural network robustness and security. This comprehensive survey examines the evolving landscape of adversarial techniques, revealing their dual nature as both sophisticated security threats and valuable defensive tools. We provide a systematic analysis of adversarial attack methodologies across three primary domains: pixel-space attacks, physically realizable attacks, and latent-space attacks. Our investigation traces the technical evolution from early gradient-based methods such as FGSM and PGD to sophisticated optimization techniques incorporating momentum, adaptive step sizes, and advanced transferability mechanisms. We examine how physically realizable attacks have successfully bridged the gap between digital vulnerabilities and real-world threats through adversarial patches, 3D textures, and dynamic optical perturbations. Additionally, we explore the emergence of latent-space attacks that leverage semantic structure in internal representations to create more transferable and meaningful adversarial examples. Beyond traditional offensive applications, we investigate the constructive use of adversarial techniques for vulnerability assessment in biometric authentication systems and protection against malicious generative models. Our analysis reveals critical research gaps, particularly in neural style transfer protection and computational efficiency requirements. This survey contributes a comprehensive taxonomy, evolution analysis, and identification of future research directions, aiming to advance understanding of adversarial vulnerabilities and inform the development of more robust and trustworthy computer vision systems.

</details>


### [29] [DiffusionFF: Face Forgery Detection via Diffusion-based Artifact Localization](https://arxiv.org/abs/2508.01873)
*Siran Peng,Haoyuan Zhang,Li Gao,Tianshuo Zhang,Bao Li,Zhen Lei*

Main category: cs.CV

TL;DR: DiffusionFF是一个基于扩散模型的框架，用于提升人脸伪造检测和伪造伪影定位的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造技术的快速发展，需要更鲁棒和准确的检测算法，同时定位伪造伪影以提高模型可解释性和用户信任。

Method: 利用去噪扩散模型生成高质量的DSSIM图，捕捉细微的伪造痕迹，并与预训练的伪造检测器提取的高级语义特征融合。

Result: 在跨数据集和数据集内基准测试中，DiffusionFF表现出卓越的检测性能和精细的伪影定位能力。

Conclusion: DiffusionFF在检测和定位伪造伪影方面具有显著效果，提升了整体性能。

Abstract: The rapid evolution of deepfake generation techniques demands robust and accurate face forgery detection algorithms. While determining whether an image has been manipulated remains essential, the ability to precisely localize forgery artifacts has become increasingly important for improving model explainability and fostering user trust. To address this challenge, we propose DiffusionFF, a novel framework that enhances face forgery detection through diffusion-based artifact localization. Our method utilizes a denoising diffusion model to generate high-quality Structural Dissimilarity (DSSIM) maps, which effectively capture subtle traces of manipulation. These DSSIM maps are then fused with high-level semantic features extracted by a pretrained forgery detector, leading to significant improvements in detection accuracy. Extensive experiments on both cross-dataset and intra-dataset benchmarks demonstrate that DiffusionFF not only achieves superior detection performance but also offers precise and fine-grained artifact localization, highlighting its overall effectiveness.

</details>


### [30] [Devil is in the Detail: Towards Injecting Fine Details of Image Prompt in Image Generation via Conflict-free Guidance and Stratified Attention](https://arxiv.org/abs/2508.02004)
*Kyungmin Jo,Jooyeol Yun,Jaegul Choo*

Main category: cs.CV

TL;DR: 本文提出了一种新的自注意力修改方法（Stratified Attention），解决了现有方法在图像提示生成中的冲突问题，并平衡了生成图像的逼真度与图像提示的对齐。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型难以捕捉复杂细节（如纹理），导致用户意图无法完全体现。图像提示方法虽能改善，但存在分类器自由引导中的冲突信号问题，且现有自注意力修改方法在逼真度与对齐性之间存在权衡。

Method: 提出冲突自由引导（仅将图像提示作为期望条件）和分层注意力（联合使用生成图像和图像提示的键值），以平衡逼真度与对齐性。

Result: 在三个图像生成任务中，新方法优于现有图像提示模型，能更准确地反映图像提示。

Conclusion: 通过冲突自由引导和分层注意力，新方法有效解决了图像提示生成中的问题，提升了生成图像的质量与对齐性。

Abstract: While large-scale text-to-image diffusion models enable the generation of high-quality, diverse images from text prompts, these prompts struggle to capture intricate details, such as textures, preventing the user intent from being reflected. This limitation has led to efforts to generate images conditioned on user-provided images, referred to as image prompts. Recent work modifies the self-attention mechanism to impose image conditions in generated images by replacing or concatenating the keys and values from the image prompt. This enables the self-attention layer to work like a cross-attention layer, generally used to incorporate text prompts. In this paper, we identify two common issues in existing methods of modifying self-attention to generate images that reflect the details of image prompts. First, existing approaches neglect the importance of image prompts in classifier-free guidance. Specifically, current methods use image prompts as both desired and undesired conditions in classifier-free guidance, causing conflicting signals. To resolve this, we propose conflict-free guidance by using image prompts only as desired conditions, ensuring that the generated image faithfully reflects the image prompt. In addition, we observe that the two most common self-attention modifications involve a trade-off between the realism of the generated image and alignment with the image prompt. Specifically, selecting more keys and values from the image prompt improves alignment, while selecting more from the generated image enhances realism. To balance both, we propose an new self-attention modification method, Stratified Attention to jointly use keys and values from both images rather than selecting between them. Through extensive experiments across three image generation tasks, we show that the proposed method outperforms existing image-prompting models in faithfully reflecting the image prompt.

</details>


### [31] [Conditional Diffusion Model with Anatomical-Dose Dual Constraints for End-to-End Multi-Tumor Dose Prediction](https://arxiv.org/abs/2508.02043)
*Hui Xie,Haiqin Hu,Lijuan Ding,Qing Li,Yue Sun,Tao Tan*

Main category: cs.CV

TL;DR: ADDiff-Dose是一种基于条件扩散模型的放疗剂量预测方法，通过多模态输入和复合损失函数显著提升了预测精度和临床适用性。


<details>
  <summary>Details</summary>
Motivation: 解决传统放疗计划依赖专家经验耗时且现有深度学习方法泛化性和准确性不足的问题。

Method: 采用LightweightVAE3D压缩CT数据，结合多模态输入和条件扩散模型，通过多注意力机制和复合损失函数优化预测。

Result: 在多个数据集上表现优异，MAE显著降低，DICE系数提升6.8%，计划生成时间缩短至22秒。

Conclusion: ADDiff-Dose为放疗剂量预测提供了高效、通用的解决方案，有望显著提升临床工作效率。

Abstract: Radiotherapy treatment planning often relies on time-consuming, trial-and-error adjustments that heavily depend on the expertise of specialists, while existing deep learning methods face limitations in generalization, prediction accuracy, and clinical applicability. To tackle these challenges, we propose ADDiff-Dose, an Anatomical-Dose Dual Constraints Conditional Diffusion Model for end-to-end multi-tumor dose prediction. The model employs LightweightVAE3D to compress high-dimensional CT data and integrates multimodal inputs, including target and organ-at-risk (OAR) masks and beam parameters, within a progressive noise addition and denoising framework. It incorporates conditional features via a multi-head attention mechanism and utilizes a composite loss function combining MSE, conditional terms, and KL divergence to ensure both dosimetric accuracy and compliance with clinical constraints. Evaluation on a large-scale public dataset (2,877 cases) and three external institutional cohorts (450 cases in total) demonstrates that ADDiff-Dose significantly outperforms traditional baselines, achieving an MAE of 0.101-0.154 (compared to 0.316 for UNet and 0.169 for GAN models), a DICE coefficient of 0.927 (a 6.8% improvement), and limiting spinal cord maximum dose error to within 0.1 Gy. The average plan generation time per case is reduced to 22 seconds. Ablation studies confirm that the structural encoder enhances compliance with clinical dose constraints by 28.5%. To our knowledge, this is the first study to introduce a conditional diffusion model framework for radiotherapy dose prediction, offering a generalizable and efficient solution for automated treatment planning across diverse tumor sites, with the potential to substantially reduce planning time and improve clinical workflow efficiency.

</details>


### [32] [StarPose: 3D Human Pose Estimation via Spatial-Temporal Autoregressive Diffusion](https://arxiv.org/abs/2508.02056)
*Haoxin Yang,Weihong Chen,Xuemiao Xu,Cheng Xu,Peng Xiao,Cuifeng Sun,Shaoyu Huang,Shengfeng He*

Main category: cs.CV

TL;DR: StarPose提出了一种自回归扩散框架，通过结合历史3D姿态预测和时空物理指导，显著提升了单目3D人体姿态估计的准确性和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散方法在时空相关性上的不足，提升3D姿态估计的准确性和时间一致性。

Method: 采用自回归扩散过程，结合历史姿态集成模块（HPIM）和时空物理指导（STPG）机制。

Result: 在基准数据集上表现优于现有方法，实现了更高的准确性和时间一致性。

Conclusion: StarPose通过自回归扩散框架和时空物理指导，显著提升了3D人体姿态估计的性能。

Abstract: Monocular 3D human pose estimation remains a challenging task due to inherent depth ambiguities and occlusions. Compared to traditional methods based on Transformers or Convolutional Neural Networks (CNNs), recent diffusion-based approaches have shown superior performance, leveraging their probabilistic nature and high-fidelity generation capabilities. However, these methods often fail to account for the spatial and temporal correlations across predicted frames, resulting in limited temporal consistency and inferior accuracy in predicted 3D pose sequences. To address these shortcomings, this paper proposes StarPose, an autoregressive diffusion framework that effectively incorporates historical 3D pose predictions and spatial-temporal physical guidance to significantly enhance both the accuracy and temporal coherence of pose predictions. Unlike existing approaches, StarPose models the 2D-to-3D pose mapping as an autoregressive diffusion process. By synergically integrating previously predicted 3D poses with 2D pose inputs via a Historical Pose Integration Module (HPIM), the framework generates rich and informative historical pose embeddings that guide subsequent denoising steps, ensuring temporally consistent predictions. In addition, a fully plug-and-play Spatial-Temporal Physical Guidance (STPG) mechanism is tailored to refine the denoising process in an iterative manner, which further enforces spatial anatomical plausibility and temporal motion dynamics, rendering robust and realistic pose estimates. Extensive experiments on benchmark datasets demonstrate that StarPose outperforms state-of-the-art methods, achieving superior accuracy and temporal consistency in 3D human pose estimation. Code is available at https://github.com/wileychan/StarPose.

</details>


### [33] [AutoLoRA: Automatic LoRA Retrieval and Fine-Grained Gated Fusion for Text-to-Image Generation](https://arxiv.org/abs/2508.02107)
*Zhiwen Li,Zhongjie Duan,Die Chen,Cen Chen,Daoyuan Chen,Yaliang Li,Yingda Chen*

Main category: cs.CV

TL;DR: 论文提出了一种新框架，通过语义驱动的LoRA检索和动态聚合，解决了分布式开源LoRA模块的三大挑战：稀疏元数据标注、零样本适应需求和多LoRA融合策略不佳。


<details>
  <summary>Details</summary>
Motivation: 尽管现有大规模模型在图像生成方面取得了进展，但其参数微调的不可行性限制了实际部署。LoRA虽能低成本定制模型，但面临元数据稀疏、零样本适应和多LoRA融合等问题。

Method: 框架包含两部分：(1)基于权重编码的LoRA检索器，建立LoRA参数矩阵与文本提示的共享语义空间；(2)细粒度门控融合机制，动态计算融合权重。

Result: 该方法显著提升了图像生成性能，支持基础模型的高效扩展。

Conclusion: 该研究填补了社区开发的LoRA与实际部署需求之间的鸿沟，通过标准化适配器集成推动协作模型演进。

Abstract: Despite recent advances in photorealistic image generation through large-scale models like FLUX and Stable Diffusion v3, the practical deployment of these architectures remains constrained by their inherent intractability to parameter fine-tuning. While low-rank adaptation (LoRA) have demonstrated efficacy in enabling model customization with minimal parameter overhead, the effective utilization of distributed open-source LoRA modules faces three critical challenges: sparse metadata annotation, the requirement for zero-shot adaptation capabilities, and suboptimal fusion strategies for multi-LoRA fusion strategies. To address these limitations, we introduce a novel framework that enables semantic-driven LoRA retrieval and dynamic aggregation through two key components: (1) weight encoding-base LoRA retriever that establishes a shared semantic space between LoRA parameter matrices and text prompts, eliminating dependence on original training data, and (2) fine-grained gated fusion mechanism that computes context-specific fusion weights across network layers and diffusion timesteps to optimally integrate multiple LoRA modules during generation. Our approach achieves significant improvement in image generation perfermance, thereby facilitating scalable and data-efficient enhancement of foundational models. This work establishes a critical bridge between the fragmented landscape of community-developed LoRAs and practical deployment requirements, enabling collaborative model evolution through standardized adapter integration.

</details>


### [34] [VDEGaussian: Video Diffusion Enhanced 4D Gaussian Splatting for Dynamic Urban Scenes Modeling](https://arxiv.org/abs/2508.02129)
*Yuru Xiao,Zihan Lin,Chao Lu,Deming Zhai,Kui Jiang,Wenbo Zhao,Wei Zhang,Junjun Jiang,Huanran Wang,Xianming Liu*

Main category: cs.CV

TL;DR: 提出了一种基于视频扩散增强的4D高斯泼溅框架，用于动态场景建模，解决了现有方法在快速运动物体建模和时序连续性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预校准物体轨迹或难以处理欠采样捕获的快速运动物体，导致建模不准确。

Method: 结合视频扩散模型提取时序一致先验，引入联合时间戳优化和不确定性蒸馏方法，提升动态建模精度。

Result: 实验表明，该方法显著提升了动态建模效果，尤其是快速运动物体，新视角合成的PSNR增益约2 dB。

Conclusion: 提出的框架有效解决了动态场景建模中的时序一致性和快速运动物体建模问题。

Abstract: Dynamic urban scene modeling is a rapidly evolving area with broad applications. While current approaches leveraging neural radiance fields or Gaussian Splatting have achieved fine-grained reconstruction and high-fidelity novel view synthesis, they still face significant limitations. These often stem from a dependence on pre-calibrated object tracks or difficulties in accurately modeling fast-moving objects from undersampled capture, particularly due to challenges in handling temporal discontinuities. To overcome these issues, we propose a novel video diffusion-enhanced 4D Gaussian Splatting framework. Our key insight is to distill robust, temporally consistent priors from a test-time adapted video diffusion model. To ensure precise pose alignment and effective integration of this denoised content, we introduce two core innovations: a joint timestamp optimization strategy that refines interpolated frame poses, and an uncertainty distillation method that adaptively extracts target content while preserving well-reconstructed regions. Extensive experiments demonstrate that our method significantly enhances dynamic modeling, especially for fast-moving objects, achieving an approximate PSNR gain of 2 dB for novel view synthesis over baseline approaches.

</details>


### [35] [DreamPainter: Image Background Inpainting for E-commerce Scenarios](https://arxiv.org/abs/2508.02155)
*Sijie Zhao,Jing Cheng,Yaoyao Wu,Hao Xu,Shaohui Jiao*

Main category: cs.CV

TL;DR: 论文提出了DreamEcom-400K数据集和DreamPainter框架，解决了电商场景中背景生成任务的两大挑战：产品一致性和视觉信息整合。


<details>
  <summary>Details</summary>
Motivation: 电商背景生成任务面临产品一致性和视觉信息整合的挑战，现有方法因缺乏领域数据和依赖单一文本提示而效果不佳。

Method: 基于高质量数据集DreamEcom-400K，提出DreamPainter框架，结合文本提示和参考图像信息进行控制。

Result: 实验表明，该方法在保持产品一致性和整合视觉信息方面显著优于现有技术。

Conclusion: DreamPainter通过多模态控制信号，有效解决了电商背景生成的核心问题。

Abstract: Although diffusion-based image genenation has been widely explored and applied, background generation tasks in e-commerce scenarios still face significant challenges. The first challenge is to ensure that the generated products are consistent with the given product inputs while maintaining a reasonable spatial arrangement, harmonious shadows, and reflections between foreground products and backgrounds. Existing inpainting methods fail to address this due to the lack of domain-specific data. The second challenge involves the limitation of relying solely on text prompts for image control, as effective integrating visual information to achieve precise control in inpainting tasks remains underexplored. To address these challenges, we introduce DreamEcom-400K, a high-quality e-commerce dataset containing accurate product instance masks, background reference images, text prompts, and aesthetically pleasing product images. Based on this dataset, we propose DreamPainter, a novel framework that not only utilizes text prompts for control but also flexibly incorporates reference image information as an additional control signal. Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art methods, maintaining high product consistency while effectively integrating both text prompt and reference image information.

</details>


### [36] [Subject or Style: Adaptive and Training-Free Mixture of LoRAs](https://arxiv.org/abs/2508.02165)
*Jia-Chen Zhang,Yu-Jie Xiong*

Main category: cs.CV

TL;DR: EST-LoRA提出了一种无需训练的LoRA融合方法，通过综合考虑矩阵能量、风格差异分数和时间步长，自适应选择主题和风格LoRA，平衡生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA融合方法难以平衡主题和风格，且需要额外训练或涉及复杂超参数。

Method: EST-LoRA采用类似MoE的架构，在注意力层自适应选择主题或风格LoRA，结合矩阵能量、风格差异和时间步长。

Result: 实验表明，EST-LoRA在质量和速度上均优于现有方法。

Conclusion: EST-LoRA是一种高效且无需训练的LoRA融合方法，显著提升了生成效果。

Abstract: Fine-tuning models via Low-Rank Adaptation (LoRA) demonstrates remarkable performance in subject-driven or style-driven generation tasks. Studies have explored combinations of different LoRAs to jointly generate learned styles and content. However, current methods struggle to balance the original subject and style, and often require additional training. Recently, K-LoRA proposed a training-free LoRA fusion method. But it involves multiple hyperparameters, making it difficult to adapt to all styles and subjects. In this paper, we propose EST-LoRA, a training-free adaptive LoRA fusion method. It comprehensively considers three critical factors: \underline{E}nergy of matrix, \underline{S}tyle discrepancy scores and \underline{T}ime steps. Analogous to the Mixture of Experts (MoE) architecture, the model adaptively selects between subject LoRA and style LoRA within each attention layer. This integrated selection mechanism ensures balanced contributions from both components during the generation process. Experimental results show that EST-LoRA outperforms state-of-the-art methods in both qualitative and quantitative evaluations and achieves faster generation speed compared to other efficient fusion approaches. Our code is publicly available at: https://anonymous.4open.science/r/EST-LoRA-F318.

</details>


### [37] [GaussianCross: Cross-modal Self-supervised 3D Representation Learning via Gaussian Splatting](https://arxiv.org/abs/2508.02172)
*Lei Yao,Yi Wang,Yi Zhang,Moyun Liu,Lap-Pui Chau*

Main category: cs.CV

TL;DR: GaussianCross是一种新颖的跨模态自监督3D表示学习架构，通过3D高斯喷洒技术解决现有方法中的模型崩溃和结构信息不足问题，显著提升了性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有自监督预训练方法在3D场景理解中存在模型崩溃和结构信息不足的问题，导致表达不可靠和性能不佳。

Method: GaussianCross通过将点云转换为统一的立方体归一化高斯表示，并结合三属性自适应蒸馏喷洒模块，构建3D特征场，实现跨模态一致性。

Result: 在多个基准测试中表现优异，参数和数据效率高，泛化能力强，显著提升了语义和实例分割任务的性能。

Conclusion: GaussianCross通过创新的跨模态学习架构，有效解决了现有问题，并在性能和泛化能力上取得了显著提升。

Abstract: The significance of informative and robust point representations has been widely acknowledged for 3D scene understanding. Despite existing self-supervised pre-training counterparts demonstrating promising performance, the model collapse and structural information deficiency remain prevalent due to insufficient point discrimination difficulty, yielding unreliable expressions and suboptimal performance. In this paper, we present GaussianCross, a novel cross-modal self-supervised 3D representation learning architecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniques to address current challenges. GaussianCross seamlessly converts scale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussian representation without missing details, enabling stable and generalizable pre-training. Subsequently, a tri-attribute adaptive distillation splatting module is incorporated to construct a 3D feature field, facilitating synergetic feature capturing of appearance, geometry, and semantic cues to maintain cross-modal consistency. To validate GaussianCross, we perform extensive evaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. In particular, GaussianCross shows a prominent parameter and data efficiency, achieving superior performance through linear probing (<0.1% parameters) and limited data training (1% of scenes) compared to state-of-the-art methods. Furthermore, GaussianCross demonstrates strong generalization capabilities, improving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ on ScanNet200 semantic and instance segmentation tasks, respectively, supporting the effectiveness of our approach. The code, weights, and visualizations are publicly available at \href{https://rayyoh.github.io/GaussianCross/}{https://rayyoh.github.io/GaussianCross/}.

</details>


### [38] [Patho-AgenticRAG: Towards Multimodal Agentic Retrieval-Augmented Generation for Pathology VLMs via Reinforcement Learning](https://arxiv.org/abs/2508.02258)
*Wenchuan Zhang,Jingru Guo,Hengzhe Zhang,Penghao Zhang,Jie Chen,Shuwan Zhang,Zhang Zhang,Yuhao Yi,Hong Bu*

Main category: cs.CV

TL;DR: Patho-AgenticRAG是一种多模态RAG框架，通过结合文本和图像检索解决病理学中视觉语言模型的幻觉问题，显著提升复杂诊断任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 病理学中的视觉语言模型因超高分辨率、复杂组织结构和临床语义的细微差异容易产生幻觉，影响临床信任。现有RAG方法主要依赖文本知识库，无法充分利用视觉信息。

Method: 提出Patho-AgenticRAG，基于权威病理学教科书构建页面级嵌入数据库，支持联合文本-图像检索，避免关键图像信息丢失，并支持推理、任务分解和多轮搜索交互。

Result: 实验表明，Patho-AgenticRAG在多选诊断和视觉问答等复杂病理任务中显著优于现有多模态模型。

Conclusion: Patho-AgenticRAG通过多模态检索和推理能力，有效解决了病理学中视觉语言模型的幻觉问题，提升了诊断准确性。

Abstract: Although Vision Language Models (VLMs) have shown strong generalization in medical imaging, pathology presents unique challenges due to ultra-high resolution, complex tissue structures, and nuanced clinical semantics. These factors make pathology VLMs prone to hallucinations, i.e., generating outputs inconsistent with visual evidence, which undermines clinical trust. Existing RAG approaches in this domain largely depend on text-based knowledge bases, limiting their ability to leverage diagnostic visual cues. To address this, we propose Patho-AgenticRAG, a multimodal RAG framework with a database built on page-level embeddings from authoritative pathology textbooks. Unlike traditional text-only retrieval systems, it supports joint text-image search, enabling direct retrieval of textbook pages that contain both the queried text and relevant visual cues, thus avoiding the loss of critical image-based information. Patho-AgenticRAG also supports reasoning, task decomposition, and multi-turn search interactions, improving accuracy in complex diagnostic scenarios. Experiments show that Patho-AgenticRAG significantly outperforms existing multimodal models in complex pathology tasks like multiple-choice diagnosis and visual question answering. Our project is available at the Patho-AgenticRAG repository: https://github.com/Wenchuan-Zhang/Patho-AgenticRAG.

</details>


### [39] [SplatSSC: Decoupled Depth-Guided Gaussian Splatting for Semantic Scene Completion](https://arxiv.org/abs/2508.02261)
*Rui Qian,Haozhi Cao,Tianchen Deng,Shenghai Yuan,Lihua Xie*

Main category: cs.CV

TL;DR: SplatSSC提出了一种基于深度引导初始化和解耦高斯聚合的单目3D语义场景补全框架，显著提升了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯基元的方法依赖随机初始化，导致效率低下和异常基元问题。

Method: 采用深度引导的初始化策略（GMF模块）和解耦高斯聚合器（DGA），结合概率尺度损失。

Result: 在Occ-ScanNet数据集上，IoU和mIoU分别提升6.3%和4.1%，延迟和内存消耗降低9.3%。

Conclusion: SplatSSC通过优化初始化和聚合策略，实现了高效且鲁棒的3D语义场景补全。

Abstract: Monocular 3D Semantic Scene Completion (SSC) is a challenging yet promising task that aims to infer dense geometric and semantic descriptions of a scene from a single image. While recent object-centric paradigms significantly improve efficiency by leveraging flexible 3D Gaussian primitives, they still rely heavily on a large number of randomly initialized primitives, which inevitably leads to 1) inefficient primitive initialization and 2) outlier primitives that introduce erroneous artifacts. In this paper, we propose SplatSSC, a novel framework that resolves these limitations with a depth-guided initialization strategy and a principled Gaussian aggregator. Instead of random initialization, SplatSSC utilizes a dedicated depth branch composed of a Group-wise Multi-scale Fusion (GMF) module, which integrates multi-scale image and depth features to generate a sparse yet representative set of initial Gaussian primitives. To mitigate noise from outlier primitives, we develop the Decoupled Gaussian Aggregator (DGA), which enhances robustness by decomposing geometric and semantic predictions during the Gaussian-to-voxel splatting process. Complemented with a specialized Probability Scale Loss, our method achieves state-of-the-art performance on the Occ-ScanNet dataset, outperforming prior approaches by over 6.3% in IoU and 4.1% in mIoU, while reducing both latency and memory consumption by more than 9.3%. The code will be released upon acceptance.

</details>


### [40] [Qwen-Image Technical Report](https://arxiv.org/abs/2508.02324)
*Chenfei Wu,Jiahao Li,Jingren Zhou,Junyang Lin,Kaiyuan Gao,Kun Yan,Sheng-ming Yin,Shuai Bai,Xiao Xu,Yilei Chen,Yuxiang Chen,Zecheng Tang,Zekai Zhang,Zhengyi Wang,An Yang,Bowen Yu,Chen Cheng,Dayiheng Liu,Deqing Li,Hang Zhang,Hao Meng,Hu Wei,Jingyuan Ni,Kai Chen,Kuan Cao,Liang Peng,Lin Qu,Minggang Wu,Peng Wang,Shuting Yu,Tingkun Wen,Wensen Feng,Xiaoxiao Xu,Yi Wang,Yichang Zhang,Yongqiang Zhu,Yujia Wu,Yuxuan Cai,Zenan Liu*

Main category: cs.CV

TL;DR: Qwen-Image是一个图像生成基础模型，通过复杂文本渲染和精确图像编辑的改进，实现了显著进展。


<details>
  <summary>Details</summary>
Motivation: 解决复杂文本渲染和图像编辑一致性的挑战。

Method: 采用全面的数据管道和渐进式训练策略，结合多任务训练范式及双编码机制。

Result: 在字母语言（如英语）和表意文字（如中文）中表现优异，图像生成和编辑能力达到最先进水平。

Conclusion: Qwen-Image在图像生成和编辑方面表现出强大的能力，适用于多种语言和任务。

Abstract: We present Qwen-Image, an image generation foundation model in the Qwen series that achieves significant advances in complex text rendering and precise image editing. To address the challenges of complex text rendering, we design a comprehensive data pipeline that includes large-scale data collection, filtering, annotation, synthesis, and balancing. Moreover, we adopt a progressive training strategy that starts with non-text-to-text rendering, evolves from simple to complex textual inputs, and gradually scales up to paragraph-level descriptions. This curriculum learning approach substantially enhances the model's native text rendering capabilities. As a result, Qwen-Image not only performs exceptionally well in alphabetic languages such as English, but also achieves remarkable progress on more challenging logographic languages like Chinese. To enhance image editing consistency, we introduce an improved multi-task training paradigm that incorporates not only traditional text-to-image (T2I) and text-image-to-image (TI2I) tasks but also image-to-image (I2I) reconstruction, effectively aligning the latent representations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed the original image into Qwen2.5-VL and the VAE encoder to obtain semantic and reconstructive representations, respectively. This dual-encoding mechanism enables the editing module to strike a balance between preserving semantic consistency and maintaining visual fidelity. Qwen-Image achieves state-of-the-art performance, demonstrating its strong capabilities in both image generation and editing across multiple benchmarks.

</details>


### [41] [Low-Frequency First: Eliminating Floating Artifacts in 3D Gaussian Splatting](https://arxiv.org/abs/2508.02493)
*Jianchao Wang,Peng Zhou,Cen Li,Rong Quan,Jie Qin*

Main category: cs.CV

TL;DR: 论文提出EFA-GS方法，通过频率域分析解决3D高斯泼溅中的浮动伪影问题，显著提升视觉质量和PSNR。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅（3DGS）在低质量初始化时会产生浮动伪影，影响视觉保真度，但其成因尚未充分研究。

Method: 从频率域角度分析伪影成因，提出EFA-GS方法，选择性扩展未优化高斯以优先学习低频信息，并引入深度和尺度策略动态优化。

Result: 在合成和真实数据集上，EFA-GS显著减少伪影并保留高频细节，PSNR提升1.68 dB。

Conclusion: EFA-GS有效解决浮动伪影问题，适用于下游3D编辑任务，代码将开源。

Abstract: 3D Gaussian Splatting (3DGS) is a powerful and computationally efficient representation for 3D reconstruction. Despite its strengths, 3DGS often produces floating artifacts, which are erroneous structures detached from the actual geometry and significantly degrade visual fidelity. The underlying mechanisms causing these artifacts, particularly in low-quality initialization scenarios, have not been fully explored. In this paper, we investigate the origins of floating artifacts from a frequency-domain perspective and identify under-optimized Gaussians as the primary source. Based on our analysis, we propose \textit{Eliminating-Floating-Artifacts} Gaussian Splatting (EFA-GS), which selectively expands under-optimized Gaussians to prioritize accurate low-frequency learning. Additionally, we introduce complementary depth-based and scale-based strategies to dynamically refine Gaussian expansion, effectively mitigating detail erosion. Extensive experiments on both synthetic and real-world datasets demonstrate that EFA-GS substantially reduces floating artifacts while preserving high-frequency details, achieving an improvement of 1.68 dB in PSNR over baseline method on our RWLQ dataset. Furthermore, we validate the effectiveness of our approach in downstream 3D editing tasks. Our implementation will be released on GitHub.

</details>


### [42] [ReMoMask: Retrieval-Augmented Masked Motion Generation](https://arxiv.org/abs/2508.02605)
*Zhengdao Li,Siheng Wang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: ReMoMask是一个统一的文本到运动生成框架，通过双向动量文本-运动模型、语义时空注意力和RAG-无分类器引导三项创新，解决了现有方法的多样性不足、物理不合理性和异步伪影等问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前文本到运动生成方法存在多样性不足、物理不合理性和异步伪影等问题，ReMoMask旨在解决这些局限性。

Method: 1) 双向动量文本-运动模型；2) 语义时空注意力机制；3) RAG-无分类器引导。

Result: 在HumanML3D和KIT-ML基准测试中，FID分数分别提升3.88%和10.97%。

Conclusion: ReMoMask在文本到运动生成任务中表现出色，显著优于现有方法。

Abstract: Text-to-Motion (T2M) generation aims to synthesize realistic and semantically aligned human motion sequences from natural language descriptions. However, current approaches face dual challenges: Generative models (e.g., diffusion models) suffer from limited diversity, error accumulation, and physical implausibility, while Retrieval-Augmented Generation (RAG) methods exhibit diffusion inertia, partial-mode collapse, and asynchronous artifacts. To address these limitations, we propose ReMoMask, a unified framework integrating three key innovations: 1) A Bidirectional Momentum Text-Motion Model decouples negative sample scale from batch size via momentum queues, substantially improving cross-modal retrieval precision; 2) A Semantic Spatio-temporal Attention mechanism enforces biomechanical constraints during part-level fusion to eliminate asynchronous artifacts; 3) RAG-Classier-Free Guidance incorporates minor unconditional generation to enhance generalization. Built upon MoMask's RVQ-VAE, ReMoMask efficiently generates temporally coherent motions in minimal steps. Extensive experiments on standard benchmarks demonstrate the state-of-the-art performance of ReMoMask, achieving a 3.88% and 10.97% improvement in FID scores on HumanML3D and KIT-ML, respectively, compared to the previous SOTA method RAG-T2M. Code: https://github.com/AIGeeksGroup/ReMoMask. Website: https://aigeeksgroup.github.io/ReMoMask.

</details>


### [43] [PMGS: Reconstruction of Projectile Motion across Large Spatiotemporal Spans via 3D Gaussian Splatting](https://arxiv.org/abs/2508.02660)
*Yijun Xu,Jingrui Zhang,Yuhan Chen,Dingwen Wang,Lei Yu,Chu He*

Main category: cs.CV

TL;DR: PMGS通过3D高斯溅射重建抛体运动，提出两阶段工作流程（目标建模和运动恢复），结合物理一致性约束和动态模拟退火策略，显著提升了高速非线性刚体运动的重建性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有动态重建方法在长时间、大空间尺度下刚性运动建模的局限性，特别是物理一致性的不足。

Method: 1) 目标建模：通过动态场景分解和改进的点密度控制实现对象集中重建；2) 运动恢复：通过每帧SE(3)位姿学习恢复完整运动序列，引入加速度一致性约束和动态模拟退火策略。

Result: PMGS在高速非线性刚体运动重建中表现优于主流动态方法。

Conclusion: PMGS通过结合物理约束和自适应学习策略，有效解决了复杂刚性运动建模的挑战。

Abstract: Modeling complex rigid motion across large spatiotemporal spans remains an unresolved challenge in dynamic reconstruction. Existing paradigms are mainly confined to short-term, small-scale deformation and offer limited consideration for physical consistency. This study proposes PMGS, focusing on reconstructing Projectile Motion via 3D Gaussian Splatting. The workflow comprises two stages: 1) Target Modeling: achieving object-centralized reconstruction through dynamic scene decomposition and an improved point density control; 2) Motion Recovery: restoring full motion sequences by learning per-frame SE(3) poses. We introduce an acceleration consistency constraint to bridge Newtonian mechanics and pose estimation, and design a dynamic simulated annealing strategy that adaptively schedules learning rates based on motion states. Futhermore, we devise a Kalman fusion scheme to optimize error accumulation from multi-source observations to mitigate disturbances. Experiments show PMGS's superior performance in reconstructing high-speed nonlinear rigid motion compared to mainstream dynamic methods.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [44] [Point-wise Diffusion Models for Physical Systems with Shape Variations: Application to Spatio-temporal and Large-scale system](https://arxiv.org/abs/2508.01230)
*Jiyong Kim,Sunwoong Yang,Namwoo Kang*

Main category: physics.comp-ph

TL;DR: 提出了一种新颖的点扩散模型，通过独立处理时空点来高效预测复杂物理系统，具有形状变化。该方法在保持几何保真度的同时，支持直接处理网格和点云等数据格式，显著提升了计算效率和预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统基于图像的扩散模型在处理结构化数据时效率较低，且难以直接处理非结构化数据（如网格和点云）。本研究旨在开发一种更高效、灵活的方法，以支持复杂物理系统的实时预测。

Method: 采用点扩散模型，结合前向和后向扩散过程，并使用点扩散变换器架构进行去噪。通过DDIM实现高效确定性采样，仅需5-10步，显著减少计算时间。

Result: 在三个物理系统中验证了方法的有效性，计算速度提升100-200倍，训练时间减少94.4%，参数减少89.0%，预测精度提升28%以上。

Conclusion: 点扩散模型在复杂物理系统预测中表现出显著优势，为实时应用提供了高效且灵活的解决方案。

Abstract: This study introduces a novel point-wise diffusion model that processes spatio-temporal points independently to efficiently predict complex physical systems with shape variations. This methodological contribution lies in applying forward and backward diffusion processes at individual spatio-temporal points, coupled with a point-wise diffusion transformer architecture for denoising. Unlike conventional image-based diffusion models that operate on structured data representations, this framework enables direct processing of any data formats including meshes and point clouds while preserving geometric fidelity. We validate our approach across three distinct physical domains with complex geometric configurations: 2D spatio-temporal systems including cylinder fluid flow and OLED drop impact test, and 3D large-scale system for road-car external aerodynamics. To justify the necessity of our point-wise approach for real-time prediction applications, we employ denoising diffusion implicit models (DDIM) for efficient deterministic sampling, requiring only 5-10 steps compared to traditional 1000-step and providing computational speedup of 100 to 200 times during inference without compromising accuracy. In addition, our proposed model achieves superior performance compared to image-based diffusion model: reducing training time by 94.4% and requiring 89.0% fewer parameters while achieving over 28% improvement in prediction accuracy. Comprehensive comparisons against data-flexible surrogate models including DeepONet and Meshgraphnet demonstrate consistent superiority of our approach across all three physical systems. To further refine the proposed model, we investigate two key aspects: 1) comparison of final physical states prediction or incremental change prediction, and 2) computational efficiency evaluation across varying subsampling ratios (10%-100%).

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [45] [ReCoSeg++:Extended Residual-Guided Cross-Modal Diffusion for Brain Tumor Segmentation](https://arxiv.org/abs/2508.01058)
*Sara Yavari,Rahul Nitin Pandya,Jacob Furst*

Main category: eess.IV

TL;DR: 提出了一种半监督的两阶段框架，用于MRI中脑肿瘤的精确分割，无需真实掩码，通过残差引导的扩散模型和轻量级U-Net实现，性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤的精确分割对临床诊断和治疗计划至关重要，但现有方法在更大、更异质的数据集上表现不足。

Method: 第一阶段使用残差引导的DDPM进行跨模态合成，第二阶段用轻量级U-Net结合残差图和多模态输入进行分割。

Result: 在BraTS 2021数据集上，Dice得分为93.02%，IoU为86.7%，优于基线。

Conclusion: 该方法在真实世界多中心MRI数据集中表现出更高的准确性和可扩展性。

Abstract: Accurate segmentation of brain tumors in MRI scans is critical for clinical diagnosis and treatment planning. We propose a semi-supervised, two-stage framework that extends the ReCoSeg approach to the larger and more heterogeneous BraTS 2021 dataset, while eliminating the need for ground-truth masks for the segmentation objective. In the first stage, a residual-guided denoising diffusion probabilistic model (DDPM) performs cross-modal synthesis by reconstructing the T1ce modality from FLAIR, T1, and T2 scans. The residual maps, capturing differences between predicted and actual T1ce images, serve as spatial priors to enhance downstream segmentation. In the second stage, a lightweight U-Net takes as input the concatenation of residual maps, computed as the difference between real T1ce and synthesized T1ce, with T1, T2, and FLAIR modalities to improve whole tumor segmentation. To address the increased scale and variability of BraTS 2021, we apply slice-level filtering to exclude non-informative samples and optimize thresholding strategies to balance precision and recall. Our method achieves a Dice score of $93.02\%$ and an IoU of $86.7\%$ for whole tumor segmentation on the BraTS 2021 dataset, outperforming the ReCoSeg baseline on BraTS 2020 (Dice: $91.7\%$, IoU: $85.3\%$), and demonstrating improved accuracy and scalability for real-world, multi-center MRI datasets.

</details>


### [46] [CoCoLIT: ControlNet-Conditioned Latent Image Translation for MRI to Amyloid PET Synthesis](https://arxiv.org/abs/2508.01292)
*Alec Sargood,Lemuel Puglisi,James H. Cole,Neil P. Oxtoby,Daniele Ravì,Daniel C. Alexander*

Main category: eess.IV

TL;DR: CoCoLIT是一种基于扩散的潜在生成框架，通过加权图像空间损失和ControlNet条件，显著提升了MRI到PET的翻译性能。


<details>
  <summary>Details</summary>
Motivation: 通过合成淀粉样蛋白PET扫描来替代昂贵的MRI检测，为大规模阿尔茨海默病筛查提供经济高效的解决方案。

Method: 提出CoCoLIT框架，包含加权图像空间损失（WISL）、潜在平均稳定化（LAS）分析和ControlNet条件。

Result: 在公开数据集上，CoCoLIT在图像和淀粉样蛋白相关指标上显著优于现有方法，分类性能提升显著。

Conclusion: CoCoLIT为MRI到PET翻译提供了高效解决方案，代码和模型已开源。

Abstract: Synthesizing amyloid PET scans from the more widely available and accessible structural MRI modality offers a promising, cost-effective approach for large-scale Alzheimer's Disease (AD) screening. This is motivated by evidence that, while MRI does not directly detect amyloid pathology, it may nonetheless encode information correlated with amyloid deposition that can be uncovered through advanced modeling. However, the high dimensionality and structural complexity of 3D neuroimaging data pose significant challenges for existing MRI-to-PET translation methods. Modeling the cross-modality relationship in a lower-dimensional latent space can simplify the learning task and enable more effective translation. As such, we present CoCoLIT (ControlNet-Conditioned Latent Image Translation), a diffusion-based latent generative framework that incorporates three main innovations: (1) a novel Weighted Image Space Loss (WISL) that improves latent representation learning and synthesis quality; (2) a theoretical and empirical analysis of Latent Average Stabilization (LAS), an existing technique used in similar generative models to enhance inference consistency; and (3) the introduction of ControlNet-based conditioning for MRI-to-PET translation. We evaluate CoCoLIT's performance on publicly available datasets and find that our model significantly outperforms state-of-the-art methods on both image-based and amyloid-related metrics. Notably, in amyloid-positivity classification, CoCoLIT outperforms the second-best method with improvements of +10.5% on the internal dataset and +23.7% on the external dataset. The code and models of our approach are available at https://github.com/brAIn-science/CoCoLIT.

</details>


### [47] [LoRA-based methods on Unet for transfer learning in Subarachnoid Hematoma Segmentation](https://arxiv.org/abs/2508.01772)
*Cristian Minoccheri,Matthew Hodgman,Haoyuan Ma,Rameez Merchant,Emily Wittrup,Craig Williamson,Kayvan Najarian*

Main category: eess.IV

TL;DR: 该论文提出了一种基于LoRA的改进方法（CP-LoRA和DoRA变体），用于动脉瘤性蛛网膜下腔出血（SAH）的医学图像分割，显著优于传统Unet微调方法。


<details>
  <summary>Details</summary>
Motivation: 动脉瘤性SAH是一种高死亡率的神经急症，现有方法在有限数据集上效果不佳，而LoRA方法在医学图像领域的应用尚未充分探索。

Method: 采用Unet架构，预训练于124名创伤性脑损伤患者的CT扫描，并在30名SAH患者数据上微调。提出CP-LoRA和DoRA变体，分解权重矩阵为幅度和方向分量。

Result: LoRA方法在SAH分割中表现优于传统微调，CP-LoRA参数更少但性能相当。大出血量下所有方法准确性更高。

Conclusion: 研究表明，LoRA方法在SAH分割中具有显著优势，且跨血肿类型的迁移学习可行。

Abstract: Aneurysmal subarachnoid hemorrhage (SAH) is a life-threatening neurological emergency with mortality rates exceeding 30%. Transfer learning from related hematoma types represents a potentially valuable but underexplored approach. Although Unet architectures remain the gold standard for medical image segmentation due to their effectiveness on limited datasets, Low-Rank Adaptation (LoRA) methods for parameter-efficient transfer learning have been rarely applied to convolutional neural networks in medical imaging contexts. We implemented a Unet architecture pre-trained on computed tomography scans from 124 traumatic brain injury patients across multiple institutions, then fine-tuned on 30 aneurysmal SAH patients from the University of Michigan Health System using 3-fold cross-validation. We developed a novel CP-LoRA method based on tensor CP-decomposition and introduced DoRA variants (DoRA-C, convDoRA, CP-DoRA) that decompose weight matrices into magnitude and directional components. We compared these approaches against existing LoRA methods (LoRA-C, convLoRA) and standard fine-tuning strategies across different modules on a multi-view Unet model. LoRA-based methods consistently outperformed standard Unet fine-tuning. Performance varied by hemorrhage volume, with all methods showing improved accuracy for larger volumes. CP-LoRA achieved comparable performance to existing methods while using significantly fewer parameters. Over-parameterization with higher ranks consistently yielded better performance than strictly low-rank adaptations. This study demonstrates that transfer learning between hematoma types is feasible and that LoRA-based methods significantly outperform conventional Unet fine-tuning for aneurysmal SAH segmentation.

</details>


### [48] [Joint Lossless Compression and Steganography for Medical Images via Large Language Models](https://arxiv.org/abs/2508.01782)
*Pengcheng Zheng,Xiaorong Pu,Kecheng Chen,Jiaxin Huang,Meng Yang,Bai Feng,Yazhou Ren,Jianan Jiang*

Main category: eess.IV

TL;DR: 提出了一种结合无损压缩和隐写术的新框架，用于医学图像，解决了现有方法在压缩性能、效率和安全性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的压缩方法在医学图像上表现不佳，且忽视压缩过程的安全性。

Method: 采用双路径无损压缩策略，结合自适应模态分解和分段隐写算法，并引入解剖学先验的低秩适应微调。

Result: 实验结果表明，该方法在压缩比、效率和安全性上优于现有方法。

Conclusion: 该框架为医学图像的无损压缩和安全性提供了有效解决方案，代码将公开。

Abstract: Recently, large language models (LLMs) have driven promis ing progress in lossless image compression. However, di rectly adopting existing paradigms for medical images suf fers from an unsatisfactory trade-off between compression   performance and efficiency. Moreover, existing LLM-based   compressors often overlook the security of the compres sion process, which is critical in modern medical scenarios.   To this end, we propose a novel joint lossless compression   and steganography framework. Inspired by bit plane slicing   (BPS), we find it feasible to securely embed privacy messages   into medical images in an invisible manner. Based on this in sight, an adaptive modalities decomposition strategy is first   devised to partition the entire image into two segments, pro viding global and local modalities for subsequent dual-path   lossless compression. During this dual-path stage, we inno vatively propose a segmented message steganography algo rithm within the local modality path to ensure the security of   the compression process. Coupled with the proposed anatom ical priors-based low-rank adaptation (A-LoRA) fine-tuning   strategy, extensive experimental results demonstrate the su periority of our proposed method in terms of compression ra tios, efficiency, and security. The source code will be made   publicly available.

</details>


### [49] [GR-Gaussian: Graph-Based Radiative Gaussian Splatting for Sparse-View CT Reconstruction](https://arxiv.org/abs/2508.02408)
*Yikuang Yuluo,Yue Ma,Kuan Shen,Tongtong Jin,Wang Liao,Yangpu Ma,Fuquan Wang*

Main category: eess.IV

TL;DR: GR-Gaussian是一种基于图的3D高斯泼溅框架，用于稀疏视图条件下的CT重建，通过减少针状伪影和提高重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在稀疏视图条件下容易产生针状伪影，影响重建质量。

Method: 提出两种创新：去噪点云初始化策略和基于图的像素梯度策略，优化梯度计算和密度表示。

Result: 在X-3D和真实数据集上验证，PSNR提升0.67 dB和0.92 dB，SSIM提升0.011和0.021。

Conclusion: GR-Gaussian在稀疏视图条件下能有效提高CT重建的准确性。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a promising approach for CT reconstruction. However, existing methods rely on the average gradient magnitude of points within the view, often leading to severe needle-like artifacts under sparse-view conditions. To address this challenge, we propose GR-Gaussian, a graph-based 3D Gaussian Splatting framework that suppresses needle-like artifacts and improves reconstruction accuracy under sparse-view conditions. Our framework introduces two key innovations: (1) a Denoised Point Cloud Initialization Strategy that reduces initialization errors and accelerates convergence; and (2) a Pixel-Graph-Aware Gradient Strategy that refines gradient computation using graph-based density differences, improving splitting accuracy and density representation. Experiments on X-3D and real-world datasets validate the effectiveness of GR-Gaussian, achieving PSNR improvements of 0.67 dB and 0.92 dB, and SSIM gains of 0.011 and 0.021. These results highlight the applicability of GR-Gaussian for accurate CT reconstruction under challenging sparse-view conditions.

</details>


### [50] [From Pixels to Pathology: Restoration Diffusion for Diagnostic-Consistent Virtual IHC](https://arxiv.org/abs/2508.02528)
*Jingsong Liu,Xiaofeng Deng,Han Li,Azar Kazemi,Christian Grashei,Gesa Wilkens,Xin You,Tanja Groll,Nassir Navab,Carolin Mogler,Peter J. Schüffler*

Main category: eess.IV

TL;DR: 论文提出了一种名为Star-Diff的结构感知染色恢复扩散模型，用于从H&E染色虚拟生成IHC染色图像，解决了现有方法在图像对齐和结构保持上的问题，并通过Semantic Fidelity Score (SFS)评估诊断一致性。


<details>
  <summary>Details</summary>
Motivation: H&E染色缺乏分子诊断信息，而IHC染色成本高且耗时。虚拟染色技术有望填补这一空白，但面临图像对齐和结构保持的挑战。

Method: 提出Star-Diff模型，结合残差和噪声生成路径，将虚拟染色任务重新定义为图像恢复任务。引入SFS指标，基于生物标志物分类准确性评估生成图像。

Result: 在BCI数据集上，Star-Diff在视觉保真度和诊断相关性上达到SOTA性能，推理速度快且临床适用性强。

Conclusion: Star-Diff为虚拟IHC合成提供了实用解决方案，尤其适用于术中应用。

Abstract: Hematoxylin and eosin (H&E) staining is the clinical standard for assessing tissue morphology, but it lacks molecular-level diagnostic information. In contrast, immunohistochemistry (IHC) provides crucial insights into biomarker expression, such as HER2 status for breast cancer grading, but remains costly and time-consuming, limiting its use in time-sensitive clinical workflows. To address this gap, virtual staining from H&E to IHC has emerged as a promising alternative, yet faces two core challenges: (1) Lack of fair evaluation of synthetic images against misaligned IHC ground truths, and (2) preserving structural integrity and biological variability during translation. To this end, we present an end-to-end framework encompassing both generation and evaluation in this work. We introduce Star-Diff, a structure-aware staining restoration diffusion model that reformulates virtual staining as an image restoration task. By combining residual and noise-based generation pathways, Star-Diff maintains tissue structure while modeling realistic biomarker variability. To evaluate the diagnostic consistency of the generated IHC patches, we propose the Semantic Fidelity Score (SFS), a clinical-grading-task-driven metric that quantifies class-wise semantic degradation based on biomarker classification accuracy. Unlike pixel-level metrics such as SSIM and PSNR, SFS remains robust under spatial misalignment and classifier uncertainty. Experiments on the BCI dataset demonstrate that Star-Diff achieves state-of-the-art (SOTA) performance in both visual fidelity and diagnostic relevance. With rapid inference and strong clinical alignment,it presents a practical solution for applications such as intraoperative virtual IHC synthesis.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [51] [ASDR: Exploiting Adaptive Sampling and Data Reuse for CIM-based Instant Neural Rendering](https://arxiv.org/abs/2508.02304)
*Fangxin Liu,Haomin Li,Bowen Zhu,Zongwu Wang,Zhuoran Song,Habing Guan,Li Jiang*

Main category: cs.AR

TL;DR: ASDR是一种基于CIM的加速器，通过算法-架构协同设计优化神经渲染，显著提升速度和能效。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染模型在实时性和能效上表现不足，ASDR旨在通过CIM技术解决这些问题。

Method: 提出动态采样和MLP解耦优化算法，并设计ReRAM-CIM架构支持高效数据映射和重用。

Result: 实验显示ASDR在图形渲染任务中比现有加速器和GPU快9.55倍和69.75倍，仅损失0.1 PSNR。

Conclusion: ASDR通过协同设计显著优化神经渲染性能，为实际应用提供高效解决方案。

Abstract: Neural Radiance Fields (NeRF) offer significant promise for generating photorealistic images and videos. However, existing mainstream neural rendering models often fall short in meeting the demands for immediacy and power efficiency in practical applications. Specifically, these models frequently exhibit irregular access patterns and substantial computational overhead, leading to undesirable inference latency and high power consumption. Computing-in-memory (CIM), an emerging computational paradigm, has the potential to address these access bottlenecks and reduce the power consumption associated with model execution.   To bridge the gap between model performance and real-world scene requirements, we propose an algorithm-architecture co-design approach, abbreviated as ASDR, a CIM-based accelerator supporting efficient neural rendering. At the algorithmic level, we propose two rendering optimization schemes: (1) Dynamic sampling by online sensing of the rendering difficulty of different pixels, thus reducing access memory and computational overhead. (2) Reducing MLP overhead by decoupling and approximating the volume rendering of color and density. At the architecture level, we design an efficient ReRAM-based CIM architecture with efficient data mapping and reuse microarchitecture. Experiments demonstrate that our design can achieve up to $9.55\times$ and $69.75\times$ speedup over state-of-the-art NeRF accelerators and Xavier NX GPU in graphics rendering tasks with only $0.1$ PSNR loss.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [52] [Imbalance-Robust and Sampling-Efficient Continuous Conditional GANs via Adaptive Vicinity and Auxiliary Regularization](https://arxiv.org/abs/2508.01725)
*Xin Ding,Yun Chen,Yongwei Wang,Kao Zhang,Sen Zhang,Peibei Cao,Xiangxue Wang*

Main category: cs.LG

TL;DR: CcGAN-AVAR是一种改进的CcGAN框架，通过自适应邻域机制和多任务判别器解决了数据不平衡和计算效率问题，显著提升了生成质量和采样速度。


<details>
  <summary>Details</summary>
Motivation: 现有方法CcGAN和CCDM分别存在数据不平衡和计算效率低的问题，需要一种更高效的解决方案。

Method: CcGAN-AVAR结合了自适应邻域机制和多任务判别器，优化了生成对抗网络的训练过程。

Result: 在四个基准数据集上，CcGAN-AVAR实现了最先进的生成质量，同时采样速度提升了300-2000倍。

Conclusion: CcGAN-AVAR在解决数据不平衡和计算效率问题上表现出色，为条件生成模型提供了新的方向。

Abstract: Recent advances in conditional generative modeling have introduced Continuous conditional Generative Adversarial Network (CcGAN) and Continuous Conditional Diffusion Model (CCDM) for estimating high-dimensional data distributions conditioned on scalar, continuous regression labels (e.g., angles, ages, or temperatures). However, these approaches face fundamental limitations: CcGAN suffers from data imbalance due to fixed-size vicinity constraints, while CCDM requires computationally expensive iterative sampling. We present CcGAN-AVAR, an enhanced CcGAN framework that addresses both challenges: (1) leveraging the GAN framework's native one-step generation to overcome CCDMs' sampling bottleneck (achieving 300x-2000x faster inference), while (2) two novel components specifically target data imbalance - an adaptive vicinity mechanism that dynamically adjusts vicinity's size, and a multi-task discriminator that constructs two regularization terms (through auxiliary regression and density ratio estimation) to significantly improve generator training. Extensive experiments on four benchmark datasets (64x64 to 192x192 resolution) across eight challenging imbalanced settings demonstrate that CcGAN-AVAR achieves state-of-the-art generation quality while maintaining sampling efficiency.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [53] [From Photons to Physics: Autonomous Indoor Drones and the Future of Objective Property Assessment](https://arxiv.org/abs/2508.01965)
*Petteri Teikari,Mike Jarrell,Irene Bandera Moreno,Harri Pesola*

Main category: cs.RO

TL;DR: 综述探讨了自主室内无人机与物理感知技术结合如何将主观视觉评估转变为客观量化测量，涵盖平台架构、先进传感、智能自主和集成路径四大领域。


<details>
  <summary>Details</summary>
Motivation: 推动财产评估从主观视觉检查向客观定量测量的转变，提升评估的准确性和效率。

Method: 分析了四个关键领域：优化的平台架构、先进传感模态、智能自主算法以及与现有工作流的集成。

Result: 展示了无人机技术在室内导航、材料识别、表面表征和信息最大化方面的创新应用。

Conclusion: 自主室内无人机与物理感知技术的结合为财产评估提供了革命性的解决方案，具有广泛的应用前景。

Abstract: The convergence of autonomous indoor drones with physics-aware sensing technologies promises to transform property assessment from subjective visual inspection to objective, quantitative measurement. This comprehensive review examines the technical foundations enabling this paradigm shift across four critical domains: (1) platform architectures optimized for indoor navigation, where weight constraints drive innovations in heterogeneous computing, collision-tolerant design, and hierarchical control systems; (2) advanced sensing modalities that extend perception beyond human vision, including hyperspectral imaging for material identification, polarimetric sensing for surface characterization, and computational imaging with metaphotonics enabling radical miniaturization; (3) intelligent autonomy through active reconstruction algorithms, where drones equipped with 3D Gaussian Splatting make strategic decisions about viewpoint selection to maximize information gain within battery constraints; and (4) integration pathways with existing property workflows, including Building Information Modeling (BIM) systems and industry standards like Uniform Appraisal Dataset (UAD) 3.6.

</details>


### [54] [ScrewSplat: An End-to-End Method for Articulated Object Recognition](https://arxiv.org/abs/2508.02146)
*Seungyeon Kim,Junsu Ha,Young Hun Kim,Yonghyeon Lee,Frank C. Park*

Main category: cs.RO

TL;DR: ScrewSplat是一种基于RGB图像的端到端方法，用于识别物体的几何和运动关节，无需额外输入或复杂步骤。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖强假设或额外输入，限制了实际应用。ScrewSplat旨在通过简单方法解决这一问题。

Method: 随机初始化螺旋轴并通过迭代优化恢复运动结构，结合高斯泼溅同时重建3D几何和分割物体。

Result: 在多样化物体上达到最先进的识别精度，并支持零样本文本引导操作。

Conclusion: ScrewSplat提供了一种高效、实用的方法，适用于真实场景中的关节物体识别。

Abstract: Articulated object recognition -- the task of identifying both the geometry and kinematic joints of objects with movable parts -- is essential for enabling robots to interact with everyday objects such as doors and laptops. However, existing approaches often rely on strong assumptions, such as a known number of articulated parts; require additional inputs, such as depth images; or involve complex intermediate steps that can introduce potential errors -- limiting their practicality in real-world settings. In this paper, we introduce ScrewSplat, a simple end-to-end method that operates solely on RGB observations. Our approach begins by randomly initializing screw axes, which are then iteratively optimized to recover the object's underlying kinematic structure. By integrating with Gaussian Splatting, we simultaneously reconstruct the 3D geometry and segment the object into rigid, movable parts. We demonstrate that our method achieves state-of-the-art recognition accuracy across a diverse set of articulated objects, and further enables zero-shot, text-guided manipulation using the recovered kinematic model.

</details>


### [55] [QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots](https://arxiv.org/abs/2508.02512)
*Sheng Wu,Fei Teng,Hao Shi,Qi Jiang,Kai Luo,Kaiwei Wang,Kailun Yang*

Main category: cs.RO

TL;DR: QuaDreamer是一个专为四足机器人设计的全景数据生成引擎，通过模拟四足机器人运动生成高质量全景视频，解决了训练数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 由于四足机器人的运动特性和传感器校准复杂性，高质量全景训练数据稀缺，限制了感知系统的发展。

Method: 提出Vertical Jitter Encoding (VJE)捕捉垂直振动特性，Scene-Object Controller (SOC)管理对象运动，Panoramic Enhancer (PE)解决全景失真问题。

Result: 生成的视频序列可用于训练四足机器人的全景视觉感知模型，提升360度场景中的多目标跟踪性能。

Conclusion: QuaDreamer为四足机器人提供了可控、真实的全景数据生成方案，推动了相关感知技术的发展。

Abstract: Panoramic cameras, capturing comprehensive 360-degree environmental data, are suitable for quadruped robots in surrounding perception and interaction with complex environments. However, the scarcity of high-quality panoramic training data-caused by inherent kinematic constraints and complex sensor calibration challenges-fundamentally limits the development of robust perception systems tailored to these embodied platforms. To address this issue, we propose QuaDreamer-the first panoramic data generation engine specifically designed for quadruped robots. QuaDreamer focuses on mimicking the motion paradigm of quadruped robots to generate highly controllable, realistic panoramic videos, providing a data source for downstream tasks. Specifically, to effectively capture the unique vertical vibration characteristics exhibited during quadruped locomotion, we introduce Vertical Jitter Encoding (VJE). VJE extracts controllable vertical signals through frequency-domain feature filtering and provides high-quality prompts. To facilitate high-quality panoramic video generation under jitter signal control, we propose a Scene-Object Controller (SOC) that effectively manages object motion and boosts background jitter control through the attention mechanism. To address panoramic distortions in wide-FoV video generation, we propose the Panoramic Enhancer (PE)-a dual-stream architecture that synergizes frequency-texture refinement for local detail enhancement with spatial-structure correction for global geometric consistency. We further demonstrate that the generated video sequences can serve as training data for the quadruped robot's panoramic visual perception model, enhancing the performance of multi-object tracking in 360-degree scenes. The source code and model weights will be publicly available at https://github.com/losehu/QuaDreamer.

</details>
