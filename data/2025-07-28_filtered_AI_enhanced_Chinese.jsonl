{"id": "2507.18656", "pdf": "https://arxiv.org/pdf/2507.18656", "abs": "https://arxiv.org/abs/2507.18656", "authors": ["Muhammad Zaeem Shahzad", "Muhammad Abdullah Hanif", "Bassem Ouni", "Muhammad Shafique"], "title": "ShrinkBox: Backdoor Attack on Object Detection to Disrupt Collision Avoidance in Machine Learning-based Advanced Driver Assistance Systems", "categories": ["cs.CV", "cs.LG"], "comment": "8 pages, 8 figures, 1 table", "summary": "Advanced Driver Assistance Systems (ADAS) significantly enhance road safety by detecting potential collisions and alerting drivers. However, their reliance on expensive sensor technologies such as LiDAR and radar limits accessibility, particularly in low- and middle-income countries. Machine learning-based ADAS (ML-ADAS), leveraging deep neural networks (DNNs) with only standard camera input, offers a cost-effective alternative. Critical to ML-ADAS is the collision avoidance feature, which requires the ability to detect objects and estimate their distances accurately. This is achieved with specialized DNNs like YOLO, which provides real-time object detection, and a lightweight, detection-wise distance estimation approach that relies on key features extracted from the detections like bounding box dimensions and size. However, the robustness of these systems is undermined by security vulnerabilities in object detectors. In this paper, we introduce ShrinkBox, a novel backdoor attack targeting object detection in collision avoidance ML-ADAS. Unlike existing attacks that manipulate object class labels or presence, ShrinkBox subtly shrinks ground truth bounding boxes. This attack remains undetected in dataset inspections and standard benchmarks while severely disrupting downstream distance estimation. We demonstrate that ShrinkBox can be realized in the YOLOv9m object detector at an Attack Success Rate (ASR) of 96%, with only a 4% poisoning ratio in the training instances of the KITTI dataset. Furthermore, given the low error targets introduced in our relaxed poisoning strategy, we find that ShrinkBox increases the Mean Absolute Error (MAE) in downstream distance estimation by more than 3x on poisoned samples, potentially resulting in delays or prevention of collision warnings altogether.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aShrinkBox\u7684\u65b0\u578b\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u9488\u5bf9\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684ADAS\u7cfb\u7edf\u4e2d\u7684\u7269\u4f53\u68c0\u6d4b\u529f\u80fd\uff0c\u901a\u8fc7\u7f29\u5c0f\u8fb9\u754c\u6846\u6765\u5e72\u6270\u8ddd\u79bb\u4f30\u8ba1\uff0c\u653b\u51fb\u6210\u529f\u7387\u9ad8\u4e14\u9690\u853d\u6027\u5f3a\u3002", "motivation": "\u4f20\u7edfADAS\u4f9d\u8d56\u6602\u8d35\u4f20\u611f\u5668\uff0c\u800c\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684ADAS\uff08ML-ADAS\uff09\u4f7f\u7528\u4f4e\u6210\u672c\u6444\u50cf\u5934\uff0c\u4f46\u5176\u7269\u4f53\u68c0\u6d4b\u529f\u80fd\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8981\u7814\u7a76\u65b0\u578b\u653b\u51fb\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u7cfb\u7edf\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51faShrinkBox\u653b\u51fb\uff0c\u901a\u8fc7\u8f7b\u5fae\u7f29\u5c0f\u7269\u4f53\u68c0\u6d4b\u7684\u8fb9\u754c\u6846\uff08\u800c\u975e\u76f4\u63a5\u4fee\u6539\u6807\u7b7e\u6216\u7269\u4f53\u5b58\u5728\u6027\uff09\uff0c\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u690d\u5165\u540e\u95e8\uff0c\u5f71\u54cd\u8ddd\u79bb\u4f30\u8ba1\u529f\u80fd\u3002", "result": "\u5728YOLOv9m\u68c0\u6d4b\u5668\u548cKITTI\u6570\u636e\u96c6\u4e0a\uff0c\u653b\u51fb\u6210\u529f\u7387\u8fbe96%\uff0c\u4ec5\u97004%\u7684\u6c61\u67d3\u6bd4\u4f8b\uff0c\u4e14\u663e\u8457\u589e\u52a0\u8ddd\u79bb\u4f30\u8ba1\u7684\u8bef\u5dee\uff08MAE\u589e\u52a03\u500d\u4ee5\u4e0a\uff09\u3002", "conclusion": "ShrinkBox\u653b\u51fb\u63ed\u793a\u4e86ML-ADAS\u7cfb\u7edf\u4e2d\u7269\u4f53\u68c0\u6d4b\u7684\u6f5c\u5728\u5b89\u5168\u98ce\u9669\uff0c\u5f3a\u8c03\u4e86\u5728\u8bbe\u8ba1\u548c\u8bc4\u4f30\u6b64\u7c7b\u7cfb\u7edf\u65f6\u9700\u8003\u8651\u5bf9\u6297\u6027\u653b\u51fb\u7684\u9632\u5fa1\u63aa\u65bd\u3002"}}
{"id": "2507.18667", "pdf": "https://arxiv.org/pdf/2507.18667", "abs": "https://arxiv.org/abs/2507.18667", "authors": ["Nicholas Fidalgo", "Aaron Contreras", "Katherine Harvey", "Johnny Ni"], "title": "Gen-AI Police Sketches with Stable Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This project investigates the use of multimodal AI-driven approaches to automate and enhance suspect sketching. Three pipelines were developed and evaluated: (1) baseline image-to-image Stable Diffusion model, (2) same model integrated with a pre-trained CLIP model for text-image alignment, and (3) novel approach incorporating LoRA fine-tuning of the CLIP model, applied to self-attention and cross-attention layers, and integrated with Stable Diffusion. An ablation study confirmed that fine-tuning both self- and cross-attention layers yielded the best alignment between text descriptions and sketches. Performance testing revealed that Model 1 achieved the highest structural similarity (SSIM) of 0.72 and a peak signal-to-noise ratio (PSNR) of 25 dB, outperforming Model 2 and Model 3. Iterative refinement enhanced perceptual similarity (LPIPS), with Model 3 showing improvement over Model 2 but still trailing Model 1. Qualitatively, sketches generated by Model 1 demonstrated the clearest facial features, highlighting its robustness as a baseline despite its simplicity.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u591a\u6a21\u6001AI\u65b9\u6cd5\u81ea\u52a8\u5316\u5e76\u4f18\u5316\u5acc\u7591\u4eba\u7d20\u63cf\uff0c\u6d4b\u8bd5\u4e86\u4e09\u79cd\u6a21\u578b\uff0c\u53d1\u73b0\u57fa\u7ebf\u6a21\u578b\u5728\u7ed3\u6784\u548c\u611f\u77e5\u76f8\u4f3c\u6027\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u591a\u6a21\u6001AI\u6280\u672f\u63d0\u5347\u5acc\u7591\u4eba\u7d20\u63cf\u7684\u81ea\u52a8\u5316\u6c34\u5e73\u548c\u8d28\u91cf\u3002", "method": "\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u4e09\u79cd\u6a21\u578b\uff1a\u57fa\u7ebfStable Diffusion\u6a21\u578b\u3001\u7ed3\u5408CLIP\u7684\u6a21\u578b\uff0c\u4ee5\u53ca\u5f15\u5165LoRA\u5fae\u8c03\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u57fa\u7ebf\u6a21\u578b\u5728SSIM\u548cPSNR\u4e0a\u8868\u73b0\u6700\u4f18\uff0c\u800c\u65b0\u65b9\u6cd5\u5728LPIPS\u4e0a\u6709\u63d0\u5347\u4f46\u4ecd\u4e0d\u53ca\u57fa\u7ebf\u3002", "conclusion": "\u57fa\u7ebf\u6a21\u578b\u867d\u7b80\u5355\u4f46\u5728\u7ed3\u6784\u548c\u6e05\u6670\u5ea6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u65b0\u65b9\u6cd5\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2507.18713", "pdf": "https://arxiv.org/pdf/2507.18713", "abs": "https://arxiv.org/abs/2507.18713", "authors": ["Yun Chen", "Matthew Haines", "Jingkang Wang", "Krzysztof Baron-Lis", "Sivabalan Manivasagam", "Ze Yang", "Raquel Urtasun"], "title": "SaLF: Sparse Local Fields for Multi-Sensor Rendering in Real-Time", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "High-fidelity sensor simulation of light-based sensors such as cameras and LiDARs is critical for safe and accurate autonomy testing. Neural radiance field (NeRF)-based methods that reconstruct sensor observations via ray-casting of implicit representations have demonstrated accurate simulation of driving scenes, but are slow to train and render, hampering scale. 3D Gaussian Splatting (3DGS) has demonstrated faster training and rendering times through rasterization, but is primarily restricted to pinhole camera sensors, preventing usage for realistic multi-sensor autonomy evaluation. Moreover, both NeRF and 3DGS couple the representation with the rendering procedure (implicit networks for ray-based evaluation, particles for rasterization), preventing interoperability, which is key for general usage. In this work, we present Sparse Local Fields (SaLF), a novel volumetric representation that supports rasterization and raytracing. SaLF represents volumes as a sparse set of 3D voxel primitives, where each voxel is a local implicit field. SaLF has fast training (<30 min) and rendering capabilities (50+ FPS for camera and 600+ FPS LiDAR), has adaptive pruning and densification to easily handle large scenes, and can support non-pinhole cameras and spinning LiDARs. We demonstrate that SaLF has similar realism as existing self-driving sensor simulation methods while improving efficiency and enhancing capabilities, enabling more scalable simulation. https://waabi.ai/salf/", "AI": {"tldr": "SaLF\u662f\u4e00\u79cd\u65b0\u578b\u4f53\u79ef\u8868\u793a\u65b9\u6cd5\uff0c\u652f\u6301\u5149\u6805\u5316\u548c\u5149\u7ebf\u8ffd\u8e2a\uff0c\u7528\u4e8e\u9ad8\u6548\u6a21\u62df\u591a\u4f20\u611f\u5668\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709NeRF\u548c3DGS\u65b9\u6cd5\u5728\u8bad\u7ec3\u548c\u6e32\u67d3\u901f\u5ea6\u3001\u4f20\u611f\u5668\u517c\u5bb9\u6027\u53ca\u901a\u7528\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u7a00\u758f3D\u4f53\u7d20\u57fa\u5143\u8868\u793a\u4f53\u79ef\uff0c\u6bcf\u4e2a\u4f53\u7d20\u4e3a\u5c40\u90e8\u9690\u5f0f\u573a\uff0c\u652f\u6301\u5feb\u901f\u8bad\u7ec3\u548c\u6e32\u67d3\u3002", "result": "SaLF\u8bad\u7ec3\u65f6\u95f4\u77ed\uff08<30\u5206\u949f\uff09\uff0c\u6e32\u67d3\u901f\u5ea6\u5feb\uff08\u76f8\u673a50+ FPS\uff0cLiDAR 600+ FPS\uff09\uff0c\u652f\u6301\u975e\u9488\u5b54\u76f8\u673a\u548c\u65cb\u8f6cLiDAR\u3002", "conclusion": "SaLF\u5728\u4fdd\u6301\u771f\u5b9e\u611f\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u529f\u80fd\uff0c\u9002\u7528\u4e8e\u66f4\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u9a7e\u9a76\u4f20\u611f\u5668\u6a21\u62df\u3002"}}
{"id": "2507.18830", "pdf": "https://arxiv.org/pdf/2507.18830", "abs": "https://arxiv.org/abs/2507.18830", "authors": ["Shen Zhu", "Yinzhu Jin", "Tyler Spears", "Ifrah Zawar", "P. Thomas Fletcher"], "title": "RealDeal: Enhancing Realism and Details in Brain Image Generation via Image-to-Image Diffusion Models", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "19 pages, 10 figures", "summary": "We propose image-to-image diffusion models that are designed to enhance the realism and details of generated brain images by introducing sharp edges, fine textures, subtle anatomical features, and imaging noise. Generative models have been widely adopted in the biomedical domain, especially in image generation applications. Latent diffusion models achieve state-of-the-art results in generating brain MRIs. However, due to latent compression, generated images from these models are overly smooth, lacking fine anatomical structures and scan acquisition noise that are typically seen in real images. This work formulates the realism enhancing and detail adding process as image-to-image diffusion models, which refines the quality of LDM-generated images. We employ commonly used metrics like FID and LPIPS for image realism assessment. Furthermore, we introduce new metrics to demonstrate the realism of images generated by RealDeal in terms of image noise distribution, sharpness, and texture.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u56fe\u50cf\u5230\u56fe\u50cf\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u589e\u5f3a\u751f\u6210\u8111\u90e8\u56fe\u50cf\u7684\u7ec6\u8282\u548c\u771f\u5b9e\u611f\uff0c\u5305\u62ec\u9510\u5229\u8fb9\u7f18\u3001\u7cbe\u7ec6\u7eb9\u7406\u3001\u7ec6\u5fae\u89e3\u5256\u7279\u5f81\u548c\u6210\u50cf\u566a\u58f0\u3002", "motivation": "\u73b0\u6709\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u8111\u90e8MRI\u65f6\u56e0\u6f5c\u5728\u538b\u7f29\u5bfc\u81f4\u56fe\u50cf\u8fc7\u4e8e\u5e73\u6ed1\uff0c\u7f3a\u4e4f\u771f\u5b9e\u56fe\u50cf\u4e2d\u7684\u7cbe\u7ec6\u7ed3\u6784\u548c\u566a\u58f0\u3002", "method": "\u901a\u8fc7\u56fe\u50cf\u5230\u56fe\u50cf\u7684\u6269\u6563\u6a21\u578b\u7ec6\u5316LDM\u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\uff0c\u5e76\u5f15\u5165\u65b0\u6307\u6807\u8bc4\u4f30\u566a\u58f0\u5206\u5e03\u3001\u9510\u5ea6\u548c\u7eb9\u7406\u7684\u771f\u5b9e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728FID\u548cLPIPS\u7b49\u5e38\u7528\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u65b0\u6307\u6807\u4e5f\u9a8c\u8bc1\u4e86\u751f\u6210\u56fe\u50cf\u7684\u771f\u5b9e\u6027\u3002", "conclusion": "RealDeal\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8111\u90e8\u56fe\u50cf\u7684\u7ec6\u8282\u548c\u771f\u5b9e\u611f\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.18863", "pdf": "https://arxiv.org/pdf/2507.18863", "abs": "https://arxiv.org/abs/2507.18863", "authors": ["Matthew Kit Khinn Teng", "Haibo Zhang", "Takeshi Saitoh"], "title": "Phoneme-Level Visual Speech Recognition via Point-Visual Fusion and Language Model Reconstruction", "categories": ["cs.CV", "cs.CL"], "comment": "10 pages, 3 figures", "summary": "Visual Automatic Speech Recognition (V-ASR) is a challenging task that involves interpreting spoken language solely from visual information, such as lip movements and facial expressions. This task is notably challenging due to the absence of auditory cues and the visual ambiguity of phonemes that exhibit similar visemes-distinct sounds that appear identical in lip motions. Existing methods often aim to predict words or characters directly from visual cues, but they commonly suffer from high error rates due to viseme ambiguity and require large amounts of pre-training data. We propose a novel phoneme-based two-stage framework that fuses visual and landmark motion features, followed by an LLM model for word reconstruction to address these challenges. Stage 1 consists of V-ASR, which outputs the predicted phonemes, thereby reducing training complexity. Meanwhile, the facial landmark features address speaker-specific facial characteristics. Stage 2 comprises an encoder-decoder LLM model, NLLB, that reconstructs the output phonemes back to words. Besides using a large visual dataset for deep learning fine-tuning, our PV-ASR method demonstrates superior performance by achieving 17.4% WER on the LRS2 and 21.0% WER on the LRS3 dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u97f3\u7d20\u7684\u89c6\u89c9\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08V-ASR\uff09\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u9762\u90e8\u7279\u5f81\uff0c\u5e76\u901a\u8fc7LLM\u6a21\u578b\u91cd\u6784\u5355\u8bcd\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u9519\u8bef\u7387\u3002", "motivation": "\u89c6\u89c9\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u56e0\u7f3a\u4e4f\u542c\u89c9\u7ebf\u7d22\u548c\u97f3\u7d20\u7684\u89c6\u89c9\u6a21\u7cca\u6027\uff08\u5373\u76f8\u4f3c\u7684\u53e3\u578b\u5bf9\u5e94\u4e0d\u540c\u7684\u97f3\u7d20\uff09\u800c\u6781\u5177\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u76f4\u63a5\u4ece\u89c6\u89c9\u7ebf\u7d22\u9884\u6d4b\u5355\u8bcd\u6216\u5b57\u7b26\uff0c\u4f46\u9519\u8bef\u7387\u9ad8\u4e14\u9700\u8981\u5927\u91cf\u9884\u8bad\u7ec3\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7V-ASR\u9884\u6d4b\u97f3\u7d20\uff0c\u51cf\u5c11\u8bad\u7ec3\u590d\u6742\u5ea6\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528LLM\u6a21\u578b\uff08NLLB\uff09\u5c06\u97f3\u7d20\u91cd\u6784\u4e3a\u5355\u8bcd\u3002\u7ed3\u5408\u89c6\u89c9\u548c\u9762\u90e8\u7279\u5f81\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "result": "\u5728LRS2\u548cLRS3\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5b9e\u73b0\u4e8617.4%\u548c21.0%\u7684\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684PV-ASR\u65b9\u6cd5\u901a\u8fc7\u97f3\u7d20\u9884\u6d4b\u548cLLM\u91cd\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.18923", "pdf": "https://arxiv.org/pdf/2507.18923", "abs": "https://arxiv.org/abs/2507.18923", "authors": ["Zhentao Huang", "Di Wu", "Zhenbang He", "Minglun Gong"], "title": "Gaussian Set Surface Reconstruction through Per-Gaussian Optimization", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) effectively synthesizes novel views through its flexible representation, yet fails to accurately reconstruct scene geometry. While modern variants like PGSR introduce additional losses to ensure proper depth and normal maps through Gaussian fusion, they still neglect individual placement optimization. This results in unevenly distributed Gaussians that deviate from the latent surface, complicating both reconstruction refinement and scene editing. Motivated by pioneering work on Point Set Surfaces, we propose Gaussian Set Surface Reconstruction (GSSR), a method designed to distribute Gaussians evenly along the latent surface while aligning their dominant normals with the surface normal. GSSR enforces fine-grained geometric alignment through a combination of pixel-level and Gaussian-level single-view normal consistency and multi-view photometric consistency, optimizing both local and global perspectives. To further refine the representation, we introduce an opacity regularization loss to eliminate redundant Gaussians and apply periodic depth- and normal-guided Gaussian reinitialization for a cleaner, more uniform spatial distribution. Our reconstruction results demonstrate significantly improved geometric precision in Gaussian placement, enabling intuitive scene editing and efficient generation of novel Gaussian-based 3D environments. Extensive experiments validate GSSR's effectiveness, showing enhanced geometric accuracy while preserving high-quality rendering performance.", "AI": {"tldr": "GSSR\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5747\u5300\u5206\u5e03\u9ad8\u65af\u5e76\u4f18\u5316\u5176\u6cd5\u7ebf\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u9ad8\u65af\u91cd\u5efa\u7684\u51e0\u4f55\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u67093D\u9ad8\u65af\u91cd\u5efa\u65b9\u6cd5\uff08\u59823DGS\u548cPGSR\uff09\u5728\u51e0\u4f55\u91cd\u5efa\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9ad8\u65af\u5206\u5e03\u4e0d\u5747\u5300\u4e14\u504f\u79bb\u6f5c\u5728\u8868\u9762\uff0c\u5f71\u54cd\u4e86\u91cd\u5efa\u548c\u7f16\u8f91\u6548\u679c\u3002", "method": "GSSR\u7ed3\u5408\u5355\u89c6\u89d2\u6cd5\u7ebf\u4e00\u81f4\u6027\u548c\u591a\u89c6\u89d2\u5149\u5ea6\u4e00\u81f4\u6027\uff0c\u4f18\u5316\u9ad8\u65af\u5206\u5e03\u548c\u6cd5\u7ebf\u5bf9\u9f50\uff0c\u5e76\u901a\u8fc7\u4e0d\u900f\u660e\u5ea6\u6b63\u5219\u5316\u548c\u5468\u671f\u6027\u91cd\u521d\u59cb\u5316\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGSSR\u663e\u8457\u63d0\u5347\u4e86\u51e0\u4f55\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u7684\u6e32\u67d3\u6027\u80fd\u3002", "conclusion": "GSSR\u4e3a\u9ad8\u65af\u91cd\u5efa\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u51e0\u4f55\u8868\u793a\uff0c\u652f\u6301\u9ad8\u6548\u7684\u573a\u666f\u7f16\u8f91\u548c\u65b0\u73af\u5883\u751f\u6210\u3002"}}
{"id": "2507.18939", "pdf": "https://arxiv.org/pdf/2507.18939", "abs": "https://arxiv.org/abs/2507.18939", "authors": ["Jionghao Wang", "Cheng Lin", "Yuan Liu", "Rui Xu", "Zhiyang Dou", "Xiao-Xiao Long", "Hao-Xiang Guo", "Taku Komura", "Wenping Wang", "Xin Li"], "title": "PDT: Point Distribution Transformation with Diffusion Models", "categories": ["cs.CV"], "comment": "Project page: https://shanemankiw.github.io/PDT/", "summary": "Point-based representations have consistently played a vital role in geometric data structures. Most point cloud learning and processing methods typically leverage the unordered and unconstrained nature to represent the underlying geometry of 3D shapes. However, how to extract meaningful structural information from unstructured point cloud distributions and transform them into semantically meaningful point distributions remains an under-explored problem. We present PDT, a novel framework for point distribution transformation with diffusion models. Given a set of input points, PDT learns to transform the point set from its original geometric distribution into a target distribution that is semantically meaningful. Our method utilizes diffusion models with novel architecture and learning strategy, which effectively correlates the source and the target distribution through a denoising process. Through extensive experiments, we show that our method successfully transforms input point clouds into various forms of structured outputs - ranging from surface-aligned keypoints, and inner sparse joints to continuous feature lines. The results showcase our framework's ability to capture both geometric and semantic features, offering a powerful tool for various 3D geometry processing tasks where structured point distributions are desired. Code will be available at this link: https://github.com/shanemankiw/PDT.", "AI": {"tldr": "PDT\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u65e0\u5e8f\u70b9\u4e91\u8f6c\u6362\u4e3a\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u7ed3\u6784\u5316\u5206\u5e03\u3002", "motivation": "\u89e3\u51b3\u5982\u4f55\u4ece\u65e0\u5e8f\u70b9\u4e91\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u7684\u7ed3\u6784\u4fe1\u606f\u5e76\u8f6c\u6362\u4e3a\u8bed\u4e49\u5206\u5e03\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u65b0\u67b6\u6784\u548c\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u53bb\u566a\u8fc7\u7a0b\u5173\u8054\u6e90\u5206\u5e03\u548c\u76ee\u6807\u5206\u5e03\u3002", "result": "\u6210\u529f\u5c06\u70b9\u4e91\u8f6c\u6362\u4e3a\u591a\u79cd\u7ed3\u6784\u5316\u8f93\u51fa\uff0c\u5982\u8868\u9762\u5173\u952e\u70b9\u3001\u5185\u90e8\u7a00\u758f\u5173\u8282\u548c\u8fde\u7eed\u7279\u5f81\u7ebf\u3002", "conclusion": "PDT\u80fd\u591f\u6355\u6349\u51e0\u4f55\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u4e3a3D\u51e0\u4f55\u5904\u7406\u4efb\u52a1\u63d0\u4f9b\u5f3a\u5927\u5de5\u5177\u3002"}}
{"id": "2507.18958", "pdf": "https://arxiv.org/pdf/2507.18958", "abs": "https://arxiv.org/abs/2507.18958", "authors": ["Xiaocheng Fang", "Jieyi Cai", "Huanyu Liu", "Chengju Zhou", "Minhua Lu", "Bingzhi Chen"], "title": "PerioDet: Large-Scale Panoramic Radiograph Benchmark for Clinical-Oriented Apical Periodontitis Detection", "categories": ["cs.CV"], "comment": "MICCAI 2025(Early Accept)", "summary": "Apical periodontitis is a prevalent oral pathology that presents significant public health challenges. Despite advances in automated diagnostic systems across various medical fields, the development of Computer-Aided Diagnosis (CAD) applications for apical periodontitis is still constrained by the lack of a large-scale, high-quality annotated dataset. To address this issue, we release a large-scale panoramic radiograph benchmark called \"PerioXrays\", comprising 3,673 images and 5,662 meticulously annotated instances of apical periodontitis. To the best of our knowledge, this is the first benchmark dataset for automated apical periodontitis diagnosis. This paper further proposes a clinical-oriented apical periodontitis detection (PerioDet) paradigm, which jointly incorporates Background-Denoising Attention (BDA) and IoU-Dynamic Calibration (IDC) mechanisms to address the challenges posed by background noise and small targets in automated detection. Extensive experiments on the PerioXrays dataset demonstrate the superiority of PerioDet in advancing automated apical periodontitis detection. Additionally, a well-designed human-computer collaborative experiment underscores the clinical applicability of our method as an auxiliary diagnostic tool for professional dentists.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u6839\u5c16\u5468\u708e\u6570\u636e\u96c6PerioXrays\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u5408\u80cc\u666f\u53bb\u566a\u548c\u5c0f\u76ee\u6807\u68c0\u6d4b\u7684\u81ea\u52a8\u5316\u8bca\u65ad\u65b9\u6cd5PerioDet\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u6839\u5c16\u5468\u708e\u81ea\u52a8\u5316\u8bca\u65ad\u4e2d\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faPerioDet\u65b9\u6cd5\uff0c\u7ed3\u5408\u80cc\u666f\u53bb\u566a\u6ce8\u610f\u529b\u673a\u5236\uff08BDA\uff09\u548cIoU\u52a8\u6001\u6821\u51c6\uff08IDC\uff09\u673a\u5236\u3002", "result": "\u5728PerioXrays\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86PerioDet\u7684\u4f18\u8d8a\u6027\uff0c\u5e76\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u5b9e\u9a8c\u5c55\u793a\u4e86\u4e34\u5e8a\u9002\u7528\u6027\u3002", "conclusion": "PerioXrays\u6570\u636e\u96c6\u548cPerioDet\u65b9\u6cd5\u4e3a\u6839\u5c16\u5468\u708e\u7684\u81ea\u52a8\u5316\u8bca\u65ad\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.18997", "pdf": "https://arxiv.org/pdf/2507.18997", "abs": "https://arxiv.org/abs/2507.18997", "authors": ["Zixiang Ai", "Zhenyu Cui", "Yuxin Peng", "Jiahuan Zhou"], "title": "UPP: Unified Point-Level Prompting for Robust Point Cloud Analysis", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025 as a Poster", "summary": "Pre-trained point cloud analysis models have shown promising advancements in various downstream tasks, yet their effectiveness is typically suffering from low-quality point cloud (i.e., noise and incompleteness), which is a common issue in real scenarios due to casual object occlusions and unsatisfactory data collected by 3D sensors. To this end, existing methods focus on enhancing point cloud quality by developing dedicated denoising and completion models. However, due to the isolation between the point cloud enhancement and downstream tasks, these methods fail to work in various real-world domains. In addition, the conflicting objectives between denoising and completing tasks further limit the ensemble paradigm to preserve critical geometric features. To tackle the above challenges, we propose a unified point-level prompting method that reformulates point cloud denoising and completion as a prompting mechanism, enabling robust analysis in a parameter-efficient manner. We start by introducing a Rectification Prompter to adapt to noisy points through the predicted rectification vector prompts, effectively filtering noise while preserving intricate geometric features essential for accurate analysis. Sequentially, we further incorporate a Completion Prompter to generate auxiliary point prompts based on the rectified point clouds, facilitating their robustness and adaptability. Finally, a Shape-Aware Unit module is exploited to efficiently unify and capture the filtered geometric features for the downstream point cloud analysis.Extensive experiments on four datasets demonstrate the superiority and robustness of our method when handling noisy and incomplete point cloud data against existing state-of-the-art methods. Our code is released at https://github.com/zhoujiahuan1991/ICCV2025-UPP.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u70b9\u7ea7\u63d0\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u793a\u673a\u5236\u89e3\u51b3\u70b9\u4e91\u53bb\u566a\u548c\u8865\u5168\u95ee\u9898\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u7684\u9c81\u68d2\u6027\u548c\u53c2\u6570\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u70b9\u4e91\u589e\u5f3a\u548c\u4e0b\u6e38\u4efb\u52a1\u4e4b\u95f4\u5b58\u5728\u9694\u79bb\uff0c\u4e14\u53bb\u566a\u548c\u8865\u5168\u4efb\u52a1\u7684\u76ee\u6807\u51b2\u7a81\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "\u5f15\u5165Rectification Prompter\u548cCompletion Prompter\uff0c\u7ed3\u5408Shape-Aware Unit\u6a21\u5757\uff0c\u7edf\u4e00\u5904\u7406\u70b9\u4e91\u53bb\u566a\u548c\u8865\u5168\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u566a\u58f0\u548c\u4e0d\u5b8c\u6574\u70b9\u4e91\u6570\u636e\u65f6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u63d0\u793a\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u70b9\u4e91\u8d28\u91cf\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5206\u6790\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2507.18998", "pdf": "https://arxiv.org/pdf/2507.18998", "abs": "https://arxiv.org/abs/2507.18998", "authors": ["Yongsong Huang", "Tomo Miyazaki", "Xiaofeng Liu", "Shinichiro Omachi"], "title": "GPSMamba: A Global Phase and Spectral Prompt-guided Mamba for Infrared Image Super-Resolution", "categories": ["cs.CV"], "comment": "This manuscript is under review, and copyright will be transferred   without notice", "summary": "Infrared Image Super-Resolution (IRSR) is challenged by the low contrast and sparse textures of infrared data, requiring robust long-range modeling to maintain global coherence. While State-Space Models like Mamba offer proficiency in modeling long-range dependencies for this task, their inherent 1D causal scanning mechanism fragments the global context of 2D images, hindering fine-detail restoration. To address this, we propose Global Phase and Spectral Prompt-guided Mamba (GPSMamba), a framework that synergizes architectural guidance with non-causal supervision. First, our Adaptive Semantic-Frequency State Space Module (ASF-SSM) injects a fused semantic-frequency prompt directly into the Mamba block, integrating non-local context to guide reconstruction. Then, a novel Thermal-Spectral Attention and Phase Consistency Loss provides explicit, non-causal supervision to enforce global structural and spectral fidelity. By combining these two innovations, our work presents a systematic strategy to mitigate the limitations of causal modeling. Extensive experiments demonstrate that GPSMamba achieves state-of-the-art performance, validating our approach as a powerful new paradigm for infrared image restoration. Code is available at https://github.com/yongsongH/GPSMamba.", "AI": {"tldr": "GPSMamba\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5168\u5c40\u76f8\u4f4d\u548c\u9891\u8c31\u63d0\u793a\u7684Mamba\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u56e0\u679c\u76d1\u7763\u548c\u8bed\u4e49-\u9891\u7387\u72b6\u6001\u7a7a\u95f4\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u7ea2\u5916\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\u56e0\u679c\u5efa\u6a21\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7ea2\u5916\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08IRSR\uff09\u56e0\u4f4e\u5bf9\u6bd4\u5ea6\u548c\u7a00\u758f\u7eb9\u7406\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u5f3a\u5927\u7684\u957f\u7a0b\u5efa\u6a21\u80fd\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u5982Mamba\u867d\u64c5\u957f\u957f\u7a0b\u4f9d\u8d56\u5efa\u6a21\uff0c\u4f46\u51761D\u56e0\u679c\u626b\u63cf\u673a\u5236\u4f1a\u7834\u574f2D\u56fe\u50cf\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u5f71\u54cd\u7ec6\u8282\u6062\u590d\u3002", "method": "\u63d0\u51faGPSMamba\u6846\u67b6\uff0c\u5305\u62ec\u81ea\u9002\u5e94\u8bed\u4e49-\u9891\u7387\u72b6\u6001\u7a7a\u95f4\u6a21\u5757\uff08ASF-SSM\uff09\u548c\u70ed\u8c31\u6ce8\u610f\u529b\u4e0e\u76f8\u4f4d\u4e00\u81f4\u6027\u635f\u5931\uff0c\u901a\u8fc7\u975e\u56e0\u679c\u76d1\u7763\u548c\u5168\u5c40\u7ed3\u6784\u6307\u5bfc\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGPSMamba\u5728\u7ea2\u5916\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "GPSMamba\u901a\u8fc7\u7cfb\u7edf\u6027\u7b56\u7565\u89e3\u51b3\u4e86\u56e0\u679c\u5efa\u6a21\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u7ea2\u5916\u56fe\u50cf\u6062\u590d\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.19002", "pdf": "https://arxiv.org/pdf/2507.19002", "abs": "https://arxiv.org/abs/2507.19002", "authors": ["Ying Ba", "Tianyu Zhang", "Yalong Bai", "Wenyi Mo", "Tao Liang", "Bing Su", "Ji-Rong Wen"], "title": "Enhancing Reward Models for High-quality Image Generation: Beyond Text-Image Alignment", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "Contemporary image generation systems have achieved high fidelity and superior aesthetic quality beyond basic text-image alignment. However, existing evaluation frameworks have failed to evolve in parallel. This study reveals that human preference reward models fine-tuned based on CLIP and BLIP architectures have inherent flaws: they inappropriately assign low scores to images with rich details and high aesthetic value, creating a significant discrepancy with actual human aesthetic preferences. To address this issue, we design a novel evaluation score, ICT (Image-Contained-Text) score, that achieves and surpasses the objectives of text-image alignment by assessing the degree to which images represent textual content. Building upon this foundation, we further train an HP (High-Preference) score model using solely the image modality to enhance image aesthetics and detail quality while maintaining text-image alignment. Experiments demonstrate that the proposed evaluation model improves scoring accuracy by over 10\\% compared to existing methods, and achieves significant results in optimizing state-of-the-art text-to-image models. This research provides theoretical and empirical support for evolving image generation technology toward higher-order human aesthetic preferences. Code is available at https://github.com/BarretBa/ICTHP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u8bc4\u4f30\u65b9\u6cd5ICT\u548cHP\u5206\u6570\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u6a21\u578b\u5728\u7ec6\u8282\u548c\u7f8e\u5b66\u8bc4\u5206\u4e0a\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u5206\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u751f\u6210\u7cfb\u7edf\u7684\u8bc4\u4f30\u6846\u67b6\u672a\u80fd\u8ddf\u4e0a\u5176\u9ad8\u4fdd\u771f\u548c\u7f8e\u5b66\u8d28\u91cf\u7684\u8fdb\u6b65\uff0c\u5bfc\u81f4\u8bc4\u5206\u4e0e\u4eba\u7c7b\u504f\u597d\u4e0d\u4e00\u81f4\u3002", "method": "\u8bbe\u8ba1\u4e86ICT\u5206\u6570\u4ee5\u6539\u8fdb\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u8bc4\u4f30\uff0c\u5e76\u8bad\u7ec3HP\u5206\u6570\u6a21\u578b\u4ee5\u63d0\u5347\u56fe\u50cf\u7f8e\u5b66\u548c\u7ec6\u8282\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u6a21\u578b\u8bc4\u5206\u51c6\u786e\u6027\u63d0\u5347\u8d85\u8fc710%\uff0c\u5e76\u5728\u4f18\u5316\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e2d\u53d6\u5f97\u663e\u8457\u6210\u679c\u3002", "conclusion": "\u7814\u7a76\u4e3a\u56fe\u50cf\u751f\u6210\u6280\u672f\u5411\u66f4\u9ad8\u9636\u4eba\u7c7b\u7f8e\u5b66\u504f\u597d\u53d1\u5c55\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8bc1\u652f\u6301\u3002"}}
{"id": "2507.19035", "pdf": "https://arxiv.org/pdf/2507.19035", "abs": "https://arxiv.org/abs/2507.19035", "authors": ["Jitindra Fartiyal", "Pedro Freire", "Yasmeen Whayeb", "James S. Wolffsohn", "Sergei K. Turitsyn", "Sergei G. Sokolov"], "title": "Dual Path Learning -- learning from noise and context for medical image denoising", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "10 pages, 7 figures", "summary": "Medical imaging plays a critical role in modern healthcare, enabling clinicians to accurately diagnose diseases and develop effective treatment plans. However, noise, often introduced by imaging devices, can degrade image quality, leading to misinterpretation and compromised clinical outcomes. Existing denoising approaches typically rely either on noise characteristics or on contextual information from the image. Moreover, they are commonly developed and evaluated for a single imaging modality and noise type. Motivated by Geng et.al CNCL, which integrates both noise and context, this study introduces a Dual-Pathway Learning (DPL) model architecture that effectively denoises medical images by leveraging both sources of information and fusing them to generate the final output. DPL is evaluated across multiple imaging modalities and various types of noise, demonstrating its robustness and generalizability. DPL improves PSNR by 3.35% compared to the baseline UNet when evaluated on Gaussian noise and trained across all modalities. The code is available at 10.5281/zenodo.15836053.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u8def\u5f84\u5b66\u4e60\uff08DPL\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u566a\u58f0\u7279\u5f81\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u6765\u53bb\u566a\u533b\u5b66\u56fe\u50cf\uff0c\u5e76\u5728\u591a\u79cd\u6a21\u6001\u548c\u566a\u58f0\u7c7b\u578b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u566a\u58f0\u4f1a\u964d\u4f4e\u56fe\u50cf\u8d28\u91cf\uff0c\u5f71\u54cd\u8bca\u65ad\u548c\u6cbb\u7597\u6548\u679c\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ec5\u4f9d\u8d56\u566a\u58f0\u7279\u5f81\u6216\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4e14\u5c40\u9650\u4e8e\u5355\u4e00\u6a21\u6001\u548c\u566a\u58f0\u7c7b\u578b\u3002", "method": "\u63d0\u51faDPL\u6a21\u578b\uff0c\u6574\u5408\u566a\u58f0\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u5b66\u4e60\u751f\u6210\u6700\u7ec8\u53bb\u566a\u7ed3\u679c\u3002", "result": "DPL\u5728\u591a\u79cd\u6a21\u6001\u548c\u566a\u58f0\u7c7b\u578b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cPSNR\u6bd4\u57fa\u7ebfUNet\u63d0\u9ad8\u4e863.35%\uff08\u9ad8\u65af\u566a\u58f0\uff09\u3002", "conclusion": "DPL\u662f\u4e00\u79cd\u901a\u7528\u4e14\u9c81\u68d2\u7684\u533b\u5b66\u56fe\u50cf\u53bb\u566a\u65b9\u6cd5\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2507.19058", "pdf": "https://arxiv.org/pdf/2507.19058", "abs": "https://arxiv.org/abs/2507.19058", "authors": ["Chong Xia", "Shengjun Zhang", "Fangfu Liu", "Chang Liu", "Khodchaphun Hirunyaratsameewong", "Yueqi Duan"], "title": "ScenePainter: Semantically Consistent Perpetual 3D Scene Generation with Concept Relation Alignment", "categories": ["cs.CV"], "comment": null, "summary": "Perpetual 3D scene generation aims to produce long-range and coherent 3D view sequences, which is applicable for long-term video synthesis and 3D scene reconstruction. Existing methods follow a \"navigate-and-imagine\" fashion and rely on outpainting for successive view expansion. However, the generated view sequences suffer from semantic drift issue derived from the accumulated deviation of the outpainting module. To tackle this challenge, we propose ScenePainter, a new framework for semantically consistent 3D scene generation, which aligns the outpainter's scene-specific prior with the comprehension of the current scene. To be specific, we introduce a hierarchical graph structure dubbed SceneConceptGraph to construct relations among multi-level scene concepts, which directs the outpainter for consistent novel views and can be dynamically refined to enhance diversity. Extensive experiments demonstrate that our framework overcomes the semantic drift issue and generates more consistent and immersive 3D view sequences. Project Page: https://xiac20.github.io/ScenePainter/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faScenePainter\u6846\u67b6\uff0c\u901a\u8fc7SceneConceptGraph\u89e3\u51b33D\u573a\u666f\u751f\u6210\u4e2d\u7684\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\uff0c\u751f\u6210\u66f4\u4e00\u81f4\u548c\u6c89\u6d78\u7684\u89c6\u56fe\u5e8f\u5217\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u63a8\u6a21\u5757\u5bfc\u81f4\u8bed\u4e49\u6f02\u79fb\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u6846\u67b6\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "method": "\u5f15\u5165SceneConceptGraph\u5c42\u6b21\u56fe\u7ed3\u6784\uff0c\u52a8\u6001\u4f18\u5316\u5916\u63a8\u6a21\u5757\u7684\u573a\u666f\u7279\u5b9a\u5148\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u6846\u67b6\u514b\u670d\u8bed\u4e49\u6f02\u79fb\uff0c\u751f\u6210\u66f4\u4e00\u81f4\u548c\u6c89\u6d78\u76843D\u89c6\u56fe\u5e8f\u5217\u3002", "conclusion": "ScenePainter\u901a\u8fc7\u573a\u666f\u6982\u5ff5\u56fe\u63d0\u53473D\u573a\u666f\u751f\u6210\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u591a\u6837\u6027\u3002"}}
{"id": "2507.19071", "pdf": "https://arxiv.org/pdf/2507.19071", "abs": "https://arxiv.org/abs/2507.19071", "authors": ["Yangyang Xu", "Bangzhen Liu", "Wenqi Shao", "Yong Du", "Shengfeng He", "Tingting Zhu"], "title": "Cross-Subject Mind Decoding from Inaccurate Representations", "categories": ["cs.CV"], "comment": null, "summary": "Decoding stimulus images from fMRI signals has advanced with pre-trained generative models. However, existing methods struggle with cross-subject mappings due to cognitive variability and subject-specific differences. This challenge arises from sequential errors, where unidirectional mappings generate partially inaccurate representations that, when fed into diffusion models, accumulate errors and degrade reconstruction fidelity. To address this, we propose the Bidirectional Autoencoder Intertwining framework for accurate decoded representation prediction. Our approach unifies multiple subjects through a Subject Bias Modulation Module while leveraging bidirectional mapping to better capture data distributions for precise representation prediction. To further enhance fidelity when decoding representations into stimulus images, we introduce a Semantic Refinement Module to improve semantic representations and a Visual Coherence Module to mitigate the effects of inaccurate visual representations. Integrated with ControlNet and Stable Diffusion, our method outperforms state-of-the-art approaches on benchmark datasets in both qualitative and quantitative evaluations. Moreover, our framework exhibits strong adaptability to new subjects with minimal training samples.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5411\u81ea\u7f16\u7801\u5668\u6846\u67b6\uff08Bidirectional Autoencoder Intertwining\uff09\uff0c\u901a\u8fc7\u53cc\u5411\u6620\u5c04\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\u89e3\u51b3\u8de8\u88ab\u8bd5fMRI\u89e3\u7801\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u88ab\u8bd5fMRI\u89e3\u7801\u4e2d\u56e0\u8ba4\u77e5\u5dee\u5f02\u548c\u5355\u5411\u6620\u5c04\u5bfc\u81f4\u8bef\u5dee\u7d2f\u79ef\uff0c\u91cd\u5efa\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u7ed3\u5408\u53cc\u5411\u6620\u5c04\u3001Subject Bias Modulation\u6a21\u5757\u3001Semantic Refinement\u6a21\u5757\u548cVisual Coherence\u6a21\u5757\uff0c\u96c6\u6210ControlNet\u548cStable Diffusion\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5bf9\u65b0\u88ab\u8bd5\u9002\u5e94\u6027\u5f3a\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u88ab\u8bd5\u89e3\u7801\u7684\u8bef\u5dee\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2507.19121", "pdf": "https://arxiv.org/pdf/2507.19121", "abs": "https://arxiv.org/abs/2507.19121", "authors": ["Kaiyue Zhou", "Zelong Tan", "Hongxiao Wang", "Ya-li Li", "Shengjin Wang"], "title": "Preserving Topological and Geometric Embeddings for Point Cloud Recovery", "categories": ["cs.CV"], "comment": null, "summary": "Recovering point clouds involves the sequential process of sampling and restoration, yet existing methods struggle to effectively leverage both topological and geometric attributes. To address this, we propose an end-to-end architecture named \\textbf{TopGeoFormer}, which maintains these critical features throughout the sampling and restoration phases. First, we revisit traditional feature extraction techniques to yield topological embedding using a continuous mapping of relative relationships between neighboring points, and integrate it in both phases for preserving the structure of the original space. Second, we propose the \\textbf{InterTwining Attention} to fully merge topological and geometric embeddings, which queries shape with local awareness in both phases to form a learnable shape context facilitated with point-wise, point-shape-wise, and intra-shape features. Third, we introduce a full geometry loss and a topological constraint loss to optimize the embeddings in both Euclidean and topological spaces. The geometry loss uses inconsistent matching between coarse-to-fine generations and targets for reconstructing better geometric details, and the constraint loss limits embedding variances for better approximation of the topological space. In experiments, we comprehensively analyze the circumstances using the conventional and learning-based sampling/upsampling algorithms. The quantitative and qualitative results demonstrate that our method significantly outperforms existing sampling and recovery methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTopGeoFormer\u7684\u7aef\u5230\u7aef\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u5408\u62d3\u6251\u548c\u51e0\u4f55\u5c5e\u6027\uff0c\u6539\u8fdb\u4e86\u70b9\u4e91\u91c7\u6837\u548c\u6062\u590d\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5229\u7528\u70b9\u4e91\u7684\u62d3\u6251\u548c\u51e0\u4f55\u5c5e\u6027\uff0c\u5bfc\u81f4\u91c7\u6837\u548c\u6062\u590d\u6548\u679c\u4e0d\u4f73\u3002", "method": "1. \u4f7f\u7528\u8fde\u7eed\u6620\u5c04\u63d0\u53d6\u62d3\u6251\u5d4c\u5165\uff1b2. \u63d0\u51faInterTwining Attention\u878d\u5408\u62d3\u6251\u548c\u51e0\u4f55\u5d4c\u5165\uff1b3. \u5f15\u5165\u51e0\u4f55\u635f\u5931\u548c\u62d3\u6251\u7ea6\u675f\u635f\u5931\u4f18\u5316\u5d4c\u5165\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TopGeoFormer\u901a\u8fc7\u7ed3\u5408\u62d3\u6251\u548c\u51e0\u4f55\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91\u91c7\u6837\u548c\u6062\u590d\u7684\u6548\u679c\u3002"}}
{"id": "2507.19141", "pdf": "https://arxiv.org/pdf/2507.19141", "abs": "https://arxiv.org/abs/2507.19141", "authors": ["Jie Chen", "Zhangchi Hu", "Peixi Wu", "Huyue Zhu", "Hebei Li", "Xiaoyan Sun"], "title": "DASH: 4D Hash Encoding with Self-Supervised Decomposition for Real-Time Dynamic Scene Rendering", "categories": ["cs.CV"], "comment": null, "summary": "Dynamic scene reconstruction is a long-term challenge in 3D vision. Existing plane-based methods in dynamic Gaussian splatting suffer from an unsuitable low-rank assumption, causing feature overlap and poor rendering quality. Although 4D hash encoding provides an explicit representation without low-rank constraints, directly applying it to the entire dynamic scene leads to substantial hash collisions and redundancy. To address these challenges, we present DASH, a real-time dynamic scene rendering framework that employs 4D hash encoding coupled with self-supervised decomposition. Our approach begins with a self-supervised decomposition mechanism that separates dynamic and static components without manual annotations or precomputed masks. Next, we introduce a multiresolution 4D hash encoder for dynamic elements, providing an explicit representation that avoids the low-rank assumption. Finally, we present a spatio-temporal smoothness regularization strategy to mitigate unstable deformation artifacts. Experiments on real-world datasets demonstrate that DASH achieves state-of-the-art dynamic rendering performance, exhibiting enhanced visual quality at real-time speeds of 264 FPS on a single 4090 GPU. Code: https://github.com/chenj02/DASH.", "AI": {"tldr": "DASH\u6846\u67b6\u901a\u8fc74D\u54c8\u5e0c\u7f16\u7801\u4e0e\u81ea\u76d1\u7763\u5206\u89e3\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u573a\u666f\u7684\u5b9e\u65f6\u9ad8\u8d28\u91cf\u6e32\u67d3\u3002", "motivation": "\u73b0\u6709\u52a8\u6001\u9ad8\u65af\u70b9\u4e91\u65b9\u6cd5\u56e0\u4f4e\u79e9\u5047\u8bbe\u5bfc\u81f4\u7279\u5f81\u91cd\u53e0\u548c\u6e32\u67d3\u8d28\u91cf\u5dee\uff0c\u800c4D\u54c8\u5e0c\u7f16\u7801\u76f4\u63a5\u5e94\u7528\u4e8e\u6574\u4e2a\u52a8\u6001\u573a\u666f\u5219\u4f1a\u5bfc\u81f4\u54c8\u5e0c\u51b2\u7a81\u548c\u5197\u4f59\u3002", "method": "1. \u81ea\u76d1\u7763\u5206\u89e3\u673a\u5236\u5206\u79bb\u52a8\u6001\u4e0e\u9759\u6001\u6210\u5206\uff1b2. \u52a8\u6001\u90e8\u5206\u91c7\u7528\u591a\u5206\u8fa8\u73874D\u54c8\u5e0c\u7f16\u7801\uff1b3. \u65f6\u7a7a\u5e73\u6ed1\u6b63\u5219\u5316\u7b56\u7565\u51cf\u5c11\u53d8\u5f62\u4f2a\u5f71\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cDASH\u5b9e\u73b0\u4e86264 FPS\u7684\u5b9e\u65f6\u6e32\u67d3\uff0c\u89c6\u89c9\u8d28\u91cf\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DASH\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u89e3\u51b3\u4e86\u52a8\u6001\u573a\u666f\u6e32\u67d3\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u5b9e\u65f6\u6e32\u67d3\u3002"}}
{"id": "2507.19184", "pdf": "https://arxiv.org/pdf/2507.19184", "abs": "https://arxiv.org/abs/2507.19184", "authors": ["Kotha Kartheek", "Lingamaneni Gnanesh Chowdary", "Snehasis Mukherjee"], "title": "Continual Learning-Based Unified Model for Unpaired Image Restoration Tasks", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Restoration of images contaminated by different adverse weather conditions such as fog, snow, and rain is a challenging task due to the varying nature of the weather conditions. Most of the existing methods focus on any one particular weather conditions. However, for applications such as autonomous driving, a unified model is necessary to perform restoration of corrupted images due to different weather conditions. We propose a continual learning approach to propose a unified framework for image restoration. The proposed framework integrates three key innovations: (1) Selective Kernel Fusion layers that dynamically combine global and local features for robust adaptive feature selection; (2) Elastic Weight Consolidation (EWC) to enable continual learning and mitigate catastrophic forgetting across multiple restoration tasks; and (3) a novel Cycle-Contrastive Loss that enhances feature discrimination while preserving semantic consistency during domain translation. Further, we propose an unpaired image restoration approach to reduce the dependance of the proposed approach on the training data. Extensive experiments on standard benchmark datasets for dehazing, desnowing and deraining tasks demonstrate significant improvements in PSNR, SSIM, and perceptual quality over the state-of-the-art.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6301\u7eed\u5b66\u4e60\u7684\u7edf\u4e00\u56fe\u50cf\u4fee\u590d\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u56fe\u50cf\u6062\u590d\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6838\u878d\u5408\u5c42\u3001\u5f39\u6027\u6743\u91cd\u5de9\u56fa\u548c\u5faa\u73af\u5bf9\u6bd4\u635f\u5931\u5b9e\u73b0\u9ad8\u6548\u4fee\u590d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u9488\u5bf9\u5355\u4e00\u5929\u6c14\u6761\u4ef6\uff0c\u800c\u81ea\u52a8\u9a7e\u9a76\u7b49\u5e94\u7528\u9700\u8981\u7edf\u4e00\u6a21\u578b\u5904\u7406\u591a\u79cd\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u56fe\u50cf\u4fee\u590d\u3002", "method": "\u7ed3\u5408\u9009\u62e9\u6027\u6838\u878d\u5408\u5c42\u3001\u5f39\u6027\u6743\u91cd\u5de9\u56fa\uff08EWC\uff09\u548c\u5faa\u73af\u5bf9\u6bd4\u635f\u5931\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u914d\u5bf9\u7684\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u3002", "result": "\u5728\u53bb\u96fe\u3001\u53bb\u96ea\u548c\u53bb\u96e8\u4efb\u52a1\u7684\u6807\u51c6\u6570\u636e\u96c6\u4e0a\uff0cPSNR\u3001SSIM\u548c\u611f\u77e5\u8d28\u91cf\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u56fe\u50cf\u4fee\u590d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19186", "pdf": "https://arxiv.org/pdf/2507.19186", "abs": "https://arxiv.org/abs/2507.19186", "authors": ["Niklas Bubeck", "Yundi Zhang", "Suprosanna Shit", "Daniel Rueckert", "Jiazhen Pan"], "title": "Reconstruct or Generate: Exploring the Spectrum of Generative Modeling for Cardiac MRI", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "In medical imaging, generative models are increasingly relied upon for two distinct but equally critical tasks: reconstruction, where the goal is to restore medical imaging (usually inverse problems like inpainting or superresolution), and generation, where synthetic data is created to augment datasets or carry out counterfactual analysis. Despite shared architecture and learning frameworks, they prioritize different goals: generation seeks high perceptual quality and diversity, while reconstruction focuses on data fidelity and faithfulness. In this work, we introduce a \"generative model zoo\" and systematically analyze how modern latent diffusion models and autoregressive models navigate the reconstruction-generation spectrum. We benchmark a suite of generative models across representative cardiac medical imaging tasks, focusing on image inpainting with varying masking ratios and sampling strategies, as well as unconditional image generation. Our findings show that diffusion models offer superior perceptual quality for unconditional generation but tend to hallucinate as masking ratios increase, whereas autoregressive models maintain stable perceptual performance across masking levels, albeit with generally lower fidelity.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u751f\u6210\u6a21\u578b\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u4e24\u79cd\u4efb\u52a1\uff1a\u91cd\u5efa\u4e0e\u751f\u6210\uff0c\u6bd4\u8f83\u4e86\u6269\u6563\u6a21\u578b\u548c\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u751f\u6210\u6a21\u578b\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u5982\u4f55\u5e73\u8861\u91cd\u5efa\uff08\u6570\u636e\u4fdd\u771f\uff09\u4e0e\u751f\u6210\uff08\u611f\u77e5\u8d28\u91cf\uff09\u7684\u4e0d\u540c\u9700\u6c42\u3002", "method": "\u5f15\u5165\u201c\u751f\u6210\u6a21\u578b\u52a8\u7269\u56ed\u201d\uff0c\u7cfb\u7edf\u8bc4\u4f30\u6269\u6563\u6a21\u578b\u548c\u81ea\u56de\u5f52\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\uff08\u5982\u4fee\u590d\u548c\u751f\u6210\uff09\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u6269\u6563\u6a21\u578b\u5728\u65e0\u6761\u4ef6\u751f\u6210\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u4fee\u590d\u4efb\u52a1\u4e2d\u6613\u4ea7\u751f\u5e7b\u89c9\uff1b\u81ea\u56de\u5f52\u6a21\u578b\u4fee\u590d\u7a33\u5b9a\u6027\u9ad8\uff0c\u4f46\u4fdd\u771f\u5ea6\u8f83\u4f4e\u3002", "conclusion": "\u4e0d\u540c\u751f\u6210\u6a21\u578b\u5404\u6709\u4f18\u52a3\uff0c\u9700\u6839\u636e\u4efb\u52a1\u9700\u6c42\u9009\u62e9\u5408\u9002\u6a21\u578b\u3002"}}
{"id": "2507.19201", "pdf": "https://arxiv.org/pdf/2507.19201", "abs": "https://arxiv.org/abs/2507.19201", "authors": ["Xin Li", "Kaixiang Yang", "Qiang Li", "Zhiwei Wang"], "title": "Joint Holistic and Lesion Controllable Mammogram Synthesis via Gated Conditional Diffusion Model", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Accepted, ACM Multimedia 2025, 10 pages, 5 figures", "summary": "Mammography is the most commonly used imaging modality for breast cancer screening, driving an increasing demand for deep-learning techniques to support large-scale analysis. However, the development of accurate and robust methods is often limited by insufficient data availability and a lack of diversity in lesion characteristics. While generative models offer a promising solution for data synthesis, current approaches often fail to adequately emphasize lesion-specific features and their relationships with surrounding tissues. In this paper, we propose Gated Conditional Diffusion Model (GCDM), a novel framework designed to jointly synthesize holistic mammogram images and localized lesions. GCDM is built upon a latent denoising diffusion framework, where the noised latent image is concatenated with a soft mask embedding that represents breast, lesion, and their transitional regions, ensuring anatomical coherence between them during the denoising process. To further emphasize lesion-specific features, GCDM incorporates a gated conditioning branch that guides the denoising process by dynamically selecting and fusing the most relevant radiomic and geometric properties of lesions, effectively capturing their interplay. Experimental results demonstrate that GCDM achieves precise control over small lesion areas while enhancing the realism and diversity of synthesized mammograms. These advancements position GCDM as a promising tool for clinical applications in mammogram synthesis. Our code is available at https://github.com/lixinHUST/Gated-Conditional-Diffusion-Model/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684Gated Conditional Diffusion Model (GCDM)\uff0c\u7528\u4e8e\u8054\u5408\u5408\u6210\u5b8c\u6574\u7684\u4e73\u817aX\u5149\u56fe\u50cf\u548c\u5c40\u90e8\u75c5\u53d8\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u75c5\u53d8\u7279\u5f81\u548c\u5468\u56f4\u7ec4\u7ec7\u5173\u7cfb\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u4e73\u817aX\u5149\u68c0\u67e5\u662f\u4e73\u817a\u764c\u7b5b\u67e5\u7684\u4e3b\u8981\u65b9\u6cd5\uff0c\u4f46\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u56e0\u6570\u636e\u4e0d\u8db3\u548c\u75c5\u53d8\u7279\u5f81\u591a\u6837\u6027\u7f3a\u4e4f\u800c\u53d7\u9650\u3002\u73b0\u6709\u751f\u6210\u6a21\u578b\u672a\u80fd\u5145\u5206\u5f3a\u8c03\u75c5\u53d8\u7279\u5f02\u6027\u7279\u5f81\u53ca\u5176\u4e0e\u5468\u56f4\u7ec4\u7ec7\u7684\u5173\u7cfb\u3002", "method": "GCDM\u57fa\u4e8e\u6f5c\u5728\u53bb\u566a\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u8f6f\u63a9\u7801\u5d4c\u5165\u8868\u793a\u4e73\u817a\u3001\u75c5\u53d8\u53ca\u5176\u8fc7\u6e21\u533a\u57df\uff0c\u786e\u4fdd\u89e3\u5256\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u5f15\u5165\u95e8\u63a7\u6761\u4ef6\u5206\u652f\u52a8\u6001\u9009\u62e9\u548c\u878d\u5408\u75c5\u53d8\u7684\u653e\u5c04\u7ec4\u5b66\u548c\u51e0\u4f55\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGCDM\u80fd\u7cbe\u786e\u63a7\u5236\u5c0f\u75c5\u53d8\u533a\u57df\uff0c\u540c\u65f6\u63d0\u5347\u5408\u6210\u4e73\u817aX\u5149\u56fe\u50cf\u7684\u771f\u5b9e\u6027\u548c\u591a\u6837\u6027\u3002", "conclusion": "GCDM\u5728\u4e73\u817aX\u5149\u56fe\u50cf\u5408\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6709\u671b\u6210\u4e3a\u4e34\u5e8a\u5e94\u7528\u7684\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.19230", "pdf": "https://arxiv.org/pdf/2507.19230", "abs": "https://arxiv.org/abs/2507.19230", "authors": ["Niels Rocholl", "Ewoud Smit", "Mathias Prokop", "Alessa Hering"], "title": "Unstable Prompts, Unreliable Segmentations: A Challenge for Longitudinal Lesion Analysis", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Longitudinal lesion analysis is crucial for oncological care, yet automated tools often struggle with temporal consistency. While universal lesion segmentation models have advanced, they are typically designed for single time points. This paper investigates the performance of the ULS23 segmentation model in a longitudinal context. Using a public clinical dataset of baseline and follow-up CT scans, we evaluated the model's ability to segment and track lesions over time. We identified two critical, interconnected failure modes: a sharp degradation in segmentation quality in follow-up cases due to inter-scan registration errors, and a subsequent breakdown of the lesion correspondence process. To systematically probe this vulnerability, we conducted a controlled experiment where we artificially displaced the input volume relative to the true lesion center. Our results demonstrate that the model's performance is highly dependent on its assumption of a centered lesion; segmentation accuracy collapses when the lesion is sufficiently displaced. These findings reveal a fundamental limitation of applying single-timepoint models to longitudinal data. We conclude that robust oncological tracking requires a paradigm shift away from cascading single-purpose tools towards integrated, end-to-end models inherently designed for temporal analysis.", "AI": {"tldr": "ULS23\u6a21\u578b\u5728\u7eb5\u5411\u75c5\u7076\u5206\u6790\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u626b\u63cf\u914d\u51c6\u9519\u8bef\u548c\u75c5\u7076\u5bf9\u5e94\u5931\u6548\u3002", "motivation": "\u7814\u7a76ULS23\u6a21\u578b\u5728\u7eb5\u5411CT\u626b\u63cf\u4e2d\u7684\u75c5\u7076\u5206\u5272\u548c\u8ffd\u8e2a\u6027\u80fd\uff0c\u63ed\u793a\u5355\u65f6\u95f4\u70b9\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u516c\u5f00\u4e34\u5e8a\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u57fa\u7ebf\u53ca\u968f\u8bbfCT\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u4eba\u4e3a\u4f4d\u79fb\u5b9e\u9a8c\u9a8c\u8bc1\u6a21\u578b\u8106\u5f31\u6027\u3002", "result": "\u6a21\u578b\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u75c5\u7076\u5c45\u4e2d\u5047\u8bbe\uff0c\u4f4d\u79fb\u540e\u5206\u5272\u51c6\u786e\u6027\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u9700\u5f00\u53d1\u7aef\u5230\u7aef\u7684\u7eb5\u5411\u5206\u6790\u6a21\u578b\uff0c\u800c\u975e\u4f9d\u8d56\u5355\u65f6\u95f4\u70b9\u5de5\u5177\u3002"}}
{"id": "2507.19328", "pdf": "https://arxiv.org/pdf/2507.19328", "abs": "https://arxiv.org/abs/2507.19328", "authors": ["Kirsten W. H. Maas", "Danny Ruijters", "Nicola Pezzotti", "Anna Vilanova"], "title": "NerT-CA: Efficient Dynamic Reconstruction from Sparse-view X-ray Coronary Angiography", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Three-dimensional (3D) and dynamic 3D+time (4D) reconstruction of coronary arteries from X-ray coronary angiography (CA) has the potential to improve clinical procedures. However, there are multiple challenges to be addressed, most notably, blood-vessel structure sparsity, poor background and blood vessel distinction, sparse-views, and intra-scan motion. State-of-the-art reconstruction approaches rely on time-consuming manual or error-prone automatic segmentations, limiting clinical usability. Recently, approaches based on Neural Radiance Fields (NeRF) have shown promise for automatic reconstructions in the sparse-view setting. However, they suffer from long training times due to their dependence on MLP-based representations. We propose NerT-CA, a hybrid approach of Neural and Tensorial representations for accelerated 4D reconstructions with sparse-view CA. Building on top of the previous NeRF-based work, we model the CA scene as a decomposition of low-rank and sparse components, utilizing fast tensorial fields for low-rank static reconstruction and neural fields for dynamic sparse reconstruction. Our approach outperforms previous works in both training time and reconstruction accuracy, yielding reasonable reconstructions from as few as three angiogram views. We validate our approach quantitatively and qualitatively on representative 4D phantom datasets.", "AI": {"tldr": "NerT-CA\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u795e\u7ecf\u548c\u5f20\u91cf\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u89c6\u89d2X\u5c04\u7ebf\u51a0\u72b6\u52a8\u8109\u9020\u5f71\u4e2d\u52a0\u901f4D\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u8bad\u7ec3\u65f6\u95f4\u957f\u548c\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u4eceX\u5c04\u7ebf\u51a0\u72b6\u52a8\u8109\u9020\u5f71\u4e2d\u5b9e\u73b03D\u548c4D\u91cd\u5efa\u5177\u6709\u4e34\u5e8a\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8017\u65f6\u7684\u624b\u52a8\u5206\u5272\u6216\u6613\u51fa\u9519\u7684\u81ea\u52a8\u5206\u5272\uff0c\u4e14\u7a00\u758f\u89c6\u89d2\u548c\u8fd0\u52a8\u95ee\u9898\u589e\u52a0\u4e86\u96be\u5ea6\u3002", "method": "NerT-CA\u7ed3\u5408\u4e86\u795e\u7ecf\u573a\u548c\u5f20\u91cf\u573a\uff0c\u5c06\u573a\u666f\u5206\u89e3\u4e3a\u4f4e\u79e9\u9759\u6001\u90e8\u5206\uff08\u5f20\u91cf\u573a\uff09\u548c\u52a8\u6001\u7a00\u758f\u90e8\u5206\uff08\u795e\u7ecf\u573a\uff09\uff0c\u4ece\u800c\u52a0\u901f\u91cd\u5efa\u5e76\u63d0\u9ad8\u7cbe\u5ea6\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u65f6\u95f4\u548c\u91cd\u5efa\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4ec5\u9700\u4e09\u4e2a\u89c6\u89d2\u5373\u53ef\u83b7\u5f97\u5408\u7406\u91cd\u5efa\u3002", "conclusion": "NerT-CA\u4e3a\u7a00\u758f\u89c6\u89d2\u51a0\u72b6\u52a8\u8109\u9020\u5f71\u76844D\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19359", "pdf": "https://arxiv.org/pdf/2507.19359", "abs": "https://arxiv.org/abs/2507.19359", "authors": ["Lanmiao Liu", "Esam Ghaleb", "Asl\u0131 \u00d6zy\u00fcrek", "Zerrin Yumak"], "title": "SemGes: Semantics-aware Co-Speech Gesture Generation using Semantic Coherence and Relevance Learning", "categories": ["cs.CV"], "comment": "Accepted to IEEE/CVF International Conference on Computer Vision   (ICCV) 2025", "summary": "Creating a virtual avatar with semantically coherent gestures that are aligned with speech is a challenging task. Existing gesture generation research mainly focused on generating rhythmic beat gestures, neglecting the semantic context of the gestures. In this paper, we propose a novel approach for semantic grounding in co-speech gesture generation that integrates semantic information at both fine-grained and global levels. Our approach starts with learning the motion prior through a vector-quantized variational autoencoder. Built on this model, a second-stage module is applied to automatically generate gestures from speech, text-based semantics and speaker identity that ensures consistency between the semantic relevance of generated gestures and co-occurring speech semantics through semantic coherence and relevance modules. Experimental results demonstrate that our approach enhances the realism and coherence of semantic gestures. Extensive experiments and user studies show that our method outperforms state-of-the-art approaches across two benchmarks in co-speech gesture generation in both objective and subjective metrics. The qualitative results of our model, code, dataset and pre-trained models can be viewed at https://semgesture.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7ec6\u7c92\u5ea6\u548c\u5168\u5c40\u8bed\u4e49\u4fe1\u606f\u7684\u8bed\u97f3\u624b\u52bf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u6a21\u578b\u63d0\u5347\u624b\u52bf\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u771f\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u624b\u52bf\u751f\u6210\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u8282\u594f\u6027\u624b\u52bf\uff0c\u5ffd\u7565\u4e86\u8bed\u4e49\u4e0a\u4e0b\u6587\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u751f\u6210\u4e0e\u8bed\u97f3\u8bed\u4e49\u4e00\u81f4\u7684\u624b\u52bf\u7684\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u901a\u8fc7\u5411\u91cf\u91cf\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u8fd0\u52a8\u5148\u9a8c\uff0c\u518d\u901a\u8fc7\u7b2c\u4e8c\u9636\u6bb5\u6a21\u5757\u7ed3\u5408\u8bed\u97f3\u3001\u6587\u672c\u8bed\u4e49\u548c\u8bf4\u8bdd\u8005\u8eab\u4efd\u751f\u6210\u624b\u52bf\uff0c\u786e\u4fdd\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5ba2\u89c2\u548c\u4e3b\u89c2\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u624b\u52bf\u7684\u771f\u5b9e\u6027\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8bed\u97f3\u624b\u52bf\u751f\u6210\u4e2d\u5b9e\u73b0\u4e86\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2507.19451", "pdf": "https://arxiv.org/pdf/2507.19451", "abs": "https://arxiv.org/abs/2507.19451", "authors": ["Baijun Ye", "Minghui Qin", "Saining Zhang", "Moonjun Gong", "Shaoting Zhu", "Zebang Shen", "Luan Zhang", "Lu Zhang", "Hao Zhao", "Hang Zhao"], "title": "GS-Occ3D: Scaling Vision-only Occupancy Reconstruction for Autonomous Driving with Gaussian Splatting", "categories": ["cs.CV"], "comment": "ICCV 2025. Project Page: https://gs-occ3d.github.io/", "summary": "Occupancy is crucial for autonomous driving, providing essential geometric priors for perception and planning. However, existing methods predominantly rely on LiDAR-based occupancy annotations, which limits scalability and prevents leveraging vast amounts of potential crowdsourced data for auto-labeling. To address this, we propose GS-Occ3D, a scalable vision-only framework that directly reconstructs occupancy. Vision-only occupancy reconstruction poses significant challenges due to sparse viewpoints, dynamic scene elements, severe occlusions, and long-horizon motion. Existing vision-based methods primarily rely on mesh representation, which suffer from incomplete geometry and additional post-processing, limiting scalability. To overcome these issues, GS-Occ3D optimizes an explicit occupancy representation using an Octree-based Gaussian Surfel formulation, ensuring efficiency and scalability. Additionally, we decompose scenes into static background, ground, and dynamic objects, enabling tailored modeling strategies: (1) Ground is explicitly reconstructed as a dominant structural element, significantly improving large-area consistency; (2) Dynamic vehicles are separately modeled to better capture motion-related occupancy patterns. Extensive experiments on the Waymo dataset demonstrate that GS-Occ3D achieves state-of-the-art geometry reconstruction results. By curating vision-only binary occupancy labels from diverse urban scenes, we show their effectiveness for downstream occupancy models on Occ3D-Waymo and superior zero-shot generalization on Occ3D-nuScenes. It highlights the potential of large-scale vision-based occupancy reconstruction as a new paradigm for autonomous driving perception. Project Page: https://gs-occ3d.github.io/", "AI": {"tldr": "GS-Occ3D\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u7684\u5360\u7528\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7Octree-based Gaussian Surfel\u4f18\u5316\u663e\u5f0f\u5360\u7528\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u573a\u666f\u548c\u906e\u6321\u95ee\u9898\uff0c\u5e76\u5728Waymo\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51e0\u4f55\u91cd\u5efa\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56LiDAR\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u5229\u7528\u4f17\u5305\u6570\u636e\u7684\u80fd\u529b\u3002\u89c6\u89c9\u5360\u7528\u91cd\u5efa\u9762\u4e34\u7a00\u758f\u89c6\u89d2\u3001\u52a8\u6001\u5143\u7d20\u548c\u906e\u6321\u7b49\u6311\u6218\u3002", "method": "\u91c7\u7528Octree-based Gaussian Surfel\u4f18\u5316\u663e\u5f0f\u5360\u7528\u8868\u793a\uff0c\u5c06\u573a\u666f\u5206\u89e3\u4e3a\u9759\u6001\u80cc\u666f\u3001\u5730\u9762\u548c\u52a8\u6001\u7269\u4f53\uff0c\u5206\u522b\u5efa\u6a21\u3002", "result": "\u5728Waymo\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u751f\u6210\u7684\u89c6\u89c9\u5360\u7528\u6807\u7b7e\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u6709\u6548\uff0c\u5e76\u5728Occ3D-nuScenes\u4e0a\u5c55\u793a\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GS-Occ3D\u5c55\u793a\u4e86\u5927\u89c4\u6a21\u89c6\u89c9\u5360\u7528\u91cd\u5efa\u5728\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.19459", "pdf": "https://arxiv.org/pdf/2507.19459", "abs": "https://arxiv.org/abs/2507.19459", "authors": ["Pol Francesch Huc", "Emily Bates", "Simone D'Amico"], "title": "Fast Learning of Non-Cooperative Spacecraft 3D Models through Primitive Initialization", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "The advent of novel view synthesis techniques such as NeRF and 3D Gaussian Splatting (3DGS) has enabled learning precise 3D models only from posed monocular images. Although these methods are attractive, they hold two major limitations that prevent their use in space applications: they require poses during training, and have high computational cost at training and inference. To address these limitations, this work contributes: (1) a Convolutional Neural Network (CNN) based primitive initializer for 3DGS using monocular images; (2) a pipeline capable of training with noisy or implicit pose estimates; and (3) and analysis of initialization variants that reduce the training cost of precise 3D models. A CNN takes a single image as input and outputs a coarse 3D model represented as an assembly of primitives, along with the target's pose relative to the camera. This assembly of primitives is then used to initialize 3DGS, significantly reducing the number of training iterations and input images needed -- by at least an order of magnitude. For additional flexibility, the CNN component has multiple variants with different pose estimation techniques. This work performs a comparison between these variants, evaluating their effectiveness for downstream 3DGS training under noisy or implicit pose estimates. The results demonstrate that even with imperfect pose supervision, the pipeline is able to learn high-fidelity 3D representations, opening the door for the use of novel view synthesis in space applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCNN\u76843D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u7cbe\u786e\u59ff\u6001\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u7a7a\u95f4\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5982NeRF\u548c3DGS\u9700\u8981\u7cbe\u786e\u7684\u59ff\u6001\u4f30\u8ba1\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5728\u7a7a\u95f4\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002", "method": "\u4f7f\u7528CNN\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u7c97\u75653D\u6a21\u578b\u548c\u59ff\u6001\u4f30\u8ba1\uff0c\u7528\u4e8e\u521d\u59cb\u53163DGS\uff0c\u51cf\u5c11\u8bad\u7ec3\u8fed\u4ee3\u548c\u8f93\u5165\u56fe\u50cf\u9700\u6c42\u3002", "result": "\u5373\u4f7f\u5728\u4e0d\u5b8c\u7f8e\u7684\u59ff\u6001\u76d1\u7763\u4e0b\uff0c\u4e5f\u80fd\u5b66\u4e60\u9ad8\u4fdd\u771f3D\u8868\u793a\uff0c\u8bad\u7ec3\u6210\u672c\u548c\u8f93\u5165\u9700\u6c42\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7a7a\u95f4\u5e94\u7528\u4e2d\u7684\u65b0\u89c6\u89d2\u5408\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19468", "pdf": "https://arxiv.org/pdf/2507.19468", "abs": "https://arxiv.org/abs/2507.19468", "authors": ["Federico Baldassarre", "Marc Szafraniec", "Basile Terver", "Vasil Khalidov", "Francisco Massa", "Yann LeCun", "Patrick Labatut", "Maximilian Seitzer", "Piotr Bojanowski"], "title": "Back to the Features: DINO as a Foundation for Video World Models", "categories": ["cs.CV"], "comment": null, "summary": "We present DINO-world, a powerful generalist video world model trained to predict future frames in the latent space of DINOv2. By leveraging a pre-trained image encoder and training a future predictor on a large-scale uncurated video dataset, DINO-world learns the temporal dynamics of diverse scenes, from driving and indoor scenes to simulated environments. We show that DINO-world outperforms previous models on a variety of video prediction benchmarks, e.g. segmentation and depth forecasting, and demonstrates strong understanding of intuitive physics. Furthermore, we show that it is possible to fine-tune the predictor on observation-action trajectories. The resulting action-conditioned world model can be used for planning by simulating candidate trajectories in latent space.", "AI": {"tldr": "DINO-world\u662f\u4e00\u4e2a\u57fa\u4e8eDINOv2\u6f5c\u5728\u7a7a\u95f4\u7684\u901a\u7528\u89c6\u9891\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u56fe\u50cf\u7f16\u7801\u5668\u548c\u672a\u6765\u9884\u6d4b\u5668\u5728\u5927\u89c4\u6a21\u65e0\u6807\u6ce8\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5b66\u4e60\u591a\u6837\u573a\u666f\u7684\u65f6\u95f4\u52a8\u6001\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u9884\u8bad\u7ec3\u56fe\u50cf\u7f16\u7801\u5668\u548c\u672a\u6765\u9884\u6d4b\u5668\uff0c\u5b66\u4e60\u591a\u6837\u573a\u666f\u7684\u65f6\u95f4\u52a8\u6001\uff0c\u63d0\u5347\u89c6\u9891\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u5229\u7528DINOv2\u9884\u8bad\u7ec3\u56fe\u50cf\u7f16\u7801\u5668\uff0c\u5728\u5927\u89c4\u6a21\u65e0\u6807\u6ce8\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u672a\u6765\u9884\u6d4b\u5668\uff0c\u5b66\u4e60\u65f6\u95f4\u52a8\u6001\u3002", "result": "\u5728\u591a\u79cd\u89c6\u9891\u9884\u6d4b\u4efb\u52a1\uff08\u5982\u5206\u5272\u548c\u6df1\u5ea6\u9884\u6d4b\uff09\u4e0a\u4f18\u4e8e\u5148\u524d\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u5bf9\u76f4\u89c2\u7269\u7406\u7684\u6df1\u523b\u7406\u89e3\u3002", "conclusion": "DINO-world\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u901a\u7528\u89c6\u9891\u4e16\u754c\u6a21\u578b\uff0c\u53ef\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u6a21\u62df\u5019\u9009\u8f68\u8ff9\u7528\u4e8e\u89c4\u5212\u3002"}}
{"id": "2507.19474", "pdf": "https://arxiv.org/pdf/2507.19474", "abs": "https://arxiv.org/abs/2507.19474", "authors": ["Ziren Gong", "Xiaohan Li", "Fabio Tosi", "Youmin Zhang", "Stefano Mattoccia", "Jun Wu", "Matteo Poggi"], "title": "DINO-SLAM: DINO-informed RGB-D SLAM for Neural Implicit and Explicit Representations", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents DINO-SLAM, a DINO-informed design strategy to enhance neural implicit (Neural Radiance Field -- NeRF) and explicit representations (3D Gaussian Splatting -- 3DGS) in SLAM systems through more comprehensive scene representations. Purposely, we rely on a Scene Structure Encoder (SSE) that enriches DINO features into Enhanced DINO ones (EDINO) to capture hierarchical scene elements and their structural relationships. Building upon it, we propose two foundational paradigms for NeRF and 3DGS SLAM systems integrating EDINO features. Our DINO-informed pipelines achieve superior performance on the Replica, ScanNet, and TUM compared to state-of-the-art methods.", "AI": {"tldr": "DINO-SLAM\u901a\u8fc7DINO\u7279\u5f81\u589e\u5f3aNeRF\u548c3DGS\u5728SLAM\u7cfb\u7edf\u4e2d\u7684\u573a\u666f\u8868\u793a\uff0c\u63d0\u51fa\u4e24\u79cd\u65b0\u8303\u5f0f\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63d0\u5347SLAM\u7cfb\u7edf\u4e2d\u795e\u7ecf\u9690\u5f0f\u548c\u663e\u5f0f\u573a\u666f\u8868\u793a\u7684\u5168\u9762\u6027\u3002", "method": "\u4f7f\u7528Scene Structure Encoder (SSE)\u5c06DINO\u7279\u5f81\u589e\u5f3a\u4e3aEDINO\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e24\u79cdNeRF\u548c3DGS SLAM\u8303\u5f0f\u3002", "result": "\u5728Replica\u3001ScanNet\u548cTUM\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DINO-SLAM\u901a\u8fc7\u589e\u5f3a\u7279\u5f81\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86SLAM\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2507.18640", "pdf": "https://arxiv.org/pdf/2507.18640", "abs": "https://arxiv.org/abs/2507.18640", "authors": ["Thomas Roca", "Anthony Cintron Roman", "Jeh\u00fa Torres Vega", "Marcelo Duarte", "Pengce Wang", "Kevin White", "Amit Misra", "Juan Lavista Ferres"], "title": "How good are humans at detecting AI-generated images? Learnings from an experiment", "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": null, "summary": "As AI-powered image generation improves, a key question is how well human beings can differentiate between \"real\" and AI-generated or modified images. Using data collected from the online game \"Real or Not Quiz.\", this study investigates how effectively people can distinguish AI-generated images from real ones. Participants viewed a randomized set of real and AI-generated images, aiming to identify their authenticity. Analysis of approximately 287,000 image evaluations by over 12,500 global participants revealed an overall success rate of only 62\\%, indicating a modest ability, slightly above chance. Participants were most accurate with human portraits but struggled significantly with natural and urban landscapes. These results highlight the inherent challenge humans face in distinguishing AI-generated visual content, particularly images without obvious artifacts or stylistic cues. This study stresses the need for transparency tools, such as watermarks and robust AI detection tools to mitigate the risks of misinformation arising from AI-generated content", "AI": {"tldr": "\u4eba\u7c7b\u533a\u5206\u771f\u5b9e\u4e0eAI\u751f\u6210\u56fe\u50cf\u7684\u80fd\u529b\u6709\u9650\uff0c\u6574\u4f53\u6210\u529f\u7387\u4ec562%\uff0c\u5c24\u5176\u5728\u81ea\u7136\u548c\u57ce\u5e02\u666f\u89c2\u4e2d\u8868\u73b0\u8f83\u5dee\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u662f\u5426\u80fd\u6709\u6548\u533a\u5206\u771f\u5b9e\u4e0eAI\u751f\u6210\u56fe\u50cf\uff0c\u4ee5\u5e94\u5bf9AI\u751f\u6210\u5185\u5bb9\u53ef\u80fd\u5e26\u6765\u7684\u4fe1\u606f\u8bef\u5bfc\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u6e38\u620f\u6536\u96c6\u6570\u636e\uff0c\u53c2\u4e0e\u8005\u5bf9\u968f\u673a\u5c55\u793a\u7684\u771f\u5b9e\u4e0eAI\u751f\u6210\u56fe\u50cf\u8fdb\u884c\u771f\u5b9e\u6027\u5224\u65ad\u3002", "result": "\u6574\u4f53\u6210\u529f\u738762%\uff0c\u4eba\u50cf\u8bc6\u522b\u8f83\u51c6\uff0c\u4f46\u81ea\u7136\u548c\u57ce\u5e02\u666f\u89c2\u8bc6\u522b\u56f0\u96be\u3002", "conclusion": "\u4eba\u7c7b\u5728\u533a\u5206AI\u751f\u6210\u56fe\u50cf\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u9700\u900f\u660e\u5de5\u5177\uff08\u5982\u6c34\u5370\u548c\u68c0\u6d4b\u5de5\u5177\uff09\u4ee5\u51cf\u5c11\u8bef\u5bfc\u98ce\u9669\u3002"}}
{"id": "2507.18654", "pdf": "https://arxiv.org/pdf/2507.18654", "abs": "https://arxiv.org/abs/2507.18654", "authors": ["Saeed Mohseni-Sehdeh", "Walid Saad", "Kei Sakaguchi", "Tao Yu"], "title": "Diffusion Models for Solving Inverse Problems via Posterior Sampling with Piecewise Guidance", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Diffusion models are powerful tools for sampling from high-dimensional distributions by progressively transforming pure noise into structured data through a denoising process. When equipped with a guidance mechanism, these models can also generate samples from conditional distributions. In this paper, a novel diffusion-based framework is introduced for solving inverse problems using a piecewise guidance scheme. The guidance term is defined as a piecewise function of the diffusion timestep, facilitating the use of different approximations during high-noise and low-noise phases. This design is shown to effectively balance computational efficiency with the accuracy of the guidance term. Unlike task-specific approaches that require retraining for each problem, the proposed method is problem-agnostic and readily adaptable to a variety of inverse problems. Additionally, it explicitly incorporates measurement noise into the reconstruction process. The effectiveness of the proposed framework is demonstrated through extensive experiments on image restoration tasks, specifically image inpainting and super-resolution. Using a class conditional diffusion model for recovery, compared to the \\pgdm baseline, the proposed framework achieves a reduction in inference time of \\(25\\%\\) for inpainting with both random and center masks, and \\(23\\%\\) and \\(24\\%\\) for \\(4\\times\\) and \\(8\\times\\) super-resolution tasks, respectively, while incurring only negligible loss in PSNR and SSIM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5206\u6bb5\u5f15\u5bfc\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u9006\u95ee\u9898\uff0c\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u9ad8\u7ef4\u5206\u5e03\u91c7\u6837\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u89e3\u51b3\u9006\u95ee\u9898\u65f6\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4e0e\u7cbe\u5ea6\u96be\u4ee5\u5e73\u8861\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u5206\u6bb5\u5f15\u5bfc\u673a\u5236\uff0c\u6839\u636e\u6269\u6563\u65f6\u95f4\u6b65\u957f\u5b9a\u4e49\u5f15\u5bfc\u9879\uff0c\u533a\u5206\u9ad8\u566a\u58f0\u548c\u4f4e\u566a\u58f0\u9636\u6bb5\u7684\u4e0d\u540c\u8fd1\u4f3c\u65b9\u6cd5\u3002", "result": "\u5728\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u4e2d\uff0c\u63a8\u7406\u65f6\u95f4\u663e\u8457\u51cf\u5c11\uff0825%\u81f324%\uff09\uff0c\u4e14PSNR\u548cSSIM\u635f\u5931\u53ef\u5ffd\u7565\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u591a\u79cd\u9006\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9002\u5e94\u6027\uff0c\u540c\u65f6\u663e\u5f0f\u7eb3\u5165\u6d4b\u91cf\u566a\u58f0\uff0c\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2507.19138", "pdf": "https://arxiv.org/pdf/2507.19138", "abs": "https://arxiv.org/abs/2507.19138", "authors": ["Weisong Zhao", "Jingkai Zhou", "Xiangyu Zhu", "Weihua Chen", "Xiao-Yu Zhang", "Zhen Lei", "Fan Wang"], "title": "RealisVSR: Detail-enhanced Diffusion for Real-World 4K Video Super-Resolution", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Video Super-Resolution (VSR) has achieved significant progress through diffusion models, effectively addressing the over-smoothing issues inherent in GAN-based methods. Despite recent advances, three critical challenges persist in VSR community: 1) Inconsistent modeling of temporal dynamics in foundational models; 2) limited high-frequency detail recovery under complex real-world degradations; and 3) insufficient evaluation of detail enhancement and 4K super-resolution, as current methods primarily rely on 720P datasets with inadequate details. To address these challenges, we propose RealisVSR, a high-frequency detail-enhanced video diffusion model with three core innovations: 1) Consistency Preserved ControlNet (CPC) architecture integrated with the Wan2.1 video diffusion to model the smooth and complex motions and suppress artifacts; 2) High-Frequency Rectified Diffusion Loss (HR-Loss) combining wavelet decomposition and HOG feature constraints for texture restoration; 3) RealisVideo-4K, the first public 4K VSR benchmark containing 1,000 high-definition video-text pairs. Leveraging the advanced spatio-temporal guidance of Wan2.1, our method requires only 5-25% of the training data volume compared to existing approaches. Extensive experiments on VSR benchmarks (REDS, SPMCS, UDM10, YouTube-HQ, VideoLQ, RealisVideo-720P) demonstrate our superiority, particularly in ultra-high-resolution scenarios.", "AI": {"tldr": "RealisVSR\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9ad8\u9891\u7ec6\u8282\u589e\u5f3a\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u95f4\u52a8\u6001\u5efa\u6a21\u3001\u9ad8\u9891\u7ec6\u8282\u6062\u590d\u548c4K\u8d85\u5206\u8fa8\u7387\u8bc4\u4f30\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u8d85\u5206\u8fa8\u7387\uff08VSR\uff09\u65b9\u6cd5\u5728\u65f6\u95f4\u52a8\u6001\u5efa\u6a21\u3001\u9ad8\u9891\u7ec6\u8282\u6062\u590d\u548c4K\u8d85\u5206\u8fa8\u7387\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cRealisVSR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "1\uff09\u63d0\u51faConsistency Preserved ControlNet\uff08CPC\uff09\u67b6\u6784\uff1b2\uff09\u5f15\u5165High-Frequency Rectified Diffusion Loss\uff08HR-Loss\uff09\uff1b3\uff09\u6784\u5efa\u9996\u4e2a\u516c\u5f00\u76844K VSR\u57fa\u51c6\u6570\u636e\u96c6RealisVideo-4K\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRealisVSR\u5728\u591a\u4e2aVSR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u8d85\u9ad8\u5206\u8fa8\u7387\u573a\u666f\u4e0b\uff0c\u4e14\u8bad\u7ec3\u6570\u636e\u91cf\u4ec5\u4e3a\u73b0\u6709\u65b9\u6cd5\u76845-25%\u3002", "conclusion": "RealisVSR\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u548c\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u7684\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u9ad8\u9891\u7ec6\u8282\u548c4K\u5206\u8fa8\u7387\u573a\u666f\u4e0b\u3002"}}
{"id": "2507.19225", "pdf": "https://arxiv.org/pdf/2507.19225", "abs": "https://arxiv.org/abs/2507.19225", "authors": ["Fang Kang", "Yin Cao", "Haoyu Chen"], "title": "Face2VoiceSync: Lightweight Face-Voice Consistency for Text-Driven Talking Face Generation", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "comment": null, "summary": "Recent studies in speech-driven talking face generation achieve promising results, but their reliance on fixed-driven speech limits further applications (e.g., face-voice mismatch). Thus, we extend the task to a more challenging setting: given a face image and text to speak, generating both talking face animation and its corresponding speeches. Accordingly, we propose a novel framework, Face2VoiceSync, with several novel contributions: 1) Voice-Face Alignment, ensuring generated voices match facial appearance; 2) Diversity \\& Manipulation, enabling generated voice control over paralinguistic features space; 3) Efficient Training, using a lightweight VAE to bridge visual and audio large-pretrained models, with significantly fewer trainable parameters than existing methods; 4) New Evaluation Metric, fairly assessing the diversity and identity consistency. Experiments show Face2VoiceSync achieves both visual and audio state-of-the-art performances on a single 40GB GPU.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFace2VoiceSync\u6846\u67b6\uff0c\u89e3\u51b3\u8bed\u97f3\u9a71\u52a8\u9762\u90e8\u751f\u6210\u4e2d\u7684\u56fa\u5b9a\u8bed\u97f3\u9650\u5236\u95ee\u9898\uff0c\u652f\u6301\u4ece\u6587\u672c\u548c\u9762\u90e8\u56fe\u50cf\u751f\u6210\u540c\u6b65\u7684\u8bed\u97f3\u548c\u9762\u90e8\u52a8\u753b\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u9a71\u52a8\u9762\u90e8\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u8bed\u97f3\uff0c\u5bfc\u81f4\u5e94\u7528\u53d7\u9650\uff08\u5982\u9762\u90e8\u4e0e\u8bed\u97f3\u4e0d\u5339\u914d\uff09\uff0c\u56e0\u6b64\u6269\u5c55\u4efb\u52a1\u81f3\u66f4\u5177\u6311\u6218\u6027\u7684\u8bbe\u7f6e\u3002", "method": "\u63d0\u51faFace2VoiceSync\u6846\u67b6\uff0c\u5305\u62ec\u8bed\u97f3-\u9762\u90e8\u5bf9\u9f50\u3001\u591a\u6837\u6027\u63a7\u5236\u3001\u9ad8\u6548\u8bad\u7ec3\uff08\u8f7b\u91cfVAE\u6865\u63a5\u89c6\u89c9\u4e0e\u97f3\u9891\u9884\u8bad\u7ec3\u6a21\u578b\uff09\u548c\u65b0\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u663e\u793aFace2VoiceSync\u5728\u535540GB GPU\u4e0a\u5b9e\u73b0\u89c6\u89c9\u548c\u97f3\u9891\u7684\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "Face2VoiceSync\u6210\u529f\u89e3\u51b3\u4e86\u8bed\u97f3\u4e0e\u9762\u90e8\u540c\u6b65\u751f\u6210\u7684\u6311\u6218\uff0c\u5e76\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
