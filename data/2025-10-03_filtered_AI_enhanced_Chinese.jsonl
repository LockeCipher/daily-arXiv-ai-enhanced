{"id": "2510.01619", "pdf": "https://arxiv.org/pdf/2510.01619", "abs": "https://arxiv.org/abs/2510.01619", "authors": ["Changmin Lee", "Jihyun Lee", "Tae-Kyun Kim"], "title": "MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust Physics-Based Dynamics", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted to NeurIPS 2025", "summary": "While there has been significant progress in the field of 3D avatar creation from visual observations, modeling physically plausible dynamics of humans with loose garments remains a challenging problem. Although a few existing works address this problem by leveraging physical simulation, they suffer from limited accuracy or robustness to novel animation inputs. In this work, we present MPMAvatar, a framework for creating 3D human avatars from multi-view videos that supports highly realistic, robust animation, as well as photorealistic rendering from free viewpoints. For accurate and robust dynamics modeling, our key idea is to use a Material Point Method-based simulator, which we carefully tailor to model garments with complex deformations and contact with the underlying body by incorporating an anisotropic constitutive model and a novel collision handling algorithm. We combine this dynamics modeling scheme with our canonical avatar that can be rendered using 3D Gaussian Splatting with quasi-shadowing, enabling high-fidelity rendering for physically realistic animations. In our experiments, we demonstrate that MPMAvatar significantly outperforms the existing state-of-the-art physics-based avatar in terms of (1) dynamics modeling accuracy, (2) rendering accuracy, and (3) robustness and efficiency. Additionally, we present a novel application in which our avatar generalizes to unseen interactions in a zero-shot manner-which was not achievable with previous learning-based methods due to their limited simulation generalizability. Our project page is at: https://KAISTChangmin.github.io/MPMAvatar/", "AI": {"tldr": "MPMAvatar\u662f\u4e00\u4e2a\u4ece\u591a\u89c6\u89d2\u89c6\u9891\u521b\u5efa3D\u4eba\u4f53\u5316\u8eab\u7684\u6846\u67b6\uff0c\u652f\u6301\u9ad8\u5ea6\u903c\u771f\u7684\u52a8\u753b\u548c\u81ea\u7531\u89c6\u89d2\u7684\u5149\u7ebf\u771f\u5b9e\u6e32\u67d3\u3002\u8be5\u6846\u67b6\u4f7f\u7528\u57fa\u4e8e\u7269\u8d28\u70b9\u6cd5\u7684\u6a21\u62df\u5668\u6765\u51c6\u786e\u5efa\u6a21\u670d\u88c5\u7684\u590d\u6742\u53d8\u5f62\u548c\u4e0e\u8eab\u4f53\u7684\u63a5\u89e6\uff0c\u5e76\u7ed3\u54083D\u9ad8\u65af\u6e85\u5c04\u6e32\u67d3\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7269\u7406\u6a21\u62df\u76843D\u5316\u8eab\u521b\u5efa\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u5bf9\u65b0\u52a8\u753b\u8f93\u5165\u7684\u9c81\u68d2\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u7279\u522b\u662f\u5728\u5efa\u6a21\u677e\u6563\u670d\u88c5\u7684\u7269\u7406\u52a8\u529b\u5b66\u65b9\u9762\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u5b9a\u5236\u7684\u7269\u8d28\u70b9\u6cd5\u6a21\u62df\u5668\uff0c\u7ed3\u5408\u5404\u5411\u5f02\u6027\u672c\u6784\u6a21\u578b\u548c\u65b0\u578b\u78b0\u649e\u5904\u7406\u7b97\u6cd5\u6765\u5efa\u6a21\u670d\u88c5\u590d\u6742\u53d8\u5f62\uff1b\u7ed3\u5408\u4f7f\u75283D\u9ad8\u65af\u6e85\u5c04\u6e32\u67d3\u7684\u89c4\u8303\u5316\u8eab\u3002", "result": "MPMAvatar\u5728\u52a8\u529b\u5b66\u5efa\u6a21\u51c6\u786e\u6027\u3001\u6e32\u67d3\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u7269\u7406\u7684\u5316\u8eab\u65b9\u6cd5\uff0c\u5e76\u80fd\u4ee5\u96f6\u6837\u672c\u65b9\u5f0f\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u4ea4\u4e92\u573a\u666f\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5efa\u6a21\u677e\u6563\u670d\u88c5\u7269\u7406\u52a8\u529b\u5b66\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u5ea6\u903c\u771f\u548c\u9c81\u68d2\u7684\u52a8\u753b\uff0c\u5e76\u4e3a\u672a\u89c1\u4ea4\u4e92\u63d0\u4f9b\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u8fd9\u5728\u4e4b\u524d\u7684\u5b66\u4e60\u65b9\u6cd5\u4e2d\u662f\u4e0d\u53ef\u5b9e\u73b0\u7684\u3002"}}
{"id": "2510.01978", "pdf": "https://arxiv.org/pdf/2510.01978", "abs": "https://arxiv.org/abs/2510.01978", "authors": ["Quoc-Anh Bui", "Gilles Rougeron", "G\u00e9raldine Morin", "Simone Gasparini"], "title": "ROI-GS: Interest-based Local Quality 3D Gaussian Splatting", "categories": ["cs.GR", "cs.CV", "68U05, 68T45 (Primary) 68T07, 68-04 (Secondary)", "I.2.10; I.3.3; I.3.5; I.3.7; I.4.5; I.4.6; I.4.8; I.4.10"], "comment": "4 pages, 3 figures, 2 tables", "summary": "We tackle the challenge of efficiently reconstructing 3D scenes with high detail on objects of interest. Existing 3D Gaussian Splatting (3DGS) methods allocate resources uniformly across the scene, limiting fine detail to Regions Of Interest (ROIs) and leading to inflated model size. We propose ROI-GS, an object-aware framework that enhances local details through object-guided camera selection, targeted Object training, and seamless integration of high-fidelity object of interest reconstructions into the global scene. Our method prioritizes higher resolution details on chosen objects while maintaining real-time performance. Experiments show that ROI-GS significantly improves local quality (up to 2.96 dB PSNR), while reducing overall model size by $\\approx 17\\%$ of baseline and achieving faster training for a scene with a single object of interest, outperforming existing methods.", "AI": {"tldr": "ROI-GS\u662f\u4e00\u4e2a\u5bf9\u8c61\u611f\u77e5\u76843D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u8c61\u5f15\u5bfc\u7684\u76f8\u673a\u9009\u62e9\u3001\u9488\u5bf9\u6027\u5bf9\u8c61\u8bad\u7ec3\u548c\u9ad8\u4fdd\u771f\u5bf9\u8c61\u91cd\u5efa\u7684\u96c6\u6210\uff0c\u5728\u611f\u5174\u8da3\u533a\u57df\u5b9e\u73b0\u66f4\u9ad8\u7ec6\u8282\u76843D\u573a\u666f\u91cd\u5efa\uff0c\u540c\u65f6\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u5e76\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u73b0\u67093D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u5728\u573a\u666f\u4e2d\u5747\u5300\u5206\u914d\u8d44\u6e90\uff0c\u9650\u5236\u4e86\u611f\u5174\u8da3\u533a\u57df\u7684\u7cbe\u7ec6\u7ec6\u8282\uff0c\u5bfc\u81f4\u6a21\u578b\u5c3a\u5bf8\u81a8\u80c0\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u4f18\u5148\u5904\u7406\u9009\u5b9a\u5bf9\u8c61\u7684\u9ad8\u5206\u8fa8\u7387\u7ec6\u8282\u3002", "method": "\u63d0\u51faROI-GS\u6846\u67b6\uff0c\u5305\u62ec\u5bf9\u8c61\u5f15\u5bfc\u7684\u76f8\u673a\u9009\u62e9\u3001\u9488\u5bf9\u6027\u5bf9\u8c61\u8bad\u7ec3\uff0c\u4ee5\u53ca\u5c06\u9ad8\u4fdd\u771f\u5bf9\u8c61\u91cd\u5efa\u65e0\u7f1d\u96c6\u6210\u5230\u5168\u5c40\u573a\u666f\u4e2d\u3002", "result": "\u5b9e\u9a8c\u663e\u793aROI-GS\u663e\u8457\u63d0\u5347\u5c40\u90e8\u8d28\u91cf\uff08PSNR\u6700\u9ad8\u63d0\u53472.96 dB\uff09\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c11\u7ea617%\uff0c\u5355\u5bf9\u8c61\u573a\u666f\u8bad\u7ec3\u66f4\u5feb\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ROI-GS\u80fd\u591f\u6709\u6548\u63d0\u5347\u611f\u5174\u8da3\u5bf9\u8c61\u7684\u7ec6\u8282\u91cd\u5efa\u8d28\u91cf\uff0c\u540c\u65f6\u51cf\u5c11\u6574\u4f53\u6a21\u578b\u5927\u5c0f\u5e76\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edf\u5747\u5300\u8d44\u6e90\u5206\u914d\u65b9\u6cd5\u3002"}}
{"id": "2510.01339", "pdf": "https://arxiv.org/pdf/2510.01339", "abs": "https://arxiv.org/abs/2510.01339", "authors": ["Alessio Spagnoletti", "Andr\u00e9s Almansa", "Marcelo Pereyra"], "title": "LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration", "categories": ["cs.CV", "stat.ML"], "comment": "23 pages, 12 figures", "summary": "Computational imaging methods increasingly rely on powerful generative diffusion models to tackle challenging image restoration tasks. In particular, state-of-the-art zero-shot image inverse solvers leverage distilled text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy and perceptual quality with high computational efficiency. However, extending these advances to high-definition video restoration remains a significant challenge, due to the need to recover fine spatial detail while capturing subtle temporal dependencies. Consequently, methods that naively apply image-based LDM priors on a frame-by-frame basis often result in temporally inconsistent reconstructions. We address this challenge by leveraging recent advances in Video Consistency Models (VCMs), which distill video latent diffusion models into fast generators that explicitly capture temporal causality. Building on this foundation, we propose LVTINO, the first zero-shot or plug-and-play inverse solver for high definition video restoration with priors encoded by VCMs. Our conditioning mechanism bypasses the need for automatic differentiation and achieves state-of-the-art video reconstruction quality with only a few neural function evaluations, while ensuring strong measurement consistency and smooth temporal transitions across frames. Extensive experiments on a diverse set of video inverse problems show significant perceptual improvements over current state-of-the-art methods that apply image LDMs frame by frame, establishing a new benchmark in both reconstruction fidelity and computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86LVTINO\uff0c\u9996\u4e2a\u57fa\u4e8e\u89c6\u9891\u4e00\u81f4\u6027\u6a21\u578b(VCMs)\u7684\u96f6\u6837\u672c\u89c6\u9891\u6062\u590d\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5e27\u7ea7\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5bfc\u81f4\u7684\u65f6\u95f4\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u91cd\u5efa\u8d28\u91cf\u4e0a\u90fd\u8fbe\u5230\u4e86\u65b0\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u7684\u96f6\u6837\u672c\u56fe\u50cf\u9006\u6c42\u89e3\u5668\u867d\u7136\u80fd\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b(LDMs)\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u56fe\u50cf\u6062\u590d\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4e8e\u89c6\u9891\u6062\u590d\u65f6\u4f1a\u5bfc\u81f4\u65f6\u95f4\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u9700\u8981\u540c\u65f6\u6062\u590d\u7cbe\u7ec6\u7a7a\u95f4\u7ec6\u8282\u548c\u6355\u6349\u5fae\u5999\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "method": "\u5229\u7528\u89c6\u9891\u4e00\u81f4\u6027\u6a21\u578b(VCMs)\u5c06\u89c6\u9891\u6f5c\u5728\u6269\u6563\u6a21\u578b\u84b8\u998f\u4e3a\u5feb\u901f\u751f\u6210\u5668\uff0c\u660e\u786e\u6355\u6349\u65f6\u95f4\u56e0\u679c\u5173\u7cfb\u3002\u63d0\u51fa\u7684LVTINO\u65b9\u6cd5\u901a\u8fc7\u6761\u4ef6\u673a\u5236\u7ed5\u8fc7\u81ea\u52a8\u5fae\u5206\u9700\u6c42\uff0c\u4ec5\u9700\u5c11\u91cf\u795e\u7ecf\u7f51\u7edc\u8bc4\u4f30\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u89c6\u9891\u91cd\u5efa\u3002", "result": "\u5728\u591a\u79cd\u89c6\u9891\u9006\u95ee\u9898\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5e27\u7ea7\u56fe\u50cfLDM\u65b9\u6cd5\uff0c\u5728\u611f\u77e5\u8d28\u91cf\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u5728\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u90fd\u5efa\u7acb\u4e86\u65b0\u57fa\u51c6\u3002", "conclusion": "LVTINO\u901a\u8fc7\u89c6\u9891\u4e00\u81f4\u6027\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u65f6\u95f4\u4e00\u81f4\u7684\u89c6\u9891\u6062\u590d\uff0c\u4e3a\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u9006\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u96f6\u6837\u672c\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6d4b\u91cf\u4e00\u81f4\u6027\u548c\u5e73\u6ed1\u65f6\u95f4\u8fc7\u6e21\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2510.02069", "pdf": "https://arxiv.org/pdf/2510.02069", "abs": "https://arxiv.org/abs/2510.02069", "authors": ["Georgios Kouros", "Minye Wu", "Tinne Tuytelaars"], "title": "Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy Objects", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Accurate reconstruction and relighting of glossy objects remain a longstanding challenge, as object shape, material properties, and illumination are inherently difficult to disentangle. Existing neural rendering approaches often rely on simplified BRDF models or parameterizations that couple diffuse and specular components, which restricts faithful material recovery and limits relighting fidelity. We propose a relightable framework that integrates a microfacet BRDF with the specular-glossiness parameterization into 2D Gaussian Splatting with deferred shading. This formulation enables more physically consistent material decomposition, while diffusion-based priors for surface normals and diffuse color guide early-stage optimization and mitigate ambiguity. A coarse-to-fine optimization of the environment map accelerates convergence and preserves high-dynamic-range specular reflections. Extensive experiments on complex, glossy scenes demonstrate that our method achieves high-quality geometry and material reconstruction, delivering substantially more realistic and consistent relighting under novel illumination compared to existing Gaussian splatting methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u91cd\u5149\u7167\u6846\u67b6\uff0c\u5c06\u5fae\u8868\u9762BRDF\u4e0e\u955c\u9762\u5149\u6cfd\u5ea6\u53c2\u6570\u5316\u96c6\u6210\u52302D\u9ad8\u65af\u6cfc\u6e85\u4e2d\uff0c\u5b9e\u73b0\u4e86\u66f4\u7269\u7406\u4e00\u81f4\u7684\u6750\u6599\u5206\u89e3\u548c\u9ad8\u8d28\u91cf\u91cd\u5149\u7167\u3002", "motivation": "\u51c6\u786e\u91cd\u5efa\u548c\u91cd\u5149\u7167\u5149\u6cfd\u7269\u4f53\u662f\u4e00\u4e2a\u957f\u671f\u6311\u6218\uff0c\u73b0\u6709\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u7b80\u5316\u7684BRDF\u6a21\u578b\u6216\u8026\u5408\u6f2b\u53cd\u5c04\u548c\u955c\u9762\u5206\u91cf\u7684\u53c2\u6570\u5316\uff0c\u9650\u5236\u4e86\u6750\u6599\u6062\u590d\u7684\u51c6\u786e\u6027\u548c\u91cd\u5149\u7167\u4fdd\u771f\u5ea6\u3002", "method": "\u5c06\u5fae\u8868\u9762BRDF\u4e0e\u955c\u9762\u5149\u6cfd\u5ea6\u53c2\u6570\u5316\u96c6\u6210\u52302D\u9ad8\u65af\u6cfc\u6e85\u4e2d\uff0c\u91c7\u7528\u5ef6\u8fdf\u7740\u8272\uff1b\u4f7f\u7528\u57fa\u4e8e\u6269\u6563\u7684\u8868\u9762\u6cd5\u7ebf\u548c\u6f2b\u53cd\u5c04\u989c\u8272\u5148\u9a8c\u6307\u5bfc\u65e9\u671f\u4f18\u5316\uff1b\u91c7\u7528\u4ece\u7c97\u5230\u7cbe\u7684\u73af\u5883\u56fe\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728\u590d\u6742\u5149\u6cfd\u573a\u666f\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u51e0\u4f55\u548c\u6750\u6599\u91cd\u5efa\uff0c\u4e0e\u73b0\u6709\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u65b0\u5149\u7167\u4e0b\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u548c\u4e00\u81f4\u7684\u91cd\u5149\u7167\u6548\u679c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u7269\u7406\u4e00\u81f4\u7684BRDF\u5efa\u6a21\u548c\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5149\u6cfd\u7269\u4f53\u7684\u91cd\u5efa\u548c\u91cd\u5149\u7167\u8d28\u91cf\uff0c\u4e3a\u795e\u7ecf\u6e32\u67d3\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u6750\u8d28\u5206\u89e3\u80fd\u529b\u3002"}}
{"id": "2510.01347", "pdf": "https://arxiv.org/pdf/2510.01347", "abs": "https://arxiv.org/abs/2510.01347", "authors": ["Shuochen Chang"], "title": "Image Generation Based on Image Style Extraction", "categories": ["cs.CV"], "comment": null, "summary": "Image generation based on text-to-image generation models is a task with practical application scenarios that fine-grained styles cannot be precisely described and controlled in natural language, while the guidance information of stylized reference images is difficult to be directly aligned with the textual conditions of traditional textual guidance generation. This study focuses on how to maximize the generative capability of the pretrained generative model, by obtaining fine-grained stylistic representations from a single given stylistic reference image, and injecting the stylistic representations into the generative body without changing the structural framework of the downstream generative model, so as to achieve fine-grained controlled stylized image generation. In this study, we propose a three-stage training style extraction-based image generation method, which uses a style encoder and a style projection layer to align the style representations with the textual representations to realize fine-grained textual cue-based style guide generation. In addition, this study constructs the Style30k-captions dataset, whose samples contain a triad of images, style labels, and text descriptions, to train the style encoder and style projection layer in this experiment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e09\u9636\u6bb5\u8bad\u7ec3\u7684\u98ce\u683c\u63d0\u53d6\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u98ce\u683c\u7f16\u7801\u5668\u548c\u98ce\u683c\u6295\u5f71\u5c42\u5c06\u98ce\u683c\u8868\u793a\u4e0e\u6587\u672c\u8868\u793a\u5bf9\u9f50\uff0c\u5b9e\u73b0\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u7684\u7ec6\u7c92\u5ea6\u98ce\u683c\u5f15\u5bfc\u751f\u6210\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\uff0c\u7ec6\u7c92\u5ea6\u98ce\u683c\u96be\u4ee5\u7528\u81ea\u7136\u8bed\u8a00\u7cbe\u786e\u63cf\u8ff0\u548c\u63a7\u5236\uff0c\u800c\u98ce\u683c\u5316\u53c2\u8003\u56fe\u50cf\u7684\u5f15\u5bfc\u4fe1\u606f\u96be\u4ee5\u4e0e\u4f20\u7edf\u6587\u672c\u5f15\u5bfc\u751f\u6210\u76f4\u63a5\u5bf9\u9f50\u3002", "method": "\u4f7f\u7528\u98ce\u683c\u7f16\u7801\u5668\u548c\u98ce\u683c\u6295\u5f71\u5c42\u4ece\u5355\u5f20\u98ce\u683c\u53c2\u8003\u56fe\u50cf\u4e2d\u63d0\u53d6\u7ec6\u7c92\u5ea6\u98ce\u683c\u8868\u793a\uff0c\u5e76\u5c06\u5176\u6ce8\u5165\u751f\u6210\u6a21\u578b\u800c\u4e0d\u6539\u53d8\u5176\u7ed3\u6784\u6846\u67b6\u3002\u6784\u5efa\u4e86\u5305\u542b\u56fe\u50cf\u3001\u98ce\u683c\u6807\u7b7e\u548c\u6587\u672c\u63cf\u8ff0\u4e09\u5143\u7ec4\u7684Style30k-captions\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u63a7\u5236\u7684\u98ce\u683c\u5316\u56fe\u50cf\u751f\u6210\uff0c\u80fd\u591f\u6700\u5927\u5316\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u98ce\u683c\u8868\u793a\u4e0e\u6587\u672c\u8868\u793a\u7684\u5bf9\u9f50\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u7684\u7ec6\u7c92\u5ea6\u98ce\u683c\u5f15\u5bfc\u56fe\u50cf\u751f\u6210\u3002"}}
{"id": "2510.01478", "pdf": "https://arxiv.org/pdf/2510.01478", "abs": "https://arxiv.org/abs/2510.01478", "authors": ["R\u0103zvan-Andrei Mati\u015fan", "Vincent Tao Hu", "Grigory Bartosh", "Bj\u00f6rn Ommer", "Cees G. M. Snoek", "Max Welling", "Jan-Willem van de Meent", "Mohammad Mahdi Derakhshani", "Floor Eijkelboom"], "title": "Purrception: Variational Flow Matching for Vector-Quantized Image Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce Purrception, a variational flow matching approach for vector-quantized image generation that provides explicit categorical supervision while maintaining continuous transport dynamics. Our method adapts Variational Flow Matching to vector-quantized latents by learning categorical posteriors over codebook indices while computing velocity fields in the continuous embedding space. This combines the geometric awareness of continuous methods with the discrete supervision of categorical approaches, enabling uncertainty quantification over plausible codes and temperature-controlled generation. We evaluate Purrception on ImageNet-1k 256x256 generation. Training converges faster than both continuous flow matching and discrete flow matching baselines while achieving competitive FID scores with state-of-the-art models. This demonstrates that Variational Flow Matching can effectively bridge continuous transport and discrete supervision for improved training efficiency in image generation.", "AI": {"tldr": "Purrception\u662f\u4e00\u79cd\u7528\u4e8e\u77e2\u91cf\u91cf\u5316\u56fe\u50cf\u751f\u6210\u7684\u53d8\u5206\u6d41\u5339\u914d\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u8fde\u7eed\u4f20\u8f93\u52a8\u529b\u5b66\u548c\u663e\u5f0f\u5206\u7c7b\u76d1\u7763\u3002", "motivation": "\u65e8\u5728\u5c06\u8fde\u7eed\u65b9\u6cd5\u7684\u51e0\u4f55\u611f\u77e5\u4e0e\u5206\u7c7b\u65b9\u6cd5\u7684\u79bb\u6563\u76d1\u7763\u76f8\u7ed3\u5408\uff0c\u4e3a\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u6539\u8fdb\u7684\u8bad\u7ec3\u6548\u7387\u3002", "method": "\u901a\u8fc7\u5728\u5b66\u4e60\u7801\u672c\u7d22\u5f15\u7684\u5206\u7c7b\u540e\u9a8c\u7684\u540c\u65f6\u5728\u8fde\u7eed\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u901f\u5ea6\u573a\uff0c\u5c06\u53d8\u5206\u6d41\u5339\u914d\u9002\u5e94\u4e8e\u77e2\u91cf\u91cf\u5316\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "\u5728ImageNet-1k 256x256\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u8bad\u7ec3\u6536\u655b\u901f\u5ea6\u6bd4\u8fde\u7eed\u548c\u79bb\u6563\u6d41\u5339\u914d\u57fa\u7ebf\u66f4\u5feb\uff0c\u540c\u65f6\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u6a21\u578b\u7ade\u4e89\u7684FID\u5206\u6570\u3002", "conclusion": "\u53d8\u5206\u6d41\u5339\u914d\u53ef\u4ee5\u6709\u6548\u5730\u6865\u63a5\u8fde\u7eed\u4f20\u8f93\u548c\u79bb\u6563\u76d1\u7763\uff0c\u63d0\u9ad8\u56fe\u50cf\u751f\u6210\u7684\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2510.01498", "pdf": "https://arxiv.org/pdf/2510.01498", "abs": "https://arxiv.org/abs/2510.01498", "authors": ["Yuxuan Ou", "Ning Bi", "Jiazhen Pan", "Jiancheng Yang", "Boliang Yu", "Usama Zidan", "Regent Lee", "Vicente Grau"], "title": "AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic aneurysms (AAA), the required iodinated contrast agents pose significant risks, including nephrotoxicity, patient allergies, and environmental harm. To reduce contrast agent use, recent deep learning methods have focused on generating synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a multi-stage pipeline that first generates images and then performs segmentation, which leads to error accumulation and fails to leverage shared semantic and anatomical structures. To address this, we propose a unified deep learning framework that generates synthetic CECT images from NCCT scans while simultaneously segmenting the aortic lumen and thrombus. Our approach integrates conditional diffusion models (CDM) with multi-task learning, enabling end-to-end joint optimization of image synthesis and anatomical segmentation. Unlike previous multitask diffusion models, our approach requires no initial predictions (e.g., a coarse segmentation mask), shares both encoder and decoder parameters across tasks, and employs a semi-supervised training strategy to learn from scans with missing segmentation labels, a common constraint in real-world clinical data. We evaluated our method on a cohort of 264 patients, where it consistently outperformed state-of-the-art single-task and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61 dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation, it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6761\u4ef6\u6269\u6563\u6a21\u578b\u548c\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u4ece\u975e\u5bf9\u6bd4CT\u626b\u63cf\u540c\u65f6\u751f\u6210\u5408\u6210\u5bf9\u6bd4\u589e\u5f3aCT\u56fe\u50cf\u5e76\u5206\u5272\u4e3b\u52a8\u8109\u8154\u548c\u8840\u6813\uff0c\u907f\u514d\u4e86\u591a\u9636\u6bb5\u65b9\u6cd5\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5bf9\u6bd4\u589e\u5f3aCT\u68c0\u67e5\u9700\u8981\u7898\u5bf9\u6bd4\u5242\uff0c\u5b58\u5728\u80be\u6bd2\u6027\u3001\u8fc7\u654f\u53cd\u5e94\u548c\u73af\u5883\u5371\u5bb3\u7b49\u98ce\u9669\u3002\u73b0\u6709\u7684\u591a\u9636\u6bb5\u65b9\u6cd5\u5148\u751f\u6210\u56fe\u50cf\u518d\u5206\u5272\uff0c\u5bfc\u81f4\u8bef\u5dee\u7d2f\u79ef\u4e14\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5171\u4eab\u7684\u8bed\u4e49\u548c\u89e3\u5256\u7ed3\u6784\u3002", "method": "\u96c6\u6210\u6761\u4ef6\u6269\u6563\u6a21\u578b\u4e0e\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u7aef\u5230\u7aef\u8054\u5408\u4f18\u5316\u56fe\u50cf\u5408\u6210\u548c\u89e3\u5256\u5206\u5272\u3002\u65e0\u9700\u521d\u59cb\u9884\u6d4b\uff0c\u5728\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u5c42\u9762\u5171\u4eab\u53c2\u6570\uff0c\u91c7\u7528\u534a\u76d1\u7763\u8bad\u7ec3\u7b56\u7565\u5904\u7406\u7f3a\u5931\u5206\u5272\u6807\u7b7e\u7684\u4e34\u5e8a\u6570\u636e\u3002", "result": "\u5728264\u540d\u60a3\u8005\u961f\u5217\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1a\u56fe\u50cf\u5408\u6210PSNR\u8fbe25.61 dB\uff08\u5355\u4efb\u52a1CDM\u4e3a23.80 dB\uff09\uff1b\u5206\u5272\u65b9\u9762\uff0c\u8154Dice\u5f97\u52060.89\uff08nnU-Net\u4e3a0.87\uff09\uff0c\u8840\u6813Dice\u5f97\u52060.53\uff08nnU-Net\u4e3a0.48\uff09\uff1b\u4e34\u5e8a\u6d4b\u91cf\u66f4\u51c6\u786e\uff0c\u8154\u76f4\u5f84MAE\u964d\u81f34.19 mm\uff08nnU-Net\u4e3a5.78 mm\uff09\u3002", "conclusion": "\u8be5\u7edf\u4e00\u6846\u67b6\u901a\u8fc7\u8054\u5408\u4f18\u5316\u56fe\u50cf\u5408\u6210\u548c\u5206\u5272\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5408\u6210\u56fe\u50cf\u8d28\u91cf\u548c\u89e3\u5256\u7ed3\u6784\u5206\u5272\u7cbe\u5ea6\uff0c\u4e3a\u51cf\u5c11\u5bf9\u6bd4\u5242\u4f7f\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.01540", "pdf": "https://arxiv.org/pdf/2510.01540", "abs": "https://arxiv.org/abs/2510.01540", "authors": ["Jiamu Bai", "Xin Yu", "Meilong Xu", "Weitao Lu", "Xin Pan", "Kiwan Maeng", "Daniel Kifer", "Jian Wang", "Yu Wang"], "title": "Towards Better Optimization For Listwise Preference in Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) has proven effectiveness for aligning text-to-image (T2I) diffusion models with human preferences. Although Direct Preference Optimization (DPO) is widely adopted for its computational efficiency and avoidance of explicit reward modeling, its applications to diffusion models have primarily relied on pairwise preferences. The precise optimization of listwise preferences remains largely unaddressed. In practice, human feedback on image preferences often contains implicit ranked information, which conveys more precise human preferences than pairwise comparisons. In this work, we propose Diffusion-LPO, a simple and effective framework for Listwise Preference Optimization in diffusion models with listwise data. Given a caption, we aggregate user feedback into a ranked list of images and derive a listwise extension of the DPO objective under the Plackett-Luce model. Diffusion-LPO enforces consistency across the entire ranking by encouraging each sample to be preferred over all of its lower-ranked alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO across various tasks, including text-to-image generation, image editing, and personalized preference alignment. Diffusion-LPO consistently outperforms pairwise DPO baselines on visual quality and preference alignment.", "AI": {"tldr": "\u63d0\u51fa\u4e86Diffusion-LPO\u6846\u67b6\uff0c\u5c06\u5217\u8868\u5f0f\u504f\u597d\u4f18\u5316\u5e94\u7528\u4e8e\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7Plackett-Luce\u6a21\u578b\u6269\u5c55DPO\u76ee\u6807\uff0c\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u3001\u56fe\u50cf\u7f16\u8f91\u548c\u4e2a\u6027\u5316\u504f\u597d\u5bf9\u9f50\u7b49\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6210\u5bf9DPO\u57fa\u7ebf\u3002", "motivation": "\u4eba\u7c7b\u5bf9\u56fe\u50cf\u7684\u504f\u597d\u53cd\u9988\u901a\u5e38\u5305\u542b\u9690\u5f0f\u7684\u6392\u5e8f\u4fe1\u606f\uff0c\u6bd4\u6210\u5bf9\u6bd4\u8f83\u66f4\u80fd\u7cbe\u786e\u8868\u8fbe\u4eba\u7c7b\u504f\u597d\u3002\u76ee\u524dDPO\u5728\u6269\u6563\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u4e3b\u8981\u4f9d\u8d56\u6210\u5bf9\u504f\u597d\uff0c\u5217\u8868\u5f0f\u504f\u597d\u7684\u7cbe\u786e\u4f18\u5316\u5c1a\u672a\u5f97\u5230\u5145\u5206\u89e3\u51b3\u3002", "method": "\u63d0\u51faDiffusion-LPO\u6846\u67b6\uff0c\u5c06\u7528\u6237\u53cd\u9988\u805a\u5408\u6210\u56fe\u50cf\u6392\u5e8f\u5217\u8868\uff0c\u5728Plackett-Luce\u6a21\u578b\u4e0b\u63a8\u5bfcDPO\u76ee\u6807\u7684\u5217\u8868\u5f0f\u6269\u5c55\uff0c\u901a\u8fc7\u9f13\u52b1\u6bcf\u4e2a\u6837\u672c\u4f18\u4e8e\u5176\u6240\u6709\u4f4e\u6392\u540d\u66ff\u4ee3\u54c1\u6765\u589e\u5f3a\u6574\u4e2a\u6392\u5e8f\u7684\u4e00\u81f4\u6027\u3002", "result": "Diffusion-LPO\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u3001\u56fe\u50cf\u7f16\u8f91\u548c\u4e2a\u6027\u5316\u504f\u597d\u5bf9\u9f50\u7b49\u4efb\u52a1\u4e2d\uff0c\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u504f\u597d\u5bf9\u9f50\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u6210\u5bf9DPO\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Diffusion-LPO\u662f\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u5217\u8868\u5f0f\u504f\u597d\u4f18\u5316\u6846\u67b6\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5229\u7528\u4eba\u7c7b\u53cd\u9988\u4e2d\u7684\u6392\u5e8f\u4fe1\u606f\uff0c\u63d0\u5347\u6269\u6563\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u6548\u679c\u3002"}}
{"id": "2510.01608", "pdf": "https://arxiv.org/pdf/2510.01608", "abs": "https://arxiv.org/abs/2510.01608", "authors": ["Roman Jacome", "Romario Gualdr\u00f3n-Hurtado", "Leon Suarez", "Henry Arguello"], "title": "NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems", "categories": ["cs.CV", "eess.SP", "math.OC"], "comment": "25 pages, 12 tables, 10 figures. Accepted to NeurIPS 2025", "summary": "Imaging inverse problems aims to recover high-dimensional signals from undersampled, noisy measurements, a fundamentally ill-posed task with infinite solutions in the null-space of the sensing operator. To resolve this ambiguity, prior information is typically incorporated through handcrafted regularizers or learned models that constrain the solution space. However, these priors typically ignore the task-specific structure of that null-space. In this work, we propose \\textit{Non-Linear Projections of the Null-Space} (NPN), a novel class of regularization that, instead of enforcing structural constraints in the image domain, promotes solutions that lie in a low-dimensional projection of the sensing matrix's null-space with a neural network. Our approach has two key advantages: (1) Interpretability: by focusing on the structure of the null-space, we design sensing-matrix-specific priors that capture information orthogonal to the signal components that are fundamentally blind to the sensing process. (2) Flexibility: NPN is adaptable to various inverse problems, compatible with existing reconstruction frameworks, and complementary to conventional image-domain priors. We provide theoretical guarantees on convergence and reconstruction accuracy when used within plug-and-play methods. Empirical results across diverse sensing matrices demonstrate that NPN priors consistently enhance reconstruction fidelity in various imaging inverse problems, such as compressive sensing, deblurring, super-resolution, computed tomography, and magnetic resonance imaging, with plug-and-play methods, unrolling networks, deep image prior, and diffusion models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6b63\u5219\u5316\u65b9\u6cd5NPN\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5728\u611f\u77e5\u77e9\u9635\u96f6\u7a7a\u95f4\u7684\u4f4e\u7ef4\u6295\u5f71\u4e0a\u65bd\u52a0\u7ea6\u675f\uff0c\u800c\u4e0d\u662f\u5728\u56fe\u50cf\u57df\u65bd\u52a0\u7ed3\u6784\u7ea6\u675f\uff0c\u4ece\u800c\u89e3\u51b3\u6210\u50cf\u9006\u95ee\u9898\u4e2d\u7684\u6a21\u7cca\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u611f\u77e5\u77e9\u9635\u96f6\u7a7a\u95f4\u7684\u7279\u5b9a\u7ed3\u6784\uff0c\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u80fd\u591f\u6355\u6349\u4e0e\u611f\u77e5\u8fc7\u7a0b\u6b63\u4ea4\u4fe1\u606f\u7684\u611f\u77e5\u77e9\u9635\u7279\u5b9a\u5148\u9a8c\u3002", "method": "\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u5728\u611f\u77e5\u77e9\u9635\u7684\u96f6\u7a7a\u95f4\u4e0a\u8fdb\u884c\u4f4e\u7ef4\u6295\u5f71\uff0c\u8bbe\u8ba1\u611f\u77e5\u77e9\u9635\u7279\u5b9a\u7684\u5148\u9a8c\uff0c\u8be5\u65b9\u6cd5\u53ef\u4e0e\u73b0\u6709\u91cd\u5efa\u6846\u67b6\u517c\u5bb9\uff0c\u5e76\u8865\u5145\u4f20\u7edf\u7684\u56fe\u50cf\u57df\u5148\u9a8c\u3002", "result": "\u5728\u538b\u7f29\u611f\u77e5\u3001\u53bb\u6a21\u7cca\u3001\u8d85\u5206\u8fa8\u7387\u3001\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\u548c\u78c1\u5171\u632f\u6210\u50cf\u7b49\u591a\u79cd\u6210\u50cf\u9006\u95ee\u9898\u4e2d\uff0cNPN\u5148\u9a8c\u80fd\u6301\u7eed\u63d0\u9ad8\u91cd\u5efa\u4fdd\u771f\u5ea6\u3002", "conclusion": "NPN\u65b9\u6cd5\u901a\u8fc7\u5173\u6ce8\u96f6\u7a7a\u95f4\u7ed3\u6784\uff0c\u63d0\u4f9b\u4e86\u89e3\u91ca\u6027\u548c\u7075\u6d3b\u6027\uff0c\u80fd\u6709\u6548\u589e\u5f3a\u5404\u79cd\u6210\u50cf\u9006\u95ee\u9898\u7684\u91cd\u5efa\u6027\u80fd\u3002"}}
{"id": "2510.01640", "pdf": "https://arxiv.org/pdf/2510.01640", "abs": "https://arxiv.org/abs/2510.01640", "authors": ["Yifan Zhao", "Liangchen Li", "Yuqi Zhou", "Kai Wang", "Yan Liang", "Juyong Zhang"], "title": "Joint Deblurring and 3D Reconstruction for Macrophotography", "categories": ["cs.CV"], "comment": "Accepted to Pacific Graphics 2025. To be published in Computer   Graphics Forum", "summary": "Macro lens has the advantages of high resolution and large magnification, and 3D modeling of small and detailed objects can provide richer information. However, defocus blur in macrophotography is a long-standing problem that heavily hinders the clear imaging of the captured objects and high-quality 3D reconstruction of them. Traditional image deblurring methods require a large number of images and annotations, and there is currently no multi-view 3D reconstruction method for macrophotography. In this work, we propose a joint deblurring and 3D reconstruction method for macrophotography. Starting from multi-view blurry images captured, we jointly optimize the clear 3D model of the object and the defocus blur kernel of each pixel. The entire framework adopts a differentiable rendering method to self-supervise the optimization of the 3D model and the defocus blur kernel. Extensive experiments show that from a small number of multi-view images, our proposed method can not only achieve high-quality image deblurring but also recover high-fidelity 3D appearance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5fae\u8ddd\u6444\u5f71\u7684\u8054\u5408\u53bb\u6a21\u7cca\u548c3D\u91cd\u5efa\u65b9\u6cd5\uff0c\u4ece\u591a\u89c6\u89d2\u6a21\u7cca\u56fe\u50cf\u4e2d\u8054\u5408\u4f18\u5316\u7269\u4f53\u7684\u6e05\u66703D\u6a21\u578b\u548c\u6bcf\u4e2a\u50cf\u7d20\u7684\u6563\u7126\u6a21\u7cca\u6838\u3002", "motivation": "\u5fae\u8ddd\u955c\u5934\u5177\u6709\u9ad8\u5206\u8fa8\u7387\u548c\u5927\u653e\u5927\u500d\u7387\u7684\u4f18\u52bf\uff0c\u4f46\u6563\u7126\u6a21\u7cca\u95ee\u9898\u4e25\u91cd\u963b\u788d\u4e86\u6e05\u6670\u6210\u50cf\u548c\u9ad8\u8d28\u91cf3D\u91cd\u5efa\u3002\u4f20\u7edf\u53bb\u6a21\u7cca\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u56fe\u50cf\u548c\u6807\u6ce8\uff0c\u4e14\u76ee\u524d\u6ca1\u6709\u9488\u5bf9\u5fae\u8ddd\u6444\u5f71\u7684\u591a\u89c6\u89d23D\u91cd\u5efa\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u53ef\u5fae\u5206\u6e32\u67d3\u65b9\u6cd5\u81ea\u76d1\u7763\u4f18\u53163D\u6a21\u578b\u548c\u6563\u7126\u6a21\u7cca\u6838\uff0c\u4ece\u591a\u89c6\u89d2\u6a21\u7cca\u56fe\u50cf\u51fa\u53d1\uff0c\u8054\u5408\u4f18\u5316\u6e05\u66703D\u6a21\u578b\u548c\u6bcf\u4e2a\u50cf\u7d20\u7684\u6a21\u7cca\u6838\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u4ece\u5c11\u91cf\u591a\u89c6\u89d2\u56fe\u50cf\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u5b9e\u73b0\u9ad8\u8d28\u91cf\u56fe\u50cf\u53bb\u6a21\u7cca\uff0c\u8fd8\u80fd\u6062\u590d\u9ad8\u4fdd\u771f\u5ea6\u76843D\u5916\u89c2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u5fae\u8ddd\u6444\u5f71\u4e2d\u7684\u6563\u7126\u6a21\u7cca\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8054\u5408\u53bb\u6a21\u7cca\u548c3D\u91cd\u5efa\uff0c\u4e3a\u5c0f\u7269\u4f53\u548c\u7ec6\u8282\u7269\u4f53\u7684\u9ad8\u8d28\u91cf3D\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.01641", "pdf": "https://arxiv.org/pdf/2510.01641", "abs": "https://arxiv.org/abs/2510.01641", "authors": ["Xiaoyang Liu", "Zhengyan Zhou", "Zihang Xu", "Jiezhang Cao", "Zheng Chen", "Yulun Zhang"], "title": "FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in image motion deblurring, driven by CNNs and transformers, have made significant progress. Large-scale pre-trained diffusion models, which are rich in true-world modeling, have shown great promise for high-quality image restoration tasks such as deblurring, demonstrating stronger generative capabilities than CNN and transformer-based methods. However, challenges such as unbearable inference time and compromised fidelity still limit the full potential of the diffusion models. To address this, we introduce FideDiff, a novel single-step diffusion model designed for high-fidelity deblurring. We reformulate motion deblurring as a diffusion-like process where each timestep represents a progressively blurred image, and we train a consistency model that aligns all timesteps to the same clean image. By reconstructing training data with matched blur trajectories, the model learns temporal consistency, enabling accurate one-step deblurring. We further enhance model performance by integrating Kernel ControlNet for blur kernel estimation and introducing adaptive timestep prediction. Our model achieves superior performance on full-reference metrics, surpassing previous diffusion-based methods and matching the performance of other state-of-the-art models. FideDiff offers a new direction for applying pre-trained diffusion models to high-fidelity image restoration tasks, establishing a robust baseline for further advancing diffusion models in real-world industrial applications. Our dataset and code will be available at https://github.com/xyLiu339/FideDiff.", "AI": {"tldr": "FideDiff\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5355\u6b65\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u8fd0\u52a8\u53bb\u6a21\u7cca\u91cd\u65b0\u8868\u8ff0\u4e3a\u6269\u6563\u8fc7\u7a0b\uff0c\u8bad\u7ec3\u4e00\u81f4\u6027\u6a21\u578b\u5b9e\u73b0\u4e00\u6b65\u9ad8\u8d28\u91cf\u53bb\u6a21\u7cca\uff0c\u5728\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u53bb\u6a21\u7cca\u65b9\u6cd5\u5b58\u5728\u63a8\u7406\u65f6\u95f4\u8fc7\u957f\u548c\u4fdd\u771f\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u5c06\u8fd0\u52a8\u53bb\u6a21\u7cca\u91cd\u65b0\u8868\u8ff0\u4e3a\u6269\u6563\u8fc7\u7a0b\uff0c\u6bcf\u4e2a\u65f6\u95f4\u6b65\u4ee3\u8868\u9010\u6e10\u6a21\u7cca\u7684\u56fe\u50cf\uff1b\u8bad\u7ec3\u4e00\u81f4\u6027\u6a21\u578b\u5c06\u6240\u6709\u65f6\u95f4\u6b65\u5bf9\u9f50\u5230\u540c\u4e00\u6e05\u6670\u56fe\u50cf\uff1b\u96c6\u6210Kernel ControlNet\u8fdb\u884c\u6a21\u7cca\u6838\u4f30\u8ba1\uff1b\u5f15\u5165\u81ea\u9002\u5e94\u65f6\u95f4\u6b65\u9884\u6d4b\u3002", "result": "\u5728\u5168\u53c2\u8003\u6307\u6807\u4e0a\u5b9e\u73b0\u4f18\u8d8a\u6027\u80fd\uff0c\u8d85\u8d8a\u4e4b\u524d\u7684\u6269\u6563\u65b9\u6cd5\uff0c\u4e0e\u5176\u4ed6\u6700\u5148\u8fdb\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "FideDiff\u4e3a\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u5728\u9ad8\u4fdd\u771f\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u4e3a\u5728\u771f\u5b9e\u5de5\u4e1a\u5e94\u7528\u4e2d\u8fdb\u4e00\u6b65\u63a8\u8fdb\u6269\u6563\u6a21\u578b\u5efa\u7acb\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2510.01662", "pdf": "https://arxiv.org/pdf/2510.01662", "abs": "https://arxiv.org/abs/2510.01662", "authors": ["Minh Tran", "Maksim Siniukov", "Zhangyu Jin", "Mohammad Soleymani"], "title": "Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery", "categories": ["cs.CV"], "comment": null, "summary": "Facial expression analysis is central to understanding human behavior, yet existing coding systems such as the Facial Action Coding System (FACS) are constrained by limited coverage and costly manual annotation. In this work, we introduce Discrete Facial Encoding (DFE), an unsupervised, data-driven alternative of compact and interpretable dictionary of facial expressions from 3D mesh sequences learned through a Residual Vector Quantized Variational Autoencoder (RVQ-VAE). Our approach first extracts identity-invariant expression features from images using a 3D Morphable Model (3DMM), effectively disentangling factors such as head pose and facial geometry. We then encode these features using an RVQ-VAE, producing a sequence of discrete tokens from a shared codebook, where each token captures a specific, reusable facial deformation pattern that contributes to the overall expression. Through extensive experiments, we demonstrate that Discrete Facial Encoding captures more precise facial behaviors than FACS and other facial encoding alternatives. We evaluate the utility of our representation across three high-level psychological tasks: stress detection, personality prediction, and depression detection. Using a simple Bag-of-Words model built on top of the learned tokens, our system consistently outperforms both FACS-based pipelines and strong image and video representation learning models such as Masked Autoencoders. Further analysis reveals that our representation covers a wider variety of facial displays, highlighting its potential as a scalable and effective alternative to FACS for psychological and affective computing applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u79bb\u6563\u9762\u90e8\u7f16\u7801(DFE)\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4ece3D\u7f51\u683c\u5e8f\u5217\u4e2d\u5b66\u4e60\u7d27\u51d1\u4e14\u53ef\u89e3\u91ca\u7684\u9762\u90e8\u8868\u60c5\u5b57\u5178\uff0c\u5728\u5fc3\u7406\u4efb\u52a1\u4e2d\u4f18\u4e8eFACS\u548c\u5176\u4ed6\u9762\u90e8\u7f16\u7801\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u9762\u90e8\u8868\u60c5\u7f16\u7801\u7cfb\u7edf\u5982FACS\u5b58\u5728\u8986\u76d6\u8303\u56f4\u6709\u9650\u548c\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u786e\u3001\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u75283D\u53ef\u53d8\u5f62\u6a21\u578b\u63d0\u53d6\u8eab\u4efd\u4e0d\u53d8\u7684\u8868\u60c5\u7279\u5f81\uff0c\u7136\u540e\u901a\u8fc7\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5c06\u8fd9\u4e9b\u7279\u5f81\u7f16\u7801\u4e3a\u6765\u81ea\u5171\u4eab\u7801\u672c\u7684\u79bb\u6563\u6807\u8bb0\u5e8f\u5217\u3002", "result": "DFE\u6bd4FACS\u548c\u5176\u4ed6\u9762\u90e8\u7f16\u7801\u65b9\u6cd5\u6355\u6349\u5230\u66f4\u7cbe\u786e\u7684\u9762\u90e8\u884c\u4e3a\uff0c\u5728\u538b\u529b\u68c0\u6d4b\u3001\u4eba\u683c\u9884\u6d4b\u548c\u6291\u90c1\u68c0\u6d4b\u7b49\u5fc3\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8eFACS\u7684\u6d41\u7a0b\u548c\u5f3a\u56fe\u50cf/\u89c6\u9891\u8868\u793a\u5b66\u4e60\u6a21\u578b\u3002", "conclusion": "DFE\u4f5c\u4e3aFACS\u7684\u53ef\u6269\u5c55\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u5fc3\u7406\u5b66\u548c\u60c5\u611f\u8ba1\u7b97\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u8986\u76d6\u66f4\u5e7f\u6cdb\u7684\u9762\u90e8\u8868\u60c5\u3002"}}
{"id": "2510.01669", "pdf": "https://arxiv.org/pdf/2510.01669", "abs": "https://arxiv.org/abs/2510.01669", "authors": ["Jin Cao", "Hongrui Wu", "Ziyong Feng", "Hujun Bao", "Xiaowei Zhou", "Sida Peng"], "title": "UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction", "categories": ["cs.CV"], "comment": "page: https://jin-cao-tma.github.io/UniVerse.github.io/ code:   https://github.com/zju3dv/UniVerse", "summary": "This paper tackles the challenge of robust reconstruction, i.e., the task of reconstructing a 3D scene from a set of inconsistent multi-view images. Some recent works have attempted to simultaneously remove image inconsistencies and perform reconstruction by integrating image degradation modeling into neural 3D scene representations.However, these methods rely heavily on dense observations for robustly optimizing model parameters.To address this issue, we propose to decouple robust reconstruction into two subtasks: restoration and reconstruction, which naturally simplifies the optimization process.To this end, we introduce UniVerse, a unified framework for robust reconstruction based on a video diffusion model. Specifically, UniVerse first converts inconsistent images into initial videos, then uses a specially designed video diffusion model to restore them into consistent images, and finally reconstructs the 3D scenes from these restored images.Compared with case-by-case per-view degradation modeling, the diffusion model learns a general scene prior from large-scale data, making it applicable to diverse image inconsistencies.Extensive experiments on both synthetic and real-world datasets demonstrate the strong generalization capability and superior performance of our method in robust reconstruction. Moreover, UniVerse can control the style of the reconstructed 3D scene. Project page: https://jin-cao-tma.github.io/UniVerse.github.io/", "AI": {"tldr": "UniVerse\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u9c81\u68d2\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4e0d\u4e00\u81f4\u7684\u591a\u89c6\u89d2\u56fe\u50cf\u8f6c\u6362\u4e3a\u89c6\u9891\uff0c\u4f7f\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u6062\u590d\u4e00\u81f4\u6027\uff0c\u7136\u540e\u91cd\u5efa3D\u573a\u666f\u3002", "motivation": "\u89e3\u51b3\u4ece\u4e0d\u4e00\u81f4\u591a\u89c6\u89d2\u56fe\u50cf\u8fdb\u884c3D\u91cd\u5efa\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5bc6\u96c6\u89c2\u6d4b\u4e14\u4f18\u5316\u56f0\u96be\u3002", "method": "\u5c06\u9c81\u68d2\u91cd\u5efa\u5206\u89e3\u4e3a\u6062\u590d\u548c\u91cd\u5efa\u4e24\u4e2a\u5b50\u4efb\u52a1\uff0c\u4f7f\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u5b66\u4e60\u901a\u7528\u573a\u666f\u5148\u9a8c\u6765\u6062\u590d\u56fe\u50cf\u4e00\u81f4\u6027\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u80fd\u63a7\u5236\u91cd\u5efa3D\u573a\u666f\u7684\u98ce\u683c\u3002", "conclusion": "UniVerse\u901a\u8fc7\u89e3\u8026\u6062\u590d\u548c\u91cd\u5efa\u4efb\u52a1\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u901a\u7528\u5148\u9a8c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u4e00\u81f4\u591a\u89c6\u89d2\u56fe\u50cf\u76843D\u91cd\u5efa\u95ee\u9898\u3002"}}
{"id": "2510.01686", "pdf": "https://arxiv.org/pdf/2510.01686", "abs": "https://arxiv.org/abs/2510.01686", "authors": ["Jiacong Xu", "Yiqun Mei", "Ke Zhang", "Vishal M. Patel"], "title": "FreeViS: Training-free Video Stylization with Inconsistent References", "categories": ["cs.CV"], "comment": "Project Page: \\url{https://xujiacong.github.io/FreeViS/}", "summary": "Video stylization plays a key role in content creation, but it remains a challenging problem. Na\\\"ively applying image stylization frame-by-frame hurts temporal consistency and reduces style richness. Alternatively, training a dedicated video stylization model typically requires paired video data and is computationally expensive. In this paper, we propose FreeViS, a training-free video stylization framework that generates stylized videos with rich style details and strong temporal coherence. Our method integrates multiple stylized references to a pretrained image-to-video (I2V) model, effectively mitigating the propagation errors observed in prior works, without introducing flickers and stutters. In addition, it leverages high-frequency compensation to constrain the content layout and motion, together with flow-based motion cues to preserve style textures in low-saliency regions. Through extensive evaluations, FreeViS delivers higher stylization fidelity and superior temporal consistency, outperforming recent baselines and achieving strong human preference. Our training-free pipeline offers a practical and economic solution for high-quality, temporally coherent video stylization. The code and videos can be accessed via https://xujiacong.github.io/FreeViS/", "AI": {"tldr": "FreeViS\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u98ce\u683c\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u591a\u4e2a\u98ce\u683c\u5316\u53c2\u8003\u5230\u9884\u8bad\u7ec3\u7684\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u578b\uff0c\u751f\u6210\u5177\u6709\u4e30\u5bcc\u98ce\u683c\u7ec6\u8282\u548c\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u98ce\u683c\u5316\u89c6\u9891\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u98ce\u683c\u5316\u65b9\u6cd5\u5b58\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u5dee\u3001\u98ce\u683c\u4e30\u5bcc\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u800c\u8bad\u7ec3\u4e13\u95e8\u7684\u89c6\u9891\u98ce\u683c\u5316\u6a21\u578b\u9700\u8981\u914d\u5bf9\u89c6\u9891\u6570\u636e\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u578b\uff0c\u6574\u5408\u591a\u4e2a\u98ce\u683c\u5316\u53c2\u8003\uff0c\u91c7\u7528\u9ad8\u9891\u8865\u507f\u7ea6\u675f\u5185\u5bb9\u5e03\u5c40\u548c\u8fd0\u52a8\uff0c\u7ed3\u5408\u57fa\u4e8e\u5149\u6d41\u7684\u8fd0\u52a8\u7ebf\u7d22\u6765\u4fdd\u7559\u4f4e\u663e\u8457\u6027\u533a\u57df\u7684\u98ce\u683c\u7eb9\u7406\u3002", "result": "FreeViS\u5728\u98ce\u683c\u5316\u4fdd\u771f\u5ea6\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u83b7\u5f97\u4e86\u5f3a\u70c8\u7684\u4eba\u7c7b\u504f\u597d\u3002", "conclusion": "FreeViS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u7ecf\u6d4e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u89c6\u9891\u98ce\u683c\u5316\uff0c\u65e0\u9700\u8bad\u7ec3\u8fc7\u7a0b\u3002"}}
{"id": "2510.01715", "pdf": "https://arxiv.org/pdf/2510.01715", "abs": "https://arxiv.org/abs/2510.01715", "authors": ["Raahul Krishna Durairaju", "K. Saruladha"], "title": "PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based algorithm, enabling AI-driven artistic image synthesis. However, existing CNN and transformer-based models struggle to scale efficiently to complex styles and high-resolution inputs. We introduce PyramidStyler, a transformer framework with Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding that captures both local details and global context while reducing computational load. We further incorporate reinforcement learning to dynamically optimize stylization, accelerating convergence. Trained on Microsoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to 2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s inference--and yields further improvements (content 2.03; style 0.75) with minimal speed penalty (1.40 s) when using RL. These results demonstrate real-time, high-quality artistic rendering, with broad applications in media and design.", "AI": {"tldr": "PyramidStyler\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u795e\u7ecf\u98ce\u683c\u8fc1\u79fb\u6846\u67b6\uff0c\u91c7\u7528\u91d1\u5b57\u5854\u4f4d\u7f6e\u7f16\u7801\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5bb9\u548c\u98ce\u683c\u635f\u5931\u3002", "motivation": "\u73b0\u6709\u7684CNN\u548cTransformer\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u98ce\u683c\u548c\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u65f6\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u6355\u6349\u5c40\u90e8\u7ec6\u8282\u53c8\u80fd\u7406\u89e3\u5168\u5c40\u4e0a\u4e0b\u6587\u7684\u9ad8\u6548\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPyramidStyler\u6846\u67b6\uff0c\u5305\u542b\u91d1\u5b57\u5854\u4f4d\u7f6e\u7f16\u7801\uff08PPE\uff09\u5b9e\u73b0\u591a\u5c3a\u5ea6\u7279\u5f81\u6355\u6349\uff0c\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u4f18\u5316\u98ce\u683c\u5316\u8fc7\u7a0b\u4ee5\u52a0\u901f\u6536\u655b\u3002", "result": "\u57284000\u8f6e\u8bad\u7ec3\u540e\uff0c\u5185\u5bb9\u635f\u5931\u964d\u4f4e62.6%\u81f32.07\uff0c\u98ce\u683c\u635f\u5931\u964d\u4f4e57.4%\u81f30.86\uff0c\u63a8\u7406\u65f6\u95f41.39\u79d2\uff1b\u4f7f\u7528RL\u540e\u8fdb\u4e00\u6b65\u6539\u5584\u81f3\u5185\u5bb9\u635f\u59312.03\u3001\u98ce\u683c\u635f\u59310.75\uff0c\u901f\u5ea6\u4ec5\u8f7b\u5fae\u4e0b\u964d\u81f31.40\u79d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5b9e\u65f6\u9ad8\u8d28\u91cf\u7684\u827a\u672f\u6e32\u67d3\uff0c\u5728\u5a92\u4f53\u548c\u8bbe\u8ba1\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.01767", "pdf": "https://arxiv.org/pdf/2510.01767", "abs": "https://arxiv.org/abs/2510.01767", "authors": ["Sheng-Hsiang Hung", "Ting-Yu Yen", "Wei-Fang Sun", "Simon See", "Shih-Hsuan Hung", "Hung-Kuo Chu"], "title": "LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has established itself as an efficient representation for real-time, high-fidelity 3D scene reconstruction. However, scaling 3DGS to large and unbounded scenes such as city blocks remains difficult. Existing divide-and-conquer methods alleviate memory pressure by partitioning the scene into blocks, but introduce new bottlenecks: (i) partitions suffer from severe load imbalance since uniform or heuristic splits do not reflect actual computational demands, and (ii) coarse-to-fine pipelines fail to exploit the coarse stage efficiently, often reloading the entire model and incurring high overhead. In this work, we introduce LoBE-GS, a novel Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning method that reduces preprocessing from hours to minutes, an optimization-based strategy that balances visible Gaussians -- a strong proxy for computational load -- across blocks, and two lightweight techniques, visibility cropping and selective densification, to further reduce training cost. Evaluations on large-scale urban and outdoor datasets show that LoBE-GS consistently achieves up to $2\\times$ faster end-to-end training time than state-of-the-art baselines, while maintaining reconstruction quality and enabling scalability to scenes infeasible with vanilla 3DGS.", "AI": {"tldr": "LoBE-GS\u662f\u4e00\u4e2a\u8d1f\u8f7d\u5e73\u8861\u7684\u9ad8\u65483D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u611f\u77e5\u5206\u533a\u3001\u4f18\u5316\u7b56\u7565\u548c\u8f7b\u91cf\u7ea7\u6280\u672f\uff0c\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u5b9e\u73b02\u500d\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u5206\u6cbb\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u65e0\u8fb9\u754c\u573a\u666f\uff08\u5982\u57ce\u5e02\u8857\u533a\uff09\u4e2d\u5b58\u5728\u8d1f\u8f7d\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u7edf\u4e00\u6216\u542f\u53d1\u5f0f\u5206\u533a\u65e0\u6cd5\u53cd\u6620\u5b9e\u9645\u8ba1\u7b97\u9700\u6c42\uff0c\u7c97\u5230\u7ec6\u6d41\u6c34\u7ebf\u6548\u7387\u4f4e\u4e0b\u4e14\u5f00\u9500\u9ad8\u3002", "method": "\u63d0\u51fa\u6df1\u5ea6\u611f\u77e5\u5206\u533a\u65b9\u6cd5\u51cf\u5c11\u9884\u5904\u7406\u65f6\u95f4\uff0c\u91c7\u7528\u57fa\u4e8e\u4f18\u5316\u7684\u7b56\u7565\u5e73\u8861\u5404\u533a\u5757\u53ef\u89c1\u9ad8\u65af\u5206\u5e03\uff0c\u5e76\u5f15\u5165\u53ef\u89c1\u6027\u88c1\u526a\u548c\u9009\u62e9\u6027\u81f4\u5bc6\u5316\u4e24\u79cd\u8f7b\u91cf\u7ea7\u6280\u672f\u3002", "result": "\u5728\u5927\u89c4\u6a21\u57ce\u5e02\u548c\u6237\u5916\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\uff0cLoBE-GS\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u5b9e\u73b02\u500d\u7aef\u5230\u7aef\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u91cd\u5efa\u8d28\u91cf\uff0c\u80fd\u591f\u6269\u5c55\u5230\u4f20\u7edf3DGS\u65e0\u6cd5\u5904\u7406\u7684\u573a\u666f\u3002", "conclusion": "LoBE-GS\u901a\u8fc7\u91cd\u65b0\u8bbe\u8ba1\u5927\u89c4\u6a213DGS\u6d41\u6c34\u7ebf\uff0c\u89e3\u51b3\u4e86\u8d1f\u8f7d\u4e0d\u5e73\u8861\u548c\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.01784", "pdf": "https://arxiv.org/pdf/2510.01784", "abs": "https://arxiv.org/abs/2510.01784", "authors": ["Xiaofei Wu", "Guozhen Zhang", "Zhiyong Xu", "Yuan Zhou", "Qinglin Lu", "Xuming He"], "title": "Pack and Force Your Memory: Long-form and Consistent Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Long-form video generation presents a dual challenge: models must capture long-range dependencies while preventing the error accumulation inherent in autoregressive decoding. To address these challenges, we make two contributions. First, for dynamic context modeling, we propose MemoryPack, a learnable context-retrieval mechanism that leverages both textual and image information as global guidance to jointly model short- and long-term dependencies, achieving minute-level temporal consistency. This design scales gracefully with video length, preserves computational efficiency, and maintains linear complexity. Second, to mitigate error accumulation, we introduce Direct Forcing, an efficient single-step approximating strategy that improves training-inference alignment and thereby curtails error propagation during inference. Together, MemoryPack and Direct Forcing substantially enhance the context consistency and reliability of long-form video generation, advancing the practical usability of autoregressive video models.", "AI": {"tldr": "\u63d0\u51faMemoryPack\u548cDirect Forcing\u4e24\u4e2a\u521b\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u957f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u957f\u671f\u4f9d\u8d56\u5efa\u6a21\u548c\u81ea\u56de\u5f52\u89e3\u7801\u9519\u8bef\u7d2f\u79ef\u95ee\u9898\u3002", "motivation": "\u957f\u89c6\u9891\u751f\u6210\u9762\u4e34\u53cc\u91cd\u6311\u6218\uff1a\u9700\u8981\u6355\u83b7\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\uff0c\u540c\u65f6\u9632\u6b62\u81ea\u56de\u5f52\u89e3\u7801\u4e2d\u56fa\u6709\u7684\u9519\u8bef\u7d2f\u79ef\u3002", "method": "1. MemoryPack\uff1a\u53ef\u5b66\u4e60\u7684\u4e0a\u4e0b\u6587\u68c0\u7d22\u673a\u5236\uff0c\u5229\u7528\u6587\u672c\u548c\u56fe\u50cf\u4fe1\u606f\u4f5c\u4e3a\u5168\u5c40\u6307\u5bfc\uff0c\u8054\u5408\u5efa\u6a21\u77ed\u671f\u548c\u957f\u671f\u4f9d\u8d56\uff1b2. Direct Forcing\uff1a\u9ad8\u6548\u7684\u5355\u6b65\u8fd1\u4f3c\u7b56\u7565\uff0c\u6539\u5584\u8bad\u7ec3-\u63a8\u7406\u5bf9\u9f50\uff0c\u51cf\u5c11\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u9519\u8bef\u4f20\u64ad\u3002", "result": "\u5b9e\u73b0\u4e86\u5206\u949f\u7ea7\u7684\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u4fdd\u6301\u7ebf\u6027\u590d\u6742\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u89c6\u9891\u751f\u6210\u7684\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "MemoryPack\u548cDirect Forcing\u5171\u540c\u63a8\u8fdb\u4e86\u81ea\u56de\u5f52\u89c6\u9891\u6a21\u578b\u7684\u5b9e\u9645\u53ef\u7528\u6027\uff0c\u4e3a\u957f\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.01841", "pdf": "https://arxiv.org/pdf/2510.01841", "abs": "https://arxiv.org/abs/2510.01841", "authors": ["Giyeol Kim", "Sooyoung Yang", "Jihyong Oh", "Myungjoo Kang", "Chanho Eom"], "title": "Leveraging Prior Knowledge of Diffusion Model for Person Search", "categories": ["cs.CV"], "comment": null, "summary": "Person search aims to jointly perform person detection and re-identification by localizing and identifying a query person within a gallery of uncropped scene images. Existing methods predominantly utilize ImageNet pre-trained backbones, which may be suboptimal for capturing the complex spatial context and fine-grained identity cues necessary for person search. Moreover, they rely on a shared backbone feature for both person detection and re-identification, leading to suboptimal features due to conflicting optimization objectives. In this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a novel framework that leverages a pre-trained diffusion model while eliminating the optimization conflict between two sub-tasks. We analyze key properties of diffusion priors and propose three specialized modules: (i) Diffusion-Guided Region Proposal Network (DGRPN) for enhanced person localization, (ii) Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and (iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage text-aligned diffusion features. DiffPS sets a new state-of-the-art on CUHK-SYSU and PRW.", "AI": {"tldr": "DiffPS\u662f\u4e00\u4e2a\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u5148\u9a8c\u77e5\u8bc6\u6765\u89e3\u51b3\u884c\u4eba\u641c\u7d22\u4efb\u52a1\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u4e13\u95e8\u6a21\u5757\u6d88\u9664\u68c0\u6d4b\u548c\u91cd\u8bc6\u522b\u5b50\u4efb\u52a1\u4e4b\u95f4\u7684\u4f18\u5316\u51b2\u7a81\uff0c\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u65b0\u7684\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528ImageNet\u9884\u8bad\u7ec3\u9aa8\u5e72\u7f51\u7edc\uff0c\u53ef\u80fd\u65e0\u6cd5\u5145\u5206\u6355\u6349\u884c\u4eba\u641c\u7d22\u6240\u9700\u7684\u590d\u6742\u7a7a\u95f4\u4e0a\u4e0b\u6587\u548c\u7ec6\u7c92\u5ea6\u8eab\u4efd\u7ebf\u7d22\uff0c\u4e14\u5171\u4eab\u9aa8\u5e72\u7f51\u7edc\u7279\u5f81\u4f1a\u5bfc\u81f4\u68c0\u6d4b\u548c\u91cd\u8bc6\u522b\u4e24\u4e2a\u5b50\u4efb\u52a1\u4e4b\u95f4\u5b58\u5728\u4f18\u5316\u51b2\u7a81\u3002", "method": "\u63d0\u51faDiffPS\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u5305\u542b\u4e09\u4e2a\u4e13\u95e8\u6a21\u5757\uff1a\u6269\u6563\u5f15\u5bfc\u533a\u57df\u63d0\u8bae\u7f51\u7edc(DGRPN)\u7528\u4e8e\u589e\u5f3a\u884c\u4eba\u5b9a\u4f4d\uff0c\u591a\u5c3a\u5ea6\u9891\u7387\u7ec6\u5316\u7f51\u7edc(MSFRN)\u7f13\u89e3\u5f62\u72b6\u504f\u5dee\uff0c\u8bed\u4e49\u81ea\u9002\u5e94\u7279\u5f81\u805a\u5408\u7f51\u7edc(SFAN)\u5229\u7528\u6587\u672c\u5bf9\u9f50\u7684\u6269\u6563\u7279\u5f81\u3002", "result": "\u5728CUHK-SYSU\u548cPRW\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u6269\u6563\u5148\u9a8c\u77e5\u8bc6\u80fd\u591f\u6709\u6548\u63d0\u5347\u884c\u4eba\u641c\u7d22\u6027\u80fd\uff0c\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u6a21\u5757\u53ef\u4ee5\u6d88\u9664\u5b50\u4efb\u52a1\u95f4\u7684\u4f18\u5316\u51b2\u7a81\uff0c\u4e3a\u884c\u4eba\u641c\u7d22\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.01991", "pdf": "https://arxiv.org/pdf/2510.01991", "abs": "https://arxiv.org/abs/2510.01991", "authors": ["Lei Liu", "Can Wang", "Zhenghao Chen", "Dong Xu"], "title": "4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges with view, temporal, and non-editing region consistency, as well as with handling complex text instructions. To address these issues, we propose 4DGS-Craft, a consistent and interactive 4DGS editing framework. We first introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal consistency. This model incorporates 4D VGGT geometry features extracted from the initial scene, enabling it to capture underlying 4D geometric structures during editing. We further enhance this model with a multi-view grid module that enforces consistency by iteratively refining multi-view input images while jointly optimizing the underlying 4D scene. Furthermore, we preserve the consistency of non-edited regions through a novel Gaussian selection mechanism, which identifies and optimizes only the Gaussians within the edited regions. Beyond consistency, facilitating user interaction is also crucial for effective 4DGS editing. Therefore, we design an LLM-based module for user intent understanding. This module employs a user instruction template to define atomic editing operations and leverages an LLM for reasoning. As a result, our framework can interpret user intent and decompose complex instructions into a logical sequence of atomic operations, enabling it to handle intricate user commands and further enhance editing performance. Compared to related works, our approach enables more consistent and controllable 4D scene editing. Our code will be made available upon acceptance.", "AI": {"tldr": "4DGS-Craft\u662f\u4e00\u4e2a\u4e00\u81f4\u7684\u4ea4\u4e92\u5f0f4D\u9ad8\u65af\u6e85\u5c04\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc74D\u611f\u77e5\u7684InstructPix2Pix\u6a21\u578b\u3001\u591a\u89c6\u56fe\u7f51\u683c\u6a21\u5757\u548cGaussian\u9009\u62e9\u673a\u5236\u89e3\u51b3\u89c6\u56fe\u3001\u65f6\u95f4\u548c\u975e\u7f16\u8f91\u533a\u57df\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5e76\u5229\u7528LLM\u7406\u89e3\u7528\u6237\u610f\u56fe\u5904\u7406\u590d\u6742\u6307\u4ee4\u3002", "motivation": "\u73b0\u6709\u76844D\u9ad8\u65af\u6e85\u5c04\u7f16\u8f91\u65b9\u6cd5\u5728\u89c6\u56fe\u4e00\u81f4\u6027\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u3001\u975e\u7f16\u8f91\u533a\u57df\u4e00\u81f4\u6027\u4ee5\u53ca\u5904\u7406\u590d\u6742\u6587\u672c\u6307\u4ee4\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u4e00\u81f4\u548c\u53ef\u63a7\u76844D\u573a\u666f\u7f16\u8f91\u6846\u67b6\u3002", "method": "1. \u5f15\u51654D\u611f\u77e5\u7684InstructPix2Pix\u6a21\u578b\uff0c\u7ed3\u54084D VGGT\u51e0\u4f55\u7279\u5f81\u786e\u4fdd\u89c6\u56fe\u548c\u65f6\u95f4\u4e00\u81f4\u6027\uff1b2. \u4f7f\u7528\u591a\u89c6\u56fe\u7f51\u683c\u6a21\u5757\u8fed\u4ee3\u4f18\u5316\u591a\u89c6\u56fe\u8f93\u5165\u56fe\u50cf\uff1b3. \u63d0\u51faGaussian\u9009\u62e9\u673a\u5236\u4ec5\u4f18\u5316\u7f16\u8f91\u533a\u57df\u7684\u9ad8\u65af\u5206\u5e03\uff1b4. \u8bbe\u8ba1\u57fa\u4e8eLLM\u7684\u7528\u6237\u610f\u56fe\u7406\u89e3\u6a21\u5757\uff0c\u5c06\u590d\u6742\u6307\u4ee4\u5206\u89e3\u4e3a\u539f\u5b50\u64cd\u4f5c\u5e8f\u5217\u3002", "result": "\u4e0e\u76f8\u5173\u5de5\u4f5c\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u66f4\u4e00\u81f4\u548c\u53ef\u63a7\u76844D\u573a\u666f\u7f16\u8f91\uff0c\u6709\u6548\u5904\u7406\u590d\u6742\u7684\u7528\u6237\u6307\u4ee4\uff0c\u5e76\u5728\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u975e\u7f16\u8f91\u533a\u57df\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "4DGS-Craft\u6846\u67b6\u901a\u8fc7\u7ed3\u54084D\u51e0\u4f55\u611f\u77e5\u3001\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u548c\u667a\u80fd\u7528\u6237\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u5347\u4e864D\u9ad8\u65af\u6e85\u5c04\u7f16\u8f91\u7684\u8d28\u91cf\u548c\u53ef\u7528\u6027\uff0c\u4e3a\u590d\u67424D\u573a\u666f\u7f16\u8f91\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.01997", "pdf": "https://arxiv.org/pdf/2510.01997", "abs": "https://arxiv.org/abs/2510.01997", "authors": ["Junyu Wu", "Jie Tang", "Jie Liu", "Gangshan Wu"], "title": "Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Image Super-Resolution (SR) aims to reconstruct high-resolution images from low-resolution counterparts, but the computational complexity of deep learning-based methods often hinders practical deployment. CAMixer is the pioneering work to integrate the advantages of existing lightweight SR methods and proposes a content-aware mixer to route token mixers of varied complexities according to the difficulty of content recovery. However, several limitations remain, such as poor adaptability, coarse-grained masking and spatial inflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking mechanism that identifies pure pixels and exempts them from expensive computations. PP utilizes fixed color center points to classify pixels into distinct categories, enabling fine-grained, spatially flexible masking while maintaining adaptive flexibility. Integrated into the state-of-the-art ATD-light model, PP-ATD-light achieves superior SR performance with minimal overhead, outperforming CAMixer-ATD-light in reconstruction quality and parameter efficiency when saving a similar amount of computation.", "AI": {"tldr": "\u63d0\u51faPure-Pass(PP)\u50cf\u7d20\u7ea7\u63a9\u7801\u673a\u5236\uff0c\u901a\u8fc7\u56fa\u5b9a\u989c\u8272\u4e2d\u5fc3\u70b9\u5206\u7c7b\u50cf\u7d20\uff0c\u514d\u9664\u7eaf\u50cf\u7d20\u7684\u6602\u8d35\u8ba1\u7b97\uff0c\u96c6\u6210\u5230ATD-light\u6a21\u578b\u4e2d\u5b9e\u73b0\u9ad8\u6548\u8d85\u5206\u8fa8\u7387\u91cd\u5efa", "motivation": "\u73b0\u6709\u8f7b\u91cf\u7ea7\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5982CAMixer\u5b58\u5728\u9002\u5e94\u6027\u5dee\u3001\u63a9\u7801\u7c92\u5ea6\u7c97\u548c\u7a7a\u95f4\u7075\u6d3b\u6027\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u8ba1\u7b97\u4f18\u5316", "method": "\u4f7f\u7528\u56fa\u5b9a\u989c\u8272\u4e2d\u5fc3\u70b9\u5bf9\u50cf\u7d20\u8fdb\u884c\u5206\u7c7b\uff0c\u8bc6\u522b\u7eaf\u50cf\u7d20\u5e76\u514d\u9664\u5176\u6602\u8d35\u8ba1\u7b97\uff0c\u5b9e\u73b0\u50cf\u7d20\u7ea7\u7ec6\u7c92\u5ea6\u63a9\u7801\uff0c\u4fdd\u6301\u81ea\u9002\u5e94\u7075\u6d3b\u6027", "result": "PP-ATD-light\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u53c2\u6570\u6548\u7387\u4e0a\u4f18\u4e8eCAMixer-ATD-light\uff0c\u5728\u8282\u7701\u76f8\u4f3c\u8ba1\u7b97\u91cf\u7684\u60c5\u51b5\u4e0b\u83b7\u5f97\u66f4\u4f18\u6027\u80fd", "conclusion": "Pure-Pass\u673a\u5236\u901a\u8fc7\u50cf\u7d20\u7ea7\u63a9\u7801\u6709\u6548\u4f18\u5316\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4e3a\u8f7b\u91cf\u7ea7\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.02034", "pdf": "https://arxiv.org/pdf/2510.02034", "abs": "https://arxiv.org/abs/2510.02034", "authors": ["Mengtian Li", "Yunshu Bai", "Yimin Chu", "Yijun Shen", "Zhongmei Li", "Weifeng Ge", "Zhifeng Xie", "Chaofeng Chen"], "title": "GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing", "categories": ["cs.CV"], "comment": "Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/", "summary": "We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape and texture morphing from multi-view images. Previous approaches usually rely on point clouds or require pre-defined homeomorphic mappings for untextured data. Our method overcomes these limitations by leveraging mesh-guided 3D Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling. The core of our framework is a unified deformation strategy that anchors 3DGaussians to reconstructed mesh patches, ensuring geometrically consistent transformations while preserving texture fidelity through topology-aware constraints. In parallel, our framework establishes unsupervised semantic correspondence by using the mesh topology as a geometric prior and maintains structural integrity via physically plausible point trajectories. This integrated approach preserves both local detail and global semantic coherence throughout the morphing process with out requiring labeled data. On our proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior 2D/3D methods, reducing color consistency error ($\\Delta E$) by 22.2% and EI by 26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/", "AI": {"tldr": "GaussianMorphing\u662f\u4e00\u4e2a\u4ece\u591a\u89c6\u89d2\u56fe\u50cf\u8fdb\u884c\u8bed\u4e49\u611f\u77e53D\u5f62\u72b6\u548c\u7eb9\u7406\u53d8\u5f62\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7f51\u683c\u5f15\u5bfc\u76843D\u9ad8\u65af\u6cfc\u6e85\u5b9e\u73b0\u9ad8\u4fdd\u771f\u51e0\u4f55\u548c\u5916\u89c2\u5efa\u6a21\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u5373\u53ef\u4fdd\u6301\u5c40\u90e8\u7ec6\u8282\u548c\u5168\u5c40\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u70b9\u4e91\u6216\u9700\u8981\u9884\u5b9a\u4e49\u540c\u80da\u6620\u5c04\u6765\u5904\u7406\u65e0\u7eb9\u7406\u6570\u636e\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002\u672c\u6587\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u5b9e\u73b0\u51e0\u4f55\u4e00\u81f4\u4e14\u7eb9\u7406\u4fdd\u771f\u76843D\u53d8\u5f62\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u53d8\u5f62\u7b56\u7565\uff0c\u5c063D\u9ad8\u65af\u951a\u5b9a\u5230\u91cd\u5efa\u7684\u7f51\u683c\u9762\u7247\u4e0a\uff0c\u901a\u8fc7\u62d3\u6251\u611f\u77e5\u7ea6\u675f\u786e\u4fdd\u51e0\u4f55\u4e00\u81f4\u53d8\u6362\u548c\u7eb9\u7406\u4fdd\u771f\u5ea6\uff1b\u540c\u65f6\u5229\u7528\u7f51\u683c\u62d3\u6251\u4f5c\u4e3a\u51e0\u4f55\u5148\u9a8c\u5efa\u7acb\u65e0\u76d1\u7763\u8bed\u4e49\u5bf9\u5e94\uff0c\u5e76\u901a\u8fc7\u7269\u7406\u5408\u7406\u7684\u70b9\u8f68\u8ff9\u4fdd\u6301\u7ed3\u6784\u5b8c\u6574\u6027\u3002", "result": "\u5728\u63d0\u51fa\u7684TexMorph\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGaussianMorphing\u663e\u8457\u4f18\u4e8e\u73b0\u67092D/3D\u65b9\u6cd5\uff0c\u5c06\u989c\u8272\u4e00\u81f4\u6027\u8bef\u5dee(\u0394E)\u964d\u4f4e\u4e8622.2%\uff0cEI\u964d\u4f4e\u4e8626.2%\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u9700\u6807\u6ce8\u6570\u636e\u7684\u8bed\u4e49\u611f\u77e53D\u5f62\u72b6\u548c\u7eb9\u7406\u53d8\u5f62\uff0c\u5728\u4fdd\u6301\u5c40\u90e8\u7ec6\u8282\u548c\u5168\u5c40\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53d8\u5f62\u8d28\u91cf\u3002"}}
{"id": "2510.02043", "pdf": "https://arxiv.org/pdf/2510.02043", "abs": "https://arxiv.org/abs/2510.02043", "authors": ["Sahil Bhandary Karnoor", "Romit Roy Choudhury"], "title": "Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers", "categories": ["cs.CV", "cs.HC", "cs.LG"], "comment": null, "summary": "Pose estimation refers to tracking a human's full body posture, including their head, torso, arms, and legs. The problem is challenging in practical settings where the number of body sensors are limited. Past work has shown promising results using conditional diffusion models, where the pose prediction is conditioned on both <location, rotation> measurements from the sensors. Unfortunately, nearly all these approaches generalize poorly across users, primarly because location measurements are highly influenced by the body size of the user. In this paper, we formulate pose estimation as an inverse problem and design an algorithm capable of zero-shot generalization. Our idea utilizes a pre-trained diffusion model and conditions it on rotational measurements alone; the priors from this model are then guided by a likelihood term, derived from the measured locations. Thus, given any user, our proposed InPose method generatively estimates the highly likely sequence of poses that best explains the sparse on-body measurements.", "AI": {"tldr": "\u63d0\u51faInPose\u65b9\u6cd5\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u548c\u4ec5\u65cb\u8f6c\u6d4b\u91cf\u8fdb\u884c\u96f6\u6837\u672c\u6cdb\u5316\u7684\u59ff\u6001\u4f30\u8ba1", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\u5728\u7528\u6237\u95f4\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u4e3b\u8981\u56e0\u4e3a\u4f4d\u7f6e\u6d4b\u91cf\u53d7\u7528\u6237\u4f53\u578b\u5f71\u54cd\u5927", "method": "\u5c06\u59ff\u6001\u4f30\u8ba1\u5efa\u6a21\u4e3a\u9006\u95ee\u9898\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4ec5\u57fa\u4e8e\u65cb\u8f6c\u6d4b\u91cf\uff0c\u901a\u8fc7\u4f3c\u7136\u9879\u5f15\u5bfc\u4f4d\u7f6e\u6d4b\u91cf", "result": "InPose\u65b9\u6cd5\u80fd\u591f\u96f6\u6837\u672c\u6cdb\u5316\u5230\u4efb\u610f\u7528\u6237\uff0c\u751f\u6210\u6700\u53ef\u80fd\u89e3\u91ca\u7a00\u758f\u8eab\u4f53\u6d4b\u91cf\u7684\u59ff\u6001\u5e8f\u5217", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5206\u79bb\u65cb\u8f6c\u548c\u4f4d\u7f6e\u6d4b\u91cf\u5904\u7406\uff0c\u89e3\u51b3\u4e86\u7528\u6237\u95f4\u6cdb\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u59ff\u6001\u4f30\u8ba1"}}
{"id": "2510.02086", "pdf": "https://arxiv.org/pdf/2510.02086", "abs": "https://arxiv.org/abs/2510.02086", "authors": ["Arman Behnam"], "title": "VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate detection and segmentation of brain tumors from magnetic resonance imaging (MRI) are essential for diagnosis, treatment planning, and clinical monitoring. While convolutional architectures such as U-Net have long been the backbone of medical image segmentation, their limited capacity to capture long-range dependencies constrains performance on complex tumor structures. Recent advances in diffusion models have demonstrated strong potential for generating high-fidelity medical images and refining segmentation boundaries.   In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation framework, a transformer-driven diffusion framework for brain tumor detection and segmentation. By embedding a vision transformer at the core of the diffusion process, the model leverages global contextual reasoning together with iterative denoising to enhance both volumetric accuracy and boundary precision. The transformer backbone enables more effective modeling of spatial relationships across entire MRI volumes, while diffusion refinement mitigates voxel-level errors and recovers fine-grained tumor details.   This hybrid design provides a pathway toward improved robustness and scalability in neuro-oncology, moving beyond conventional U-Net baselines. Experimental validation on MRI brain tumor datasets demonstrates consistent gains in Dice similarity and Hausdorff distance, underscoring the potential of transformer-guided diffusion models to advance the state of the art in tumor segmentation.", "AI": {"tldr": "\u63d0\u51faVGDM\uff1a\u57fa\u4e8e\u89c6\u89c9\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u7684\u8111\u80bf\u7624\u68c0\u6d4b\u4e0e\u5206\u5272\u6846\u67b6\uff0c\u7ed3\u5408Transformer\u548c\u6269\u6563\u6a21\u578b\u63d0\u5347\u8111\u80bf\u7624\u5206\u5272\u6027\u80fd", "motivation": "\u4f20\u7edf\u5377\u79ef\u67b6\u6784\u5982U-Net\u5728\u6355\u83b7\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u9650\u5236\u4e86\u5728\u590d\u6742\u80bf\u7624\u7ed3\u6784\u4e0a\u7684\u6027\u80fd\u3002\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u4fdd\u771f\u533b\u5b66\u56fe\u50cf\u548c\u7ec6\u5316\u5206\u5272\u8fb9\u754c\u65b9\u9762\u663e\u793a\u51fa\u5f3a\u5927\u6f5c\u529b", "method": "\u5728\u6269\u6563\u8fc7\u7a0b\u6838\u5fc3\u5d4c\u5165\u89c6\u89c9Transformer\uff0c\u5229\u7528\u5168\u5c40\u4e0a\u4e0b\u6587\u63a8\u7406\u4e0e\u8fed\u4ee3\u53bb\u566a\u76f8\u7ed3\u5408\uff0c\u589e\u5f3a\u4f53\u79ef\u7cbe\u5ea6\u548c\u8fb9\u754c\u7cbe\u5ea6\u3002Transformer\u4e3b\u5e72\u80fd\u591f\u66f4\u6709\u6548\u5730\u5efa\u6a21\u6574\u4e2aMRI\u4f53\u79ef\u7684\u7a7a\u95f4\u5173\u7cfb\uff0c\u6269\u6563\u7ec6\u5316\u51cf\u8f7b\u4f53\u7d20\u7ea7\u8bef\u5dee\u5e76\u6062\u590d\u7ec6\u7c92\u5ea6\u80bf\u7624\u7ec6\u8282", "result": "\u5728MRI\u8111\u80bf\u7624\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\uff0c\u5728Dice\u76f8\u4f3c\u6027\u548cHausdorff\u8ddd\u79bb\u6307\u6807\u4e0a\u6301\u7eed\u63d0\u5347", "conclusion": "\u8fd9\u79cd\u6df7\u5408\u8bbe\u8ba1\u4e3a\u795e\u7ecf\u80bf\u7624\u5b66\u4e2d\u6539\u8fdb\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u63d0\u4f9b\u4e86\u9014\u5f84\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684U-Net\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86Transformer\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u5728\u63a8\u8fdb\u80bf\u7624\u5206\u5272\u6280\u672f\u524d\u6cbf\u7684\u6f5c\u529b"}}
{"id": "2510.02226", "pdf": "https://arxiv.org/pdf/2510.02226", "abs": "https://arxiv.org/abs/2510.02226", "authors": ["Shira Schiber", "Ofir Lindenbaum", "Idan Schwartz"], "title": "TempoControl: Temporal Attention Guidance for Text-to-Video Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Under Review", "summary": "Recent advances in generative video models have enabled the creation of high-quality videos based on natural language prompts. However, these models frequently lack fine-grained temporal control, meaning they do not allow users to specify when particular visual elements should appear within a generated sequence. In this work, we introduce TempoControl, a method that allows for temporal alignment of visual concepts during inference, without requiring retraining or additional supervision. TempoControl utilizes cross-attention maps, a key component of text-to-video diffusion models, to guide the timing of concepts through a novel optimization approach. Our method steers attention using three complementary principles: aligning its temporal shape with a control signal (via correlation), amplifying it where visibility is needed (via energy), and maintaining spatial focus (via entropy). TempoControl allows precise control over timing while ensuring high video quality and diversity. We demonstrate its effectiveness across various video generation applications, including temporal reordering for single and multiple objects, as well as action and audio-aligned generation.", "AI": {"tldr": "TempoControl\u662f\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u989d\u5916\u76d1\u7763\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u56fe\uff0c\u5b9e\u73b0\u89c6\u89c9\u6982\u5ff5\u7684\u65f6\u95f4\u5bf9\u9f50\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u89c6\u9891\u6a21\u578b\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u65f6\u95f4\u63a7\u5236\u80fd\u529b\uff0c\u65e0\u6cd5\u8ba9\u7528\u6237\u6307\u5b9a\u7279\u5b9a\u89c6\u89c9\u5143\u7d20\u5728\u751f\u6210\u5e8f\u5217\u4e2d\u51fa\u73b0\u7684\u65f6\u95f4\u3002", "method": "\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u56fe\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u4f18\u5316\u65b9\u6cd5\u5f15\u5bfc\u6982\u5ff5\u65f6\u5e8f\uff0c\u91c7\u7528\u4e09\u4e2a\u4e92\u8865\u539f\u5219\uff1a\u901a\u8fc7\u76f8\u5173\u6027\u5bf9\u9f50\u65f6\u95f4\u5f62\u72b6\u3001\u901a\u8fc7\u80fd\u91cf\u653e\u5927\u53ef\u89c1\u6027\u533a\u57df\u3001\u901a\u8fc7\u71b5\u4fdd\u6301\u7a7a\u95f4\u805a\u7126\u3002", "result": "TempoControl\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u65f6\u95f4\u63a7\u5236\uff0c\u540c\u65f6\u786e\u4fdd\u9ad8\u8d28\u91cf\u548c\u591a\u6837\u6027\u7684\u89c6\u9891\u751f\u6210\uff0c\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u8868\u73b0\u6709\u6548\uff0c\u5305\u62ec\u5355/\u591a\u5bf9\u8c61\u65f6\u95f4\u91cd\u6392\u5e8f\u3001\u52a8\u4f5c\u548c\u97f3\u9891\u5bf9\u9f50\u751f\u6210\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u751f\u6210\u89c6\u9891\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u7684\u65f6\u95f4\u63a7\u5236\u80fd\u529b\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.02253", "pdf": "https://arxiv.org/pdf/2510.02253", "abs": "https://arxiv.org/abs/2510.02253", "authors": ["Zihan Zhou", "Shilin Lu", "Shuli Leng", "Shaocong Zhang", "Zhuming Lian", "Xinlei Yu", "Adams Wai-Kin Kong"], "title": "DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Preprint", "summary": "Drag-based image editing has long suffered from distortions in the target region, largely because the priors of earlier base models, Stable Diffusion, are insufficient to project optimized latents back onto the natural image manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow matching (e.g., SD3.5, FLUX), generative priors have become significantly stronger, enabling advances across diverse editing tasks. However, drag-based editing has yet to benefit from these stronger priors. This work proposes the first framework to effectively harness FLUX's rich prior for drag-based editing, dubbed DragFlow, achieving substantial gains over baselines. We first show that directly applying point-based drag editing to DiTs performs poorly: unlike the highly compressed features of UNets, DiT features are insufficiently structured to provide reliable guidance for point-wise motion supervision. To overcome this limitation, DragFlow introduces a region-based editing paradigm, where affine transformations enable richer and more consistent feature supervision. Additionally, we integrate pretrained open-domain personalization adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving background fidelity through gradient mask-based hard constraints. Multimodal large language models (MLLMs) are further employed to resolve task ambiguities. For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench) featuring region-level dragging instructions. Extensive experiments on DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and region-based baselines, setting a new state-of-the-art in drag-based image editing. Code and datasets will be publicly available upon publication.", "AI": {"tldr": "DragFlow\u662f\u9996\u4e2a\u6709\u6548\u5229\u7528FLUX\u5f3a\u5927\u5148\u9a8c\u8fdb\u884c\u62d6\u62fd\u5f0f\u56fe\u50cf\u7f16\u8f91\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u533a\u57df\u5316\u7f16\u8f91\u8303\u5f0f\u3001\u4e2a\u6027\u5316\u9002\u914d\u5668\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6a21\u578b\u4eceUNet\u8f6c\u5411DiT\uff0c\u751f\u6210\u5148\u9a8c\u663e\u8457\u589e\u5f3a\uff0c\u4f46\u62d6\u62fd\u5f0f\u7f16\u8f91\u5c1a\u672a\u53d7\u76ca\u4e8e\u8fd9\u4e9b\u66f4\u5f3a\u5148\u9a8c\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u76ee\u6807\u533a\u57df\u5b58\u5728\u5931\u771f\u95ee\u9898\uff0c\u9700\u8981\u6709\u6548\u5229\u7528\u65b0\u6a21\u578b\u7684\u5f3a\u5927\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u533a\u57df\u5316\u7f16\u8f91\u8303\u5f0f\uff0c\u4f7f\u7528\u4eff\u5c04\u53d8\u6362\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u7279\u5f81\u76d1\u7763\uff1b\u96c6\u6210\u9884\u8bad\u7ec3\u4e2a\u6027\u5316\u9002\u914d\u5668\u589e\u5f3a\u4e3b\u4f53\u4e00\u81f4\u6027\uff1b\u91c7\u7528\u68af\u5ea6\u63a9\u7801\u786c\u7ea6\u675f\u4fdd\u62a4\u80cc\u666f\uff1b\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u4efb\u52a1\u6b67\u4e49\u3002", "result": "\u5728DragBench-DR\u548cReD Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDragFlow\u8d85\u8d8a\u4e86\u57fa\u4e8e\u70b9\u548c\u57fa\u4e8e\u533a\u57df\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u62d6\u62fd\u5f0f\u56fe\u50cf\u7f16\u8f91\u4e2d\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "DragFlow\u6210\u529f\u5229\u7528FLUX\u7684\u5f3a\u5927\u5148\u9a8c\uff0c\u901a\u8fc7\u533a\u57df\u5316\u7f16\u8f91\u548c\u591a\u79cd\u6280\u672f\u96c6\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u62d6\u62fd\u5f0f\u56fe\u50cf\u7f16\u8f91\u7684\u8d28\u91cf\u548c\u6548\u679c\u3002"}}
{"id": "2510.02266", "pdf": "https://arxiv.org/pdf/2510.02266", "abs": "https://arxiv.org/abs/2510.02266", "authors": ["Shiyi Zhang", "Dong Liang", "Yihang Zhou"], "title": "NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Reconstructing visual information from brain activity via computer vision technology provides an intuitive understanding of visual neural mechanisms. Despite progress in decoding fMRI data with generative models, achieving accurate cross-subject reconstruction of visual stimuli remains challenging and computationally demanding. This difficulty arises from inter-subject variability in neural representations and the brain's abstract encoding of core semantic features in complex visual inputs. To address these challenges, we propose NeuroSwift, which integrates complementary adapters via diffusion: AutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter is trained on Stable Diffusion generated images paired with COCO captions to emulate higher visual cortex encoding. For cross-subject generalization, we pretrain on one subject and then fine-tune only 17 percent of parameters (fully connected layers) for new subjects, while freezing other components. This enables state-of-the-art performance with only one hour of training per subject on lightweight GPUs (three RTX 4090), and it outperforms existing methods.", "AI": {"tldr": "NeuroSwift\u662f\u4e00\u4e2a\u7528\u4e8e\u4ecefMRI\u6570\u636e\u91cd\u5efa\u89c6\u89c9\u4fe1\u606f\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u96c6\u6210AutoKL\u548cCLIP\u9002\u914d\u5668\u6765\u5904\u7406\u4f4e\u5c42\u7279\u5f81\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u8de8\u88ab\u8bd5\u7684\u5feb\u901f\u8bad\u7ec3\u548c\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4ece\u8111\u6d3b\u52a8\u91cd\u5efa\u89c6\u89c9\u4fe1\u606f\u65f6\u9762\u4e34\u7684\u8de8\u88ab\u8bd5\u5dee\u5f02\u5927\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u4ee5\u53ca\u5927\u8111\u5bf9\u590d\u6742\u89c6\u89c9\u8f93\u5165\u7684\u62bd\u8c61\u8bed\u4e49\u7f16\u7801\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faNeuroSwift\u6a21\u578b\uff0c\u96c6\u6210AutoKL\u9002\u914d\u5668\u5904\u7406\u4f4e\u5c42\u7279\u5f81\u548cCLIP\u9002\u914d\u5668\u5904\u7406\u8bed\u4e49\u4fe1\u606f\u3002CLIP\u9002\u914d\u5668\u5728Stable Diffusion\u751f\u6210\u56fe\u50cf\u548cCOCO\u6807\u9898\u5bf9\u4e0a\u8bad\u7ec3\u4ee5\u6a21\u62df\u9ad8\u7ea7\u89c6\u89c9\u76ae\u5c42\u7f16\u7801\u3002\u91c7\u7528\u9884\u8bad\u7ec3\u52a0\u5fae\u8c03\u7b56\u7565\uff0c\u4ec5\u5fae\u8c0317%\u53c2\u6570\u5b9e\u73b0\u8de8\u88ab\u8bd5\u6cdb\u5316\u3002", "result": "\u5728\u8f7b\u91cf\u7ea7GPU\uff08\u4e09\u5757RTX 4090\uff09\u4e0a\u6bcf\u4e2a\u88ab\u8bd5\u4ec5\u97001\u5c0f\u65f6\u8bad\u7ec3\uff0c\u5373\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "NeuroSwift\u901a\u8fc7\u4e92\u8865\u9002\u914d\u5668\u96c6\u6210\u548c\u9ad8\u6548\u7684\u8de8\u88ab\u8bd5\u8bad\u7ec3\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4ecefMRI\u6570\u636e\u91cd\u5efa\u89c6\u89c9\u4fe1\u606f\u7684\u6838\u5fc3\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u7684\u5e73\u8861\u3002"}}
{"id": "2510.02283", "pdf": "https://arxiv.org/pdf/2510.02283", "abs": "https://arxiv.org/abs/2510.02283", "authors": ["Justin Cui", "Jie Wu", "Ming Li", "Tao Yang", "Xiaojie Li", "Rui Wang", "Andrew Bai", "Yuanhao Ban", "Cho-Jui Hsieh"], "title": "Self-Forcing++: Towards Minute-Scale High-Quality Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "preprint", "summary": "Diffusion models have revolutionized image and video generation, achieving unprecedented visual quality. However, their reliance on transformer architectures incurs prohibitively high computational costs, particularly when extending generation to long videos. Recent work has explored autoregressive formulations for long video generation, typically by distilling from short-horizon bidirectional teachers. Nevertheless, given that teacher models cannot synthesize long videos, the extrapolation of student models beyond their training horizon often leads to pronounced quality degradation, arising from the compounding of errors within the continuous latent space. In this paper, we propose a simple yet effective approach to mitigate quality degradation in long-horizon video generation without requiring supervision from long-video teachers or retraining on long video datasets. Our approach centers on exploiting the rich knowledge of teacher models to provide guidance for the student model through sampled segments drawn from self-generated long videos. Our method maintains temporal consistency while scaling video length by up to 20x beyond teacher's capability, avoiding common issues such as over-exposure and error-accumulation without recomputing overlapping frames like previous methods. When scaling up the computation, our method shows the capability of generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the maximum span supported by our base model's position embedding and more than 50x longer than that of our baseline model. Experiments on standard benchmarks and our proposed improved benchmark demonstrate that our approach substantially outperforms baseline methods in both fidelity and consistency. Our long-horizon videos demo can be found at https://self-forcing-plus-plus.github.io/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\u6765\u7f13\u89e3\u957f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u8d28\u91cf\u9000\u5316\u95ee\u9898\uff0c\u65e0\u9700\u957f\u89c6\u9891\u6559\u5e08\u76d1\u7763\u6216\u91cd\u65b0\u8bad\u7ec3\u957f\u89c6\u9891\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5229\u7528\u6559\u5e08\u6a21\u578b\u7684\u4e30\u5bcc\u77e5\u8bc6\u4e3a\u5b66\u751f\u6a21\u578b\u63d0\u4f9b\u6307\u5bfc\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u9769\u547d\u6027\u8fdb\u5c55\uff0c\u4f46\u57fa\u4e8etransformer\u67b6\u6784\u7684\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u7279\u522b\u662f\u5728\u751f\u6210\u957f\u89c6\u9891\u65f6\u3002\u73b0\u6709\u7684\u81ea\u56de\u5f52\u65b9\u6cd5\u5728\u8d85\u51fa\u8bad\u7ec3\u8303\u56f4\u65f6\u4f1a\u51fa\u73b0\u660e\u663e\u7684\u8d28\u91cf\u9000\u5316\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4ece\u81ea\u751f\u6210\u957f\u89c6\u9891\u4e2d\u62bd\u53d6\u7247\u6bb5\uff0c\u5229\u7528\u6559\u5e08\u6a21\u578b\u7684\u77e5\u8bc6\u4e3a\u5b66\u751f\u6a21\u578b\u63d0\u4f9b\u6307\u5bfc\uff0c\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u907f\u514d\u8fc7\u66dd\u5149\u548c\u9519\u8bef\u7d2f\u79ef\u95ee\u9898\u3002", "result": "\u65b9\u6cd5\u80fd\u591f\u5c06\u89c6\u9891\u957f\u5ea6\u6269\u5c55\u5230\u6559\u5e08\u6a21\u578b\u80fd\u529b\u768420\u500d\uff0c\u751f\u6210\u6700\u957f4\u520615\u79d2\u7684\u89c6\u9891\uff0c\u76f8\u5f53\u4e8e\u57fa\u7840\u6a21\u578b\u4f4d\u7f6e\u5d4c\u5165\u652f\u6301\u7684\u6700\u5927\u8de8\u5ea6\u768499.9%\uff0c\u6bd4\u57fa\u7ebf\u6a21\u578b\u957f50\u591a\u500d\u3002\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b9\u6cd5\u5728\u4fdd\u771f\u5ea6\u548c\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u8d28\u91cf\u9000\u5316\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u76d1\u7763\u6216\u91cd\u65b0\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u957f\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2510.02284", "pdf": "https://arxiv.org/pdf/2510.02284", "abs": "https://arxiv.org/abs/2510.02284", "authors": ["David Romero", "Ariana Bermudez", "Hao Li", "Fabio Pizzati", "Ivan Laptev"], "title": "Learning to Generate Object Interactions with Physics-Guided Video Diffusion", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent models for video generation have achieved remarkable progress and are now deployed in film, social media production, and advertising. Beyond their creative potential, such models also hold promise as world simulators for robotics and embodied decision making. Despite strong advances, however, current approaches still struggle to generate physically plausible object interactions and lack physics-grounded control mechanisms. To address this limitation, we introduce KineMask, an approach for physics-guided video generation that enables realistic rigid body control, interactions, and effects. Given a single image and a specified object velocity, our method generates videos with inferred motions and future object interactions. We propose a two-stage training strategy that gradually removes future motion supervision via object masks. Using this strategy we train video diffusion models (VDMs) on synthetic scenes of simple interactions and demonstrate significant improvements of object interactions in real scenes. Furthermore, KineMask integrates low-level motion control with high-level textual conditioning via predictive scene descriptions, leading to effective support for synthesis of complex dynamical phenomena. Extensive experiments show that KineMask achieves strong improvements over recent models of comparable size. Ablation studies further highlight the complementary roles of low- and high-level conditioning in VDMs. Our code, model, and data will be made publicly available.", "AI": {"tldr": "KineMask\u662f\u4e00\u79cd\u7269\u7406\u5f15\u5bfc\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u5b9e\u73b0\u903c\u771f\u7684\u521a\u4f53\u63a7\u5236\u3001\u4ea4\u4e92\u548c\u6548\u679c\uff0c\u663e\u8457\u6539\u5584\u4e86\u7269\u4f53\u4ea4\u4e92\u7684\u771f\u5b9e\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u7269\u7406\u5408\u7406\u7684\u7269\u4f53\u4ea4\u4e92\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u7f3a\u4e4f\u57fa\u4e8e\u7269\u7406\u7684\u63a7\u5236\u673a\u5236\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6a21\u62df\u771f\u5b9e\u7269\u7406\u4ea4\u4e92\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u7269\u4f53\u63a9\u7801\u9010\u6b65\u79fb\u9664\u672a\u6765\u8fd0\u52a8\u76d1\u7763\uff0c\u5728\u5408\u6210\u573a\u666f\u4e0a\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u4f4e\u7ea7\u8fd0\u52a8\u63a7\u5236\u4e0e\u9ad8\u7ea7\u6587\u672c\u6761\u4ef6\u3002", "result": "\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u663e\u8457\u6539\u5584\u4e86\u7269\u4f53\u4ea4\u4e92\u6548\u679c\uff0c\u76f8\u6bd4\u540c\u7c7b\u89c4\u6a21\u6a21\u578b\u6709\u660e\u663e\u63d0\u5347\uff0c\u80fd\u591f\u5408\u6210\u590d\u6742\u7684\u52a8\u529b\u5b66\u73b0\u8c61\u3002", "conclusion": "KineMask\u901a\u8fc7\u7269\u7406\u5f15\u5bfc\u7684\u89c6\u9891\u751f\u6210\u5b9e\u73b0\u4e86\u903c\u771f\u7684\u521a\u4f53\u63a7\u5236\u548c\u4ea4\u4e92\uff0c\u9ad8\u4f4e\u7ea7\u6761\u4ef6\u5728\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u5177\u6709\u4e92\u8865\u4f5c\u7528\u3002"}}
{"id": "2510.02287", "pdf": "https://arxiv.org/pdf/2510.02287", "abs": "https://arxiv.org/abs/2510.02287", "authors": ["Yichen Li", "Antonio Torralba"], "title": "MultiModal Action Conditioned Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Current video models fail as world model as they lack fine-graiend control. General-purpose household robots require real-time fine motor control to handle delicate tasks and urgent situations. In this work, we introduce fine-grained multimodal actions to capture such precise control. We consider senses of proprioception, kinesthesia, force haptics, and muscle activation. Such multimodal senses naturally enables fine-grained interactions that are difficult to simulate with text-conditioned generative models. To effectively simulate fine-grained multisensory actions, we develop a feature learning paradigm that aligns these modalities while preserving the unique information each modality provides. We further propose a regularization scheme to enhance causality of the action trajectory features in representing intricate interaction dynamics. Experiments show that incorporating multimodal senses improves simulation accuracy and reduces temporal drift. Extensive ablation studies and downstream applications demonstrate the effectiveness and practicality of our work.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u7cbe\u7ec6\u52a8\u4f5c\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u672c\u4f53\u611f\u89c9\u3001\u52a8\u89c9\u3001\u529b\u89e6\u89c9\u548c\u808c\u8089\u6fc0\u6d3b\u7b49\u591a\u79cd\u611f\u5b98\uff0c\u63d0\u5347\u89c6\u9891\u6a21\u578b\u5728\u7cbe\u7ec6\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u6a21\u578b\u7f3a\u4e4f\u7cbe\u7ec6\u63a7\u5236\u80fd\u529b\uff0c\u65e0\u6cd5\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\u4f7f\u7528\u3002\u901a\u7528\u5bb6\u7528\u673a\u5668\u4eba\u9700\u8981\u5b9e\u65f6\u7cbe\u7ec6\u8fd0\u52a8\u63a7\u5236\u6765\u5904\u7406\u7cbe\u7ec6\u4efb\u52a1\u548c\u7d27\u6025\u60c5\u51b5\u3002", "method": "\u5f15\u5165\u591a\u6a21\u6001\u7cbe\u7ec6\u52a8\u4f5c\uff0c\u5f00\u53d1\u7279\u5f81\u5b66\u4e60\u8303\u5f0f\u5bf9\u9f50\u4e0d\u540c\u6a21\u6001\uff0c\u540c\u65f6\u4fdd\u7559\u5404\u6a21\u6001\u72ec\u7279\u4fe1\u606f\uff1b\u63d0\u51fa\u6b63\u5219\u5316\u65b9\u6848\u589e\u5f3a\u52a8\u4f5c\u8f68\u8ff9\u7279\u5f81\u7684\u56e0\u679c\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6574\u5408\u591a\u6a21\u6001\u611f\u5b98\u63d0\u9ad8\u4e86\u6a21\u62df\u7cbe\u5ea6\uff0c\u51cf\u5c11\u4e86\u65f6\u95f4\u6f02\u79fb\u3002\u5e7f\u6cdb\u7684\u6d88\u878d\u7814\u7a76\u548c\u4e0b\u6e38\u5e94\u7528\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u591a\u6a21\u6001\u611f\u5b98\u6574\u5408\u80fd\u591f\u6709\u6548\u63d0\u5347\u7cbe\u7ec6\u63a7\u5236\u4efb\u52a1\u7684\u6a21\u62df\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u7cbe\u7ec6\u8fd0\u52a8\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.02307", "pdf": "https://arxiv.org/pdf/2510.02307", "abs": "https://arxiv.org/abs/2510.02307", "authors": ["Ruozhen He", "Moayed Haji-Ali", "Ziyan Yang", "Vicente Ordonez"], "title": "NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-image diffusion models trained on a fixed set of resolutions often fail to generalize, even when asked to generate images at lower resolutions than those seen during training. High-resolution text-to-image generators are currently unable to easily offer an out-of-the-box budget-efficient alternative to their users who might not need high-resolution images. We identify a key technical insight in diffusion models that when addressed can help tackle this limitation: Noise schedulers have unequal perceptual effects across resolutions. The same level of noise removes disproportionately more signal from lower-resolution images than from high-resolution images, leading to a train-test mismatch. We propose NoiseShift, a training-free method that recalibrates the noise level of the denoiser conditioned on resolution size. NoiseShift requires no changes to model architecture or sampling schedule and is compatible with existing models. When applied to Stable Diffusion 3, Stable Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by 10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent artifacts and enhancing the quality of low-resolution image generation.", "AI": {"tldr": "NoiseShift\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6839\u636e\u5206\u8fa8\u7387\u5927\u5c0f\u91cd\u65b0\u6821\u51c6\u53bb\u566a\u5668\u7684\u566a\u58f0\u6c34\u5e73\uff0c\u663e\u8457\u63d0\u5347\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u5668\u65e0\u6cd5\u4e3a\u4e0d\u9700\u8981\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u7528\u6237\u63d0\u4f9b\u5f00\u7bb1\u5373\u7528\u7684\u7ecf\u6d4e\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u56e0\u4e3a\u6269\u6563\u6a21\u578b\u5728\u56fa\u5b9a\u5206\u8fa8\u7387\u96c6\u4e0a\u8bad\u7ec3\u540e\u5f80\u5f80\u65e0\u6cd5\u6cdb\u5316\u5230\u5176\u4ed6\u5206\u8fa8\u7387\u3002", "method": "\u8bc6\u522b\u566a\u58f0\u8c03\u5ea6\u5668\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u4e0b\u5177\u6709\u4e0d\u7b49\u7684\u611f\u77e5\u6548\u679c\uff0c\u63d0\u51faNoiseShift\u65b9\u6cd5\u91cd\u65b0\u6821\u51c6\u6761\u4ef6\u4e8e\u5206\u8fa8\u7387\u5927\u5c0f\u7684\u53bb\u566a\u5668\u566a\u58f0\u6c34\u5e73\uff0c\u65e0\u9700\u6539\u53d8\u6a21\u578b\u67b6\u6784\u6216\u91c7\u6837\u8ba1\u5212\u3002", "result": "\u5728LAION-COCO\u6570\u636e\u96c6\u4e0a\uff0cNoiseShift\u5c06SD3.5\u7684FID\u63d0\u534715.89%\uff0cSD3\u63d0\u53478.56%\uff0cFlux-Dev\u63d0\u53472.44%\uff1b\u5728CelebA\u6570\u636e\u96c6\u4e0a\uff0c\u5206\u522b\u63d0\u534710.36%\u30015.19%\u548c3.02%\u3002", "conclusion": "NoiseShift\u80fd\u6709\u6548\u7f13\u89e3\u5206\u8fa8\u7387\u76f8\u5173\u7684\u4f2a\u5f71\uff0c\u63d0\u5347\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u8d28\u91cf\uff0c\u4e14\u4e0e\u73b0\u6709\u6a21\u578b\u517c\u5bb9\u3002"}}
{"id": "2510.02314", "pdf": "https://arxiv.org/pdf/2510.02314", "abs": "https://arxiv.org/abs/2510.02314", "authors": ["Bo-Hsu Ke", "You-Zhe Xie", "Yu-Lun Liu", "Wei-Chen Chiu"], "title": "StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions", "categories": ["cs.CV"], "comment": "ICCV 2025. Project page: https://hentci.github.io/stealthattack/", "summary": "3D scene representation methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As these methods become prevalent, addressing their vulnerabilities becomes critical. We analyze 3DGS robustness against image-level poisoning attacks and propose a novel density-guided poisoning method. Our method strategically injects Gaussian points into low-density regions identified via Kernel Density Estimation (KDE), embedding viewpoint-dependent illusory objects clearly visible from poisoned views while minimally affecting innocent views. Additionally, we introduce an adaptive noise strategy to disrupt multi-view consistency, further enhancing attack effectiveness. We propose a KDE-based evaluation protocol to assess attack difficulty systematically, enabling objective benchmarking for future research. Extensive experiments demonstrate our method's superior performance compared to state-of-the-art techniques. Project page: https://hentci.github.io/stealthattack/", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e863D\u9ad8\u65af\u6cfc\u6e85(3DGS)\u5bf9\u56fe\u50cf\u7ea7\u6295\u6bd2\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bc6\u5ea6\u7684\u6295\u6bd2\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4f4e\u5bc6\u5ea6\u533a\u57df\u6ce8\u5165\u9ad8\u65af\u70b9\u6765\u5d4c\u5165\u89c6\u89d2\u4f9d\u8d56\u7684\u5e7b\u89c9\u5bf9\u8c61\u3002", "motivation": "\u968f\u7740NeRF\u548c3DGS\u7b493D\u573a\u666f\u8868\u793a\u65b9\u6cd5\u7684\u666e\u53ca\uff0c\u89e3\u51b3\u5176\u5b89\u5168\u6f0f\u6d1e\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u5206\u67903DGS\u5bf9\u6295\u6bd2\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5f00\u53d1\u66f4\u6709\u6548\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5bc6\u5ea6\u5f15\u5bfc\u7684\u6295\u6bd2\u65b9\u6cd5\uff0c\u4f7f\u7528\u6838\u5bc6\u5ea6\u4f30\u8ba1\u8bc6\u522b\u4f4e\u5bc6\u5ea6\u533a\u57df\uff0c\u6218\u7565\u6027\u5730\u6ce8\u5165\u9ad8\u65af\u70b9\u6765\u5d4c\u5165\u89c6\u89d2\u4f9d\u8d56\u7684\u5e7b\u89c9\u5bf9\u8c61\u3002\u540c\u65f6\u5f15\u5165\u81ea\u9002\u5e94\u566a\u58f0\u7b56\u7565\u7834\u574f\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u80fd\u591f\u6709\u6548\u5d4c\u5165\u53ef\u89c1\u7684\u5e7b\u89c9\u5bf9\u8c61\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u5bf9\u65e0\u8f9c\u89c6\u89d2\u7684\u5f71\u54cd\u3002", "conclusion": "\u672c\u6587\u4e0d\u4ec5\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u76843DGS\u6295\u6bd2\u653b\u51fb\u65b9\u6cd5\uff0c\u8fd8\u5f15\u5165\u4e86\u57fa\u4e8eKDE\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5ba2\u89c2\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u3002"}}
{"id": "2510.02315", "pdf": "https://arxiv.org/pdf/2510.02315", "abs": "https://arxiv.org/abs/2510.02315", "authors": ["Eric Tillmann Bill", "Enis Simsar", "Thomas Hofmann"], "title": "Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity", "categories": ["cs.CV"], "comment": "Code: https://github.com/ericbill21/FOCUS/", "summary": "Text-to-image (T2I) models excel on single-entity prompts but struggle with multi-subject descriptions, often showing attribute leakage, identity entanglement, and subject omissions. We introduce the first theoretical framework with a principled, optimizable objective for steering sampling dynamics toward multi-subject fidelity. Viewing flow matching (FM) through stochastic optimal control (SOC), we formulate subject disentanglement as control over a trained FM sampler. This yields two architecture-agnostic algorithms: (i) a training-free test-time controller that perturbs the base velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight fine-tuning rule that regresses a control network to a backward adjoint signal while preserving base-model capabilities. The same formulation unifies prior attention heuristics, extends to diffusion models via a flow-diffusion correspondence, and provides the first fine-tuning route explicitly designed for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and Stable Diffusion XL, both algorithms consistently improve multi-subject alignment while maintaining base-model style. Test-time control runs efficiently on commodity GPUs, and fine-tuned controllers trained on limited prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal Control for Unentangled Subjects), which achieves state-of-the-art multi-subject fidelity across models.", "AI": {"tldr": "\u63d0\u51fa\u4e86FOCUS\u6846\u67b6\uff0c\u901a\u8fc7\u968f\u673a\u6700\u4f18\u63a7\u5236\u7406\u8bba\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u591a\u4e3b\u4f53\u751f\u6210\u65f6\u7684\u5c5e\u6027\u6cc4\u6f0f\u3001\u8eab\u4efd\u7ea0\u7f20\u548c\u4e3b\u4f53\u9057\u6f0f\u95ee\u9898\uff0c\u5305\u542b\u65e0\u9700\u8bad\u7ec3\u7684\u6d4b\u8bd5\u65f6\u63a7\u5236\u5668\u548c\u8f7b\u91cf\u7ea7\u5fae\u8c03\u65b9\u6cd5Adjoint Matching\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u5355\u4e3b\u4f53\u63d0\u793a\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u4e3b\u4f53\u63cf\u8ff0\u65f6\u7ecf\u5e38\u51fa\u73b0\u5c5e\u6027\u6cc4\u6f0f\u3001\u8eab\u4efd\u7ea0\u7f20\u548c\u4e3b\u4f53\u9057\u6f0f\u7b49\u95ee\u9898\uff0c\u9700\u8981\u7406\u8bba\u6846\u67b6\u6765\u6307\u5bfc\u91c7\u6837\u8fc7\u7a0b\u3002", "method": "\u5c06\u6d41\u5339\u914d\u901a\u8fc7\u968f\u673a\u6700\u4f18\u63a7\u5236\u89c6\u89d2\u91cd\u65b0\u8868\u8ff0\uff0c\u5f00\u53d1\u4e86\u4e24\u79cd\u67b6\u6784\u65e0\u5173\u7b97\u6cd5\uff1a\u6d4b\u8bd5\u65f6\u63a7\u5236\u5668\u901a\u8fc7\u5355\u6b21\u66f4\u65b0\u6270\u52a8\u57fa\u7840\u901f\u5ea6\u573a\uff0c\u4ee5\u53caAdjoint Matching\u901a\u8fc7\u56de\u5f52\u63a7\u5236\u7f51\u7edc\u5230\u53cd\u5411\u4f34\u968f\u4fe1\u53f7\u8fdb\u884c\u8f7b\u91cf\u7ea7\u5fae\u8c03\u3002", "result": "\u5728Stable Diffusion 3.5\u3001FLUX\u548cStable Diffusion XL\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e24\u79cd\u7b97\u6cd5\u90fd\u80fd\u6301\u7eed\u6539\u5584\u591a\u4e3b\u4f53\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u6301\u57fa\u7840\u6a21\u578b\u98ce\u683c\u3002\u6d4b\u8bd5\u65f6\u63a7\u5236\u5668\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u9ad8\u6548\u8fd0\u884c\uff0c\u5fae\u8c03\u63a7\u5236\u5668\u5728\u6709\u9650\u63d0\u793a\u4e0a\u8bad\u7ec3\u540e\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u63d0\u793a\u3002", "conclusion": "FOCUS\u6846\u67b6\u5728\u591a\u4e3b\u4f53\u4fdd\u771f\u5ea6\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u4e3a\u591a\u4e3b\u4f53\u751f\u6210\u63d0\u4f9b\u4e86\u9996\u4e2a\u7406\u8bba\u6846\u67b6\u548c\u663e\u5f0f\u8bbe\u8ba1\u7684\u5fae\u8c03\u8def\u5f84\u3002"}}
{"id": "2510.01284", "pdf": "https://arxiv.org/pdf/2510.01284", "abs": "https://arxiv.org/abs/2510.01284", "authors": ["Chetwin Low", "Weimin Wang", "Calder Katyal"], "title": "Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "comment": null, "summary": "Audio-video generation has often relied on complex multi-stage architectures or sequential synthesis of sound and visuals. We introduce Ovi, a unified paradigm for audio-video generation that models the two modalities as a single generative process. By using blockwise cross-modal fusion of twin-DiT modules, Ovi achieves natural synchronization and removes the need for separate pipelines or post hoc alignment. To facilitate fine-grained multimodal fusion modeling, we initialize an audio tower with an architecture identical to that of a strong pretrained video model. Trained from scratch on hundreds of thousands of hours of raw audio, the audio tower learns to generate realistic sound effects, as well as speech that conveys rich speaker identity and emotion. Fusion is obtained by jointly training the identical video and audio towers via blockwise exchange of timing (via scaled-RoPE embeddings) and semantics (through bidirectional cross-attention) on a vast video corpus. Our model enables cinematic storytelling with natural speech and accurate, context-matched sound effects, producing movie-grade video clips. All the demos, code and model weights are published at https://aaxwaz.github.io/Ovi", "AI": {"tldr": "Ovi\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u97f3\u9891-\u89c6\u9891\u751f\u6210\u8303\u5f0f\uff0c\u901a\u8fc7\u5757\u72b6\u8de8\u6a21\u6001\u878d\u5408\u7684\u53ccDiT\u6a21\u5757\u5c06\u4e24\u79cd\u6a21\u6001\u5efa\u6a21\u4e3a\u5355\u4e00\u751f\u6210\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u81ea\u7136\u540c\u6b65\uff0c\u65e0\u9700\u5355\u72ec\u6d41\u7a0b\u6216\u540e\u5904\u7406\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u7684\u97f3\u9891-\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u590d\u6742\u7684\u591a\u9636\u6bb5\u67b6\u6784\u6216\u58f0\u97f3\u4e0e\u89c6\u89c9\u7684\u987a\u5e8f\u5408\u6210\uff0c\u8fd9\u5bfc\u81f4\u4e86\u540c\u6b65\u95ee\u9898\u548c\u989d\u5916\u7684\u5bf9\u9f50\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u5757\u72b6\u8de8\u6a21\u6001\u878d\u5408\u7684\u53ccDiT\u6a21\u5757\uff0c\u901a\u8fc7\u521d\u59cb\u5316\u4e0e\u9884\u8bad\u7ec3\u89c6\u9891\u6a21\u578b\u67b6\u6784\u76f8\u540c\u7684\u97f3\u9891\u5854\uff0c\u5728\u5927\u91cf\u539f\u59cb\u97f3\u9891\u4e0a\u8bad\u7ec3\u97f3\u9891\u5854\u5b66\u4e60\u751f\u6210\u903c\u771f\u97f3\u6548\u548c\u5bcc\u6709\u8868\u73b0\u529b\u7684\u8bed\u97f3\uff0c\u7136\u540e\u901a\u8fc7\u65f6\u95f4\uff08\u7f29\u653eRoPE\u5d4c\u5165\uff09\u548c\u8bed\u4e49\uff08\u53cc\u5411\u4ea4\u53c9\u6ce8\u610f\u529b\uff09\u7684\u5757\u72b6\u4ea4\u6362\u8054\u5408\u8bad\u7ec3\u89c6\u9891\u548c\u97f3\u9891\u5854\u3002", "result": "\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u7535\u5f71\u7ea7\u7684\u89c6\u9891\u7247\u6bb5\u751f\u6210\uff0c\u5177\u6709\u81ea\u7136\u8bed\u97f3\u548c\u51c6\u786e\u3001\u4e0a\u4e0b\u6587\u5339\u914d\u7684\u97f3\u6548\uff0c\u652f\u6301\u7535\u5f71\u53d9\u4e8b\u3002", "conclusion": "Ovi\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u97f3\u9891-\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u878d\u5408\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u540c\u6b65\u751f\u6210\uff0c\u4e3a\u7535\u5f71\u7ea7\u5185\u5bb9\u521b\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.01361", "pdf": "https://arxiv.org/pdf/2510.01361", "abs": "https://arxiv.org/abs/2510.01361", "authors": ["Conall Daly", "Darren Ramsook", "Anil Kokaram"], "title": "An Efficient Quality Metric for Video Frame Interpolation Based on Motion-Field Divergence", "categories": ["eess.IV", "cs.CV", "cs.MM"], "comment": "IEEE 17th International Conference on Quality of Multimedia   Experience 2025 accepted manuscript, 7 pages", "summary": "Video frame interpolation is a fundamental tool for temporal video enhancement, but existing quality metrics struggle to evaluate the perceptual impact of interpolation artefacts effectively. Metrics like PSNR, SSIM and LPIPS ignore temporal coherence. State-of-the-art quality metrics tailored towards video frame interpolation, like FloLPIPS, have been developed but suffer from computational inefficiency that limits their practical application. We present $\\text{PSNR}_{\\text{DIV}}$, a novel full-reference quality metric that enhances PSNR through motion divergence weighting, a technique adapted from archival film restoration where it was developed to detect temporal inconsistencies. Our approach highlights singularities in motion fields which is then used to weight image errors. Evaluation on the BVI-VFI dataset (180 sequences across multiple frame rates, resolutions and interpolation methods) shows $\\text{PSNR}_{\\text{DIV}}$ achieves statistically significant improvements: +0.09 Pearson Linear Correlation Coefficient over FloLPIPS, while being 2.5$\\times$ faster and using 4$\\times$ less memory. Performance remains consistent across all content categories and are robust to the motion estimator used. The efficiency and accuracy of $\\text{PSNR}_{\\text{DIV}}$ enables fast quality evaluation and practical use as a loss function for training neural networks for video frame interpolation tasks. An implementation of our metric is available at www.github.com/conalld/psnr-div.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u5e27\u63d2\u503c\u8d28\u91cf\u8bc4\u4f30\u6307\u6807PSNR_DIV\uff0c\u901a\u8fc7\u8fd0\u52a8\u53d1\u6563\u52a0\u6743\u589e\u5f3aPSNR\uff0c\u5728BVI-VFI\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4FloLPIPS\u63d0\u5347\u4e860.09\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\uff0c\u540c\u65f6\u901f\u5ea6\u63d0\u53472.5\u500d\u3001\u5185\u5b58\u4f7f\u7528\u51cf\u5c114\u500d\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5e27\u63d2\u503c\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\uff08\u5982PSNR\u3001SSIM\u3001LPIPS\uff09\u5ffd\u7565\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u800c\u4e13\u95e8\u9488\u5bf9\u5e27\u63d2\u503c\u7684\u6307\u6807\uff08\u5982FloLPIPS\uff09\u8ba1\u7b97\u6548\u7387\u4f4e\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86PSNR_DIV\u6307\u6807\uff0c\u901a\u8fc7\u8fd0\u52a8\u53d1\u6563\u52a0\u6743\u6765\u589e\u5f3aPSNR\uff0c\u8be5\u65b9\u6cd5\u6539\u7f16\u81ea\u6863\u6848\u80f6\u7247\u4fee\u590d\u4e2d\u68c0\u6d4b\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u7684\u6280\u672f\uff0c\u7a81\u51fa\u8fd0\u52a8\u573a\u4e2d\u7684\u5947\u70b9\u6765\u52a0\u6743\u56fe\u50cf\u8bef\u5dee\u3002", "result": "\u5728BVI-VFI\u6570\u636e\u96c6\uff08\u5305\u542b180\u4e2a\u5e8f\u5217\uff0c\u6db5\u76d6\u591a\u79cd\u5e27\u7387\u3001\u5206\u8fa8\u7387\u548c\u63d2\u503c\u65b9\u6cd5\uff09\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cPSNR_DIV\u76f8\u6bd4FloLPIPS\u76ae\u5c14\u900a\u7ebf\u6027\u76f8\u5173\u7cfb\u6570\u63d0\u53470.09\uff0c\u901f\u5ea6\u63d0\u53472.5\u500d\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c114\u500d\u3002", "conclusion": "PSNR_DIV\u7684\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\u4f7f\u5176\u80fd\u591f\u5feb\u901f\u8bc4\u4f30\u8d28\u91cf\uff0c\u5e76\u53ef\u4f5c\u4e3a\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u89c6\u9891\u5e27\u63d2\u503c\u4efb\u52a1\u7684\u635f\u5931\u51fd\u6570\u3002"}}
{"id": "2510.01502", "pdf": "https://arxiv.org/pdf/2510.01502", "abs": "https://arxiv.org/abs/2510.01502", "authors": ["Kathy Garcia", "Leyla Isik"], "title": "Aligning Video Models with Human Social Judgments via Behavior-Guided Fine-Tuning", "categories": ["q-bio.NC", "cs.CV", "cs.LG"], "comment": "15 pages total, 4 figures. Includes 1 algorithm and 2 tables in the   appendix", "summary": "Humans intuitively perceive complex social signals in visual scenes, yet it remains unclear whether state-of-the-art AI models encode the same similarity structure. We study (Q1) whether modern video and language models capture human-perceived similarity in social videos, and (Q2) how to instill this structure into models using human behavioral data. To address this, we introduce a new benchmark of over 49,000 odd-one-out similarity judgments on 250 three-second video clips of social interactions, and discover a modality gap: despite the task being visual, caption-based language embeddings align better with human similarity than any pretrained video model. We close this gap by fine-tuning a TimeSformer video model on these human judgments with our novel hybrid triplet-RSA objective using low-rank adaptation (LoRA), aligning pairwise distances to human similarity. This fine-tuning protocol yields significantly improved alignment with human perceptions on held-out videos in terms of both explained variance and odd-one-out triplet accuracy. Variance partitioning shows that the fine-tuned video model increases shared variance with language embeddings and explains additional unique variance not captured by the language model. Finally, we test transfer via linear probes and find that human-similarity fine-tuning strengthens the encoding of social-affective attributes (intimacy, valence, dominance, communication) relative to the pretrained baseline. Overall, our findings highlight a gap in pretrained video models' social recognition and demonstrate that behavior-guided fine-tuning shapes video representations toward human social perception.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u9884\u8bad\u7ec3\u89c6\u9891\u6a21\u578b\u5728\u793e\u4ea4\u89c6\u9891\u76f8\u4f3c\u6027\u5224\u65ad\u4e0a\u4e0e\u4eba\u7c7b\u611f\u77e5\u5b58\u5728\u5dee\u8ddd\uff0c\u8bed\u8a00\u6a21\u578b\u53cd\u800c\u8868\u73b0\u66f4\u597d\u3002\u901a\u8fc7\u4eba\u7c7b\u884c\u4e3a\u6570\u636e\u5fae\u8c03\u89c6\u9891\u6a21\u578b\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u4e0e\u4eba\u7c7b\u793e\u4ea4\u611f\u77e5\u7684\u5bf9\u9f50\u5ea6\u3002", "motivation": "\u63a2\u7a76\u73b0\u4ee3AI\u6a21\u578b\u662f\u5426\u80fd\u6355\u6349\u4eba\u7c7b\u5728\u89c6\u89c9\u573a\u666f\u4e2d\u611f\u77e5\u7684\u590d\u6742\u793e\u4ea4\u4fe1\u53f7\uff0c\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7\u4eba\u7c7b\u884c\u4e3a\u6570\u636e\u5c06\u8fd9\u79cd\u76f8\u4f3c\u6027\u7ed3\u6784\u6ce8\u5165\u6a21\u578b\u3002", "method": "\u6784\u5efa\u5305\u542b49,000\u591a\u4e2a\u4e09\u9009\u4e00\u76f8\u4f3c\u6027\u5224\u65ad\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u6df7\u5408\u4e09\u5143\u7ec4-RSA\u76ee\u6807\u901a\u8fc7LoRA\u5fae\u8c03TimeSformer\u89c6\u9891\u6a21\u578b\uff0c\u4f7f\u5176\u6210\u5bf9\u8ddd\u79bb\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u6027\u5bf9\u9f50\u3002", "result": "\u5fae\u8c03\u540e\u7684\u89c6\u9891\u6a21\u578b\u5728\u4fdd\u7559\u89c6\u9891\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u4e0e\u4eba\u7c7b\u611f\u77e5\u7684\u5bf9\u9f50\u5ea6\uff0c\u65b9\u5dee\u5206\u89e3\u663e\u793a\u8be5\u6a21\u578b\u589e\u52a0\u4e86\u4e0e\u8bed\u8a00\u5d4c\u5165\u7684\u5171\u4eab\u65b9\u5dee\uff0c\u5e76\u89e3\u91ca\u4e86\u8bed\u8a00\u6a21\u578b\u672a\u6355\u83b7\u7684\u72ec\u7279\u65b9\u5dee\u3002", "conclusion": "\u9884\u8bad\u7ec3\u89c6\u9891\u6a21\u578b\u5728\u793e\u4ea4\u8bc6\u522b\u65b9\u9762\u5b58\u5728\u5dee\u8ddd\uff0c\u884c\u4e3a\u5f15\u5bfc\u7684\u5fae\u8c03\u80fd\u591f\u5851\u9020\u89c6\u9891\u8868\u793a\uff0c\u4f7f\u5176\u66f4\u63a5\u8fd1\u4eba\u7c7b\u793e\u4ea4\u611f\u77e5\u3002"}}
{"id": "2510.01666", "pdf": "https://arxiv.org/pdf/2510.01666", "abs": "https://arxiv.org/abs/2510.01666", "authors": ["Jianxu Wang", "Ge Wang"], "title": "Median2Median: Zero-shot Suppression of Structured Noise in Images", "categories": ["eess.IV", "cs.CV", "q-bio.QM", "stat.ML"], "comment": "13 pages, 6 figures, not published yet", "summary": "Image denoising is a fundamental problem in computer vision and medical imaging. However, real-world images are often degraded by structured noise with strong anisotropic correlations that existing methods struggle to remove. Most data-driven approaches rely on large datasets with high-quality labels and still suffer from limited generalizability, whereas existing zero-shot methods avoid this limitation but remain effective only for independent and identically distributed (i.i.d.) noise. To address this gap, we propose Median2Median (M2M), a zero-shot denoising framework designed for structured noise. M2M introduces a novel sampling strategy that generates pseudo-independent sub-image pairs from a single noisy input. This strategy leverages directional interpolation and generalized median filtering to adaptively exclude values distorted by structured artifacts. To further enlarge the effective sampling space and eliminate systematic bias, a randomized assignment strategy is employed, ensuring that the sampled sub-image pairs are suitable for Noise2Noise training. In our realistic simulation studies, M2M performs on par with state-of-the-art zero-shot methods under i.i.d. noise, while consistently outperforming them under correlated noise. These findings establish M2M as an efficient, data-free solution for structured noise suppression and mark the first step toward effective zero-shot denoising beyond the strict i.i.d. assumption.", "AI": {"tldr": "\u63d0\u51fa\u4e86Median2Median\uff08M2M\uff09\u96f6\u6837\u672c\u53bb\u566a\u6846\u67b6\uff0c\u4e13\u95e8\u9488\u5bf9\u7ed3\u6784\u5316\u566a\u58f0\u8bbe\u8ba1\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u91c7\u6837\u7b56\u7565\u751f\u6210\u4f2a\u72ec\u7acb\u5b50\u56fe\u50cf\u5bf9\uff0c\u5728\u4fdd\u6301i.i.d.\u566a\u58f0\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76f8\u5173\u566a\u58f0\u4e0b\u7684\u53bb\u566a\u6548\u679c\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u56fe\u50cf\u5e38\u88ab\u5177\u6709\u5f3a\u5404\u5411\u5f02\u6027\u76f8\u5173\u7684\u7ed3\u6784\u5316\u566a\u58f0\u9000\u5316\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u53bb\u9664\u3002\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u6cdb\u5316\u6027\u6709\u9650\uff0c\u800c\u96f6\u6837\u672c\u65b9\u6cd5\u867d\u7136\u907f\u514d\u4e86\u6570\u636e\u4f9d\u8d56\uff0c\u4f46\u4ec5\u5bf9\u72ec\u7acb\u540c\u5206\u5e03\u566a\u58f0\u6709\u6548\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u91c7\u6837\u7b56\u7565\uff0c\u4ece\u5355\u4e2a\u566a\u58f0\u8f93\u5165\u751f\u6210\u4f2a\u72ec\u7acb\u5b50\u56fe\u50cf\u5bf9\uff1a\u4f7f\u7528\u65b9\u5411\u6027\u63d2\u503c\u548c\u5e7f\u4e49\u4e2d\u503c\u6ee4\u6ce2\u81ea\u9002\u5e94\u6392\u9664\u7ed3\u6784\u5316\u4f2a\u5f71\u7684\u5931\u771f\u503c\uff1b\u91c7\u7528\u968f\u673a\u5206\u914d\u7b56\u7565\u6269\u5927\u6709\u6548\u91c7\u6837\u7a7a\u95f4\u5e76\u6d88\u9664\u7cfb\u7edf\u504f\u5dee\uff0c\u4f7f\u91c7\u6837\u5b50\u56fe\u50cf\u5bf9\u9002\u7528\u4e8eNoise2Noise\u8bad\u7ec3\u3002", "result": "\u5728\u771f\u5b9e\u6a21\u62df\u7814\u7a76\u4e2d\uff0cM2M\u5728i.i.d.\u566a\u58f0\u4e0b\u4e0e\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\uff0c\u5728\u76f8\u5173\u566a\u58f0\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u8fd9\u4e9b\u65b9\u6cd5\u3002", "conclusion": "M2M\u4e3a\u7ed3\u6784\u5316\u566a\u58f0\u6291\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u6570\u636e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6807\u5fd7\u7740\u5728\u8d85\u8d8a\u4e25\u683ci.i.d.\u5047\u8bbe\u65b9\u9762\u8fc8\u51fa\u4e86\u6709\u6548\u96f6\u6837\u672c\u53bb\u566a\u7684\u7b2c\u4e00\u6b65\u3002"}}
{"id": "2510.01749", "pdf": "https://arxiv.org/pdf/2510.01749", "abs": "https://arxiv.org/abs/2510.01749", "authors": ["Valentin Delchevalerie", "Nicolas Roy", "Arnaud Bougaham", "Alexandre Mayer", "Beno\u00eet Fr\u00e9nay", "Micha\u00ebl Lobet"], "title": "Towards Photonic Band Diagram Generation with Transformer-Latent Diffusion Models", "categories": ["physics.optics", "cs.CV"], "comment": null, "summary": "Photonic crystals enable fine control over light propagation at the nanoscale, and thus play a central role in the development of photonic and quantum technologies. Photonic band diagrams (BDs) are a key tool to investigate light propagation into such inhomogeneous structured materials. However, computing BDs requires solving Maxwell's equations across many configurations, making it numerically expensive, especially when embedded in optimization loops for inverse design techniques, for example. To address this challenge, we introduce the first approach for BD generation based on diffusion models, with the capacity to later generalize and scale to arbitrary three dimensional structures. Our method couples a transformer encoder, which extracts contextual embeddings from the input structure, with a latent diffusion model to generate the corresponding BD. In addition, we provide insights into why transformers and diffusion models are well suited to capture the complex interference and scattering phenomena inherent to photonics, paving the way for new surrogate modeling strategies in this domain.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5149\u5b50\u6676\u4f53\u80fd\u5e26\u56fe\u751f\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408Transformer\u7f16\u7801\u5668\u548c\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u6269\u5c55\u5230\u4efb\u610f\u4e09\u7ef4\u7ed3\u6784\u3002", "motivation": "\u8ba1\u7b97\u5149\u5b50\u80fd\u5e26\u56fe\u9700\u8981\u6c42\u89e3\u5927\u91cf\u9ea6\u514b\u65af\u97e6\u65b9\u7a0b\uff0c\u6570\u503c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u7279\u522b\u662f\u5728\u9006\u5411\u8bbe\u8ba1\u4f18\u5316\u5faa\u73af\u4e2d\u3002", "method": "\u5c06Transformer\u7f16\u7801\u5668\u4e0e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u76f8\u7ed3\u5408\uff0cTransformer\u63d0\u53d6\u8f93\u5165\u7ed3\u6784\u7684\u4e0a\u4e0b\u6587\u5d4c\u5165\uff0c\u6269\u6563\u6a21\u578b\u751f\u6210\u5bf9\u5e94\u7684\u80fd\u5e26\u56fe\u3002", "result": "\u5f00\u53d1\u4e86\u80fd\u591f\u751f\u6210\u5149\u5b50\u80fd\u5e26\u56fe\u7684\u65b9\u6cd5\uff0c\u4e3a\u5149\u5b50\u5b66\u9886\u57df\u7684\u65b0\u4ee3\u7406\u5efa\u6a21\u7b56\u7565\u94fa\u5e73\u9053\u8def\u3002", "conclusion": "Transformer\u548c\u6269\u6563\u6a21\u578b\u80fd\u591f\u6709\u6548\u6355\u6349\u5149\u5b50\u5b66\u4e2d\u590d\u6742\u7684\u5e72\u6d89\u548c\u6563\u5c04\u73b0\u8c61\uff0c\u4e3a\u5149\u5b50\u6676\u4f53\u80fd\u5e26\u56fe\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.01758", "pdf": "https://arxiv.org/pdf/2510.01758", "abs": "https://arxiv.org/abs/2510.01758", "authors": ["Bruno Corcuera", "Carlos Eiras-Franco", "Brais Cancela"], "title": "Unsupervised Dynamic Feature Selection for Robust Latent Spaces in Vision Tasks", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Latent representations are critical for the performance and robustness of machine learning models, as they encode the essential features of data in a compact and informative manner. However, in vision tasks, these representations are often affected by noisy or irrelevant features, which can degrade the model's performance and generalization capabilities. This paper presents a novel approach for enhancing latent representations using unsupervised Dynamic Feature Selection (DFS). For each instance, the proposed method identifies and removes misleading or redundant information in images, ensuring that only the most relevant features contribute to the latent space. By leveraging an unsupervised framework, our approach avoids reliance on labeled data, making it broadly applicable across various domains and datasets. Experiments conducted on image datasets demonstrate that models equipped with unsupervised DFS achieve significant improvements in generalization performance across various tasks, including clustering and image generation, while incurring a minimal increase in the computational cost.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u52a8\u6001\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u79fb\u9664\u56fe\u50cf\u4e2d\u7684\u8bef\u5bfc\u6027\u6216\u5197\u4f59\u4fe1\u606f\u6765\u589e\u5f3a\u6f5c\u5728\u8868\u793a\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6f5c\u5728\u8868\u793a\u5e38\u53d7\u566a\u58f0\u6216\u4e0d\u76f8\u5173\u7279\u5f81\u5f71\u54cd\uff0c\u8fd9\u4f1a\u964d\u4f4e\u6a21\u578b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u65e0\u76d1\u7763\u52a8\u6001\u7279\u5f81\u9009\u62e9\u6846\u67b6\uff0c\u4e3a\u6bcf\u4e2a\u5b9e\u4f8b\u8bc6\u522b\u5e76\u79fb\u9664\u56fe\u50cf\u4e2d\u7684\u8bef\u5bfc\u6216\u5197\u4f59\u4fe1\u606f\uff0c\u786e\u4fdd\u53ea\u6709\u6700\u76f8\u5173\u7279\u5f81\u8d21\u732e\u4e8e\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "\u5728\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u914d\u5907\u65e0\u76d1\u7763DFS\u7684\u6a21\u578b\u5728\u805a\u7c7b\u548c\u56fe\u50cf\u751f\u6210\u7b49\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6cdb\u5316\u6027\u80fd\uff0c\u8ba1\u7b97\u6210\u672c\u4ec5\u8f7b\u5fae\u589e\u52a0\u3002", "conclusion": "\u65e0\u76d1\u7763\u52a8\u6001\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u80fd\u6709\u6548\u589e\u5f3a\u6f5c\u5728\u8868\u793a\uff0c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2510.01967", "pdf": "https://arxiv.org/pdf/2510.01967", "abs": "https://arxiv.org/abs/2510.01967", "authors": ["Aadarsh Anantha Ramakrishnan", "Shubham Agarwal", "Selvanayagam S", "Kunwar Singh"], "title": "ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": "Accepted at AI-ML Systems 2025, Bangalore, India,   https://www.aimlsystems.org/2025/", "summary": "As image generation models grow increasingly powerful and accessible, concerns around authenticity, ownership, and misuse of synthetic media have become critical. The ability to generate lifelike images indistinguishable from real ones introduces risks such as misinformation, deepfakes, and intellectual property violations. Traditional watermarking methods either degrade image quality, are easily removed, or require access to confidential model internals - making them unsuitable for secure and scalable deployment. We are the first to introduce ZK-WAGON, a novel system for watermarking image generation models using the Zero-Knowledge Succinct Non Interactive Argument of Knowledge (ZK-SNARKs). Our approach enables verifiable proof of origin without exposing model weights, generation prompts, or any sensitive internal information. We propose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively convert key layers of an image generation model into a circuit, reducing proof generation time significantly. Generated ZK-SNARK proofs are imperceptibly embedded into a generated image via Least Significant Bit (LSB) steganography. We demonstrate this system on both GAN and Diffusion models, providing a secure, model-agnostic pipeline for trustworthy AI image generation.", "AI": {"tldr": "ZK-WAGON\u662f\u4e00\u4e2a\u57fa\u4e8eZK-SNARKs\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578b\u6c34\u5370\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u4e0d\u66b4\u9732\u6a21\u578b\u6743\u91cd\u6216\u654f\u611f\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u6765\u6e90\u8bc1\u660e\u3002", "motivation": "\u968f\u7740\u56fe\u50cf\u751f\u6210\u6a21\u578b\u53d8\u5f97\u8d8a\u6765\u8d8a\u5f3a\u5927\u548c\u6613\u7528\uff0c\u5408\u6210\u5a92\u4f53\u7684\u771f\u5b9e\u6027\u3001\u6240\u6709\u6743\u548c\u6ee5\u7528\u95ee\u9898\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u6c34\u5370\u65b9\u6cd5\u8981\u4e48\u4f1a\u964d\u4f4e\u56fe\u50cf\u8d28\u91cf\uff0c\u8981\u4e48\u5bb9\u6613\u88ab\u79fb\u9664\uff0c\u6216\u8005\u9700\u8981\u8bbf\u95ee\u673a\u5bc6\u6a21\u578b\u5185\u90e8\u4fe1\u606f\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5b89\u5168\u53ef\u6269\u5c55\u7684\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u9009\u62e9\u6027\u5c42ZK\u7535\u8def\u521b\u5efa(SL-ZKCC)\u65b9\u6cd5\uff0c\u9009\u62e9\u6027\u5730\u5c06\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u5173\u952e\u5c42\u8f6c\u6362\u4e3a\u7535\u8def\uff0c\u663e\u8457\u51cf\u5c11\u8bc1\u660e\u751f\u6210\u65f6\u95f4\u3002\u751f\u6210\u7684ZK-SNARK\u8bc1\u660e\u901a\u8fc7\u6700\u4f4e\u6709\u6548\u4f4d(LSB)\u9690\u5199\u672f\u4e0d\u53ef\u611f\u77e5\u5730\u5d4c\u5165\u5230\u751f\u6210\u7684\u56fe\u50cf\u4e2d\u3002", "result": "\u8be5\u7cfb\u7edf\u5728GAN\u548cDiffusion\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u6f14\u793a\uff0c\u4e3a\u53ef\u4fe1AI\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b89\u5168\u3001\u6a21\u578b\u65e0\u5173\u7684\u6d41\u6c34\u7ebf\u3002", "conclusion": "ZK-WAGON\u662f\u9996\u4e2a\u4f7f\u7528ZK-SNARKs\u4e3a\u56fe\u50cf\u751f\u6210\u6a21\u578b\u6dfb\u52a0\u6c34\u5370\u7684\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u4fdd\u62a4\u6a21\u578b\u673a\u5bc6\u6027\u7684\u540c\u65f6\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u6765\u6e90\u8bc1\u660e\u3002"}}
{"id": "2510.01982", "pdf": "https://arxiv.org/pdf/2510.01982", "abs": "https://arxiv.org/abs/2510.01982", "authors": ["Yujie Zhou", "Pengyang Ling", "Jiazi Bu", "Yibin Wang", "Yuhang Zang", "Jiaqi Wang", "Li Niu", "Guangtao Zhai"], "title": "$\\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models", "categories": ["cs.LG", "cs.CV"], "comment": "Github Page: https://github.com/bcmi/Granular-GRPO", "summary": "The integration of online reinforcement learning (RL) into diffusion and flow models has recently emerged as a promising approach for aligning generative models with human preferences. Stochastic sampling via Stochastic Differential Equations (SDE) is employed during the denoising process to generate diverse denoising directions for RL exploration. While existing methods effectively explore potential high-value samples, they suffer from sub-optimal preference alignment due to sparse and narrow reward signals. To address these challenges, we propose a novel Granular-GRPO ($\\text{G}^2$RPO ) framework that achieves precise and comprehensive reward assessments of sampling directions in reinforcement learning of flow models. Specifically, a Singular Stochastic Sampling strategy is introduced to support step-wise stochastic exploration while enforcing a high correlation between the reward and the injected noise, thereby facilitating a faithful reward for each SDE perturbation. Concurrently, to eliminate the bias inherent in fixed-granularity denoising, we introduce a Multi-Granularity Advantage Integration module that aggregates advantages computed at multiple diffusion scales, producing a more comprehensive and robust evaluation of the sampling directions. Experiments conducted on various reward models, including both in-domain and out-of-domain evaluations, demonstrate that our $\\text{G}^2$RPO significantly outperforms existing flow-based GRPO baselines,highlighting its effectiveness and robustness.", "AI": {"tldr": "\u63d0\u51faGranular-GRPO\u6846\u67b6\uff0c\u901a\u8fc7\u5947\u5f02\u968f\u673a\u91c7\u6837\u7b56\u7565\u548c\u591a\u7c92\u5ea6\u4f18\u52bf\u96c6\u6210\u6a21\u5757\uff0c\u89e3\u51b3\u6d41\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u5956\u52b1\u4fe1\u53f7\u7a00\u758f\u548c\u7a84\u5316\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u7cbe\u786e\u5168\u9762\u7684\u91c7\u6837\u65b9\u5411\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u63a2\u7d22\u6f5c\u5728\u9ad8\u4ef7\u503c\u6837\u672c\u65b9\u9762\u6709\u6548\uff0c\u4f46\u7531\u4e8e\u7a00\u758f\u548c\u7a84\u5316\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u5bfc\u81f4\u504f\u597d\u5bf9\u9f50\u6548\u679c\u4e0d\u4f73\u3002", "method": "1. \u5947\u5f02\u968f\u673a\u91c7\u6837\u7b56\u7565\uff1a\u652f\u6301\u9010\u6b65\u968f\u673a\u63a2\u7d22\uff0c\u589e\u5f3a\u5956\u52b1\u4e0e\u6ce8\u5165\u566a\u58f0\u7684\u76f8\u5173\u6027\uff1b2. \u591a\u7c92\u5ea6\u4f18\u52bf\u96c6\u6210\u6a21\u5757\uff1a\u805a\u5408\u591a\u4e2a\u6269\u6563\u5c3a\u5ea6\u7684\u4f18\u52bf\u8ba1\u7b97\uff0c\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u91c7\u6837\u65b9\u5411\u8bc4\u4f30\u3002", "result": "\u5728\u5404\u79cd\u5956\u52b1\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cG\u00b2RPO\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u6d41\u7684GRPO\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "G\u00b2RPO\u6846\u67b6\u901a\u8fc7\u6539\u8fdb\u7684\u91c7\u6837\u7b56\u7565\u548c\u591a\u7c92\u5ea6\u8bc4\u4f30\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u504f\u597d\u5bf9\u9f50\uff0c\u4e3a\u6d41\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.02182", "pdf": "https://arxiv.org/pdf/2510.02182", "abs": "https://arxiv.org/abs/2510.02182", "authors": ["Yule Wang", "Joseph Yu", "Chengrui Li", "Weihan Li", "Anqi Wu"], "title": "Uncovering Semantic Selectivity of Latent Groups in Higher Visual Cortex with Mutual Information-Guided Diffusion", "categories": ["q-bio.NC", "cs.CV", "cs.LG"], "comment": null, "summary": "Understanding how neural populations in higher visual areas encode object-centered visual information remains a central challenge in computational neuroscience. Prior works have investigated representational alignment between artificial neural networks and the visual cortex. Nevertheless, these findings are indirect and offer limited insights to the structure of neural populations themselves. Similarly, decoding-based methods have quantified semantic features from neural populations but have not uncovered their underlying organizations. This leaves open a scientific question: \"how feature-specific visual information is distributed across neural populations in higher visual areas, and whether it is organized into structured, semantically meaningful subspaces.\" To tackle this problem, we present MIG-Vis, a method that leverages the generative power of diffusion models to visualize and validate the visual-semantic attributes encoded in neural latent subspaces. Our method first uses a variational autoencoder to infer a group-wise disentangled neural latent subspace from neural populations. Subsequently, we propose a mutual information (MI)-guided diffusion synthesis procedure to visualize the specific visual-semantic features encoded by each latent group. We validate MIG-Vis on multi-session neural spiking datasets from the inferior temporal (IT) cortex of two macaques. The synthesized results demonstrate that our method identifies neural latent groups with clear semantic selectivity to diverse visual features, including object pose, inter-category transformations, and intra-class content. These findings provide direct, interpretable evidence of structured semantic representation in the higher visual cortex and advance our understanding of its encoding principles.", "AI": {"tldr": "\u63d0\u51faMIG-Vis\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u53ef\u89c6\u5316\u795e\u7ecf\u6f5c\u5728\u5b50\u7a7a\u95f4\u4e2d\u7f16\u7801\u7684\u89c6\u89c9\u8bed\u4e49\u7279\u5f81\uff0c\u53d1\u73b0\u9ad8\u7ea7\u89c6\u89c9\u76ae\u5c42\u4e2d\u5b58\u5728\u7ed3\u6784\u5316\u8bed\u4e49\u8868\u5f81", "motivation": "\u7406\u89e3\u9ad8\u7ea7\u89c6\u89c9\u533a\u57df\u4e2d\u795e\u7ecf\u7fa4\u4f53\u5982\u4f55\u7f16\u7801\u4ee5\u7269\u4f53\u4e3a\u4e2d\u5fc3\u7684\u89c6\u89c9\u4fe1\u606f\uff0c\u63a2\u7d22\u7279\u5f81\u7279\u5f02\u6027\u89c6\u89c9\u4fe1\u606f\u5728\u795e\u7ecf\u7fa4\u4f53\u4e2d\u7684\u5206\u5e03\u548c\u7ec4\u7ec7\u7ed3\u6784", "method": "\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u63a8\u65ad\u795e\u7ecf\u7fa4\u4f53\u7684\u7ec4\u95f4\u89e3\u7f20\u795e\u7ecf\u6f5c\u5728\u5b50\u7a7a\u95f4\uff0c\u7136\u540e\u63d0\u51fa\u4e92\u4fe1\u606f\u5f15\u5bfc\u7684\u6269\u6563\u5408\u6210\u8fc7\u7a0b\u6765\u53ef\u89c6\u5316\u6bcf\u4e2a\u6f5c\u5728\u7ec4\u7f16\u7801\u7684\u7279\u5b9a\u89c6\u89c9\u8bed\u4e49\u7279\u5f81", "result": "\u5728\u4e24\u53ea\u7315\u7334\u989e\u4e0b\u76ae\u5c42\u591a\u4f1a\u8bdd\u795e\u7ecf\u53d1\u653e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u53d1\u73b0\u795e\u7ecf\u6f5c\u5728\u7ec4\u5bf9\u591a\u6837\u5316\u89c6\u89c9\u7279\u5f81\u5177\u6709\u6e05\u6670\u7684\u8bed\u4e49\u9009\u62e9\u6027\uff0c\u5305\u62ec\u7269\u4f53\u59ff\u6001\u3001\u7c7b\u522b\u95f4\u8f6c\u6362\u548c\u7c7b\u5185\u5185\u5bb9", "conclusion": "\u4e3a\u9ad8\u7ea7\u89c6\u89c9\u76ae\u5c42\u4e2d\u5b58\u5728\u7ed3\u6784\u5316\u8bed\u4e49\u8868\u5f81\u63d0\u4f9b\u4e86\u76f4\u63a5\u3001\u53ef\u89e3\u91ca\u7684\u8bc1\u636e\uff0c\u63a8\u8fdb\u4e86\u5bf9\u7f16\u7801\u539f\u7406\u7684\u7406\u89e3"}}
{"id": "2510.02296", "pdf": "https://arxiv.org/pdf/2510.02296", "abs": "https://arxiv.org/abs/2510.02296", "authors": ["Yu-Chien Liao", "Jr-Jen Chen", "Chi-Pin Huang", "Ci-Siang Lin", "Meng-Lin Wu", "Yu-Chiang Frank Wang"], "title": "Continual Personalization for Diffusion Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Updating diffusion models in an incremental setting would be practical in real-world applications yet computationally challenging. We present a novel learning strategy of Concept Neuron Selection (CNS), a simple yet effective approach to perform personalization in a continual learning scheme. CNS uniquely identifies neurons in diffusion models that are closely related to the target concepts. In order to mitigate catastrophic forgetting problems while preserving zero-shot text-to-image generation ability, CNS finetunes concept neurons in an incremental manner and jointly preserves knowledge learned of previous concepts. Evaluation of real-world datasets demonstrates that CNS achieves state-of-the-art performance with minimal parameter adjustments, outperforming previous methods in both single and multi-concept personalization works. CNS also achieves fusion-free operation, reducing memory storage and processing time for continual personalization.", "AI": {"tldr": "\u63d0\u51fa\u6982\u5ff5\u795e\u7ecf\u5143\u9009\u62e9(CNS)\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u5fae\u8c03\u4e0e\u76ee\u6807\u6982\u5ff5\u76f8\u5173\u7684\u795e\u7ecf\u5143\uff0c\u5728\u6269\u6563\u6a21\u578b\u4e2d\u5b9e\u73b0\u589e\u91cf\u5f0f\u4e2a\u6027\u5316\u5b66\u4e60\uff0c\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u5e76\u4fdd\u6301\u96f6\u6837\u672c\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u589e\u91cf\u66f4\u65b0\u6269\u6563\u6a21\u578b\u5177\u6709\u5b9e\u7528\u6027\uff0c\u4f46\u8ba1\u7b97\u4e0a\u5177\u6709\u6311\u6218\u6027\u3002\u9700\u8981\u89e3\u51b3\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u96f6\u6837\u672c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u80fd\u529b\u3002", "method": "CNS\u65b9\u6cd5\u72ec\u7279\u8bc6\u522b\u6269\u6563\u6a21\u578b\u4e2d\u4e0e\u76ee\u6807\u6982\u5ff5\u5bc6\u5207\u76f8\u5173\u7684\u795e\u7ecf\u5143\uff0c\u4ee5\u589e\u91cf\u65b9\u5f0f\u5fae\u8c03\u8fd9\u4e9b\u6982\u5ff5\u795e\u7ecf\u5143\uff0c\u5e76\u8054\u5408\u4fdd\u7559\u5148\u524d\u6982\u5ff5\u7684\u77e5\u8bc6\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cCNS\u4ee5\u6700\u5c11\u7684\u53c2\u6570\u8c03\u6574\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u5355\u6982\u5ff5\u548c\u591a\u6982\u5ff5\u4e2a\u6027\u5316\u4efb\u52a1\u4e2d\u90fd\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "CNS\u5b9e\u73b0\u4e86\u65e0\u878d\u5408\u64cd\u4f5c\uff0c\u51cf\u5c11\u4e86\u6301\u7eed\u4e2a\u6027\u5316\u6240\u9700\u7684\u5185\u5b58\u5b58\u50a8\u548c\u5904\u7406\u65f6\u95f4\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u589e\u91cf\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.02300", "pdf": "https://arxiv.org/pdf/2510.02300", "abs": "https://arxiv.org/abs/2510.02300", "authors": ["Runqian Wang", "Yilun Du"], "title": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "We introduce Equilibrium Matching (EqM), a generative modeling framework built from an equilibrium dynamics perspective. EqM discards the non-equilibrium, time-conditional dynamics in traditional diffusion and flow-based generative models and instead learns the equilibrium gradient of an implicit energy landscape. Through this approach, we can adopt an optimization-based sampling process at inference time, where samples are obtained by gradient descent on the learned landscape with adjustable step sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation performance of diffusion/flow models empirically, achieving an FID of 1.90 on ImageNet 256$\\times$256. EqM is also theoretically justified to learn and sample from the data manifold. Beyond generation, EqM is a flexible framework that naturally handles tasks including partially noised image denoising, OOD detection, and image composition. By replacing time-conditional velocities with a unified equilibrium landscape, EqM offers a tighter bridge between flow and energy-based models and a simple route to optimization-driven inference.", "AI": {"tldr": "\u63d0\u51faEquilibrium Matching (EqM)\u751f\u6210\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5e73\u8861\u52a8\u529b\u5b66\u89c6\u89d2\u5b66\u4e60\u9690\u5f0f\u80fd\u91cf\u666f\u89c2\u7684\u5e73\u8861\u68af\u5ea6\uff0c\u91c7\u7528\u57fa\u4e8e\u4f18\u5316\u7684\u91c7\u6837\u8fc7\u7a0b\uff0c\u5728ImageNet 256\u00d7256\u4e0a\u8fbe\u52301.90 FID\uff0c\u5e76\u652f\u6301\u591a\u79cd\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edf\u6269\u6563\u548c\u6d41\u6a21\u578b\u4f7f\u7528\u975e\u5e73\u8861\u3001\u65f6\u95f4\u6761\u4ef6\u52a8\u529b\u5b66\uff0cEqM\u65e8\u5728\u901a\u8fc7\u5e73\u8861\u52a8\u529b\u5b66\u89c6\u89d2\u7b80\u5316\u751f\u6210\u5efa\u6a21\uff0c\u5efa\u7acb\u6d41\u6a21\u578b\u4e0e\u80fd\u91cf\u6a21\u578b\u4e4b\u95f4\u7684\u6865\u6881\u3002", "method": "EqM\u653e\u5f03\u4f20\u7edf\u6a21\u578b\u7684\u65f6\u95f4\u6761\u4ef6\u52a8\u529b\u5b66\uff0c\u5b66\u4e60\u9690\u5f0f\u80fd\u91cf\u666f\u89c2\u7684\u5e73\u8861\u68af\u5ea6\uff0c\u5728\u63a8\u7406\u65f6\u91c7\u7528\u57fa\u4e8e\u4f18\u5316\u7684\u91c7\u6837\u8fc7\u7a0b\uff0c\u4f7f\u7528\u53ef\u8c03\u6b65\u957f\u3001\u81ea\u9002\u5e94\u4f18\u5316\u5668\u548c\u81ea\u9002\u5e94\u8ba1\u7b97\u3002", "result": "\u5728ImageNet 256\u00d7256\u4e0a\u8fbe\u52301.90 FID\uff0c\u8d85\u8d8a\u6269\u6563/\u6d41\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u80fd\u5904\u7406\u90e8\u5206\u566a\u58f0\u56fe\u50cf\u53bb\u566a\u3001OOD\u68c0\u6d4b\u548c\u56fe\u50cf\u5408\u6210\u7b49\u4efb\u52a1\u3002", "conclusion": "EqM\u901a\u8fc7\u7edf\u4e00\u5e73\u8861\u666f\u89c2\u53d6\u4ee3\u65f6\u95f4\u6761\u4ef6\u901f\u5ea6\uff0c\u63d0\u4f9b\u4e86\u6d41\u6a21\u578b\u4e0e\u80fd\u91cf\u6a21\u578b\u4e4b\u95f4\u66f4\u7d27\u5bc6\u7684\u6865\u6881\uff0c\u4ee5\u53ca\u4f18\u5316\u9a71\u52a8\u63a8\u7406\u7684\u7b80\u5355\u8def\u5f84\u3002"}}
