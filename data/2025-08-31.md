<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 24]
- [cs.LG](#cs.LG) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Mixture of Contexts for Long Video Generation](https://arxiv.org/abs/2508.21058)
*Shengqu Cai,Ceyuan Yang,Lvmin Zhang,Yuwei Guo,Junfei Xiao,Ziyan Yang,Yinghao Xu,Zhenheng Yang,Alan Yuille,Leonidas Guibas,Maneesh Agrawala,Lu Jiang,Gordon Wetzstein*

Main category: cs.GR

TL;DR: 提出Mixture of Contexts (MoC)稀疏注意力路由模块，通过动态选择信息块和强制锚点来解决长视频生成中的长上下文记忆问题，实现近线性计算缩放和分钟级一致性生成。


<details>
  <summary>Details</summary>
Motivation: 解决扩散变换器在生成长视频时因自注意力二次计算成本导致的内存和计算不可行问题，以及长序列优化困难的问题。

Method: 将长上下文视频生成重新定义为内部信息检索任务，使用可学习的稀疏注意力路由模块MoC，每个查询动态选择少量信息块和强制锚点（标题、局部窗口）进行关注，采用因果路由防止循环闭合。

Result: 通过扩展数据和逐步稀疏化路由，模型能够将计算资源分配给显著历史信息，在分钟级内容中保持身份、动作和场景的一致性，实现近线性计算缩放。

Conclusion: MoC模块作为有效的长期记忆检索引擎，解决了长视频生成中的内存和计算瓶颈，实现了实用的训练和合成，并在分钟尺度上实现了记忆和一致性的涌现。

Abstract: Long video generation is fundamentally a long context memory problem: models must retain and retrieve salient events across a long range without collapsing or drifting. However, scaling diffusion transformers to generate long-context videos is fundamentally limited by the quadratic cost of self-attention, which makes memory and computation intractable and difficult to optimize for long sequences. We recast long-context video generation as an internal information retrieval task and propose a simple, learnable sparse attention routing module, Mixture of Contexts (MoC), as an effective long-term memory retrieval engine. In MoC, each query dynamically selects a few informative chunks plus mandatory anchors (caption, local windows) to attend to, with causal routing that prevents loop closures. As we scale the data and gradually sparsify the routing, the model allocates compute to salient history, preserving identities, actions, and scenes over minutes of content. Efficiency follows as a byproduct of retrieval (near-linear scaling), which enables practical training and synthesis, and the emergence of memory and consistency at the scale of minutes.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By Value Sign Flip](https://arxiv.org/abs/2508.10931)
*Wenqi Guo,Shan Du*

Main category: cs.CV

TL;DR: VSF是一种简单高效的负向提示引导方法，通过翻转注意力值的符号来抑制不需要的内容，在少步扩散和流匹配模型中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的负向提示引导方法如CFG、NASA和NAG在少步生成模型中效果有限，需要一种更高效的方法来动态抑制不需要的内容。

Method: VSF通过动态翻转负向提示的注意力值符号来抑制不需要的内容，计算开销小，兼容MMDiT和交叉注意力架构。

Result: 在复杂提示对数据集上验证显示，VSF在少步模型中显著优于现有方法，在非少步模型中甚至优于CFG，同时保持图像质量。

Conclusion: VSF是一种高效且有效的负向提示引导方法，适用于静态图像和视频生成，代码已开源。

Abstract: We introduce Value Sign Flip (VSF), a simple and efficient method for incorporating negative prompt guidance in few-step diffusion and flow-matching image generation models. Unlike existing approaches such as classifier-free guidance (CFG), NASA, and NAG, VSF dynamically suppresses undesired content by flipping the sign of attention values from negative prompts. Our method requires only small computational overhead and integrates effectively with MMDiT-style architectures such as Stable Diffusion 3.5 Turbo, as well as cross-attention-based models like Wan. We validate VSF on challenging datasets with complex prompt pairs and demonstrate superior performance in both static image and video generation tasks. Experimental results show that VSF significantly improves negative prompt adherence compared to prior methods in few-step models, and even CFG in non-few-step models, while maintaining competitive image quality. Code and ComfyUI node are available in https://github.com/weathon/VSF/tree/main.

</details>


### [3] [SDiFL: Stable Diffusion-Driven Framework for Image Forgery Localization](https://arxiv.org/abs/2508.20182)
*Yang Su,Shunquan Tan,Jiwu Huang*

Main category: cs.CV

TL;DR: 本研究首次将Stable Diffusion的多模态生成和感知能力整合到图像取证框架中，通过理论证明SD架构可基于伪造相关信息进行条件化处理，并利用SD3的多模态框架在潜在空间中处理图像伪造残差作为显式模态，实现了无需大量标注数据的高效准确伪造定位。


<details>
  <summary>Details</summary>
Motivation: 现有图像伪造定位方法严重依赖人工标注数据，难以跟上多模态大模型驱动的图像处理技术发展速度，需要开发更高效准确的伪造检测方法。

Method: 利用Stable Diffusion V3的多模态架构，将图像伪造残差（通过高通滤波器提取的高频信号）作为显式模态融合到潜在空间中，同时完整保留SD3提取的潜在特征和输入图像的丰富语义信息。

Result: 在广泛使用的基准数据集上比当前最先进的图像伪造定位模型性能提升高达12%，在训练中未见过的真实文档伪造图像和自然场景伪造图像上也表现出强大性能。

Conclusion: 该方法成功将多模态大模型的生成和感知能力整合到图像取证中，实现了无需大量标注数据的高效伪造定位，为应对新兴图像处理技术带来的取证挑战提供了有效解决方案。

Abstract: Driven by the new generation of multi-modal large models, such as Stable Diffusion (SD), image manipulation technologies have advanced rapidly, posing significant challenges to image forensics. However, existing image forgery localization methods, which heavily rely on labor-intensive and costly annotated data, are struggling to keep pace with these emerging image manipulation technologies. To address these challenges, we are the first to integrate both image generation and powerful perceptual capabilities of SD into an image forensic framework, enabling more efficient and accurate forgery localization. First, we theoretically show that the multi-modal architecture of SD can be conditioned on forgery-related information, enabling the model to inherently output forgery localization results. Then, building on this foundation, we specifically leverage the multimodal framework of Stable DiffusionV3 (SD3) to enhance forgery localization performance.We leverage the multi-modal processing capabilities of SD3 in the latent space by treating image forgery residuals -- high-frequency signals extracted using specific highpass filters -- as an explicit modality. This modality is fused into the latent space during training to enhance forgery localization performance. Notably, our method fully preserves the latent features extracted by SD3, thereby retaining the rich semantic information of the input image. Experimental results show that our framework achieves up to 12% improvements in performance on widely used benchmarking datasets compared to current state-of-the-art image forgery localization models. Encouragingly, the model demonstrates strong performance on forensic tasks involving real-world document forgery images and natural scene forging images, even when such data were entirely unseen during training.

</details>


### [4] [Audio-Guided Visual Editing with Complex Multi-Modal Prompts](https://arxiv.org/abs/2508.20379)
*Hyeonyu Kim,Seokhoon Jeong,Seonghee Han,Chanhyuk Choi,Taehwan Kim*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的音频指导视觉编辑框架，能够处理复杂的多模态编辑任务，而无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有的统一模型通过文本指导进行视觉编辑在复杂场景下存在限制，需要额外的非文本编辑提示，而现有音频指导方法通常需要特定数据集训练来对齐音频和文本，限制了其泛化能力。

Method: 利用预训练的多模态编码器，通过减少音频编码空间与去散模型提示编码空间之间的差异来集成多样化音频到视觉编辑任务中，并提出分离噪声分支和适应性补丁选择来处理复杂的多模态编辑提示。

Result: 在多样化编辑任务上进行的全面实验表明，该框架能够通过融合音频的丰富信息来处理复杂的编辑场景，而仅依靠文本的方法则无法完成。

Conclusion: 该研究提出的框架为复杂多模态视觉编辑任务提供了一种无需额外训练的有效解决方案，显著提升了音频指导在视觉编辑中的应用效果。

Abstract: Visual editing with diffusion models has made significant progress but often struggles with complex scenarios that textual guidance alone could not adequately describe, highlighting the need for additional non-text editing prompts. In this work, we introduce a novel audio-guided visual editing framework that can handle complex editing tasks with multiple text and audio prompts without requiring additional training. Existing audio-guided visual editing methods often necessitate training on specific datasets to align audio with text, limiting their generalization to real-world situations. We leverage a pre-trained multi-modal encoder with strong zero-shot capabilities and integrate diverse audio into visual editing tasks, by alleviating the discrepancy between the audio encoder space and the diffusion model's prompt encoder space. Additionally, we propose a novel approach to handle complex scenarios with multiple and multi-modal editing prompts through our separate noise branching and adaptive patch selection. Our comprehensive experiments on diverse editing tasks demonstrate that our framework excels in handling complicated editing scenarios by incorporating rich information from audio, where text-only approaches fail.

</details>


### [5] [Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation](https://arxiv.org/abs/2508.20471)
*Jiusi Li,Jackson Jiang,Jinyu Miao,Miao Long,Tuopu Wen,Peijin Jia,Shengxiang Liu,Chunlei Yu,Maolin Liu,Yuzhan Cai,Kun Jiang,Mengmeng Yang,Diange Yang*

Main category: cs.CV

TL;DR: G^2Editor是一个用于驾驶视频中逼真精确物体编辑的框架，通过3D高斯表示和层次化特征控制，实现物体重新定位、插入和删除，在姿态控制和视觉质量方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统的训练和验证需要大量角点案例，但真实世界收集成本高且危险。现有编辑方法存在视觉保真度有限或姿态控制不精确的问题。

Method: 利用编辑对象的3D高斯表示作为密集先验，注入去噪过程确保精确姿态控制和空间一致性；使用场景级3D边界框布局重建非目标对象的遮挡区域；引入层次化细粒度特征指导编辑对象的外观细节。

Result: 在Waymo Open Dataset上的实验表明，G^2Editor在姿态可控性和视觉质量方面优于现有方法，并能有效支持下游数据驱动任务。

Conclusion: G^2Editor提供了一个统一的框架，能够实现驾驶视频中逼真且精确的物体编辑，解决了现有方法的局限性，为自动驾驶系统提供了高质量的训练数据生成方案。

Abstract: Corner cases are crucial for training and validating autonomous driving systems, yet collecting them from the real world is often costly and hazardous. Editing objects within captured sensor data offers an effective alternative for generating diverse scenarios, commonly achieved through 3D Gaussian Splatting or image generative models. However, these approaches often suffer from limited visual fidelity or imprecise pose control. To address these issues, we propose G^2Editor, a framework designed for photorealistic and precise object editing in driving videos. Our method leverages a 3D Gaussian representation of the edited object as a dense prior, injected into the denoising process to ensure accurate pose control and spatial consistency. A scene-level 3D bounding box layout is employed to reconstruct occluded areas of non-target objects. Furthermore, to guide the appearance details of the edited object, we incorporate hierarchical fine-grained features as additional conditions during generation. Experiments on the Waymo Open Dataset demonstrate that G^2Editor effectively supports object repositioning, insertion, and deletion within a unified framework, outperforming existing methods in both pose controllability and visual quality, while also benefiting downstream data-driven tasks.

</details>


### [6] [Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding](https://arxiv.org/abs/2508.20476)
*Jeong Hun Yeo,Hyeongseop Rha,Sungjune Park,Junil Won,Yong Man Ro*

Main category: cs.CV

TL;DR: 首个统一框架，能够处理手语、唇部动作和音频等多模态组合，用于语言文本生成，性能超过专门模型


<details>
  <summary>Details</summary>
Motivation: 现有语音识别技术对聋听障碍人士不友好，而手语和唇读等视觉模态又通常单独研究，需要统一框架来整合多模态信息

Method: 设计了一个统一的、模态无关的架构，能够有效处理异构输入，明确将唇部动作建模为独立模态

Result: 在SLT、VSR、ASR和AVSR等任务上达到或超过了专门模型的最高水平，明确建模唇部动作显著提升了手语翻译性能

Conclusion: 多模态统一框架在语音识别领域具有重要价值，唇部动作作为非手势索引在手语理解中发挥关键作用，为聋听障碍人士提供了更好的通信解决方案

Abstract: Audio is the primary modality for human communication and has driven the success of Automatic Speech Recognition (ASR) technologies. However, such systems remain inherently inaccessible to individuals who are deaf or hard of hearing. Visual alternatives such as sign language and lip reading offer effective substitutes, and recent advances in Sign Language Translation (SLT) and Visual Speech Recognition (VSR) have improved audio-less communication. Yet, these modalities have largely been studied in isolation, and their integration within a unified framework remains underexplored. In this paper, we introduce the first unified framework capable of handling diverse combinations of sign language, lip movements, and audio for spoken-language text generation. We focus on three main objectives: (i) designing a unified, modality-agnostic architecture capable of effectively processing heterogeneous inputs; (ii) exploring the underexamined synergy among modalities, particularly the role of lip movements as non-manual cues in sign language comprehension; and (iii) achieving performance on par with or superior to state-of-the-art models specialized for individual tasks. Building on this framework, we achieve performance on par with or better than task-specific state-of-the-art models across SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that explicitly modeling lip movements as a separate modality significantly improves SLT performance.

</details>


### [7] [Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent](https://arxiv.org/abs/2508.20505)
*En Ci,Shanyan Guan,Yanhao Ge,Yilin Zhang,Wei Li,Zhenyu Zhang,Jian Yang,Ying Tai*

Main category: cs.CV

TL;DR: DescriptiveEdit是一个基于描述性提示的图像编辑框架，将指令式编辑重新定义为基于参考图像的文本到图像生成，避免了重建误差和数据集限制问题


<details>
  <summary>Details</summary>
Motivation: 现有的语义图像编辑方法存在重建误差（基于反转的方法）和数据集质量限制（基于指令的方法）的问题，需要一种既能保持生成质量又不受数据集限制的解决方案

Method: 提出Cross-Attentive UNet架构，通过注意力桥接机制将参考图像特征注入到提示词到编辑图像的生成过程中，保持文本到图像生成模型的生成能力

Result: 在Emu Edit基准测试中显示出编辑准确性和一致性的提升，能够无缝集成ControlNet、IP-Adapter等扩展

Conclusion: DescriptiveEdit框架通过重新定义问题范式，有效解决了语义图像编辑中的关键挑战，具有更好的可扩展性和集成能力

Abstract: Despite the progress in text-to-image generation, semantic image editing remains a challenge. Inversion-based algorithms unavoidably introduce reconstruction errors, while instruction-based models mainly suffer from limited dataset quality and scale. To address these problems, we propose a descriptive-prompt-based editing framework, named DescriptiveEdit. The core idea is to re-frame `instruction-based image editing' as `reference-image-based text-to-image generation', which preserves the generative power of well-trained Text-to-Image models without architectural modifications or inversion. Specifically, taking the reference image and a prompt as input, we introduce a Cross-Attentive UNet, which newly adds attention bridges to inject reference image features into the prompt-to-edit-image generation process. Owing to its text-to-image nature, DescriptiveEdit overcomes limitations in instruction dataset quality, integrates seamlessly with ControlNet, IP-Adapter, and other extensions, and is more scalable. Experiments on the Emu Edit benchmark show it improves editing accuracy and consistency.

</details>


### [8] [Adam SLAM - the last mile of camera calibration with 3DGS](https://arxiv.org/abs/2508.20526)
*Matthieu Gendrin,Stéphane Pateux,Xiaoran Jiang,Théo Ladune,Luce Morin*

Main category: cs.CV

TL;DR: 使用3DGS模型通过反向传播新视角颜色损失来微调相机标定，在参考数据集上平均提升0.4 dB PSNR


<details>
  <summary>Details</summary>
Motivation: 相机标定质量对新颖视图合成至关重要，1像素的标定误差会对重建质量产生显著影响，但真实场景缺乏地面真值

Method: 利用3DGS模型，通过反向传播新视角颜色损失来优化相机参数，实现标定微调

Result: 在3DGS使用的参考数据集上，新标定方法平均带来0.4 dB PSNR的提升

Conclusion: 虽然微调过程可能耗时，但对于参考场景（如Mip-NeRF 360）的标定，新视角质量是最重要的考量因素

Abstract: The quality of the camera calibration is of major importance for evaluating progresses in novel view synthesis, as a 1-pixel error on the calibration has a significant impact on the reconstruction quality. While there is no ground truth for real scenes, the quality of the calibration is assessed by the quality of the novel view synthesis. This paper proposes to use a 3DGS model to fine tune calibration by backpropagation of novel view color loss with respect to the cameras parameters. The new calibration alone brings an average improvement of 0.4 dB PSNR on the dataset used as reference by 3DGS. The fine tuning may be long and its suitability depends on the criticity of training time, but for calibration of reference scenes, such as Mip-NeRF 360, the stake of novel view quality is the most important.

</details>


### [9] [FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models](https://arxiv.org/abs/2508.20586)
*Zheng Chong,Yanwei Lei,Shiyue Zhang,Zhuandi He,Zhen Wang,Xujie Zhang,Xiao Dong,Yiling Wu,Dongmei Jiang,Xiaodan Liang*

Main category: cs.CV

TL;DR: FastFit是一个基于可缓存扩散架构的高速多参考虚拟试穿框架，通过半注意力机制和类别嵌入技术，实现了参考特征的一次计算多次复用，平均加速3.5倍，并在多个数据集上达到最先进的保真度指标。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟试穿技术面临两大挑战：无法支持多参考服装组合（包括服装和配饰），以及由于在每个去噪步骤中重复计算参考特征导致的显著低效率。

Method: 提出基于可缓存扩散架构的FastFit框架，采用半注意力机制，用类别嵌入替代传统时间步嵌入，实现参考特征编码与去噪过程的完全解耦，只需计算一次参考特征即可在所有步骤中无损复用。

Result: 在VITON-HD、DressCode和新建的DressCode-MR数据集上的广泛实验表明，FastFit在关键保真度指标上超越最先进方法，同时提供显著的推理效率优势，平均加速3.5倍。

Conclusion: FastFit成功解决了多参考虚拟试穿的效率和功能限制问题，为复杂多参考虚拟试穿研究提供了新的技术方案和数据集支持。

Abstract: Despite its great potential, virtual try-on technology is hindered from real-world application by two major challenges: the inability of current methods to support multi-reference outfit compositions (including garments and accessories), and their significant inefficiency caused by the redundant re-computation of reference features in each denoising step. To address these challenges, we propose FastFit, a high-speed multi-reference virtual try-on framework based on a novel cacheable diffusion architecture. By employing a Semi-Attention mechanism and substituting traditional timestep embeddings with class embeddings for reference items, our model fully decouples reference feature encoding from the denoising process with negligible parameter overhead. This allows reference features to be computed only once and losslessly reused across all steps, fundamentally breaking the efficiency bottleneck and achieving an average 3.5x speedup over comparable methods. Furthermore, to facilitate research on complex, multi-reference virtual try-on, we introduce DressCode-MR, a new large-scale dataset. It comprises 28,179 sets of high-quality, paired images covering five key categories (tops, bottoms, dresses, shoes, and bags), constructed through a pipeline of expert models and human feedback refinement. Extensive experiments on the VITON-HD, DressCode, and our DressCode-MR datasets show that FastFit surpasses state-of-the-art methods on key fidelity metrics while offering its significant advantage in inference efficiency.

</details>


### [10] [Physics Informed Generative Models for Magnetic Field Images](https://arxiv.org/abs/2508.20612)
*Aye Phyu Phyu Aung,Lucas Lum,Zhansen Shi,Wen Qiu,Bernice Zee,JM Chin,Yeow Kheng Lim,J. Senthilnath*

Main category: cs.CV

TL;DR: 提出PI-GenMFI方法，利用扩散模型结合物理约束生成合成磁場图像，解决半导体制造中MFI数据集稀缺问题，用于缺陷定位训练。


<details>
  <summary>Details</summary>
Motivation: 半导体制造中X射线检测内存密集且耗时，MFI能高效定位感兴趣区域，但MFI数据集因专有问题稀缺，限制了机器学习模型的训练。

Method: 提出物理信息生成模型PI-GenMFI，集成特定物理信息，使用扩散模型生成合成MFI样本，重点关注电源短路等常见缺陷类型。

Result: 与最先进的VAE和扩散生成模型比较，通过领域专家评估和多种图像生成、信号处理指标进行定性和定量评估，显示出优化缺陷定位过程的潜力。

Conclusion: PI-GenMFI方法能够有效生成合成MFI图像，为解决MFI数据集稀缺问题提供了可行方案，有望提升半导体缺陷检测效率。

Abstract: In semiconductor manufacturing, defect detection and localization are critical to ensuring product quality and yield. While X-ray imaging is a reliable non-destructive testing method, it is memory-intensive and time-consuming for large-scale scanning, Magnetic Field Imaging (MFI) offers a more efficient means to localize regions of interest (ROI) for targeted X-ray scanning. However, the limited availability of MFI datasets due to proprietary concerns presents a significant bottleneck for training machine learning (ML) models using MFI. To address this challenge, we consider an ML-driven approach leveraging diffusion models with two physical constraints. We propose Physics Informed Generative Models for Magnetic Field Images (PI-GenMFI) to generate synthetic MFI samples by integrating specific physical information. We generate MFI images for the most common defect types: power shorts. These synthetic images will serve as training data for ML algorithms designed to localize defect areas efficiently. To evaluate generated MFIs, we compare our model to SOTA generative models from both variational autoencoder (VAE) and diffusion methods. We present a domain expert evaluation to assess the generated samples. In addition, we present qualitative and quantitative evaluation using various metrics used for image generation and signal processing, showing promising results to optimize the defect localization process.

</details>


### [11] [Revisiting the Privacy Risks of Split Inference: A GAN-Based Data Reconstruction Attack via Progressive Feature Optimization](https://arxiv.org/abs/2508.20613)
*Yixiang Qiu,Yanhan Liu,Hongyao Yu,Hao Fang,Bin Chen,Shu-Tao Xia,Ke Xu*

Main category: cs.CV

TL;DR: 本文提出一种基于GAN的进阶特征优化方法，能够在分开推理环境中更有效地恢复敏感输入数据，在高分辨率、非同分布和深层模型场景下都显著超过现有攻击方法。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络复杂度增加，分开推理模式被广泛采用以降低延迟和保护隐私。但中间特征可被利用来恢复敏感数据，现有的数据重构攻击方法通常只在浅层模型上有效，且没有充分利用语义先验知识。

Method: 提出了一种新的GAN基于的数据重构攻击框架，采用进阶式特征优化(PFO)技术。将生成器分解为层次块，逐步精炼中间表征以提升重构图像的语义保真度，并在重构过程中引入L1球约束来稳定优化和提高图像真实性。

Result: 经过大量实验验证，该方法在高分辨率场景、非同分布设置以及面对更深层和更复杂的DNN模型时，都显著超过了之前的攻击方法。

Conclusion: 该研究提出的GAN基于PFO的数据重构攻击框架，能够更有效地利用中间特征恢复敏感输入数据，在多种复杂场景下都展现出优异的性能，显示了分开推理模式中存在的重大隐私风险。

Abstract: The growing complexity of Deep Neural Networks (DNNs) has led to the adoption of Split Inference (SI), a collaborative paradigm that partitions computation between edge devices and the cloud to reduce latency and protect user privacy. However, recent advances in Data Reconstruction Attacks (DRAs) reveal that intermediate features exchanged in SI can be exploited to recover sensitive input data, posing significant privacy risks. Existing DRAs are typically effective only on shallow models and fail to fully leverage semantic priors, limiting their reconstruction quality and generalizability across datasets and model architectures. In this paper, we propose a novel GAN-based DRA framework with Progressive Feature Optimization (PFO), which decomposes the generator into hierarchical blocks and incrementally refines intermediate representations to enhance the semantic fidelity of reconstructed images. To stabilize the optimization and improve image realism, we introduce an L1-ball constraint during reconstruction. Extensive experiments show that our method outperforms prior attacks by a large margin, especially in high-resolution scenarios, out-of-distribution settings, and against deeper and more complex DNNs.

</details>


### [12] [AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View Images](https://arxiv.org/abs/2508.20623)
*Shiqi Xin,Xiaolin Zhang,Yanbin Liu,Peng Zhang,Caifeng Shan*

Main category: cs.CV

TL;DR: AvatarBack是一个创新的即插即用框架，通过生成身份一致的后视图伪图像和学习性空间对齐策略，解决了3D高斯头像重建中后脑区域缺失的问题，显著提升了完整头像的重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有的高斯泼溅头像重建方法主要依赖正面视图图像，导致后脑区域重建质量差，存在几何不一致、结构模糊和真实感降低等问题，限制了重建头像的保真度。

Method: 提出AvatarBack框架，包含两个核心技术：1）主体特定生成器（SSG）利用生成先验从稀疏正面输入合成身份一致的后视图伪图像；2）自适应空间对齐策略（ASA）通过学习性变换矩阵优化合成视图与3D高斯表示之间的几何对齐。

Result: 在NeRSemble和K-hairstyle数据集上的实验表明，AvatarBack在保持正面保真度的同时，显著提升了后脑重建质量，重建的头像在不同运动下保持一致的视觉真实感且完全可动画化。

Conclusion: AvatarBack有效解决了3D高斯头像重建中后脑区域缺失的问题，为完整且一致的3D头像重建提供了有效的解决方案，在几何、光度和感知指标上均表现出优异性能。

Abstract: Recent advances in Gaussian Splatting have significantly boosted the reconstruction of head avatars, enabling high-quality facial modeling by representing an 3D avatar as a collection of 3D Gaussians. However, existing methods predominantly rely on frontal-view images, leaving the back-head poorly constructed. This leads to geometric inconsistencies, structural blurring, and reduced realism in the rear regions, ultimately limiting the fidelity of reconstructed avatars. To address this challenge, we propose AvatarBack, a novel plug-and-play framework specifically designed to reconstruct complete and consistent 3D Gaussian avatars by explicitly modeling the missing back-head regions. AvatarBack integrates two core technical innovations,i.e., the Subject-specific Generator (SSG) and the Adaptive Spatial Alignment Strategy (ASA). The former leverages a generative prior to synthesize identity-consistent, plausible back-view pseudo-images from sparse frontal inputs, providing robust multi-view supervision. To achieve precise geometric alignment between these synthetic views and the 3D Gaussian representation, the later employs learnable transformation matrices optimized during training, effectively resolving inherent pose and coordinate discrepancies. Extensive experiments on NeRSemble and K-hairstyle datasets, evaluated using geometric, photometric, and GPT-4o-based perceptual metrics, demonstrate that AvatarBack significantly enhances back-head reconstruction quality while preserving frontal fidelity. Moreover, the reconstructed avatars maintain consistent visual realism under diverse motions and remain fully animatable.

</details>


### [13] [CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models](https://arxiv.org/abs/2508.20640)
*Ayan Banerjee,Fernando Vilariño,Josep Lladós*

Main category: cs.CV

TL;DR: CraftGraffiti是一个端到端的文本引导涂鸦生成框架，通过LoRA微调扩散变换器和面部一致性自注意力机制，在极端风格转换中保持面部身份识别性。


<details>
  <summary>Details</summary>
Motivation: 解决涂鸦艺术中极端风格转换导致面部身份识别性丧失的问题，保持个人和文化真实性。

Method: 采用"风格优先、身份后处理"范式：首先通过LoRA微调的预训练扩散变换器进行涂鸦风格迁移，然后通过面部一致性自注意力机制增强身份嵌入来保持身份保真度，使用CLIP引导的提示扩展实现无关键点的姿态定制。

Result: 在定量评估中表现出竞争力的面部特征一致性，获得最先进的审美和人类偏好评分，定性分析和在Cruilla音乐节的现场部署展示了系统的实际创意影响。

Conclusion: CraftGraffiti推进了身份尊重的AI辅助艺术目标，为创意AI应用中融合风格自由和可识别性提供了原则性方法。

Abstract: Preserving facial identity under extreme stylistic transformation remains a major challenge in generative art. In graffiti, a high-contrast, abstract medium, subtle distortions to the eyes, nose, or mouth can erase the subject's recognizability, undermining both personal and cultural authenticity. We present CraftGraffiti, an end-to-end text-guided graffiti generation framework designed with facial feature preservation as a primary objective. Given an input image and a style and pose descriptive prompt, CraftGraffiti first applies graffiti style transfer via LoRA-fine-tuned pretrained diffusion transformer, then enforces identity fidelity through a face-consistent self-attention mechanism that augments attention layers with explicit identity embeddings. Pose customization is achieved without keypoints, using CLIP-guided prompt extension to enable dynamic re-posing while retaining facial coherence. We formally justify and empirically validate the "style-first, identity-after" paradigm, showing it reduces attribute drift compared to the reverse order. Quantitative results demonstrate competitive facial feature consistency and state-of-the-art aesthetic and human preference scores, while qualitative analyses and a live deployment at the Cruilla Festival highlight the system's real-world creative impact. CraftGraffiti advances the goal of identity-respectful AI-assisted artistry, offering a principled approach for blending stylistic freedom with recognizability in creative AI applications.

</details>


### [14] ["Humor, Art, or Misinformation?": A Multimodal Dataset for Intent-Aware Synthetic Image Detection](https://arxiv.org/abs/2508.20670)
*Anastasios Skoularikis,Stefanos-Iordanis Papadopoulos,Symeon Papadopoulos,Panagiotis C. Petrantonakis*

Main category: cs.CV

TL;DR: S-HArM数据集用于意图感知分类，包含9,576个社交媒体图像-文本对，标注为幽默/讽刺、艺术或虚假信息。研究比较了三种提示策略生成合成训练数据，发现图像和多模态引导的数据在真实内容上泛化更好，但整体性能有限。


<details>
  <summary>Details</summary>
Motivation: 现有多模态AI研究主要关注检测合成和脱节内容，但忽视了AI生成图像背后的意图。为了填补这一空白，需要专门的数据集和方法来识别内容创作意图。

Method: 构建S-HArM多模态数据集，使用三种提示策略（图像引导、描述引导、多模态引导）通过Stable Diffusion生成大规模合成训练数据。比较了模态融合、对比学习、重建网络、注意力机制和大视觉语言模型等多种方法。

Result: 使用图像引导和多模态引导数据训练的模型在真实内容上泛化效果更好，因为保留了视觉上下文。但整体分类性能仍然有限，表明推断意图的复杂性。

Conclusion: 推断AI生成内容的意图具有挑战性，需要专门的架构来改进性能。图像和多模态引导的方法显示出更好的泛化能力，但该领域仍需进一步研究。

Abstract: Recent advances in multimodal AI have enabled progress in detecting synthetic and out-of-context content. However, existing efforts largely overlook the intent behind AI-generated images. To fill this gap, we introduce S-HArM, a multimodal dataset for intent-aware classification, comprising 9,576 "in the wild" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art, or Misinformation. Additionally, we explore three prompting strategies (image-guided, description-guided, and multimodally-guided) to construct a large-scale synthetic training dataset with Stable Diffusion. We conduct an extensive comparative study including modality fusion, contrastive learning, reconstruction networks, attention mechanisms, and large vision-language models. Our results show that models trained on image- and multimodally-guided data generalize better to "in the wild" content, due to preserved visual context. However, overall performance remains limited, highlighting the complexity of inferring intent and the need for specialized architectures.

</details>


### [15] [CardioMorphNet: Cardiac Motion Prediction Using a Shape-Guided Bayesian Recurrent Deep Network](https://arxiv.org/abs/2508.20734)
*Reza Akbari Movahed,Abuzar Rezaee,Arezoo Zakeri,Colin Berry,Edmond S. L. Ho,Ali Gooya*

Main category: cs.CV

TL;DR: CardioMorphNet是一个基于贝叶斯深度学习的3D心脏形状引导变形配准框架，通过循环变分自编码器建模心脏周期时空依赖关系，不使用基于强度的图像配准相似性损失，在心脏运动估计方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖基于强度的图像配准相似性损失，可能忽略心脏解剖区域，导致心脏运动估计不准确。需要一种能够专注于解剖区域的心脏运动估计方法。

Method: 提出循环贝叶斯深度学习框架CardioMorphNet，使用循环变分自编码器建模时空依赖，通过两个后验模型进行双心室分割和运动估计，利用分割图递归配准而不使用强度相似性损失。

Result: 在UK Biobank数据集上验证，CardioMorphNet在心脏运动估计方面优于最先进方法，且相比其他概率性心脏配准方法，在心脏区域产生更低的不确定性值，预测置信度更高。

Conclusion: CardioMorphNet通过形状引导的贝叶斯深度学习框架，成功解决了心脏运动估计中忽略解剖区域的问题，提供了更准确和可靠的心脏功能评估工具。

Abstract: Accurate cardiac motion estimation from cine cardiac magnetic resonance (CMR) images is vital for assessing cardiac function and detecting its abnormalities. Existing methods often struggle to capture heart motion accurately because they rely on intensity-based image registration similarity losses that may overlook cardiac anatomical regions. To address this, we propose CardioMorphNet, a recurrent Bayesian deep learning framework for 3D cardiac shape-guided deformable registration using short-axis (SAX) CMR images. It employs a recurrent variational autoencoder to model spatio-temporal dependencies over the cardiac cycle and two posterior models for bi-ventricular segmentation and motion estimation. The derived loss function from the Bayesian formulation guides the framework to focus on anatomical regions by recursively registering segmentation maps without using intensity-based image registration similarity loss, while leveraging sequential SAX volumes and spatio-temporal features. The Bayesian modelling also enables computation of uncertainty maps for the estimated motion fields. Validated on the UK Biobank dataset by comparing warped mask shapes with ground truth masks, CardioMorphNet demonstrates superior performance in cardiac motion estimation, outperforming state-of-the-art methods. Uncertainty assessment shows that it also yields lower uncertainty values for estimated motion fields in the cardiac region compared with other probabilistic-based cardiac registration methods, indicating higher confidence in its predictions.

</details>


### [16] [Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning](https://arxiv.org/abs/2508.20751)
*Yibin Wang,Zhimin Li,Yuhang Zang,Yujie Zhou,Jiazi Bu,Chunyu Wang,Qinglin Lu,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 本文提出了Pref-GRPO方法来解决T2I生成中的奖励黑客问题，通过从分数最大化转向偏好拟合来稳定训练，并引入了UniGenBench基准测试来全面评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于点式奖励模型的T2I生成方法容易受到奖励黑客攻击，即微小分数差异在归一化后被放大，导致模型过度优化琐碎增益而破坏生成稳定性。同时现有基准测试评估标准粗糙，无法全面评估模型。

Method: 提出Pref-GRPO方法：使用成对偏好奖励模型，在每组图像中进行成对比较，以胜率作为奖励信号。引入UniGenBench基准测试，包含600个提示词，通过10个主要标准和27个子标准评估语义一致性，利用MLLM进行构建和评估。

Result: 实验表明Pref-GRPO能够区分细微的图像质量差异，提供更稳定的优势并缓解奖励黑客问题。UniGenBench基准测试揭示了开源和闭源T2I模型的优缺点，并验证了Pref-GRPO的有效性。

Conclusion: Pref-GRPO通过偏好拟合方法有效解决了奖励黑客问题，UniGenBench为T2I模型提供了更全面的评估框架，两者共同推动了文本到图像生成领域的发展。

Abstract: Recent advancements highlight the importance of GRPO-based reinforcement learning methods and benchmarking in enhancing text-to-image (T2I) generation. However, current methods using pointwise reward models (RM) for scoring generated images are susceptible to reward hacking. We reveal that this happens when minimal score differences between images are amplified after normalization, creating illusory advantages that drive the model to over-optimize for trivial gains, ultimately destabilizing the image generation process. To address this, we propose Pref-GRPO, a pairwise preference reward-based GRPO method that shifts the optimization objective from score maximization to preference fitting, ensuring more stable training. In Pref-GRPO, images are pairwise compared within each group using preference RM, and the win rate is used as the reward signal. Extensive experiments demonstrate that PREF-GRPO differentiates subtle image quality differences, providing more stable advantages and mitigating reward hacking. Additionally, existing T2I benchmarks are limited by coarse evaluation criteria, hindering comprehensive model assessment. To solve this, we introduce UniGenBench, a unified T2I benchmark comprising 600 prompts across 5 main themes and 20 subthemes. It evaluates semantic consistency through 10 primary and 27 sub-criteria, leveraging MLLM for benchmark construction and evaluation. Our benchmarks uncover the strengths and weaknesses of both open and closed-source T2I models and validate the effectiveness of Pref-GRPO.

</details>


### [17] [${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale Feature for Generalizable Gaussian Splatting](https://arxiv.org/abs/2508.20754)
*Yuxi Hu,Jun Zhang,Kuangyi Chen,Zhe Zhang,Friedrich Fraundorfer*

Main category: cs.CV

TL;DR: C3-GS是一个用于未见场景新视角合成的通用高斯泼溅框架，通过上下文感知、跨维度和跨尺度约束增强特征学习，无需逐场景优化即可实现高质量渲染


<details>
  <summary>Details</summary>
Motivation: 现有方法在编码判别性、多视角一致特征方面存在不足，难以从稀疏输入视图构建准确几何结构

Method: 提出C3-GS框架，集成三个轻量级模块到统一渲染流程中，包含上下文感知、跨维度和跨尺度约束，改进特征融合

Result: 在基准数据集上的广泛实验验证了C3-GS实现了最先进的渲染质量和泛化能力

Conclusion: 该方法能够实现逼真的合成效果，无需额外监督，代码已开源

Abstract: Generalizable Gaussian Splatting aims to synthesize novel views for unseen scenes without per-scene optimization. In particular, recent advancements utilize feed-forward networks to predict per-pixel Gaussian parameters, enabling high-quality synthesis from sparse input views. However, existing approaches fall short in encoding discriminative, multi-view consistent features for Gaussian predictions, which struggle to construct accurate geometry with sparse views. To address this, we propose $\mathbf{C}^{3}$-GS, a framework that enhances feature learning by incorporating context-aware, cross-dimension, and cross-scale constraints. Our architecture integrates three lightweight modules into a unified rendering pipeline, improving feature fusion and enabling photorealistic synthesis without requiring additional supervision. Extensive experiments on benchmark datasets validate that $\mathbf{C}^{3}$-GS achieves state-of-the-art rendering quality and generalization ability. Code is available at: https://github.com/YuhsiHu/C3-GS.

</details>


### [18] [Estimating 2D Keypoints of Surgical Tools Using Vision-Language Models with Low-Rank Adaptation](https://arxiv.org/abs/2508.20830)
*Krit Duangprom,Tryphon Lambrou,Binod Bhattarai*

Main category: cs.CV

TL;DR: 提出基于视觉语言模型(VLMs)和LoRA微调技术的外科手术工具2D关键点估计新方法，在低资源场景下仅需2个epoch微调即可超越基线模型


<details>
  <summary>Details</summary>
Motivation: 传统CNN和Transformer方法在小规模医疗数据集上容易过拟合，需要利用预训练VLMs的泛化能力来解决这个问题

Method: 使用LoRA技术微调预训练VLMs，精心设计提示词创建指令调优数据集，将视觉特征与语义关键点描述对齐

Result: 仅需2个epoch微调，适应后的VLM就超越了基线模型，证明了LoRA在低资源场景下的有效性

Conclusion: 该方法不仅提高了关键点检测性能，还为未来3D外科手术手和工具姿态估计研究铺平了道路

Abstract: This paper presents a novel pipeline for 2D keypoint estima- tion of surgical tools by leveraging Vision Language Models (VLMs) fine- tuned using a low rank adjusting (LoRA) technique. Unlike traditional Convolutional Neural Network (CNN) or Transformer-based approaches, which often suffer from overfitting in small-scale medical datasets, our method harnesses the generalization capabilities of pre-trained VLMs. We carefully design prompts to create an instruction-tuning dataset and use them to align visual features with semantic keypoint descriptions. Experimental results show that with only two epochs of fine tuning, the adapted VLM outperforms the baseline models, demonstrating the ef- fectiveness of LoRA in low-resource scenarios. This approach not only improves keypoint detection performance, but also paves the way for future work in 3D surgical hands and tools pose estimation.

</details>


### [19] [Understanding and evaluating computer vision models through the lens of counterfactuals](https://arxiv.org/abs/2508.20881)
*Pushkar Shukla*

Main category: cs.CV

TL;DR: 该论文开发了基于反事实推理的框架，用于解释、审计和减轻视觉分类器和生成模型中的偏见，通过系统性地改变语义属性来揭示虚假相关性并构建更鲁棒的系统。


<details>
  <summary>Details</summary>
Motivation: 反事实推理已成为可解释和公平AI的核心方法，但需要开发系统化框架来有效识别和缓解视觉分类器及生成模型中的偏见问题。

Method: 提出了多个方法：CAVLI结合归因分析和概念级分析来量化决策依赖；ASAC通过对抗性反事实扰动保护属性；TIBET提供可扩展的偏见评估流程；BiasConnect构建因果图诊断偏见；InterMit提供无需训练的偏见缓解算法。

Result: 开发的方法能够有效识别模型对无关线索的依赖，提高模型公平性和准确性，避免刻板印象，并建立了可扩展的社会责任偏见评估和缓解方法。

Conclusion: 反事实推理为判别性和生成性模型中的可解释性、公平性和因果性提供了统一视角，建立了原则性和可扩展的方法体系。

Abstract: Counterfactual reasoning -- the practice of asking ``what if'' by varying inputs and observing changes in model behavior -- has become central to interpretable and fair AI. This thesis develops frameworks that use counterfactuals to explain, audit, and mitigate bias in vision classifiers and generative models. By systematically altering semantically meaningful attributes while holding others fixed, these methods uncover spurious correlations, probe causal dependencies, and help build more robust systems.   The first part addresses vision classifiers. CAVLI integrates attribution (LIME) with concept-level analysis (TCAV) to quantify how strongly decisions rely on human-interpretable concepts. With localized heatmaps and a Concept Dependency Score, CAVLI shows when models depend on irrelevant cues like backgrounds. Extending this, ASAC introduces adversarial counterfactuals that perturb protected attributes while preserving semantics. Through curriculum learning, ASAC fine-tunes biased models for improved fairness and accuracy while avoiding stereotype-laden artifacts.   The second part targets generative Text-to-Image (TTI) models. TIBET provides a scalable pipeline for evaluating prompt-sensitive biases by varying identity-related terms, enabling causal auditing of how race, gender, and age affect image generation. To capture interactions, BiasConnect builds causal graphs diagnosing intersectional biases. Finally, InterMit offers a modular, training-free algorithm that mitigates intersectional bias via causal sensitivity scores and user-defined fairness goals.   Together, these contributions show counterfactuals as a unifying lens for interpretability, fairness, and causality in both discriminative and generative models, establishing principled, scalable methods for socially responsible bias evaluation and mitigation.

</details>


### [20] [COMETH: Convex Optimization for Multiview Estimation and Tracking of Humans](https://arxiv.org/abs/2508.20920)
*Enrico Martini,Ho Jin Choi,Nadia Figueroa,Nicola Bombieri*

Main category: cs.CV

TL;DR: COMETH是一个轻量级的多视角人体姿态融合算法，通过凸优化和生物力学约束实现实时准确的人体运动跟踪，适用于工业5.0场景。


<details>
  <summary>Details</summary>
Motivation: 工业5.0时代需要实时监控人体活动以确保工效安全和健康，但多摄像头集中式系统计算成本高、带宽需求大，而边缘设备处理又存在精度下降和时空不一致的问题。

Method: 提出COMETH算法，集成运动学和生物力学约束提高关节定位精度，采用凸优化逆向运动学进行空间融合，并实现状态观测器改善时间一致性。

Result: 在公共和工业数据集上评估，COMETH在定位、检测和跟踪精度方面优于现有最先进方法。

Conclusion: 该融合管道实现了准确且可扩展的人体运动跟踪，特别适合工业和安全关键应用，代码已开源。

Abstract: In the era of Industry 5.0, monitoring human activity is essential for ensuring both ergonomic safety and overall well-being. While multi-camera centralized setups improve pose estimation accuracy, they often suffer from high computational costs and bandwidth requirements, limiting scalability and real-time applicability. Distributing processing across edge devices can reduce network bandwidth and computational load. On the other hand, the constrained resources of edge devices lead to accuracy degradation, and the distribution of computation leads to temporal and spatial inconsistencies. We address this challenge by proposing COMETH (Convex Optimization for Multiview Estimation and Tracking of Humans), a lightweight algorithm for real-time multi-view human pose fusion that relies on three concepts: it integrates kinematic and biomechanical constraints to increase the joint positioning accuracy; it employs convex optimization-based inverse kinematics for spatial fusion; and it implements a state observer to improve temporal consistency. We evaluate COMETH on both public and industrial datasets, where it outperforms state-of-the-art methods in localization, detection, and tracking accuracy. The proposed fusion pipeline enables accurate and scalable human motion tracking, making it well-suited for industrial and safety-critical applications. The code is publicly available at https://github.com/PARCO-LAB/COMETH.

</details>


### [21] [POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models](https://arxiv.org/abs/2508.21019)
*Jiaxiang Cheng,Bing Ma,Xuhua Ren,Hongyi Jin,Kai Yu,Peng Zhang,Wenyue Li,Yuan Zhou,Tianxiang Zheng,Qinglin Lu*

Main category: cs.CV

TL;DR: POSE是一个单步蒸馏框架，通过两阶段过程（稳定性预热和统一对抗平衡）实现大规模视频扩散模型的高效采样，将推理时间从1000秒减少到10秒，同时保持视频质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频加速方法存在根本性限制：既不能建模视频帧的时间连贯性，也无法为大规模视频模型提供单步蒸馏，导致采样效率低下。

Method: 采用两阶段蒸馏过程：1）稳定性预热机制，在高到低信噪比区间稳定对抗蒸馏；2）统一对抗平衡机制，在高斯噪声空间实现稳定的单步对抗训练；对于条件生成还提出条件对抗一致性方法。

Result: 在VBench-I2V基准上平均提升7.15%的语义对齐、时间一致性和帧质量，将预训练模型的延迟降低100倍（从1000秒到10秒）。

Conclusion: POSE成功解决了大规模视频扩散模型的采样效率瓶颈，实现了高质量单步视频生成，在保持性能的同时显著提升了推理速度。

Abstract: The field of video diffusion generation faces critical bottlenecks in sampling efficiency, especially for large-scale models and long sequences. Existing video acceleration methods adopt image-based techniques but suffer from fundamental limitations: they neither model the temporal coherence of video frames nor provide single-step distillation for large-scale video models. To bridge this gap, we propose POSE (Phased One-Step Equilibrium), a distillation framework that reduces the sampling steps of large-scale video diffusion models, enabling the generation of high-quality videos in a single step. POSE employs a carefully designed two-phase process to distill video models:(i) stability priming: a warm-up mechanism to stabilize adversarial distillation that adapts the high-quality trajectory of the one-step generator from high to low signal-to-noise ratio regimes, optimizing the video quality of single-step mappings near the endpoints of flow trajectories. (ii) unified adversarial equilibrium: a flexible self-adversarial distillation mechanism that promotes stable single-step adversarial training towards a Nash equilibrium within the Gaussian noise space, generating realistic single-step videos close to real videos. For conditional video generation, we propose (iii) conditional adversarial consistency, a method to improve both semantic consistency and frame consistency between conditional frames and generated frames. Comprehensive experiments demonstrate that POSE outperforms other acceleration methods on VBench-I2V by average 7.15% in semantic alignment, temporal conference and frame quality, reducing the latency of the pre-trained model by 100$\times$, from 1000 seconds to 10 seconds, while maintaining competitive performance.

</details>


### [22] [Reusing Computation in Text-to-Image Diffusion for Efficient Generation of Image Sets](https://arxiv.org/abs/2508.21032)
*Dale Decatur,Thibault Groueix,Wang Yifan,Rana Hanocka,Vladimir Kim,Matheus Gadelha*

Main category: cs.CV

TL;DR: 提出一种训练免费的方法，通过聚类语义相似的提示词并在早期扩散步骤中共享计算，显著降低文本到图像生成的算力成本并提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型虽然能生成高质量图像但计算成本高昂。现有工作主要优化单次推理效率，本文探索另一种途径：减少相关提示词之间的冗余计算。

Method: 利用扩散模型从粗到细的特性，在早期去噪步骤中捕获相似提示词的共享结构。基于语义相似性对提示词进行聚类，在早期扩散步骤中共享计算。结合UnClip的文本到图像先验优化扩散步骤分配。

Result: 实验表明，对于基于图像嵌入条件训练的模型，该方法显著降低了计算成本同时提高了图像质量。方法可无缝集成到现有流程中，随提示词集规模扩展。

Conclusion: 该方法有效减少了大规模文本到图像生成的环境和经济负担，提供了一种高效的计算共享策略。

Abstract: Text-to-image diffusion models enable high-quality image generation but are computationally expensive. While prior work optimizes per-inference efficiency, we explore an orthogonal approach: reducing redundancy across correlated prompts. Our method leverages the coarse-to-fine nature of diffusion models, where early denoising steps capture shared structures among similar prompts. We propose a training-free approach that clusters prompts based on semantic similarity and shares computation in early diffusion steps. Experiments show that for models trained conditioned on image embeddings, our approach significantly reduces compute cost while improving image quality. By leveraging UnClip's text-to-image prior, we enhance diffusion step allocation for greater efficiency. Our method seamlessly integrates with existing pipelines, scales with prompt sets, and reduces the environmental and financial burden of large-scale text-to-image generation. Project page: https://ddecatur.github.io/hierarchical-diffusion/

</details>


### [23] [FW-GAN: Frequency-Driven Handwriting Synthesis with Wave-Modulated MLP Generator](https://arxiv.org/abs/2508.21040)
*Huynh Tong Dang Khoa,Dang Hoai Nam,Vo Nguyen Le Duy*

Main category: cs.CV

TL;DR: FW-GAN是一个基于频率感知的单样本手写合成框架，通过Wave-MLP和频率引导鉴别器生成高质量、风格一致的手写文本，解决了传统方法在长距离依赖和频率信息建模方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 标记手写数据稀缺限制了识别系统的效果，现有合成方法存在两个主要问题：传统卷积架构难以建模长距离依赖和复杂笔画模式，以及忽视频率信息在捕捉手写细节中的关键作用。

Method: 提出FW-GAN框架，包含相位感知Wave-MLP生成器来捕捉空间关系并保留风格线索，频率引导鉴别器利用高频分量增强真实性检测，以及新颖的频率分布损失函数对齐合成与真实手写的频率特征。

Result: 在越南语和英语手写数据集上的实验表明，FW-GAN能够生成高质量、风格一致的手写文本，有效增强低资源手写识别流程的数据。

Conclusion: FW-GAN通过频率感知方法显著提升了手写合成的真实性和风格一致性，为低资源手写识别提供了有价值的数据增强工具。

Abstract: Labeled handwriting data is often scarce, limiting the effectiveness of recognition systems that require diverse, style-consistent training samples. Handwriting synthesis offers a promising solution by generating artificial data to augment training. However, current methods face two major limitations. First, most are built on conventional convolutional architectures, which struggle to model long-range dependencies and complex stroke patterns. Second, they largely ignore the crucial role of frequency information, which is essential for capturing fine-grained stylistic and structural details in handwriting. To address these challenges, we propose FW-GAN, a one-shot handwriting synthesis framework that generates realistic, writer-consistent text from a single example. Our generator integrates a phase-aware Wave-MLP to better capture spatial relationships while preserving subtle stylistic cues. We further introduce a frequency-guided discriminator that leverages high-frequency components to enhance the authenticity detection of generated samples. Additionally, we introduce a novel Frequency Distribution Loss that aligns the frequency characteristics of synthetic and real handwriting, thereby enhancing visual fidelity. Experiments on Vietnamese and English handwriting datasets demonstrate that FW-GAN generates high-quality, style-consistent handwriting, making it a valuable tool for augmenting data in low-resource handwriting recognition (HTR) pipelines. Official implementation is available at https://github.com/DAIR-Group/FW-GAN

</details>


### [24] [OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning](https://arxiv.org/abs/2508.21066)
*Yuan Gong,Xionghui Wang,Jie Wu,Shiyin Wang,Yitong Wang,Xinglong Wu*

Main category: cs.CV

TL;DR: OneReward是一个统一的强化学习框架，使用单一奖励模型提升多任务生成能力，应用于掩码引导图像生成任务，无需任务特定的监督微调。


<details>
  <summary>Details</summary>
Motivation: 现有的多任务生成方法通常需要针对每个任务进行专门的监督微调，这限制了模型的泛化能力和训练效率。不同任务虽然共享相同的条件范式，但在数据分布和评估指标上存在显著差异。

Method: 使用单一视觉语言模型作为生成奖励模型，能够区分给定任务和评估标准下的优胜者和失败者。基于OneReward框架开发Seedream 3.0 Fill模型，通过多任务强化学习直接在预训练基础模型上进行训练。

Result: 实验结果表明，统一的编辑模型在多个评估维度上持续优于商业和开源竞争对手，包括Ideogram、Adobe Photoshop和FLUX Fill [Pro]。

Conclusion: OneReward框架证明了使用单一奖励模型可以有效提升多任务生成性能，无需任务特定的监督微调，为多任务生成模型提供了高效的训练范式。

Abstract: In this paper, we introduce OneReward, a unified reinforcement learning framework that enhances the model's generative capabilities across multiple tasks under different evaluation criteria using only \textit{One Reward} model. By employing a single vision-language model (VLM) as the generative reward model, which can distinguish the winner and loser for a given task and a given evaluation criterion, it can be effectively applied to multi-task generation models, particularly in contexts with varied data and diverse task objectives. We utilize OneReward for mask-guided image generation, which can be further divided into several sub-tasks such as image fill, image extend, object removal, and text rendering, involving a binary mask as the edit area. Although these domain-specific tasks share same conditioning paradigm, they differ significantly in underlying data distributions and evaluation metrics. Existing methods often rely on task-specific supervised fine-tuning (SFT), which limits generalization and training efficiency. Building on OneReward, we develop Seedream 3.0 Fill, a mask-guided generation model trained via multi-task reinforcement learning directly on a pre-trained base model, eliminating the need for task-specific SFT. Experimental results demonstrate that our unified edit model consistently outperforms both commercial and open-source competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across multiple evaluation dimensions. Code and model are available at: https://one-reward.github.io

</details>


### [25] [First-Place Solution to NeurIPS 2024 Invisible Watermark Removal Challenge](https://arxiv.org/abs/2508.21072)
*Fahad Shamshad,Tameem Bakr,Yahia Shaaban,Noor Hussein,Karthik Nandakumar,Nils Lukas*

Main category: cs.CV

TL;DR: 本文提出了针对NeurIPS 2024挑战赛的获胜水印移除方案，包含黑盒和白盒两种攻击方法，实现了95.7%的水印移除率且保持图像质量


<details>
  <summary>Details</summary>
Motivation: 测试现有水印技术对对抗攻击的鲁棒性，推动更强大的图像水印方法发展

Method: 白盒跟踪使用自适应VAE规避攻击，结合测试时优化和CIELAB色彩空间对比度恢复；黑盒跟踪通过图像聚类，应用扩散模型和ChatGPT生成的语义先验进行水印移除

Result: 实现了近完美的水印移除效果（95.7%），对剩余图像质量影响极小

Conclusion: 该方法成功展示了现有水印技术的脆弱性，希望激励开发更鲁棒的图像水印方法

Abstract: Content watermarking is an important tool for the authentication and copyright protection of digital media. However, it is unclear whether existing watermarks are robust against adversarial attacks. We present the winning solution to the NeurIPS 2024 Erasing the Invisible challenge, which stress-tests watermark robustness under varying degrees of adversary knowledge. The challenge consisted of two tracks: a black-box and beige-box track, depending on whether the adversary knows which watermarking method was used by the provider. For the beige-box track, we leverage an adaptive VAE-based evasion attack, with a test-time optimization and color-contrast restoration in CIELAB space to preserve the image's quality. For the black-box track, we first cluster images based on their artifacts in the spatial or frequency-domain. Then, we apply image-to-image diffusion models with controlled noise injection and semantic priors from ChatGPT-generated captions to each cluster with optimized parameter settings. Empirical evaluations demonstrate that our method successfully achieves near-perfect watermark removal (95.7%) with negligible impact on the residual image's quality. We hope that our attacks inspire the development of more robust image watermarking methods.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [26] [Unleashing Uncertainty: Efficient Machine Unlearning for Generative AI](https://arxiv.org/abs/2508.20773)
*Christoforos N. Spartalis,Theodoros Semertzidis,Petros Daras,Efstratios Gavves*

Main category: cs.LG

TL;DR: SAFEMax是一种基于信息论原理的扩散模型机器遗忘方法，通过最大化生成图像的熵，使模型在条件于不允许类别时生成高斯噪声并停止去噪过程


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型中敏感或不允许类别的遗忘问题，同时保持对其他类别的良好生成性能

Method: 基于信息论原理，在早期扩散步骤中最大化生成图像的熵，选择性关注类别信息显著的阶段，控制遗忘与保留的平衡

Result: SAFEMax方法有效实现了目标类别的遗忘，并在效率上显著优于现有最先进方法

Conclusion: SAFEMax为扩散模型提供了一种高效且有效的机器遗忘解决方案，通过信息论方法实现了精确的类别选择性遗忘

Abstract: We introduce SAFEMax, a novel method for Machine Unlearning in diffusion models. Grounded in information-theoretic principles, SAFEMax maximizes the entropy in generated images, causing the model to generate Gaussian noise when conditioned on impermissible classes by ultimately halting its denoising process. Also, our method controls the balance between forgetting and retention by selectively focusing on the early diffusion steps, where class-specific information is prominent. Our results demonstrate the effectiveness of SAFEMax and highlight its substantial efficiency gains over state-of-the-art methods.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [27] [Efficient and Privacy-Protecting Background Removal for 2D Video Streaming using iPhone 15 Pro Max LiDAR](https://arxiv.org/abs/2508.20250)
*Jessica Kinnevan,Naifa Alqahtani,Toral Chauhan*

Main category: eess.IV

TL;DR: 这篇论文探讨了使用iPhone 15 Pro Max上的LiDAR技术来替代传统背景移除技术，利用深度信息实现不受光照条件影响的高性能背景移除。


<details>
  <summary>Details</summary>
Motivation: 传统的背景移除技术如蓝幕拉幕和AI模型存在对光照条件敏感、性能不稳定等问题，LiDAR技术能够提供更稳定且不受光照影响的深度信息。

Method: 集成iPhone 15 Pro Max的LiDAR和颜色摄像头，使用SwiftUI和Swift框架进行界面和后端开发，采用Metal Shader Language(MSL)实现实时图像增强，达到60fps的标准游流帧率。

Result: 系统能够在各种光照条件下稳定运行，但深度数据游流带宽限制了深度图分辨率为320x240，同时LiDAR IR激光对某些材质的深度重建也有限制。

Conclusion: 如果iPhone等移动设备上的LiDAR分辨率能够提升到与颜色图像相同的水平，LiDAR技术有望成为视频和摄影应用中最优称的背景移除方法。

Abstract: Light Detection and Ranging (LiDAR) technology in consumer-grade mobile devices can be used as a replacement for traditional background removal and compositing techniques. Unlike approaches such as chroma keying and trained AI models, LiDAR's depth information is independent of subject lighting, and performs equally well in low-light and well-lit environments. We integrate the LiDAR and color cameras on the iPhone 15 Pro Max with GPU-based image processing. We use Apple's SwiftUI and Swift frameworks for user interface and backend development, and Metal Shader Language (MSL) for realtime image enhancement at the standard iPhone streaming frame rate of 60 frames per second. The only meaningful limitations of the technology are the streaming bandwidth of the depth data, which currently reduces the depth map resolution to 320x240, and any pre-existing limitations of the LiDAR IR laser to reflect accurate depth from some materials. If the LiDAR resolution on a mobile device like the iPhone can be improved to match the color image resolution, LiDAR could feasibly become the preeminent method of background removal for video applications and photography.

</details>


### [28] [GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac MRI Reconstruction](https://arxiv.org/abs/2508.20600)
*Kian Anvari Hamedani,Narges Razizadeh,Shahabedin Nabavi,Mohsen Ebrahimi Moghaddam*

Main category: eess.IV

TL;DR: GENRE-CMR是一种基于GAN的生成对抗网络架构，采用残差深度展开重建框架来提升心脏磁共振图像重建的保真度和泛化能力，通过边缘感知区域损失和统计分布对齐损失实现优异性能。


<details>
  <summary>Details</summary>
Motivation: 解决加速心脏磁共振成像中扫描时间与图像质量之间的权衡问题，特别是在不同采集设置下的泛化能力挑战。

Method: 使用生成对抗网络(GAN)架构，结合残差深度展开重建框架，包含边缘感知区域(EAR)损失和统计分布对齐(SDA)损失来指导网络训练。

Result: 在未见数据分布上达到0.9552 SSIM和38.90 dB PSNR，超越了现有最先进方法，在各种加速因子和采样轨迹下均表现优异。

Conclusion: 该框架为高质量心脏磁共振重建提供了统一且鲁棒的解决方案，为跨异构采集协议的临床适应性部署铺平了道路。

Abstract: Accelerated Cardiovascular Magnetic Resonance (CMR) image reconstruction remains a critical challenge due to the trade-off between scan time and image quality, particularly when generalizing across diverse acquisition settings. We propose GENRE-CMR, a generative adversarial network (GAN)-based architecture employing a residual deep unrolled reconstruction framework to enhance reconstruction fidelity and generalization. The architecture unrolls iterative optimization into a cascade of convolutional subnetworks, enriched with residual connections to enable progressive feature propagation from shallow to deeper stages. To further improve performance, we integrate two loss functions: (1) an Edge-Aware Region (EAR) loss, which guides the network to focus on structurally informative regions and helps prevent common reconstruction blurriness; and (2) a Statistical Distribution Alignment (SDA) loss, which regularizes the feature space across diverse data distributions via a symmetric KL divergence formulation. Extensive experiments confirm that GENRE-CMR surpasses state-of-the-art methods on training and unseen data, achieving 0.9552 SSIM and 38.90 dB PSNR on unseen distributions across various acceleration factors and sampling trajectories. Ablation studies confirm the contribution of each proposed component to reconstruction quality and generalization. Our framework presents a unified and robust solution for high-quality CMR reconstruction, paving the way for clinically adaptable deployment across heterogeneous acquisition protocols.

</details>


### [29] [Efficient Fine-Tuning of DINOv3 Pretrained on Natural Images for Atypical Mitotic Figure Classification in MIDOG 2025](https://arxiv.org/abs/2508.21041)
*Guillaume Balezo,Raphaël Bourgade,Thomas Walter*

Main category: eess.IV

TL;DR: DINOv3-H+视觉变换器通过LoRA微调和数据增强，在MIDOG 2025挑战赛的非典型有丝分裂图像分类任务中取得了0.8871的平衡准确率，证明了其在组织病理学图像上的有效迁移能力。


<details>
  <summary>Details</summary>
Motivation: 非典型有丝分裂图像(AMFs)是预后不良的重要标志物，但由于出现频率低、形态学特征细微以及观察者间差异大，检测难度很高。MIDOG 2025挑战赛为此建立了跨域基准测试。

Method: 使用在自然图像上预训练的DINOv3-H+视觉变换器，采用低秩适应(LoRA)方法进行微调（仅65万可训练参数），并配合广泛的数据增强技术。

Result: 尽管存在领域差异，DINOv3在组织病理学图像上表现出良好的迁移性能，在初步测试集上达到了0.8871的平衡准确率。

Conclusion: DINOv3预训练模型具有强大的鲁棒性，结合参数高效的微调方法，为MIDOG 2025的非典型有丝分裂分类任务提供了强有力的基线模型。

Abstract: Atypical mitotic figures (AMFs) are markers of abnormal cell division associated with poor prognosis, yet their detection remains difficult due to low prevalence, subtle morphology, and inter-observer variability. The MIDOG 2025 challenge introduces a benchmark for AMF classification across multiple domains. In this work, we evaluate the recently published DINOv3-H+ vision transformer, pretrained on natural images, which we fine-tuned using low-rank adaptation (LoRA, 650k trainable parameters) and extensive augmentation. Despite the domain gap, DINOv3 transfers effectively to histopathology, achieving a balanced accuracy of 0.8871 on the preliminary test set. These results highlight the robustness of DINOv3 pretraining and show that, when combined with parameter-efficient fine-tuning, it provides a strong baseline for atypical mitosis classification in MIDOG 2025.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [30] [SPGrasp: Spatiotemporal Prompt-driven Grasp Synthesis in Dynamic Scenes](https://arxiv.org/abs/2508.20547)
*Yunpeng Mei,Hongjie Cao,Yinqiu Xia,Wei Xiao,Zhaohan Feng,Gang Wang,Jie Chen*

Main category: cs.RO

TL;DR: SPGrasp是一个基于SAMv2的实时动态抓取合成框架，通过整合用户提示和时空上下文，实现了59ms的低延迟推理，在多个数据集上达到90%以上的抓取准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在实时交互式动态物体抓取合成中无法同时实现低延迟推理和提示功能，需要解决延迟与交互性之间的权衡问题。

Method: 扩展SAMv2模型用于视频流抓取估计，整合用户提示与时空上下文信息，确保动态物体的时间一致性。

Result: 在OCID数据集上达到90.6%的实例级抓取准确率，Jacquard数据集上93.8%，GraspNet-1Billion数据集上92.0%准确率，延迟降低58.5%，真实世界实验中94.8%的成功率。

Conclusion: SPGrasp有效解决了动态抓取合成中的延迟-交互性权衡问题，实现了实时交互式抓取合成。

Abstract: Real-time interactive grasp synthesis for dynamic objects remains challenging as existing methods fail to achieve low-latency inference while maintaining promptability. To bridge this gap, we propose SPGrasp (spatiotemporal prompt-driven dynamic grasp synthesis), a novel framework extending segment anything model v2 (SAMv2) for video stream grasp estimation. Our core innovation integrates user prompts with spatiotemporal context, enabling real-time interaction with end-to-end latency as low as 59 ms while ensuring temporal consistency for dynamic objects. In benchmark evaluations, SPGrasp achieves instance-level grasp accuracies of 90.6% on OCID and 93.8% on Jacquard. On the challenging GraspNet-1Billion dataset under continuous tracking, SPGrasp achieves 92.0% accuracy with 73.1 ms per-frame latency, representing a 58.5% reduction compared to the prior state-of-the-art promptable method RoG-SAM while maintaining competitive accuracy. Real-world experiments involving 13 moving objects demonstrate a 94.8% success rate in interactive grasping scenarios. These results confirm SPGrasp effectively resolves the latency-interactivity trade-off in dynamic grasp synthesis. Code is available at https://github.com/sejmoonwei/SPGrasp.

</details>
