{"id": "2509.08228", "pdf": "https://arxiv.org/pdf/2509.08228", "abs": "https://arxiv.org/abs/2509.08228", "authors": ["Miao Cao", "Siming Zheng", "Lishun Wang", "Ziyang Chen", "David Brady", "Xin Yuan"], "title": "Sparse Transformer for Ultra-sparse Sampled Video Compressive Sensing", "categories": ["cs.CV"], "comment": null, "summary": "Digital cameras consume ~0.1 microjoule per pixel to capture and encode video, resulting in a power usage of ~20W for a 4K sensor operating at 30 fps. Imagining gigapixel cameras operating at 100-1000 fps, the current processing model is unsustainable. To address this, physical layer compressive measurement has been proposed to reduce power consumption per pixel by 10-100X. Video Snapshot Compressive Imaging (SCI) introduces high frequency modulation in the optical sensor layer to increase effective frame rate. A commonly used sampling strategy of video SCI is Random Sampling (RS) where each mask element value is randomly set to be 0 or 1. Similarly, image inpainting (I2P) has demonstrated that images can be recovered from a fraction of the image pixels. Inspired by I2P, we propose Ultra-Sparse Sampling (USS) regime, where at each spatial location, only one sub-frame is set to 1 and all others are set to 0. We then build a Digital Micro-mirror Device (DMD) encoding system to verify the effectiveness of our USS strategy. Ideally, we can decompose the USS measurement into sub-measurements for which we can utilize I2P algorithms to recover high-speed frames. However, due to the mismatch between the DMD and CCD, the USS measurement cannot be perfectly decomposed. To this end, we propose BSTFormer, a sparse TransFormer that utilizes local Block attention, global Sparse attention, and global Temporal attention to exploit the sparsity of the USS measurement. Extensive results on both simulated and real-world data show that our method significantly outperforms all previous state-of-the-art algorithms. Additionally, an essential advantage of the USS strategy is its higher dynamic range than that of the RS strategy. Finally, from the application perspective, the USS strategy is a good choice to implement a complete video SCI system on chip due to its fixed exposure time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8d85\u7a00\u758f\u91c7\u6837(USS)\u7b56\u7565\uff0c\u901a\u8fc7\u5728\u6bcf\u4e2a\u7a7a\u95f4\u4f4d\u7f6e\u4ec5\u8bbe\u7f6e\u4e00\u4e2a\u5b50\u5e27\u4e3a1\u7684\u65b9\u5f0f\uff0c\u5927\u5e45\u964d\u4f4e\u89c6\u9891\u6444\u50cf\u7684\u7535\u529b\u6d88\u8017\uff0c\u5e76\u8bbe\u8ba1BSTFormer\u6a21\u578b\u6765\u5904\u7406\u91c7\u6837\u6570\u636e\u3002", "motivation": "\u4f20\u7edf\u6570\u7801\u76f8\u673a\u5728\u9ad8\u5206\u8fa8\u7387\u9ad8\u5e27\u7387\u6444\u50cf\u65f6\u7535\u529b\u6d88\u8017\u8fc7\u9ad8\uff0c\u65e0\u6cd5\u652f\u6301\u672a\u6765\u7684\u5409\u50cf\u7d20\u6444\u50cf\u9700\u6c42\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u8282\u80fd\u7684\u91c7\u6837\u6280\u672f\u3002", "method": "\u63d0\u51faUSS\u7a00\u758f\u91c7\u6837\u7b56\u7565\uff0c\u5e76\u5efa\u7acbDMD\u7f16\u7801\u7cfb\u7edf\u9a8c\u8bc1\u3002\u8bbe\u8ba1BSTFormer\u6a21\u578b\uff0c\u7ed3\u5408\u5c40\u90e8\u5750\u6807\u6ce8\u610f\u529b\u3001\u5168\u5c40\u7a00\u758f\u6ce8\u610f\u529b\u548c\u5168\u5c40\u65f6\u95f4\u6ce8\u610f\u529b\u6765\u5904\u7406USS\u91c7\u6837\u6570\u636e\u3002", "result": "\u5728\u6a21\u62df\u548c\u5b9e\u9645\u6570\u636e\u4e0a\uff0c\u65b9\u6cd5\u8868\u73b0\u663e\u8457\u8d85\u8fc7\u4ee5\u5f80\u6700\u4f18\u7b97\u6cd5\u3002USS\u7b56\u7565\u8fd8\u5177\u6709\u6bd4\u968f\u673a\u91c7\u6837\u66f4\u9ad8\u7684\u52a8\u6001\u8303\u56f4\u548c\u56fa\u5b9a\u66dd\u5149\u65f6\u95f4\u4f18\u52bf\u3002", "conclusion": "USS\u7b56\u7565\u901a\u8fc7\u6781\u7a00\u758f\u91c7\u6837\u5b9e\u73b0\u4e86\u9ad8\u6548\u80fd\u7684\u89c6\u9891\u91c7\u96c6\uff0c\u4e3a\u672a\u6765\u9ad8\u901f\u9ad8\u5206\u8fa8\u7387\u6444\u50cf\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u96a7\u9053\u5f0f\u82af\u7247\u96c6\u6210\u3002"}}
{"id": "2509.08260", "pdf": "https://arxiv.org/pdf/2509.08260", "abs": "https://arxiv.org/abs/2509.08260", "authors": ["Chi Zhang", "Xiang Zhang", "Chenxu Jiang", "Gui-Song Xia", "Lei Yu"], "title": "EVDI++: Event-based Video Deblurring and Interpolation via Self-Supervised Learning", "categories": ["cs.CV"], "comment": "18 pages", "summary": "Frame-based cameras with extended exposure times often produce perceptible visual blurring and information loss between frames, significantly degrading video quality. To address this challenge, we introduce EVDI++, a unified self-supervised framework for Event-based Video Deblurring and Interpolation that leverages the high temporal resolution of event cameras to mitigate motion blur and enable intermediate frame prediction. Specifically, the Learnable Double Integral (LDI) network is designed to estimate the mapping relation between reference frames and sharp latent images. Then, we refine the coarse results and optimize overall training efficiency by introducing a learning-based division reconstruction module, enabling images to be converted with varying exposure intervals. We devise an adaptive parameter-free fusion strategy to obtain the final results, utilizing the confidence embedded in the LDI outputs of concurrent events. A self-supervised learning framework is proposed to enable network training with real-world blurry videos and events by exploring the mutual constraints among blurry frames, latent images, and event streams. We further construct a dataset with real-world blurry images and events using a DAVIS346c camera, demonstrating the generalizability of the proposed EVDI++ in real-world scenarios. Extensive experiments on both synthetic and real-world datasets show that our method achieves state-of-the-art performance in video deblurring and interpolation tasks.", "AI": {"tldr": "EVDI++\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u81ea\u76d1\u7763\u89c6\u9891\u53bb\u6a21\u7cca\u548c\u63d2\u503c\u6846\u67b6\uff0c\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u6765\u89e3\u51b3\u4f20\u7edf\u76f8\u673a\u957f\u66dd\u5149\u5bfc\u81f4\u7684\u8fd0\u52a8\u6a21\u7cca\u95ee\u9898\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5e27\u5f0f\u76f8\u673a\u5728\u957f\u66dd\u5149\u65f6\u4f1a\u4ea7\u751f\u660e\u663e\u7684\u89c6\u89c9\u6a21\u7cca\u548c\u5e27\u95f4\u4fe1\u606f\u4e22\u5931\uff0c\u4e25\u91cd\u5f71\u54cd\u89c6\u9891\u8d28\u91cf\u3002\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u7279\u6027\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51faLearnable Double Integral (LDI)\u7f51\u7edc\u4f30\u8ba1\u53c2\u8003\u5e27\u4e0e\u6e05\u6670\u6f5c\u5728\u56fe\u50cf\u7684\u6620\u5c04\u5173\u7cfb\uff1b\u5f15\u5165\u57fa\u4e8e\u5b66\u4e60\u7684\u9664\u6cd5\u91cd\u5efa\u6a21\u5757\u4f18\u5316\u7ed3\u679c\u548c\u8bad\u7ec3\u6548\u7387\uff1b\u8bbe\u8ba1\u81ea\u9002\u5e94\u65e0\u53c2\u6570\u878d\u5408\u7b56\u7565\u5229\u7528LDI\u8f93\u51fa\u7684\u7f6e\u4fe1\u5ea6\uff1b\u6784\u5efa\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u5229\u7528\u6a21\u7cca\u5e27\u3001\u6f5c\u5728\u56fe\u50cf\u548c\u4e8b\u4ef6\u6d41\u4e4b\u95f4\u7684\u76f8\u4e92\u7ea6\u675f\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u9891\u53bb\u6a21\u7cca\u548c\u63d2\u503c\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u4f7f\u7528DAVIS346c\u76f8\u673a\u6784\u5efa\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "EVDI++\u6210\u529f\u5730\u5c06\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u4f18\u52bf\u5e94\u7528\u4e8e\u89c6\u9891\u53bb\u6a21\u7cca\u548c\u63d2\u503c\u4efb\u52a1\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u81ea\u6211\u76d1\u7763\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u76f8\u673a\u957f\u66dd\u5149\u5bfc\u81f4\u7684\u8fd0\u52a8\u6a21\u7cca\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08376", "pdf": "https://arxiv.org/pdf/2509.08376", "abs": "https://arxiv.org/abs/2509.08376", "authors": ["Xiao Li", "Qi Chen", "Xiulian Peng", "Kai Yu", "Xie Chen", "Yan Lu"], "title": "Bitrate-Controlled Diffusion for Disentangling Motion and Content in Video", "categories": ["cs.CV"], "comment": null, "summary": "We propose a novel and general framework to disentangle video data into its dynamic motion and static content components. Our proposed method is a self-supervised pipeline with less assumptions and inductive biases than previous works: it utilizes a transformer-based architecture to jointly generate flexible implicit features for frame-wise motion and clip-wise content, and incorporates a low-bitrate vector quantization as an information bottleneck to promote disentanglement and form a meaningful discrete motion space. The bitrate-controlled latent motion and content are used as conditional inputs to a denoising diffusion model to facilitate self-supervised representation learning. We validate our disentangled representation learning framework on real-world talking head videos with motion transfer and auto-regressive motion generation tasks. Furthermore, we also show that our method can generalize to other types of video data, such as pixel sprites of 2D cartoon characters. Our work presents a new perspective on self-supervised learning of disentangled video representations, contributing to the broader field of video analysis and generation.", "AI": {"tldr": "\u901a\u8fc7\u53d8\u636e\u5668\u67b6\u6784\u548c\u4f4e\u7801\u7387\u5411\u91cf\u91cf\u5316\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u7684\u89c6\u9891\u52a8\u6001\u8fd0\u52a8\u4e0e\u9759\u6001\u5185\u5bb9\u89e3\u8026\u8868\u5f81\u5b66\u4e60\u6846\u67b6", "motivation": "\u89e3\u51b3\u89c6\u9891\u6570\u636e\u4e2d\u52a8\u6001\u8fd0\u52a8\u4e0e\u9759\u6001\u5185\u5bb9\u7684\u6df7\u5408\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u504f\u8f7b\u504f\u5dee\u66f4\u5c11\u7684\u81ea\u76d1\u7763\u89e3\u8026\u65b9\u6cd5", "method": "\u4f7f\u7528\u53d8\u636e\u5668\u67b6\u6784\u8054\u5408\u751f\u6210\u5e27\u95f4\u8fd0\u52a8\u548c\u5267\u60c5\u5185\u5bb9\u9690\u5f0f\u7279\u5f81\uff0c\u901a\u8fc7\u4f4e\u7801\u7387\u5411\u91cf\u91cf\u5316\u4f5c\u4e3a\u4fe1\u606f\u74f6\u9888\u4fc3\u8fdb\u89e3\u8026\uff0c\u5e76\u7528\u4e8e\u964d\u566a\u6269\u6563\u6a21\u578b\u7684\u6761\u4ef6\u8f93\u5165", "result": "\u5728\u8bf4\u8bdd\u5934\u50cf\u89c6\u9891\u7684\u8fd0\u52a8\u8f6c\u79fb\u548c\u81ea\u56de\u5f52\u8fd0\u52a8\u751f\u6210\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u540c\u65f6\u80fd\u591f\u6cdb\u5316\u52302D\u5361\u901a\u89d2\u8272\u7b49\u5176\u4ed6\u7c7b\u578b\u89c6\u9891\u6570\u636e", "conclusion": "\u4e3a\u81ea\u76d1\u7763\u89e3\u8026\u89c6\u9891\u8868\u5f81\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5bf9\u89c6\u9891\u5206\u6790\u548c\u751f\u6210\u9886\u57df\u505a\u51fa\u4e86\u8d21\u732e"}}
{"id": "2509.08392", "pdf": "https://arxiv.org/pdf/2509.08392", "abs": "https://arxiv.org/abs/2509.08392", "authors": ["Cuong Nguyen", "Dung T. Tran", "Hong Nguyen", "Xuan-Vu Phan", "Nam-Phong Nguyen"], "title": "VRAE: Vertical Residual Autoencoder for License Plate Denoising and Deblurring", "categories": ["cs.CV"], "comment": null, "summary": "In real-world traffic surveillance, vehicle images captured under adverse weather, poor lighting, or high-speed motion often suffer from severe noise and blur. Such degradations significantly reduce the accuracy of license plate recognition systems, especially when the plate occupies only a small region within the full vehicle image. Restoring these degraded images a fast realtime manner is thus a crucial pre-processing step to enhance recognition performance. In this work, we propose a Vertical Residual Autoencoder (VRAE) architecture designed for the image enhancement task in traffic surveillance. The method incorporates an enhancement strategy that employs an auxiliary block, which injects input-aware features at each encoding stage to guide the representation learning process, enabling better general information preservation throughout the network compared to conventional autoencoders. Experiments on a vehicle image dataset with visible license plates demonstrate that our method consistently outperforms Autoencoder (AE), Generative Adversarial Network (GAN), and Flow-Based (FB) approaches. Compared with AE at the same depth, it improves PSNR by about 20\\%, reduces NMSE by around 50\\%, and enhances SSIM by 1\\%, while requiring only a marginal increase of roughly 1\\% in parameters.", "AI": {"tldr": "\u63d0\u51fa\u5782\u76f4\u6b8b\u5dee\u81ea\u7f16\u7801\u5668(VRAE)\u7528\u4e8e\u4ea4\u901a\u76d1\u63a7\u56fe\u50cf\u589e\u5f3a\uff0c\u901a\u8fc7\u8f85\u52a9\u5757\u6ce8\u5165\u8f93\u5165\u611f\u77e5\u7279\u5f81\u6765\u6307\u5bfc\u8868\u793a\u5b66\u4e60\uff0c\u5728\u4fdd\u6301\u53c2\u6570\u5c11\u91cf\u589e\u52a0\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u6307\u6807", "motivation": "\u73b0\u5b9e\u4ea4\u901a\u76d1\u63a7\u4e2d\uff0c\u6076\u52a3\u5929\u6c14\u3001\u5149\u7167\u4e0d\u8db3\u6216\u9ad8\u901f\u8fd0\u52a8\u5bfc\u81f4\u8f66\u8f86\u56fe\u50cf\u4e25\u91cd\u566a\u58f0\u548c\u6a21\u7cca\uff0c\u7279\u522b\u662f\u8f66\u724c\u533a\u57df\u8f83\u5c0f\u7684\u60c5\u51b5\u4f1a\u663e\u8457\u964d\u4f4e\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u9700\u8981\u5b9e\u65f6\u56fe\u50cf\u589e\u5f3a\u4f5c\u4e3a\u9884\u5904\u7406\u6b65\u9aa4", "method": "\u5782\u76f4\u6b8b\u5dee\u81ea\u7f16\u7801\u5668\u67b6\u6784\uff0c\u5728\u6bcf\u4e2a\u7f16\u7801\u9636\u6bb5\u4f7f\u7528\u8f85\u52a9\u5757\u6ce8\u5165\u8f93\u5165\u611f\u77e5\u7279\u5f81\u6765\u6307\u5bfc\u8868\u793a\u5b66\u4e60\u8fc7\u7a0b\uff0c\u6bd4\u4f20\u7edf\u81ea\u7f16\u7801\u5668\u80fd\u66f4\u597d\u5730\u4fdd\u6301\u901a\u7528\u4fe1\u606f", "result": "\u5728\u8f66\u724c\u53ef\u89c1\u7684\u8f66\u8f86\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5 consistently \u4f18\u4e8e\u81ea\u7f16\u7801\u5668\u3001\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u548c\u57fa\u4e8e\u6d41\u7684\u65b9\u6cd5\u3002\u4e0e\u540c\u6df1\u5ea6\u81ea\u7f16\u7801\u5668\u76f8\u6bd4\uff0cPSNR\u63d0\u5347\u7ea620%\uff0cNMSE\u964d\u4f4e\u7ea650%\uff0cSSIM\u63d0\u53471%\uff0c\u53c2\u6570\u4ec5\u589e\u52a0\u7ea61%", "conclusion": "VRAE\u67b6\u6784\u901a\u8fc7\u8f93\u5165\u611f\u77e5\u7279\u5f81\u6ce8\u5165\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u4ea4\u901a\u76d1\u63a7\u56fe\u50cf\u589e\u5f3a\u6027\u80fd\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u6539\u5584\u4e86\u56fe\u50cf\u8d28\u91cf\u6307\u6807"}}
{"id": "2509.08422", "pdf": "https://arxiv.org/pdf/2509.08422", "abs": "https://arxiv.org/abs/2509.08422", "authors": ["Payal Varshney", "Adriano Lucieri", "Christoph Balada", "Sheraz Ahmed", "Andreas Dengel"], "title": "LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations", "categories": ["cs.CV", "cs.LG"], "comment": "30 pages", "summary": "Video-based AI systems are increasingly adopted in safety-critical domains such as autonomous driving and healthcare. However, interpreting their decisions remains challenging due to the inherent spatiotemporal complexity of video data and the opacity of deep learning models. Existing explanation techniques often suffer from limited temporal coherence, insufficient robustness, and a lack of actionable causal insights. Current counterfactual explanation methods typically do not incorporate guidance from the target model, reducing semantic fidelity and practical utility. We introduce Latent Diffusion for Video Counterfactual Explanations (LD-ViCE), a novel framework designed to explain the behavior of video-based AI models. Compared to previous approaches, LD-ViCE reduces the computational costs of generating explanations by operating in latent space using a state-of-the-art diffusion model, while producing realistic and interpretable counterfactuals through an additional refinement step. Our experiments demonstrate the effectiveness of LD-ViCE across three diverse video datasets, including EchoNet-Dynamic (cardiac ultrasound), FERV39k (facial expression), and Something-Something V2 (action recognition). LD-ViCE outperforms a recent state-of-the-art method, achieving an increase in R2 score of up to 68% while reducing inference time by half. Qualitative analysis confirms that LD-ViCE generates semantically meaningful and temporally coherent explanations, offering valuable insights into the target model behavior. LD-ViCE represents a valuable step toward the trustworthy deployment of AI in safety-critical domains.", "AI": {"tldr": "LD-ViCE\u662f\u4e00\u4e2a\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u53cd\u4e8b\u5b9e\u89e3\u91ca\u6846\u67b6\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u64cd\u4f5c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u751f\u6210\u73b0\u5b9e\u4e14\u53ef\u89e3\u91ca\u7684\u53cd\u4e8b\u5b9e\u89c6\u9891\u89e3\u91ca\uff0c\u5728\u591a\u4e2a\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89c6\u9891AI\u7cfb\u7edf\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u73b0\u6709\u89e3\u91ca\u6280\u672f\u5b58\u5728\u65f6\u95f4\u8fde\u8d2f\u6027\u4e0d\u8db3\u3001\u9c81\u68d2\u6027\u5dee\u3001\u7f3a\u4e4f\u56e0\u679c\u6d1e\u5bdf\u7b49\u95ee\u9898\uff0c\u4e14\u5f53\u524d\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\u901a\u5e38\u672a\u7ed3\u5408\u76ee\u6807\u6a21\u578b\u6307\u5bfc\uff0c\u964d\u4f4e\u4e86\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u5b9e\u7528\u4ef7\u503c\u3002", "method": "\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u6269\u6563\u6a21\u578b\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u901a\u8fc7\u989d\u5916\u7684\u7cbe\u70bc\u6b65\u9aa4\u751f\u6210\u73b0\u5b9e\u4e14\u53ef\u89e3\u91ca\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u751f\u6210\u89e3\u91ca\u7684\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u89c6\u9891\u6570\u636e\u96c6\uff08EchoNet-Dynamic\u3001FERV39k\u3001Something-Something V2\uff09\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cLD-ViCE\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0cR2\u5206\u6570\u63d0\u5347\u9ad8\u8fbe68%\uff0c\u540c\u65f6\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u4e00\u534a\u3002\u5b9a\u6027\u5206\u6790\u786e\u8ba4\u751f\u6210\u7684\u89e3\u91ca\u5177\u6709\u8bed\u4e49\u610f\u4e49\u548c\u65f6\u95f4\u8fde\u8d2f\u6027\u3002", "conclusion": "LD-ViCE\u4ee3\u8868\u4e86\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u5b9e\u73b0\u53ef\u4fe1AI\u90e8\u7f72\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u80fd\u591f\u4e3a\u89c6\u9891AI\u6a21\u578b\u884c\u4e3a\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u6d1e\u5bdf\u3002"}}
{"id": "2509.08442", "pdf": "https://arxiv.org/pdf/2509.08442", "abs": "https://arxiv.org/abs/2509.08442", "authors": ["Ivan Stoyanov", "Fabian Bongratz", "Christian Wachinger"], "title": "Spherical Brownian Bridge Diffusion Models for Conditional Cortical Thickness Forecasting", "categories": ["cs.CV", "cs.AI", "cs.LG", "q-bio.NC"], "comment": null, "summary": "Accurate forecasting of individualized, high-resolution cortical thickness (CTh) trajectories is essential for detecting subtle cortical changes, providing invaluable insights into neurodegenerative processes and facilitating earlier and more precise intervention strategies. However, CTh forecasting is a challenging task due to the intricate non-Euclidean geometry of the cerebral cortex and the need to integrate multi-modal data for subject-specific predictions. To address these challenges, we introduce the Spherical Brownian Bridge Diffusion Model (SBDM). Specifically, we propose a bidirectional conditional Brownian bridge diffusion process to forecast CTh trajectories at the vertex level of registered cortical surfaces. Our technical contribution includes a new denoising model, the conditional spherical U-Net (CoS-UNet), which combines spherical convolutions and dense cross-attention to integrate cortical surfaces and tabular conditions seamlessly. Compared to previous approaches, SBDM achieves significantly reduced prediction errors, as demonstrated by our experiments based on longitudinal datasets from the ADNI and OASIS. Additionally, we demonstrate SBDM's ability to generate individual factual and counterfactual CTh trajectories, offering a novel framework for exploring hypothetical scenarios of cortical development.", "AI": {"tldr": "\u63d0\u51faSBDM\u6a21\u578b\uff0c\u901a\u8fc7\u7403\u5f62\u5e03\u6717\u6865\u6269\u6563\u8fc7\u7a0b\u9884\u6d4b\u4e2a\u4f53\u5316\u76ae\u5c42\u539a\u5ea6\u8f68\u8ff9\uff0c\u5728ADNI\u548cOASIS\u6570\u636e\u96c6\u4e0a\u663e\u8457\u964d\u4f4e\u9884\u6d4b\u8bef\u5dee", "motivation": "\u51c6\u786e\u9884\u6d4b\u4e2a\u4f53\u5316\u9ad8\u5206\u8fa8\u7387\u76ae\u5c42\u539a\u5ea6\u8f68\u8ff9\u5bf9\u68c0\u6d4b\u795e\u7ecf\u9000\u884c\u6027\u75c5\u53d8\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u76ae\u5c42\u590d\u6742\u51e0\u4f55\u7ed3\u6784\u548c\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u7684\u6311\u6218", "method": "\u4f7f\u7528\u53cc\u5411\u6761\u4ef6\u5e03\u6717\u6865\u6269\u6563\u8fc7\u7a0b\uff0c\u7ed3\u5408\u6761\u4ef6\u7403\u5f62U-Net\uff08CoS-UNet\uff09\u8fdb\u884c\u53bb\u566a\uff0c\u6574\u5408\u7403\u5f62\u5377\u79ef\u548c\u5bc6\u96c6\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u9884\u6d4b\u8bef\u5dee\uff0c\u80fd\u591f\u751f\u6210\u4e2a\u4f53\u4e8b\u5b9e\u548c\u53cd\u4e8b\u5b9e\u76ae\u5c42\u539a\u5ea6\u8f68\u8ff9", "conclusion": "SBDM\u4e3a\u63a2\u7d22\u76ae\u5c42\u53d1\u80b2\u5047\u8bbe\u573a\u666f\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\uff0c\u5728\u795e\u7ecf\u5f71\u50cf\u5206\u6790\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c"}}
{"id": "2509.08458", "pdf": "https://arxiv.org/pdf/2509.08458", "abs": "https://arxiv.org/abs/2509.08458", "authors": ["Yujie Zhu", "Xinyi Zhang", "Yekai Lu", "Guang Yang", "Faming Fang", "Guixu Zhang"], "title": "First-order State Space Model for Lightweight Image Super-resolution", "categories": ["cs.CV"], "comment": "Accept by ICASSP 2025 (Oral)", "summary": "State space models (SSMs), particularly Mamba, have shown promise in NLP tasks and are increasingly applied to vision tasks. However, most Mamba-based vision models focus on network architecture and scan paths, with little attention to the SSM module. In order to explore the potential of SSMs, we modified the calculation process of SSM without increasing the number of parameters to improve the performance on lightweight super-resolution tasks. In this paper, we introduce the First-order State Space Model (FSSM) to improve the original Mamba module, enhancing performance by incorporating token correlations. We apply a first-order hold condition in SSMs, derive the new discretized form, and analyzed cumulative error. Extensive experimental results demonstrate that FSSM improves the performance of MambaIR on five benchmark datasets without additionally increasing the number of parameters, and surpasses current lightweight SR methods, achieving state-of-the-art results.", "AI": {"tldr": "\u63d0\u51fa\u4e86First-order State Space Model (FSSM)\u6765\u6539\u8fdbMamba\u6a21\u5757\uff0c\u901a\u8fc7\u4e00\u9636\u4fdd\u6301\u6761\u4ef6\u548c\u65b0\u7684\u79bb\u6563\u5316\u5f62\u5f0f\u63d0\u5347\u8f7b\u91cf\u7ea7\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5728\u4e0d\u589e\u52a0\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684Mamba\u89c6\u89c9\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u7f51\u7edc\u67b6\u6784\u548c\u626b\u63cf\u8def\u5f84\uff0c\u800c\u5f88\u5c11\u5173\u6ce8SSM\u6a21\u5757\u672c\u8eab\u3002\u4e3a\u4e86\u63a2\u7d22SSM\u7684\u6f5c\u529b\uff0c\u9700\u8981\u6539\u8fdbSSM\u8ba1\u7b97\u8fc7\u7a0b\u6765\u63d0\u5347\u8f7b\u91cf\u7ea7\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u5f15\u5165\u4e00\u9636\u4fdd\u6301\u6761\u4ef6\u5230SSM\u4e2d\uff0c\u63a8\u5bfc\u51fa\u65b0\u7684\u79bb\u6563\u5316\u5f62\u5f0f\uff0c\u5e76\u5206\u6790\u7d2f\u79ef\u8bef\u5dee\u3002\u63d0\u51fa\u4e86First-order State Space Model (FSSM)\u6765\u6539\u8fdb\u539f\u59cbMamba\u6a21\u5757\uff0c\u901a\u8fc7\u878d\u5165token\u76f8\u5173\u6027\u6765\u589e\u5f3a\u6027\u80fd\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cFSSM\u5728\u4e0d\u589e\u52a0\u989d\u5916\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u4e86MambaIR\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u5f53\u524d\u7684\u8f7b\u91cf\u7ea7\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002", "conclusion": "FSSM\u901a\u8fc7\u6539\u8fdbSSM\u6a21\u5757\u7684\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8f7b\u91cf\u7ea7\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5728\u4fdd\u6301\u53c2\u6570\u6548\u7387\u7684\u540c\u65f6\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2509.08489", "pdf": "https://arxiv.org/pdf/2509.08489", "abs": "https://arxiv.org/abs/2509.08489", "authors": ["Kaleem Ahmad"], "title": "Prompt-Driven Image Analysis with Multimodal Generative AI: Detection, Segmentation, Inpainting, and Interpretation", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages. Preprint", "summary": "Prompt-driven image analysis converts a single natural-language instruction into multiple steps: locate, segment, edit, and describe. We present a practical case study of a unified pipeline that combines open-vocabulary detection, promptable segmentation, text-conditioned inpainting, and vision-language description into a single workflow. The system works end to end from a single prompt, retains intermediate artifacts for transparent debugging (such as detections, masks, overlays, edited images, and before and after composites), and provides the same functionality through an interactive UI and a scriptable CLI for consistent, repeatable runs. We highlight integration choices that reduce brittleness, including threshold adjustments, mask inspection with light morphology, and resource-aware defaults. In a small, single-word prompt segment, detection and segmentation produced usable masks in over 90% of cases with an accuracy above 85% based on our criteria. On a high-end GPU, inpainting makes up 60 to 75% of total runtime under typical guidance and sampling settings, which highlights the need for careful tuning. The study offers implementation-guided advice on thresholds, mask tightness, and diffusion parameters, and details version pinning, artifact logging, and seed control to support replay. Our contribution is a transparent, reliable pattern for assembling modern vision and multimodal models behind a single prompt, with clear guardrails and operational practices that improve reliability in object replacement, scene augmentation, and removal.", "AI": {"tldr": "\u57fa\u4e8e\u5355\u4e2a\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u7684\u7edf\u4e00\u56fe\u50cf\u5206\u6790\u7ba1\u9053\uff0c\u7ed3\u5408\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u3001\u63d0\u793a\u5206\u5272\u3001\u6587\u672c\u6761\u4ef6\u585e\u5145\u548c\u89c6\u89c9\u8bed\u8a00\u63cf\u8ff0\u529f\u80fd", "motivation": "\u5efa\u7acb\u4e00\u4e2a\u4ece\u5355\u4e2a\u63d0\u793a\u5230\u591a\u6b65\u64cd\u4f5c\u7684\u7edf\u4e00\u5de5\u4f5c\u6d41\uff0c\u63d0\u9ad8\u56fe\u50cf\u5206\u6790\u7684\u53ef\u9760\u6027\u548c\u53ef\u91cd\u73b0\u6027", "method": "\u7ec4\u5408\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u3001\u63d0\u793a\u5206\u5272\u3001\u6587\u672c\u6761\u4ef6\u585e\u5145\u548c\u89c6\u89c9\u8bed\u8a00\u63cf\u8ff0\u7684\u7edf\u4e00\u7ba1\u9053\uff0c\u5305\u62ec\u9608\u503c\u8c03\u6574\u3001\u9762\u5177\u68c0\u67e5\u3001\u8d44\u6e90\u8ba4\u77e5\u9ed8\u8ba4\u503c\u7b49\u96c6\u6210\u9009\u62e9", "result": "\u5728\u5355\u8bcd\u63d0\u793a\u5206\u5272\u4e2d\uff0c\u68c0\u6d4b\u548c\u5206\u5272\u572890%\u4ee5\u4e0a\u60c5\u51b5\u4e0b\u751f\u6210\u53ef\u7528\u9762\u5177\uff0c\u51c6\u786e\u7387\u8d8515%\uff1b\u5728\u9ad8\u7aefGPU\u4e0a\uff0c\u585e\u5145\u5360\u603b\u8fd0\u884c\u65f6\u95f4\u768460-75%", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u900f\u660e\u3001\u53ef\u9760\u7684\u6a21\u5f0f\uff0c\u901a\u8fc7\u660e\u786e\u7684\u62a4\u680f\u548c\u8fd0\u8425\u5b9e\u8df5\u63d0\u9ad8\u5bf9\u8c61\u66ff\u6362\u3001\u573a\u666f\u589e\u5f3a\u548c\u5220\u9664\u7684\u53ef\u9760\u6027"}}
{"id": "2509.08519", "pdf": "https://arxiv.org/pdf/2509.08519", "abs": "https://arxiv.org/abs/2509.08519", "authors": ["Liyang Chen", "Tianxiang Ma", "Jiawei Liu", "Bingchuan Li", "Zhuowei Chen", "Lijie Liu", "Xu He", "Gen Li", "Qian He", "Zhiyong Wu"], "title": "HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Human-Centric Video Generation (HCVG) methods seek to synthesize human videos from multimodal inputs, including text, image, and audio. Existing methods struggle to effectively coordinate these heterogeneous modalities due to two challenges: the scarcity of training data with paired triplet conditions and the difficulty of collaborating the sub-tasks of subject preservation and audio-visual sync with multimodal inputs. In this work, we present HuMo, a unified HCVG framework for collaborative multimodal control. For the first challenge, we construct a high-quality dataset with diverse and paired text, reference images, and audio. For the second challenge, we propose a two-stage progressive multimodal training paradigm with task-specific strategies. For the subject preservation task, to maintain the prompt following and visual generation abilities of the foundation model, we adopt the minimal-invasive image injection strategy. For the audio-visual sync task, besides the commonly adopted audio cross-attention layer, we propose a focus-by-predicting strategy that implicitly guides the model to associate audio with facial regions. For joint learning of controllabilities across multimodal inputs, building on previously acquired capabilities, we progressively incorporate the audio-visual sync task. During inference, for flexible and fine-grained multimodal control, we design a time-adaptive Classifier-Free Guidance strategy that dynamically adjusts guidance weights across denoising steps. Extensive experimental results demonstrate that HuMo surpasses specialized state-of-the-art methods in sub-tasks, establishing a unified framework for collaborative multimodal-conditioned HCVG. Project Page: https://phantom-video.github.io/HuMo.", "AI": {"tldr": "HuMo\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u4eba\u7c7b\u4e2d\u5fc3\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\u548c\u4efb\u52a1\u7279\u5b9a\u7b56\u7565\uff0c\u6709\u6548\u534f\u8c03\u6587\u672c\u3001\u56fe\u50cf\u548c\u97f3\u9891\u7b49\u591a\u6a21\u6001\u8f93\u5165\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4e3b\u4f53\u4fdd\u6301\u548c\u97f3\u89c6\u9891\u540c\u6b65\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u7c7b\u4e2d\u5fc3\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5728\u5904\u7406\u5f02\u6784\u591a\u6a21\u6001\u8f93\u5165\u65f6\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u914d\u5bf9\u4e09\u5143\u7ec4\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u7f3a\u6027\uff0c\u4ee5\u53ca\u4e3b\u4f53\u4fdd\u6301\u548c\u97f3\u89c6\u9891\u540c\u6b65\u5b50\u4efb\u52a1\u5728\u591a\u6a21\u6001\u8f93\u5165\u4e0b\u7684\u534f\u4f5c\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6e10\u8fdb\u5f0f\u591a\u6a21\u6001\u8bad\u7ec3\u8303\u5f0f\uff1a1) \u6784\u5efa\u9ad8\u8d28\u91cf\u914d\u5bf9\u6570\u636e\u96c6\uff1b2) \u91c7\u7528\u6700\u5c0f\u4fb5\u5165\u5f0f\u56fe\u50cf\u6ce8\u5165\u7b56\u7565\u4fdd\u6301\u4e3b\u4f53\uff1b3) \u63d0\u51fa\u7126\u70b9\u9884\u6d4b\u7b56\u7565\u5173\u8054\u97f3\u9891\u4e0e\u9762\u90e8\u533a\u57df\uff1b4) \u8bbe\u8ba1\u65f6\u95f4\u81ea\u9002\u5e94\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u7b56\u7565\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHuMo\u5728\u5404\u9879\u5b50\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u4e13\u95e8\u7684\u5148\u8fdb\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6761\u4ef6\u4eba\u7c7b\u4e2d\u5fc3\u89c6\u9891\u751f\u6210\u6846\u67b6\u3002", "conclusion": "HuMo\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u67b6\u6784\u8bbe\u8ba1\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u8f93\u5165\u7684\u534f\u540c\u63a7\u5236\uff0c\u4e3a\u4eba\u7c7b\u4e2d\u5fc3\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08628", "pdf": "https://arxiv.org/pdf/2509.08628", "abs": "https://arxiv.org/abs/2509.08628", "authors": ["Xuqin Wang", "Tao Wu", "Yanfeng Zhang", "Lu Liu", "Dong Wang", "Mingwei Sun", "Yongliang Wang", "Niclas Zeller", "Daniel Cremers"], "title": "LADB: Latent Aligned Diffusion Bridges for Semi-Supervised Domain Translation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models excel at generating high-quality outputs but face challenges in data-scarce domains, where exhaustive retraining or costly paired data are often required. To address these limitations, we propose Latent Aligned Diffusion Bridges (LADB), a semi-supervised framework for sample-to-sample translation that effectively bridges domain gaps using partially paired data. By aligning source and target distributions within a shared latent space, LADB seamlessly integrates pretrained source-domain diffusion models with a target-domain Latent Aligned Diffusion Model (LADM), trained on partially paired latent representations. This approach enables deterministic domain mapping without the need for full supervision. Compared to unpaired methods, which often lack controllability, and fully paired approaches that require large, domain-specific datasets, LADB strikes a balance between fidelity and diversity by leveraging a mixture of paired and unpaired latent-target couplings. Our experimental results demonstrate superior performance in depth-to-image translation under partial supervision. Furthermore, we extend LADB to handle multi-source translation (from depth maps and segmentation masks) and multi-target translation in a class-conditioned style transfer task, showcasing its versatility in handling diverse and heterogeneous use cases. Ultimately, we present LADB as a scalable and versatile solution for real-world domain translation, particularly in scenarios where data annotation is costly or incomplete.", "AI": {"tldr": "LADB\u662f\u4e00\u4e2a\u534a\u76d1\u7763\u7684\u6837\u672c\u5230\u6837\u672c\u8f6c\u6362\u6846\u67b6\uff0c\u5229\u7528\u90e8\u5206\u914d\u5bf9\u6570\u636e\u5728\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u6e90\u57df\u548c\u76ee\u6807\u57df\u5206\u5e03\uff0c\u65e0\u9700\u5b8c\u5168\u76d1\u7763\u5373\u53ef\u5b9e\u73b0\u786e\u5b9a\u6027\u57df\u6620\u5c04\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u6570\u636e\u7a00\u7f3a\u9886\u57df\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u5927\u91cf\u91cd\u65b0\u8bad\u7ec3\u6216\u6602\u8d35\u7684\u914d\u5bf9\u6570\u636e\u3002LADB\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u6216\u4e0d\u5b8c\u6574\u7684\u5b9e\u9645\u573a\u666f\u4e2d\u3002", "method": "\u901a\u8fc7\u5728\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u6e90\u57df\u548c\u76ee\u6807\u57df\u5206\u5e03\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u6e90\u57df\u6269\u6563\u6a21\u578b\u4e0e\u76ee\u6807\u57df\u6f5c\u5728\u5bf9\u9f50\u6269\u6563\u6a21\u578b\uff08LADM\uff09\u96c6\u6210\uff0c\u5229\u7528\u90e8\u5206\u914d\u5bf9\u7684\u6f5c\u5728\u8868\u793a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728\u90e8\u5206\u76d1\u7763\u4e0b\u7684\u6df1\u5ea6\u5230\u56fe\u50cf\u8f6c\u6362\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u6269\u5c55\u5230\u591a\u6e90\u8f6c\u6362\uff08\u6df1\u5ea6\u56fe\u548c\u5206\u5272\u63a9\u7801\uff09\u548c\u591a\u76ee\u6807\u8f6c\u6362\u7684\u7c7b\u6761\u4ef6\u98ce\u683c\u8fc1\u79fb\u4efb\u52a1\u3002", "conclusion": "LADB\u4e3a\u5b9e\u9645\u57df\u8f6c\u6362\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u771f\u5ea6\u548c\u591a\u6837\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u6216\u4e0d\u5b8c\u6574\u7684\u573a\u666f\u3002"}}
{"id": "2509.08712", "pdf": "https://arxiv.org/pdf/2509.08712", "abs": "https://arxiv.org/abs/2509.08712", "authors": ["Humera Shaikh", "Kaur Jashanpreet"], "title": "Computational Imaging for Enhanced Computer Vision", "categories": ["cs.CV"], "comment": "International Journal of Engineering Research & Technology, 2025", "summary": "This paper presents a comprehensive survey of computational imaging (CI) techniques and their transformative impact on computer vision (CV) applications. Conventional imaging methods often fail to deliver high-fidelity visual data in challenging conditions, such as low light, motion blur, or high dynamic range scenes, thereby limiting the performance of state-of-the-art CV systems. Computational imaging techniques, including light field imaging, high dynamic range (HDR) imaging, deblurring, high-speed imaging, and glare mitigation, address these limitations by enhancing image acquisition and reconstruction processes. This survey systematically explores the synergies between CI techniques and core CV tasks, including object detection, depth estimation, optical flow, face recognition, and keypoint detection. By analyzing the relationships between CI methods and their practical contributions to CV applications, this work highlights emerging opportunities, challenges, and future research directions. We emphasize the potential for task-specific, adaptive imaging pipelines that improve robustness, accuracy, and efficiency in real-world scenarios, such as autonomous navigation, surveillance, augmented reality, and robotics.", "AI": {"tldr": "\u672c\u6587\u5bf9\u8ba1\u7b97\u6210\u50cf\u6280\u672f\u53ca\u5176\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u4e2d\u7684\u53d8\u9769\u6027\u5f71\u54cd\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u5404\u79cdCI\u65b9\u6cd5\u5982\u4f55\u63d0\u5347CV\u7cfb\u7edf\u5728\u6311\u6218\u6027\u73af\u5883\u4e0b\u7684\u6027\u80fd", "motivation": "\u4f20\u7edf\u6210\u50cf\u65b9\u6cd5\u5728\u4f4e\u5149\u7167\u3001\u8fd0\u52a8\u6a21\u7cca\u3001\u9ad8\u52a8\u6001\u8303\u56f4\u7b49\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u5f80\u5f80\u65e0\u6cd5\u63d0\u4f9b\u9ad8\u8d28\u91cf\u89c6\u89c9\u6570\u636e\uff0c\u9650\u5236\u4e86\u5148\u8fdb\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u7684\u6027\u80fd\u8868\u73b0", "method": "\u7cfb\u7edf\u6027\u5730\u8c03\u7814\u4e86\u5149\u573a\u6210\u50cf\u3001\u9ad8\u52a8\u6001\u8303\u56f4\u6210\u50cf\u3001\u53bb\u6a21\u7cca\u3001\u9ad8\u901f\u6210\u50cf\u548c\u7729\u5149\u6291\u5236\u7b49\u591a\u79cd\u8ba1\u7b97\u6210\u50cf\u6280\u672f\uff0c\u5e76\u5206\u6790\u5b83\u4eec\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\u6838\u5fc3\u4efb\u52a1\u7684\u534f\u540c\u5173\u7cfb", "result": "\u63ed\u793a\u4e86\u8ba1\u7b97\u6210\u50cf\u6280\u672f\u4e0e\u76ee\u6807\u68c0\u6d4b\u3001\u6df1\u5ea6\u4f30\u8ba1\u3001\u5149\u6d41\u3001\u4eba\u8138\u8bc6\u522b\u548c\u5173\u952e\u70b9\u68c0\u6d4b\u7b49CV\u4efb\u52a1\u4e4b\u95f4\u7684\u5173\u8054\u6027\u53ca\u5176\u5b9e\u9645\u8d21\u732e", "conclusion": "\u5f3a\u8c03\u4e86\u4efb\u52a1\u7279\u5f02\u6027\u81ea\u9002\u5e94\u6210\u50cf\u7ba1\u9053\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u3001\u76d1\u63a7\u3001\u589e\u5f3a\u73b0\u5b9e\u548c\u673a\u5668\u4eba\u7b49\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u3001\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u6307\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u548c\u6311\u6218"}}
{"id": "2509.08818", "pdf": "https://arxiv.org/pdf/2509.08818", "abs": "https://arxiv.org/abs/2509.08818", "authors": ["Jenna Kang", "Maria Silva", "Patsorn Sangkloy", "Kenneth Chen", "Niall Williams", "Qi Sun"], "title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video Artifacts", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in probabilistic generative models have extended capabilities from static image synthesis to text-driven video generation. However, the inherent randomness of their generation process can lead to unpredictable artifacts, such as impossible physics and temporal inconsistency. Progress in addressing these challenges requires systematic benchmarks, yet existing datasets primarily focus on generative images due to the unique spatio-temporal complexities of videos. To bridge this gap, we introduce GeneVA, a large-scale artifact dataset with rich human annotations that focuses on spatio-temporal artifacts in videos generated from natural text prompts. We hope GeneVA can enable and assist critical applications, such as benchmarking model performance and improving generative video quality.", "AI": {"tldr": "GeneVA\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u7684\u89c6\u9891\u751f\u6210\u4f2a\u5f71\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u6587\u672c\u9a71\u52a8\u89c6\u9891\u751f\u6210\u4e2d\u7684\u65f6\u7a7a\u4f2a\u5f71\u95ee\u9898", "motivation": "\u73b0\u6709\u7684\u6982\u7387\u751f\u6210\u6a21\u578b\u5728\u6587\u672c\u9a71\u52a8\u89c6\u9891\u751f\u6210\u4e2d\u5b58\u5728\u968f\u673a\u6027\u5bfc\u81f4\u7684\u7269\u7406\u4e0d\u53ef\u80fd\u6027\u548c\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u7b49\u4f2a\u5f71\u95ee\u9898\uff0c\u4f46\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u89c6\u9891\u65f6\u7a7a\u590d\u6742\u6027\u7684\u7cfb\u7edf\u57fa\u51c6\u6570\u636e\u96c6", "method": "\u6784\u5efa\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6GeneVA\uff0c\u5305\u542b\u81ea\u7136\u6587\u672c\u63d0\u793a\u751f\u6210\u7684\u89c6\u9891\u4e2d\u7684\u4e30\u5bcc\u65f6\u7a7a\u4f2a\u5f71\u6807\u6ce8", "result": "\u6210\u529f\u521b\u5efa\u4e86\u4e13\u6ce8\u4e8e\u89c6\u9891\u751f\u6210\u65f6\u7a7a\u4f2a\u5f71\u7684\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6", "conclusion": "GeneVA\u6570\u636e\u96c6\u80fd\u591f\u652f\u6301\u548c\u8f85\u52a9\u5173\u952e\u5e94\u7528\uff0c\u5982\u57fa\u51c6\u6d4b\u8bd5\u6a21\u578b\u6027\u80fd\u548c\u63d0\u5347\u751f\u6210\u89c6\u9891\u8d28\u91cf"}}
{"id": "2509.08826", "pdf": "https://arxiv.org/pdf/2509.08826", "abs": "https://arxiv.org/abs/2509.08826", "authors": ["Jie Wu", "Yu Gao", "Zilyu Ye", "Ming Li", "Liang Li", "Hanzhong Guo", "Jie Liu", "Zeyue Xue", "Xiaoxia Hou", "Wei Liu", "Yan Zeng", "Weilin Huang"], "title": "RewardDance: Reward Scaling in Visual Generation", "categories": ["cs.CV"], "comment": "Bytedance Seed Technical Report", "summary": "Reward Models (RMs) are critical for improving generation models via Reinforcement Learning (RL), yet the RM scaling paradigm in visual generation remains largely unexplored. It primarily due to fundamental limitations in existing approaches: CLIP-based RMs suffer from architectural and input modality constraints, while prevalent Bradley-Terry losses are fundamentally misaligned with the next-token prediction mechanism of Vision-Language Models (VLMs), hindering effective scaling. More critically, the RLHF optimization process is plagued by Reward Hacking issue, where models exploit flaws in the reward signal without improving true quality. To address these challenges, we introduce RewardDance, a scalable reward modeling framework that overcomes these barriers through a novel generative reward paradigm. By reformulating the reward score as the model's probability of predicting a \"yes\" token, indicating that the generated image outperforms a reference image according to specific criteria, RewardDance intrinsically aligns reward objectives with VLM architectures. This alignment unlocks scaling across two dimensions: (1) Model Scaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context Scaling: Integration of task-specific instructions, reference examples, and chain-of-thought (CoT) reasoning. Extensive experiments demonstrate that RewardDance significantly surpasses state-of-the-art methods in text-to-image, text-to-video, and image-to-video generation. Crucially, we resolve the persistent challenge of \"reward hacking\": Our large-scale RMs exhibit and maintain high reward variance during RL fine-tuning, proving their resistance to hacking and ability to produce diverse, high-quality outputs. It greatly relieves the mode collapse problem that plagues smaller models.", "AI": {"tldr": "RewardDance\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5956\u52b1\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u5956\u52b1\u8303\u5f0f\u89e3\u51b3\u89c6\u89c9\u751f\u6210\u4e2d\u5956\u52b1\u6a21\u578b\u6269\u5c55\u7684\u6311\u6218\uff0c\u652f\u6301260\u4ebf\u53c2\u6570\u89c4\u6a21\uff0c\u6709\u6548\u9632\u6b62\u5956\u52b1\u7834\u89e3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5956\u52b1\u6a21\u578b\u5728\u89c6\u89c9\u751f\u6210\u4e2d\u5b58\u5728\u67b6\u6784\u9650\u5236\uff1aCLIP-based\u6a21\u578b\u53d7\u8f93\u5165\u6a21\u6001\u7ea6\u675f\uff0cBradley-Terry\u635f\u5931\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u673a\u5236\u4e0d\u5339\u914d\uff0c\u4e14RLHF\u4f18\u5316\u8fc7\u7a0b\u5b58\u5728\u5956\u52b1\u7834\u89e3\u95ee\u9898\u3002", "method": "\u63d0\u51faRewardDance\u6846\u67b6\uff0c\u5c06\u5956\u52b1\u5206\u6570\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6a21\u578b\u9884\u6d4b\"yes\"token\u7684\u6982\u7387\uff0c\u8868\u793a\u751f\u6210\u56fe\u50cf\u5728\u7279\u5b9a\u6807\u51c6\u4e0b\u4f18\u4e8e\u53c2\u8003\u56fe\u50cf\uff0c\u4f7f\u5956\u52b1\u76ee\u6807\u4e0eVLM\u67b6\u6784\u5185\u5728\u5bf9\u9f50\u3002", "result": "\u5728\u6587\u672c\u5230\u56fe\u50cf\u3001\u6587\u672c\u5230\u89c6\u9891\u548c\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5927\u89c4\u6a21\u5956\u52b1\u6a21\u578b\u5728RL\u5fae\u8c03\u671f\u95f4\u4fdd\u6301\u9ad8\u5956\u52b1\u65b9\u5dee\uff0c\u8bc1\u660e\u5176\u6297\u7834\u89e3\u80fd\u529b\u3002", "conclusion": "RewardDance\u89e3\u51b3\u4e86\u5956\u52b1\u6a21\u578b\u6269\u5c55\u7684\u6839\u672c\u9650\u5236\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u5956\u52b1\u8303\u5f0f\u5b9e\u73b0\u4e86\u6a21\u578b\u89c4\u6a21\u548c\u4e0a\u4e0b\u6587\u89c4\u6a21\u7684\u6269\u5c55\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\u3002"}}
{"id": "2509.08828", "pdf": "https://arxiv.org/pdf/2509.08828", "abs": "https://arxiv.org/abs/2509.08828", "authors": ["David Stotko", "Reinhard Klein"], "title": "SAFT: Shape and Appearance of Fabrics from Template via Differentiable Physical Simulations from Monocular Video", "categories": ["cs.CV"], "comment": "Project page: https://cg.cs.uni-bonn.de/publication/stotko-2025-saft   Video: https://www.youtube.com/watch?v=EvioNjBOARc GitHub:   https://github.com/vc-bonn/saft", "summary": "The reconstruction of three-dimensional dynamic scenes is a well-established yet challenging task within the domain of computer vision. In this paper, we propose a novel approach that combines the domains of 3D geometry reconstruction and appearance estimation for physically based rendering and present a system that is able to perform both tasks for fabrics, utilizing only a single monocular RGB video sequence as input. In order to obtain realistic and high-quality deformations and renderings, a physical simulation of the cloth geometry and differentiable rendering are employed. In this paper, we introduce two novel regularization terms for the 3D reconstruction task that improve the plausibility of the reconstruction by addressing the depth ambiguity problem in monocular video. In comparison with the most recent methods in the field, we have reduced the error in the 3D reconstruction by a factor of 2.64 while requiring a medium runtime of 30 min per scene. Furthermore, the optimized motion achieves sufficient quality to perform an appearance estimation of the deforming object, recovering sharp details from this single monocular RGB video.", "AI": {"tldr": "\u901a\u8fc7\u7269\u7406\u6a21\u62df\u548c\u53ef\u5fae\u6e32\u67d3\u6280\u672f\uff0c\u4ece\u5355\u76eeRGB\u89c6\u9891\u91cd\u5efa\u7eba\u7269\u76843D\u52a8\u6001\u573a\u666f\u548c\u5916\u89c2\u7279\u5f81\uff0c\u63d0\u51fa\u4e24\u4e2a\u65b0\u7684\u6b63\u5219\u5316\u9879\u89e3\u51b3\u5355\u76ee\u6df1\u5ea6\u6b67\u4e49\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u4ece\u5355\u76eeRGB\u89c6\u9891\u91cd\u5efa3D\u52a8\u6001\u7eba\u7269\u573a\u666f\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u6df1\u5ea6\u6b67\u4e49\u95ee\u9898\uff0c\u5e76\u540c\u65f6\u5b8c\u6210\u51e0\u4f55\u91cd\u5efa\u548c\u5916\u89c2\u4f30\u8ba1\u4efb\u52a1\u3002", "method": "\u7ed3\u5408\u7269\u7406\u6a21\u62df\u548c\u53ef\u5fae\u6e32\u67d3\u6280\u672f\uff0c\u63d0\u51fa\u4e24\u4e2a\u65b0\u7684\u6b63\u5219\u5316\u9879\u6765\u6539\u5584\u5355\u76ee\u89c6\u9891\u76843D\u91cd\u5efa\u8d28\u91cf\uff0c\u89e3\u51b3\u6df1\u5ea6\u6b67\u4e49\u95ee\u9898\u3002", "result": "\u4e0e\u6700\u65b0\u65b9\u6cd5\u76f8\u6bd4\uff0c3D\u91cd\u5efa\u9519\u8bef\u51cf\u5c113.64\u500d\uff0c\u6bcf\u4e2a\u573a\u666f\u8fd0\u884c\u65f6\u95f4\u4e3a30\u5206\u949f\uff0c\u80fd\u591f\u4ece\u5355\u76eeRGB\u89c6\u9891\u4e2d\u6062\u590d\u51fa\u6e05\u6670\u7684\u7ec6\u8282\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u5355\u76eeRGB\u89c6\u9891\u9ad8\u8d28\u91cf\u5730\u91cd\u5efa\u7eba\u7269\u76843D\u52a8\u6001\u573a\u666f\u548c\u5916\u89c2\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u7cbe\u5ea6\u548c\u6548\u7387\u3002"}}
{"id": "2509.08330", "pdf": "https://arxiv.org/pdf/2509.08330", "abs": "https://arxiv.org/abs/2509.08330", "authors": ["Juntai Zeng"], "title": "Physics-Guided Rectified Flow for Low-light RAW Image Enhancement", "categories": ["eess.IV", "cs.CV"], "comment": "21pages,7figures", "summary": "Enhancing RAW images captured under low light conditions is a challenging task. Recent deep learning based RAW enhancement methods have shifted from using real paired data to relying on synthetic datasets. These synthetic datasets are typically generated by physically modeling sensor noise, but existing approaches often consider only additive noise, ignore multiplicative components, and rely on global calibration that overlooks pixel level manufacturing variations. As a result, such methods struggle to accurately reproduce real sensor noise. To address these limitations, this paper derives a noise model from the physical noise generation mechanisms that occur under low illumination and proposes a novel composite model that integrates both additive and multiplicative noise. To solve the model, we introduce a physics based per pixel noise simulation and calibration scheme that estimates and synthesizes noise for each individual pixel, thereby overcoming the restrictions of traditional global calibration and capturing spatial noise variations induced by microscopic CMOS manufacturing differences. Motivated by the strong performance of rectified flow methods in image generation and processing, we further combine the physics-based noise synthesis with a rectified flow generative framework and present PGRF a physics-guided rectified flow framework for low light image enhancement. PGRF leverages the ability of rectified flows to model complex data distributions and uses physical guidance to steer the generation toward the desired clean image. To validate the effectiveness of the proposed model, we established the LLID dataset, an indoor low light benchmark captured with the Sony A7S II camera. Experimental results demonstrate that the proposed framework achieves significant improvements in low light RAW image enhancement.", "AI": {"tldr": "\u63d0\u51faPGRF\u6846\u67b6\uff0c\u7ed3\u5408\u7269\u7406\u566a\u58f0\u6a21\u578b\u548c\u6574\u6d41\u6d41\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u50cf\u7d20\u7ea7\u566a\u58f0\u6821\u51c6\u63d0\u5347\u4f4e\u5149\u7167RAW\u56fe\u50cf\u589e\u5f3a\u6548\u679c", "motivation": "\u73b0\u6709\u5408\u6210\u6570\u636e\u96c6\u65b9\u6cd5\u4e3b\u8981\u8003\u8651\u52a0\u6027\u566a\u58f0\uff0c\u5ffd\u7565\u4e58\u6027\u5206\u91cf\u548c\u50cf\u7d20\u7ea7\u5236\u9020\u5dee\u5f02\uff0c\u96be\u4ee5\u51c6\u786e\u6a21\u62df\u771f\u5b9e\u4f20\u611f\u5668\u566a\u58f0", "method": "\u4ece\u7269\u7406\u566a\u58f0\u673a\u5236\u63a8\u5bfc\u566a\u58f0\u6a21\u578b\uff0c\u63d0\u51fa\u7ed3\u5408\u52a0\u6027\u4e58\u6027\u566a\u58f0\u7684\u590d\u5408\u6a21\u578b\uff0c\u91c7\u7528\u57fa\u4e8e\u7269\u7406\u7684\u50cf\u7d20\u7ea7\u566a\u58f0\u6a21\u62df\u6821\u51c6\u65b9\u6848\uff0c\u7ed3\u5408\u6574\u6d41\u6d41\u751f\u6210\u6846\u67b6", "result": "\u5728\u81ea\u5efa\u7684LLID\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4f4e\u5149\u7167RAW\u56fe\u50cf\u589e\u5f3a\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6539\u8fdb", "conclusion": "PGRF\u6846\u67b6\u901a\u8fc7\u7269\u7406\u5f15\u5bfc\u7684\u6574\u6d41\u6d41\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5168\u5c40\u6821\u51c6\u7684\u5c40\u9650\u6027"}}
