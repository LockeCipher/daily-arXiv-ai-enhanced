{"id": "2508.02443", "pdf": "https://arxiv.org/pdf/2508.02443", "abs": "https://arxiv.org/abs/2508.02443", "authors": ["Thomas Gottwald", "Edgar Heinert", "Matthias Rottmann"], "title": "Uncertainty Estimation for Novel Views in Gaussian Splatting from Primitive-Based Representations of Error and Visibility", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "In this work, we present a novel method for uncertainty estimation (UE) in Gaussian Splatting. UE is crucial for using Gaussian Splatting in critical applications such as robotics and medicine. Previous methods typically estimate the variance of Gaussian primitives and use the rendering process to obtain pixel-wise uncertainties. Our method establishes primitive representations of error and visibility of trainings views, which carries meaningful uncertainty information. This representation is obtained by projection of training error and visibility onto the primitives. Uncertainties of novel views are obtained by rendering the primitive representations of uncertainty for those novel views, yielding uncertainty feature maps. To aggregate these uncertainty feature maps of novel views, we perform a pixel-wise regression on holdout data. In our experiments, we analyze the different components of our method, investigating various combinations of uncertainty feature maps and regression models. Furthermore, we considered the effect of separating splatting into foreground and background. Our UEs show high correlations to true errors, outperforming state-of-the-art methods, especially on foreground objects. The trained regression models show generalization capabilities to new scenes, allowing uncertainty estimation without the need for holdout data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u65af\u6cfc\u6e85\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u8bef\u5dee\u548c\u53ef\u89c1\u6027\u7684\u539f\u59cb\u8868\u793a\u751f\u6210\u4e0d\u786e\u5b9a\u6027\u7279\u5f81\u56fe\uff0c\u5e76\u901a\u8fc7\u56de\u5f52\u6a21\u578b\u805a\u5408\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9ad8\u65af\u6cfc\u6e85\u5728\u673a\u5668\u4eba\u548c\u533b\u5b66\u7b49\u5173\u952e\u5e94\u7528\u4e2d\u9700\u8981\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f30\u8ba1\u9ad8\u65af\u57fa\u5143\u7684\u65b9\u5dee\uff0c\u7f3a\u4e4f\u66f4\u5168\u9762\u7684\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3\u8bef\u5dee\u548c\u53ef\u89c1\u6027\u7684\u6295\u5f71\u5efa\u7acb\u539f\u59cb\u8868\u793a\uff0c\u751f\u6210\u4e0d\u786e\u5b9a\u6027\u7279\u5f81\u56fe\uff0c\u5e76\u901a\u8fc7\u56de\u5f52\u6a21\u578b\u805a\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4e0e\u771f\u5b9e\u8bef\u5dee\u9ad8\u5ea6\u76f8\u5173\uff0c\u5c24\u5176\u5728\u524d\u666f\u5bf9\u8c61\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u56de\u5f52\u6a21\u578b\u5177\u6709\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u65af\u6cfc\u6e85\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u9002\u7528\u4e8e\u65b0\u573a\u666f\u4e14\u65e0\u9700\u989d\u5916\u6570\u636e\u3002"}}
{"id": "2508.00896", "pdf": "https://arxiv.org/pdf/2508.00896", "abs": "https://arxiv.org/abs/2508.00896", "authors": ["Hoang Hai Nam Nguyen", "Minh Tien Tran", "Hoheok Kim", "Ho Won Lee"], "title": "Phase-fraction guided denoising diffusion model for augmenting multiphase steel microstructure segmentation via micrograph image-mask pair synthesis", "categories": ["cs.CV", "cond-mat.mtrl-sci", "eess.IV"], "comment": null, "summary": "The effectiveness of machine learning in metallographic microstructure segmentation is often constrained by the lack of human-annotated phase masks, particularly for rare or compositionally complex morphologies within the metal alloy. We introduce PF-DiffSeg, a phase-fraction controlled, one-stage denoising diffusion framework that jointly synthesizes microstructure images and their corresponding segmentation masks in a single generative trajectory to further improve segmentation accuracy. By conditioning on global phase-fraction vectors, augmented to represent real data distribution and emphasize minority classes, our model generates compositionally valid and structurally coherent microstructure image and mask samples that improve both data diversity and training efficiency. Evaluated on the MetalDAM benchmark for additively manufactured multiphase steel, our synthetic augmentation method yields notable improvements in segmentation accuracy compared to standard augmentation strategies especially in minority classes and further outperforms a two-stage mask-guided diffusion and generative adversarial network (GAN) baselines, while also reducing inference time compared to conventional approach. The method integrates generation and conditioning into a unified framework, offering a scalable solution for data augmentation in metallographic applications.", "AI": {"tldr": "PF-DiffSeg\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6846\u67b6\u7684\u5355\u9636\u6bb5\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u540c\u65f6\u5408\u6210\u663e\u5fae\u7ed3\u6784\u56fe\u50cf\u548c\u5206\u5272\u63a9\u7801\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u5272\u7cbe\u5ea6\uff0c\u5c24\u5176\u5728\u5c11\u6570\u7c7b\u522b\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u91d1\u5c5e\u5408\u91d1\u663e\u5fae\u7ed3\u6784\u5206\u5272\u4e2d\u56e0\u7f3a\u4e4f\u4eba\u5de5\u6807\u6ce8\u63a9\u7801\uff08\u7279\u522b\u662f\u7a00\u6709\u6216\u590d\u6742\u5f62\u6001\uff09\u800c\u5bfc\u81f4\u7684\u673a\u5668\u5b66\u4e60\u6548\u679c\u53d7\u9650\u95ee\u9898\u3002", "method": "\u63d0\u51faPF-DiffSeg\uff0c\u4e00\u79cd\u57fa\u4e8e\u5168\u5c40\u76f8\u5206\u6570\u63a7\u5236\u7684\u5355\u9636\u6bb5\u53bb\u566a\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u6570\u636e\u589e\u5f3a\u591a\u6837\u6027\u548c\u8bad\u7ec3\u6548\u7387\u3002", "result": "\u5728MetalDAM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u7cbe\u5ea6\uff08\u5c24\u5176\u662f\u5c11\u6570\u7c7b\u522b\uff09\uff0c\u5e76\u4f18\u4e8e\u4e24\u9636\u6bb5\u63a9\u7801\u5f15\u5bfc\u6269\u6563\u548cGAN\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "PF-DiffSeg\u5c06\u751f\u6210\u4e0e\u6761\u4ef6\u6574\u5408\u4e3a\u7edf\u4e00\u6846\u67b6\uff0c\u4e3a\u91d1\u76f8\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6570\u636e\u589e\u5f3a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.02304", "pdf": "https://arxiv.org/pdf/2508.02304", "abs": "https://arxiv.org/abs/2508.02304", "authors": ["Fangxin Liu", "Haomin Li", "Bowen Zhu", "Zongwu Wang", "Zhuoran Song", "Habing Guan", "Li Jiang"], "title": "ASDR: Exploiting Adaptive Sampling and Data Reuse for CIM-based Instant Neural Rendering", "categories": ["cs.AR", "cs.ET", "cs.GR"], "comment": "Accepted by the 2025 International Conference on Architectural   Support for Programming Languages and Operating Systems (ASPLOS 2025). The   paper will be presented at ASPLOS 2026", "summary": "Neural Radiance Fields (NeRF) offer significant promise for generating photorealistic images and videos. However, existing mainstream neural rendering models often fall short in meeting the demands for immediacy and power efficiency in practical applications. Specifically, these models frequently exhibit irregular access patterns and substantial computational overhead, leading to undesirable inference latency and high power consumption. Computing-in-memory (CIM), an emerging computational paradigm, has the potential to address these access bottlenecks and reduce the power consumption associated with model execution.   To bridge the gap between model performance and real-world scene requirements, we propose an algorithm-architecture co-design approach, abbreviated as ASDR, a CIM-based accelerator supporting efficient neural rendering. At the algorithmic level, we propose two rendering optimization schemes: (1) Dynamic sampling by online sensing of the rendering difficulty of different pixels, thus reducing access memory and computational overhead. (2) Reducing MLP overhead by decoupling and approximating the volume rendering of color and density. At the architecture level, we design an efficient ReRAM-based CIM architecture with efficient data mapping and reuse microarchitecture. Experiments demonstrate that our design can achieve up to $9.55\\times$ and $69.75\\times$ speedup over state-of-the-art NeRF accelerators and Xavier NX GPU in graphics rendering tasks with only $0.1$ PSNR loss.", "AI": {"tldr": "ASDR\u662f\u4e00\u79cd\u57fa\u4e8eCIM\u7684\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u7b97\u6cd5-\u67b6\u6784\u534f\u540c\u8bbe\u8ba1\u4f18\u5316\u795e\u7ecf\u6e32\u67d3\uff0c\u663e\u8457\u63d0\u5347\u901f\u5ea6\u548c\u80fd\u6548\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u6e32\u67d3\u6a21\u578b\u5728\u5b9e\u65f6\u6027\u548c\u80fd\u6548\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0cASDR\u65e8\u5728\u901a\u8fc7CIM\u6280\u672f\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u91c7\u6837\u548cMLP\u89e3\u8026\u4f18\u5316\u7b97\u6cd5\uff0c\u5e76\u8bbe\u8ba1ReRAM-CIM\u67b6\u6784\u652f\u6301\u9ad8\u6548\u6570\u636e\u6620\u5c04\u548c\u91cd\u7528\u3002", "result": "\u5b9e\u9a8c\u663e\u793aASDR\u5728\u56fe\u5f62\u6e32\u67d3\u4efb\u52a1\u4e2d\u6bd4\u73b0\u6709\u52a0\u901f\u5668\u548cGPU\u5feb9.55\u500d\u548c69.75\u500d\uff0c\u4ec5\u635f\u59310.1 PSNR\u3002", "conclusion": "ASDR\u901a\u8fc7\u534f\u540c\u8bbe\u8ba1\u663e\u8457\u4f18\u5316\u795e\u7ecf\u6e32\u67d3\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00898", "pdf": "https://arxiv.org/pdf/2508.00898", "abs": "https://arxiv.org/abs/2508.00898", "authors": ["Jose M. S\u00e1nchez Vel\u00e1zquez", "Mingbo Cai", "Andrew Coney", "\u00c1lvaro J. Garc\u00eda- Tejedor", "Alberto Nogales"], "title": "Benefits of Feature Extraction and Temporal Sequence Analysis for Video Frame Prediction: An Evaluation of Hybrid Deep Learning Models", "categories": ["cs.CV", "cs.AI"], "comment": "2 Figures, 12 Tables, 21 pages", "summary": "In recent years, advances in Artificial Intelligence have significantly impacted computer science, particularly in the field of computer vision, enabling solutions to complex problems such as video frame prediction. Video frame prediction has critical applications in weather forecasting or autonomous systems and can provide technical improvements, such as video compression and streaming. Among Artificial Intelligence methods, Deep Learning has emerged as highly effective for solving vision-related tasks, although current frame prediction models still have room for enhancement. This paper evaluates several hybrid deep learning approaches that combine the feature extraction capabilities of autoencoders with temporal sequence modelling using Recurrent Neural Networks (RNNs), 3D Convolutional Neural Networks (3D CNNs), and related architectures. The proposed solutions were rigorously evaluated on three datasets that differ in terms of synthetic versus real-world scenarios and grayscale versus color imagery. Results demonstrate that the approaches perform well, with SSIM metrics increasing from 0.69 to 0.82, indicating that hybrid models utilizing 3DCNNs and ConvLSTMs are the most effective, and greyscale videos with real data are the easiest to predict.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u7ed3\u5408\u81ea\u7f16\u7801\u5668\u4e0eRNN\u30013D CNN\u7b49\u67b6\u6784\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u89c6\u9891\u5e27\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u7ed3\u679c\u8868\u660e3D CNN\u548cConvLSTM\u7684\u6df7\u5408\u6a21\u578b\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u89c6\u9891\u5e27\u9884\u6d4b\u5728\u5929\u6c14\u9884\u62a5\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u7ed3\u5408\u81ea\u7f16\u7801\u5668\u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u4e0eRNN\u30013D CNN\u7b49\u67b6\u6784\u8fdb\u884c\u65f6\u5e8f\u5efa\u6a21\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cSSIM\u6307\u6807\u4ece0.69\u63d0\u5347\u81f30.82\uff0c3D CNN\u548cConvLSTM\u7684\u6df7\u5408\u6a21\u578b\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "\u6df7\u5408\u6a21\u578b\u5728\u89c6\u9891\u5e27\u9884\u6d4b\u4e2d\u6548\u679c\u663e\u8457\uff0c\u5c24\u5176\u662f3D CNN\u548cConvLSTM\u7684\u7ec4\u5408\uff0c\u4e14\u7070\u5ea6\u89c6\u9891\u7684\u9884\u6d4b\u66f4\u5bb9\u6613\u3002"}}
{"id": "2508.00941", "pdf": "https://arxiv.org/pdf/2508.00941", "abs": "https://arxiv.org/abs/2508.00941", "authors": ["Hassan Ugail", "Hamad Mansour Alawar", "AbdulNasser Abbas Zehi", "Ahmed Mohammad Alkendi", "Ismail Lujain Jaleel"], "title": "Latent Diffusion Based Face Enhancement under Degraded Conditions for Forensic Face Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Face recognition systems experience severe performance degradation when processing low-quality forensic evidence imagery. This paper presents an evaluation of latent diffusion-based enhancement for improving face recognition under forensically relevant degradations. Using a dataset of 3,000 individuals from LFW with 24,000 recognition attempts, we implement the Flux.1 Kontext Dev pipeline with Facezoom LoRA adaptation to test against seven degradation categories, including compression artefacts, blur effects, and noise contamination. Our approach demonstrates substantial improvements, increasing overall recognition accuracy from 29.1% to 84.5% (55.4 percentage point improvement, 95% CI: [54.1, 56.7]). Statistical analysis reveals significant performance gains across all degradation types, with effect sizes exceeding conventional thresholds for practical significance. These findings establish the potential of sophisticated diffusion based enhancement in forensic face recognition applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u589e\u5f3a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u8d28\u91cf\u6cd5\u533b\u56fe\u50cf\u7684\u4eba\u8138\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u6cd5\u533b\u56fe\u50cf\u8d28\u91cf\u4f4e\u5bfc\u81f4\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Flux.1 Kontext Dev\u7ba1\u9053\u548cFacezoom LoRA\u9002\u5e94\uff0c\u6d4b\u8bd5\u4e867\u79cd\u9000\u5316\u7c7b\u578b\uff0c\u5305\u62ec\u538b\u7f29\u4f2a\u5f71\u3001\u6a21\u7cca\u548c\u566a\u58f0\u3002", "result": "\u8bc6\u522b\u51c6\u786e\u7387\u4ece29.1%\u63d0\u5347\u81f384.5%\uff0c\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u6269\u6563\u589e\u5f3a\u6280\u672f\u5728\u6cd5\u533b\u4eba\u8138\u8bc6\u522b\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.01008", "pdf": "https://arxiv.org/pdf/2508.01008", "abs": "https://arxiv.org/abs/2508.01008", "authors": ["Cihang Peng", "Qiming Hou", "Zhong Ren", "Kun Zhou"], "title": "ROVI: A VLM-LLM Re-Captioned Dataset for Open-Vocabulary Instance-Grounded Text-to-Image Generation", "categories": ["cs.CV"], "comment": "Accepted at ICCV 2025", "summary": "We present ROVI, a high-quality synthetic dataset for instance-grounded text-to-image generation, created by labeling 1M curated web images. Our key innovation is a strategy called re-captioning, focusing on the pre-detection stage, where a VLM (Vision-Language Model) generates comprehensive visual descriptions that are then processed by an LLM (Large Language Model) to extract a flat list of potential categories for OVDs (Open-Vocabulary Detectors) to detect. This approach yields a global prompt inherently linked to instance annotations while capturing secondary visual elements humans typically overlook. Evaluations show that ROVI exceeds existing detection datasets in image quality and resolution while containing two orders of magnitude more categories with an open-vocabulary nature. For demonstrative purposes, a text-to-image model GLIGEN trained on ROVI significantly outperforms state-of-the-art alternatives in instance grounding accuracy, prompt fidelity, and aesthetic quality. Our dataset and reproducible pipeline are available at https://github.com/CihangPeng/ROVI.", "AI": {"tldr": "ROVI\u662f\u4e00\u4e2a\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5b9e\u4f8b\u63a5\u5730\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff0c\u901a\u8fc7\u6807\u6ce8100\u4e07\u5f20\u7cbe\u9009\u7f51\u7edc\u56fe\u50cf\u521b\u5efa\u3002\u5176\u521b\u65b0\u5728\u4e8e\u91cd\u65b0\u6807\u6ce8\u7b56\u7565\uff0c\u7ed3\u5408VLM\u548cLLM\u751f\u6210\u89c6\u89c9\u63cf\u8ff0\u548c\u7c7b\u522b\u5217\u8868\uff0c\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u3002ROVI\u5728\u8d28\u91cf\u548c\u7c7b\u522b\u6570\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u540e\u7684GLIGEN\u6a21\u578b\u8868\u73b0\u5353\u8d8a\u3002", "motivation": "\u73b0\u6709\u68c0\u6d4b\u6570\u636e\u96c6\u5728\u56fe\u50cf\u8d28\u91cf\u3001\u5206\u8fa8\u7387\u548c\u7c7b\u522b\u6570\u91cf\u4e0a\u6709\u9650\uff0cROVI\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u5b9e\u4f8b\u63a5\u5730\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u652f\u6301\u3002", "method": "\u91c7\u7528\u91cd\u65b0\u6807\u6ce8\u7b56\u7565\uff0c\u7ed3\u5408VLM\u751f\u6210\u89c6\u89c9\u63cf\u8ff0\u548cLLM\u63d0\u53d6\u7c7b\u522b\u5217\u8868\uff0c\u7528\u4e8eOVD\u68c0\u6d4b\uff0c\u751f\u6210\u5168\u5c40\u63d0\u793a\u5e76\u6355\u83b7\u6b21\u8981\u89c6\u89c9\u5143\u7d20\u3002", "result": "ROVI\u5728\u56fe\u50cf\u8d28\u91cf\u3001\u5206\u8fa8\u7387\u548c\u7c7b\u522b\u6570\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u540e\u7684GLIGEN\u6a21\u578b\u5728\u5b9e\u4f8b\u63a5\u5730\u51c6\u786e\u6027\u3001\u63d0\u793a\u4fdd\u771f\u5ea6\u548c\u7f8e\u5b66\u8d28\u91cf\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "ROVI\u4e3a\u5b9e\u4f8b\u63a5\u5730\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u53ef\u590d\u73b0\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2508.01058", "pdf": "https://arxiv.org/pdf/2508.01058", "abs": "https://arxiv.org/abs/2508.01058", "authors": ["Sara Yavari", "Rahul Nitin Pandya", "Jacob Furst"], "title": "ReCoSeg++:Extended Residual-Guided Cross-Modal Diffusion for Brain Tumor Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate segmentation of brain tumors in MRI scans is critical for clinical diagnosis and treatment planning. We propose a semi-supervised, two-stage framework that extends the ReCoSeg approach to the larger and more heterogeneous BraTS 2021 dataset, while eliminating the need for ground-truth masks for the segmentation objective. In the first stage, a residual-guided denoising diffusion probabilistic model (DDPM) performs cross-modal synthesis by reconstructing the T1ce modality from FLAIR, T1, and T2 scans. The residual maps, capturing differences between predicted and actual T1ce images, serve as spatial priors to enhance downstream segmentation. In the second stage, a lightweight U-Net takes as input the concatenation of residual maps, computed as the difference between real T1ce and synthesized T1ce, with T1, T2, and FLAIR modalities to improve whole tumor segmentation. To address the increased scale and variability of BraTS 2021, we apply slice-level filtering to exclude non-informative samples and optimize thresholding strategies to balance precision and recall. Our method achieves a Dice score of $93.02\\%$ and an IoU of $86.7\\%$ for whole tumor segmentation on the BraTS 2021 dataset, outperforming the ReCoSeg baseline on BraTS 2020 (Dice: $91.7\\%$, IoU: $85.3\\%$), and demonstrating improved accuracy and scalability for real-world, multi-center MRI datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8eMRI\u4e2d\u8111\u80bf\u7624\u7684\u7cbe\u786e\u5206\u5272\uff0c\u65e0\u9700\u771f\u5b9e\u63a9\u7801\uff0c\u901a\u8fc7\u6b8b\u5dee\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\u548c\u8f7b\u91cf\u7ea7U-Net\u5b9e\u73b0\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u8111\u80bf\u7624\u7684\u7cbe\u786e\u5206\u5272\u5bf9\u4e34\u5e8a\u8bca\u65ad\u548c\u6cbb\u7597\u8ba1\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u66f4\u5927\u3001\u66f4\u5f02\u8d28\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u6b8b\u5dee\u5f15\u5bfc\u7684DDPM\u8fdb\u884c\u8de8\u6a21\u6001\u5408\u6210\uff0c\u7b2c\u4e8c\u9636\u6bb5\u7528\u8f7b\u91cf\u7ea7U-Net\u7ed3\u5408\u6b8b\u5dee\u56fe\u548c\u591a\u6a21\u6001\u8f93\u5165\u8fdb\u884c\u5206\u5272\u3002", "result": "\u5728BraTS 2021\u6570\u636e\u96c6\u4e0a\uff0cDice\u5f97\u5206\u4e3a93.02%\uff0cIoU\u4e3a86.7%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u591a\u4e2d\u5fc3MRI\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.01098", "pdf": "https://arxiv.org/pdf/2508.01098", "abs": "https://arxiv.org/abs/2508.01098", "authors": ["Yuekun Dai", "Haitian Li", "Shangchen Zhou", "Chen Change Loy"], "title": "Trans-Adapter: A Plug-and-Play Framework for Transparent Image Inpainting", "categories": ["cs.CV"], "comment": "accepted to ICCV 2025", "summary": "RGBA images, with the additional alpha channel, are crucial for any application that needs blending, masking, or transparency effects, making them more versatile than standard RGB images. Nevertheless, existing image inpainting methods are designed exclusively for RGB images. Conventional approaches to transparent image inpainting typically involve placing a background underneath RGBA images and employing a two-stage process: image inpainting followed by image matting. This pipeline, however, struggles to preserve transparency consistency in edited regions, and matting can introduce jagged edges along transparency boundaries. To address these challenges, we propose Trans-Adapter, a plug-and-play adapter that enables diffusion-based inpainting models to process transparent images directly. Trans-Adapter also supports controllable editing via ControlNet and can be seamlessly integrated into various community models. To evaluate our method, we introduce LayerBench, along with a novel non-reference alpha edge quality evaluation metric for assessing transparency edge quality. We conduct extensive experiments on LayerBench to demonstrate the effectiveness of our approach.", "AI": {"tldr": "Trans-Adapter\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u9002\u914d\u5668\uff0c\u4f7f\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u4fee\u590d\u6a21\u578b\u80fd\u591f\u76f4\u63a5\u5904\u7406\u900f\u660e\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u900f\u660e\u4e00\u81f4\u6027\u548c\u8fb9\u7f18\u952f\u9f7f\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u4ec5\u9002\u7528\u4e8eRGB\u56fe\u50cf\uff0c\u4f20\u7edf\u900f\u660e\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u5b58\u5728\u900f\u660e\u4e00\u81f4\u6027\u5dee\u548c\u8fb9\u7f18\u952f\u9f7f\u95ee\u9898\u3002", "method": "\u63d0\u51faTrans-Adapter\u9002\u914d\u5668\uff0c\u652f\u6301ControlNet\u53ef\u63a7\u7f16\u8f91\uff0c\u5e76\u5f15\u5165LayerBench\u548c\u65b0\u7684\u900f\u660e\u5ea6\u8fb9\u7f18\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eTrans-Adapter\u5728\u900f\u660e\u56fe\u50cf\u4fee\u590d\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Trans-Adapter\u4e3a\u900f\u660e\u56fe\u50cf\u4fee\u590d\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01126", "pdf": "https://arxiv.org/pdf/2508.01126", "abs": "https://arxiv.org/abs/2508.01126", "authors": ["Chaitanya Patel", "Hiroki Nakamura", "Yuta Kyuragi", "Kazuki Kozuka", "Juan Carlos Niebles", "Ehsan Adeli"], "title": "UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction, Forecasting, and Generation", "categories": ["cs.CV"], "comment": "ICCV 2025. Project Page:   https://chaitanya100100.github.io/UniEgoMotion/", "summary": "Egocentric human motion generation and forecasting with scene-context is crucial for enhancing AR/VR experiences, improving human-robot interaction, advancing assistive technologies, and enabling adaptive healthcare solutions by accurately predicting and simulating movement from a first-person perspective. However, existing methods primarily focus on third-person motion synthesis with structured 3D scene contexts, limiting their effectiveness in real-world egocentric settings where limited field of view, frequent occlusions, and dynamic cameras hinder scene perception. To bridge this gap, we introduce Egocentric Motion Generation and Egocentric Motion Forecasting, two novel tasks that utilize first-person images for scene-aware motion synthesis without relying on explicit 3D scene. We propose UniEgoMotion, a unified conditional motion diffusion model with a novel head-centric motion representation tailored for egocentric devices. UniEgoMotion's simple yet effective design supports egocentric motion reconstruction, forecasting, and generation from first-person visual inputs in a unified framework. Unlike previous works that overlook scene semantics, our model effectively extracts image-based scene context to infer plausible 3D motion. To facilitate training, we introduce EE4D-Motion, a large-scale dataset derived from EgoExo4D, augmented with pseudo-ground-truth 3D motion annotations. UniEgoMotion achieves state-of-the-art performance in egocentric motion reconstruction and is the first to generate motion from a single egocentric image. Extensive evaluations demonstrate the effectiveness of our unified framework, setting a new benchmark for egocentric motion modeling and unlocking new possibilities for egocentric applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Egocentric Motion Generation\u548cEgocentric Motion Forecasting\u4e24\u4e2a\u65b0\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86UniEgoMotion\u6a21\u578b\uff0c\u5229\u7528\u7b2c\u4e00\u4eba\u79f0\u56fe\u50cf\u8fdb\u884c\u573a\u666f\u611f\u77e5\u8fd0\u52a8\u5408\u6210\uff0c\u65e0\u9700\u4f9d\u8d56\u663e\u5f0f3D\u573a\u666f\u3002", "motivation": "\u63d0\u5347AR/VR\u4f53\u9a8c\u3001\u6539\u5584\u4eba\u673a\u4ea4\u4e92\u3001\u63a8\u52a8\u8f85\u52a9\u6280\u672f\u548c\u81ea\u9002\u5e94\u533b\u7597\u89e3\u51b3\u65b9\u6848\uff0c\u9700\u8981\u4ece\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u51c6\u786e\u9884\u6d4b\u548c\u6a21\u62df\u8fd0\u52a8\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u5173\u6ce8\u7b2c\u4e09\u4eba\u79f0\u8fd0\u52a8\u5408\u6210\uff0c\u96be\u4ee5\u9002\u5e94\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u7684\u9650\u5236\u3002", "method": "\u63d0\u51faUniEgoMotion\uff0c\u4e00\u79cd\u7edf\u4e00\u7684\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u7684\u6a21\u578b\uff0c\u91c7\u7528\u5934\u90e8\u4e2d\u5fc3\u8fd0\u52a8\u8868\u793a\uff0c\u652f\u6301\u4ece\u7b2c\u4e00\u4eba\u79f0\u89c6\u89c9\u8f93\u5165\u4e2d\u91cd\u5efa\u3001\u9884\u6d4b\u548c\u751f\u6210\u8fd0\u52a8\u3002", "result": "UniEgoMotion\u5728\u8fd0\u52a8\u91cd\u5efa\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9996\u6b21\u5b9e\u73b0\u4ece\u5355\u5f20\u7b2c\u4e00\u4eba\u79f0\u56fe\u50cf\u751f\u6210\u8fd0\u52a8\uff0c\u5e76\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002", "conclusion": "UniEgoMotion\u4e3a\u7b2c\u4e00\u4eba\u79f0\u8fd0\u52a8\u5efa\u6a21\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u62d3\u5c55\u4e86\u76f8\u5173\u5e94\u7528\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.01139", "pdf": "https://arxiv.org/pdf/2508.01139", "abs": "https://arxiv.org/abs/2508.01139", "authors": ["Huyu Wu", "Duo Su", "Junjie Hou", "Guang Li"], "title": "Dataset Condensation with Color Compensation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Dataset condensation always faces a constitutive trade-off: balancing performance and fidelity under extreme compression. Existing methods struggle with two bottlenecks: image-level selection methods (Coreset Selection, Dataset Quantization) suffer from inefficiency condensation, while pixel-level optimization (Dataset Distillation) introduces semantic distortion due to over-parameterization. With empirical observations, we find that a critical problem in dataset condensation is the oversight of color's dual role as an information carrier and a basic semantic representation unit. We argue that improving the colorfulness of condensed images is beneficial for representation learning. Motivated by this, we propose DC3: a Dataset Condensation framework with Color Compensation. After a calibrated selection strategy, DC3 utilizes the latent diffusion model to enhance the color diversity of an image rather than creating a brand-new one. Extensive experiments demonstrate the superior performance and generalization of DC3 that outperforms SOTA methods across multiple benchmarks. To the best of our knowledge, besides focusing on downstream tasks, DC3 is the first research to fine-tune pre-trained diffusion models with condensed datasets. The FID results prove that training networks with our high-quality datasets is feasible without model collapse or other degradation issues. Code and generated data will be released soon.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDC3\u6846\u67b6\uff0c\u901a\u8fc7\u989c\u8272\u8865\u507f\u89e3\u51b3\u6570\u636e\u96c6\u538b\u7f29\u4e2d\u7684\u6027\u80fd\u4e0e\u4fdd\u771f\u5ea6\u95ee\u9898\uff0c\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u589e\u5f3a\u989c\u8272\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u96c6\u538b\u7f29\u4e2d\u5b58\u5728\u6548\u7387\u4f4e\u6216\u8bed\u4e49\u5931\u771f\u7684\u95ee\u9898\uff0c\u5ffd\u89c6\u4e86\u989c\u8272\u4f5c\u4e3a\u4fe1\u606f\u8f7d\u4f53\u548c\u8bed\u4e49\u5355\u5143\u7684\u53cc\u91cd\u4f5c\u7528\u3002", "method": "\u63d0\u51faDC3\u6846\u67b6\uff0c\u7ed3\u5408\u6821\u51c6\u9009\u62e9\u7b56\u7565\u548c\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u589e\u5f3a\u56fe\u50cf\u989c\u8272\u591a\u6837\u6027\u800c\u975e\u751f\u6210\u5168\u65b0\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDC3\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u80fd\u907f\u514d\u6a21\u578b\u5d29\u6e83\u6216\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "DC3\u662f\u9996\u4e2a\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u5fae\u8c03\u538b\u7f29\u6570\u636e\u96c6\u7684\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2508.01150", "pdf": "https://arxiv.org/pdf/2508.01150", "abs": "https://arxiv.org/abs/2508.01150", "authors": ["Dianyi Yang", "Xihan Wang", "Yu Gao", "Shiyang Liu", "Bohan Ren", "Yufeng Yue", "Yi Yang"], "title": "OpenGS-Fusion: Open-Vocabulary Dense Mapping with Hybrid 3D Gaussian Splatting for Refined Object-Level Understanding", "categories": ["cs.CV"], "comment": "IROS2025", "summary": "Recent advancements in 3D scene understanding have made significant strides in enabling interaction with scenes using open-vocabulary queries, particularly for VR/AR and robotic applications. Nevertheless, existing methods are hindered by rigid offline pipelines and the inability to provide precise 3D object-level understanding given open-ended queries. In this paper, we present OpenGS-Fusion, an innovative open-vocabulary dense mapping framework that improves semantic modeling and refines object-level understanding. OpenGS-Fusion combines 3D Gaussian representation with a Truncated Signed Distance Field to facilitate lossless fusion of semantic features on-the-fly. Furthermore, we introduce a novel multimodal language-guided approach named MLLM-Assisted Adaptive Thresholding, which refines the segmentation of 3D objects by adaptively adjusting similarity thresholds, achieving an improvement 17\\% in 3D mIoU compared to the fixed threshold strategy. Extensive experiments demonstrate that our method outperforms existing methods in 3D object understanding and scene reconstruction quality, as well as showcasing its effectiveness in language-guided scene interaction. The code is available at https://young-bit.github.io/opengs-fusion.github.io/ .", "AI": {"tldr": "OpenGS-Fusion\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u5f00\u653e\u8bcd\u6c47\u5bc6\u96c6\u6620\u5c04\u6846\u67b6\uff0c\u7ed3\u54083D\u9ad8\u65af\u8868\u793a\u548c\u622a\u65ad\u7b26\u53f7\u8ddd\u79bb\u573a\uff0c\u63d0\u5347\u8bed\u4e49\u5efa\u6a21\u548c\u5bf9\u8c61\u7ea7\u7406\u89e3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5f00\u653e\u8bcd\u6c47\u67e5\u8be2\u4e0b\u5b58\u5728\u79bb\u7ebf\u6d41\u7a0b\u50f5\u5316\u548c\u5bf9\u8c61\u7ea7\u7406\u89e3\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u54083D\u9ad8\u65af\u8868\u793a\u4e0e\u622a\u65ad\u7b26\u53f7\u8ddd\u79bb\u573a\uff0c\u63d0\u51fa\u591a\u6a21\u6001\u8bed\u8a00\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u9608\u503c\u65b9\u6cd5\u3002", "result": "\u57283D mIoU\u4e0a\u6bd4\u56fa\u5b9a\u9608\u503c\u7b56\u7565\u63d0\u534717%\uff0c\u5728\u5bf9\u8c61\u7406\u89e3\u548c\u573a\u666f\u91cd\u5efa\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "OpenGS-Fusion\u5728\u8bed\u8a00\u5f15\u5bfc\u573a\u666f\u4ea4\u4e92\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.01152", "pdf": "https://arxiv.org/pdf/2508.01152", "abs": "https://arxiv.org/abs/2508.01152", "authors": ["Xinyu Yan", "Meijun Sun", "Ge-Peng Ji", "Fahad Shahbaz Khan", "Salman Khan", "Deng-Ping Fan"], "title": "LawDIS: Language-Window-based Controllable Dichotomous Image Segmentation", "categories": ["cs.CV"], "comment": "17 pages, 10 figures, ICCV 2025", "summary": "We present LawDIS, a language-window-based controllable dichotomous image segmentation (DIS) framework that produces high-quality object masks. Our framework recasts DIS as an image-conditioned mask generation task within a latent diffusion model, enabling seamless integration of user controls. LawDIS is enhanced with macro-to-micro control modes. Specifically, in macro mode, we introduce a language-controlled segmentation strategy (LS) to generate an initial mask based on user-provided language prompts. In micro mode, a window-controlled refinement strategy (WR) allows flexible refinement of user-defined regions (i.e., size-adjustable windows) within the initial mask. Coordinated by a mode switcher, these modes can operate independently or jointly, making the framework well-suited for high-accuracy, personalised applications. Extensive experiments on the DIS5K benchmark reveal that our LawDIS significantly outperforms 11 cutting-edge methods across all metrics. Notably, compared to the second-best model MVANet, we achieve $F_\\beta^\\omega$ gains of 4.6\\% with both the LS and WR strategies and 3.6\\% gains with only the LS strategy on DIS-TE. Codes will be made available at https://github.com/XinyuYanTJU/LawDIS.", "AI": {"tldr": "LawDIS\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bed\u8a00\u7a97\u53e3\u7684\u53ef\u63a7\u4e8c\u5206\u56fe\u50cf\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u8a00\u63d0\u793a\u548c\u7a97\u53e3\u7ec6\u5316\u7b56\u7565\u751f\u6210\u9ad8\u8d28\u91cf\u5bf9\u8c61\u63a9\u7801\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u4e8c\u5206\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u5728\u7528\u6237\u63a7\u5236\u548c\u4e2a\u6027\u5316\u5e94\u7528\u4e2d\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u8bed\u8a00\u63a7\u5236\u7684\u521d\u59cb\u63a9\u7801\u751f\u6210\uff08LS\uff09\u548c\u7a97\u53e3\u63a7\u5236\u7684\u7ec6\u5316\uff08WR\uff09\u7b56\u7565\uff0c\u7ed3\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\u3002", "result": "\u5728DIS5K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e11\u79cd\u5148\u8fdb\u65b9\u6cd5\uff0cF\u03b2\u03c9\u63d0\u53474.6%\u3002", "conclusion": "LawDIS\u6846\u67b6\u5728\u7528\u6237\u63a7\u5236\u548c\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u4e2a\u6027\u5316\u5e94\u7528\u3002"}}
{"id": "2508.01171", "pdf": "https://arxiv.org/pdf/2508.01171", "abs": "https://arxiv.org/abs/2508.01171", "authors": ["Ranran Huang", "Krystian Mikolajczyk"], "title": "No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views", "categories": ["cs.CV"], "comment": "Project Page: https://ranrhuang.github.io/spfsplat/", "summary": "We introduce SPFSplat, an efficient framework for 3D Gaussian splatting from sparse multi-view images, requiring no ground-truth poses during training or inference. It employs a shared feature extraction backbone, enabling simultaneous prediction of 3D Gaussian primitives and camera poses in a canonical space from unposed inputs within a single feed-forward step. Alongside the rendering loss based on estimated novel-view poses, a reprojection loss is integrated to enforce the learning of pixel-aligned Gaussian primitives for enhanced geometric constraints. This pose-free training paradigm and efficient one-step feed-forward design make SPFSplat well-suited for practical applications. Remarkably, despite the absence of pose supervision, SPFSplat achieves state-of-the-art performance in novel view synthesis even under significant viewpoint changes and limited image overlap. It also surpasses recent methods trained with geometry priors in relative pose estimation. Code and trained models are available on our project page: https://ranrhuang.github.io/spfsplat/.", "AI": {"tldr": "SPFSplat\u662f\u4e00\u4e2a\u65e0\u9700\u771f\u5b9e\u59ff\u6001\u76843D\u9ad8\u65af\u6e85\u5c04\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u7279\u5f81\u63d0\u53d6\u548c\u5355\u6b65\u524d\u9988\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u65b0\u89c6\u89d2\u5408\u6210\u548c\u59ff\u6001\u4f30\u8ba1\u3002", "motivation": "\u89e3\u51b3\u591a\u89c6\u89d2\u7a00\u758f\u56fe\u50cf\u4e0b3D\u9ad8\u65af\u6e85\u5c04\u7684\u6548\u7387\u548c\u59ff\u6001\u4f9d\u8d56\u95ee\u9898\uff0c\u63d0\u51fa\u65e0\u9700\u771f\u5b9e\u59ff\u6001\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u5171\u4eab\u7279\u5f81\u63d0\u53d6\u4e3b\u5e72\uff0c\u5355\u6b65\u524d\u9988\u9884\u6d4b3D\u9ad8\u65af\u57fa\u5143\u548c\u76f8\u673a\u59ff\u6001\uff0c\u7ed3\u5408\u6e32\u67d3\u635f\u5931\u548c\u91cd\u6295\u5f71\u635f\u5931\u4f18\u5316\u51e0\u4f55\u7ea6\u675f\u3002", "result": "\u5728\u65e0\u59ff\u6001\u76d1\u7763\u4e0b\uff0cSPFSplat\u5728\u65b0\u89c6\u89d2\u5408\u6210\u548c\u76f8\u5bf9\u59ff\u6001\u4f30\u8ba1\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "SPFSplat\u7684\u514d\u59ff\u6001\u8bad\u7ec3\u548c\u9ad8\u6548\u8bbe\u8ba1\u4f7f\u5176\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.01215", "pdf": "https://arxiv.org/pdf/2508.01215", "abs": "https://arxiv.org/abs/2508.01215", "authors": ["Yuanlin Yang", "Quanjian Song", "Zhexian Gao", "Ge Wang", "Shanshan Li", "Xiaoyan Zhang"], "title": "StyDeco: Unsupervised Style Transfer with Distilling Priors and Semantic Decoupling", "categories": ["cs.CV"], "comment": "9 pages in total", "summary": "Diffusion models have emerged as the dominant paradigm for style transfer, but their text-driven mechanism is hindered by a core limitation: it treats textual descriptions as uniform, monolithic guidance. This limitation overlooks the semantic gap between the non-spatial nature of textual descriptions and the spatially-aware attributes of visual style, often leading to the loss of semantic structure and fine-grained details during stylization. In this paper, we propose StyDeco, an unsupervised framework that resolves this limitation by learning text representations specifically tailored for the style transfer task. Our framework first employs Prior-Guided Data Distillation (PGD), a strategy designed to distill stylistic knowledge without human supervision. It leverages a powerful frozen generative model to automatically synthesize pseudo-paired data. Subsequently, we introduce Contrastive Semantic Decoupling (CSD), a task-specific objective that adapts a text encoder using domain-specific weights. CSD performs a two-class clustering in the semantic space, encouraging source and target representations to form distinct clusters. Extensive experiments on three classic benchmarks demonstrate that our framework outperforms several existing approaches in both stylistic fidelity and structural preservation, highlighting its effectiveness in style transfer with semantic preservation. In addition, our framework supports a unique de-stylization process, further demonstrating its extensibility. Our code is vailable at https://github.com/QuanjianSong/StyDeco.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStyDeco\u7684\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u95e8\u4e3a\u98ce\u683c\u8fc1\u79fb\u4efb\u52a1\u5b66\u4e60\u6587\u672c\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u4e2d\u6587\u672c\u63cf\u8ff0\u4f5c\u4e3a\u7edf\u4e00\u6307\u5bfc\u7684\u6838\u5fc3\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\u5728\u98ce\u683c\u8fc1\u79fb\u4e2d\u5c06\u6587\u672c\u63cf\u8ff0\u89c6\u4e3a\u7edf\u4e00\u7684\u6307\u5bfc\uff0c\u5ffd\u7565\u4e86\u6587\u672c\u63cf\u8ff0\u7684\u975e\u7a7a\u95f4\u6027\u4e0e\u89c6\u89c9\u98ce\u683c\u7684\u7a7a\u95f4\u5c5e\u6027\u4e4b\u95f4\u7684\u8bed\u4e49\u5dee\u8ddd\uff0c\u5bfc\u81f4\u8bed\u4e49\u7ed3\u6784\u548c\u7ec6\u8282\u4e22\u5931\u3002", "method": "\u6846\u67b6\u91c7\u7528Prior-Guided Data Distillation (PGD)\u7b56\u7565\u65e0\u76d1\u7763\u5730\u63d0\u53d6\u98ce\u683c\u77e5\u8bc6\uff0c\u5e76\u5f15\u5165Contrastive Semantic Decoupling (CSD)\u76ee\u6807\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u6743\u91cd\u8c03\u6574\u6587\u672c\u7f16\u7801\u5668\u3002", "result": "\u5728\u4e09\u4e2a\u7ecf\u5178\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cStyDeco\u5728\u98ce\u683c\u4fdd\u771f\u5ea6\u548c\u7ed3\u6784\u4fdd\u7559\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u652f\u6301\u72ec\u7279\u7684\u53bb\u98ce\u683c\u5316\u8fc7\u7a0b\u3002", "conclusion": "StyDeco\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u9a71\u52a8\u98ce\u683c\u8fc1\u79fb\u4e2d\u7684\u8bed\u4e49\u4fdd\u7559\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u6269\u5c55\u6027\u3002"}}
{"id": "2508.01239", "pdf": "https://arxiv.org/pdf/2508.01239", "abs": "https://arxiv.org/abs/2508.01239", "authors": ["Han Ling", "Xian Xu", "Yinghui Sun", "Quansen Sun"], "title": "OCSplats: Observation Completeness Quantification and Label Noise Separation in 3DGS", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has become one of the most promising 3D reconstruction technologies. However, label noise in real-world scenarios-such as moving objects, non-Lambertian surfaces, and shadows-often leads to reconstruction errors. Existing 3DGS-Bsed anti-noise reconstruction methods either fail to separate noise effectively or require scene-specific fine-tuning of hyperparameters, making them difficult to apply in practice. This paper re-examines the problem of anti-noise reconstruction from the perspective of epistemic uncertainty, proposing a novel framework, OCSplats. By combining key technologies such as hybrid noise assessment and observation-based cognitive correction, the accuracy of noise classification in areas with cognitive differences has been significantly improved. Moreover, to address the issue of varying noise proportions in different scenarios, we have designed a label noise classification pipeline based on dynamic anchor points. This pipeline enables OCSplats to be applied simultaneously to scenarios with vastly different noise proportions without adjusting parameters. Extensive experiments demonstrate that OCSplats always achieve leading reconstruction performance and precise label noise classification in scenes of different complexity levels.", "AI": {"tldr": "3DGS\u57283D\u91cd\u5efa\u4e2d\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6807\u7b7e\u566a\u58f0\uff08\u5982\u79fb\u52a8\u7269\u4f53\u3001\u975e\u6717\u4f2f\u8868\u9762\u548c\u9634\u5f71\uff09\u4f1a\u5bfc\u81f4\u91cd\u5efa\u9519\u8bef\u3002\u73b0\u6709\u6297\u566a\u58f0\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\u6216\u9700\u573a\u666f\u8c03\u53c2\u3002\u672c\u6587\u63d0\u51faOCSplats\u6846\u67b6\uff0c\u7ed3\u5408\u6df7\u5408\u566a\u58f0\u8bc4\u4f30\u548c\u57fa\u4e8e\u89c2\u5bdf\u7684\u8ba4\u77e5\u6821\u6b63\uff0c\u663e\u8457\u63d0\u5347\u566a\u58f0\u5206\u7c7b\u7cbe\u5ea6\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u951a\u70b9\u5b9e\u73b0\u591a\u573a\u666f\u5e94\u7528\u3002\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u9886\u5148\u3002", "motivation": "\u89e3\u51b33DGS\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u56e0\u6807\u7b7e\u566a\u58f0\u5bfc\u81f4\u7684\u91cd\u5efa\u9519\u8bef\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u6548\u679c\u6709\u9650\u6216\u9700\u8c03\u53c2\u3002", "method": "\u63d0\u51faOCSplats\u6846\u67b6\uff0c\u7ed3\u5408\u6df7\u5408\u566a\u58f0\u8bc4\u4f30\u3001\u89c2\u5bdf\u8ba4\u77e5\u6821\u6b63\u548c\u52a8\u6001\u951a\u70b9\u5206\u7c7b\u6280\u672f\u3002", "result": "\u663e\u8457\u63d0\u5347\u566a\u58f0\u5206\u7c7b\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u566a\u58f0\u6bd4\u4f8b\u573a\u666f\uff0c\u5b9e\u9a8c\u6027\u80fd\u9886\u5148\u3002", "conclusion": "OCSplats\u6709\u6548\u89e3\u51b33DGS\u6297\u566a\u58f0\u91cd\u5efa\u95ee\u9898\uff0c\u65e0\u9700\u8c03\u53c2\u4e14\u6027\u80fd\u4f18\u8d8a\u3002"}}
{"id": "2508.01248", "pdf": "https://arxiv.org/pdf/2508.01248", "abs": "https://arxiv.org/abs/2508.01248", "authors": ["Jiazhen Yan", "Fan Wang", "Weiwei Jiang", "Ziqiang Li", "Zhangjie Fu"], "title": "NS-Net: Decoupling CLIP Semantic Information through NULL-Space for Generalizable AI-Generated Image Detection", "categories": ["cs.CV"], "comment": null, "summary": "The rapid progress of generative models, such as GANs and diffusion models, has facilitated the creation of highly realistic images, raising growing concerns over their misuse in security-sensitive domains. While existing detectors perform well under known generative settings, they often fail to generalize to unknown generative models, especially when semantic content between real and fake images is closely aligned. In this paper, we revisit the use of CLIP features for AI-generated image detection and uncover a critical limitation: the high-level semantic information embedded in CLIP's visual features hinders effective discrimination. To address this, we propose NS-Net, a novel detection framework that leverages NULL-Space projection to decouple semantic information from CLIP's visual features, followed by contrastive learning to capture intrinsic distributional differences between real and generated images. Furthermore, we design a Patch Selection strategy to preserve fine-grained artifacts by mitigating semantic bias caused by global image structures. Extensive experiments on an open-world benchmark comprising images generated by 40 diverse generative models show that NS-Net outperforms existing state-of-the-art methods, achieving a 7.4\\% improvement in detection accuracy, thereby demonstrating strong generalization across both GAN- and diffusion-based image generation techniques.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faNS-Net\uff0c\u901a\u8fc7NULL-Space\u6295\u5f71\u548c\u5bf9\u6bd4\u5b66\u4e60\u6539\u8fdbAI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u751f\u6210\u6a21\u578b\uff08\u5982GAN\u548c\u6269\u6563\u6a21\u578b\uff09\u751f\u6210\u7684\u56fe\u50cf\u8d8a\u6765\u8d8a\u903c\u771f\uff0c\u73b0\u6709\u68c0\u6d4b\u5668\u5728\u672a\u77e5\u6a21\u578b\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u8bed\u4e49\u5185\u5bb9\u76f8\u8fd1\u65f6\u3002", "method": "\u63d0\u51faNS-Net\uff0c\u5229\u7528NULL-Space\u6295\u5f71\u89e3\u8026CLIP\u89c6\u89c9\u7279\u5f81\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u6355\u6349\u771f\u5b9e\u4e0e\u751f\u6210\u56fe\u50cf\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u5e76\u8bbe\u8ba1Patch Selection\u7b56\u7565\u4fdd\u7559\u7ec6\u7c92\u5ea6\u4f2a\u5f71\u3002", "result": "\u5728\u5305\u542b40\u79cd\u751f\u6210\u6a21\u578b\u7684\u5f00\u653e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNS-Net\u68c0\u6d4b\u7cbe\u5ea6\u63d0\u53477.4%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "NS-Net\u5728GAN\u548c\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.01259", "pdf": "https://arxiv.org/pdf/2508.01259", "abs": "https://arxiv.org/abs/2508.01259", "authors": ["Zhengxue Wang", "Yuan Wu", "Xiang Li", "Zhiqiang Yan", "Jian Yang"], "title": "SpatioTemporal Difference Network for Video Depth Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Depth super-resolution has achieved impressive performance, and the incorporation of multi-frame information further enhances reconstruction quality. Nevertheless, statistical analyses reveal that video depth super-resolution remains affected by pronounced long-tailed distributions, with the long-tailed effects primarily manifesting in spatial non-smooth regions and temporal variation zones. To address these challenges, we propose a novel SpatioTemporal Difference Network (STDNet) comprising two core branches: a spatial difference branch and a temporal difference branch. In the spatial difference branch, we introduce a spatial difference mechanism to mitigate the long-tailed issues in spatial non-smooth regions. This mechanism dynamically aligns RGB features with learned spatial difference representations, enabling intra-frame RGB-D aggregation for depth calibration. In the temporal difference branch, we further design a temporal difference strategy that preferentially propagates temporal variation information from adjacent RGB and depth frames to the current depth frame, leveraging temporal difference representations to achieve precise motion compensation in temporal long-tailed areas. Extensive experimental results across multiple datasets demonstrate the effectiveness of our STDNet, outperforming existing approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSTDNet\u7684\u65f6\u7a7a\u5dee\u5f02\u7f51\u7edc\uff0c\u901a\u8fc7\u7a7a\u95f4\u548c\u65f6\u95f4\u5dee\u5f02\u5206\u652f\u89e3\u51b3\u89c6\u9891\u6df1\u5ea6\u8d85\u5206\u8fa8\u7387\u4e2d\u7684\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u89c6\u9891\u6df1\u5ea6\u8d85\u5206\u8fa8\u7387\u5728\u7a7a\u95f4\u975e\u5e73\u6ed1\u533a\u57df\u548c\u65f6\u95f4\u53d8\u5316\u533a\u57df\u5b58\u5728\u660e\u663e\u7684\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u91cd\u5efa\u8d28\u91cf\u3002", "method": "STDNet\u5305\u542b\u7a7a\u95f4\u5dee\u5f02\u5206\u652f\u548c\u65f6\u95f4\u5dee\u5f02\u5206\u652f\u3002\u7a7a\u95f4\u5dee\u5f02\u5206\u652f\u901a\u8fc7\u52a8\u6001\u5bf9\u9f50RGB\u7279\u5f81\u548c\u5b66\u4e60\u7a7a\u95f4\u5dee\u5f02\u8868\u793a\u6765\u6821\u51c6\u6df1\u5ea6\uff1b\u65f6\u95f4\u5dee\u5f02\u5206\u652f\u901a\u8fc7\u4f20\u64ad\u76f8\u90bb\u5e27\u7684\u65f6\u7a7a\u5dee\u5f02\u4fe1\u606f\u5b9e\u73b0\u7cbe\u786e\u8fd0\u52a8\u8865\u507f\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSTDNet\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "STDNet\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u6df1\u5ea6\u8d85\u5206\u8fa8\u7387\u4e2d\u7684\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2508.01272", "pdf": "https://arxiv.org/pdf/2508.01272", "abs": "https://arxiv.org/abs/2508.01272", "authors": ["Zonglei Jing", "Xiao Yang", "Xiaoqian Li", "Siyuan Liang", "Aishan Liu", "Mingchuan Zhang", "Xianglong Liu"], "title": "PromptSafe: Gated Prompt Tuning for Safe Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image (T2I) models have demonstrated remarkable generative capabilities but remain vulnerable to producing not-safe-for-work (NSFW) content, such as violent or explicit imagery. While recent moderation efforts have introduced soft prompt-guided tuning by appending defensive tokens to the input, these approaches often rely on large-scale curated image-text datasets and apply static, one-size-fits-all defenses at inference time. However, this results not only in high computational cost and degraded benign image quality, but also in limited adaptability to the diverse and nuanced safety requirements of real-world prompts. To address these challenges, we propose PromptSafe, a gated prompt tuning framework that combines a lightweight, text-only supervised soft embedding with an inference-time gated control network. Instead of training on expensive image-text datasets, we first rewrite unsafe prompts into semantically aligned but safe alternatives using an LLM, constructing an efficient text-only training corpus. Based on this, we optimize a universal soft prompt that repels unsafe and attracts safe embeddings during the diffusion denoising process. To avoid over-suppressing benign prompts, we introduce a gated mechanism that adaptively adjusts the defensive strength based on estimated prompt toxicity, thereby aligning defense intensity with prompt risk and ensuring strong protection for harmful inputs while preserving benign generation quality. Extensive experiments across multiple benchmarks and T2I models show that PromptSafe achieves a SOTA unsafe generation rate (2.36%), while preserving high benign fidelity. Furthermore, PromptSafe demonstrates strong generalization to unseen harmful categories, robust transferability across diffusion model architectures, and resilience under adaptive adversarial attacks, highlighting its practical value for safe and scalable deployment.", "AI": {"tldr": "PromptSafe\u662f\u4e00\u4e2a\u57fa\u4e8e\u95e8\u63a7\u63d0\u793a\u8c03\u4f18\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9632\u6b62\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u751f\u6210\u4e0d\u5b89\u5168\u5185\u5bb9\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u6027\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u56fe\u50cf-\u6587\u672c\u6570\u636e\u96c6\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u9002\u5e94\u6027\u5dee\uff0c\u65e0\u6cd5\u6ee1\u8db3\u591a\u6837\u5316\u7684\u5b89\u5168\u9700\u6c42\u3002", "method": "\u901a\u8fc7LLM\u91cd\u5199\u4e0d\u5b89\u5168\u63d0\u793a\uff0c\u6784\u5efa\u6587\u672c\u8bad\u7ec3\u8bed\u6599\u5e93\uff0c\u4f18\u5316\u901a\u7528\u8f6f\u63d0\u793a\uff0c\u5e76\u5f15\u5165\u95e8\u63a7\u673a\u5236\u52a8\u6001\u8c03\u6574\u9632\u5fa1\u5f3a\u5ea6\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPromptSafe\u5c06\u4e0d\u5b89\u5168\u751f\u6210\u7387\u964d\u81f32.36%\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u826f\u6027\u4fdd\u771f\u5ea6\u3002", "conclusion": "PromptSafe\u5728\u5b89\u5168\u6027\u3001\u6cdb\u5316\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2508.01386", "pdf": "https://arxiv.org/pdf/2508.01386", "abs": "https://arxiv.org/abs/2508.01386", "authors": ["Josef X. Biberstein", "Guilherme Cavalheiro", "Juyeop Han", "Sertac Karaman"], "title": "Construction of Digital Terrain Maps from Multi-view Satellite Imagery using Neural Volume Rendering", "categories": ["cs.CV"], "comment": null, "summary": "Digital terrain maps (DTMs) are an important part of planetary exploration, enabling operations such as terrain relative navigation during entry, descent, and landing for spacecraft and aiding in navigation on the ground. As robotic exploration missions become more ambitious, the need for high quality DTMs will only increase. However, producing DTMs via multi-view stereo pipelines for satellite imagery, the current state-of-the-art, can be cumbersome and require significant manual image preprocessing to produce satisfactory results. In this work, we seek to address these shortcomings by adapting neural volume rendering techniques to learn textured digital terrain maps directly from satellite imagery. Our method, neural terrain maps (NTM), only requires the locus for each image pixel and does not rely on depth or any other structural priors. We demonstrate our method on both synthetic and real satellite data from Earth and Mars encompassing scenes on the order of $100 \\textrm{km}^2$. We evaluate the accuracy of our output terrain maps by comparing with existing high-quality DTMs produced using traditional multi-view stereo pipelines. Our method shows promising results, with the precision of terrain prediction almost equal to the resolution of the satellite images even in the presence of imperfect camera intrinsics and extrinsics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u795e\u7ecf\u5730\u5f62\u56fe\uff08NTM\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u4f53\u79ef\u6e32\u67d3\u6280\u672f\u76f4\u63a5\u4ece\u536b\u661f\u56fe\u50cf\u5b66\u4e60\u5730\u5f62\u56fe\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u591a\u89c6\u56fe\u7acb\u4f53\u6d41\u7a0b\u7684\u7e41\u7410\u9884\u5904\u7406\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u63a2\u7d22\u4efb\u52a1\u7684\u590d\u6742\u6027\u589e\u52a0\uff0c\u9ad8\u8d28\u91cf\u6570\u5b57\u5730\u5f62\u56fe\uff08DTM\uff09\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u624b\u52a8\u9884\u5904\u7406\uff0c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u4f53\u79ef\u6e32\u67d3\u6280\u672f\uff0c\u4ec5\u9700\u6bcf\u4e2a\u56fe\u50cf\u50cf\u7d20\u7684\u8f68\u8ff9\u4fe1\u606f\uff0c\u65e0\u9700\u6df1\u5ea6\u6216\u5176\u4ed6\u7ed3\u6784\u5148\u9a8c\uff0c\u76f4\u63a5\u4ece\u536b\u661f\u56fe\u50cf\u5b66\u4e60\u5730\u5f62\u56fe\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u536b\u661f\u6570\u636e\uff08\u5730\u7403\u548c\u706b\u661f\uff09\u4e0a\u9a8c\u8bc1\uff0c\u8986\u76d6\u9762\u79ef\u7ea6100\u5e73\u65b9\u516c\u91cc\uff0c\u5730\u5f62\u9884\u6d4b\u7cbe\u5ea6\u63a5\u8fd1\u536b\u661f\u56fe\u50cf\u5206\u8fa8\u7387\u3002", "conclusion": "NTM\u65b9\u6cd5\u5728\u76f8\u673a\u53c2\u6570\u4e0d\u5b8c\u7f8e\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u5730\u5f62\u56fe\uff0c\u5c55\u793a\u4e86\u5176\u6f5c\u529b\u3002"}}
{"id": "2508.01464", "pdf": "https://arxiv.org/pdf/2508.01464", "abs": "https://arxiv.org/abs/2508.01464", "authors": ["Quankai Gao", "Iliyan Georgiev", "Tuanfeng Y. Wang", "Krishna Kumar Singh", "Ulrich Neumann", "Jae Shin Yoon"], "title": "Can3Tok: Canonical 3D Tokenization and Latent Modeling of Scene-Level 3D Gaussians", "categories": ["cs.CV"], "comment": null, "summary": "3D generation has made significant progress, however, it still largely remains at the object-level. Feedforward 3D scene-level generation has been rarely explored due to the lack of models capable of scaling-up latent representation learning on 3D scene-level data. Unlike object-level generative models, which are trained on well-labeled 3D data in a bounded canonical space, scene-level generations with 3D scenes represented by 3D Gaussian Splatting (3DGS) are unbounded and exhibit scale inconsistency across different scenes, making unified latent representation learning for generative purposes extremely challenging. In this paper, we introduce Can3Tok, the first 3D scene-level variational autoencoder (VAE) capable of encoding a large number of Gaussian primitives into a low-dimensional latent embedding, which effectively captures both semantic and spatial information of the inputs. Beyond model design, we propose a general pipeline for 3D scene data processing to address scale inconsistency issue. We validate our method on the recent scene-level 3D dataset DL3DV-10K, where we found that only Can3Tok successfully generalizes to novel 3D scenes, while compared methods fail to converge on even a few hundred scene inputs during training and exhibit zero generalization ability during inference. Finally, we demonstrate image-to-3DGS and text-to-3DGS generation as our applications to demonstrate its ability to facilitate downstream generation tasks.", "AI": {"tldr": "Can3Tok\u662f\u9996\u4e2a3D\u573a\u666f\u7ea7\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\uff0c\u80fd\u591f\u5c06\u5927\u91cf\u9ad8\u65af\u57fa\u5143\u7f16\u7801\u4e3a\u4f4e\u7ef4\u6f5c\u5728\u5d4c\u5165\uff0c\u89e3\u51b3\u4e863D\u573a\u666f\u751f\u6210\u4e2d\u7684\u5c3a\u5ea6\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u73b0\u67093D\u751f\u6210\u6280\u672f\u4e3b\u8981\u9488\u5bf9\u5bf9\u8c61\u7ea7\uff0c\u573a\u666f\u7ea7\u751f\u6210\u56e0\u7f3a\u4e4f\u7edf\u4e00\u6f5c\u5728\u8868\u793a\u5b66\u4e60\u6a21\u578b\u800c\u9c9c\u6709\u63a2\u7d22\u3002", "method": "\u63d0\u51faCan3Tok\u6a21\u578b\u53ca\u901a\u75283D\u573a\u666f\u6570\u636e\u5904\u7406\u6d41\u7a0b\uff0c\u89e3\u51b3\u5c3a\u5ea6\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "result": "\u5728DL3DV-10K\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cCan3Tok\u80fd\u6cdb\u5316\u5230\u65b0\u573a\u666f\uff0c\u800c\u5176\u4ed6\u65b9\u6cd5\u65e0\u6cd5\u6536\u655b\u3002", "conclusion": "Can3Tok\u6210\u529f\u5e94\u7528\u4e8e\u56fe\u50cf\u52303DGS\u548c\u6587\u672c\u52303DGS\u751f\u6210\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4e0b\u6e38\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.01533", "pdf": "https://arxiv.org/pdf/2508.01533", "abs": "https://arxiv.org/abs/2508.01533", "authors": ["Jiaxin Liu", "Zhaolu Kang"], "title": "ReasonAct: Progressive Training for Fine-Grained Video Reasoning in Small Models", "categories": ["cs.CV"], "comment": null, "summary": "While recent multimodal models have shown progress in vision-language tasks, small-scale variants still struggle with the fine-grained temporal reasoning required for video understanding. We introduce ReasonAct, a method that enhances video reasoning in smaller models through a three-stage training process: first building a foundation with text-only reasoning, then fine-tuning on video, and finally refining with temporal-aware reinforcement learning. We build upon Temporal Group Relative Policy Optimization (T-GRPO) by incorporating temporal consistency modeling into policy optimization. We also propose a biomechanically-motivated sub-action decomposition mechanism that provides graduated rewards for constituent action phases. Through experiments on HMDB51, UCF-101, and Kinetics-400, our 3B-parameter model achieves 67.2%, 94.1%, and 78.9% accuracy respectively, demonstrating improvements of 17.9, 15.8, and 12.3 points over baselines. Ablation studies validate that our progressive training methodology enables smaller models to achieve competitive video reasoning performance while maintaining computational efficiency.", "AI": {"tldr": "ReasonAct\u901a\u8fc7\u4e09\u9636\u6bb5\u8bad\u7ec3\u63d0\u5347\u5c0f\u578b\u6a21\u578b\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u7ec6\u7c92\u5ea6\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\uff0c\u7ed3\u5408T-GRPO\u4e0e\u65f6\u5e8f\u4e00\u81f4\u6027\u5efa\u6a21\uff0c\u63d0\u51fa\u751f\u7269\u529b\u5b66\u542f\u53d1\u7684\u5b50\u52a8\u4f5c\u5206\u89e3\u673a\u5236\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3\u5c0f\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u7ec6\u7c92\u5ea6\u65f6\u5e8f\u63a8\u7406\u7684\u4e0d\u8db3\u3002", "method": "\u4e09\u9636\u6bb5\u8bad\u7ec3\uff1a\u6587\u672c\u63a8\u7406\u57fa\u7840\u8bad\u7ec3\u3001\u89c6\u9891\u5fae\u8c03\u3001\u65f6\u5e8f\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff1b\u7ed3\u5408T-GRPO\u4e0e\u65f6\u5e8f\u4e00\u81f4\u6027\u5efa\u6a21\uff1b\u63d0\u51fa\u5b50\u52a8\u4f5c\u5206\u89e3\u673a\u5236\u3002", "result": "\u5728HMDB51\u3001UCF-101\u548cKinetics-400\u4e0a\u5206\u522b\u8fbe\u523067.2%\u300194.1%\u548c78.9%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u65b9\u6cd5\u4f7f\u5c0f\u578b\u6a21\u578b\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u7ade\u4e89\u6027\u89c6\u9891\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2508.01617", "pdf": "https://arxiv.org/pdf/2508.01617", "abs": "https://arxiv.org/abs/2508.01617", "authors": ["Xuanzhao Dong", "Wenhui Zhu", "Xiwen Chen", "Zhipeng Wang", "Peijie Qiu", "Shao Tang", "Xin Li", "Yalin Wang"], "title": "LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical Image Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Autoregressive models (ARMs) have long dominated the landscape of biomedical vision-language models (VLMs). Recently, masked diffusion models such as LLaDA have emerged as promising alternatives, yet their application in the biomedical domain remains largely underexplored. To bridge this gap, we introduce \\textbf{LLaDA-MedV}, the first large language diffusion model tailored for biomedical image understanding through vision instruction tuning. LLaDA-MedV achieves relative performance gains of 7.855\\% over LLaVA-Med and 1.867\\% over LLaDA-V in the open-ended biomedical visual conversation task, and sets new state-of-the-art accuracy on the closed-form subset of three VQA benchmarks: 84.93\\% on VQA-RAD, 92.31\\% on SLAKE, and 95.15\\% on PathVQA. Furthermore, a detailed comparison with LLaVA-Med suggests that LLaDA-MedV is capable of generating reasonably longer responses by explicitly controlling response length, which can lead to more informative outputs. We also conduct an in-depth analysis of both the training and inference stages, highlighting the critical roles of initialization weight selection, fine-tuning strategies, and the interplay between sampling steps and response repetition. The code and model weight is released at https://github.com/LLM-VLM-GSL/LLaDA-MedV.", "AI": {"tldr": "LLaDA-MedV\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u4e3a\u751f\u7269\u533b\u5b66\u56fe\u50cf\u7406\u89e3\u8bbe\u8ba1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u586b\u8865\u6269\u6563\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9886\u57df\u7684\u7a7a\u767d\uff0c\u63d0\u5347\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u89c6\u89c9\u6307\u4ee4\u8c03\u4f18\uff0c\u7ed3\u5408\u521d\u59cb\u5316\u6743\u91cd\u9009\u62e9\u3001\u5fae\u8c03\u7b56\u7565\u548c\u91c7\u6837\u6b65\u9aa4\u4f18\u5316\u3002", "result": "\u5728\u5f00\u653e\u548c\u5c01\u95ed\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u6700\u4f73\u6027\u80fd\uff0c\u751f\u6210\u66f4\u957f\u4e14\u4fe1\u606f\u66f4\u4e30\u5bcc\u7684\u56de\u7b54\u3002", "conclusion": "LLaDA-MedV\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.01650", "pdf": "https://arxiv.org/pdf/2508.01650", "abs": "https://arxiv.org/abs/2508.01650", "authors": ["Na Zhang", "Moran Li", "Chengming Xu", "Han Feng", "Xiaobin Hu", "Jiangning Zhang", "Weijian Cao", "Chengjie Wang", "Yanwei Fu"], "title": "StrandDesigner: Towards Practical Strand Generation with Sketch Guidance", "categories": ["cs.CV"], "comment": "Accepted to ACM Multimedia 2025", "summary": "Realistic hair strand generation is crucial for applications like computer graphics and virtual reality. While diffusion models can generate hairstyles from text or images, these inputs lack precision and user-friendliness. Instead, we propose the first sketch-based strand generation model, which offers finer control while remaining user-friendly. Our framework tackles key challenges, such as modeling complex strand interactions and diverse sketch patterns, through two main innovations: a learnable strand upsampling strategy that encodes 3D strands into multi-scale latent spaces, and a multi-scale adaptive conditioning mechanism using a transformer with diffusion heads to ensure consistency across granularity levels. Experiments on several benchmark datasets show our method outperforms existing approaches in realism and precision. Qualitative results further confirm its effectiveness. Code will be released at [GitHub](https://github.com/fighting-Zhang/StrandDesigner).", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u57fa\u4e8e\u8349\u56fe\u7684\u53d1\u4e1d\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u53d1\u4e1d\u4e0a\u91c7\u6837\u548c\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u6761\u4ef6\u673a\u5236\u63d0\u5347\u751f\u6210\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6587\u672c\u6216\u56fe\u50cf\u7684\u6269\u6563\u6a21\u578b\u5728\u53d1\u4e1d\u751f\u6210\u4e2d\u7f3a\u4e4f\u7cbe\u786e\u6027\u548c\u7528\u6237\u53cb\u597d\u6027\uff0c\u8349\u56fe\u8f93\u5165\u80fd\u63d0\u4f9b\u66f4\u7cbe\u7ec6\u7684\u63a7\u5236\u3002", "method": "\u91c7\u7528\u53ef\u5b66\u4e60\u7684\u53d1\u4e1d\u4e0a\u91c7\u6837\u7b56\u7565\u548c\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u6761\u4ef6\u673a\u5236\uff08\u57fa\u4e8eTransformer\u548c\u6269\u6563\u5934\uff09\uff0c\u89e3\u51b3\u53d1\u4e1d\u4ea4\u4e92\u548c\u8349\u56fe\u591a\u6837\u6027\u7684\u6311\u6218\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u6548\u679c\u66f4\u771f\u5b9e\u548c\u7cbe\u786e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u53d1\u4e1d\u751f\u6210\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u63a7\u5236\u6027\u548c\u7528\u6237\u53cb\u597d\u6027\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2508.01651", "pdf": "https://arxiv.org/pdf/2508.01651", "abs": "https://arxiv.org/abs/2508.01651", "authors": ["Hanqing Wang", "Zhenhao Zhang", "Kaiyang Ji", "Mingyu Liu", "Wenti Yin", "Yuchao Chen", "Zhirui Liu", "Xiangyu Zeng", "Tianxiang Gui", "Hangxing Zhang"], "title": "DAG: Unleash the Potential of Diffusion Model for Open-Vocabulary 3D Affordance Grounding", "categories": ["cs.CV"], "comment": null, "summary": "3D object affordance grounding aims to predict the touchable regions on a 3d object, which is crucial for human-object interaction, human-robot interaction, embodied perception, and robot learning. Recent advances tackle this problem via learning from demonstration images. However, these methods fail to capture the general affordance knowledge within the image, leading to poor generalization. To address this issue, we propose to use text-to-image diffusion models to extract the general affordance knowledge because we find that such models can generate semantically valid HOI images, which demonstrate that their internal representation space is highly correlated with real-world affordance concepts. Specifically, we introduce the DAG, a diffusion-based 3d affordance grounding framework, which leverages the frozen internal representations of the text-to-image diffusion model and unlocks affordance knowledge within the diffusion model to perform 3D affordance grounding. We further introduce an affordance block and a multi-source affordance decoder to endow 3D dense affordance prediction. Extensive experimental evaluations show that our model excels over well-established methods and exhibits open-world generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u76843D\u7269\u4f53\u53ef\u4f9b\u6027\uff08affordance\uff09\u5b9a\u4f4d\u65b9\u6cd5DAG\uff0c\u901a\u8fc7\u5229\u7528\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\u7a7a\u95f4\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u6f14\u793a\u56fe\u50cf\u9884\u6d4b3D\u7269\u4f53\u7684\u53ef\u89e6\u6478\u533a\u57df\uff0c\u4f46\u65e0\u6cd5\u6355\u6349\u56fe\u50cf\u4e2d\u7684\u901a\u7528\u53ef\u4f9b\u6027\u77e5\u8bc6\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51faDAG\u6846\u67b6\uff0c\u5229\u7528\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u51bb\u7ed3\u5185\u90e8\u8868\u793a\u7a7a\u95f4\uff0c\u7ed3\u5408\u53ef\u4f9b\u6027\u6a21\u5757\u548c\u591a\u6e90\u53ef\u4f9b\u6027\u89e3\u7801\u5668\uff0c\u5b9e\u73b03D\u5bc6\u96c6\u53ef\u4f9b\u6027\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDAG\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u5f00\u653e\u4e16\u754c\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DAG\u901a\u8fc7\u6269\u6563\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\u7a7a\u95f4\u6210\u529f\u63d0\u53d6\u4e86\u901a\u7528\u53ef\u4f9b\u6027\u77e5\u8bc6\uff0c\u4e3a3D\u53ef\u4f9b\u6027\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01684", "pdf": "https://arxiv.org/pdf/2508.01684", "abs": "https://arxiv.org/abs/2508.01684", "authors": ["Yufeng Chi", "Huimin Ma", "Kafeng Wang", "Jianmin Li"], "title": "DisCo3D: Distilling Multi-View Consistency for 3D Scene Editing", "categories": ["cs.CV"], "comment": "17 pages, 7 figures", "summary": "While diffusion models have demonstrated remarkable progress in 2D image generation and editing, extending these capabilities to 3D editing remains challenging, particularly in maintaining multi-view consistency. Classical approaches typically update 3D representations through iterative refinement based on a single editing view. However, these methods often suffer from slow convergence and blurry artifacts caused by cross-view inconsistencies. Recent methods improve efficiency by propagating 2D editing attention features, yet still exhibit fine-grained inconsistencies and failure modes in complex scenes due to insufficient constraints. To address this, we propose \\textbf{DisCo3D}, a novel framework that distills 3D consistency priors into a 2D editor. Our method first fine-tunes a 3D generator using multi-view inputs for scene adaptation, then trains a 2D editor through consistency distillation. The edited multi-view outputs are finally optimized into 3D representations via Gaussian Splatting. Experimental results show DisCo3D achieves stable multi-view consistency and outperforms state-of-the-art methods in editing quality.", "AI": {"tldr": "DisCo3D\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5c063D\u4e00\u81f4\u6027\u5148\u9a8c\u77e5\u8bc6\u84b8\u998f\u52302D\u7f16\u8f91\u5668\u4e2d\uff0c\u89e3\u51b3\u4e863D\u7f16\u8f91\u4e2d\u7684\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u57282D\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u6269\u5c55\u52303D\u7f16\u8f91\u65f6\u9762\u4e34\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u7684\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6536\u655b\u6162\u3001\u6a21\u7cca\u4f2a\u5f71\u548c\u7ec6\u7c92\u5ea6\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898\u3002", "method": "DisCo3D\u9996\u5148\u901a\u8fc7\u591a\u89c6\u89d2\u8f93\u5165\u5fae\u8c033D\u751f\u6210\u5668\u4ee5\u9002\u5e94\u573a\u666f\uff0c\u7136\u540e\u901a\u8fc7\u4e00\u81f4\u6027\u84b8\u998f\u8bad\u7ec32D\u7f16\u8f91\u5668\uff0c\u6700\u540e\u901a\u8fc7\u9ad8\u65af\u6cfc\u6e85\u5c06\u7f16\u8f91\u540e\u7684\u591a\u89c6\u89d2\u8f93\u51fa\u4f18\u5316\u4e3a3D\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDisCo3D\u5728\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u7a33\u5b9a\uff0c\u7f16\u8f91\u8d28\u91cf\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DisCo3D\u901a\u8fc7\u4e00\u81f4\u6027\u84b8\u998f\u548c\u9ad8\u65af\u6cfc\u6e85\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e863D\u7f16\u8f91\u4e2d\u7684\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7f16\u8f91\u8d28\u91cf\u3002"}}
{"id": "2508.01698", "pdf": "https://arxiv.org/pdf/2508.01698", "abs": "https://arxiv.org/abs/2508.01698", "authors": ["Zuhao Yang", "Jiahui Zhang", "Yingchen Yu", "Shijian Lu", "Song Bai"], "title": "Versatile Transition Generation with Image-to-Video Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Leveraging text, images, structure maps, or motion trajectories as conditional guidance, diffusion models have achieved great success in automated and high-quality video generation. However, generating smooth and rational transition videos given the first and last video frames as well as descriptive text prompts is far underexplored. We present VTG, a Versatile Transition video Generation framework that can generate smooth, high-fidelity, and semantically coherent video transitions. VTG introduces interpolation-based initialization that helps preserve object identity and handle abrupt content changes effectively. In addition, it incorporates dual-directional motion fine-tuning and representation alignment regularization to mitigate the limitations of pre-trained image-to-video diffusion models in motion smoothness and generation fidelity, respectively. To evaluate VTG and facilitate future studies on unified transition generation, we collected TransitBench, a comprehensive benchmark for transition generation covering two representative transition tasks: concept blending and scene transition. Extensive experiments show that VTG achieves superior transition performance consistently across all four tasks.", "AI": {"tldr": "VTG\u662f\u4e00\u4e2a\u591a\u529f\u80fd\u8fc7\u6e21\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u63d2\u503c\u521d\u59cb\u5316\u548c\u53cc\u65b9\u5411\u8fd0\u52a8\u5fae\u8c03\u7b49\u6280\u672f\uff0c\u751f\u6210\u5e73\u6ed1\u3001\u9ad8\u4fdd\u771f\u4e14\u8bed\u4e49\u8fde\u8d2f\u7684\u89c6\u9891\u8fc7\u6e21\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5728\u7ed9\u5b9a\u9996\u5c3e\u5e27\u548c\u6587\u672c\u63d0\u793a\u65f6\uff0c\u751f\u6210\u5e73\u6ed1\u5408\u7406\u7684\u8fc7\u6e21\u89c6\u9891\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "VTG\u91c7\u7528\u63d2\u503c\u521d\u59cb\u5316\u3001\u53cc\u65b9\u5411\u8fd0\u52a8\u5fae\u8c03\u548c\u8868\u793a\u5bf9\u9f50\u6b63\u5219\u5316\u6280\u672f\u3002", "result": "VTG\u5728TransitBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5728\u6982\u5ff5\u6df7\u5408\u548c\u573a\u666f\u8fc7\u6e21\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "conclusion": "VTG\u4e3a\u8fc7\u6e21\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63a8\u52a8\u4e86\u7edf\u4e00\u8fc7\u6e21\u751f\u6210\u7814\u7a76\u7684\u8fdb\u5c55\u3002"}}
{"id": "2508.01704", "pdf": "https://arxiv.org/pdf/2508.01704", "abs": "https://arxiv.org/abs/2508.01704", "authors": ["Luqi Cheng", "Zhangshuo Qi", "Zijie Zhou", "Chao Lu", "Guangming Xiong"], "title": "LT-Gaussian: Long-Term Map Update Using 3D Gaussian Splatting for Autonomous Driving", "categories": ["cs.CV"], "comment": "Accepted by IV 2025", "summary": "Maps play an important role in autonomous driving systems. The recently proposed 3D Gaussian Splatting (3D-GS) produces rendering-quality explicit scene reconstruction results, demonstrating the potential for map construction in autonomous driving scenarios. However, because of the time and computational costs involved in generating Gaussian scenes, how to update the map becomes a significant challenge. In this paper, we propose LT-Gaussian, a map update method for 3D-GS-based maps. LT-Gaussian consists of three main components: Multimodal Gaussian Splatting, Structural Change Detection Module, and Gaussian-Map Update Module. Firstly, the Gaussian map of the old scene is generated using our proposed Multimodal Gaussian Splatting. Subsequently, during the map update process, we compare the outdated Gaussian map with the current LiDAR data stream to identify structural changes. Finally, we perform targeted updates to the Gaussian-map to generate an up-to-date map. We establish a benchmark for map updating on the nuScenes dataset to quantitatively evaluate our method. The experimental results show that LT-Gaussian can effectively and efficiently update the Gaussian-map, handling common environmental changes in autonomous driving scenarios. Furthermore, by taking full advantage of information from both new and old scenes, LT-Gaussian is able to produce higher quality reconstruction results compared to map update strategies that reconstruct maps from scratch. Our open-source code is available at https://github.com/ChengLuqi/LT-gaussian.", "AI": {"tldr": "\u63d0\u51faLT-Gaussian\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u66f4\u65b0\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u5730\u56fe\uff0c\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u5730\u56fe\u66f4\u65b0\u7684\u6311\u6218\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\uff083D-GS\uff09\u5728\u81ea\u52a8\u9a7e\u9a76\u5730\u56fe\u6784\u5efa\u4e2d\u6f5c\u529b\u663e\u8457\uff0c\u4f46\u751f\u6210\u548c\u66f4\u65b0\u9ad8\u65af\u573a\u666f\u7684\u65f6\u95f4\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u89e3\u51b3\u5730\u56fe\u66f4\u65b0\u95ee\u9898\u3002", "method": "LT-Gaussian\u5305\u542b\u591a\u6a21\u6001\u9ad8\u65af\u6cfc\u6e85\u3001\u7ed3\u6784\u53d8\u5316\u68c0\u6d4b\u6a21\u5757\u548c\u9ad8\u65af\u5730\u56fe\u66f4\u65b0\u6a21\u5757\uff0c\u901a\u8fc7\u5bf9\u6bd4\u65e7\u5730\u56fe\u4e0e\u5f53\u524dLiDAR\u6570\u636e\u5b9e\u73b0\u9488\u5bf9\u6027\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLT-Gaussian\u80fd\u9ad8\u6548\u66f4\u65b0\u5730\u56fe\uff0c\u5904\u7406\u5e38\u89c1\u73af\u5883\u53d8\u5316\uff0c\u4e14\u91cd\u5efa\u8d28\u91cf\u4f18\u4e8e\u4ece\u5934\u6784\u5efa\u7684\u7b56\u7565\u3002", "conclusion": "LT-Gaussian\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5730\u56fe\u66f4\u65b0\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01740", "pdf": "https://arxiv.org/pdf/2508.01740", "abs": "https://arxiv.org/abs/2508.01740", "authors": ["Zhaonan Wang", "Manyi Li", "Changhe Tu"], "title": "AG$^2$aussian: Anchor-Graph Structured Gaussian Splatting for Instance-Level 3D Scene Understanding and Editing", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has witnessed exponential adoption across diverse applications, driving a critical need for semantic-aware 3D Gaussian representations to enable scene understanding and editing tasks. Existing approaches typically attach semantic features to a collection of free Gaussians and distill the features via differentiable rendering, leading to noisy segmentation and a messy selection of Gaussians. In this paper, we introduce AG$^2$aussian, a novel framework that leverages an anchor-graph structure to organize semantic features and regulate Gaussian primitives. Our anchor-graph structure not only promotes compact and instance-aware Gaussian distributions, but also facilitates graph-based propagation, achieving a clean and accurate instance-level Gaussian selection. Extensive validation across four applications, i.e. interactive click-based query, open-vocabulary text-driven query, object removal editing, and physics simulation, demonstrates the advantages of our approach and its benefits to various applications. The experiments and ablation studies further evaluate the effectiveness of the key designs of our approach.", "AI": {"tldr": "AG$^2$aussian\u6846\u67b6\u901a\u8fc7\u951a\u70b9\u56fe\u7ed3\u6784\u7ec4\u7ec7\u8bed\u4e49\u7279\u5f81\uff0c\u4f18\u53163D\u9ad8\u65af\u8868\u793a\uff0c\u5b9e\u73b0\u66f4\u6e05\u6670\u7684\u5b9e\u4f8b\u7ea7\u5206\u5272\u548c\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u57283D\u9ad8\u65af\u8868\u793a\u4e2d\u9644\u52a0\u8bed\u4e49\u7279\u5f81\u65f6\u5bfc\u81f4\u566a\u58f0\u5206\u5272\u548c\u9ad8\u65af\u9009\u62e9\u6df7\u4e71\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528\u951a\u70b9\u56fe\u7ed3\u6784\u7ec4\u7ec7\u8bed\u4e49\u7279\u5f81\u5e76\u89c4\u8303\u9ad8\u65af\u57fa\u5143\uff0c\u4fc3\u8fdb\u7d27\u51d1\u4e14\u5b9e\u4f8b\u611f\u77e5\u7684\u9ad8\u65af\u5206\u5e03\u3002", "result": "\u5728\u56db\u79cd\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u5305\u62ec\u4ea4\u4e92\u5f0f\u67e5\u8be2\u3001\u6587\u672c\u9a71\u52a8\u67e5\u8be2\u3001\u5bf9\u8c61\u79fb\u9664\u7f16\u8f91\u548c\u7269\u7406\u6a21\u62df\u3002", "conclusion": "AG$^2$aussian\u901a\u8fc7\u951a\u70b9\u56fe\u7ed3\u6784\u663e\u8457\u63d0\u5347\u4e86\u8bed\u4e49\u611f\u77e53D\u9ad8\u65af\u7684\u51c6\u786e\u6027\u548c\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2508.01835", "pdf": "https://arxiv.org/pdf/2508.01835", "abs": "https://arxiv.org/abs/2508.01835", "authors": ["Yufei Zhang", "Zijun Cui", "Jeffrey O. Kephart", "Qiang Ji"], "title": "Diffusion-based 3D Hand Motion Recovery with Intuitive Physics", "categories": ["cs.CV"], "comment": "ICCV2025", "summary": "While 3D hand reconstruction from monocular images has made significant progress, generating accurate and temporally coherent motion estimates from videos remains challenging, particularly during hand-object interactions. In this paper, we present a novel 3D hand motion recovery framework that enhances image-based reconstructions through a diffusion-based and physics-augmented motion refinement model. Our model captures the distribution of refined motion estimates conditioned on initial ones, generating improved sequences through an iterative denoising process. Instead of relying on scarce annotated video data, we train our model only using motion capture data without images. We identify valuable intuitive physics knowledge during hand-object interactions, including key motion states and their associated motion constraints. We effectively integrate these physical insights into our diffusion model to improve its performance. Extensive experiments demonstrate that our approach significantly improves various frame-wise reconstruction methods, achieving state-of-the-art (SOTA) performance on existing benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u548c\u7269\u7406\u589e\u5f3a\u7684\u8fd0\u52a8\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u6062\u590d3D\u624b\u90e8\u8fd0\u52a8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u7684\u51c6\u786e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u5c3d\u7ba1\u4ece\u5355\u76ee\u56fe\u50cf\u8fdb\u884c3D\u624b\u90e8\u91cd\u5efa\u5df2\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u89c6\u9891\u4e2d\u751f\u6210\u51c6\u786e\u4e14\u65f6\u95f4\u4e00\u81f4\u7684\u8fd0\u52a8\u4f30\u8ba1\u4ecd\u5177\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u624b\u4e0e\u7269\u4f53\u4ea4\u4e92\u65f6\u3002", "method": "\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u7269\u7406\u589e\u5f3a\u7684\u8fd0\u52a8\u4f18\u5316\u6a21\u578b\uff0c\u5229\u7528\u8fd0\u52a8\u6355\u6349\u6570\u636e\uff08\u65e0\u9700\u56fe\u50cf\uff09\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u7269\u7406\u77e5\u8bc6\uff08\u5982\u5173\u952e\u8fd0\u52a8\u72b6\u6001\u53ca\u5176\u7ea6\u675f\uff09\u4f18\u5316\u8fd0\u52a8\u5e8f\u5217\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cd\u5e27\u7ea7\u91cd\u5efa\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u5728\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\uff08SOTA\uff09\u6c34\u5e73\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u7269\u7406\u77e5\u8bc6\uff0c\u6709\u6548\u63d0\u5347\u4e863D\u624b\u90e8\u8fd0\u52a8\u91cd\u5efa\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.01845", "pdf": "https://arxiv.org/pdf/2508.01845", "abs": "https://arxiv.org/abs/2508.01845", "authors": ["Zhongliang Guo", "Yifei Qian", "Yanli Li", "Weiye Li", "Chun Tong Lei", "Shuai Zhao", "Lei Fang", "Ognjen Arandjelovi\u0107", "Chun Pong Lau"], "title": "Beyond Vulnerabilities: A Survey of Adversarial Attacks as Both Threats and Defenses in Computer Vision Systems", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": "33 pages", "summary": "Adversarial attacks against computer vision systems have emerged as a critical research area that challenges the fundamental assumptions about neural network robustness and security. This comprehensive survey examines the evolving landscape of adversarial techniques, revealing their dual nature as both sophisticated security threats and valuable defensive tools. We provide a systematic analysis of adversarial attack methodologies across three primary domains: pixel-space attacks, physically realizable attacks, and latent-space attacks. Our investigation traces the technical evolution from early gradient-based methods such as FGSM and PGD to sophisticated optimization techniques incorporating momentum, adaptive step sizes, and advanced transferability mechanisms. We examine how physically realizable attacks have successfully bridged the gap between digital vulnerabilities and real-world threats through adversarial patches, 3D textures, and dynamic optical perturbations. Additionally, we explore the emergence of latent-space attacks that leverage semantic structure in internal representations to create more transferable and meaningful adversarial examples. Beyond traditional offensive applications, we investigate the constructive use of adversarial techniques for vulnerability assessment in biometric authentication systems and protection against malicious generative models. Our analysis reveals critical research gaps, particularly in neural style transfer protection and computational efficiency requirements. This survey contributes a comprehensive taxonomy, evolution analysis, and identification of future research directions, aiming to advance understanding of adversarial vulnerabilities and inform the development of more robust and trustworthy computer vision systems.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5bf9\u6297\u6027\u653b\u51fb\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u4e2d\u7684\u53cc\u91cd\u89d2\u8272\uff0c\u5206\u6790\u4e86\u50cf\u7d20\u7a7a\u95f4\u3001\u7269\u7406\u53ef\u5b9e\u73b0\u548c\u6f5c\u5728\u7a7a\u95f4\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u9632\u5fa1\u548c\u6f0f\u6d1e\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u5bf9\u6297\u6027\u653b\u51fb\u63ed\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u7684\u6839\u672c\u95ee\u9898\uff0c\u7814\u7a76\u5176\u65b9\u6cd5\u548c\u5e94\u7528\u6709\u52a9\u4e8e\u63d0\u5347\u7cfb\u7edf\u5b89\u5168\u6027\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u4e86\u4e09\u79cd\u5bf9\u6297\u6027\u653b\u51fb\u65b9\u6cd5\uff1a\u50cf\u7d20\u7a7a\u95f4\u653b\u51fb\u3001\u7269\u7406\u53ef\u5b9e\u73b0\u653b\u51fb\u548c\u6f5c\u5728\u7a7a\u95f4\u653b\u51fb\uff0c\u5e76\u8ffd\u8e2a\u4e86\u4ece\u68af\u5ea6\u57fa\u7840\u65b9\u6cd5\u5230\u9ad8\u7ea7\u4f18\u5316\u6280\u672f\u7684\u6f14\u53d8\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5bf9\u6297\u6027\u653b\u51fb\u4e0d\u4ec5\u662f\u5b89\u5168\u5a01\u80c1\uff0c\u8fd8\u53ef\u7528\u4e8e\u9632\u5fa1\u548c\u6f0f\u6d1e\u8bc4\u4f30\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u795e\u7ecf\u98ce\u683c\u8f6c\u79fb\u4fdd\u62a4\u548c\u8ba1\u7b97\u6548\u7387\u7684\u7814\u7a76\u7a7a\u767d\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u5bf9\u6297\u6027\u653b\u51fb\u7684\u5168\u9762\u5206\u7c7b\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u63a8\u52a8\u66f4\u9c81\u68d2\u548c\u53ef\u4fe1\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2508.01873", "pdf": "https://arxiv.org/pdf/2508.01873", "abs": "https://arxiv.org/abs/2508.01873", "authors": ["Siran Peng", "Haoyuan Zhang", "Li Gao", "Tianshuo Zhang", "Bao Li", "Zhen Lei"], "title": "DiffusionFF: Face Forgery Detection via Diffusion-based Artifact Localization", "categories": ["cs.CV"], "comment": null, "summary": "The rapid evolution of deepfake generation techniques demands robust and accurate face forgery detection algorithms. While determining whether an image has been manipulated remains essential, the ability to precisely localize forgery artifacts has become increasingly important for improving model explainability and fostering user trust. To address this challenge, we propose DiffusionFF, a novel framework that enhances face forgery detection through diffusion-based artifact localization. Our method utilizes a denoising diffusion model to generate high-quality Structural Dissimilarity (DSSIM) maps, which effectively capture subtle traces of manipulation. These DSSIM maps are then fused with high-level semantic features extracted by a pretrained forgery detector, leading to significant improvements in detection accuracy. Extensive experiments on both cross-dataset and intra-dataset benchmarks demonstrate that DiffusionFF not only achieves superior detection performance but also offers precise and fine-grained artifact localization, highlighting its overall effectiveness.", "AI": {"tldr": "DiffusionFF\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u548c\u4f2a\u9020\u4f2a\u5f71\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u548c\u51c6\u786e\u7684\u68c0\u6d4b\u7b97\u6cd5\uff0c\u540c\u65f6\u5b9a\u4f4d\u4f2a\u9020\u4f2a\u5f71\u4ee5\u63d0\u9ad8\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u7528\u6237\u4fe1\u4efb\u3002", "method": "\u5229\u7528\u53bb\u566a\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u7684DSSIM\u56fe\uff0c\u6355\u6349\u7ec6\u5fae\u7684\u4f2a\u9020\u75d5\u8ff9\uff0c\u5e76\u4e0e\u9884\u8bad\u7ec3\u7684\u4f2a\u9020\u68c0\u6d4b\u5668\u63d0\u53d6\u7684\u9ad8\u7ea7\u8bed\u4e49\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728\u8de8\u6570\u636e\u96c6\u548c\u6570\u636e\u96c6\u5185\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDiffusionFF\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u68c0\u6d4b\u6027\u80fd\u548c\u7cbe\u7ec6\u7684\u4f2a\u5f71\u5b9a\u4f4d\u80fd\u529b\u3002", "conclusion": "DiffusionFF\u5728\u68c0\u6d4b\u548c\u5b9a\u4f4d\u4f2a\u9020\u4f2a\u5f71\u65b9\u9762\u5177\u6709\u663e\u8457\u6548\u679c\uff0c\u63d0\u5347\u4e86\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2508.02004", "pdf": "https://arxiv.org/pdf/2508.02004", "abs": "https://arxiv.org/abs/2508.02004", "authors": ["Kyungmin Jo", "Jooyeol Yun", "Jaegul Choo"], "title": "Devil is in the Detail: Towards Injecting Fine Details of Image Prompt in Image Generation via Conflict-free Guidance and Stratified Attention", "categories": ["cs.CV"], "comment": null, "summary": "While large-scale text-to-image diffusion models enable the generation of high-quality, diverse images from text prompts, these prompts struggle to capture intricate details, such as textures, preventing the user intent from being reflected. This limitation has led to efforts to generate images conditioned on user-provided images, referred to as image prompts. Recent work modifies the self-attention mechanism to impose image conditions in generated images by replacing or concatenating the keys and values from the image prompt. This enables the self-attention layer to work like a cross-attention layer, generally used to incorporate text prompts. In this paper, we identify two common issues in existing methods of modifying self-attention to generate images that reflect the details of image prompts. First, existing approaches neglect the importance of image prompts in classifier-free guidance. Specifically, current methods use image prompts as both desired and undesired conditions in classifier-free guidance, causing conflicting signals. To resolve this, we propose conflict-free guidance by using image prompts only as desired conditions, ensuring that the generated image faithfully reflects the image prompt. In addition, we observe that the two most common self-attention modifications involve a trade-off between the realism of the generated image and alignment with the image prompt. Specifically, selecting more keys and values from the image prompt improves alignment, while selecting more from the generated image enhances realism. To balance both, we propose an new self-attention modification method, Stratified Attention to jointly use keys and values from both images rather than selecting between them. Through extensive experiments across three image generation tasks, we show that the proposed method outperforms existing image-prompting models in faithfully reflecting the image prompt.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u6ce8\u610f\u529b\u4fee\u6539\u65b9\u6cd5\uff08Stratified Attention\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u56fe\u50cf\u63d0\u793a\u751f\u6210\u4e2d\u7684\u51b2\u7a81\u95ee\u9898\uff0c\u5e76\u5e73\u8861\u4e86\u751f\u6210\u56fe\u50cf\u7684\u903c\u771f\u5ea6\u4e0e\u56fe\u50cf\u63d0\u793a\u7684\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u96be\u4ee5\u6355\u6349\u590d\u6742\u7ec6\u8282\uff08\u5982\u7eb9\u7406\uff09\uff0c\u5bfc\u81f4\u7528\u6237\u610f\u56fe\u65e0\u6cd5\u5b8c\u5168\u4f53\u73b0\u3002\u56fe\u50cf\u63d0\u793a\u65b9\u6cd5\u867d\u80fd\u6539\u5584\uff0c\u4f46\u5b58\u5728\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u4e2d\u7684\u51b2\u7a81\u4fe1\u53f7\u95ee\u9898\uff0c\u4e14\u73b0\u6709\u81ea\u6ce8\u610f\u529b\u4fee\u6539\u65b9\u6cd5\u5728\u903c\u771f\u5ea6\u4e0e\u5bf9\u9f50\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "method": "\u63d0\u51fa\u51b2\u7a81\u81ea\u7531\u5f15\u5bfc\uff08\u4ec5\u5c06\u56fe\u50cf\u63d0\u793a\u4f5c\u4e3a\u671f\u671b\u6761\u4ef6\uff09\u548c\u5206\u5c42\u6ce8\u610f\u529b\uff08\u8054\u5408\u4f7f\u7528\u751f\u6210\u56fe\u50cf\u548c\u56fe\u50cf\u63d0\u793a\u7684\u952e\u503c\uff09\uff0c\u4ee5\u5e73\u8861\u903c\u771f\u5ea6\u4e0e\u5bf9\u9f50\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u65b0\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u56fe\u50cf\u63d0\u793a\u6a21\u578b\uff0c\u80fd\u66f4\u51c6\u786e\u5730\u53cd\u6620\u56fe\u50cf\u63d0\u793a\u3002", "conclusion": "\u901a\u8fc7\u51b2\u7a81\u81ea\u7531\u5f15\u5bfc\u548c\u5206\u5c42\u6ce8\u610f\u529b\uff0c\u65b0\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u50cf\u63d0\u793a\u751f\u6210\u4e2d\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u4e0e\u5bf9\u9f50\u6027\u3002"}}
{"id": "2508.02043", "pdf": "https://arxiv.org/pdf/2508.02043", "abs": "https://arxiv.org/abs/2508.02043", "authors": ["Hui Xie", "Haiqin Hu", "Lijuan Ding", "Qing Li", "Yue Sun", "Tao Tan"], "title": "Conditional Diffusion Model with Anatomical-Dose Dual Constraints for End-to-End Multi-Tumor Dose Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Radiotherapy treatment planning often relies on time-consuming, trial-and-error adjustments that heavily depend on the expertise of specialists, while existing deep learning methods face limitations in generalization, prediction accuracy, and clinical applicability. To tackle these challenges, we propose ADDiff-Dose, an Anatomical-Dose Dual Constraints Conditional Diffusion Model for end-to-end multi-tumor dose prediction. The model employs LightweightVAE3D to compress high-dimensional CT data and integrates multimodal inputs, including target and organ-at-risk (OAR) masks and beam parameters, within a progressive noise addition and denoising framework. It incorporates conditional features via a multi-head attention mechanism and utilizes a composite loss function combining MSE, conditional terms, and KL divergence to ensure both dosimetric accuracy and compliance with clinical constraints. Evaluation on a large-scale public dataset (2,877 cases) and three external institutional cohorts (450 cases in total) demonstrates that ADDiff-Dose significantly outperforms traditional baselines, achieving an MAE of 0.101-0.154 (compared to 0.316 for UNet and 0.169 for GAN models), a DICE coefficient of 0.927 (a 6.8% improvement), and limiting spinal cord maximum dose error to within 0.1 Gy. The average plan generation time per case is reduced to 22 seconds. Ablation studies confirm that the structural encoder enhances compliance with clinical dose constraints by 28.5%. To our knowledge, this is the first study to introduce a conditional diffusion model framework for radiotherapy dose prediction, offering a generalizable and efficient solution for automated treatment planning across diverse tumor sites, with the potential to substantially reduce planning time and improve clinical workflow efficiency.", "AI": {"tldr": "ADDiff-Dose\u662f\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u653e\u7597\u5242\u91cf\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8f93\u5165\u548c\u590d\u5408\u635f\u5931\u51fd\u6570\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u4e34\u5e8a\u9002\u7528\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u653e\u7597\u8ba1\u5212\u4f9d\u8d56\u4e13\u5bb6\u7ecf\u9a8c\u8017\u65f6\u4e14\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6cdb\u5316\u6027\u548c\u51c6\u786e\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528LightweightVAE3D\u538b\u7f29CT\u6570\u636e\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u8f93\u5165\u548c\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6ce8\u610f\u529b\u673a\u5236\u548c\u590d\u5408\u635f\u5931\u51fd\u6570\u4f18\u5316\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cMAE\u663e\u8457\u964d\u4f4e\uff0cDICE\u7cfb\u6570\u63d0\u53476.8%\uff0c\u8ba1\u5212\u751f\u6210\u65f6\u95f4\u7f29\u77ed\u81f322\u79d2\u3002", "conclusion": "ADDiff-Dose\u4e3a\u653e\u7597\u5242\u91cf\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u663e\u8457\u63d0\u5347\u4e34\u5e8a\u5de5\u4f5c\u6548\u7387\u3002"}}
{"id": "2508.02056", "pdf": "https://arxiv.org/pdf/2508.02056", "abs": "https://arxiv.org/abs/2508.02056", "authors": ["Haoxin Yang", "Weihong Chen", "Xuemiao Xu", "Cheng Xu", "Peng Xiao", "Cuifeng Sun", "Shaoyu Huang", "Shengfeng He"], "title": "StarPose: 3D Human Pose Estimation via Spatial-Temporal Autoregressive Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Monocular 3D human pose estimation remains a challenging task due to inherent depth ambiguities and occlusions. Compared to traditional methods based on Transformers or Convolutional Neural Networks (CNNs), recent diffusion-based approaches have shown superior performance, leveraging their probabilistic nature and high-fidelity generation capabilities. However, these methods often fail to account for the spatial and temporal correlations across predicted frames, resulting in limited temporal consistency and inferior accuracy in predicted 3D pose sequences. To address these shortcomings, this paper proposes StarPose, an autoregressive diffusion framework that effectively incorporates historical 3D pose predictions and spatial-temporal physical guidance to significantly enhance both the accuracy and temporal coherence of pose predictions. Unlike existing approaches, StarPose models the 2D-to-3D pose mapping as an autoregressive diffusion process. By synergically integrating previously predicted 3D poses with 2D pose inputs via a Historical Pose Integration Module (HPIM), the framework generates rich and informative historical pose embeddings that guide subsequent denoising steps, ensuring temporally consistent predictions. In addition, a fully plug-and-play Spatial-Temporal Physical Guidance (STPG) mechanism is tailored to refine the denoising process in an iterative manner, which further enforces spatial anatomical plausibility and temporal motion dynamics, rendering robust and realistic pose estimates. Extensive experiments on benchmark datasets demonstrate that StarPose outperforms state-of-the-art methods, achieving superior accuracy and temporal consistency in 3D human pose estimation. Code is available at https://github.com/wileychan/StarPose.", "AI": {"tldr": "StarPose\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u56de\u5f52\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5386\u53f23D\u59ff\u6001\u9884\u6d4b\u548c\u65f6\u7a7a\u7269\u7406\u6307\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u76ee3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6269\u6563\u65b9\u6cd5\u5728\u65f6\u7a7a\u76f8\u5173\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u63d0\u53473D\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "method": "\u91c7\u7528\u81ea\u56de\u5f52\u6269\u6563\u8fc7\u7a0b\uff0c\u7ed3\u5408\u5386\u53f2\u59ff\u6001\u96c6\u6210\u6a21\u5757\uff08HPIM\uff09\u548c\u65f6\u7a7a\u7269\u7406\u6307\u5bfc\uff08STPG\uff09\u673a\u5236\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "conclusion": "StarPose\u901a\u8fc7\u81ea\u56de\u5f52\u6269\u6563\u6846\u67b6\u548c\u65f6\u7a7a\u7269\u7406\u6307\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u6027\u80fd\u3002"}}
{"id": "2508.02107", "pdf": "https://arxiv.org/pdf/2508.02107", "abs": "https://arxiv.org/abs/2508.02107", "authors": ["Zhiwen Li", "Zhongjie Duan", "Die Chen", "Cen Chen", "Daoyuan Chen", "Yaliang Li", "Yingda Chen"], "title": "AutoLoRA: Automatic LoRA Retrieval and Fine-Grained Gated Fusion for Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent advances in photorealistic image generation through large-scale models like FLUX and Stable Diffusion v3, the practical deployment of these architectures remains constrained by their inherent intractability to parameter fine-tuning. While low-rank adaptation (LoRA) have demonstrated efficacy in enabling model customization with minimal parameter overhead, the effective utilization of distributed open-source LoRA modules faces three critical challenges: sparse metadata annotation, the requirement for zero-shot adaptation capabilities, and suboptimal fusion strategies for multi-LoRA fusion strategies. To address these limitations, we introduce a novel framework that enables semantic-driven LoRA retrieval and dynamic aggregation through two key components: (1) weight encoding-base LoRA retriever that establishes a shared semantic space between LoRA parameter matrices and text prompts, eliminating dependence on original training data, and (2) fine-grained gated fusion mechanism that computes context-specific fusion weights across network layers and diffusion timesteps to optimally integrate multiple LoRA modules during generation. Our approach achieves significant improvement in image generation perfermance, thereby facilitating scalable and data-efficient enhancement of foundational models. This work establishes a critical bridge between the fragmented landscape of community-developed LoRAs and practical deployment requirements, enabling collaborative model evolution through standardized adapter integration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u9a71\u52a8\u7684LoRA\u68c0\u7d22\u548c\u52a8\u6001\u805a\u5408\uff0c\u89e3\u51b3\u4e86\u5206\u5e03\u5f0f\u5f00\u6e90LoRA\u6a21\u5757\u7684\u4e09\u5927\u6311\u6218\uff1a\u7a00\u758f\u5143\u6570\u636e\u6807\u6ce8\u3001\u96f6\u6837\u672c\u9002\u5e94\u9700\u6c42\u548c\u591aLoRA\u878d\u5408\u7b56\u7565\u4e0d\u4f73\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u5927\u89c4\u6a21\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5176\u53c2\u6570\u5fae\u8c03\u7684\u4e0d\u53ef\u884c\u6027\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002LoRA\u867d\u80fd\u4f4e\u6210\u672c\u5b9a\u5236\u6a21\u578b\uff0c\u4f46\u9762\u4e34\u5143\u6570\u636e\u7a00\u758f\u3001\u96f6\u6837\u672c\u9002\u5e94\u548c\u591aLoRA\u878d\u5408\u7b49\u95ee\u9898\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e24\u90e8\u5206\uff1a(1)\u57fa\u4e8e\u6743\u91cd\u7f16\u7801\u7684LoRA\u68c0\u7d22\u5668\uff0c\u5efa\u7acbLoRA\u53c2\u6570\u77e9\u9635\u4e0e\u6587\u672c\u63d0\u793a\u7684\u5171\u4eab\u8bed\u4e49\u7a7a\u95f4\uff1b(2)\u7ec6\u7c92\u5ea6\u95e8\u63a7\u878d\u5408\u673a\u5236\uff0c\u52a8\u6001\u8ba1\u7b97\u878d\u5408\u6743\u91cd\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u751f\u6210\u6027\u80fd\uff0c\u652f\u6301\u57fa\u7840\u6a21\u578b\u7684\u9ad8\u6548\u6269\u5c55\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u793e\u533a\u5f00\u53d1\u7684LoRA\u4e0e\u5b9e\u9645\u90e8\u7f72\u9700\u6c42\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u9002\u914d\u5668\u96c6\u6210\u63a8\u52a8\u534f\u4f5c\u6a21\u578b\u6f14\u8fdb\u3002"}}
{"id": "2508.02129", "pdf": "https://arxiv.org/pdf/2508.02129", "abs": "https://arxiv.org/abs/2508.02129", "authors": ["Yuru Xiao", "Zihan Lin", "Chao Lu", "Deming Zhai", "Kui Jiang", "Wenbo Zhao", "Wei Zhang", "Junjun Jiang", "Huanran Wang", "Xianming Liu"], "title": "VDEGaussian: Video Diffusion Enhanced 4D Gaussian Splatting for Dynamic Urban Scenes Modeling", "categories": ["cs.CV"], "comment": null, "summary": "Dynamic urban scene modeling is a rapidly evolving area with broad applications. While current approaches leveraging neural radiance fields or Gaussian Splatting have achieved fine-grained reconstruction and high-fidelity novel view synthesis, they still face significant limitations. These often stem from a dependence on pre-calibrated object tracks or difficulties in accurately modeling fast-moving objects from undersampled capture, particularly due to challenges in handling temporal discontinuities. To overcome these issues, we propose a novel video diffusion-enhanced 4D Gaussian Splatting framework. Our key insight is to distill robust, temporally consistent priors from a test-time adapted video diffusion model. To ensure precise pose alignment and effective integration of this denoised content, we introduce two core innovations: a joint timestamp optimization strategy that refines interpolated frame poses, and an uncertainty distillation method that adaptively extracts target content while preserving well-reconstructed regions. Extensive experiments demonstrate that our method significantly enhances dynamic modeling, especially for fast-moving objects, achieving an approximate PSNR gain of 2 dB for novel view synthesis over baseline approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u6269\u6563\u589e\u5f3a\u76844D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u573a\u666f\u5efa\u6a21\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5feb\u901f\u8fd0\u52a8\u7269\u4f53\u5efa\u6a21\u548c\u65f6\u5e8f\u8fde\u7eed\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u6821\u51c6\u7269\u4f53\u8f68\u8ff9\u6216\u96be\u4ee5\u5904\u7406\u6b20\u91c7\u6837\u6355\u83b7\u7684\u5feb\u901f\u8fd0\u52a8\u7269\u4f53\uff0c\u5bfc\u81f4\u5efa\u6a21\u4e0d\u51c6\u786e\u3002", "method": "\u7ed3\u5408\u89c6\u9891\u6269\u6563\u6a21\u578b\u63d0\u53d6\u65f6\u5e8f\u4e00\u81f4\u5148\u9a8c\uff0c\u5f15\u5165\u8054\u5408\u65f6\u95f4\u6233\u4f18\u5316\u548c\u4e0d\u786e\u5b9a\u6027\u84b8\u998f\u65b9\u6cd5\uff0c\u63d0\u5347\u52a8\u6001\u5efa\u6a21\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u5efa\u6a21\u6548\u679c\uff0c\u5c24\u5176\u662f\u5feb\u901f\u8fd0\u52a8\u7269\u4f53\uff0c\u65b0\u89c6\u89d2\u5408\u6210\u7684PSNR\u589e\u76ca\u7ea62 dB\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u573a\u666f\u5efa\u6a21\u4e2d\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u5feb\u901f\u8fd0\u52a8\u7269\u4f53\u5efa\u6a21\u95ee\u9898\u3002"}}
{"id": "2508.02155", "pdf": "https://arxiv.org/pdf/2508.02155", "abs": "https://arxiv.org/abs/2508.02155", "authors": ["Sijie Zhao", "Jing Cheng", "Yaoyao Wu", "Hao Xu", "Shaohui Jiao"], "title": "DreamPainter: Image Background Inpainting for E-commerce Scenarios", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Although diffusion-based image genenation has been widely explored and applied, background generation tasks in e-commerce scenarios still face significant challenges. The first challenge is to ensure that the generated products are consistent with the given product inputs while maintaining a reasonable spatial arrangement, harmonious shadows, and reflections between foreground products and backgrounds. Existing inpainting methods fail to address this due to the lack of domain-specific data. The second challenge involves the limitation of relying solely on text prompts for image control, as effective integrating visual information to achieve precise control in inpainting tasks remains underexplored. To address these challenges, we introduce DreamEcom-400K, a high-quality e-commerce dataset containing accurate product instance masks, background reference images, text prompts, and aesthetically pleasing product images. Based on this dataset, we propose DreamPainter, a novel framework that not only utilizes text prompts for control but also flexibly incorporates reference image information as an additional control signal. Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art methods, maintaining high product consistency while effectively integrating both text prompt and reference image information.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86DreamEcom-400K\u6570\u636e\u96c6\u548cDreamPainter\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u7535\u5546\u573a\u666f\u4e2d\u80cc\u666f\u751f\u6210\u4efb\u52a1\u7684\u4e24\u5927\u6311\u6218\uff1a\u4ea7\u54c1\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u4fe1\u606f\u6574\u5408\u3002", "motivation": "\u7535\u5546\u80cc\u666f\u751f\u6210\u4efb\u52a1\u9762\u4e34\u4ea7\u54c1\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u4fe1\u606f\u6574\u5408\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u9886\u57df\u6570\u636e\u548c\u4f9d\u8d56\u5355\u4e00\u6587\u672c\u63d0\u793a\u800c\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u57fa\u4e8e\u9ad8\u8d28\u91cf\u6570\u636e\u96c6DreamEcom-400K\uff0c\u63d0\u51faDreamPainter\u6846\u67b6\uff0c\u7ed3\u5408\u6587\u672c\u63d0\u793a\u548c\u53c2\u8003\u56fe\u50cf\u4fe1\u606f\u8fdb\u884c\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u4ea7\u54c1\u4e00\u81f4\u6027\u548c\u6574\u5408\u89c6\u89c9\u4fe1\u606f\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "DreamPainter\u901a\u8fc7\u591a\u6a21\u6001\u63a7\u5236\u4fe1\u53f7\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7535\u5546\u80cc\u666f\u751f\u6210\u7684\u6838\u5fc3\u95ee\u9898\u3002"}}
{"id": "2508.02165", "pdf": "https://arxiv.org/pdf/2508.02165", "abs": "https://arxiv.org/abs/2508.02165", "authors": ["Jia-Chen Zhang", "Yu-Jie Xiong"], "title": "Subject or Style: Adaptive and Training-Free Mixture of LoRAs", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Fine-tuning models via Low-Rank Adaptation (LoRA) demonstrates remarkable performance in subject-driven or style-driven generation tasks. Studies have explored combinations of different LoRAs to jointly generate learned styles and content. However, current methods struggle to balance the original subject and style, and often require additional training. Recently, K-LoRA proposed a training-free LoRA fusion method. But it involves multiple hyperparameters, making it difficult to adapt to all styles and subjects. In this paper, we propose EST-LoRA, a training-free adaptive LoRA fusion method. It comprehensively considers three critical factors: \\underline{E}nergy of matrix, \\underline{S}tyle discrepancy scores and \\underline{T}ime steps. Analogous to the Mixture of Experts (MoE) architecture, the model adaptively selects between subject LoRA and style LoRA within each attention layer. This integrated selection mechanism ensures balanced contributions from both components during the generation process. Experimental results show that EST-LoRA outperforms state-of-the-art methods in both qualitative and quantitative evaluations and achieves faster generation speed compared to other efficient fusion approaches. Our code is publicly available at: https://anonymous.4open.science/r/EST-LoRA-F318.", "AI": {"tldr": "EST-LoRA\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684LoRA\u878d\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u7efc\u5408\u8003\u8651\u77e9\u9635\u80fd\u91cf\u3001\u98ce\u683c\u5dee\u5f02\u5206\u6570\u548c\u65f6\u95f4\u6b65\u957f\uff0c\u81ea\u9002\u5e94\u9009\u62e9\u4e3b\u9898\u548c\u98ce\u683cLoRA\uff0c\u5e73\u8861\u751f\u6210\u6548\u679c\u3002", "motivation": "\u73b0\u6709LoRA\u878d\u5408\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u4e3b\u9898\u548c\u98ce\u683c\uff0c\u4e14\u9700\u8981\u989d\u5916\u8bad\u7ec3\u6216\u6d89\u53ca\u590d\u6742\u8d85\u53c2\u6570\u3002", "method": "EST-LoRA\u91c7\u7528\u7c7b\u4f3cMoE\u7684\u67b6\u6784\uff0c\u5728\u6ce8\u610f\u529b\u5c42\u81ea\u9002\u5e94\u9009\u62e9\u4e3b\u9898\u6216\u98ce\u683cLoRA\uff0c\u7ed3\u5408\u77e9\u9635\u80fd\u91cf\u3001\u98ce\u683c\u5dee\u5f02\u548c\u65f6\u95f4\u6b65\u957f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEST-LoRA\u5728\u8d28\u91cf\u548c\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "EST-LoRA\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684LoRA\u878d\u5408\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6548\u679c\u3002"}}
{"id": "2508.02172", "pdf": "https://arxiv.org/pdf/2508.02172", "abs": "https://arxiv.org/abs/2508.02172", "authors": ["Lei Yao", "Yi Wang", "Yi Zhang", "Moyun Liu", "Lap-Pui Chau"], "title": "GaussianCross: Cross-modal Self-supervised 3D Representation Learning via Gaussian Splatting", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "14 pages, 8 figures, accepted by MM'25", "summary": "The significance of informative and robust point representations has been widely acknowledged for 3D scene understanding. Despite existing self-supervised pre-training counterparts demonstrating promising performance, the model collapse and structural information deficiency remain prevalent due to insufficient point discrimination difficulty, yielding unreliable expressions and suboptimal performance. In this paper, we present GaussianCross, a novel cross-modal self-supervised 3D representation learning architecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniques to address current challenges. GaussianCross seamlessly converts scale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussian representation without missing details, enabling stable and generalizable pre-training. Subsequently, a tri-attribute adaptive distillation splatting module is incorporated to construct a 3D feature field, facilitating synergetic feature capturing of appearance, geometry, and semantic cues to maintain cross-modal consistency. To validate GaussianCross, we perform extensive evaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. In particular, GaussianCross shows a prominent parameter and data efficiency, achieving superior performance through linear probing (<0.1% parameters) and limited data training (1% of scenes) compared to state-of-the-art methods. Furthermore, GaussianCross demonstrates strong generalization capabilities, improving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ on ScanNet200 semantic and instance segmentation tasks, respectively, supporting the effectiveness of our approach. The code, weights, and visualizations are publicly available at \\href{https://rayyoh.github.io/GaussianCross/}{https://rayyoh.github.io/GaussianCross/}.", "AI": {"tldr": "GaussianCross\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8de8\u6a21\u6001\u81ea\u76d1\u77633D\u8868\u793a\u5b66\u4e60\u67b6\u6784\uff0c\u901a\u8fc73D\u9ad8\u65af\u55b7\u6d12\u6280\u672f\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u6a21\u578b\u5d29\u6e83\u548c\u7ed3\u6784\u4fe1\u606f\u4e0d\u8db3\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\u57283D\u573a\u666f\u7406\u89e3\u4e2d\u5b58\u5728\u6a21\u578b\u5d29\u6e83\u548c\u7ed3\u6784\u4fe1\u606f\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u8868\u8fbe\u4e0d\u53ef\u9760\u548c\u6027\u80fd\u4e0d\u4f73\u3002", "method": "GaussianCross\u901a\u8fc7\u5c06\u70b9\u4e91\u8f6c\u6362\u4e3a\u7edf\u4e00\u7684\u7acb\u65b9\u4f53\u5f52\u4e00\u5316\u9ad8\u65af\u8868\u793a\uff0c\u5e76\u7ed3\u5408\u4e09\u5c5e\u6027\u81ea\u9002\u5e94\u84b8\u998f\u55b7\u6d12\u6a21\u5757\uff0c\u6784\u5efa3D\u7279\u5f81\u573a\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u53c2\u6570\u548c\u6570\u636e\u6548\u7387\u9ad8\uff0c\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u4e49\u548c\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "GaussianCross\u901a\u8fc7\u521b\u65b0\u7684\u8de8\u6a21\u6001\u5b66\u4e60\u67b6\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u95ee\u9898\uff0c\u5e76\u5728\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2508.02258", "pdf": "https://arxiv.org/pdf/2508.02258", "abs": "https://arxiv.org/abs/2508.02258", "authors": ["Wenchuan Zhang", "Jingru Guo", "Hengzhe Zhang", "Penghao Zhang", "Jie Chen", "Shuwan Zhang", "Zhang Zhang", "Yuhao Yi", "Hong Bu"], "title": "Patho-AgenticRAG: Towards Multimodal Agentic Retrieval-Augmented Generation for Pathology VLMs via Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Although Vision Language Models (VLMs) have shown strong generalization in medical imaging, pathology presents unique challenges due to ultra-high resolution, complex tissue structures, and nuanced clinical semantics. These factors make pathology VLMs prone to hallucinations, i.e., generating outputs inconsistent with visual evidence, which undermines clinical trust. Existing RAG approaches in this domain largely depend on text-based knowledge bases, limiting their ability to leverage diagnostic visual cues. To address this, we propose Patho-AgenticRAG, a multimodal RAG framework with a database built on page-level embeddings from authoritative pathology textbooks. Unlike traditional text-only retrieval systems, it supports joint text-image search, enabling direct retrieval of textbook pages that contain both the queried text and relevant visual cues, thus avoiding the loss of critical image-based information. Patho-AgenticRAG also supports reasoning, task decomposition, and multi-turn search interactions, improving accuracy in complex diagnostic scenarios. Experiments show that Patho-AgenticRAG significantly outperforms existing multimodal models in complex pathology tasks like multiple-choice diagnosis and visual question answering. Our project is available at the Patho-AgenticRAG repository: https://github.com/Wenchuan-Zhang/Patho-AgenticRAG.", "AI": {"tldr": "Patho-AgenticRAG\u662f\u4e00\u79cd\u591a\u6a21\u6001RAG\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u548c\u56fe\u50cf\u68c0\u7d22\u89e3\u51b3\u75c5\u7406\u5b66\u4e2d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u8bca\u65ad\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u75c5\u7406\u5b66\u4e2d\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u56e0\u8d85\u9ad8\u5206\u8fa8\u7387\u3001\u590d\u6742\u7ec4\u7ec7\u7ed3\u6784\u548c\u4e34\u5e8a\u8bed\u4e49\u7684\u7ec6\u5fae\u5dee\u5f02\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u5f71\u54cd\u4e34\u5e8a\u4fe1\u4efb\u3002\u73b0\u6709RAG\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u77e5\u8bc6\u5e93\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u89c6\u89c9\u4fe1\u606f\u3002", "method": "\u63d0\u51faPatho-AgenticRAG\uff0c\u57fa\u4e8e\u6743\u5a01\u75c5\u7406\u5b66\u6559\u79d1\u4e66\u6784\u5efa\u9875\u9762\u7ea7\u5d4c\u5165\u6570\u636e\u5e93\uff0c\u652f\u6301\u8054\u5408\u6587\u672c-\u56fe\u50cf\u68c0\u7d22\uff0c\u907f\u514d\u5173\u952e\u56fe\u50cf\u4fe1\u606f\u4e22\u5931\uff0c\u5e76\u652f\u6301\u63a8\u7406\u3001\u4efb\u52a1\u5206\u89e3\u548c\u591a\u8f6e\u641c\u7d22\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPatho-AgenticRAG\u5728\u591a\u9009\u8bca\u65ad\u548c\u89c6\u89c9\u95ee\u7b54\u7b49\u590d\u6742\u75c5\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u591a\u6a21\u6001\u6a21\u578b\u3002", "conclusion": "Patho-AgenticRAG\u901a\u8fc7\u591a\u6a21\u6001\u68c0\u7d22\u548c\u63a8\u7406\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u75c5\u7406\u5b66\u4e2d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u3002"}}
{"id": "2508.02261", "pdf": "https://arxiv.org/pdf/2508.02261", "abs": "https://arxiv.org/abs/2508.02261", "authors": ["Rui Qian", "Haozhi Cao", "Tianchen Deng", "Shenghai Yuan", "Lihua Xie"], "title": "SplatSSC: Decoupled Depth-Guided Gaussian Splatting for Semantic Scene Completion", "categories": ["cs.CV"], "comment": null, "summary": "Monocular 3D Semantic Scene Completion (SSC) is a challenging yet promising task that aims to infer dense geometric and semantic descriptions of a scene from a single image. While recent object-centric paradigms significantly improve efficiency by leveraging flexible 3D Gaussian primitives, they still rely heavily on a large number of randomly initialized primitives, which inevitably leads to 1) inefficient primitive initialization and 2) outlier primitives that introduce erroneous artifacts. In this paper, we propose SplatSSC, a novel framework that resolves these limitations with a depth-guided initialization strategy and a principled Gaussian aggregator. Instead of random initialization, SplatSSC utilizes a dedicated depth branch composed of a Group-wise Multi-scale Fusion (GMF) module, which integrates multi-scale image and depth features to generate a sparse yet representative set of initial Gaussian primitives. To mitigate noise from outlier primitives, we develop the Decoupled Gaussian Aggregator (DGA), which enhances robustness by decomposing geometric and semantic predictions during the Gaussian-to-voxel splatting process. Complemented with a specialized Probability Scale Loss, our method achieves state-of-the-art performance on the Occ-ScanNet dataset, outperforming prior approaches by over 6.3% in IoU and 4.1% in mIoU, while reducing both latency and memory consumption by more than 9.3%. The code will be released upon acceptance.", "AI": {"tldr": "SplatSSC\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f15\u5bfc\u521d\u59cb\u5316\u548c\u89e3\u8026\u9ad8\u65af\u805a\u5408\u7684\u5355\u76ee3D\u8bed\u4e49\u573a\u666f\u8865\u5168\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e3D\u9ad8\u65af\u57fa\u5143\u7684\u65b9\u6cd5\u4f9d\u8d56\u968f\u673a\u521d\u59cb\u5316\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u5f02\u5e38\u57fa\u5143\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5f15\u5bfc\u7684\u521d\u59cb\u5316\u7b56\u7565\uff08GMF\u6a21\u5757\uff09\u548c\u89e3\u8026\u9ad8\u65af\u805a\u5408\u5668\uff08DGA\uff09\uff0c\u7ed3\u5408\u6982\u7387\u5c3a\u5ea6\u635f\u5931\u3002", "result": "\u5728Occ-ScanNet\u6570\u636e\u96c6\u4e0a\uff0cIoU\u548cmIoU\u5206\u522b\u63d0\u53476.3%\u548c4.1%\uff0c\u5ef6\u8fdf\u548c\u5185\u5b58\u6d88\u8017\u964d\u4f4e9.3%\u3002", "conclusion": "SplatSSC\u901a\u8fc7\u4f18\u5316\u521d\u59cb\u5316\u548c\u805a\u5408\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u76843D\u8bed\u4e49\u573a\u666f\u8865\u5168\u3002"}}
{"id": "2508.02324", "pdf": "https://arxiv.org/pdf/2508.02324", "abs": "https://arxiv.org/abs/2508.02324", "authors": ["Chenfei Wu", "Jiahao Li", "Jingren Zhou", "Junyang Lin", "Kaiyuan Gao", "Kun Yan", "Sheng-ming Yin", "Shuai Bai", "Xiao Xu", "Yilei Chen", "Yuxiang Chen", "Zecheng Tang", "Zekai Zhang", "Zhengyi Wang", "An Yang", "Bowen Yu", "Chen Cheng", "Dayiheng Liu", "Deqing Li", "Hang Zhang", "Hao Meng", "Hu Wei", "Jingyuan Ni", "Kai Chen", "Kuan Cao", "Liang Peng", "Lin Qu", "Minggang Wu", "Peng Wang", "Shuting Yu", "Tingkun Wen", "Wensen Feng", "Xiaoxiao Xu", "Yi Wang", "Yichang Zhang", "Yongqiang Zhu", "Yujia Wu", "Yuxuan Cai", "Zenan Liu"], "title": "Qwen-Image Technical Report", "categories": ["cs.CV"], "comment": "https://github.com/QwenLM/Qwen-Image", "summary": "We present Qwen-Image, an image generation foundation model in the Qwen series that achieves significant advances in complex text rendering and precise image editing. To address the challenges of complex text rendering, we design a comprehensive data pipeline that includes large-scale data collection, filtering, annotation, synthesis, and balancing. Moreover, we adopt a progressive training strategy that starts with non-text-to-text rendering, evolves from simple to complex textual inputs, and gradually scales up to paragraph-level descriptions. This curriculum learning approach substantially enhances the model's native text rendering capabilities. As a result, Qwen-Image not only performs exceptionally well in alphabetic languages such as English, but also achieves remarkable progress on more challenging logographic languages like Chinese. To enhance image editing consistency, we introduce an improved multi-task training paradigm that incorporates not only traditional text-to-image (T2I) and text-image-to-image (TI2I) tasks but also image-to-image (I2I) reconstruction, effectively aligning the latent representations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed the original image into Qwen2.5-VL and the VAE encoder to obtain semantic and reconstructive representations, respectively. This dual-encoding mechanism enables the editing module to strike a balance between preserving semantic consistency and maintaining visual fidelity. Qwen-Image achieves state-of-the-art performance, demonstrating its strong capabilities in both image generation and editing across multiple benchmarks.", "AI": {"tldr": "Qwen-Image\u662f\u4e00\u4e2a\u56fe\u50cf\u751f\u6210\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u590d\u6742\u6587\u672c\u6e32\u67d3\u548c\u7cbe\u786e\u56fe\u50cf\u7f16\u8f91\u7684\u6539\u8fdb\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u8fdb\u5c55\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u6587\u672c\u6e32\u67d3\u548c\u56fe\u50cf\u7f16\u8f91\u4e00\u81f4\u6027\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u5168\u9762\u7684\u6570\u636e\u7ba1\u9053\u548c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u591a\u4efb\u52a1\u8bad\u7ec3\u8303\u5f0f\u53ca\u53cc\u7f16\u7801\u673a\u5236\u3002", "result": "\u5728\u5b57\u6bcd\u8bed\u8a00\uff08\u5982\u82f1\u8bed\uff09\u548c\u8868\u610f\u6587\u5b57\uff08\u5982\u4e2d\u6587\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u80fd\u529b\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "Qwen-Image\u5728\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8bed\u8a00\u548c\u4efb\u52a1\u3002"}}
{"id": "2508.02493", "pdf": "https://arxiv.org/pdf/2508.02493", "abs": "https://arxiv.org/abs/2508.02493", "authors": ["Jianchao Wang", "Peng Zhou", "Cen Li", "Rong Quan", "Jie Qin"], "title": "Low-Frequency First: Eliminating Floating Artifacts in 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is a powerful and computationally efficient representation for 3D reconstruction. Despite its strengths, 3DGS often produces floating artifacts, which are erroneous structures detached from the actual geometry and significantly degrade visual fidelity. The underlying mechanisms causing these artifacts, particularly in low-quality initialization scenarios, have not been fully explored. In this paper, we investigate the origins of floating artifacts from a frequency-domain perspective and identify under-optimized Gaussians as the primary source. Based on our analysis, we propose \\textit{Eliminating-Floating-Artifacts} Gaussian Splatting (EFA-GS), which selectively expands under-optimized Gaussians to prioritize accurate low-frequency learning. Additionally, we introduce complementary depth-based and scale-based strategies to dynamically refine Gaussian expansion, effectively mitigating detail erosion. Extensive experiments on both synthetic and real-world datasets demonstrate that EFA-GS substantially reduces floating artifacts while preserving high-frequency details, achieving an improvement of 1.68 dB in PSNR over baseline method on our RWLQ dataset. Furthermore, we validate the effectiveness of our approach in downstream 3D editing tasks. Our implementation will be released on GitHub.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEFA-GS\u65b9\u6cd5\uff0c\u901a\u8fc7\u9891\u7387\u57df\u5206\u6790\u89e3\u51b33D\u9ad8\u65af\u6cfc\u6e85\u4e2d\u7684\u6d6e\u52a8\u4f2a\u5f71\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u89c6\u89c9\u8d28\u91cf\u548cPSNR\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u5728\u4f4e\u8d28\u91cf\u521d\u59cb\u5316\u65f6\u4f1a\u4ea7\u751f\u6d6e\u52a8\u4f2a\u5f71\uff0c\u5f71\u54cd\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u4f46\u5176\u6210\u56e0\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u4ece\u9891\u7387\u57df\u89d2\u5ea6\u5206\u6790\u4f2a\u5f71\u6210\u56e0\uff0c\u63d0\u51faEFA-GS\u65b9\u6cd5\uff0c\u9009\u62e9\u6027\u6269\u5c55\u672a\u4f18\u5316\u9ad8\u65af\u4ee5\u4f18\u5148\u5b66\u4e60\u4f4e\u9891\u4fe1\u606f\uff0c\u5e76\u5f15\u5165\u6df1\u5ea6\u548c\u5c3a\u5ea6\u7b56\u7565\u52a8\u6001\u4f18\u5316\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cEFA-GS\u663e\u8457\u51cf\u5c11\u4f2a\u5f71\u5e76\u4fdd\u7559\u9ad8\u9891\u7ec6\u8282\uff0cPSNR\u63d0\u53471.68 dB\u3002", "conclusion": "EFA-GS\u6709\u6548\u89e3\u51b3\u6d6e\u52a8\u4f2a\u5f71\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u4e0b\u6e383D\u7f16\u8f91\u4efb\u52a1\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2508.02605", "pdf": "https://arxiv.org/pdf/2508.02605", "abs": "https://arxiv.org/abs/2508.02605", "authors": ["Zhengdao Li", "Siheng Wang", "Zeyu Zhang", "Hao Tang"], "title": "ReMoMask: Retrieval-Augmented Masked Motion Generation", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-Motion (T2M) generation aims to synthesize realistic and semantically aligned human motion sequences from natural language descriptions. However, current approaches face dual challenges: Generative models (e.g., diffusion models) suffer from limited diversity, error accumulation, and physical implausibility, while Retrieval-Augmented Generation (RAG) methods exhibit diffusion inertia, partial-mode collapse, and asynchronous artifacts. To address these limitations, we propose ReMoMask, a unified framework integrating three key innovations: 1) A Bidirectional Momentum Text-Motion Model decouples negative sample scale from batch size via momentum queues, substantially improving cross-modal retrieval precision; 2) A Semantic Spatio-temporal Attention mechanism enforces biomechanical constraints during part-level fusion to eliminate asynchronous artifacts; 3) RAG-Classier-Free Guidance incorporates minor unconditional generation to enhance generalization. Built upon MoMask's RVQ-VAE, ReMoMask efficiently generates temporally coherent motions in minimal steps. Extensive experiments on standard benchmarks demonstrate the state-of-the-art performance of ReMoMask, achieving a 3.88% and 10.97% improvement in FID scores on HumanML3D and KIT-ML, respectively, compared to the previous SOTA method RAG-T2M. Code: https://github.com/AIGeeksGroup/ReMoMask. Website: https://aigeeksgroup.github.io/ReMoMask.", "AI": {"tldr": "ReMoMask\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5411\u52a8\u91cf\u6587\u672c-\u8fd0\u52a8\u6a21\u578b\u3001\u8bed\u4e49\u65f6\u7a7a\u6ce8\u610f\u529b\u548cRAG-\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u4e09\u9879\u521b\u65b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u591a\u6837\u6027\u4e0d\u8db3\u3001\u7269\u7406\u4e0d\u5408\u7406\u6027\u548c\u5f02\u6b65\u4f2a\u5f71\u7b49\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u591a\u6837\u6027\u4e0d\u8db3\u3001\u7269\u7406\u4e0d\u5408\u7406\u6027\u548c\u5f02\u6b65\u4f2a\u5f71\u7b49\u95ee\u9898\uff0cReMoMask\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "1) \u53cc\u5411\u52a8\u91cf\u6587\u672c-\u8fd0\u52a8\u6a21\u578b\uff1b2) \u8bed\u4e49\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\uff1b3) RAG-\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u3002", "result": "\u5728HumanML3D\u548cKIT-ML\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFID\u5206\u6570\u5206\u522b\u63d0\u53473.88%\u548c10.97%\u3002", "conclusion": "ReMoMask\u5728\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.02660", "pdf": "https://arxiv.org/pdf/2508.02660", "abs": "https://arxiv.org/abs/2508.02660", "authors": ["Yijun Xu", "Jingrui Zhang", "Yuhan Chen", "Dingwen Wang", "Lei Yu", "Chu He"], "title": "PMGS: Reconstruction of Projectile Motion across Large Spatiotemporal Spans via 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Modeling complex rigid motion across large spatiotemporal spans remains an unresolved challenge in dynamic reconstruction. Existing paradigms are mainly confined to short-term, small-scale deformation and offer limited consideration for physical consistency. This study proposes PMGS, focusing on reconstructing Projectile Motion via 3D Gaussian Splatting. The workflow comprises two stages: 1) Target Modeling: achieving object-centralized reconstruction through dynamic scene decomposition and an improved point density control; 2) Motion Recovery: restoring full motion sequences by learning per-frame SE(3) poses. We introduce an acceleration consistency constraint to bridge Newtonian mechanics and pose estimation, and design a dynamic simulated annealing strategy that adaptively schedules learning rates based on motion states. Futhermore, we devise a Kalman fusion scheme to optimize error accumulation from multi-source observations to mitigate disturbances. Experiments show PMGS's superior performance in reconstructing high-speed nonlinear rigid motion compared to mainstream dynamic methods.", "AI": {"tldr": "PMGS\u901a\u8fc73D\u9ad8\u65af\u6e85\u5c04\u91cd\u5efa\u629b\u4f53\u8fd0\u52a8\uff0c\u63d0\u51fa\u4e24\u9636\u6bb5\u5de5\u4f5c\u6d41\u7a0b\uff08\u76ee\u6807\u5efa\u6a21\u548c\u8fd0\u52a8\u6062\u590d\uff09\uff0c\u7ed3\u5408\u7269\u7406\u4e00\u81f4\u6027\u7ea6\u675f\u548c\u52a8\u6001\u6a21\u62df\u9000\u706b\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u901f\u975e\u7ebf\u6027\u521a\u4f53\u8fd0\u52a8\u7684\u91cd\u5efa\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u52a8\u6001\u91cd\u5efa\u65b9\u6cd5\u5728\u957f\u65f6\u95f4\u3001\u5927\u7a7a\u95f4\u5c3a\u5ea6\u4e0b\u521a\u6027\u8fd0\u52a8\u5efa\u6a21\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u7269\u7406\u4e00\u81f4\u6027\u7684\u4e0d\u8db3\u3002", "method": "1) \u76ee\u6807\u5efa\u6a21\uff1a\u901a\u8fc7\u52a8\u6001\u573a\u666f\u5206\u89e3\u548c\u6539\u8fdb\u7684\u70b9\u5bc6\u5ea6\u63a7\u5236\u5b9e\u73b0\u5bf9\u8c61\u96c6\u4e2d\u91cd\u5efa\uff1b2) \u8fd0\u52a8\u6062\u590d\uff1a\u901a\u8fc7\u6bcf\u5e27SE(3)\u4f4d\u59ff\u5b66\u4e60\u6062\u590d\u5b8c\u6574\u8fd0\u52a8\u5e8f\u5217\uff0c\u5f15\u5165\u52a0\u901f\u5ea6\u4e00\u81f4\u6027\u7ea6\u675f\u548c\u52a8\u6001\u6a21\u62df\u9000\u706b\u7b56\u7565\u3002", "result": "PMGS\u5728\u9ad8\u901f\u975e\u7ebf\u6027\u521a\u4f53\u8fd0\u52a8\u91cd\u5efa\u4e2d\u8868\u73b0\u4f18\u4e8e\u4e3b\u6d41\u52a8\u6001\u65b9\u6cd5\u3002", "conclusion": "PMGS\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u7ea6\u675f\u548c\u81ea\u9002\u5e94\u5b66\u4e60\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u521a\u6027\u8fd0\u52a8\u5efa\u6a21\u7684\u6311\u6218\u3002"}}
{"id": "2508.01230", "pdf": "https://arxiv.org/pdf/2508.01230", "abs": "https://arxiv.org/abs/2508.01230", "authors": ["Jiyong Kim", "Sunwoong Yang", "Namwoo Kang"], "title": "Point-wise Diffusion Models for Physical Systems with Shape Variations: Application to Spatio-temporal and Large-scale system", "categories": ["physics.comp-ph", "cs.CV"], "comment": null, "summary": "This study introduces a novel point-wise diffusion model that processes spatio-temporal points independently to efficiently predict complex physical systems with shape variations. This methodological contribution lies in applying forward and backward diffusion processes at individual spatio-temporal points, coupled with a point-wise diffusion transformer architecture for denoising. Unlike conventional image-based diffusion models that operate on structured data representations, this framework enables direct processing of any data formats including meshes and point clouds while preserving geometric fidelity. We validate our approach across three distinct physical domains with complex geometric configurations: 2D spatio-temporal systems including cylinder fluid flow and OLED drop impact test, and 3D large-scale system for road-car external aerodynamics. To justify the necessity of our point-wise approach for real-time prediction applications, we employ denoising diffusion implicit models (DDIM) for efficient deterministic sampling, requiring only 5-10 steps compared to traditional 1000-step and providing computational speedup of 100 to 200 times during inference without compromising accuracy. In addition, our proposed model achieves superior performance compared to image-based diffusion model: reducing training time by 94.4% and requiring 89.0% fewer parameters while achieving over 28% improvement in prediction accuracy. Comprehensive comparisons against data-flexible surrogate models including DeepONet and Meshgraphnet demonstrate consistent superiority of our approach across all three physical systems. To further refine the proposed model, we investigate two key aspects: 1) comparison of final physical states prediction or incremental change prediction, and 2) computational efficiency evaluation across varying subsampling ratios (10%-100%).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u70b9\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u72ec\u7acb\u5904\u7406\u65f6\u7a7a\u70b9\u6765\u9ad8\u6548\u9884\u6d4b\u590d\u6742\u7269\u7406\u7cfb\u7edf\uff0c\u5177\u6709\u5f62\u72b6\u53d8\u5316\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u51e0\u4f55\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\uff0c\u652f\u6301\u76f4\u63a5\u5904\u7406\u7f51\u683c\u548c\u70b9\u4e91\u7b49\u6570\u636e\u683c\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u56fe\u50cf\u7684\u6269\u6563\u6a21\u578b\u5728\u5904\u7406\u7ed3\u6784\u5316\u6570\u636e\u65f6\u6548\u7387\u8f83\u4f4e\uff0c\u4e14\u96be\u4ee5\u76f4\u63a5\u5904\u7406\u975e\u7ed3\u6784\u5316\u6570\u636e\uff08\u5982\u7f51\u683c\u548c\u70b9\u4e91\uff09\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u65b9\u6cd5\uff0c\u4ee5\u652f\u6301\u590d\u6742\u7269\u7406\u7cfb\u7edf\u7684\u5b9e\u65f6\u9884\u6d4b\u3002", "method": "\u91c7\u7528\u70b9\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u524d\u5411\u548c\u540e\u5411\u6269\u6563\u8fc7\u7a0b\uff0c\u5e76\u4f7f\u7528\u70b9\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\u8fdb\u884c\u53bb\u566a\u3002\u901a\u8fc7DDIM\u5b9e\u73b0\u9ad8\u6548\u786e\u5b9a\u6027\u91c7\u6837\uff0c\u4ec5\u97005-10\u6b65\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\u3002", "result": "\u5728\u4e09\u4e2a\u7269\u7406\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8ba1\u7b97\u901f\u5ea6\u63d0\u5347100-200\u500d\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1194.4%\uff0c\u53c2\u6570\u51cf\u5c1189.0%\uff0c\u9884\u6d4b\u7cbe\u5ea6\u63d0\u534728%\u4ee5\u4e0a\u3002", "conclusion": "\u70b9\u6269\u6563\u6a21\u578b\u5728\u590d\u6742\u7269\u7406\u7cfb\u7edf\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01292", "pdf": "https://arxiv.org/pdf/2508.01292", "abs": "https://arxiv.org/abs/2508.01292", "authors": ["Alec Sargood", "Lemuel Puglisi", "James H. Cole", "Neil P. Oxtoby", "Daniele Rav\u00ec", "Daniel C. Alexander"], "title": "CoCoLIT: ControlNet-Conditioned Latent Image Translation for MRI to Amyloid PET Synthesis", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Synthesizing amyloid PET scans from the more widely available and accessible structural MRI modality offers a promising, cost-effective approach for large-scale Alzheimer's Disease (AD) screening. This is motivated by evidence that, while MRI does not directly detect amyloid pathology, it may nonetheless encode information correlated with amyloid deposition that can be uncovered through advanced modeling. However, the high dimensionality and structural complexity of 3D neuroimaging data pose significant challenges for existing MRI-to-PET translation methods. Modeling the cross-modality relationship in a lower-dimensional latent space can simplify the learning task and enable more effective translation. As such, we present CoCoLIT (ControlNet-Conditioned Latent Image Translation), a diffusion-based latent generative framework that incorporates three main innovations: (1) a novel Weighted Image Space Loss (WISL) that improves latent representation learning and synthesis quality; (2) a theoretical and empirical analysis of Latent Average Stabilization (LAS), an existing technique used in similar generative models to enhance inference consistency; and (3) the introduction of ControlNet-based conditioning for MRI-to-PET translation. We evaluate CoCoLIT's performance on publicly available datasets and find that our model significantly outperforms state-of-the-art methods on both image-based and amyloid-related metrics. Notably, in amyloid-positivity classification, CoCoLIT outperforms the second-best method with improvements of +10.5% on the internal dataset and +23.7% on the external dataset. The code and models of our approach are available at https://github.com/brAIn-science/CoCoLIT.", "AI": {"tldr": "CoCoLIT\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u6f5c\u5728\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u52a0\u6743\u56fe\u50cf\u7a7a\u95f4\u635f\u5931\u548cControlNet\u6761\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u4e86MRI\u5230PET\u7684\u7ffb\u8bd1\u6027\u80fd\u3002", "motivation": "\u901a\u8fc7\u5408\u6210\u6dc0\u7c89\u6837\u86cb\u767dPET\u626b\u63cf\u6765\u66ff\u4ee3\u6602\u8d35\u7684MRI\u68c0\u6d4b\uff0c\u4e3a\u5927\u89c4\u6a21\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7b5b\u67e5\u63d0\u4f9b\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faCoCoLIT\u6846\u67b6\uff0c\u5305\u542b\u52a0\u6743\u56fe\u50cf\u7a7a\u95f4\u635f\u5931\uff08WISL\uff09\u3001\u6f5c\u5728\u5e73\u5747\u7a33\u5b9a\u5316\uff08LAS\uff09\u5206\u6790\u548cControlNet\u6761\u4ef6\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cCoCoLIT\u5728\u56fe\u50cf\u548c\u6dc0\u7c89\u6837\u86cb\u767d\u76f8\u5173\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5206\u7c7b\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "conclusion": "CoCoLIT\u4e3aMRI\u5230PET\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.01725", "pdf": "https://arxiv.org/pdf/2508.01725", "abs": "https://arxiv.org/abs/2508.01725", "authors": ["Xin Ding", "Yun Chen", "Yongwei Wang", "Kao Zhang", "Sen Zhang", "Peibei Cao", "Xiangxue Wang"], "title": "Imbalance-Robust and Sampling-Efficient Continuous Conditional GANs via Adaptive Vicinity and Auxiliary Regularization", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Recent advances in conditional generative modeling have introduced Continuous conditional Generative Adversarial Network (CcGAN) and Continuous Conditional Diffusion Model (CCDM) for estimating high-dimensional data distributions conditioned on scalar, continuous regression labels (e.g., angles, ages, or temperatures). However, these approaches face fundamental limitations: CcGAN suffers from data imbalance due to fixed-size vicinity constraints, while CCDM requires computationally expensive iterative sampling. We present CcGAN-AVAR, an enhanced CcGAN framework that addresses both challenges: (1) leveraging the GAN framework's native one-step generation to overcome CCDMs' sampling bottleneck (achieving 300x-2000x faster inference), while (2) two novel components specifically target data imbalance - an adaptive vicinity mechanism that dynamically adjusts vicinity's size, and a multi-task discriminator that constructs two regularization terms (through auxiliary regression and density ratio estimation) to significantly improve generator training. Extensive experiments on four benchmark datasets (64x64 to 192x192 resolution) across eight challenging imbalanced settings demonstrate that CcGAN-AVAR achieves state-of-the-art generation quality while maintaining sampling efficiency.", "AI": {"tldr": "CcGAN-AVAR\u662f\u4e00\u79cd\u6539\u8fdb\u7684CcGAN\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u90bb\u57df\u673a\u5236\u548c\u591a\u4efb\u52a1\u5224\u522b\u5668\u89e3\u51b3\u4e86\u6570\u636e\u4e0d\u5e73\u8861\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u91c7\u6837\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5CcGAN\u548cCCDM\u5206\u522b\u5b58\u5728\u6570\u636e\u4e0d\u5e73\u8861\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "CcGAN-AVAR\u7ed3\u5408\u4e86\u81ea\u9002\u5e94\u90bb\u57df\u673a\u5236\u548c\u591a\u4efb\u52a1\u5224\u522b\u5668\uff0c\u4f18\u5316\u4e86\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cCcGAN-AVAR\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u91c7\u6837\u901f\u5ea6\u63d0\u5347\u4e86300-2000\u500d\u3002", "conclusion": "CcGAN-AVAR\u5728\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u6761\u4ef6\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2508.01772", "pdf": "https://arxiv.org/pdf/2508.01772", "abs": "https://arxiv.org/abs/2508.01772", "authors": ["Cristian Minoccheri", "Matthew Hodgman", "Haoyuan Ma", "Rameez Merchant", "Emily Wittrup", "Craig Williamson", "Kayvan Najarian"], "title": "LoRA-based methods on Unet for transfer learning in Subarachnoid Hematoma Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Aneurysmal subarachnoid hemorrhage (SAH) is a life-threatening neurological emergency with mortality rates exceeding 30%. Transfer learning from related hematoma types represents a potentially valuable but underexplored approach. Although Unet architectures remain the gold standard for medical image segmentation due to their effectiveness on limited datasets, Low-Rank Adaptation (LoRA) methods for parameter-efficient transfer learning have been rarely applied to convolutional neural networks in medical imaging contexts. We implemented a Unet architecture pre-trained on computed tomography scans from 124 traumatic brain injury patients across multiple institutions, then fine-tuned on 30 aneurysmal SAH patients from the University of Michigan Health System using 3-fold cross-validation. We developed a novel CP-LoRA method based on tensor CP-decomposition and introduced DoRA variants (DoRA-C, convDoRA, CP-DoRA) that decompose weight matrices into magnitude and directional components. We compared these approaches against existing LoRA methods (LoRA-C, convLoRA) and standard fine-tuning strategies across different modules on a multi-view Unet model. LoRA-based methods consistently outperformed standard Unet fine-tuning. Performance varied by hemorrhage volume, with all methods showing improved accuracy for larger volumes. CP-LoRA achieved comparable performance to existing methods while using significantly fewer parameters. Over-parameterization with higher ranks consistently yielded better performance than strictly low-rank adaptations. This study demonstrates that transfer learning between hematoma types is feasible and that LoRA-based methods significantly outperform conventional Unet fine-tuning for aneurysmal SAH segmentation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLoRA\u7684\u6539\u8fdb\u65b9\u6cd5\uff08CP-LoRA\u548cDoRA\u53d8\u4f53\uff09\uff0c\u7528\u4e8e\u52a8\u8109\u7624\u6027\u86db\u7f51\u819c\u4e0b\u8154\u51fa\u8840\uff08SAH\uff09\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edfUnet\u5fae\u8c03\u65b9\u6cd5\u3002", "motivation": "\u52a8\u8109\u7624\u6027SAH\u662f\u4e00\u79cd\u9ad8\u6b7b\u4ea1\u7387\u7684\u795e\u7ecf\u6025\u75c7\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6709\u9650\u6570\u636e\u96c6\u4e0a\u6548\u679c\u4e0d\u4f73\uff0c\u800cLoRA\u65b9\u6cd5\u5728\u533b\u5b66\u56fe\u50cf\u9886\u57df\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u91c7\u7528Unet\u67b6\u6784\uff0c\u9884\u8bad\u7ec3\u4e8e124\u540d\u521b\u4f24\u6027\u8111\u635f\u4f24\u60a3\u8005\u7684CT\u626b\u63cf\uff0c\u5e76\u572830\u540dSAH\u60a3\u8005\u6570\u636e\u4e0a\u5fae\u8c03\u3002\u63d0\u51faCP-LoRA\u548cDoRA\u53d8\u4f53\uff0c\u5206\u89e3\u6743\u91cd\u77e9\u9635\u4e3a\u5e45\u5ea6\u548c\u65b9\u5411\u5206\u91cf\u3002", "result": "LoRA\u65b9\u6cd5\u5728SAH\u5206\u5272\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u5fae\u8c03\uff0cCP-LoRA\u53c2\u6570\u66f4\u5c11\u4f46\u6027\u80fd\u76f8\u5f53\u3002\u5927\u51fa\u8840\u91cf\u4e0b\u6240\u6709\u65b9\u6cd5\u51c6\u786e\u6027\u66f4\u9ad8\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cLoRA\u65b9\u6cd5\u5728SAH\u5206\u5272\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e14\u8de8\u8840\u80bf\u7c7b\u578b\u7684\u8fc1\u79fb\u5b66\u4e60\u53ef\u884c\u3002"}}
{"id": "2508.01782", "pdf": "https://arxiv.org/pdf/2508.01782", "abs": "https://arxiv.org/abs/2508.01782", "authors": ["Pengcheng Zheng", "Xiaorong Pu", "Kecheng Chen", "Jiaxin Huang", "Meng Yang", "Bai Feng", "Yazhou Ren", "Jianan Jiang"], "title": "Joint Lossless Compression and Steganography for Medical Images via Large Language Models", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Recently, large language models (LLMs) have driven promis ing progress in lossless image compression. However, di rectly adopting existing paradigms for medical images suf fers from an unsatisfactory trade-off between compression   performance and efficiency. Moreover, existing LLM-based   compressors often overlook the security of the compres sion process, which is critical in modern medical scenarios.   To this end, we propose a novel joint lossless compression   and steganography framework. Inspired by bit plane slicing   (BPS), we find it feasible to securely embed privacy messages   into medical images in an invisible manner. Based on this in sight, an adaptive modalities decomposition strategy is first   devised to partition the entire image into two segments, pro viding global and local modalities for subsequent dual-path   lossless compression. During this dual-path stage, we inno vatively propose a segmented message steganography algo rithm within the local modality path to ensure the security of   the compression process. Coupled with the proposed anatom ical priors-based low-rank adaptation (A-LoRA) fine-tuning   strategy, extensive experimental results demonstrate the su periority of our proposed method in terms of compression ra tios, efficiency, and security. The source code will be made   publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u65e0\u635f\u538b\u7f29\u548c\u9690\u5199\u672f\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u538b\u7f29\u6027\u80fd\u3001\u6548\u7387\u548c\u5b89\u5168\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u538b\u7f29\u65b9\u6cd5\u5728\u533b\u5b66\u56fe\u50cf\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u5ffd\u89c6\u538b\u7f29\u8fc7\u7a0b\u7684\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u53cc\u8def\u5f84\u65e0\u635f\u538b\u7f29\u7b56\u7565\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u6a21\u6001\u5206\u89e3\u548c\u5206\u6bb5\u9690\u5199\u7b97\u6cd5\uff0c\u5e76\u5f15\u5165\u89e3\u5256\u5b66\u5148\u9a8c\u7684\u4f4e\u79e9\u9002\u5e94\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u538b\u7f29\u6bd4\u3001\u6548\u7387\u548c\u5b89\u5168\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u533b\u5b66\u56fe\u50cf\u7684\u65e0\u635f\u538b\u7f29\u548c\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2508.01965", "pdf": "https://arxiv.org/pdf/2508.01965", "abs": "https://arxiv.org/abs/2508.01965", "authors": ["Petteri Teikari", "Mike Jarrell", "Irene Bandera Moreno", "Harri Pesola"], "title": "From Photons to Physics: Autonomous Indoor Drones and the Future of Objective Property Assessment", "categories": ["cs.RO", "cs.CV", "I.2.9; I.4.8; I.2.10; C.3; J.2"], "comment": "63 pages, 5 figures", "summary": "The convergence of autonomous indoor drones with physics-aware sensing technologies promises to transform property assessment from subjective visual inspection to objective, quantitative measurement. This comprehensive review examines the technical foundations enabling this paradigm shift across four critical domains: (1) platform architectures optimized for indoor navigation, where weight constraints drive innovations in heterogeneous computing, collision-tolerant design, and hierarchical control systems; (2) advanced sensing modalities that extend perception beyond human vision, including hyperspectral imaging for material identification, polarimetric sensing for surface characterization, and computational imaging with metaphotonics enabling radical miniaturization; (3) intelligent autonomy through active reconstruction algorithms, where drones equipped with 3D Gaussian Splatting make strategic decisions about viewpoint selection to maximize information gain within battery constraints; and (4) integration pathways with existing property workflows, including Building Information Modeling (BIM) systems and industry standards like Uniform Appraisal Dataset (UAD) 3.6.", "AI": {"tldr": "\u7efc\u8ff0\u63a2\u8ba8\u4e86\u81ea\u4e3b\u5ba4\u5185\u65e0\u4eba\u673a\u4e0e\u7269\u7406\u611f\u77e5\u6280\u672f\u7ed3\u5408\u5982\u4f55\u5c06\u4e3b\u89c2\u89c6\u89c9\u8bc4\u4f30\u8f6c\u53d8\u4e3a\u5ba2\u89c2\u91cf\u5316\u6d4b\u91cf\uff0c\u6db5\u76d6\u5e73\u53f0\u67b6\u6784\u3001\u5148\u8fdb\u4f20\u611f\u3001\u667a\u80fd\u81ea\u4e3b\u548c\u96c6\u6210\u8def\u5f84\u56db\u5927\u9886\u57df\u3002", "motivation": "\u63a8\u52a8\u8d22\u4ea7\u8bc4\u4f30\u4ece\u4e3b\u89c2\u89c6\u89c9\u68c0\u67e5\u5411\u5ba2\u89c2\u5b9a\u91cf\u6d4b\u91cf\u7684\u8f6c\u53d8\uff0c\u63d0\u5347\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u5206\u6790\u4e86\u56db\u4e2a\u5173\u952e\u9886\u57df\uff1a\u4f18\u5316\u7684\u5e73\u53f0\u67b6\u6784\u3001\u5148\u8fdb\u4f20\u611f\u6a21\u6001\u3001\u667a\u80fd\u81ea\u4e3b\u7b97\u6cd5\u4ee5\u53ca\u4e0e\u73b0\u6709\u5de5\u4f5c\u6d41\u7684\u96c6\u6210\u3002", "result": "\u5c55\u793a\u4e86\u65e0\u4eba\u673a\u6280\u672f\u5728\u5ba4\u5185\u5bfc\u822a\u3001\u6750\u6599\u8bc6\u522b\u3001\u8868\u9762\u8868\u5f81\u548c\u4fe1\u606f\u6700\u5927\u5316\u65b9\u9762\u7684\u521b\u65b0\u5e94\u7528\u3002", "conclusion": "\u81ea\u4e3b\u5ba4\u5185\u65e0\u4eba\u673a\u4e0e\u7269\u7406\u611f\u77e5\u6280\u672f\u7684\u7ed3\u5408\u4e3a\u8d22\u4ea7\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9769\u547d\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2508.02146", "pdf": "https://arxiv.org/pdf/2508.02146", "abs": "https://arxiv.org/abs/2508.02146", "authors": ["Seungyeon Kim", "Junsu Ha", "Young Hun Kim", "Yonghyeon Lee", "Frank C. Park"], "title": "ScrewSplat: An End-to-End Method for Articulated Object Recognition", "categories": ["cs.RO", "cs.CV"], "comment": "26 pages, 12 figures, Conference on Robot Learning (CoRL) 2025", "summary": "Articulated object recognition -- the task of identifying both the geometry and kinematic joints of objects with movable parts -- is essential for enabling robots to interact with everyday objects such as doors and laptops. However, existing approaches often rely on strong assumptions, such as a known number of articulated parts; require additional inputs, such as depth images; or involve complex intermediate steps that can introduce potential errors -- limiting their practicality in real-world settings. In this paper, we introduce ScrewSplat, a simple end-to-end method that operates solely on RGB observations. Our approach begins by randomly initializing screw axes, which are then iteratively optimized to recover the object's underlying kinematic structure. By integrating with Gaussian Splatting, we simultaneously reconstruct the 3D geometry and segment the object into rigid, movable parts. We demonstrate that our method achieves state-of-the-art recognition accuracy across a diverse set of articulated objects, and further enables zero-shot, text-guided manipulation using the recovered kinematic model.", "AI": {"tldr": "ScrewSplat\u662f\u4e00\u79cd\u57fa\u4e8eRGB\u56fe\u50cf\u7684\u7aef\u5230\u7aef\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u7269\u4f53\u7684\u51e0\u4f55\u548c\u8fd0\u52a8\u5173\u8282\uff0c\u65e0\u9700\u989d\u5916\u8f93\u5165\u6216\u590d\u6742\u6b65\u9aa4\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5f3a\u5047\u8bbe\u6216\u989d\u5916\u8f93\u5165\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002ScrewSplat\u65e8\u5728\u901a\u8fc7\u7b80\u5355\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u968f\u673a\u521d\u59cb\u5316\u87ba\u65cb\u8f74\u5e76\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u6062\u590d\u8fd0\u52a8\u7ed3\u6784\uff0c\u7ed3\u5408\u9ad8\u65af\u6cfc\u6e85\u540c\u65f6\u91cd\u5efa3D\u51e0\u4f55\u548c\u5206\u5272\u7269\u4f53\u3002", "result": "\u5728\u591a\u6837\u5316\u7269\u4f53\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u8bc6\u522b\u7cbe\u5ea6\uff0c\u5e76\u652f\u6301\u96f6\u6837\u672c\u6587\u672c\u5f15\u5bfc\u64cd\u4f5c\u3002", "conclusion": "ScrewSplat\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5173\u8282\u7269\u4f53\u8bc6\u522b\u3002"}}
{"id": "2508.02408", "pdf": "https://arxiv.org/pdf/2508.02408", "abs": "https://arxiv.org/abs/2508.02408", "authors": ["Yikuang Yuluo", "Yue Ma", "Kuan Shen", "Tongtong Jin", "Wang Liao", "Yangpu Ma", "Fuquan Wang"], "title": "GR-Gaussian: Graph-Based Radiative Gaussian Splatting for Sparse-View CT Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": "10", "summary": "3D Gaussian Splatting (3DGS) has emerged as a promising approach for CT reconstruction. However, existing methods rely on the average gradient magnitude of points within the view, often leading to severe needle-like artifacts under sparse-view conditions. To address this challenge, we propose GR-Gaussian, a graph-based 3D Gaussian Splatting framework that suppresses needle-like artifacts and improves reconstruction accuracy under sparse-view conditions. Our framework introduces two key innovations: (1) a Denoised Point Cloud Initialization Strategy that reduces initialization errors and accelerates convergence; and (2) a Pixel-Graph-Aware Gradient Strategy that refines gradient computation using graph-based density differences, improving splitting accuracy and density representation. Experiments on X-3D and real-world datasets validate the effectiveness of GR-Gaussian, achieving PSNR improvements of 0.67 dB and 0.92 dB, and SSIM gains of 0.011 and 0.021. These results highlight the applicability of GR-Gaussian for accurate CT reconstruction under challenging sparse-view conditions.", "AI": {"tldr": "GR-Gaussian\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u76843D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u7528\u4e8e\u7a00\u758f\u89c6\u56fe\u6761\u4ef6\u4e0b\u7684CT\u91cd\u5efa\uff0c\u901a\u8fc7\u51cf\u5c11\u9488\u72b6\u4f2a\u5f71\u548c\u63d0\u9ad8\u91cd\u5efa\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7a00\u758f\u89c6\u56fe\u6761\u4ef6\u4e0b\u5bb9\u6613\u4ea7\u751f\u9488\u72b6\u4f2a\u5f71\uff0c\u5f71\u54cd\u91cd\u5efa\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u521b\u65b0\uff1a\u53bb\u566a\u70b9\u4e91\u521d\u59cb\u5316\u7b56\u7565\u548c\u57fa\u4e8e\u56fe\u7684\u50cf\u7d20\u68af\u5ea6\u7b56\u7565\uff0c\u4f18\u5316\u68af\u5ea6\u8ba1\u7b97\u548c\u5bc6\u5ea6\u8868\u793a\u3002", "result": "\u5728X-3D\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cPSNR\u63d0\u53470.67 dB\u548c0.92 dB\uff0cSSIM\u63d0\u53470.011\u548c0.021\u3002", "conclusion": "GR-Gaussian\u5728\u7a00\u758f\u89c6\u56fe\u6761\u4ef6\u4e0b\u80fd\u6709\u6548\u63d0\u9ad8CT\u91cd\u5efa\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.02512", "pdf": "https://arxiv.org/pdf/2508.02512", "abs": "https://arxiv.org/abs/2508.02512", "authors": ["Sheng Wu", "Fei Teng", "Hao Shi", "Qi Jiang", "Kai Luo", "Kaiwei Wang", "Kailun Yang"], "title": "QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots", "categories": ["cs.RO", "cs.CV", "eess.IV"], "comment": "Accepted to CoRL 2025. The source code and model weights will be   publicly available at https://github.com/losehu/QuaDreamer", "summary": "Panoramic cameras, capturing comprehensive 360-degree environmental data, are suitable for quadruped robots in surrounding perception and interaction with complex environments. However, the scarcity of high-quality panoramic training data-caused by inherent kinematic constraints and complex sensor calibration challenges-fundamentally limits the development of robust perception systems tailored to these embodied platforms. To address this issue, we propose QuaDreamer-the first panoramic data generation engine specifically designed for quadruped robots. QuaDreamer focuses on mimicking the motion paradigm of quadruped robots to generate highly controllable, realistic panoramic videos, providing a data source for downstream tasks. Specifically, to effectively capture the unique vertical vibration characteristics exhibited during quadruped locomotion, we introduce Vertical Jitter Encoding (VJE). VJE extracts controllable vertical signals through frequency-domain feature filtering and provides high-quality prompts. To facilitate high-quality panoramic video generation under jitter signal control, we propose a Scene-Object Controller (SOC) that effectively manages object motion and boosts background jitter control through the attention mechanism. To address panoramic distortions in wide-FoV video generation, we propose the Panoramic Enhancer (PE)-a dual-stream architecture that synergizes frequency-texture refinement for local detail enhancement with spatial-structure correction for global geometric consistency. We further demonstrate that the generated video sequences can serve as training data for the quadruped robot's panoramic visual perception model, enhancing the performance of multi-object tracking in 360-degree scenes. The source code and model weights will be publicly available at https://github.com/losehu/QuaDreamer.", "AI": {"tldr": "QuaDreamer\u662f\u4e00\u4e2a\u4e13\u4e3a\u56db\u8db3\u673a\u5668\u4eba\u8bbe\u8ba1\u7684\u5168\u666f\u6570\u636e\u751f\u6210\u5f15\u64ce\uff0c\u901a\u8fc7\u6a21\u62df\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u5168\u666f\u89c6\u9891\uff0c\u89e3\u51b3\u4e86\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u7279\u6027\u548c\u4f20\u611f\u5668\u6821\u51c6\u590d\u6742\u6027\uff0c\u9ad8\u8d28\u91cf\u5168\u666f\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\uff0c\u9650\u5236\u4e86\u611f\u77e5\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51faVertical Jitter Encoding (VJE)\u6355\u6349\u5782\u76f4\u632f\u52a8\u7279\u6027\uff0cScene-Object Controller (SOC)\u7ba1\u7406\u5bf9\u8c61\u8fd0\u52a8\uff0cPanoramic Enhancer (PE)\u89e3\u51b3\u5168\u666f\u5931\u771f\u95ee\u9898\u3002", "result": "\u751f\u6210\u7684\u89c6\u9891\u5e8f\u5217\u53ef\u7528\u4e8e\u8bad\u7ec3\u56db\u8db3\u673a\u5668\u4eba\u7684\u5168\u666f\u89c6\u89c9\u611f\u77e5\u6a21\u578b\uff0c\u63d0\u5347360\u5ea6\u573a\u666f\u4e2d\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u6027\u80fd\u3002", "conclusion": "QuaDreamer\u4e3a\u56db\u8db3\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u53ef\u63a7\u3001\u771f\u5b9e\u7684\u5168\u666f\u6570\u636e\u751f\u6210\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u76f8\u5173\u611f\u77e5\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.02528", "pdf": "https://arxiv.org/pdf/2508.02528", "abs": "https://arxiv.org/abs/2508.02528", "authors": ["Jingsong Liu", "Xiaofeng Deng", "Han Li", "Azar Kazemi", "Christian Grashei", "Gesa Wilkens", "Xin You", "Tanja Groll", "Nassir Navab", "Carolin Mogler", "Peter J. Sch\u00fcffler"], "title": "From Pixels to Pathology: Restoration Diffusion for Diagnostic-Consistent Virtual IHC", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Hematoxylin and eosin (H&E) staining is the clinical standard for assessing tissue morphology, but it lacks molecular-level diagnostic information. In contrast, immunohistochemistry (IHC) provides crucial insights into biomarker expression, such as HER2 status for breast cancer grading, but remains costly and time-consuming, limiting its use in time-sensitive clinical workflows. To address this gap, virtual staining from H&E to IHC has emerged as a promising alternative, yet faces two core challenges: (1) Lack of fair evaluation of synthetic images against misaligned IHC ground truths, and (2) preserving structural integrity and biological variability during translation. To this end, we present an end-to-end framework encompassing both generation and evaluation in this work. We introduce Star-Diff, a structure-aware staining restoration diffusion model that reformulates virtual staining as an image restoration task. By combining residual and noise-based generation pathways, Star-Diff maintains tissue structure while modeling realistic biomarker variability. To evaluate the diagnostic consistency of the generated IHC patches, we propose the Semantic Fidelity Score (SFS), a clinical-grading-task-driven metric that quantifies class-wise semantic degradation based on biomarker classification accuracy. Unlike pixel-level metrics such as SSIM and PSNR, SFS remains robust under spatial misalignment and classifier uncertainty. Experiments on the BCI dataset demonstrate that Star-Diff achieves state-of-the-art (SOTA) performance in both visual fidelity and diagnostic relevance. With rapid inference and strong clinical alignment,it presents a practical solution for applications such as intraoperative virtual IHC synthesis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStar-Diff\u7684\u7ed3\u6784\u611f\u77e5\u67d3\u8272\u6062\u590d\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u4eceH&E\u67d3\u8272\u865a\u62df\u751f\u6210IHC\u67d3\u8272\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u56fe\u50cf\u5bf9\u9f50\u548c\u7ed3\u6784\u4fdd\u6301\u4e0a\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7Semantic Fidelity Score (SFS)\u8bc4\u4f30\u8bca\u65ad\u4e00\u81f4\u6027\u3002", "motivation": "H&E\u67d3\u8272\u7f3a\u4e4f\u5206\u5b50\u8bca\u65ad\u4fe1\u606f\uff0c\u800cIHC\u67d3\u8272\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u3002\u865a\u62df\u67d3\u8272\u6280\u672f\u6709\u671b\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4f46\u9762\u4e34\u56fe\u50cf\u5bf9\u9f50\u548c\u7ed3\u6784\u4fdd\u6301\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faStar-Diff\u6a21\u578b\uff0c\u7ed3\u5408\u6b8b\u5dee\u548c\u566a\u58f0\u751f\u6210\u8def\u5f84\uff0c\u5c06\u865a\u62df\u67d3\u8272\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u3002\u5f15\u5165SFS\u6307\u6807\uff0c\u57fa\u4e8e\u751f\u7269\u6807\u5fd7\u7269\u5206\u7c7b\u51c6\u786e\u6027\u8bc4\u4f30\u751f\u6210\u56fe\u50cf\u3002", "result": "\u5728BCI\u6570\u636e\u96c6\u4e0a\uff0cStar-Diff\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u8bca\u65ad\u76f8\u5173\u6027\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u63a8\u7406\u901f\u5ea6\u5feb\u4e14\u4e34\u5e8a\u9002\u7528\u6027\u5f3a\u3002", "conclusion": "Star-Diff\u4e3a\u865a\u62dfIHC\u5408\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u672f\u4e2d\u5e94\u7528\u3002"}}
