<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 3]
- [cs.CV](#cs.CV) [Total: 29]
- [physics.optics](#physics.optics) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust Physics-Based Dynamics](https://arxiv.org/abs/2510.01619)
*Changmin Lee,Jihyun Lee,Tae-Kyun Kim*

Main category: cs.GR

TL;DR: MPMAvatar是一个从多视角视频创建3D人体化身的框架，支持高度逼真的动画和自由视角的光线真实渲染。该框架使用基于物质点法的模拟器来准确建模服装的复杂变形和与身体的接触，并结合3D高斯溅射渲染技术。


<details>
  <summary>Details</summary>
Motivation: 现有基于物理模拟的3D化身创建方法在准确性和对新动画输入的鲁棒性方面存在局限，特别是在建模松散服装的物理动力学方面仍具挑战性。

Method: 使用定制的物质点法模拟器，结合各向异性本构模型和新型碰撞处理算法来建模服装复杂变形；结合使用3D高斯溅射渲染的规范化身。

Result: MPMAvatar在动力学建模准确性、渲染准确性、鲁棒性和效率方面显著优于现有最先进的基于物理的化身方法，并能以零样本方式泛化到未见过的交互场景。

Conclusion: 该框架成功解决了建模松散服装物理动力学的挑战，实现了高度逼真和鲁棒的动画，并为未见交互提供了泛化能力，这在之前的学习方法中是不可实现的。

Abstract: While there has been significant progress in the field of 3D avatar creation from visual observations, modeling physically plausible dynamics of humans with loose garments remains a challenging problem. Although a few existing works address this problem by leveraging physical simulation, they suffer from limited accuracy or robustness to novel animation inputs. In this work, we present MPMAvatar, a framework for creating 3D human avatars from multi-view videos that supports highly realistic, robust animation, as well as photorealistic rendering from free viewpoints. For accurate and robust dynamics modeling, our key idea is to use a Material Point Method-based simulator, which we carefully tailor to model garments with complex deformations and contact with the underlying body by incorporating an anisotropic constitutive model and a novel collision handling algorithm. We combine this dynamics modeling scheme with our canonical avatar that can be rendered using 3D Gaussian Splatting with quasi-shadowing, enabling high-fidelity rendering for physically realistic animations. In our experiments, we demonstrate that MPMAvatar significantly outperforms the existing state-of-the-art physics-based avatar in terms of (1) dynamics modeling accuracy, (2) rendering accuracy, and (3) robustness and efficiency. Additionally, we present a novel application in which our avatar generalizes to unseen interactions in a zero-shot manner-which was not achievable with previous learning-based methods due to their limited simulation generalizability. Our project page is at: https://KAISTChangmin.github.io/MPMAvatar/

</details>


### [2] [ROI-GS: Interest-based Local Quality 3D Gaussian Splatting](https://arxiv.org/abs/2510.01978)
*Quoc-Anh Bui,Gilles Rougeron,Géraldine Morin,Simone Gasparini*

Main category: cs.GR

TL;DR: ROI-GS是一个对象感知的3D高斯泼溅框架，通过对象引导的相机选择、针对性对象训练和高保真对象重建的集成，在感兴趣区域实现更高细节的3D场景重建，同时减少模型大小并保持实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法在场景中均匀分配资源，限制了感兴趣区域的精细细节，导致模型尺寸膨胀。需要一种方法能够优先处理选定对象的高分辨率细节。

Method: 提出ROI-GS框架，包括对象引导的相机选择、针对性对象训练，以及将高保真对象重建无缝集成到全局场景中。

Result: 实验显示ROI-GS显著提升局部质量（PSNR最高提升2.96 dB），模型大小减少约17%，单对象场景训练更快，优于现有方法。

Conclusion: ROI-GS能够有效提升感兴趣对象的细节重建质量，同时减少整体模型大小并保持实时性能，优于传统均匀资源分配方法。

Abstract: We tackle the challenge of efficiently reconstructing 3D scenes with high detail on objects of interest. Existing 3D Gaussian Splatting (3DGS) methods allocate resources uniformly across the scene, limiting fine detail to Regions Of Interest (ROIs) and leading to inflated model size. We propose ROI-GS, an object-aware framework that enhances local details through object-guided camera selection, targeted Object training, and seamless integration of high-fidelity object of interest reconstructions into the global scene. Our method prioritizes higher resolution details on chosen objects while maintaining real-time performance. Experiments show that ROI-GS significantly improves local quality (up to 2.96 dB PSNR), while reducing overall model size by $\approx 17\%$ of baseline and achieving faster training for a scene with a single object of interest, outperforming existing methods.

</details>


### [3] [Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy Objects](https://arxiv.org/abs/2510.02069)
*Georgios Kouros,Minye Wu,Tinne Tuytelaars*

Main category: cs.GR

TL;DR: 提出了一种可重光照框架，将微表面BRDF与镜面光泽度参数化集成到2D高斯泼溅中，实现了更物理一致的材料分解和高质量重光照。


<details>
  <summary>Details</summary>
Motivation: 准确重建和重光照光泽物体是一个长期挑战，现有神经渲染方法通常依赖简化的BRDF模型或耦合漫反射和镜面分量的参数化，限制了材料恢复的准确性和重光照保真度。

Method: 将微表面BRDF与镜面光泽度参数化集成到2D高斯泼溅中，采用延迟着色；使用基于扩散的表面法线和漫反射颜色先验指导早期优化；采用从粗到精的环境图优化策略。

Result: 在复杂光泽场景上的广泛实验表明，该方法实现了高质量的几何和材料重建，与现有高斯泼溅方法相比，在新光照下提供了更真实和一致的重光照效果。

Conclusion: 所提出的框架通过物理一致的BRDF建模和优化策略，显著提升了光泽物体的重建和重光照质量，为神经渲染提供了更准确的材质分解能力。

Abstract: Accurate reconstruction and relighting of glossy objects remain a longstanding challenge, as object shape, material properties, and illumination are inherently difficult to disentangle. Existing neural rendering approaches often rely on simplified BRDF models or parameterizations that couple diffuse and specular components, which restricts faithful material recovery and limits relighting fidelity. We propose a relightable framework that integrates a microfacet BRDF with the specular-glossiness parameterization into 2D Gaussian Splatting with deferred shading. This formulation enables more physically consistent material decomposition, while diffusion-based priors for surface normals and diffuse color guide early-stage optimization and mitigate ambiguity. A coarse-to-fine optimization of the environment map accelerates convergence and preserves high-dynamic-range specular reflections. Extensive experiments on complex, glossy scenes demonstrate that our method achieves high-quality geometry and material reconstruction, delivering substantially more realistic and consistent relighting under novel illumination compared to existing Gaussian splatting methods.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration](https://arxiv.org/abs/2510.01339)
*Alessio Spagnoletti,Andrés Almansa,Marcelo Pereyra*

Main category: cs.CV

TL;DR: 提出了LVTINO，首个基于视频一致性模型(VCMs)的零样本视频恢复方法，解决了帧级图像扩散模型导致的时间不一致问题，在计算效率和重建质量上都达到了新水平。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本图像逆求解器虽然能利用潜在扩散模型(LDMs)实现高精度图像恢复，但直接应用于视频恢复时会导致时间不一致问题，需要同时恢复精细空间细节和捕捉微妙时间依赖性。

Method: 利用视频一致性模型(VCMs)将视频潜在扩散模型蒸馏为快速生成器，明确捕捉时间因果关系。提出的LVTINO方法通过条件机制绕过自动微分需求，仅需少量神经网络评估即可实现高质量视频重建。

Result: 在多种视频逆问题上的广泛实验表明，相比当前最先进的帧级图像LDM方法，在感知质量上有显著提升，在重建保真度和计算效率方面都建立了新基准。

Conclusion: LVTINO通过视频一致性模型实现了高质量、时间一致的视频恢复，为高分辨率视频逆问题提供了有效的零样本解决方案，在保持测量一致性和平滑时间过渡的同时大幅提升了计算效率。

Abstract: Computational imaging methods increasingly rely on powerful generative diffusion models to tackle challenging image restoration tasks. In particular, state-of-the-art zero-shot image inverse solvers leverage distilled text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy and perceptual quality with high computational efficiency. However, extending these advances to high-definition video restoration remains a significant challenge, due to the need to recover fine spatial detail while capturing subtle temporal dependencies. Consequently, methods that naively apply image-based LDM priors on a frame-by-frame basis often result in temporally inconsistent reconstructions. We address this challenge by leveraging recent advances in Video Consistency Models (VCMs), which distill video latent diffusion models into fast generators that explicitly capture temporal causality. Building on this foundation, we propose LVTINO, the first zero-shot or plug-and-play inverse solver for high definition video restoration with priors encoded by VCMs. Our conditioning mechanism bypasses the need for automatic differentiation and achieves state-of-the-art video reconstruction quality with only a few neural function evaluations, while ensuring strong measurement consistency and smooth temporal transitions across frames. Extensive experiments on a diverse set of video inverse problems show significant perceptual improvements over current state-of-the-art methods that apply image LDMs frame by frame, establishing a new benchmark in both reconstruction fidelity and computational efficiency.

</details>


### [5] [Image Generation Based on Image Style Extraction](https://arxiv.org/abs/2510.01347)
*Shuochen Chang*

Main category: cs.CV

TL;DR: 提出了一种基于三阶段训练的风格提取图像生成方法，通过风格编码器和风格投影层将风格表示与文本表示对齐，实现基于文本提示的细粒度风格引导生成。


<details>
  <summary>Details</summary>
Motivation: 文本到图像生成模型中，细粒度风格难以用自然语言精确描述和控制，而风格化参考图像的引导信息难以与传统文本引导生成直接对齐。

Method: 使用风格编码器和风格投影层从单张风格参考图像中提取细粒度风格表示，并将其注入生成模型而不改变其结构框架。构建了包含图像、风格标签和文本描述三元组的Style30k-captions数据集。

Result: 实现了细粒度控制的风格化图像生成，能够最大化预训练生成模型的生成能力。

Conclusion: 该方法通过风格表示与文本表示的对齐，成功实现了基于文本提示的细粒度风格引导图像生成。

Abstract: Image generation based on text-to-image generation models is a task with practical application scenarios that fine-grained styles cannot be precisely described and controlled in natural language, while the guidance information of stylized reference images is difficult to be directly aligned with the textual conditions of traditional textual guidance generation. This study focuses on how to maximize the generative capability of the pretrained generative model, by obtaining fine-grained stylistic representations from a single given stylistic reference image, and injecting the stylistic representations into the generative body without changing the structural framework of the downstream generative model, so as to achieve fine-grained controlled stylized image generation. In this study, we propose a three-stage training style extraction-based image generation method, which uses a style encoder and a style projection layer to align the style representations with the textual representations to realize fine-grained textual cue-based style guide generation. In addition, this study constructs the Style30k-captions dataset, whose samples contain a triad of images, style labels, and text descriptions, to train the style encoder and style projection layer in this experiment.

</details>


### [6] [Purrception: Variational Flow Matching for Vector-Quantized Image Generation](https://arxiv.org/abs/2510.01478)
*Răzvan-Andrei Matişan,Vincent Tao Hu,Grigory Bartosh,Björn Ommer,Cees G. M. Snoek,Max Welling,Jan-Willem van de Meent,Mohammad Mahdi Derakhshani,Floor Eijkelboom*

Main category: cs.CV

TL;DR: Purrception是一种用于矢量量化图像生成的变分流匹配方法，结合了连续传输动力学和显式分类监督。


<details>
  <summary>Details</summary>
Motivation: 旨在将连续方法的几何感知与分类方法的离散监督相结合，为图像生成提供改进的训练效率。

Method: 通过在学习码本索引的分类后验的同时在连续嵌入空间中计算速度场，将变分流匹配适应于矢量量化潜在空间。

Result: 在ImageNet-1k 256x256生成任务中，训练收敛速度比连续和离散流匹配基线更快，同时达到与最先进模型竞争的FID分数。

Conclusion: 变分流匹配可以有效地桥接连续传输和离散监督，提高图像生成的训练效率。

Abstract: We introduce Purrception, a variational flow matching approach for vector-quantized image generation that provides explicit categorical supervision while maintaining continuous transport dynamics. Our method adapts Variational Flow Matching to vector-quantized latents by learning categorical posteriors over codebook indices while computing velocity fields in the continuous embedding space. This combines the geometric awareness of continuous methods with the discrete supervision of categorical approaches, enabling uncertainty quantification over plausible codes and temperature-controlled generation. We evaluate Purrception on ImageNet-1k 256x256 generation. Training converges faster than both continuous flow matching and discrete flow matching baselines while achieving competitive FID scores with state-of-the-art models. This demonstrates that Variational Flow Matching can effectively bridge continuous transport and discrete supervision for improved training efficiency in image generation.

</details>


### [7] [AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging](https://arxiv.org/abs/2510.01498)
*Yuxuan Ou,Ning Bi,Jiazhen Pan,Jiancheng Yang,Boliang Yu,Usama Zidan,Regent Lee,Vicente Grau*

Main category: cs.CV

TL;DR: 提出了一种统一的深度学习框架，通过条件扩散模型和多任务学习，从非对比CT扫描同时生成合成对比增强CT图像并分割主动脉腔和血栓，避免了多阶段方法的误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 传统对比增强CT检查需要碘对比剂，存在肾毒性、过敏反应和环境危害等风险。现有的多阶段方法先生成图像再分割，导致误差累积且无法充分利用共享的语义和解剖结构。

Method: 集成条件扩散模型与多任务学习，端到端联合优化图像合成和解剖分割。无需初始预测，在编码器和解码器层面共享参数，采用半监督训练策略处理缺失分割标签的临床数据。

Result: 在264名患者队列中表现优于现有方法：图像合成PSNR达25.61 dB（单任务CDM为23.80 dB）；分割方面，腔Dice得分0.89（nnU-Net为0.87），血栓Dice得分0.53（nnU-Net为0.48）；临床测量更准确，腔直径MAE降至4.19 mm（nnU-Net为5.78 mm）。

Conclusion: 该统一框架通过联合优化图像合成和分割任务，显著提升了合成图像质量和解剖结构分割精度，为减少对比剂使用提供了有效解决方案。

Abstract: While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic aneurysms (AAA), the required iodinated contrast agents pose significant risks, including nephrotoxicity, patient allergies, and environmental harm. To reduce contrast agent use, recent deep learning methods have focused on generating synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a multi-stage pipeline that first generates images and then performs segmentation, which leads to error accumulation and fails to leverage shared semantic and anatomical structures. To address this, we propose a unified deep learning framework that generates synthetic CECT images from NCCT scans while simultaneously segmenting the aortic lumen and thrombus. Our approach integrates conditional diffusion models (CDM) with multi-task learning, enabling end-to-end joint optimization of image synthesis and anatomical segmentation. Unlike previous multitask diffusion models, our approach requires no initial predictions (e.g., a coarse segmentation mask), shares both encoder and decoder parameters across tasks, and employs a semi-supervised training strategy to learn from scans with missing segmentation labels, a common constraint in real-world clinical data. We evaluated our method on a cohort of 264 patients, where it consistently outperformed state-of-the-art single-task and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61 dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation, it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.

</details>


### [8] [Towards Better Optimization For Listwise Preference in Diffusion Models](https://arxiv.org/abs/2510.01540)
*Jiamu Bai,Xin Yu,Meilong Xu,Weitao Lu,Xin Pan,Kiwan Maeng,Daniel Kifer,Jian Wang,Yu Wang*

Main category: cs.CV

TL;DR: 提出了Diffusion-LPO框架，将列表式偏好优化应用于扩散模型，通过Plackett-Luce模型扩展DPO目标，在文本到图像生成、图像编辑和个性化偏好对齐等任务中优于成对DPO基线。


<details>
  <summary>Details</summary>
Motivation: 人类对图像的偏好反馈通常包含隐式的排序信息，比成对比较更能精确表达人类偏好。目前DPO在扩散模型中的应用主要依赖成对偏好，列表式偏好的精确优化尚未得到充分解决。

Method: 提出Diffusion-LPO框架，将用户反馈聚合成图像排序列表，在Plackett-Luce模型下推导DPO目标的列表式扩展，通过鼓励每个样本优于其所有低排名替代品来增强整个排序的一致性。

Result: Diffusion-LPO在文本到图像生成、图像编辑和个性化偏好对齐等任务中，在视觉质量和偏好对齐方面持续优于成对DPO基线方法。

Conclusion: Diffusion-LPO是一个简单有效的列表式偏好优化框架，能够更好地利用人类反馈中的排序信息，提升扩散模型与人类偏好的对齐效果。

Abstract: Reinforcement learning from human feedback (RLHF) has proven effectiveness for aligning text-to-image (T2I) diffusion models with human preferences. Although Direct Preference Optimization (DPO) is widely adopted for its computational efficiency and avoidance of explicit reward modeling, its applications to diffusion models have primarily relied on pairwise preferences. The precise optimization of listwise preferences remains largely unaddressed. In practice, human feedback on image preferences often contains implicit ranked information, which conveys more precise human preferences than pairwise comparisons. In this work, we propose Diffusion-LPO, a simple and effective framework for Listwise Preference Optimization in diffusion models with listwise data. Given a caption, we aggregate user feedback into a ranked list of images and derive a listwise extension of the DPO objective under the Plackett-Luce model. Diffusion-LPO enforces consistency across the entire ranking by encouraging each sample to be preferred over all of its lower-ranked alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO across various tasks, including text-to-image generation, image editing, and personalized preference alignment. Diffusion-LPO consistently outperforms pairwise DPO baselines on visual quality and preference alignment.

</details>


### [9] [NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems](https://arxiv.org/abs/2510.01608)
*Roman Jacome,Romario Gualdrón-Hurtado,Leon Suarez,Henry Arguello*

Main category: cs.CV

TL;DR: 提出了一种新的正则化方法NPN，通过神经网络在感知矩阵零空间的低维投影上施加约束，而不是在图像域施加结构约束，从而解决成像逆问题中的模糊性。


<details>
  <summary>Details</summary>
Motivation: 传统的正则化方法通常忽略感知矩阵零空间的特定结构，本文旨在设计能够捕捉与感知过程正交信息的感知矩阵特定先验。

Method: 使用神经网络在感知矩阵的零空间上进行低维投影，设计感知矩阵特定的先验，该方法可与现有重建框架兼容，并补充传统的图像域先验。

Result: 在压缩感知、去模糊、超分辨率、计算机断层扫描和磁共振成像等多种成像逆问题中，NPN先验能持续提高重建保真度。

Conclusion: NPN方法通过关注零空间结构，提供了解释性和灵活性，能有效增强各种成像逆问题的重建性能。

Abstract: Imaging inverse problems aims to recover high-dimensional signals from undersampled, noisy measurements, a fundamentally ill-posed task with infinite solutions in the null-space of the sensing operator. To resolve this ambiguity, prior information is typically incorporated through handcrafted regularizers or learned models that constrain the solution space. However, these priors typically ignore the task-specific structure of that null-space. In this work, we propose \textit{Non-Linear Projections of the Null-Space} (NPN), a novel class of regularization that, instead of enforcing structural constraints in the image domain, promotes solutions that lie in a low-dimensional projection of the sensing matrix's null-space with a neural network. Our approach has two key advantages: (1) Interpretability: by focusing on the structure of the null-space, we design sensing-matrix-specific priors that capture information orthogonal to the signal components that are fundamentally blind to the sensing process. (2) Flexibility: NPN is adaptable to various inverse problems, compatible with existing reconstruction frameworks, and complementary to conventional image-domain priors. We provide theoretical guarantees on convergence and reconstruction accuracy when used within plug-and-play methods. Empirical results across diverse sensing matrices demonstrate that NPN priors consistently enhance reconstruction fidelity in various imaging inverse problems, such as compressive sensing, deblurring, super-resolution, computed tomography, and magnetic resonance imaging, with plug-and-play methods, unrolling networks, deep image prior, and diffusion models.

</details>


### [10] [Joint Deblurring and 3D Reconstruction for Macrophotography](https://arxiv.org/abs/2510.01640)
*Yifan Zhao,Liangchen Li,Yuqi Zhou,Kai Wang,Yan Liang,Juyong Zhang*

Main category: cs.CV

TL;DR: 提出了一种用于微距摄影的联合去模糊和3D重建方法，从多视角模糊图像中联合优化物体的清晰3D模型和每个像素的散焦模糊核。


<details>
  <summary>Details</summary>
Motivation: 微距镜头具有高分辨率和大放大倍率的优势，但散焦模糊问题严重阻碍了清晰成像和高质量3D重建。传统去模糊方法需要大量图像和标注，且目前没有针对微距摄影的多视角3D重建方法。

Method: 采用可微分渲染方法自监督优化3D模型和散焦模糊核，从多视角模糊图像出发，联合优化清晰3D模型和每个像素的模糊核。

Result: 大量实验表明，从少量多视角图像中，该方法不仅能实现高质量图像去模糊，还能恢复高保真度的3D外观。

Conclusion: 该方法成功解决了微距摄影中的散焦模糊问题，实现了联合去模糊和3D重建，为小物体和细节物体的高质量3D建模提供了有效解决方案。

Abstract: Macro lens has the advantages of high resolution and large magnification, and 3D modeling of small and detailed objects can provide richer information. However, defocus blur in macrophotography is a long-standing problem that heavily hinders the clear imaging of the captured objects and high-quality 3D reconstruction of them. Traditional image deblurring methods require a large number of images and annotations, and there is currently no multi-view 3D reconstruction method for macrophotography. In this work, we propose a joint deblurring and 3D reconstruction method for macrophotography. Starting from multi-view blurry images captured, we jointly optimize the clear 3D model of the object and the defocus blur kernel of each pixel. The entire framework adopts a differentiable rendering method to self-supervise the optimization of the 3D model and the defocus blur kernel. Extensive experiments show that from a small number of multi-view images, our proposed method can not only achieve high-quality image deblurring but also recover high-fidelity 3D appearance.

</details>


### [11] [FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring](https://arxiv.org/abs/2510.01641)
*Xiaoyang Liu,Zhengyan Zhou,Zihang Xu,Jiezhang Cao,Zheng Chen,Yulun Zhang*

Main category: cs.CV

TL;DR: FideDiff是一种新颖的单步扩散模型，通过将运动去模糊重新表述为扩散过程，训练一致性模型实现一步高质量去模糊，在保持高保真度的同时显著减少推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的图像去模糊方法存在推理时间过长和保真度不足的问题，限制了其在真实世界应用中的潜力。

Method: 将运动去模糊重新表述为扩散过程，每个时间步代表逐渐模糊的图像；训练一致性模型将所有时间步对齐到同一清晰图像；集成Kernel ControlNet进行模糊核估计；引入自适应时间步预测。

Result: 在全参考指标上实现优越性能，超越之前的扩散方法，与其他最先进模型性能相当。

Conclusion: FideDiff为预训练扩散模型在高保真图像恢复任务中的应用提供了新方向，为在真实工业应用中进一步推进扩散模型建立了坚实基础。

Abstract: Recent advancements in image motion deblurring, driven by CNNs and transformers, have made significant progress. Large-scale pre-trained diffusion models, which are rich in true-world modeling, have shown great promise for high-quality image restoration tasks such as deblurring, demonstrating stronger generative capabilities than CNN and transformer-based methods. However, challenges such as unbearable inference time and compromised fidelity still limit the full potential of the diffusion models. To address this, we introduce FideDiff, a novel single-step diffusion model designed for high-fidelity deblurring. We reformulate motion deblurring as a diffusion-like process where each timestep represents a progressively blurred image, and we train a consistency model that aligns all timesteps to the same clean image. By reconstructing training data with matched blur trajectories, the model learns temporal consistency, enabling accurate one-step deblurring. We further enhance model performance by integrating Kernel ControlNet for blur kernel estimation and introducing adaptive timestep prediction. Our model achieves superior performance on full-reference metrics, surpassing previous diffusion-based methods and matching the performance of other state-of-the-art models. FideDiff offers a new direction for applying pre-trained diffusion models to high-fidelity image restoration tasks, establishing a robust baseline for further advancing diffusion models in real-world industrial applications. Our dataset and code will be available at https://github.com/xyLiu339/FideDiff.

</details>


### [12] [Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery](https://arxiv.org/abs/2510.01662)
*Minh Tran,Maksim Siniukov,Zhangyu Jin,Mohammad Soleymani*

Main category: cs.CV

TL;DR: 提出了一种名为离散面部编码(DFE)的无监督方法，通过残差向量量化变分自编码器从3D网格序列中学习紧凑且可解释的面部表情字典，在心理任务中优于FACS和其他面部编码方法。


<details>
  <summary>Details</summary>
Motivation: 现有面部表情编码系统如FACS存在覆盖范围有限和人工标注成本高的问题，需要一种更精确、可扩展的替代方案。

Method: 使用3D可变形模型提取身份不变的表情特征，然后通过残差向量量化变分自编码器将这些特征编码为来自共享码本的离散标记序列。

Result: DFE比FACS和其他面部编码方法捕捉到更精确的面部行为，在压力检测、人格预测和抑郁检测等心理任务中表现优于基于FACS的流程和强图像/视频表示学习模型。

Conclusion: DFE作为FACS的可扩展有效替代方案，在心理学和情感计算应用中具有潜力，能够覆盖更广泛的面部表情。

Abstract: Facial expression analysis is central to understanding human behavior, yet existing coding systems such as the Facial Action Coding System (FACS) are constrained by limited coverage and costly manual annotation. In this work, we introduce Discrete Facial Encoding (DFE), an unsupervised, data-driven alternative of compact and interpretable dictionary of facial expressions from 3D mesh sequences learned through a Residual Vector Quantized Variational Autoencoder (RVQ-VAE). Our approach first extracts identity-invariant expression features from images using a 3D Morphable Model (3DMM), effectively disentangling factors such as head pose and facial geometry. We then encode these features using an RVQ-VAE, producing a sequence of discrete tokens from a shared codebook, where each token captures a specific, reusable facial deformation pattern that contributes to the overall expression. Through extensive experiments, we demonstrate that Discrete Facial Encoding captures more precise facial behaviors than FACS and other facial encoding alternatives. We evaluate the utility of our representation across three high-level psychological tasks: stress detection, personality prediction, and depression detection. Using a simple Bag-of-Words model built on top of the learned tokens, our system consistently outperforms both FACS-based pipelines and strong image and video representation learning models such as Masked Autoencoders. Further analysis reveals that our representation covers a wider variety of facial displays, highlighting its potential as a scalable and effective alternative to FACS for psychological and affective computing applications.

</details>


### [13] [UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction](https://arxiv.org/abs/2510.01669)
*Jin Cao,Hongrui Wu,Ziyong Feng,Hujun Bao,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: UniVerse是一个统一的鲁棒重建框架，通过将不一致的多视角图像转换为视频，使用视频扩散模型恢复一致性，然后重建3D场景。


<details>
  <summary>Details</summary>
Motivation: 解决从不一致多视角图像进行3D重建的挑战，现有方法依赖密集观测且优化困难。

Method: 将鲁棒重建分解为恢复和重建两个子任务，使用视频扩散模型学习通用场景先验来恢复图像一致性。

Result: 在合成和真实数据集上展示了强大的泛化能力和优越性能，并能控制重建3D场景的风格。

Conclusion: UniVerse通过解耦恢复和重建任务，利用扩散模型的通用先验，有效解决了不一致多视角图像的3D重建问题。

Abstract: This paper tackles the challenge of robust reconstruction, i.e., the task of reconstructing a 3D scene from a set of inconsistent multi-view images. Some recent works have attempted to simultaneously remove image inconsistencies and perform reconstruction by integrating image degradation modeling into neural 3D scene representations.However, these methods rely heavily on dense observations for robustly optimizing model parameters.To address this issue, we propose to decouple robust reconstruction into two subtasks: restoration and reconstruction, which naturally simplifies the optimization process.To this end, we introduce UniVerse, a unified framework for robust reconstruction based on a video diffusion model. Specifically, UniVerse first converts inconsistent images into initial videos, then uses a specially designed video diffusion model to restore them into consistent images, and finally reconstructs the 3D scenes from these restored images.Compared with case-by-case per-view degradation modeling, the diffusion model learns a general scene prior from large-scale data, making it applicable to diverse image inconsistencies.Extensive experiments on both synthetic and real-world datasets demonstrate the strong generalization capability and superior performance of our method in robust reconstruction. Moreover, UniVerse can control the style of the reconstructed 3D scene. Project page: https://jin-cao-tma.github.io/UniVerse.github.io/

</details>


### [14] [FreeViS: Training-free Video Stylization with Inconsistent References](https://arxiv.org/abs/2510.01686)
*Jiacong Xu,Yiqun Mei,Ke Zhang,Vishal M. Patel*

Main category: cs.CV

TL;DR: FreeViS是一种无需训练的视频风格化框架，通过整合多个风格化参考到预训练的图像到视频模型，生成具有丰富风格细节和强时间一致性的风格化视频。


<details>
  <summary>Details</summary>
Motivation: 现有的视频风格化方法存在时间一致性差、风格丰富度不足的问题，而训练专门的视频风格化模型需要配对视频数据且计算成本高。

Method: 使用预训练的图像到视频模型，整合多个风格化参考，采用高频补偿约束内容布局和运动，结合基于光流的运动线索来保留低显著性区域的风格纹理。

Result: FreeViS在风格化保真度和时间一致性方面表现优异，超越了现有基线方法，获得了强烈的人类偏好。

Conclusion: FreeViS提供了一个实用且经济的解决方案，能够实现高质量、时间一致性的视频风格化，无需训练过程。

Abstract: Video stylization plays a key role in content creation, but it remains a challenging problem. Na\"ively applying image stylization frame-by-frame hurts temporal consistency and reduces style richness. Alternatively, training a dedicated video stylization model typically requires paired video data and is computationally expensive. In this paper, we propose FreeViS, a training-free video stylization framework that generates stylized videos with rich style details and strong temporal coherence. Our method integrates multiple stylized references to a pretrained image-to-video (I2V) model, effectively mitigating the propagation errors observed in prior works, without introducing flickers and stutters. In addition, it leverages high-frequency compensation to constrain the content layout and motion, together with flow-based motion cues to preserve style textures in low-saliency regions. Through extensive evaluations, FreeViS delivers higher stylization fidelity and superior temporal consistency, outperforming recent baselines and achieving strong human preference. Our training-free pipeline offers a practical and economic solution for high-quality, temporally coherent video stylization. The code and videos can be accessed via https://xujiacong.github.io/FreeViS/

</details>


### [15] [PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning](https://arxiv.org/abs/2510.01715)
*Raahul Krishna Durairaju,K. Saruladha*

Main category: cs.CV

TL;DR: PyramidStyler是一个基于Transformer的神经风格迁移框架，采用金字塔位置编码和强化学习优化，在保持实时推理速度的同时显著降低了内容和风格损失。


<details>
  <summary>Details</summary>
Motivation: 现有的CNN和Transformer模型在处理复杂风格和高分辨率输入时存在计算效率低下的问题，需要一种既能捕捉局部细节又能理解全局上下文的高效方法。

Method: 提出PyramidStyler框架，包含金字塔位置编码（PPE）实现多尺度特征捕捉，并结合强化学习动态优化风格化过程以加速收敛。

Result: 在4000轮训练后，内容损失降低62.6%至2.07，风格损失降低57.4%至0.86，推理时间1.39秒；使用RL后进一步改善至内容损失2.03、风格损失0.75，速度仅轻微下降至1.40秒。

Conclusion: 该方法实现了实时高质量的艺术渲染，在媒体和设计领域具有广泛应用前景。

Abstract: Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based algorithm, enabling AI-driven artistic image synthesis. However, existing CNN and transformer-based models struggle to scale efficiently to complex styles and high-resolution inputs. We introduce PyramidStyler, a transformer framework with Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding that captures both local details and global context while reducing computational load. We further incorporate reinforcement learning to dynamically optimize stylization, accelerating convergence. Trained on Microsoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to 2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s inference--and yields further improvements (content 2.03; style 0.75) with minimal speed penalty (1.40 s) when using RL. These results demonstrate real-time, high-quality artistic rendering, with broad applications in media and design.

</details>


### [16] [LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction](https://arxiv.org/abs/2510.01767)
*Sheng-Hsiang Hung,Ting-Yu Yen,Wei-Fang Sun,Simon See,Shih-Hsuan Hung,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: LoBE-GS是一个负载平衡的高效3D高斯泼溅框架，通过深度感知分区、优化策略和轻量级技术，在大规模场景中实现2倍训练速度提升，同时保持重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有分治方法在大规模无边界场景（如城市街区）中存在负载不平衡问题，统一或启发式分区无法反映实际计算需求，粗到细流水线效率低下且开销高。

Method: 提出深度感知分区方法减少预处理时间，采用基于优化的策略平衡各区块可见高斯分布，并引入可见性裁剪和选择性致密化两种轻量级技术。

Result: 在大规模城市和户外数据集评估中，LoBE-GS比现有最优方法实现2倍端到端训练速度提升，同时保持重建质量，能够扩展到传统3DGS无法处理的场景。

Conclusion: LoBE-GS通过重新设计大规模3DGS流水线，解决了负载不平衡和效率问题，为大规模场景重建提供了高效解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has established itself as an efficient representation for real-time, high-fidelity 3D scene reconstruction. However, scaling 3DGS to large and unbounded scenes such as city blocks remains difficult. Existing divide-and-conquer methods alleviate memory pressure by partitioning the scene into blocks, but introduce new bottlenecks: (i) partitions suffer from severe load imbalance since uniform or heuristic splits do not reflect actual computational demands, and (ii) coarse-to-fine pipelines fail to exploit the coarse stage efficiently, often reloading the entire model and incurring high overhead. In this work, we introduce LoBE-GS, a novel Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning method that reduces preprocessing from hours to minutes, an optimization-based strategy that balances visible Gaussians -- a strong proxy for computational load -- across blocks, and two lightweight techniques, visibility cropping and selective densification, to further reduce training cost. Evaluations on large-scale urban and outdoor datasets show that LoBE-GS consistently achieves up to $2\times$ faster end-to-end training time than state-of-the-art baselines, while maintaining reconstruction quality and enabling scalability to scenes infeasible with vanilla 3DGS.

</details>


### [17] [Pack and Force Your Memory: Long-form and Consistent Video Generation](https://arxiv.org/abs/2510.01784)
*Xiaofei Wu,Guozhen Zhang,Zhiyong Xu,Yuan Zhou,Qinglin Lu,Xuming He*

Main category: cs.CV

TL;DR: 提出MemoryPack和Direct Forcing两个创新方法，解决长视频生成中的长期依赖建模和自回归解码错误累积问题。


<details>
  <summary>Details</summary>
Motivation: 长视频生成面临双重挑战：需要捕获长期依赖关系，同时防止自回归解码中固有的错误累积。

Method: 1. MemoryPack：可学习的上下文检索机制，利用文本和图像信息作为全局指导，联合建模短期和长期依赖；2. Direct Forcing：高效的单步近似策略，改善训练-推理对齐，减少推理过程中的错误传播。

Result: 实现了分钟级的时间一致性，计算效率高且保持线性复杂度，显著提升了长视频生成的上下文一致性和可靠性。

Conclusion: MemoryPack和Direct Forcing共同推进了自回归视频模型的实际可用性，为长视频生成提供了有效的解决方案。

Abstract: Long-form video generation presents a dual challenge: models must capture long-range dependencies while preventing the error accumulation inherent in autoregressive decoding. To address these challenges, we make two contributions. First, for dynamic context modeling, we propose MemoryPack, a learnable context-retrieval mechanism that leverages both textual and image information as global guidance to jointly model short- and long-term dependencies, achieving minute-level temporal consistency. This design scales gracefully with video length, preserves computational efficiency, and maintains linear complexity. Second, to mitigate error accumulation, we introduce Direct Forcing, an efficient single-step approximating strategy that improves training-inference alignment and thereby curtails error propagation during inference. Together, MemoryPack and Direct Forcing substantially enhance the context consistency and reliability of long-form video generation, advancing the practical usability of autoregressive video models.

</details>


### [18] [Leveraging Prior Knowledge of Diffusion Model for Person Search](https://arxiv.org/abs/2510.01841)
*Giyeol Kim,Sooyoung Yang,Jihyong Oh,Myungjoo Kang,Chanho Eom*

Main category: cs.CV

TL;DR: DiffPS是一个利用预训练扩散模型先验知识来解决行人搜索任务的新框架，通过三个专门模块消除检测和重识别子任务之间的优化冲突，在两个基准数据集上达到新的最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要使用ImageNet预训练骨干网络，可能无法充分捕捉行人搜索所需的复杂空间上下文和细粒度身份线索，且共享骨干网络特征会导致检测和重识别两个子任务之间存在优化冲突。

Method: 提出DiffPS框架，利用预训练扩散模型的先验知识，包含三个专门模块：扩散引导区域提议网络(DGRPN)用于增强行人定位，多尺度频率细化网络(MSFRN)缓解形状偏差，语义自适应特征聚合网络(SFAN)利用文本对齐的扩散特征。

Result: 在CUHK-SYSU和PRW两个基准数据集上达到了新的最优性能。

Conclusion: 扩散先验知识能够有效提升行人搜索性能，通过专门设计的模块可以消除子任务间的优化冲突，为行人搜索任务提供了新的解决方案。

Abstract: Person search aims to jointly perform person detection and re-identification by localizing and identifying a query person within a gallery of uncropped scene images. Existing methods predominantly utilize ImageNet pre-trained backbones, which may be suboptimal for capturing the complex spatial context and fine-grained identity cues necessary for person search. Moreover, they rely on a shared backbone feature for both person detection and re-identification, leading to suboptimal features due to conflicting optimization objectives. In this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a novel framework that leverages a pre-trained diffusion model while eliminating the optimization conflict between two sub-tasks. We analyze key properties of diffusion priors and propose three specialized modules: (i) Diffusion-Guided Region Proposal Network (DGRPN) for enhanced person localization, (ii) Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and (iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage text-aligned diffusion features. DiffPS sets a new state-of-the-art on CUHK-SYSU and PRW.

</details>


### [19] [4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing](https://arxiv.org/abs/2510.01991)
*Lei Liu,Can Wang,Zhenghao Chen,Dong Xu*

Main category: cs.CV

TL;DR: 4DGS-Craft是一个一致的交互式4D高斯溅射编辑框架，通过4D感知的InstructPix2Pix模型、多视图网格模块和Gaussian选择机制解决视图、时间和非编辑区域一致性问题，并利用LLM理解用户意图处理复杂指令。


<details>
  <summary>Details</summary>
Motivation: 现有的4D高斯溅射编辑方法在视图一致性、时间一致性、非编辑区域一致性以及处理复杂文本指令方面仍面临挑战，需要开发更一致和可控的4D场景编辑框架。

Method: 1. 引入4D感知的InstructPix2Pix模型，结合4D VGGT几何特征确保视图和时间一致性；2. 使用多视图网格模块迭代优化多视图输入图像；3. 提出Gaussian选择机制仅优化编辑区域的高斯分布；4. 设计基于LLM的用户意图理解模块，将复杂指令分解为原子操作序列。

Result: 与相关工作相比，该方法能够实现更一致和可控的4D场景编辑，有效处理复杂的用户指令，并在编辑过程中保持非编辑区域的稳定性。

Conclusion: 4DGS-Craft框架通过结合4D几何感知、多视图一致性和智能用户交互，显著提升了4D高斯溅射编辑的质量和可用性，为复杂4D场景编辑提供了有效的解决方案。

Abstract: Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges with view, temporal, and non-editing region consistency, as well as with handling complex text instructions. To address these issues, we propose 4DGS-Craft, a consistent and interactive 4DGS editing framework. We first introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal consistency. This model incorporates 4D VGGT geometry features extracted from the initial scene, enabling it to capture underlying 4D geometric structures during editing. We further enhance this model with a multi-view grid module that enforces consistency by iteratively refining multi-view input images while jointly optimizing the underlying 4D scene. Furthermore, we preserve the consistency of non-edited regions through a novel Gaussian selection mechanism, which identifies and optimizes only the Gaussians within the edited regions. Beyond consistency, facilitating user interaction is also crucial for effective 4DGS editing. Therefore, we design an LLM-based module for user intent understanding. This module employs a user instruction template to define atomic editing operations and leverages an LLM for reasoning. As a result, our framework can interpret user intent and decompose complex instructions into a logical sequence of atomic operations, enabling it to handle intricate user commands and further enhance editing performance. Compared to related works, our approach enables more consistent and controllable 4D scene editing. Our code will be made available upon acceptance.

</details>


### [20] [Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution](https://arxiv.org/abs/2510.01997)
*Junyu Wu,Jie Tang,Jie Liu,Gangshan Wu*

Main category: cs.CV

TL;DR: 提出Pure-Pass(PP)像素级掩码机制，通过固定颜色中心点分类像素，免除纯像素的昂贵计算，集成到ATD-light模型中实现高效超分辨率重建


<details>
  <summary>Details</summary>
Motivation: 现有轻量级超分辨率方法如CAMixer存在适应性差、掩码粒度粗和空间灵活性不足等问题，需要更精细的计算优化

Method: 使用固定颜色中心点对像素进行分类，识别纯像素并免除其昂贵计算，实现像素级细粒度掩码，保持自适应灵活性

Result: PP-ATD-light在重建质量和参数效率上优于CAMixer-ATD-light，在节省相似计算量的情况下获得更优性能

Conclusion: Pure-Pass机制通过像素级掩码有效优化计算复杂度，为轻量级图像超分辨率提供了高效解决方案

Abstract: Image Super-Resolution (SR) aims to reconstruct high-resolution images from low-resolution counterparts, but the computational complexity of deep learning-based methods often hinders practical deployment. CAMixer is the pioneering work to integrate the advantages of existing lightweight SR methods and proposes a content-aware mixer to route token mixers of varied complexities according to the difficulty of content recovery. However, several limitations remain, such as poor adaptability, coarse-grained masking and spatial inflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking mechanism that identifies pure pixels and exempts them from expensive computations. PP utilizes fixed color center points to classify pixels into distinct categories, enabling fine-grained, spatially flexible masking while maintaining adaptive flexibility. Integrated into the state-of-the-art ATD-light model, PP-ATD-light achieves superior SR performance with minimal overhead, outperforming CAMixer-ATD-light in reconstruction quality and parameter efficiency when saving a similar amount of computation.

</details>


### [21] [GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing](https://arxiv.org/abs/2510.02034)
*Mengtian Li,Yunshu Bai,Yimin Chu,Yijun Shen,Zhongmei Li,Weifeng Ge,Zhifeng Xie,Chaofeng Chen*

Main category: cs.CV

TL;DR: GaussianMorphing是一个从多视角图像进行语义感知3D形状和纹理变形的新框架，通过网格引导的3D高斯泼溅实现高保真几何和外观建模，无需标注数据即可保持局部细节和全局语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖点云或需要预定义同胚映射来处理无纹理数据，这些方法存在局限性。本文旨在克服这些限制，实现几何一致且纹理保真的3D变形。

Method: 采用统一变形策略，将3D高斯锚定到重建的网格面片上，通过拓扑感知约束确保几何一致变换和纹理保真度；同时利用网格拓扑作为几何先验建立无监督语义对应，并通过物理合理的点轨迹保持结构完整性。

Result: 在提出的TexMorph基准测试中，GaussianMorphing显著优于现有2D/3D方法，将颜色一致性误差(ΔE)降低了22.2%，EI降低了26.2%。

Conclusion: 该框架成功实现了无需标注数据的语义感知3D形状和纹理变形，在保持局部细节和全局语义一致性的同时，显著提升了变形质量。

Abstract: We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape and texture morphing from multi-view images. Previous approaches usually rely on point clouds or require pre-defined homeomorphic mappings for untextured data. Our method overcomes these limitations by leveraging mesh-guided 3D Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling. The core of our framework is a unified deformation strategy that anchors 3DGaussians to reconstructed mesh patches, ensuring geometrically consistent transformations while preserving texture fidelity through topology-aware constraints. In parallel, our framework establishes unsupervised semantic correspondence by using the mesh topology as a geometric prior and maintains structural integrity via physically plausible point trajectories. This integrated approach preserves both local detail and global semantic coherence throughout the morphing process with out requiring labeled data. On our proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior 2D/3D methods, reducing color consistency error ($\Delta E$) by 22.2% and EI by 26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

</details>


### [22] [Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers](https://arxiv.org/abs/2510.02043)
*Sahil Bhandary Karnoor,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 提出InPose方法，使用预训练扩散模型和仅旋转测量进行零样本泛化的姿态估计


<details>
  <summary>Details</summary>
Motivation: 现有基于条件扩散模型的方法在用户间泛化能力差，主要因为位置测量受用户体型影响大

Method: 将姿态估计建模为逆问题，使用预训练扩散模型仅基于旋转测量，通过似然项引导位置测量

Result: InPose方法能够零样本泛化到任意用户，生成最可能解释稀疏身体测量的姿态序列

Conclusion: 该方法通过分离旋转和位置测量处理，解决了用户间泛化问题，实现了有效的姿态估计

Abstract: Pose estimation refers to tracking a human's full body posture, including their head, torso, arms, and legs. The problem is challenging in practical settings where the number of body sensors are limited. Past work has shown promising results using conditional diffusion models, where the pose prediction is conditioned on both <location, rotation> measurements from the sensors. Unfortunately, nearly all these approaches generalize poorly across users, primarly because location measurements are highly influenced by the body size of the user. In this paper, we formulate pose estimation as an inverse problem and design an algorithm capable of zero-shot generalization. Our idea utilizes a pre-trained diffusion model and conditions it on rotational measurements alone; the priors from this model are then guided by a likelihood term, derived from the measured locations. Thus, given any user, our proposed InPose method generatively estimates the highly likely sequence of poses that best explains the sparse on-body measurements.

</details>


### [23] [VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation](https://arxiv.org/abs/2510.02086)
*Arman Behnam*

Main category: cs.CV

TL;DR: 提出VGDM：基于视觉引导扩散模型的脑肿瘤检测与分割框架，结合Transformer和扩散模型提升脑肿瘤分割性能


<details>
  <summary>Details</summary>
Motivation: 传统卷积架构如U-Net在捕获长距离依赖关系方面能力有限，限制了在复杂肿瘤结构上的性能。扩散模型在生成高保真医学图像和细化分割边界方面显示出强大潜力

Method: 在扩散过程核心嵌入视觉Transformer，利用全局上下文推理与迭代去噪相结合，增强体积精度和边界精度。Transformer主干能够更有效地建模整个MRI体积的空间关系，扩散细化减轻体素级误差并恢复细粒度肿瘤细节

Result: 在MRI脑肿瘤数据集上的实验验证显示，在Dice相似性和Hausdorff距离指标上持续提升

Conclusion: 这种混合设计为神经肿瘤学中改进的鲁棒性和可扩展性提供了途径，超越了传统的U-Net基线，展示了Transformer引导扩散模型在推进肿瘤分割技术前沿的潜力

Abstract: Accurate detection and segmentation of brain tumors from magnetic resonance imaging (MRI) are essential for diagnosis, treatment planning, and clinical monitoring. While convolutional architectures such as U-Net have long been the backbone of medical image segmentation, their limited capacity to capture long-range dependencies constrains performance on complex tumor structures. Recent advances in diffusion models have demonstrated strong potential for generating high-fidelity medical images and refining segmentation boundaries.   In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation framework, a transformer-driven diffusion framework for brain tumor detection and segmentation. By embedding a vision transformer at the core of the diffusion process, the model leverages global contextual reasoning together with iterative denoising to enhance both volumetric accuracy and boundary precision. The transformer backbone enables more effective modeling of spatial relationships across entire MRI volumes, while diffusion refinement mitigates voxel-level errors and recovers fine-grained tumor details.   This hybrid design provides a pathway toward improved robustness and scalability in neuro-oncology, moving beyond conventional U-Net baselines. Experimental validation on MRI brain tumor datasets demonstrates consistent gains in Dice similarity and Hausdorff distance, underscoring the potential of transformer-guided diffusion models to advance the state of the art in tumor segmentation.

</details>


### [24] [TempoControl: Temporal Attention Guidance for Text-to-Video Models](https://arxiv.org/abs/2510.02226)
*Shira Schiber,Ofir Lindenbaum,Idan Schwartz*

Main category: cs.CV

TL;DR: TempoControl是一种无需重新训练或额外监督的方法，通过利用文本到视频扩散模型中的交叉注意力图，实现视觉概念的时间对齐控制。


<details>
  <summary>Details</summary>
Motivation: 现有生成视频模型缺乏细粒度的时间控制能力，无法让用户指定特定视觉元素在生成序列中出现的时间。

Method: 使用交叉注意力图，通过新颖的优化方法引导概念时序，采用三个互补原则：通过相关性对齐时间形状、通过能量放大可见性区域、通过熵保持空间聚焦。

Result: TempoControl实现了精确的时间控制，同时确保高质量和多样性的视频生成，在多种应用中表现有效，包括单/多对象时间重排序、动作和音频对齐生成。

Conclusion: 该方法为生成视频提供了细粒度的时间控制能力，无需重新训练模型，具有广泛的应用前景。

Abstract: Recent advances in generative video models have enabled the creation of high-quality videos based on natural language prompts. However, these models frequently lack fine-grained temporal control, meaning they do not allow users to specify when particular visual elements should appear within a generated sequence. In this work, we introduce TempoControl, a method that allows for temporal alignment of visual concepts during inference, without requiring retraining or additional supervision. TempoControl utilizes cross-attention maps, a key component of text-to-video diffusion models, to guide the timing of concepts through a novel optimization approach. Our method steers attention using three complementary principles: aligning its temporal shape with a control signal (via correlation), amplifying it where visibility is needed (via energy), and maintaining spatial focus (via entropy). TempoControl allows precise control over timing while ensuring high video quality and diversity. We demonstrate its effectiveness across various video generation applications, including temporal reordering for single and multiple objects, as well as action and audio-aligned generation.

</details>


### [25] [DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing](https://arxiv.org/abs/2510.02253)
*Zihan Zhou,Shilin Lu,Shuli Leng,Shaocong Zhang,Zhuming Lian,Xinlei Yu,Adams Wai-Kin Kong*

Main category: cs.CV

TL;DR: DragFlow是首个有效利用FLUX强大先验进行拖拽式图像编辑的框架，通过区域化编辑范式、个性化适配器和多模态大语言模型，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型从UNet转向DiT，生成先验显著增强，但拖拽式编辑尚未受益于这些更强先验。现有方法在目标区域存在失真问题，需要有效利用新模型的强大能力。

Method: 提出区域化编辑范式，使用仿射变换提供更丰富的特征监督；集成预训练个性化适配器增强主体一致性；采用梯度掩码硬约束保护背景；使用多模态大语言模型解决任务歧义。

Result: 在DragBench-DR和ReD Bench基准测试中，DragFlow超越了基于点和基于区域的基线方法，在拖拽式图像编辑中达到新的最先进水平。

Conclusion: DragFlow成功利用FLUX的强大先验，通过区域化编辑和多种技术集成，显著提升了拖拽式图像编辑的质量和效果。

Abstract: Drag-based image editing has long suffered from distortions in the target region, largely because the priors of earlier base models, Stable Diffusion, are insufficient to project optimized latents back onto the natural image manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow matching (e.g., SD3.5, FLUX), generative priors have become significantly stronger, enabling advances across diverse editing tasks. However, drag-based editing has yet to benefit from these stronger priors. This work proposes the first framework to effectively harness FLUX's rich prior for drag-based editing, dubbed DragFlow, achieving substantial gains over baselines. We first show that directly applying point-based drag editing to DiTs performs poorly: unlike the highly compressed features of UNets, DiT features are insufficiently structured to provide reliable guidance for point-wise motion supervision. To overcome this limitation, DragFlow introduces a region-based editing paradigm, where affine transformations enable richer and more consistent feature supervision. Additionally, we integrate pretrained open-domain personalization adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving background fidelity through gradient mask-based hard constraints. Multimodal large language models (MLLMs) are further employed to resolve task ambiguities. For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench) featuring region-level dragging instructions. Extensive experiments on DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and region-based baselines, setting a new state-of-the-art in drag-based image editing. Code and datasets will be publicly available upon publication.

</details>


### [26] [NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes](https://arxiv.org/abs/2510.02266)
*Shiyi Zhang,Dong Liang,Yihang Zhou*

Main category: cs.CV

TL;DR: NeuroSwift是一个用于从fMRI数据重建视觉信息的扩散模型，通过集成AutoKL和CLIP适配器来处理低层特征和语义信息，实现了跨被试的快速训练和先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决从脑活动重建视觉信息时面临的跨被试差异大、计算成本高以及大脑对复杂视觉输入的抽象语义编码等挑战。

Method: 提出NeuroSwift模型，集成AutoKL适配器处理低层特征和CLIP适配器处理语义信息。CLIP适配器在Stable Diffusion生成图像和COCO标题对上训练以模拟高级视觉皮层编码。采用预训练加微调策略，仅微调17%参数实现跨被试泛化。

Result: 在轻量级GPU（三块RTX 4090）上每个被试仅需1小时训练，即达到最先进性能，超越了现有方法。

Conclusion: NeuroSwift通过互补适配器集成和高效的跨被试训练策略，成功解决了从fMRI数据重建视觉信息的核心挑战，实现了高性能和计算效率的平衡。

Abstract: Reconstructing visual information from brain activity via computer vision technology provides an intuitive understanding of visual neural mechanisms. Despite progress in decoding fMRI data with generative models, achieving accurate cross-subject reconstruction of visual stimuli remains challenging and computationally demanding. This difficulty arises from inter-subject variability in neural representations and the brain's abstract encoding of core semantic features in complex visual inputs. To address these challenges, we propose NeuroSwift, which integrates complementary adapters via diffusion: AutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter is trained on Stable Diffusion generated images paired with COCO captions to emulate higher visual cortex encoding. For cross-subject generalization, we pretrain on one subject and then fine-tune only 17 percent of parameters (fully connected layers) for new subjects, while freezing other components. This enables state-of-the-art performance with only one hour of training per subject on lightweight GPUs (three RTX 4090), and it outperforms existing methods.

</details>


### [27] [Self-Forcing++: Towards Minute-Scale High-Quality Video Generation](https://arxiv.org/abs/2510.02283)
*Justin Cui,Jie Wu,Ming Li,Tao Yang,Xiaojie Li,Rui Wang,Andrew Bai,Yuanhao Ban,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 提出了一种简单有效的方法来缓解长视频生成中的质量退化问题，无需长视频教师监督或重新训练长视频数据集，通过利用教师模型的丰富知识为学生模型提供指导。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像和视频生成方面取得了革命性进展，但基于transformer架构的计算成本过高，特别是在生成长视频时。现有的自回归方法在超出训练范围时会出现明显的质量退化问题。

Method: 通过从自生成长视频中抽取片段，利用教师模型的知识为学生模型提供指导，保持时间一致性，避免过曝光和错误累积问题。

Result: 方法能够将视频长度扩展到教师模型能力的20倍，生成最长4分15秒的视频，相当于基础模型位置嵌入支持的最大跨度的99.9%，比基线模型长50多倍。在标准基准测试中，方法在保真度和一致性方面显著优于基线方法。

Conclusion: 该方法有效解决了长视频生成中的质量退化问题，无需额外监督或重新训练，实现了高质量的长视频生成。

Abstract: Diffusion models have revolutionized image and video generation, achieving unprecedented visual quality. However, their reliance on transformer architectures incurs prohibitively high computational costs, particularly when extending generation to long videos. Recent work has explored autoregressive formulations for long video generation, typically by distilling from short-horizon bidirectional teachers. Nevertheless, given that teacher models cannot synthesize long videos, the extrapolation of student models beyond their training horizon often leads to pronounced quality degradation, arising from the compounding of errors within the continuous latent space. In this paper, we propose a simple yet effective approach to mitigate quality degradation in long-horizon video generation without requiring supervision from long-video teachers or retraining on long video datasets. Our approach centers on exploiting the rich knowledge of teacher models to provide guidance for the student model through sampled segments drawn from self-generated long videos. Our method maintains temporal consistency while scaling video length by up to 20x beyond teacher's capability, avoiding common issues such as over-exposure and error-accumulation without recomputing overlapping frames like previous methods. When scaling up the computation, our method shows the capability of generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the maximum span supported by our base model's position embedding and more than 50x longer than that of our baseline model. Experiments on standard benchmarks and our proposed improved benchmark demonstrate that our approach substantially outperforms baseline methods in both fidelity and consistency. Our long-horizon videos demo can be found at https://self-forcing-plus-plus.github.io/

</details>


### [28] [Learning to Generate Object Interactions with Physics-Guided Video Diffusion](https://arxiv.org/abs/2510.02284)
*David Romero,Ariana Bermudez,Hao Li,Fabio Pizzati,Ivan Laptev*

Main category: cs.CV

TL;DR: KineMask是一种物理引导的视频生成方法，通过两阶段训练策略实现逼真的刚体控制、交互和效果，显著改善了物体交互的真实性。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在物理合理的物体交互方面仍存在困难，缺乏基于物理的控制机制，需要开发能够模拟真实物理交互的视频生成方法。

Method: 提出两阶段训练策略，通过物体掩码逐步移除未来运动监督，在合成场景上训练视频扩散模型，并结合低级运动控制与高级文本条件。

Result: 在真实场景中显著改善了物体交互效果，相比同类规模模型有明显提升，能够合成复杂的动力学现象。

Conclusion: KineMask通过物理引导的视频生成实现了逼真的刚体控制和交互，高低级条件在视频扩散模型中具有互补作用。

Abstract: Recent models for video generation have achieved remarkable progress and are now deployed in film, social media production, and advertising. Beyond their creative potential, such models also hold promise as world simulators for robotics and embodied decision making. Despite strong advances, however, current approaches still struggle to generate physically plausible object interactions and lack physics-grounded control mechanisms. To address this limitation, we introduce KineMask, an approach for physics-guided video generation that enables realistic rigid body control, interactions, and effects. Given a single image and a specified object velocity, our method generates videos with inferred motions and future object interactions. We propose a two-stage training strategy that gradually removes future motion supervision via object masks. Using this strategy we train video diffusion models (VDMs) on synthetic scenes of simple interactions and demonstrate significant improvements of object interactions in real scenes. Furthermore, KineMask integrates low-level motion control with high-level textual conditioning via predictive scene descriptions, leading to effective support for synthesis of complex dynamical phenomena. Extensive experiments show that KineMask achieves strong improvements over recent models of comparable size. Ablation studies further highlight the complementary roles of low- and high-level conditioning in VDMs. Our code, model, and data will be made publicly available.

</details>


### [29] [MultiModal Action Conditioned Video Generation](https://arxiv.org/abs/2510.02287)
*Yichen Li,Antonio Torralba*

Main category: cs.CV

TL;DR: 该论文提出了多模态精细动作控制方法，通过整合本体感觉、动觉、力触觉和肌肉激活等多种感官，提升视频模型在精细控制任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前视频模型缺乏精细控制能力，无法作为世界模型使用。通用家用机器人需要实时精细运动控制来处理精细任务和紧急情况。

Method: 引入多模态精细动作，开发特征学习范式对齐不同模态，同时保留各模态独特信息；提出正则化方案增强动作轨迹特征的因果性。

Result: 实验表明，整合多模态感官提高了模拟精度，减少了时间漂移。广泛的消融研究和下游应用证明了方法的有效性和实用性。

Conclusion: 多模态感官整合能够有效提升精细控制任务的模拟性能，为机器人精细运动控制提供了实用解决方案。

Abstract: Current video models fail as world model as they lack fine-graiend control. General-purpose household robots require real-time fine motor control to handle delicate tasks and urgent situations. In this work, we introduce fine-grained multimodal actions to capture such precise control. We consider senses of proprioception, kinesthesia, force haptics, and muscle activation. Such multimodal senses naturally enables fine-grained interactions that are difficult to simulate with text-conditioned generative models. To effectively simulate fine-grained multisensory actions, we develop a feature learning paradigm that aligns these modalities while preserving the unique information each modality provides. We further propose a regularization scheme to enhance causality of the action trajectory features in representing intricate interaction dynamics. Experiments show that incorporating multimodal senses improves simulation accuracy and reduces temporal drift. Extensive ablation studies and downstream applications demonstrate the effectiveness and practicality of our work.

</details>


### [30] [NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation](https://arxiv.org/abs/2510.02307)
*Ruozhen He,Moayed Haji-Ali,Ziyan Yang,Vicente Ordonez*

Main category: cs.CV

TL;DR: NoiseShift是一种无需训练的方法，通过根据分辨率大小重新校准去噪器的噪声水平，显著提升低分辨率图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 高分辨率文本到图像生成器无法为不需要高分辨率图像的用户提供开箱即用的经济高效替代方案，因为扩散模型在固定分辨率集上训练后往往无法泛化到其他分辨率。

Method: 识别噪声调度器在不同分辨率下具有不等的感知效果，提出NoiseShift方法重新校准条件于分辨率大小的去噪器噪声水平，无需改变模型架构或采样计划。

Result: 在LAION-COCO数据集上，NoiseShift将SD3.5的FID提升15.89%，SD3提升8.56%，Flux-Dev提升2.44%；在CelebA数据集上，分别提升10.36%、5.19%和3.02%。

Conclusion: NoiseShift能有效缓解分辨率相关的伪影，提升低分辨率图像生成质量，且与现有模型兼容。

Abstract: Text-to-image diffusion models trained on a fixed set of resolutions often fail to generalize, even when asked to generate images at lower resolutions than those seen during training. High-resolution text-to-image generators are currently unable to easily offer an out-of-the-box budget-efficient alternative to their users who might not need high-resolution images. We identify a key technical insight in diffusion models that when addressed can help tackle this limitation: Noise schedulers have unequal perceptual effects across resolutions. The same level of noise removes disproportionately more signal from lower-resolution images than from high-resolution images, leading to a train-test mismatch. We propose NoiseShift, a training-free method that recalibrates the noise level of the denoiser conditioned on resolution size. NoiseShift requires no changes to model architecture or sampling schedule and is compatible with existing models. When applied to Stable Diffusion 3, Stable Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by 10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent artifacts and enhancing the quality of low-resolution image generation.

</details>


### [31] [StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions](https://arxiv.org/abs/2510.02314)
*Bo-Hsu Ke,You-Zhe Xie,Yu-Lun Liu,Wei-Chen Chiu*

Main category: cs.CV

TL;DR: 本文分析了3D高斯泼溅(3DGS)对图像级投毒攻击的脆弱性，提出了一种基于密度的投毒方法，通过在低密度区域注入高斯点来嵌入视角依赖的幻觉对象。


<details>
  <summary>Details</summary>
Motivation: 随着NeRF和3DGS等3D场景表示方法的普及，解决其安全漏洞变得至关重要。本文旨在分析3DGS对投毒攻击的鲁棒性，并开发更有效的攻击方法。

Method: 提出密度引导的投毒方法，使用核密度估计识别低密度区域，战略性地注入高斯点来嵌入视角依赖的幻觉对象。同时引入自适应噪声策略破坏多视角一致性。

Result: 大量实验表明，该方法在性能上优于现有技术，能够有效嵌入可见的幻觉对象，同时最小化对无辜视角的影响。

Conclusion: 本文不仅提出了一种有效的3DGS投毒攻击方法，还引入了基于KDE的评估协议，为未来研究提供了客观的基准测试框架。

Abstract: 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As these methods become prevalent, addressing their vulnerabilities becomes critical. We analyze 3DGS robustness against image-level poisoning attacks and propose a novel density-guided poisoning method. Our method strategically injects Gaussian points into low-density regions identified via Kernel Density Estimation (KDE), embedding viewpoint-dependent illusory objects clearly visible from poisoned views while minimally affecting innocent views. Additionally, we introduce an adaptive noise strategy to disrupt multi-view consistency, further enhancing attack effectiveness. We propose a KDE-based evaluation protocol to assess attack difficulty systematically, enabling objective benchmarking for future research. Extensive experiments demonstrate our method's superior performance compared to state-of-the-art techniques. Project page: https://hentci.github.io/stealthattack/

</details>


### [32] [Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity](https://arxiv.org/abs/2510.02315)
*Eric Tillmann Bill,Enis Simsar,Thomas Hofmann*

Main category: cs.CV

TL;DR: 提出了FOCUS框架，通过随机最优控制理论解决文本到图像模型在多主体生成时的属性泄漏、身份纠缠和主体遗漏问题，包含无需训练的测试时控制器和轻量级微调方法Adjoint Matching。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型在单主体提示上表现良好，但在多主体描述时经常出现属性泄漏、身份纠缠和主体遗漏等问题，需要理论框架来指导采样过程。

Method: 将流匹配通过随机最优控制视角重新表述，开发了两种架构无关算法：测试时控制器通过单次更新扰动基础速度场，以及Adjoint Matching通过回归控制网络到反向伴随信号进行轻量级微调。

Result: 在Stable Diffusion 3.5、FLUX和Stable Diffusion XL上的实验表明，两种算法都能持续改善多主体对齐，同时保持基础模型风格。测试时控制器在消费级GPU上高效运行，微调控制器在有限提示上训练后能泛化到未见提示。

Conclusion: FOCUS框架在多主体保真度方面达到最先进水平，为多主体生成提供了首个理论框架和显式设计的微调路径。

Abstract: Text-to-image (T2I) models excel on single-entity prompts but struggle with multi-subject descriptions, often showing attribute leakage, identity entanglement, and subject omissions. We introduce the first theoretical framework with a principled, optimizable objective for steering sampling dynamics toward multi-subject fidelity. Viewing flow matching (FM) through stochastic optimal control (SOC), we formulate subject disentanglement as control over a trained FM sampler. This yields two architecture-agnostic algorithms: (i) a training-free test-time controller that perturbs the base velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight fine-tuning rule that regresses a control network to a backward adjoint signal while preserving base-model capabilities. The same formulation unifies prior attention heuristics, extends to diffusion models via a flow-diffusion correspondence, and provides the first fine-tuning route explicitly designed for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and Stable Diffusion XL, both algorithms consistently improve multi-subject alignment while maintaining base-model style. Test-time control runs efficiently on commodity GPUs, and fine-tuned controllers trained on limited prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal Control for Unentangled Subjects), which achieves state-of-the-art multi-subject fidelity across models.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [33] [Towards Photonic Band Diagram Generation with Transformer-Latent Diffusion Models](https://arxiv.org/abs/2510.01749)
*Valentin Delchevalerie,Nicolas Roy,Arnaud Bougaham,Alexandre Mayer,Benoît Frénay,Michaël Lobet*

Main category: physics.optics

TL;DR: 提出了首个基于扩散模型的光子晶体能带图生成方法，结合Transformer编码器和潜在扩散模型，能够扩展到任意三维结构。


<details>
  <summary>Details</summary>
Motivation: 计算光子能带图需要求解大量麦克斯韦方程，数值计算成本高昂，特别是在逆向设计优化循环中。

Method: 将Transformer编码器与潜在扩散模型相结合，Transformer提取输入结构的上下文嵌入，扩散模型生成对应的能带图。

Result: 开发了能够生成光子能带图的方法，为光子学领域的新代理建模策略铺平道路。

Conclusion: Transformer和扩散模型能够有效捕捉光子学中复杂的干涉和散射现象，为光子晶体能带图生成提供了高效解决方案。

Abstract: Photonic crystals enable fine control over light propagation at the nanoscale, and thus play a central role in the development of photonic and quantum technologies. Photonic band diagrams (BDs) are a key tool to investigate light propagation into such inhomogeneous structured materials. However, computing BDs requires solving Maxwell's equations across many configurations, making it numerically expensive, especially when embedded in optimization loops for inverse design techniques, for example. To address this challenge, we introduce the first approach for BD generation based on diffusion models, with the capacity to later generalize and scale to arbitrary three dimensional structures. Our method couples a transformer encoder, which extracts contextual embeddings from the input structure, with a latent diffusion model to generate the corresponding BD. In addition, we provide insights into why transformers and diffusion models are well suited to capture the complex interference and scattering phenomena inherent to photonics, paving the way for new surrogate modeling strategies in this domain.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [34] [Aligning Video Models with Human Social Judgments via Behavior-Guided Fine-Tuning](https://arxiv.org/abs/2510.01502)
*Kathy Garcia,Leyla Isik*

Main category: q-bio.NC

TL;DR: 研究发现预训练视频模型在社交视频相似性判断上与人类感知存在差距，语言模型反而表现更好。通过人类行为数据微调视频模型，可以显著提升与人类社交感知的对齐度。


<details>
  <summary>Details</summary>
Motivation: 探究现代AI模型是否能捕捉人类在视觉场景中感知的复杂社交信号，以及如何通过人类行为数据将这种相似性结构注入模型。

Method: 构建包含49,000多个三选一相似性判断的基准数据集，使用混合三元组-RSA目标通过LoRA微调TimeSformer视频模型，使其成对距离与人类相似性对齐。

Result: 微调后的视频模型在保留视频上显著提高了与人类感知的对齐度，方差分解显示该模型增加了与语言嵌入的共享方差，并解释了语言模型未捕获的独特方差。

Conclusion: 预训练视频模型在社交识别方面存在差距，行为引导的微调能够塑造视频表示，使其更接近人类社交感知。

Abstract: Humans intuitively perceive complex social signals in visual scenes, yet it remains unclear whether state-of-the-art AI models encode the same similarity structure. We study (Q1) whether modern video and language models capture human-perceived similarity in social videos, and (Q2) how to instill this structure into models using human behavioral data. To address this, we introduce a new benchmark of over 49,000 odd-one-out similarity judgments on 250 three-second video clips of social interactions, and discover a modality gap: despite the task being visual, caption-based language embeddings align better with human similarity than any pretrained video model. We close this gap by fine-tuning a TimeSformer video model on these human judgments with our novel hybrid triplet-RSA objective using low-rank adaptation (LoRA), aligning pairwise distances to human similarity. This fine-tuning protocol yields significantly improved alignment with human perceptions on held-out videos in terms of both explained variance and odd-one-out triplet accuracy. Variance partitioning shows that the fine-tuned video model increases shared variance with language embeddings and explains additional unique variance not captured by the language model. Finally, we test transfer via linear probes and find that human-similarity fine-tuning strengthens the encoding of social-affective attributes (intimacy, valence, dominance, communication) relative to the pretrained baseline. Overall, our findings highlight a gap in pretrained video models' social recognition and demonstrate that behavior-guided fine-tuning shapes video representations toward human social perception.

</details>


### [35] [Uncovering Semantic Selectivity of Latent Groups in Higher Visual Cortex with Mutual Information-Guided Diffusion](https://arxiv.org/abs/2510.02182)
*Yule Wang,Joseph Yu,Chengrui Li,Weihan Li,Anqi Wu*

Main category: q-bio.NC

TL;DR: 提出MIG-Vis方法，利用扩散模型可视化神经潜在子空间中编码的视觉语义特征，发现高级视觉皮层中存在结构化语义表征


<details>
  <summary>Details</summary>
Motivation: 理解高级视觉区域中神经群体如何编码以物体为中心的视觉信息，探索特征特异性视觉信息在神经群体中的分布和组织结构

Method: 使用变分自编码器推断神经群体的组间解缠神经潜在子空间，然后提出互信息引导的扩散合成过程来可视化每个潜在组编码的特定视觉语义特征

Result: 在两只猕猴颞下皮层多会话神经发放数据集上验证，发现神经潜在组对多样化视觉特征具有清晰的语义选择性，包括物体姿态、类别间转换和类内内容

Conclusion: 为高级视觉皮层中存在结构化语义表征提供了直接、可解释的证据，推进了对编码原理的理解

Abstract: Understanding how neural populations in higher visual areas encode object-centered visual information remains a central challenge in computational neuroscience. Prior works have investigated representational alignment between artificial neural networks and the visual cortex. Nevertheless, these findings are indirect and offer limited insights to the structure of neural populations themselves. Similarly, decoding-based methods have quantified semantic features from neural populations but have not uncovered their underlying organizations. This leaves open a scientific question: "how feature-specific visual information is distributed across neural populations in higher visual areas, and whether it is organized into structured, semantically meaningful subspaces." To tackle this problem, we present MIG-Vis, a method that leverages the generative power of diffusion models to visualize and validate the visual-semantic attributes encoded in neural latent subspaces. Our method first uses a variational autoencoder to infer a group-wise disentangled neural latent subspace from neural populations. Subsequently, we propose a mutual information (MI)-guided diffusion synthesis procedure to visualize the specific visual-semantic features encoded by each latent group. We validate MIG-Vis on multi-session neural spiking datasets from the inferior temporal (IT) cortex of two macaques. The synthesized results demonstrate that our method identifies neural latent groups with clear semantic selectivity to diverse visual features, including object pose, inter-category transformations, and intra-class content. These findings provide direct, interpretable evidence of structured semantic representation in the higher visual cortex and advance our understanding of its encoding principles.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [36] [An Efficient Quality Metric for Video Frame Interpolation Based on Motion-Field Divergence](https://arxiv.org/abs/2510.01361)
*Conall Daly,Darren Ramsook,Anil Kokaram*

Main category: eess.IV

TL;DR: 提出了一种新的视频帧插值质量评估指标PSNR_DIV，通过运动发散加权增强PSNR，在BVI-VFI数据集上相比FloLPIPS提升了0.09皮尔逊相关系数，同时速度提升2.5倍、内存使用减少4倍。


<details>
  <summary>Details</summary>
Motivation: 现有视频帧插值质量评估指标（如PSNR、SSIM、LPIPS）忽略时间一致性，而专门针对帧插值的指标（如FloLPIPS）计算效率低，限制了实际应用。

Method: 提出了PSNR_DIV指标，通过运动发散加权来增强PSNR，该方法改编自档案胶片修复中检测时间不一致性的技术，突出运动场中的奇点来加权图像误差。

Result: 在BVI-VFI数据集（包含180个序列，涵盖多种帧率、分辨率和插值方法）上的评估显示，PSNR_DIV相比FloLPIPS皮尔逊线性相关系数提升0.09，速度提升2.5倍，内存使用减少4倍。

Conclusion: PSNR_DIV的高效性和准确性使其能够快速评估质量，并可作为训练神经网络进行视频帧插值任务的损失函数。

Abstract: Video frame interpolation is a fundamental tool for temporal video enhancement, but existing quality metrics struggle to evaluate the perceptual impact of interpolation artefacts effectively. Metrics like PSNR, SSIM and LPIPS ignore temporal coherence. State-of-the-art quality metrics tailored towards video frame interpolation, like FloLPIPS, have been developed but suffer from computational inefficiency that limits their practical application. We present $\text{PSNR}_{\text{DIV}}$, a novel full-reference quality metric that enhances PSNR through motion divergence weighting, a technique adapted from archival film restoration where it was developed to detect temporal inconsistencies. Our approach highlights singularities in motion fields which is then used to weight image errors. Evaluation on the BVI-VFI dataset (180 sequences across multiple frame rates, resolutions and interpolation methods) shows $\text{PSNR}_{\text{DIV}}$ achieves statistically significant improvements: +0.09 Pearson Linear Correlation Coefficient over FloLPIPS, while being 2.5$\times$ faster and using 4$\times$ less memory. Performance remains consistent across all content categories and are robust to the motion estimator used. The efficiency and accuracy of $\text{PSNR}_{\text{DIV}}$ enables fast quality evaluation and practical use as a loss function for training neural networks for video frame interpolation tasks. An implementation of our metric is available at www.github.com/conalld/psnr-div.

</details>


### [37] [Median2Median: Zero-shot Suppression of Structured Noise in Images](https://arxiv.org/abs/2510.01666)
*Jianxu Wang,Ge Wang*

Main category: eess.IV

TL;DR: 提出了Median2Median（M2M）零样本去噪框架，专门针对结构化噪声设计，通过创新的采样策略生成伪独立子图像对，在保持i.i.d.噪声性能的同时，显著提升了相关噪声下的去噪效果。


<details>
  <summary>Details</summary>
Motivation: 现实世界图像常被具有强各向异性相关的结构化噪声退化，现有方法难以有效去除。数据驱动方法依赖高质量标注数据且泛化性有限，而零样本方法虽然避免了数据依赖，但仅对独立同分布噪声有效。

Method: 提出新颖的采样策略，从单个噪声输入生成伪独立子图像对：使用方向性插值和广义中值滤波自适应排除结构化伪影的失真值；采用随机分配策略扩大有效采样空间并消除系统偏差，使采样子图像对适用于Noise2Noise训练。

Result: 在真实模拟研究中，M2M在i.i.d.噪声下与最先进的零样本方法性能相当，在相关噪声下始终优于这些方法。

Conclusion: M2M为结构化噪声抑制提供了一种高效、无需数据的解决方案，标志着在超越严格i.i.d.假设方面迈出了有效零样本去噪的第一步。

Abstract: Image denoising is a fundamental problem in computer vision and medical imaging. However, real-world images are often degraded by structured noise with strong anisotropic correlations that existing methods struggle to remove. Most data-driven approaches rely on large datasets with high-quality labels and still suffer from limited generalizability, whereas existing zero-shot methods avoid this limitation but remain effective only for independent and identically distributed (i.i.d.) noise. To address this gap, we propose Median2Median (M2M), a zero-shot denoising framework designed for structured noise. M2M introduces a novel sampling strategy that generates pseudo-independent sub-image pairs from a single noisy input. This strategy leverages directional interpolation and generalized median filtering to adaptively exclude values distorted by structured artifacts. To further enlarge the effective sampling space and eliminate systematic bias, a randomized assignment strategy is employed, ensuring that the sampled sub-image pairs are suitable for Noise2Noise training. In our realistic simulation studies, M2M performs on par with state-of-the-art zero-shot methods under i.i.d. noise, while consistently outperforming them under correlated noise. These findings establish M2M as an efficient, data-free solution for structured noise suppression and mark the first step toward effective zero-shot denoising beyond the strict i.i.d. assumption.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [38] [Unsupervised Dynamic Feature Selection for Robust Latent Spaces in Vision Tasks](https://arxiv.org/abs/2510.01758)
*Bruno Corcuera,Carlos Eiras-Franco,Brais Cancela*

Main category: cs.LG

TL;DR: 提出了一种无监督动态特征选择方法，通过移除图像中的误导性或冗余信息来增强潜在表示，提高模型的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 视觉任务中的潜在表示常受噪声或不相关特征影响，这会降低模型性能和泛化能力。

Method: 采用无监督动态特征选择框架，为每个实例识别并移除图像中的误导或冗余信息，确保只有最相关特征贡献于潜在空间。

Result: 在图像数据集上的实验表明，配备无监督DFS的模型在聚类和图像生成等任务中显著提升了泛化性能，计算成本仅轻微增加。

Conclusion: 无监督动态特征选择方法能有效增强潜在表示，提高模型性能，且具有广泛适用性。

Abstract: Latent representations are critical for the performance and robustness of machine learning models, as they encode the essential features of data in a compact and informative manner. However, in vision tasks, these representations are often affected by noisy or irrelevant features, which can degrade the model's performance and generalization capabilities. This paper presents a novel approach for enhancing latent representations using unsupervised Dynamic Feature Selection (DFS). For each instance, the proposed method identifies and removes misleading or redundant information in images, ensuring that only the most relevant features contribute to the latent space. By leveraging an unsupervised framework, our approach avoids reliance on labeled data, making it broadly applicable across various domains and datasets. Experiments conducted on image datasets demonstrate that models equipped with unsupervised DFS achieve significant improvements in generalization performance across various tasks, including clustering and image generation, while incurring a minimal increase in the computational cost.

</details>


### [39] [$\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models](https://arxiv.org/abs/2510.01982)
*Yujie Zhou,Pengyang Ling,Jiazi Bu,Yibin Wang,Yuhang Zang,Jiaqi Wang,Li Niu,Guangtao Zhai*

Main category: cs.LG

TL;DR: 提出Granular-GRPO框架，通过奇异随机采样策略和多粒度优势集成模块，解决流模型强化学习中奖励信号稀疏和窄化的问题，实现更精确全面的采样方向评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法在探索潜在高价值样本方面有效，但由于稀疏和窄化的奖励信号，导致偏好对齐效果不佳。

Method: 1. 奇异随机采样策略：支持逐步随机探索，增强奖励与注入噪声的相关性；2. 多粒度优势集成模块：聚合多个扩散尺度的优势计算，提供更全面的采样方向评估。

Result: 在各种奖励模型上的实验表明，G²RPO显著优于现有的基于流的GRPO基线方法，证明了其有效性和鲁棒性。

Conclusion: G²RPO框架通过改进的采样策略和多粒度评估机制，实现了更精确的偏好对齐，为流模型的强化学习提供了有效解决方案。

Abstract: The integration of online reinforcement learning (RL) into diffusion and flow models has recently emerged as a promising approach for aligning generative models with human preferences. Stochastic sampling via Stochastic Differential Equations (SDE) is employed during the denoising process to generate diverse denoising directions for RL exploration. While existing methods effectively explore potential high-value samples, they suffer from sub-optimal preference alignment due to sparse and narrow reward signals. To address these challenges, we propose a novel Granular-GRPO ($\text{G}^2$RPO ) framework that achieves precise and comprehensive reward assessments of sampling directions in reinforcement learning of flow models. Specifically, a Singular Stochastic Sampling strategy is introduced to support step-wise stochastic exploration while enforcing a high correlation between the reward and the injected noise, thereby facilitating a faithful reward for each SDE perturbation. Concurrently, to eliminate the bias inherent in fixed-granularity denoising, we introduce a Multi-Granularity Advantage Integration module that aggregates advantages computed at multiple diffusion scales, producing a more comprehensive and robust evaluation of the sampling directions. Experiments conducted on various reward models, including both in-domain and out-of-domain evaluations, demonstrate that our $\text{G}^2$RPO significantly outperforms existing flow-based GRPO baselines,highlighting its effectiveness and robustness.

</details>


### [40] [Continual Personalization for Diffusion Models](https://arxiv.org/abs/2510.02296)
*Yu-Chien Liao,Jr-Jen Chen,Chi-Pin Huang,Ci-Siang Lin,Meng-Lin Wu,Yu-Chiang Frank Wang*

Main category: cs.LG

TL;DR: 提出概念神经元选择(CNS)方法，通过识别和微调与目标概念相关的神经元，在扩散模型中实现增量式个性化学习，避免灾难性遗忘并保持零样本生成能力。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中增量更新扩散模型具有实用性，但计算上具有挑战性。需要解决灾难性遗忘问题，同时保持模型的零样本文本到图像生成能力。

Method: CNS方法独特识别扩散模型中与目标概念密切相关的神经元，以增量方式微调这些概念神经元，并联合保留先前概念的知识。

Result: 在真实数据集上的评估显示，CNS以最少的参数调整实现了最先进的性能，在单概念和多概念个性化任务中都优于先前方法。

Conclusion: CNS实现了无融合操作，减少了持续个性化所需的内存存储和处理时间，为扩散模型的增量学习提供了有效的解决方案。

Abstract: Updating diffusion models in an incremental setting would be practical in real-world applications yet computationally challenging. We present a novel learning strategy of Concept Neuron Selection (CNS), a simple yet effective approach to perform personalization in a continual learning scheme. CNS uniquely identifies neurons in diffusion models that are closely related to the target concepts. In order to mitigate catastrophic forgetting problems while preserving zero-shot text-to-image generation ability, CNS finetunes concept neurons in an incremental manner and jointly preserves knowledge learned of previous concepts. Evaluation of real-world datasets demonstrates that CNS achieves state-of-the-art performance with minimal parameter adjustments, outperforming previous methods in both single and multi-concept personalization works. CNS also achieves fusion-free operation, reducing memory storage and processing time for continual personalization.

</details>


### [41] [Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models](https://arxiv.org/abs/2510.02300)
*Runqian Wang,Yilun Du*

Main category: cs.LG

TL;DR: 提出Equilibrium Matching (EqM)生成模型框架，通过平衡动力学视角学习隐式能量景观的平衡梯度，采用基于优化的采样过程，在ImageNet 256×256上达到1.90 FID，并支持多种任务。


<details>
  <summary>Details</summary>
Motivation: 传统扩散和流模型使用非平衡、时间条件动力学，EqM旨在通过平衡动力学视角简化生成建模，建立流模型与能量模型之间的桥梁。

Method: EqM放弃传统模型的时间条件动力学，学习隐式能量景观的平衡梯度，在推理时采用基于优化的采样过程，使用可调步长、自适应优化器和自适应计算。

Result: 在ImageNet 256×256上达到1.90 FID，超越扩散/流模型性能，并能处理部分噪声图像去噪、OOD检测和图像合成等任务。

Conclusion: EqM通过统一平衡景观取代时间条件速度，提供了流模型与能量模型之间更紧密的桥梁，以及优化驱动推理的简单路径。

Abstract: We introduce Equilibrium Matching (EqM), a generative modeling framework built from an equilibrium dynamics perspective. EqM discards the non-equilibrium, time-conditional dynamics in traditional diffusion and flow-based generative models and instead learns the equilibrium gradient of an implicit energy landscape. Through this approach, we can adopt an optimization-based sampling process at inference time, where samples are obtained by gradient descent on the learned landscape with adjustable step sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation performance of diffusion/flow models empirically, achieving an FID of 1.90 on ImageNet 256$\times$256. EqM is also theoretically justified to learn and sample from the data manifold. Beyond generation, EqM is a flexible framework that naturally handles tasks including partially noised image denoising, OOD detection, and image composition. By replacing time-conditional velocities with a unified equilibrium landscape, EqM offers a tighter bridge between flow and energy-based models and a simple route to optimization-driven inference.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [42] [ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs](https://arxiv.org/abs/2510.01967)
*Aadarsh Anantha Ramakrishnan,Shubham Agarwal,Selvanayagam S,Kunwar Singh*

Main category: cs.CR

TL;DR: ZK-WAGON是一个基于ZK-SNARKs的图像生成模型水印系统，能够在不暴露模型权重或敏感信息的情况下提供可验证的来源证明。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成模型变得越来越强大和易用，合成媒体的真实性、所有权和滥用问题变得至关重要。传统水印方法要么会降低图像质量，要么容易被移除，或者需要访问机密模型内部信息，无法实现安全可扩展的部署。

Method: 提出选择性层ZK电路创建(SL-ZKCC)方法，选择性地将图像生成模型的关键层转换为电路，显著减少证明生成时间。生成的ZK-SNARK证明通过最低有效位(LSB)隐写术不可感知地嵌入到生成的图像中。

Result: 该系统在GAN和Diffusion模型上进行了演示，为可信AI图像生成提供了一个安全、模型无关的流水线。

Conclusion: ZK-WAGON是首个使用ZK-SNARKs为图像生成模型添加水印的系统，能够在保护模型机密性的同时提供可验证的来源证明。

Abstract: As image generation models grow increasingly powerful and accessible, concerns around authenticity, ownership, and misuse of synthetic media have become critical. The ability to generate lifelike images indistinguishable from real ones introduces risks such as misinformation, deepfakes, and intellectual property violations. Traditional watermarking methods either degrade image quality, are easily removed, or require access to confidential model internals - making them unsuitable for secure and scalable deployment. We are the first to introduce ZK-WAGON, a novel system for watermarking image generation models using the Zero-Knowledge Succinct Non Interactive Argument of Knowledge (ZK-SNARKs). Our approach enables verifiable proof of origin without exposing model weights, generation prompts, or any sensitive internal information. We propose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively convert key layers of an image generation model into a circuit, reducing proof generation time significantly. Generated ZK-SNARK proofs are imperceptibly embedded into a generated image via Least Significant Bit (LSB) steganography. We demonstrate this system on both GAN and Diffusion models, providing a secure, model-agnostic pipeline for trustworthy AI image generation.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [43] [Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation](https://arxiv.org/abs/2510.01284)
*Chetwin Low,Weimin Wang,Calder Katyal*

Main category: cs.MM

TL;DR: Ovi是一个统一的音频-视频生成范式，通过块状跨模态融合的双DiT模块将两种模态建模为单一生成过程，实现自然同步，无需单独流程或后处理对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的音频-视频生成方法通常依赖复杂的多阶段架构或声音与视觉的顺序合成，这导致了同步问题和额外的对齐需求。

Method: 使用块状跨模态融合的双DiT模块，通过初始化与预训练视频模型架构相同的音频塔，在大量原始音频上训练音频塔学习生成逼真音效和富有表现力的语音，然后通过时间（缩放RoPE嵌入）和语义（双向交叉注意力）的块状交换联合训练视频和音频塔。

Result: 模型能够实现电影级的视频片段生成，具有自然语音和准确、上下文匹配的音效，支持电影叙事。

Conclusion: Ovi提供了一个统一的音频-视频生成框架，通过跨模态融合实现了高质量的同步生成，为电影级内容创作提供了有效解决方案。

Abstract: Audio-video generation has often relied on complex multi-stage architectures or sequential synthesis of sound and visuals. We introduce Ovi, a unified paradigm for audio-video generation that models the two modalities as a single generative process. By using blockwise cross-modal fusion of twin-DiT modules, Ovi achieves natural synchronization and removes the need for separate pipelines or post hoc alignment. To facilitate fine-grained multimodal fusion modeling, we initialize an audio tower with an architecture identical to that of a strong pretrained video model. Trained from scratch on hundreds of thousands of hours of raw audio, the audio tower learns to generate realistic sound effects, as well as speech that conveys rich speaker identity and emotion. Fusion is obtained by jointly training the identical video and audio towers via blockwise exchange of timing (via scaled-RoPE embeddings) and semantics (through bidirectional cross-attention) on a vast video corpus. Our model enables cinematic storytelling with natural speech and accurate, context-matched sound effects, producing movie-grade video clips. All the demos, code and model weights are published at https://aaxwaz.github.io/Ovi

</details>
