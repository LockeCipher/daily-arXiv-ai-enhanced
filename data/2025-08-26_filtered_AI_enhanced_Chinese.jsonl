{"id": "2508.16696", "pdf": "https://arxiv.org/pdf/2508.16696", "abs": "https://arxiv.org/abs/2508.16696", "authors": ["Reema Alshehri", "Rawan Alotaibi", "Leen Almasri", "Rawan Altaweel"], "title": "DecoMind: A Generative AI System for Personalized Interior Design Layouts", "categories": ["cs.GR", "cs.AI"], "comment": "~7 pages; ~32 figures; compiled with pdfLaTeX. Primary category:   cs.CV. (Secondary: cs.AI)", "summary": "This paper introduces a system for generating interior design layouts based on user inputs, such as room type, style, and furniture preferences. CLIP extracts relevant furniture from a dataset, and a layout that contains furniture and a prompt are fed to Stable Diffusion with ControlNet to generate a design that incorporates the selected furniture. The design is then evaluated by classifiers to ensure alignment with the user's inputs, offering an automated solution for realistic interior design.", "AI": {"tldr": "\u57fa\u4e8e\u7528\u6237\u8f93\u5165\u81ea\u52a8\u751f\u6210\u5ba4\u5185\u8bbe\u8ba1\u5e03\u5c40\u7684\u7cfb\u7edf\uff0c\u4f7f\u7528CLIP\u9009\u62e9\u5bb6\u5177\u3001Stable Diffusion\u751f\u6210\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u5206\u7c7b\u5668\u9a8c\u8bc1\u8bbe\u8ba1\u8d28\u91cf", "motivation": "\u4e3a\u7528\u6237\u63d0\u4f9b\u81ea\u52a8\u5316\u3001\u4e2a\u6027\u5316\u7684\u5ba4\u5185\u8bbe\u8ba1\u65b9\u6848\uff0c\u7b80\u5316\u4f20\u7edf\u5ba4\u5185\u8bbe\u8ba1\u6d41\u7a0b\uff0c\u63d0\u9ad8\u8bbe\u8ba1\u6548\u7387\u548c\u7528\u6237\u6ee1\u610f\u5ea6", "method": "\u4f7f\u7528CLIP\u4ece\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u76f8\u5173\u5bb6\u5177\uff0c\u901a\u8fc7Stable Diffusion\u7ed3\u5408ControlNet\u751f\u6210\u5305\u542b\u9009\u5b9a\u5bb6\u5177\u7684\u8bbe\u8ba1\u5e03\u5c40\uff0c\u6700\u540e\u7528\u5206\u7c7b\u5668\u9a8c\u8bc1\u8bbe\u8ba1\u662f\u5426\u7b26\u5408\u7528\u6237\u8f93\u5165\u8981\u6c42", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u6839\u636e\u7528\u6237\u504f\u597d\u81ea\u52a8\u751f\u6210\u903c\u771f\u5ba4\u5185\u8bbe\u8ba1\u5e03\u5c40\u7684\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u4ece\u5bb6\u5177\u9009\u62e9\u5230\u6700\u7ec8\u8bbe\u8ba1\u751f\u6210\u7684\u5168\u6d41\u7a0b\u81ea\u52a8\u5316", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u5ba4\u5185\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u751f\u6210\u7b26\u5408\u7528\u6237\u9700\u6c42\u7684\u903c\u771f\u8bbe\u8ba1\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2508.17011", "pdf": "https://arxiv.org/pdf/2508.17011", "abs": "https://arxiv.org/abs/2508.17011", "authors": ["Jinxi Wang", "Ben Fei", "Dasith de Silva Edirimuni", "Zheng Liu", "Ying He", "Xuequan Lu"], "title": "A Survey of Deep Learning-based Point Cloud Denoising", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Accurate 3D geometry acquisition is essential for a wide range of applications, such as computer graphics, autonomous driving, robotics, and augmented reality. However, raw point clouds acquired in real-world environments are often corrupted with noise due to various factors such as sensor, lighting, material, environment etc, which reduces geometric fidelity and degrades downstream performance. Point cloud denoising is a fundamental problem, aiming to recover clean point sets while preserving underlying structures. Classical optimization-based methods, guided by hand-crafted filters or geometric priors, have been extensively studied but struggle to handle diverse and complex noise patterns. Recent deep learning approaches leverage neural network architectures to learn distinctive representations and demonstrate strong outcomes, particularly on complex and large-scale point clouds. Provided these significant advances, this survey provides a comprehensive and up-to-date review of deep learning-based point cloud denoising methods up to August 2025. We organize the literature from two perspectives: (1) supervision level (supervised vs. unsupervised), and (2) modeling perspective, proposing a functional taxonomy that unifies diverse approaches by their denoising principles. We further analyze architectural trends both structurally and chronologically, establish a unified benchmark with consistent training settings, and evaluate methods in terms of denoising quality, surface fidelity, point distribution, and computational efficiency. Finally, we discuss open challenges and outline directions for future research in this rapidly evolving field.", "AI": {"tldr": "\u672c\u6587\u662f\u5173\u4e8e\u6df1\u5ea6\u5b66\u4e60\u70b9\u4e91\u53bb\u566a\u65b9\u6cd5\u7684\u7efc\u8ff0\uff0c\u6db5\u76d6\u4e86\u76d1\u7763\u548c\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u529f\u80fd\u5206\u7c7b\u6cd5\uff0c\u5efa\u7acb\u4e86\u7edf\u4e00\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u771f\u5b9e\u73af\u5883\u4e2d\u83b7\u53d6\u7684\u70b9\u4e91\u6570\u636e\u5e38\u56e0\u4f20\u611f\u5668\u3001\u5149\u7167\u3001\u6750\u8d28\u7b49\u56e0\u7d20\u800c\u5305\u542b\u566a\u58f0\uff0c\u5f71\u54cd\u51e0\u4f55\u4fdd\u771f\u5ea6\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u566a\u58f0\u6a21\u5f0f\uff0c\u6df1\u5ea6\u5b66\u4e60\u5c55\u73b0\u51fa\u5f3a\u5927\u6f5c\u529b\u3002", "method": "\u4ece\u4e24\u4e2a\u7ef4\u5ea6\u7ec4\u7ec7\u6587\u732e\uff1a(1)\u76d1\u7763\u7ea7\u522b\uff08\u76d1\u7763vs\u65e0\u76d1\u7763\uff09(2)\u5efa\u6a21\u89c6\u89d2\uff0c\u63d0\u51fa\u57fa\u4e8e\u53bb\u566a\u539f\u7406\u7684\u529f\u80fd\u5206\u7c7b\u6cd5\u3002\u5efa\u7acb\u7edf\u4e00\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u53bb\u566a\u8d28\u91cf\u3001\u8868\u9762\u4fdd\u771f\u5ea6\u3001\u70b9\u5206\u5e03\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u63d0\u4f9b\u4e86\u622a\u81f32025\u5e748\u6708\u7684\u6df1\u5ea6\u5b66\u4e60\u70b9\u4e91\u53bb\u566a\u65b9\u6cd5\u5168\u9762\u7efc\u8ff0\uff0c\u5206\u6790\u4e86\u67b6\u6784\u8d8b\u52bf\uff0c\u5efa\u7acb\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u3002", "conclusion": "\u70b9\u4e91\u53bb\u566a\u662f\u57fa\u7840\u6027\u95ee\u9898\uff0c\u6df1\u5ea6\u5b66\u4e60\u5728\u8be5\u9886\u57df\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u5b58\u5728\u5f00\u653e\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2508.16579", "pdf": "https://arxiv.org/pdf/2508.16579", "abs": "https://arxiv.org/abs/2508.16579", "authors": ["Yansong Du", "Yutong Deng", "Yuting Zhou", "Feiyu Jiao", "Jian Song", "Xun Guan"], "title": "Towards High-Precision Depth Sensing via Monocular-Aided iToF and RGB Integration", "categories": ["cs.CV"], "comment": "7 pages, 5 figures", "summary": "This paper presents a novel iToF-RGB fusion framework designed to address the inherent limitations of indirect Time-of-Flight (iToF) depth sensing, such as low spatial resolution, limited field-of-view (FoV), and structural distortion in complex scenes. The proposed method first reprojects the narrow-FoV iToF depth map onto the wide-FoV RGB coordinate system through a precise geometric calibration and alignment module, ensuring pixel-level correspondence between modalities. A dual-encoder fusion network is then employed to jointly extract complementary features from the reprojected iToF depth and RGB image, guided by monocular depth priors to recover fine-grained structural details and perform depth super-resolution. By integrating cross-modal structural cues and depth consistency constraints, our approach achieves enhanced depth accuracy, improved edge sharpness, and seamless FoV expansion. Extensive experiments on both synthetic and real-world datasets demonstrate that the proposed framework significantly outperforms state-of-the-art methods in terms of accuracy, structural consistency, and visual quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684iToF-RGB\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u6821\u51c6\u548c\u53cc\u7f16\u7801\u5668\u7f51\u7edc\uff0c\u89e3\u51b3\u4e86iToF\u6df1\u5ea6\u4f20\u611f\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\u4f4e\u3001\u89c6\u573a\u7a84\u548c\u7ed3\u6784\u5931\u771f\u7b49\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6df1\u5ea6\u8d85\u5206\u8fa8\u7387\u548c\u89c6\u573a\u6269\u5c55\u3002", "motivation": "\u95f4\u63a5\u98de\u884c\u65f6\u95f4(iToF)\u6df1\u5ea6\u4f20\u611f\u5b58\u5728\u7a7a\u95f4\u5206\u8fa8\u7387\u4f4e\u3001\u89c6\u573a\u6709\u9650\u4ee5\u53ca\u5728\u590d\u6742\u573a\u666f\u4e2d\u7ed3\u6784\u5931\u771f\u7684\u56fa\u6709\u9650\u5236\uff0c\u9700\u8981\u4e0eRGB\u56fe\u50cf\u878d\u5408\u6765\u63d0\u5347\u6df1\u5ea6\u611f\u77e5\u8d28\u91cf\u3002", "method": "\u9996\u5148\u901a\u8fc7\u7cbe\u786e\u7684\u51e0\u4f55\u6821\u51c6\u5c06\u7a84\u89c6\u573aiToF\u6df1\u5ea6\u56fe\u91cd\u6295\u5f71\u5230\u5bbd\u89c6\u573aRGB\u5750\u6807\u7cfb\uff0c\u7136\u540e\u4f7f\u7528\u53cc\u7f16\u7801\u5668\u878d\u5408\u7f51\u7edc\u8054\u5408\u63d0\u53d6iToF\u6df1\u5ea6\u548cRGB\u56fe\u50cf\u7684\u4e92\u8865\u7279\u5f81\uff0c\u5e76\u5229\u7528\u5355\u76ee\u6df1\u5ea6\u5148\u9a8c\u6062\u590d\u7cbe\u7ec6\u7ed3\u6784\u7ec6\u8282\u548c\u8fdb\u884c\u6df1\u5ea6\u8d85\u5206\u8fa8\u7387\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u8de8\u6a21\u6001\u7ed3\u6784\u7ebf\u7d22\u548c\u6df1\u5ea6\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u5b9e\u73b0\u4e86\u589e\u5f3a\u7684\u6df1\u5ea6\u51c6\u786e\u6027\u3001\u6539\u8fdb\u7684\u8fb9\u7f18\u9510\u5ea6\u548c\u65e0\u7f1d\u7684\u89c6\u573a\u6269\u5c55\uff0c\u4e3aiToF\u6df1\u5ea6\u611f\u77e5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.16644", "pdf": "https://arxiv.org/pdf/2508.16644", "abs": "https://arxiv.org/abs/2508.16644", "authors": ["Anindya Mondal", "Ayan Banerjee", "Sauradip Nag", "Josep Llad\u00f3s", "Xiatian Zhu", "Anjan Dutta"], "title": "CountLoop: Training-Free High-Instance Image Generation via Iterative Agent Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have shown remarkable progress in photorealistic image synthesis, yet they remain unreliable for generating scenes with a precise number of object instances, particularly in complex and high-density settings. We present CountLoop, a training-free framework that provides diffusion models with accurate instance control through iterative structured feedback. The approach alternates between image generation and multimodal agent evaluation, where a language-guided planner and critic assess object counts, spatial arrangements, and attribute consistency. This feedback is then used to refine layouts and guide subsequent generations. To further improve separation between objects, especially in occluded scenes, we introduce instance-driven attention masking and compositional generation techniques. Experiments on COCO Count, T2I CompBench, and two new high-instance benchmarks show that CountLoop achieves counting accuracy of up to 98% while maintaining spatial fidelity and visual quality, outperforming layout-based and gradient-guided baselines with a score of 0.97.", "AI": {"tldr": "CountLoop\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u7ed3\u6784\u5316\u53cd\u9988\u4e3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u7cbe\u786e\u7684\u5b9e\u4f8b\u63a7\u5236\uff0c\u5728\u590d\u6742\u9ad8\u5bc6\u5ea6\u573a\u666f\u4e2d\u5b9e\u73b0\u9ad8\u8fbe98%\u7684\u8ba1\u6570\u51c6\u786e\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u903c\u771f\u56fe\u50cf\u5408\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u751f\u6210\u5177\u6709\u7cbe\u786e\u5bf9\u8c61\u5b9e\u4f8b\u6570\u91cf\u7684\u573a\u666f\u65f6\u4ecd\u4e0d\u53ef\u9760\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u548c\u9ad8\u5bc6\u5ea6\u8bbe\u7f6e\u4e2d\u3002", "method": "\u91c7\u7528\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u50cf\u751f\u6210\u548c\u591a\u6a21\u6001\u4ee3\u7406\u8bc4\u4f30\u7684\u4ea4\u66ff\u8fc7\u7a0b\uff0c\u4f7f\u7528\u8bed\u8a00\u5f15\u5bfc\u7684\u89c4\u5212\u5668\u548c\u6279\u8bc4\u5668\u8bc4\u4f30\u5bf9\u8c61\u8ba1\u6570\u3001\u7a7a\u95f4\u5e03\u5c40\u548c\u5c5e\u6027\u4e00\u81f4\u6027\uff0c\u5e76\u5f15\u5165\u5b9e\u4f8b\u9a71\u52a8\u7684\u6ce8\u610f\u529b\u63a9\u7801\u548c\u7ec4\u5408\u751f\u6210\u6280\u672f\u6765\u6539\u5584\u5bf9\u8c61\u5206\u79bb\u3002", "result": "\u5728COCO Count\u3001T2I CompBench\u548c\u4e24\u4e2a\u65b0\u9ad8\u5b9e\u4f8b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCountLoop\u5b9e\u73b0\u4e86\u9ad8\u8fbe98%\u7684\u8ba1\u6570\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u7a7a\u95f4\u4fdd\u771f\u5ea6\u548c\u89c6\u89c9\u8d28\u91cf\uff0c\u4ee50.97\u7684\u5206\u6570\u4f18\u4e8e\u57fa\u4e8e\u5e03\u5c40\u548c\u68af\u5ea6\u5f15\u5bfc\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CountLoop\u901a\u8fc7\u8fed\u4ee3\u7ed3\u6784\u5316\u53cd\u9988\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u7cbe\u786e\u5b9e\u4f8b\u63a7\u5236\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u590d\u6742\u573a\u666f\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.17645", "pdf": "https://arxiv.org/pdf/2508.17645", "abs": "https://arxiv.org/abs/2508.17645", "authors": ["Xiaoyang Huang", "Bingbing Ni", "Wenjun Zhang"], "title": "Generating Human-AI Collaborative Design Sequence for 3D Assets via Differentiable Operation Graph", "categories": ["cs.GR"], "comment": null, "summary": "The emergence of 3D artificial intelligence-generated content (3D-AIGC) has enabled rapid synthesis of intricate geometries. However, a fundamental disconnect persists between AI-generated content and human-centric design paradigms, rooted in representational incompatibilities: conventional AI frameworks predominantly manipulate meshes or neural representations (\\emph{e.g.}, NeRF, Gaussian Splatting), while designers operate within parametric modeling tools. This disconnection diminishes the practical value of AI for 3D industry, undermining the efficiency of human-AI collaboration. To resolve this disparity, we focus on generating design operation sequences, which are structured modeling histories that comprehensively capture the step-by-step construction process of 3D assets and align with designers' typical workflows in modern 3D software. We first reformulate fundamental modeling operations (\\emph{e.g.}, \\emph{Extrude}, \\emph{Boolean}) into differentiable units, enabling joint optimization of continuous (\\emph{e.g.}, \\emph{Extrude} height) and discrete (\\emph{e.g.}, \\emph{Boolean} type) parameters via gradient-based learning. Based on these differentiable operations, a hierarchical graph with gating mechanism is constructed and optimized end-to-end by minimizing Chamfer Distance to target geometries. Multi-stage sequence length constraint and domain rule penalties enable unsupervised learning of compact design sequences without ground-truth sequence supervision. Extensive validation demonstrates that the generated operation sequences achieve high geometric fidelity, smooth mesh wiring, rational step composition and flexible editing capacity, with full compatibility within design industry.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06AI\u751f\u6210\u76843D\u5185\u5bb9\u4e0e\u53c2\u6570\u5316\u8bbe\u8ba1\u5de5\u5177\u8fde\u63a5\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u5efa\u6a21\u64cd\u4f5c\u548c\u5206\u5c42\u56fe\u4f18\u5316\u6765\u751f\u6210\u8bbe\u8ba1\u64cd\u4f5c\u5e8f\u5217\uff0c\u5b9e\u73b0\u9ad8\u51e0\u4f55\u4fdd\u771f\u5ea6\u548c\u884c\u4e1a\u517c\u5bb9\u6027\u3002", "motivation": "\u89e3\u51b3AI\u751f\u6210\u5185\u5bb9\u4e0e\u4eba\u7c7b\u8bbe\u8ba1\u8303\u5f0f\u4e4b\u95f4\u7684\u8868\u793a\u4e0d\u517c\u5bb9\u95ee\u9898\uff0c\u4f20\u7edfAI\u6846\u67b6\u4f7f\u7528\u7f51\u683c\u6216\u795e\u7ecf\u8868\u793a\uff0c\u800c\u8bbe\u8ba1\u5e08\u4f7f\u7528\u53c2\u6570\u5316\u5efa\u6a21\u5de5\u5177\uff0c\u8fd9\u79cd\u8131\u8282\u964d\u4f4e\u4e86AI\u57283D\u884c\u4e1a\u7684\u5b9e\u7528\u4ef7\u503c\u3002", "method": "\u5c06\u57fa\u672c\u5efa\u6a21\u64cd\u4f5c\uff08\u5982\u62c9\u4f38\u3001\u5e03\u5c14\u8fd0\u7b97\uff09\u91cd\u65b0\u8868\u8ff0\u4e3a\u53ef\u5fae\u5206\u5355\u5143\uff0c\u901a\u8fc7\u57fa\u4e8e\u68af\u5ea6\u7684\u5b66\u4e60\u8054\u5408\u4f18\u5316\u8fde\u7eed\u548c\u79bb\u6563\u53c2\u6570\uff1b\u6784\u5efa\u5177\u6709\u95e8\u63a7\u673a\u5236\u7684\u5206\u5c42\u56fe\uff0c\u901a\u8fc7\u6700\u5c0f\u5316Chamfer\u8ddd\u79bb\u8fdb\u884c\u7aef\u5230\u7aef\u4f18\u5316\uff1b\u4f7f\u7528\u591a\u9636\u6bb5\u5e8f\u5217\u957f\u5ea6\u7ea6\u675f\u548c\u9886\u57df\u89c4\u5219\u60e9\u7f5a\u5b9e\u73b0\u65e0\u76d1\u7763\u5b66\u4e60\u3002", "result": "\u751f\u6210\u7684\u64cd\u4f5c\u5e8f\u5217\u5b9e\u73b0\u4e86\u9ad8\u51e0\u4f55\u4fdd\u771f\u5ea6\u3001\u5e73\u6ed1\u7f51\u683c\u5e03\u7ebf\u3001\u5408\u7406\u7684\u6b65\u9aa4\u7ec4\u6210\u548c\u7075\u6d3b\u7684\u7f16\u8f91\u80fd\u529b\uff0c\u5b8c\u5168\u517c\u5bb9\u8bbe\u8ba1\u884c\u4e1a\u6807\u51c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5f25\u5408\u4e86AI\u751f\u6210\u5185\u5bb9\u4e0e\u4eba\u7c7b\u8bbe\u8ba1\u5de5\u4f5c\u6d41\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a3D\u884c\u4e1a\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9ad8\u6548\u7684AI\u534f\u4f5c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.16654", "pdf": "https://arxiv.org/pdf/2508.16654", "abs": "https://arxiv.org/abs/2508.16654", "authors": ["Chenghao Liu", "Zhimu Zhou", "Jiachen Zhang", "Minghao Zhang", "Songfang Huang", "Huiling Duan"], "title": "MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and LLM Spatial Reasoning", "categories": ["cs.CV"], "comment": "9 pages, 4 figures", "summary": "Vision-and-Language Navigation (VLN) requires an agent to interpret natural language instructions and navigate complex environments. Current approaches often adopt a \"black-box\" paradigm, where a single Large Language Model (LLM) makes end-to-end decisions. However, it is plagued by critical vulnerabilities, including poor spatial reasoning, weak cross-modal grounding, and memory overload in long-horizon tasks. To systematically address these issues, we propose Memory Spatial Navigation(MSNav), a framework that fuses three modules into a synergistic architecture, which transforms fragile inference into a robust, integrated intelligence. MSNav integrates three modules: Memory Module, a dynamic map memory module that tackles memory overload through selective node pruning, enhancing long-range exploration; Spatial Module, a module for spatial reasoning and object relationship inference that improves endpoint recognition; and Decision Module, a module using LLM-based path planning to execute robust actions. Powering Spatial Module, we also introduce an Instruction-Object-Space (I-O-S) dataset and fine-tune the Qwen3-4B model into Qwen-Spatial (Qwen-Sp), which outperforms leading commercial LLMs in object list extraction, achieving higher F1 and NDCG scores on the I-O-S test set. Extensive experiments on the Room-to-Room (R2R) and REVERIE datasets demonstrate MSNav's state-of-the-art performance with significant improvements in Success Rate (SR) and Success weighted by Path Length (SPL).", "AI": {"tldr": "MSNav\u662f\u4e00\u4e2a\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u8bb0\u5fc6\u3001\u7a7a\u95f4\u548c\u51b3\u7b56\u4e09\u4e2a\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7a7a\u95f4\u63a8\u7406\u3001\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u957f\u65f6\u8bb0\u5fc6\u65b9\u9762\u7684\u7f3a\u9677\uff0c\u5728R2R\u548cREVERIE\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7aef\u5230\u7aef\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5\u5b58\u5728\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u5f31\u3001\u8de8\u6a21\u6001\u5bf9\u9f50\u5dee\u3001\u957f\u65f6\u4efb\u52a1\u5185\u5b58\u8fc7\u8f7d\u7b49\u5173\u952e\u7f3a\u9677\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faMSNav\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u534f\u540c\u6a21\u5757\uff1a1)\u8bb0\u5fc6\u6a21\u5757-\u52a8\u6001\u5730\u56fe\u8bb0\u5fc6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u8282\u70b9\u526a\u679d\u5904\u7406\u5185\u5b58\u8fc7\u8f7d\uff1b2)\u7a7a\u95f4\u6a21\u5757-\u7a7a\u95f4\u63a8\u7406\u548c\u7269\u4f53\u5173\u7cfb\u63a8\u65ad\uff0c\u57fa\u4e8e\u65b0\u6784\u5efa\u7684I-O-S\u6570\u636e\u96c6\u5fae\u8c03Qwen3-4B\u6a21\u578b\uff1b3)\u51b3\u7b56\u6a21\u5757-\u57fa\u4e8eLLM\u7684\u8def\u5f84\u89c4\u5212\u6267\u884c\u52a8\u4f5c\u3002", "result": "\u5728Room-to-Room\u548cREVERIE\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMSNav\u5728\u6210\u529f\u7387\u548c\u8def\u5f84\u957f\u5ea6\u52a0\u6743\u6210\u529f\u7387\u7b49\u6307\u6807\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002Qwen-Spatial\u6a21\u578b\u5728\u7269\u4f53\u5217\u8868\u63d0\u53d6\u4efb\u52a1\u4e0a\u4f18\u4e8e\u4e3b\u6d41\u5546\u4e1aLLM\uff0c\u5728I-O-S\u6d4b\u8bd5\u96c6\u4e0a\u83b7\u5f97\u66f4\u9ad8\u7684F1\u548cNDCG\u5206\u6570\u3002", "conclusion": "MSNav\u901a\u8fc7\u6a21\u5757\u5316\u67b6\u6784\u5c06\u8106\u5f31\u7684\u63a8\u7406\u8f6c\u53d8\u4e3a\u9c81\u68d2\u7684\u96c6\u6210\u667a\u80fd\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u591a\u6a21\u6001\u5bfc\u822a\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.17811", "pdf": "https://arxiv.org/pdf/2508.17811", "abs": "https://arxiv.org/abs/2508.17811", "authors": ["Hanzhi Chang", "Ruijie Zhu", "Wenjie Chang", "Mulin Yu", "Yanzhe Liang", "Jiahao Lu", "Zhuoyuan Li", "Tianzhu Zhang"], "title": "MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": "17 pages, 15 figures, 5 tables", "summary": "Surface reconstruction has been widely studied in computer vision and graphics. However, existing surface reconstruction works struggle to recover accurate scene geometry when the input views are extremely sparse. To address this issue, we propose MeshSplat, a generalizable sparse-view surface reconstruction framework via Gaussian Splatting. Our key idea is to leverage 2DGS as a bridge, which connects novel view synthesis to learned geometric priors and then transfers these priors to achieve surface reconstruction. Specifically, we incorporate a feed-forward network to predict per-view pixel-aligned 2DGS, which enables the network to synthesize novel view images and thus eliminates the need for direct 3D ground-truth supervision. To improve the accuracy of 2DGS position and orientation prediction, we propose a Weighted Chamfer Distance Loss to regularize the depth maps, especially in overlapping areas of input views, and also a normal prediction network to align the orientation of 2DGS with normal vectors predicted by a monocular normal estimator. Extensive experiments validate the effectiveness of our proposed improvement, demonstrating that our method achieves state-of-the-art performance in generalizable sparse-view mesh reconstruction tasks. Project Page: https://hanzhichang.github.io/meshsplat_web", "AI": {"tldr": "MeshSplat\u662f\u4e00\u4e2a\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u7684\u53ef\u6cdb\u5316\u7a00\u758f\u89c6\u56fe\u8868\u9762\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc72D\u9ad8\u65af\u6cfc\u6e85\u8fde\u63a5\u65b0\u89c6\u89d2\u5408\u6210\u4e0e\u51e0\u4f55\u5148\u9a8c\uff0c\u65e0\u97003D\u771f\u503c\u76d1\u7763\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7f51\u683c\u91cd\u5efa", "motivation": "\u73b0\u6709\u8868\u9762\u91cd\u5efa\u65b9\u6cd5\u5728\u8f93\u5165\u89c6\u56fe\u6781\u5176\u7a00\u758f\u65f6\u96be\u4ee5\u6062\u590d\u51c6\u786e\u7684\u573a\u666f\u51e0\u4f55\uff0c\u9700\u8981\u89e3\u51b3\u7a00\u758f\u89c6\u56fe\u4e0b\u7684\u51e0\u4f55\u91cd\u5efa\u96be\u9898", "method": "\u4f7f\u7528\u524d\u9988\u7f51\u7edc\u9884\u6d4b\u6bcf\u89c6\u56fe\u50cf\u7d20\u5bf9\u9f50\u76842D\u9ad8\u65af\u6cfc\u6e85\uff0c\u63d0\u51fa\u52a0\u6743Chamfer\u8ddd\u79bb\u635f\u5931\u6b63\u5219\u5316\u6df1\u5ea6\u56fe\uff0c\u5e76\u5f15\u5165\u6cd5\u5411\u9884\u6d4b\u7f51\u7edc\u5bf9\u9f50\u9ad8\u65af\u6cfc\u6e85\u65b9\u5411", "result": "\u5728\u53ef\u6cdb\u5316\u7a00\u758f\u89c6\u56fe\u7f51\u683c\u91cd\u5efa\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u6539\u8fdb\u7684\u6709\u6548\u6027", "conclusion": "MeshSplat\u901a\u8fc72D\u9ad8\u65af\u6cfc\u6e85\u6865\u63a5\u65b0\u89c6\u89d2\u5408\u6210\u548c\u51e0\u4f55\u5148\u9a8c\u5b66\u4e60\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7a00\u758f\u89c6\u56fe\u4e0b\u7684\u8868\u9762\u91cd\u5efa\u6311\u6218"}}
{"id": "2508.16752", "pdf": "https://arxiv.org/pdf/2508.16752", "abs": "https://arxiv.org/abs/2508.16752", "authors": ["Marco N. Bochernitsan", "Rodrigo C. Barros", "Lucas S. Kupssinsk\u00fc"], "title": "A Framework for Benchmarking Fairness-Utility Trade-offs in Text-to-Image Models via Pareto Frontiers", "categories": ["cs.CV"], "comment": null, "summary": "Achieving fairness in text-to-image generation demands mitigating social biases without compromising visual fidelity, a challenge critical to responsible AI. Current fairness evaluation procedures for text-to-image models rely on qualitative judgment or narrow comparisons, which limit the capacity to assess both fairness and utility in these models and prevent reproducible assessment of debiasing methods. Existing approaches typically employ ad-hoc, human-centered visual inspections that are both error-prone and difficult to replicate. We propose a method for evaluating fairness and utility in text-to-image models using Pareto-optimal frontiers across hyperparametrization of debiasing methods. Our method allows for comparison between distinct text-to-image models, outlining all configurations that optimize fairness for a given utility and vice-versa. To illustrate our evaluation method, we use Normalized Shannon Entropy and ClipScore for fairness and utility evaluation, respectively. We assess fairness and utility in Stable Diffusion, Fair Diffusion, SDXL, DeCoDi, and FLUX text-to-image models. Our method shows that most default hyperparameterizations of the text-to-image model are dominated solutions in the fairness-utility space, and it is straightforward to find better hyperparameters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5e15\u7d2f\u6258\u6700\u4f18\u524d\u6cbf\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u516c\u5e73\u6027\u548c\u6548\u7528\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d85\u53c2\u6570\u4f18\u5316\u6765\u5e73\u8861\u516c\u5e73\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u516c\u5e73\u6027\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u5b9a\u6027\u5224\u65ad\u6216\u72ed\u7a84\u6bd4\u8f83\uff0c\u7f3a\u4e4f\u540c\u65f6\u8bc4\u4f30\u516c\u5e73\u6027\u548c\u6548\u7528\u7684\u53ef\u91cd\u590d\u65b9\u6cd5\uff0c\u9650\u5236\u4e86\u53bb\u504f\u65b9\u6cd5\u7684\u6709\u6548\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528\u5e15\u7d2f\u6258\u6700\u4f18\u524d\u6cbf\u5206\u6790\u53bb\u504f\u65b9\u6cd5\u7684\u8d85\u53c2\u6570\u914d\u7f6e\uff0c\u4ee5Normalized Shannon Entropy\u8bc4\u4f30\u516c\u5e73\u6027\uff0cClipScore\u8bc4\u4f30\u6548\u7528\uff0c\u5bf9\u591a\u79cd\u4e3b\u6d41\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5927\u591a\u6570\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u9ed8\u8ba4\u8d85\u53c2\u6570\u914d\u7f6e\u5728\u516c\u5e73\u6027-\u6548\u7528\u7a7a\u95f4\u4e2d\u5904\u4e8e\u88ab\u652f\u914d\u5730\u4f4d\uff0c\u53ef\u4ee5\u8f7b\u677e\u627e\u5230\u66f4\u597d\u7684\u8d85\u53c2\u6570\u914d\u7f6e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u91cd\u590d\u3001\u7cfb\u7edf\u5316\u7684\u516c\u5e73\u6027\u548c\u6548\u7528\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u6307\u5bfc\u6a21\u578b\u4f18\u5316\u548c\u53bb\u504f\u65b9\u6cd5\u7684\u6539\u8fdb\u3002"}}
{"id": "2508.16783", "pdf": "https://arxiv.org/pdf/2508.16783", "abs": "https://arxiv.org/abs/2508.16783", "authors": ["Stefania L. Moroianu", "Christian Bluethgen", "Pierre Chambon", "Mehdi Cherti", "Jean-Benoit Delbrouck", "Magdalini Paschali", "Brandon Price", "Judy Gichoya", "Jenia Jitsev", "Curtis P. Langlotz", "Akshay S. Chaudhari"], "title": "Improving Performance, Robustness, and Fairness of Radiographic AI Models with Finely-Controllable Synthetic Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Achieving robust performance and fairness across diverse patient populations remains a challenge in developing clinically deployable deep learning models for diagnostic imaging. Synthetic data generation has emerged as a promising strategy to address limitations in dataset scale and diversity. We introduce RoentGen-v2, a text-to-image diffusion model for chest radiographs that enables fine-grained control over both radiographic findings and patient demographic attributes, including sex, age, and race/ethnicity. RoentGen-v2 is the first model to generate clinically plausible images with demographic conditioning, facilitating the creation of a large, demographically balanced synthetic dataset comprising over 565,000 images. We use this large synthetic dataset to evaluate optimal training pipelines for downstream disease classification models. In contrast to prior work that combines real and synthetic data naively, we propose an improved training strategy that leverages synthetic data for supervised pretraining, followed by fine-tuning on real data. Through extensive evaluation on over 137,000 chest radiographs from five institutions, we demonstrate that synthetic pretraining consistently improves model performance, generalization to out-of-distribution settings, and fairness across demographic subgroups. Across datasets, synthetic pretraining led to a 6.5% accuracy increase in the performance of downstream classification models, compared to a modest 2.7% increase when naively combining real and synthetic data. We observe this performance improvement simultaneously with the reduction of the underdiagnosis fairness gap by 19.3%. These results highlight the potential of synthetic imaging to advance equitable and generalizable medical deep learning under real-world data constraints. We open source our code, trained models, and synthetic dataset at https://github.com/StanfordMIMI/RoentGen-v2 .", "AI": {"tldr": "RoentGen-v2\u662f\u4e00\u4e2a\u7528\u4e8e\u80f8\u90e8X\u5149\u7247\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u751f\u6210\u5177\u6709\u4eba\u53e3\u7edf\u8ba1\u5b66\u6761\u4ef6\u63a7\u5236\u7684\u5408\u6210\u6570\u636e\uff0c\u901a\u8fc7\u5408\u6210\u9884\u8bad\u7ec3\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u75be\u75c5\u5206\u7c7b\u6a21\u578b\u7684\u6027\u80fd\u3001\u6cdb\u5316\u80fd\u529b\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u5f71\u50cf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u4e0d\u540c\u60a3\u8005\u7fa4\u4f53\u4e2d\u5b9e\u73b0\u7a33\u5065\u6027\u80fd\u548c\u516c\u5e73\u6027\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u751f\u6210\u6765\u5f25\u8865\u6570\u636e\u96c6\u89c4\u6a21\u548c\u591a\u6837\u6027\u7684\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1RoentGen-v2\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u751f\u6210\u8d85\u8fc7565,000\u5f20\u5177\u6709\u4eba\u53e3\u7edf\u8ba1\u5b66\u5c5e\u6027\u63a7\u5236\u7684\u5408\u6210\u80f8\u90e8X\u5149\u7247\uff0c\u63d0\u51fa\u5408\u6210\u9884\u8bad\u7ec3\u540e\u5fae\u8c03\u771f\u5b9e\u6570\u636e\u7684\u6539\u8fdb\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728\u4e94\u4e2a\u673a\u6784\u7684137,000\u591a\u5f20\u80f8\u90e8X\u5149\u7247\u4e0a\u8bc4\u4f30\uff0c\u5408\u6210\u9884\u8bad\u7ec3\u4f7f\u4e0b\u6e38\u5206\u7c7b\u6a21\u578b\u51c6\u786e\u7387\u63d0\u9ad86.5%\uff0c\u516c\u5e73\u6027\u5dee\u8ddd\u51cf\u5c1119.3%\uff0c\u663e\u8457\u4f18\u4e8e\u7b80\u5355\u6df7\u5408\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u7684\u65b9\u6cd5\u3002", "conclusion": "\u5408\u6210\u5f71\u50cf\u6570\u636e\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7ea6\u675f\u4e0b\u5177\u6709\u63a8\u8fdb\u516c\u5e73\u548c\u53ef\u6cdb\u5316\u533b\u5b66\u6df1\u5ea6\u5b66\u4e60\u7684\u6f5c\u529b\uff0c\u4e3a\u4e34\u5e8a\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.16830", "pdf": "https://arxiv.org/pdf/2508.16830", "abs": "https://arxiv.org/abs/2508.16830", "authors": ["Alexander Yakovenko", "George Chakvetadze", "Ilya Khrapov", "Maksim Zhelezov", "Dmitry Vatolin", "Radu Timofte", "Youngjin Oh", "Junhyeong Kwon", "Junyoung Park", "Nam Ik Cho", "Senyan Xu", "Ruixuan Jiang", "Long Peng", "Xueyang Fu", "Zheng-Jun Zha", "Xiaoping Peng", "Hansen Feng", "Zhanyi Tie", "Ziming Xia", "Lizhi Wang"], "title": "AIM 2025 Low-light RAW Video Denoising Challenge: Dataset, Methods and Results", "categories": ["cs.CV", "eess.IV"], "comment": "Challenge report from Advances in Image Manipulation workshop held at   ICCV 2025", "summary": "This paper reviews the AIM 2025 (Advances in Image Manipulation) Low-Light RAW Video Denoising Challenge. The task is to develop methods that denoise low-light RAW video by exploiting temporal redundancy while operating under exposure-time limits imposed by frame rate and adapting to sensor-specific, signal-dependent noise. We introduce a new benchmark of 756 ten-frame sequences captured with 14 smartphone camera sensors across nine conditions (illumination: 1/5/10 lx; exposure: 1/24, 1/60, 1/120 s), with high-SNR references obtained via burst averaging. Participants process linear RAW sequences and output the denoised 10th frame while preserving the Bayer pattern. Submissions are evaluated on a private test set using full-reference PSNR and SSIM, with final ranking given by the mean of per-metric ranks. This report describes the dataset, challenge protocol, and submitted approaches.", "AI": {"tldr": "AIM 2025\u4f4e\u5149\u7167RAW\u89c6\u9891\u53bb\u566a\u6311\u6218\u8d5b\u7efc\u8ff0\uff0c\u65e8\u5728\u5f00\u53d1\u5229\u7528\u65f6\u95f4\u5197\u4f59\u6027\u3001\u5728\u5e27\u7387\u9650\u5236\u66dd\u5149\u65f6\u95f4\u5185\u5904\u7406\u4f20\u611f\u5668\u7279\u5b9a\u4fe1\u53f7\u76f8\u5173\u566a\u58f0\u7684\u65b9\u6cd5", "motivation": "\u89e3\u51b3\u4f4e\u5149\u7167\u6761\u4ef6\u4e0bRAW\u89c6\u9891\u7684\u566a\u58f0\u95ee\u9898\uff0c\u5229\u7528\u65f6\u95f4\u5197\u4f59\u6027\u8fdb\u884c\u53bb\u566a\uff0c\u540c\u65f6\u9002\u5e94\u4e0d\u540c\u4f20\u611f\u5668\u7684\u4fe1\u53f7\u76f8\u5173\u566a\u58f0\u7279\u6027", "method": "\u5efa\u7acb\u5305\u542b756\u4e2a\u5341\u5e27\u5e8f\u5217\u7684\u65b0\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4f7f\u752814\u79cd\u667a\u80fd\u624b\u673a\u4f20\u611f\u5668\u57289\u79cd\u5149\u7167\u548c\u66dd\u5149\u6761\u4ef6\u4e0b\u91c7\u96c6\uff0c\u901a\u8fc7\u7206\u53d1\u5e73\u5747\u83b7\u5f97\u9ad8\u4fe1\u566a\u6bd4\u53c2\u8003\u5e27", "result": "\u53c2\u4e0e\u8005\u5904\u7406\u7ebf\u6027RAW\u5e8f\u5217\u5e76\u8f93\u51fa\u53bb\u566a\u540e\u7684\u7b2c10\u5e27\uff0c\u4fdd\u6301\u62dc\u8033\u6a21\u5f0f\uff0c\u4f7f\u7528PSNR\u548cSSIM\u6307\u6807\u5728\u79c1\u6709\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30", "conclusion": "\u8be5\u6311\u6218\u8d5b\u4e3a\u4f4e\u5149\u7167RAW\u89c6\u9891\u53bb\u566a\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u548c\u65b0\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u65b9\u6cd5\u7684\u53d1\u5c55"}}
{"id": "2508.16845", "pdf": "https://arxiv.org/pdf/2508.16845", "abs": "https://arxiv.org/abs/2508.16845", "authors": ["Denis Tarasov", "Alexander Nikulin", "Ilya Zisman", "Albina Klepach", "Nikita Lyubaykin", "Andrei Polubarov", "Alexander Derevyagin", "Vladislav Kurenkov"], "title": "NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flows", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in Vision-Language-Action (VLA) models have established a two-component architecture, where a pre-trained Vision-Language Model (VLM) encodes visual observations and task descriptions, and an action decoder maps these representations to continuous actions. Diffusion models have been widely adopted as action decoders due to their ability to model complex, multimodal action distributions. However, they require multiple iterative denoising steps at inference time or downstream techniques to speed up sampling, limiting their practicality in real-world settings where high-frequency control is crucial. In this work, we present NinA (Normalizing Flows in Action), a fast and expressive alter- native to diffusion-based decoders for VLAs. NinA replaces the diffusion action decoder with a Normalizing Flow (NF) that enables one-shot sampling through an invertible transformation, significantly reducing inference time. We integrate NinA into the FLOWER VLA architecture and fine-tune on the LIBERO benchmark. Our experiments show that NinA matches the performance of its diffusion-based counterpart under the same training regime, while achieving substantially faster inference. These results suggest that NinA offers a promising path toward efficient, high-frequency VLA control without compromising performance.", "AI": {"tldr": "NinA\u4f7f\u7528\u5f52\u4e00\u5316\u6d41\u66ff\u4ee3\u6269\u6563\u6a21\u578b\u4f5c\u4e3aVLA\u7684\u52a8\u4f5c\u89e3\u7801\u5668\uff0c\u5b9e\u73b0\u5355\u6b21\u91c7\u6837\uff0c\u5927\u5e45\u63d0\u5347\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u76f8\u5f53", "motivation": "\u6269\u6563\u6a21\u578b\u9700\u8981\u591a\u6b21\u8fed\u4ee3\u53bb\u566a\u6b65\u9aa4\u6216\u4e0b\u6e38\u52a0\u901f\u6280\u672f\uff0c\u9650\u5236\u4e86\u5728\u9700\u8981\u9ad8\u9891\u63a7\u5236\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027", "method": "\u7528\u5f52\u4e00\u5316\u6d41(Normalizing Flow)\u66ff\u6362\u6269\u6563\u52a8\u4f5c\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u53ef\u9006\u53d8\u6362\u5b9e\u73b0\u4e00\u6b21\u6027\u91c7\u6837\uff0c\u96c6\u6210\u5230FLOWER VLA\u67b6\u6784\u5e76\u5728LIBERO\u57fa\u51c6\u4e0a\u5fae\u8c03", "result": "\u5728\u76f8\u540c\u8bad\u7ec3\u673a\u5236\u4e0b\uff0cNinA\u6027\u80fd\u4e0e\u57fa\u4e8e\u6269\u6563\u7684\u89e3\u7801\u5668\u76f8\u5f53\uff0c\u4f46\u63a8\u7406\u901f\u5ea6\u663e\u8457\u66f4\u5feb", "conclusion": "NinA\u4e3a\u9ad8\u6548\u3001\u9ad8\u9891\u7684VLA\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u8def\u5f84\uff0c\u4e14\u4e0d\u727a\u7272\u6027\u80fd"}}
{"id": "2508.16849", "pdf": "https://arxiv.org/pdf/2508.16849", "abs": "https://arxiv.org/abs/2508.16849", "authors": ["Lihao Zhang", "Zongtan Li", "Haijian Sun"], "title": "RF-PGS: Fully-structured Spatial Wireless Channel Representation with Planar Gaussian Splatting", "categories": ["cs.CV", "cs.NI"], "comment": "13 pages, 16 figures, in submission to IEEE journal", "summary": "In the 6G era, the demand for higher system throughput and the implementation of emerging 6G technologies require large-scale antenna arrays and accurate spatial channel state information (Spatial-CSI). Traditional channel modeling approaches, such as empirical models, ray tracing, and measurement-based methods, face challenges in spatial resolution, efficiency, and scalability. Radiance field-based methods have emerged as promising alternatives but still suffer from geometric inaccuracy and costly supervision. This paper proposes RF-PGS, a novel framework that reconstructs high-fidelity radio propagation paths from only sparse path loss spectra. By introducing Planar Gaussians as geometry primitives with certain RF-specific optimizations, RF-PGS achieves dense, surface-aligned scene reconstruction in the first geometry training stage. In the subsequent Radio Frequency (RF) training stage, the proposed fully-structured radio radiance, combined with a tailored multi-view loss, accurately models radio propagation behavior. Compared to prior radiance field methods, RF-PGS significantly improves reconstruction accuracy, reduces training costs, and enables efficient representation of wireless channels, offering a practical solution for scalable 6G Spatial-CSI modeling.", "AI": {"tldr": "RF-PGS\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5e73\u9762\u9ad8\u65af\u548c\u7279\u5b9a\u5c04\u9891\u4f18\u5316\uff0c\u4ec5\u4ece\u7a00\u758f\u8def\u5f84\u635f\u8017\u8c31\u91cd\u5efa\u9ad8\u4fdd\u771f\u65e0\u7ebf\u7535\u4f20\u64ad\u8def\u5f84\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u91cd\u5efa\u7cbe\u5ea6\u5e76\u964d\u4f4e\u4e86\u8bad\u7ec3\u6210\u672c", "motivation": "6G\u65f6\u4ee3\u9700\u8981\u5927\u89c4\u6a21\u5929\u7ebf\u9635\u5217\u548c\u7cbe\u786e\u7684\u7a7a\u95f4\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff0c\u4f20\u7edf\u4fe1\u9053\u5efa\u6a21\u65b9\u6cd5\u5728\u7a7a\u95f4\u5206\u8fa8\u7387\u3001\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u8f90\u5c04\u573a\u65b9\u6cd5\u5b58\u5728\u51e0\u4f55\u4e0d\u51c6\u786e\u548c\u6602\u8d35\u76d1\u7763\u7684\u95ee\u9898", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1\uff09\u51e0\u4f55\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u5e73\u9762\u9ad8\u65af\u4f5c\u4e3a\u51e0\u4f55\u57fa\u5143\u5b9e\u73b0\u5bc6\u96c6\u8868\u9762\u5bf9\u9f50\u7684\u573a\u666f\u91cd\u5efa\uff1b2\uff09\u5c04\u9891\u8bad\u7ec3\u9636\u6bb5\u7ed3\u5408\u5b8c\u5168\u7ed3\u6784\u5316\u7684\u65e0\u7ebf\u7535\u8f90\u5c04\u548c\u5b9a\u5236\u591a\u89c6\u56fe\u635f\u5931\u6765\u7cbe\u786e\u5efa\u6a21\u65e0\u7ebf\u7535\u4f20\u64ad\u884c\u4e3a", "result": "\u76f8\u6bd4\u5148\u524d\u7684\u8f90\u5c04\u573a\u65b9\u6cd5\uff0cRF-PGS\u663e\u8457\u63d0\u9ad8\u4e86\u91cd\u5efa\u7cbe\u5ea6\uff0c\u964d\u4f4e\u4e86\u8bad\u7ec3\u6210\u672c\uff0c\u5e76\u5b9e\u73b0\u4e86\u65e0\u7ebf\u4fe1\u9053\u7684\u9ad8\u6548\u8868\u793a", "conclusion": "RF-PGS\u4e3a\u53ef\u6269\u5c55\u76846G\u7a7a\u95f4CSI\u5efa\u6a21\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u6709\u8f90\u5c04\u573a\u65b9\u6cd5\u7684\u5c40\u9650\u6027"}}
{"id": "2508.16863", "pdf": "https://arxiv.org/pdf/2508.16863", "abs": "https://arxiv.org/abs/2508.16863", "authors": ["Tangyuan Zhang", "Shangyu Chen", "Qixiang Chen", "Jianfei Cai"], "title": "Delta-SVD: Efficient Compression for Personalized Text-to-Image Models", "categories": ["cs.CV"], "comment": null, "summary": "Personalized text-to-image models such as DreamBooth require fine-tuning large-scale diffusion backbones, resulting in significant storage overhead when maintaining many subject-specific models. We present Delta-SVD, a post-hoc, training-free compression method that targets the parameter weights update induced by DreamBooth fine-tuning. Our key observation is that these delta weights exhibit strong low-rank structure due to the sparse and localized nature of personalization. Delta-SVD first applies Singular Value Decomposition (SVD) to factorize the weight deltas, followed by an energy-based rank truncation strategy to balance compression efficiency and reconstruction fidelity. The resulting compressed models are fully plug-and-play and can be re-constructed on-the-fly during inference. Notably, the proposed approach is simple, efficient, and preserves the original model architecture. Experiments on a multiple subject dataset demonstrate that Delta-SVD achieves substantial compression with negligible loss in generation quality measured by CLIP score, SSIM and FID. Our method enables scalable and efficient deployment of personalized diffusion models, making it a practical solution for real-world applications that require storing and deploying large-scale subject customizations.", "AI": {"tldr": "Delta-SVD\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u540e\u5904\u7406\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u5947\u5f02\u503c\u5206\u89e3\u538b\u7f29DreamBooth\u5fae\u8c03\u4ea7\u751f\u7684\u6743\u91cd\u589e\u91cf\uff0c\u5b9e\u73b0\u5927\u5e45\u538b\u7f29\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf", "motivation": "DreamBooth\u7b49\u4e2a\u6027\u5316\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u9700\u8981\u5fae\u8c03\u5927\u89c4\u6a21\u6269\u6563\u6a21\u578b\uff0c\u5bfc\u81f4\u5b58\u50a8\u5927\u91cf\u7279\u5b9a\u4e3b\u9898\u6a21\u578b\u65f6\u4ea7\u751f\u663e\u8457\u5b58\u50a8\u5f00\u9500", "method": "\u5229\u7528\u6743\u91cd\u589e\u91cf\u5177\u6709\u5f3a\u4f4e\u79e9\u7ed3\u6784\u7684\u7279\u70b9\uff0c\u9996\u5148\u5e94\u7528\u5947\u5f02\u503c\u5206\u89e3\u5206\u89e3\u6743\u91cd\u589e\u91cf\uff0c\u7136\u540e\u91c7\u7528\u57fa\u4e8e\u80fd\u91cf\u7684\u79e9\u622a\u65ad\u7b56\u7565\u5e73\u8861\u538b\u7f29\u6548\u7387\u548c\u91cd\u5efa\u4fdd\u771f\u5ea6", "result": "\u5728\u591a\u4e3b\u9898\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDelta-SVD\u5b9e\u73b0\u4e86\u663e\u8457\u538b\u7f29\uff0c\u5728CLIP\u5206\u6570\u3001SSIM\u548cFID\u7b49\u6307\u6807\u4e0a\u751f\u6210\u8d28\u91cf\u635f\u5931\u53ef\u5ffd\u7565\u4e0d\u8ba1", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u7684\u4e2a\u6027\u5316\u6269\u6563\u6a21\u578b\u90e8\u7f72\uff0c\u4e3a\u9700\u8981\u5b58\u50a8\u548c\u90e8\u7f72\u5927\u89c4\u6a21\u4e3b\u9898\u5b9a\u5236\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.16887", "pdf": "https://arxiv.org/pdf/2508.16887", "abs": "https://arxiv.org/abs/2508.16887", "authors": ["Shunyu Yao", "Ming Liu", "Zhilu Zhang", "Zhaolin Wan", "Zhilong Ji", "Jinfeng Bai", "Wangmeng Zuo"], "title": "MDIQA: Unified Image Quality Assessment for Multi-dimensional Evaluation and Restoration", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Recent advancements in image quality assessment (IQA), driven by sophisticated deep neural network designs, have significantly improved the ability to approach human perceptions. However, most existing methods are obsessed with fitting the overall score, neglecting the fact that humans typically evaluate image quality from different dimensions before arriving at an overall quality assessment. To overcome this problem, we propose a multi-dimensional image quality assessment (MDIQA) framework. Specifically, we model image quality across various perceptual dimensions, including five technical and four aesthetic dimensions, to capture the multifaceted nature of human visual perception within distinct branches. Each branch of our MDIQA is initially trained under the guidance of a separate dimension, and the respective features are then amalgamated to generate the final IQA score. Additionally, when the MDIQA model is ready, we can deploy it for a flexible training of image restoration (IR) models, enabling the restoration results to better align with varying user preferences through the adjustment of perceptual dimension weights. Extensive experiments demonstrate that our MDIQA achieves superior performance and can be effectively and flexibly applied to image restoration tasks. The code is available: https://github.com/YaoShunyu19/MDIQA.", "AI": {"tldr": "\u63d0\u51fa\u591a\u7ef4\u5ea6\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6MDIQA\uff0c\u4ece\u6280\u672f\u548c\u7f8e\u5b66\u7b49\u591a\u4e2a\u7ef4\u5ea6\u5efa\u6a21\u56fe\u50cf\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u878d\u5408\u5404\u7ef4\u5ea6\u7279\u5f81\u751f\u6210\u6700\u7ec8\u8bc4\u5206\uff0c\u8fd8\u80fd\u7075\u6d3b\u5e94\u7528\u4e8e\u56fe\u50cf\u590d\u539f\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u62df\u5408\u6574\u4f53\u8bc4\u5206\uff0c\u5ffd\u7565\u4e86\u4eba\u7c7b\u901a\u5e38\u4ece\u4e0d\u540c\u7ef4\u5ea6\u8bc4\u4f30\u56fe\u50cf\u8d28\u91cf\u7684\u4e8b\u5b9e\uff0c\u9700\u8981\u66f4\u7b26\u5408\u4eba\u7c7b\u611f\u77e5\u7684\u591a\u7ef4\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u591a\u7ef4\u5ea6\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec5\u4e2a\u6280\u672f\u7ef4\u5ea6\u548c4\u4e2a\u7f8e\u5b66\u7ef4\u5ea6\uff0c\u6bcf\u4e2a\u7ef4\u5ea6\u5355\u72ec\u8bad\u7ec3\u5206\u652f\uff0c\u7136\u540e\u878d\u5408\u7279\u5f81\u751f\u6210\u6700\u7ec8IQA\u8bc4\u5206\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMDIQA\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u6709\u6548\u7075\u6d3b\u5730\u5e94\u7528\u4e8e\u56fe\u50cf\u590d\u539f\u4efb\u52a1\uff0c\u901a\u8fc7\u8c03\u6574\u611f\u77e5\u7ef4\u5ea6\u6743\u91cd\u4f7f\u590d\u539f\u7ed3\u679c\u66f4\u597d\u5730\u7b26\u5408\u4e0d\u540c\u7528\u6237\u504f\u597d\u3002", "conclusion": "MDIQA\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u591a\u7ef4\u5ea6\u8bc4\u4f30\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u66f4\u7b26\u5408\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u7684\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u56fe\u50cf\u590d\u539f\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.16917", "pdf": "https://arxiv.org/pdf/2508.16917", "abs": "https://arxiv.org/abs/2508.16917", "authors": ["Qing Zhang", "Jinguang Tong", "Jie Hong", "Jing Zhang", "Xuesong Li"], "title": "Structural Energy-Guided Sampling for View-Consistent Text-to-3D", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-3D generation often suffers from the Janus problem, where objects look correct from the front but collapse into duplicated or distorted geometry from other angles. We attribute this failure to viewpoint bias in 2D diffusion priors, which propagates into 3D optimization. To address this, we propose Structural Energy-Guided Sampling (SEGS), a training-free, plug-and-play framework that enforces multi-view consistency entirely at sampling time. SEGS defines a structural energy in a PCA subspace of intermediate U-Net features and injects its gradients into the denoising trajectory, steering geometry toward the intended viewpoint while preserving appearance fidelity. Integrated seamlessly into SDS/VSD pipelines, SEGS significantly reduces Janus artifacts, achieving improved geometric alignment and viewpoint consistency without retraining or weight modification.", "AI": {"tldr": "SEGS\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u91c7\u6837\u65f6\u6ce8\u5165\u7ed3\u6784\u80fd\u91cf\u68af\u5ea6\u6765\u89e3\u51b3\u6587\u672c\u52303D\u751f\u6210\u4e2d\u7684Janus\u95ee\u9898\uff08\u6b63\u9762\u6b63\u786e\u4f46\u5176\u4ed6\u89d2\u5ea6\u51e0\u4f55\u91cd\u590d\u6216\u626d\u66f2\u7684\u95ee\u9898\uff09\u3002", "motivation": "\u6587\u672c\u52303D\u751f\u6210\u5b58\u5728Janus\u95ee\u9898\uff0c\u5373\u5bf9\u8c61\u4ece\u6b63\u9762\u770b\u6b63\u786e\u4f46\u4ece\u5176\u4ed6\u89d2\u5ea6\u4f1a\u51fa\u73b0\u51e0\u4f55\u91cd\u590d\u6216\u626d\u66f2\u3002\u8fd9\u6e90\u4e8e2D\u6269\u6563\u5148\u9a8c\u4e2d\u7684\u89c6\u89d2\u504f\u5dee\uff0c\u8be5\u504f\u5dee\u4f1a\u4f20\u64ad\u52303D\u4f18\u5316\u8fc7\u7a0b\u4e2d\u3002", "method": "\u63d0\u51fa\u7ed3\u6784\u80fd\u91cf\u5f15\u5bfc\u91c7\u6837\uff08SEGS\uff09\u6846\u67b6\uff1a\u5728U-Net\u4e2d\u95f4\u7279\u5f81\u7684PCA\u5b50\u7a7a\u95f4\u4e2d\u5b9a\u4e49\u7ed3\u6784\u80fd\u91cf\uff0c\u5e76\u5c06\u5176\u68af\u5ea6\u6ce8\u5165\u53bb\u566a\u8f68\u8ff9\uff0c\u5f15\u5bfc\u51e0\u4f55\u671d\u5411\u9884\u671f\u89c6\u89d2\uff0c\u540c\u65f6\u4fdd\u6301\u5916\u89c2\u4fdd\u771f\u5ea6\u3002\u53ef\u65e0\u7f1d\u96c6\u6210\u5230SDS/VSD\u6d41\u7a0b\u4e2d\u3002", "result": "SEGS\u663e\u8457\u51cf\u5c11\u4e86Janus\u4f2a\u5f71\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u51e0\u4f55\u5bf9\u9f50\u548c\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u6743\u91cd\u4fee\u6539\u3002", "conclusion": "SEGS\u901a\u8fc7\u91c7\u6837\u65f6\u7684\u7ed3\u6784\u80fd\u91cf\u5f15\u5bfc\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u52303D\u751f\u6210\u4e2d\u7684\u89c6\u89d2\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.16932", "pdf": "https://arxiv.org/pdf/2508.16932", "abs": "https://arxiv.org/abs/2508.16932", "authors": ["Qi Song", "Ziyuan Luo", "Ka Chun Cheung", "Simon See", "Renjie Wan"], "title": "Align 3D Representation and Text Embedding for 3D Content Personalization", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in NeRF and 3DGS have significantly enhanced the efficiency and quality of 3D content synthesis. However, efficient personalization of generated 3D content remains a critical challenge. Current 3D personalization approaches predominantly rely on knowledge distillation-based methods, which require computationally expensive retraining procedures. To address this challenge, we propose \\textbf{Invert3D}, a novel framework for convenient 3D content personalization. Nowadays, vision-language models such as CLIP enable direct image personalization through aligned vision-text embedding spaces. However, the inherent structural differences between 3D content and 2D images preclude direct application of these techniques to 3D personalization. Our approach bridges this gap by establishing alignment between 3D representations and text embedding spaces. Specifically, we develop a camera-conditioned 3D-to-text inverse mechanism that projects 3D contents into a 3D embedding aligned with text embeddings. This alignment enables efficient manipulation and personalization of 3D content through natural language prompts, eliminating the need for computationally retraining procedures. Extensive experiments demonstrate that Invert3D achieves effective personalization of 3D content. Our work is available at: https://github.com/qsong2001/Invert3D.", "AI": {"tldr": "Invert3D\u662f\u4e00\u4e2a\u65b0\u9896\u76843D\u5185\u5bb9\u4e2a\u6027\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u7acb3D\u8868\u793a\u4e0e\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u7684\u5bf9\u9f50\uff0c\u5b9e\u73b0\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\u76843D\u5185\u5bb9\u4e2a\u6027\u5316\u3002", "motivation": "\u5f53\u524d3D\u4e2a\u6027\u5316\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u77e5\u8bc6\u84b8\u998f\uff0c\u9700\u8981\u8ba1\u7b97\u6602\u8d35\u7684\u91cd\u65b0\u8bad\u7ec3\u8fc7\u7a0b\u3002\u867d\u7136\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5982CLIP\u80fd\u591f\u5b9e\u73b02D\u56fe\u50cf\u7684\u76f4\u63a5\u4e2a\u6027\u5316\uff0c\u4f46\u7531\u4e8e3D\u5185\u5bb9\u4e0e2D\u56fe\u50cf\u7684\u7ed3\u6784\u5dee\u5f02\uff0c\u8fd9\u4e9b\u6280\u672f\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e3D\u4e2a\u6027\u5316\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u76f8\u673a\u6761\u4ef6\u76843D\u5230\u6587\u672c\u9006\u673a\u5236\uff0c\u5c063D\u5185\u5bb9\u6295\u5f71\u5230\u4e0e\u6587\u672c\u5d4c\u5165\u5bf9\u9f50\u76843D\u5d4c\u5165\u7a7a\u95f4\u4e2d\uff0c\u5efa\u7acb3D\u8868\u793a\u4e0e\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u7684\u5bf9\u9f50\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cInvert3D\u80fd\u591f\u6709\u6548\u5b9e\u73b03D\u5185\u5bb9\u7684\u4e2a\u6027\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u5b9e\u73b0\u9ad8\u6548\u76843D\u5185\u5bb9\u64cd\u4f5c\u548c\u4e2a\u6027\u5316\uff0c\u6d88\u9664\u4e86\u8ba1\u7b97\u5bc6\u96c6\u578b\u91cd\u65b0\u8bad\u7ec3\u7684\u9700\u6c42\uff0c\u4e3a3D\u5185\u5bb9\u4e2a\u6027\u5316\u63d0\u4f9b\u4e86\u4fbf\u6377\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.16956", "pdf": "https://arxiv.org/pdf/2508.16956", "abs": "https://arxiv.org/abs/2508.16956", "authors": ["Ruicheng Zhang", "Puxin Yan", "Zeyu Zhang", "Yicheng Chang", "Hongyi Chen", "Zhi Jin"], "title": "RPD-Diff: Region-Adaptive Physics-Guided Diffusion Model for Visibility Enhancement under Dense and Non-Uniform Haze", "categories": ["cs.CV"], "comment": null, "summary": "Single-image dehazing under dense and non-uniform haze conditions remains challenging due to severe information degradation and spatial heterogeneity. Traditional diffusion-based dehazing methods struggle with insufficient generation conditioning and lack of adaptability to spatially varying haze distributions, which leads to suboptimal restoration. To address these limitations, we propose RPD-Diff, a Region-adaptive Physics-guided Dehazing Diffusion Model for robust visibility enhancement in complex haze scenarios. RPD-Diff introduces a Physics-guided Intermediate State Targeting (PIST) strategy, which leverages physical priors to reformulate the diffusion Markov chain by generation target transitions, mitigating the issue of insufficient conditioning in dense haze scenarios. Additionally, the Haze-Aware Denoising Timestep Predictor (HADTP) dynamically adjusts patch-specific denoising timesteps employing a transmission map cross-attention mechanism, adeptly managing non-uniform haze distributions. Extensive experiments across four real-world datasets demonstrate that RPD-Diff achieves state-of-the-art performance in challenging dense and non-uniform haze scenarios, delivering high-quality, haze-free images with superior detail clarity and color fidelity.", "AI": {"tldr": "RPD-Diff\u662f\u4e00\u79cd\u533a\u57df\u81ea\u9002\u5e94\u7269\u7406\u5f15\u5bfc\u53bb\u96fe\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u7269\u7406\u5f15\u5bfc\u4e2d\u95f4\u72b6\u6001\u76ee\u6807\u548c\u96fe\u973e\u611f\u77e5\u53bb\u566a\u65f6\u95f4\u6b65\u9884\u6d4b\u5668\uff0c\u6709\u6548\u5904\u7406\u5bc6\u96c6\u975e\u5747\u5300\u96fe\u973e\u573a\u666f\uff0c\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6269\u6563\u7684\u53bb\u96fe\u65b9\u6cd5\u5728\u5bc6\u96c6\u975e\u5747\u5300\u96fe\u973e\u6761\u4ef6\u4e0b\u5b58\u5728\u751f\u6210\u6761\u4ef6\u4e0d\u8db3\u548c\u7a7a\u95f4\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6062\u590d\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faRPD-Diff\u6a21\u578b\uff0c\u5305\u542b\u7269\u7406\u5f15\u5bfc\u4e2d\u95f4\u72b6\u6001\u76ee\u6807\u7b56\u7565(PIST)\u5229\u7528\u7269\u7406\u5148\u9a8c\u91cd\u6784\u6269\u6563\u9a6c\u5c14\u53ef\u592b\u94fe\uff0c\u4ee5\u53ca\u96fe\u973e\u611f\u77e5\u53bb\u566a\u65f6\u95f4\u6b65\u9884\u6d4b\u5668(HADTP)\u901a\u8fc7\u900f\u5c04\u56fe\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u8c03\u6574\u8865\u4e01\u7279\u5b9a\u53bb\u566a\u65f6\u95f4\u6b65\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cRPD-Diff\u5728\u6311\u6218\u6027\u7684\u5bc6\u96c6\u975e\u5747\u5300\u96fe\u973e\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u65e0\u96fe\u56fe\u50cf\uff0c\u5177\u6709\u4f18\u5f02\u7684\u7ec6\u8282\u6e05\u6670\u5ea6\u548c\u8272\u5f69\u4fdd\u771f\u5ea6\u3002", "conclusion": "RPD-Diff\u901a\u8fc7\u7269\u7406\u5f15\u5bfc\u548c\u533a\u57df\u81ea\u9002\u5e94\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5bc6\u96c6\u975e\u5747\u5300\u96fe\u973e\u53bb\u96fe\u7684\u6311\u6218\uff0c\u4e3a\u590d\u6742\u96fe\u973e\u573a\u666f\u4e0b\u7684\u80fd\u89c1\u5ea6\u589e\u5f3a\u63d0\u4f9b\u4e86\u9c81\u68d2\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.16984", "pdf": "https://arxiv.org/pdf/2508.16984", "abs": "https://arxiv.org/abs/2508.16984", "authors": ["Liang Feng", "Shikang Zheng", "Jiacheng Liu", "Yuqi Lin", "Qinming Zhou", "Peiliang Cai", "Xinyu Wang", "Junjie Chen", "Chang Zou", "Yue Ma", "Linfeng Zhang"], "title": "HiCache: Training-free Acceleration of Diffusion Models via Hermite Polynomial-based Feature Caching", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have achieved remarkable success in content generation but suffer from prohibitive computational costs due to iterative sampling. While recent feature caching methods tend to accelerate inference through temporal extrapolation, these methods still suffer from server quality loss due to the failure in modeling the complex dynamics of feature evolution. To solve this problem, this paper presents HiCache, a training-free acceleration framework that fundamentally improves feature prediction by aligning mathematical tools with empirical properties. Our key insight is that feature derivative approximations in Diffusion Transformers exhibit multivariate Gaussian characteristics, motivating the use of Hermite polynomials-the potentially theoretically optimal basis for Gaussian-correlated processes. Besides, We further introduce a dual-scaling mechanism that ensures numerical stability while preserving predictive accuracy. Extensive experiments demonstrate HiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding baseline quality, maintaining strong performance across text-to-image, video generation, and super-resolution tasks. Core implementation is provided in the appendix, with complete code to be released upon acceptance.", "AI": {"tldr": "HiCache\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u7279\u5f81\u7f13\u5b58\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7Hermite\u591a\u9879\u5f0f\u5bf9\u9f50\u7279\u5f81\u5bfc\u6570\u7684\u9ad8\u65af\u7279\u6027\uff0c\u5728\u6269\u6563\u6a21\u578b\u4e2d\u5b9e\u73b06.24\u500d\u52a0\u901f\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf", "motivation": "\u6269\u6563\u6a21\u578b\u867d\u7136\u751f\u6210\u6548\u679c\u51fa\u8272\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709\u7279\u5f81\u7f13\u5b58\u65b9\u6cd5\u56e0\u65e0\u6cd5\u51c6\u786e\u5efa\u6a21\u7279\u5f81\u6f14\u5316\u590d\u6742\u52a8\u6001\u800c\u5bfc\u81f4\u8d28\u91cf\u635f\u5931", "method": "\u5229\u7528\u6269\u6563\u53d8\u6362\u5668\u4e2d\u7279\u5f81\u5bfc\u6570\u8fd1\u4f3c\u5177\u6709\u591a\u5143\u9ad8\u65af\u7279\u6027\u7684\u6d1e\u5bdf\uff0c\u91c7\u7528Hermite\u591a\u9879\u5f0f\u4f5c\u4e3a\u7406\u8bba\u6700\u4f18\u57fa\u51fd\u6570\uff0c\u5e76\u5f15\u5165\u53cc\u5c3a\u5ea6\u673a\u5236\u786e\u4fdd\u6570\u503c\u7a33\u5b9a\u6027\u548c\u9884\u6d4b\u7cbe\u5ea6", "result": "\u5728FLUX.1-dev\u4e0a\u5b9e\u73b06.24\u500d\u52a0\u901f\u4e14\u8d28\u91cf\u8d85\u8fc7\u57fa\u7ebf\uff0c\u5728\u6587\u672c\u5230\u56fe\u50cf\u3001\u89c6\u9891\u751f\u6210\u548c\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u4f18\u5f02", "conclusion": "HiCache\u901a\u8fc7\u7406\u8bba\u5bf9\u9f50\u548c\u7ecf\u9a8c\u7279\u6027\u7684\u7ed3\u5408\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u63a8\u7406\u52a0\u901f\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.17012", "pdf": "https://arxiv.org/pdf/2508.17012", "abs": "https://arxiv.org/abs/2508.17012", "authors": ["Diram Tabaa", "Gianni Di Caro"], "title": "Fiducial Marker Splatting for High-Fidelity Robotics Simulations", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "High-fidelity 3D simulation is critical for training mobile robots, but its traditional reliance on mesh-based representations often struggle in complex environments, such as densely packed greenhouses featuring occlusions and repetitive structures. Recent neural rendering methods, like Gaussian Splatting (GS), achieve remarkable visual realism but lack flexibility to incorporate fiducial markers, which are essential for robotic localization and control. We propose a hybrid framework that combines the photorealism of GS with structured marker representations. Our core contribution is a novel algorithm for efficiently generating GS-based fiducial markers (e.g., AprilTags) within cluttered scenes. Experiments show that our approach outperforms traditional image-fitting techniques in both efficiency and pose-estimation accuracy. We further demonstrate the framework's potential in a greenhouse simulation. This agricultural setting serves as a challenging testbed, as its combination of dense foliage, similar-looking elements, and occlusions pushes the limits of perception, thereby highlighting the framework's value for real-world applications.", "AI": {"tldr": "\u57fa\u4e8e\u9ad8\u65af\u6298\u5c04\u7684\u6df7\u5408\u6846\u67b6\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u9ad8\u6548\u751f\u6210\u51c6\u786e\u7684\u6807\u8bb0\u70b9\uff0c\u63d0\u5347\u6a21\u62df\u771f\u5b9e\u6027\u548c\u673a\u5668\u4eba\u5b9a\u4f4d\u7cbe\u5ea6", "motivation": "\u4f20\u7edf\u7f51\u683c\u57fa\u4e8e\u6e32\u67d3\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u5c40\u9650\uff0c\u800c\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\u867d\u7136\u89c6\u89c9\u771f\u5b9e\u5374\u7f3a\u4e4f\u6807\u8bb0\u70b9\u96c6\u6210\u80fd\u529b\uff0c\u800c\u6807\u8bb0\u70b9\u5bf9\u4e8e\u673a\u5668\u4eba\u5b9a\u4f4d\u548c\u63a7\u5236\u81f3\u5173\u91cd\u8981", "method": "\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u9ad8\u65af\u6298\u5c04(GS)\u7684\u7167\u7247\u5b9e\u611f\u6027\u548c\u7ed3\u6784\u5316\u6807\u8bb0\u8868\u793a\uff0c\u8bbe\u8ba1\u65b0\u7b97\u6cd5\u9ad8\u6548\u751f\u6210GS\u57fa\u4e8e\u6807\u8bb0\u70b9", "result": "\u5728\u6548\u7387\u548c\u59ff\u6001\u4f30\u8ba1\u51c6\u786e\u6027\u65b9\u9762\u8d85\u8fc7\u4f20\u7edf\u56fe\u50cf\u62df\u5408\u6280\u672f\uff0c\u5728\u6e29\u5ba4\u6a21\u62df\u73af\u5883\u4e2d\u9a8c\u8bc1\u6846\u67b6\u6548\u679c", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u9ad8\u4fdd\u771f\u5ea6\u6a21\u62df\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u519c\u4e1a\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u5e94\u7528\u573a\u666f\u4e2d\u663e\u793a\u51fa\u5f3a\u5927\u6f5c\u529b"}}
{"id": "2508.17017", "pdf": "https://arxiv.org/pdf/2508.17017", "abs": "https://arxiv.org/abs/2508.17017", "authors": ["Konstantina Nikolaidou", "George Retsinas", "Giorgos Sfikas", "Silvia Cascianelli", "Rita Cucchiara", "Marcus Liwicki"], "title": "Dual Orthogonal Guidance for Robust Diffusion-based Handwritten Text Generation", "categories": ["cs.CV"], "comment": "10 pages, 10 figures", "summary": "Diffusion-based Handwritten Text Generation (HTG) approaches achieve impressive results on frequent, in-vocabulary words observed at training time and on regular styles. However, they are prone to memorizing training samples and often struggle with style variability and generation clarity. In particular, standard diffusion models tend to produce artifacts or distortions that negatively affect the readability of the generated text, especially when the style is hard to produce. To tackle these issues, we propose a novel sampling guidance strategy, Dual Orthogonal Guidance (DOG), that leverages an orthogonal projection of a negatively perturbed prompt onto the original positive prompt. This approach helps steer the generation away from artifacts while maintaining the intended content, and encourages more diverse, yet plausible, outputs. Unlike standard Classifier-Free Guidance (CFG), which relies on unconditional predictions and produces noise at high guidance scales, DOG introduces a more stable, disentangled direction in the latent space. To control the strength of the guidance across the denoising process, we apply a triangular schedule: weak at the start and end of denoising, when the process is most sensitive, and strongest in the middle steps. Experimental results on the state-of-the-art DiffusionPen and One-DM demonstrate that DOG improves both content clarity and style variability, even for out-of-vocabulary words and challenging writing styles.", "AI": {"tldr": "\u63d0\u51faDual Orthogonal Guidance (DOG)\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u4ea4\u6295\u5f71\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u751f\u6210\u66f4\u6e05\u6670\u3001\u66f4\u591a\u6837\u5316\u7684\u624b\u5199\u6587\u672c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u98ce\u683c\u53d8\u5316\u548c\u751f\u6210\u6e05\u6670\u5ea6\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u624b\u5199\u6587\u672c\u751f\u6210\u65b9\u6cd5\u5bb9\u6613\u8bb0\u5fc6\u8bad\u7ec3\u6837\u672c\uff0c\u5728\u98ce\u683c\u53d8\u5316\u548c\u751f\u6210\u6e05\u6670\u5ea6\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u56f0\u96be\u98ce\u683c\u65f6\u5bb9\u6613\u4ea7\u751f\u4f2a\u5f71\u548c\u5931\u771f\uff0c\u5f71\u54cd\u6587\u672c\u53ef\u8bfb\u6027\u3002", "method": "\u63d0\u51faDOG\u5f15\u5bfc\u7b56\u7565\uff0c\u5229\u7528\u8d1f\u5411\u6270\u52a8\u63d0\u793a\u7684\u6b63\u4ea4\u6295\u5f71\u5230\u539f\u59cb\u6b63\u5411\u63d0\u793a\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5f15\u5165\u66f4\u7a33\u5b9a\u3001\u89e3\u8026\u7684\u65b9\u5411\u3002\u91c7\u7528\u4e09\u89d2\u5f62\u8c03\u5ea6\u63a7\u5236\u53bb\u566a\u8fc7\u7a0b\u4e2d\u5f15\u5bfc\u5f3a\u5ea6\uff0c\u5728\u5f00\u59cb\u548c\u7ed3\u675f\u65f6\u8f83\u5f31\uff0c\u4e2d\u95f4\u6b65\u9aa4\u6700\u5f3a\u3002", "result": "\u5728DiffusionPen\u548cOne-DM\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDOG\u63d0\u9ad8\u4e86\u5185\u5bb9\u6e05\u6670\u5ea6\u548c\u98ce\u683c\u591a\u6837\u6027\uff0c\u5373\u4f7f\u5728\u8bcd\u6c47\u8868\u5916\u5355\u8bcd\u548c\u6311\u6218\u6027\u4e66\u5199\u98ce\u683c\u4e0b\u4e5f\u80fd\u53d6\u5f97\u826f\u597d\u6548\u679c\u3002", "conclusion": "DOG\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u624b\u5199\u6587\u672c\u751f\u6210\u4e2d\u7684\u4f2a\u5f71\u548c\u5931\u771f\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u548c\u591a\u6837\u5316\u7684\u751f\u6210\u6548\u679c\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684Classifier-Free Guidance\u65b9\u6cd5\u3002"}}
{"id": "2508.17029", "pdf": "https://arxiv.org/pdf/2508.17029", "abs": "https://arxiv.org/abs/2508.17029", "authors": ["Mingliang Li", "Lin Yuanbo Wu", "Changhong Liu", "Hanxi Li"], "title": "A Novel Local Focusing Mechanism for Deepfake Detection Generalization", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of deepfake generation techniques has intensified the need for robust and generalizable detection methods. Existing approaches based on reconstruction learning typically leverage deep convolutional networks to extract differential features. However, these methods show poor generalization across object categories (e.g., from faces to cars) and generation domains (e.g., from GANs to Stable Diffusion), due to intrinsic limitations of deep CNNs. First, models trained on a specific category tend to overfit to semantic feature distributions, making them less transferable to other categories, especially as network depth increases. Second, Global Average Pooling (GAP) compresses critical local forgery cues into a single vector, thus discarding discriminative patterns vital for real-fake classification. To address these issues, we propose a novel Local Focus Mechanism (LFM) that explicitly attends to discriminative local features for differentiating fake from real images. LFM integrates a Salience Network (SNet) with a task-specific Top-K Pooling (TKP) module to select the K most informative local patterns. To mitigate potential overfitting introduced by Top-K pooling, we introduce two regularization techniques: Rank-Based Linear Dropout (RBLD) and Random-K Sampling (RKS), which enhance the model's robustness. LFM achieves a 3.7 improvement in accuracy and a 2.8 increase in average precision over the state-of-the-art Neighboring Pixel Relationships (NPR) method, while maintaining exceptional efficiency at 1789 FPS on a single NVIDIA A6000 GPU. Our approach sets a new benchmark for cross-domain deepfake detection. The source code are available in https://github.com/lmlpy/LFM.git", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5c40\u90e8\u805a\u7126\u673a\u5236(LFM)\u6765\u89e3\u51b3\u73b0\u6709\u6df1\u4f5c\u5076\u68c0\u6d4b\u65b9\u6cd5\u5728\u8de8\u7269\u4f53\u7c7b\u522b\u548c\u8de8\u751f\u6210\u57df\u4e0a\u7684\u7cbe\u5ea6\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u91cd\u5efa\u5b66\u4e60\u65b9\u6cd5\u57fa\u4e8e\u6df1\u5ea6\u5377\u79ef\u7f51\u7edc\uff0c\u4f46\u5b58\u5728\u4e24\u4e2a\u6838\u5fc3\u95ee\u9898\uff1a\u4e00\u662f\u6a21\u578b\u5bb9\u6613\u8fc7\u62df\u5408\u7279\u5b9a\u8bed\u4e49\u7279\u5f81\u5206\u5e03\uff0c\u4e8c\u662f\u5168\u5c40\u5e73\u5747\u6c60\u5316\u4f1a\u4e22\u5931\u5173\u952e\u7684\u5c40\u90e8\u5076\u5197\u7ebf\u7d22\u3002\u8fd9\u5bfc\u81f4\u65b9\u6cd5\u5728\u8de8\u7269\u4f53\u7c7b\u522b\u548c\u8de8\u751f\u6210\u57df\u65f6\u6027\u80fd\u5dee\u3002", "method": "\u63d0\u51fa\u5c40\u90e8\u805a\u7126\u673a\u5236(LFM)\uff0c\u901a\u8fc7\u663e\u5f0f\u5173\u6ce8\u533a\u522b\u6027\u5c40\u90e8\u7279\u5f81\u6765\u533a\u5206\u771f\u4f2a\u56fe\u50cf\u3002LFM\u6574\u5408\u4e86\u663e\u8457\u6027\u7f51\u7edc(SNet)\u548c\u4efb\u52a1\u7279\u5b9a\u7684Top-K\u6c60\u5316(TKP)\u6a21\u5757\uff0c\u9009\u62e9K\u4e2a\u6700\u6709\u4fe1\u606f\u7684\u5c40\u90e8\u6a21\u5f0f\u3002\u4e3a\u4e86\u51cf\u8f7bTop-K\u6c60\u5316\u5f15\u5165\u7684\u8fc7\u62df\u5408\u98ce\u9669\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e24\u79cd\u6b63\u5219\u5316\u6280\u672f\uff1a\u57fa\u4e8e\u6392\u540d\u7684\u7ebf\u6027\u6291\u5236(RBLD)\u548c\u968f\u673aK\u91c7\u6837(RKS)\u3002", "result": "LFM\u5728\u51c6\u786e\u7387\u4e0a\u6bd4\u6700\u5148\u8fdb\u7684Neighboring Pixel Relationships(NPR)\u65b9\u6cd5\u63d0\u9ad83.7%\uff0c\u5e73\u5747\u7cbe\u5ea6\u63d0\u9ad82.8%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u6027\uff0c\u5728\u5355\u53f0NVIDIA A6000 GPU\u4e0a\u8fbe\u52301789 FPS\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8de8\u57df\u6df1\u4f5c\u5076\u68c0\u6d4b\u8bbe\u7f6e\u4e86\u65b0\u7684\u6807\u6746\uff0c\u901a\u8fc7\u5c40\u90e8\u7279\u5f81\u805a\u7126\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8de8\u57df\u6027\u80fd\u4e0d\u8db3\u95ee\u9898\u3002"}}
{"id": "2508.17045", "pdf": "https://arxiv.org/pdf/2508.17045", "abs": "https://arxiv.org/abs/2508.17045", "authors": ["Neeraj Matiyali", "Siddharth Srivastava", "Gaurav Sharma"], "title": "Styleclone: Face Stylization with Diffusion Based Data Augmentation", "categories": ["cs.CV"], "comment": null, "summary": "We present StyleClone, a method for training image-to-image translation networks to stylize faces in a specific style, even with limited style images. Our approach leverages textual inversion and diffusion-based guided image generation to augment small style datasets. By systematically generating diverse style samples guided by both the original style images and real face images, we significantly enhance the diversity of the style dataset. Using this augmented dataset, we train fast image-to-image translation networks that outperform diffusion-based methods in speed and quality. Experiments on multiple styles demonstrate that our method improves stylization quality, better preserves source image content, and significantly accelerates inference. Additionally, we provide a systematic evaluation of the augmentation techniques and their impact on stylization performance.", "AI": {"tldr": "StyleClone\u662f\u4e00\u79cd\u901a\u8fc7\u6587\u672c\u53cd\u6f14\u548c\u6269\u6563\u5f15\u5bfc\u56fe\u50cf\u751f\u6210\u6765\u589e\u5f3a\u5c0f\u89c4\u6a21\u98ce\u683c\u6570\u636e\u96c6\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u5feb\u901f\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u7f51\u7edc\uff0c\u5728\u4fdd\u6301\u6e90\u56fe\u50cf\u5185\u5bb9\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8d28\u91cf\u98ce\u683c\u5316", "motivation": "\u89e3\u51b3\u6709\u9650\u98ce\u683c\u56fe\u50cf\u6761\u4ef6\u4e0b\u7684\u4eba\u8138\u98ce\u683c\u5316\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u7f3a\u65f6\u6027\u80fd\u53d7\u9650\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u6269\u5145\u98ce\u683c\u6570\u636e\u96c6\u5e76\u63d0\u5347\u98ce\u683c\u5316\u8d28\u91cf\u7684\u65b9\u6cd5", "method": "\u7ed3\u5408\u6587\u672c\u53cd\u6f14\u548c\u57fa\u4e8e\u6269\u6563\u7684\u5f15\u5bfc\u56fe\u50cf\u751f\u6210\u6280\u672f\uff0c\u7cfb\u7edf\u6027\u5730\u751f\u6210\u591a\u6837\u5316\u98ce\u683c\u6837\u672c\uff0c\u4f7f\u7528\u589e\u5f3a\u540e\u7684\u6570\u636e\u96c6\u8bad\u7ec3\u5feb\u901f\u7684\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u7f51\u7edc", "result": "\u5728\u591a\u79cd\u98ce\u683c\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u98ce\u683c\u5316\u8d28\u91cf\uff0c\u66f4\u597d\u5730\u4fdd\u6301\u4e86\u6e90\u56fe\u50cf\u5185\u5bb9\uff0c\u5e76\u663e\u8457\u52a0\u901f\u4e86\u63a8\u7406\u901f\u5ea6", "conclusion": "StyleClone\u901a\u8fc7\u6709\u6548\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\u89e3\u51b3\u4e86\u5c0f\u6837\u672c\u98ce\u683c\u5316\u95ee\u9898\uff0c\u5728\u901f\u5ea6\u548c\u8d28\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\uff0c\u4e3a\u56fe\u50cf\u98ce\u683c\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.17050", "pdf": "https://arxiv.org/pdf/2508.17050", "abs": "https://arxiv.org/abs/2508.17050", "authors": ["Xianjing Cheng", "Lintai Wu", "Zuowen Wang", "Junhui Hou", "Jie Wen", "Yong Xu"], "title": "PVNet: Point-Voxel Interaction LiDAR Scene Upsampling Via Diffusion Models", "categories": ["cs.CV"], "comment": "14 pages, 11 figures", "summary": "Accurate 3D scene understanding in outdoor environments heavily relies on high-quality point clouds. However, LiDAR-scanned data often suffer from extreme sparsity, severely hindering downstream 3D perception tasks. Existing point cloud upsampling methods primarily focus on individual objects, thus demonstrating limited generalization capability for complex outdoor scenes. To address this issue, we propose PVNet, a diffusion model-based point-voxel interaction framework to perform LiDAR point cloud upsampling without dense supervision. Specifically, we adopt the classifier-free guidance-based DDPMs to guide the generation, in which we employ a sparse point cloud as the guiding condition and the synthesized point clouds derived from its nearby frames as the input. Moreover, we design a voxel completion module to refine and complete the coarse voxel features for enriching the feature representation. In addition, we propose a point-voxel interaction module to integrate features from both points and voxels, which efficiently improves the environmental perception capability of each upsampled point. To the best of our knowledge, our approach is the first scene-level point cloud upsampling method supporting arbitrary upsampling rates. Extensive experiments on various benchmarks demonstrate that our method achieves state-of-the-art performance. The source code will be available at https://github.com/chengxianjing/PVNet.", "AI": {"tldr": "PVNet\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u70b9\u4e91\u4e0a\u91c7\u6837\u6846\u67b6\uff0c\u901a\u8fc7\u70b9-\u4f53\u7d20\u4ea4\u4e92\u673a\u5236\u5904\u7406\u5ba4\u5916\u7a00\u758fLiDAR\u70b9\u4e91\uff0c\u65e0\u9700\u5bc6\u96c6\u76d1\u7763\u5373\u53ef\u5b9e\u73b0\u4efb\u610f\u500d\u7387\u7684\u4e0a\u91c7\u6837\u3002", "motivation": "\u5ba4\u5916LiDAR\u70b9\u4e91\u901a\u5e38\u6781\u5176\u7a00\u758f\uff0c\u4e25\u91cd\u5f71\u54cd3D\u611f\u77e5\u4efb\u52a1\u6027\u80fd\u3002\u73b0\u6709\u4e0a\u91c7\u6837\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u4e2a\u7269\u4f53\uff0c\u5728\u590d\u6742\u5ba4\u5916\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u91c7\u7528\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u7684DDPM\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u7a00\u758f\u70b9\u4e91\u4e3a\u6761\u4ef6\u8f93\u5165\uff0c\u7ed3\u5408\u9644\u8fd1\u5e27\u5408\u6210\u70b9\u4e91\u3002\u8bbe\u8ba1\u4f53\u7d20\u8865\u5168\u6a21\u5757\u548c\u70b9-\u4f53\u7d20\u4ea4\u4e92\u6a21\u5757\uff0c\u878d\u5408\u70b9\u548c\u4f53\u7d20\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u662f\u9996\u4e2a\u652f\u6301\u4efb\u610f\u4e0a\u91c7\u6837\u500d\u7387\u7684\u573a\u666f\u7ea7\u70b9\u4e91\u4e0a\u91c7\u6837\u65b9\u6cd5\u3002", "conclusion": "PVNet\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u70b9-\u4f53\u7d20\u4ea4\u4e92\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5ba4\u5916\u7a00\u758f\u70b9\u4e91\u4e0a\u91c7\u6837\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u573a\u666f\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2508.17061", "pdf": "https://arxiv.org/pdf/2508.17061", "abs": "https://arxiv.org/abs/2508.17061", "authors": ["Stefanos Pasios", "Nikos Nikolaidis"], "title": "REGEN: Real-Time Photorealism Enhancement in Games via a Dual-Stage Generative Network Framework", "categories": ["cs.CV"], "comment": "6 pages", "summary": "Photorealism is an important aspect of modern video games since it can shape the player experience and simultaneously impact the immersion, narrative engagement, and visual fidelity. Although recent hardware technological breakthroughs, along with state-of-the-art rendering technologies, have significantly improved the visual realism of video games, achieving true photorealism in dynamic environments at real-time frame rates still remains a major challenge due to the tradeoff between visual quality and performance. In this short paper, we present a novel approach for enhancing the photorealism of rendered game frames using generative adversarial networks. To this end, we propose Real-time photorealism Enhancement in Games via a dual-stage gEnerative Network framework (REGEN), which employs a robust unpaired image-to-image translation model to produce semantically consistent photorealistic frames that transform the problem into a simpler paired image-to-image translation task. This enables training with a lightweight method that can achieve real-time inference time without compromising visual quality. We demonstrate the effectiveness of our framework on Grand Theft Auto V, showing that the approach achieves visual results comparable to the ones produced by the robust unpaired Im2Im method while improving inference speed by 32.14 times. Our findings also indicate that the results outperform the photorealism-enhanced frames produced by directly training a lightweight unpaired Im2Im translation method to translate the video game frames towards the visual characteristics of real-world images. Code, pre-trained models, and demos for this work are available at: https://github.com/stefanos50/REGEN.", "AI": {"tldr": "REGEN\u6846\u67b6\u901a\u8fc7\u53cc\u9636\u6bb5\u751f\u6210\u7f51\u7edc\u5b9e\u73b0\u6e38\u620f\u753b\u9762\u7684\u5b9e\u65f6\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u589e\u5f3a\uff0c\u5728\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u63a8\u7406\u901f\u5ea6", "motivation": "\u73b0\u4ee3\u6e38\u620f\u867d\u7136\u786c\u4ef6\u548c\u6e32\u67d3\u6280\u672f\u4e0d\u65ad\u8fdb\u6b65\uff0c\u4f46\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u5e27\u7387\u7684\u771f\u6b63\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u4ecd\u7136\u9762\u4e34\u89c6\u89c9\u8d28\u91cf\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u6311\u6218", "method": "\u63d0\u51faREGEN\u6846\u67b6\uff0c\u91c7\u7528\u9c81\u68d2\u7684\u65e0\u914d\u5bf9\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u6a21\u578b\u751f\u6210\u8bed\u4e49\u4e00\u81f4\u7684\u7167\u7247\u7ea7\u771f\u5b9e\u5e27\uff0c\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u66f4\u7b80\u5355\u7684\u914d\u5bf9\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u4efb\u52a1\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u5b9e\u73b0\u5b9e\u65f6\u63a8\u7406", "result": "\u5728GTA V\u4e0a\u9a8c\u8bc1\uff0c\u89c6\u89c9\u6548\u679c\u4e0e\u9c81\u68d2\u65e0\u914d\u5bf9Im2Im\u65b9\u6cd5\u76f8\u5f53\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u534732.14\u500d\uff0c\u4e14\u4f18\u4e8e\u76f4\u63a5\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u65e0\u914d\u5bf9Im2Im\u65b9\u6cd5\u7684\u7ed3\u679c", "conclusion": "REGEN\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u6e38\u620f\u753b\u9762\u5b9e\u65f6\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u589e\u5f3a\u7684\u6311\u6218\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u89c6\u89c9\u6548\u679c\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347"}}
{"id": "2508.17062", "pdf": "https://arxiv.org/pdf/2508.17062", "abs": "https://arxiv.org/abs/2508.17062", "authors": ["Peng Hu", "Yu Gu", "Liang Luo", "Fuji Ren"], "title": "SSG-Dit: A Spatial Signal Guided Framework for Controllable Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Controllable video generation aims to synthesize video content that aligns precisely with user-provided conditions, such as text descriptions and initial images. However, a significant challenge persists in this domain: existing models often struggle to maintain strong semantic consistency, frequently generating videos that deviate from the nuanced details specified in the prompts. To address this issue, we propose SSG-DiT (Spatial Signal Guided Diffusion Transformer), a novel and efficient framework for high-fidelity controllable video generation. Our approach introduces a decoupled two-stage process. The first stage, Spatial Signal Prompting, generates a spatially aware visual prompt by leveraging the rich internal representations of a pre-trained multi-modal model. This prompt, combined with the original text, forms a joint condition that is then injected into a frozen video DiT backbone via our lightweight and parameter-efficient SSG-Adapter. This unique design, featuring a dual-branch attention mechanism, allows the model to simultaneously harness its powerful generative priors while being precisely steered by external spatial signals. Extensive experiments demonstrate that SSG-DiT achieves state-of-the-art performance, outperforming existing models on multiple key metrics in the VBench benchmark, particularly in spatial relationship control and overall consistency.", "AI": {"tldr": "SSG-DiT\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u53ef\u63a7\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u4fe1\u53f7\u5f15\u5bfc\u548c\u53cc\u9636\u6bb5\u89e3\u8026\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u89c6\u9891\u4e0e\u6587\u672c\u63d0\u793a\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u53ef\u63a7\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7ecf\u5e38\u751f\u6210\u4e0e\u63d0\u793a\u7ec6\u8282\u4e0d\u7b26\u7684\u89c6\u9891\u5185\u5bb9\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u89e3\u8026\u6d41\u7a0b\uff1a1\uff09\u7a7a\u95f4\u4fe1\u53f7\u63d0\u793a\u9636\u6bb5\u5229\u7528\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\u751f\u6210\u7a7a\u95f4\u611f\u77e5\u89c6\u89c9\u63d0\u793a\uff1b2\uff09\u901a\u8fc7\u8f7b\u91cf\u7ea7SSG-Adapter\u5c06\u89c6\u89c9\u63d0\u793a\u4e0e\u6587\u672c\u7ed3\u5408\uff0c\u6ce8\u5165\u51bb\u7ed3\u7684\u89c6\u9891DiT\u4e3b\u5e72\uff0c\u91c7\u7528\u53cc\u5206\u652f\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728VBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u7a7a\u95f4\u5173\u7cfb\u63a7\u5236\u548c\u6574\u4f53\u4e00\u81f4\u6027\u7b49\u5173\u952e\u6307\u6807\u4e0a\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "SSG-DiT\u6846\u67b6\u901a\u8fc7\u7a7a\u95f4\u4fe1\u53f7\u5f15\u5bfc\u548c\u53c2\u6570\u9ad8\u6548\u7684\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u53ef\u63a7\u89c6\u9891\u751f\u6210\u4e2d\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e3a\u9ad8\u8d28\u91cf\u89c6\u9891\u5408\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.17130", "pdf": "https://arxiv.org/pdf/2508.17130", "abs": "https://arxiv.org/abs/2508.17130", "authors": ["Catherine Hoier", "Khandaker Mamun Ahmed"], "title": "Structural Damage Detection Using AI Super Resolution and Visual Language Model", "categories": ["cs.CV"], "comment": null, "summary": "Natural disasters pose significant challenges to timely and accurate damage assessment due to their sudden onset and the extensive areas they affect. Traditional assessment methods are often labor-intensive, costly, and hazardous to personnel, making them impractical for rapid response, especially in resource-limited settings. This study proposes a novel, cost-effective framework that leverages aerial drone footage, an advanced AI-based video super-resolution model, Video Restoration Transformer (VRT), and Gemma3:27b, a 27 billion parameter Visual Language Model (VLM). This integrated system is designed to improve low-resolution disaster footage, identify structural damage, and classify buildings into four damage categories, ranging from no/slight damage to total destruction, along with associated risk levels. The methodology was validated using pre- and post-event drone imagery from the 2023 Turkey earthquakes (courtesy of The Guardian) and satellite data from the 2013 Moore Tornado (xBD dataset). The framework achieved a classification accuracy of 84.5%, demonstrating its ability to provide highly accurate results. Furthermore, the system's accessibility allows non-technical users to perform preliminary analyses, thereby improving the responsiveness and efficiency of disaster management efforts.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u65e0\u4eba\u673a\u5f71\u50cf\u3001AI\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u707e\u5bb3\u635f\u5bb3\u8bc4\u4f30\u6846\u67b6\uff0c\u5728\u571f\u8033\u5176\u5730\u9707\u548c\u6469\u5c14\u9f99\u5377\u98ce\u6570\u636e\u4e0a\u8fbe\u523084.5%\u5206\u7c7b\u51c6\u786e\u7387", "motivation": "\u4f20\u7edf\u707e\u5bb3\u635f\u5bb3\u8bc4\u4f30\u65b9\u6cd5\u52b3\u52a8\u5bc6\u96c6\u3001\u6210\u672c\u9ad8\u4e14\u5371\u9669\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u6709\u9650\u73af\u5883\u4e0b\u5feb\u901f\u54cd\u5e94\uff0c\u9700\u8981\u81ea\u52a8\u5316\u3001\u4f4e\u6210\u672c\u89e3\u51b3\u65b9\u6848", "method": "\u96c6\u6210\u65e0\u4eba\u673a\u5f71\u50cf\u3001Video Restoration Transformer\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u548cGemma3:27b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u6784\u5efa\u56db\u7c7b\u635f\u5bb3\u5206\u7c7b\u7cfb\u7edf", "result": "\u57282023\u571f\u8033\u5176\u5730\u9707\u548c2013\u6469\u5c14\u9f99\u5377\u98ce\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0c\u8fbe\u523084.5%\u7684\u5206\u7c7b\u51c6\u786e\u7387", "conclusion": "\u8be5\u6846\u67b6\u80fd\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u635f\u5bb3\u8bc4\u4f30\uff0c\u975e\u6280\u672f\u7528\u6237\u4e5f\u53ef\u8fdb\u884c\u521d\u6b65\u5206\u6790\uff0c\u663e\u8457\u63d0\u5347\u707e\u5bb3\u7ba1\u7406\u54cd\u5e94\u6548\u7387\u548c\u53ef\u53ca\u6027"}}
{"id": "2508.17199", "pdf": "https://arxiv.org/pdf/2508.17199", "abs": "https://arxiv.org/abs/2508.17199", "authors": ["Hyeyeon Kim", "Sungwoo Han", "Jingun Kwon", "Hidetaka Kamigaito", "Manabu Okumura"], "title": "MMCIG: Multimodal Cover Image Generation for Text-only Documents and Its Dataset Construction via Pseudo-labeling", "categories": ["cs.CV"], "comment": null, "summary": "In this study, we introduce a novel cover image generation task that produces both a concise summary and a visually corresponding image from a given text-only document. Because no existing datasets are available for this task, we propose a multimodal pseudo-labeling method to construct high-quality datasets at low cost. We first collect documents that contain multiple images with their captions, and their summaries by excluding factually inconsistent instances. Our approach selects one image from the multiple images accompanying the documents. Using the gold summary, we independently rank both the images and their captions. Then, we annotate a pseudo-label for an image when both the image and its corresponding caption are ranked first in their respective rankings. Finally, we remove documents that contain direct image references within texts. Experimental results demonstrate that the proposed multimodal pseudo-labeling method constructs more precise datasets and generates higher quality images than text- and image-only pseudo-labeling methods, which consider captions and images separately. We release our code at: https://github.com/HyeyeeonKim/MMCIG", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u4f2a\u6807\u6ce8\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u7eaf\u6587\u672c\u6587\u6863\u751f\u6210\u5c01\u9762\u56fe\u50cf\u548c\u6458\u8981\uff0c\u901a\u8fc7\u8054\u5408\u8bc4\u4f30\u56fe\u50cf\u548c\u6807\u9898\u6392\u540d\u6765\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u65e0\u6cd5\u652f\u6301\u4ece\u7eaf\u6587\u672c\u6587\u6863\u751f\u6210\u5bf9\u5e94\u5c01\u9762\u56fe\u50cf\u548c\u6458\u8981\u7684\u4efb\u52a1\uff0c\u9700\u8981\u4f4e\u6210\u672c\u6784\u5efa\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e", "method": "\u591a\u6a21\u6001\u4f2a\u6807\u6ce8\u65b9\u6cd5\uff1a\u6536\u96c6\u542b\u591a\u56fe\u50cf\u548c\u6807\u9898\u7684\u6587\u6863\uff0c\u6392\u9664\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u5b9e\u4f8b\uff1b\u57fa\u4e8e\u9ec4\u91d1\u6458\u8981\u5206\u522b\u5bf9\u56fe\u50cf\u548c\u6807\u9898\u6392\u540d\uff1b\u5f53\u56fe\u50cf\u548c\u5bf9\u5e94\u6807\u9898\u5747\u6392\u540d\u7b2c\u4e00\u65f6\u6807\u6ce8\u4f2a\u6807\u7b7e\uff1b\u79fb\u9664\u6587\u672c\u4e2d\u76f4\u63a5\u5f15\u7528\u56fe\u50cf\u7684\u6587\u6863", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u591a\u6a21\u6001\u4f2a\u6807\u6ce8\u65b9\u6cd5\u6bd4\u5355\u72ec\u8003\u8651\u6807\u9898\u548c\u56fe\u50cf\u7684\u65b9\u6cd5\u6784\u5efa\u66f4\u7cbe\u786e\u7684\u6570\u636e\u96c6\uff0c\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf", "conclusion": "\u591a\u6a21\u6001\u4f2a\u6807\u6ce8\u65b9\u6cd5\u80fd\u6709\u6548\u6784\u5efa\u5c01\u9762\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5728\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u4e0a\u4f18\u4e8e\u5355\u6a21\u6001\u65b9\u6cd5"}}
{"id": "2508.17230", "pdf": "https://arxiv.org/pdf/2508.17230", "abs": "https://arxiv.org/abs/2508.17230", "authors": ["Chengkai Hou", "Yanjie Ze", "Yankai Fu", "Zeyu Gao", "Songbo Hu", "Yue Yu", "Shanghang Zhang", "Huazhe Xu"], "title": "4D Visual Pre-training for Robot Learning", "categories": ["cs.CV"], "comment": null, "summary": "General visual representations learned from web-scale datasets for robotics have achieved great success in recent years, enabling data-efficient robot learning on manipulation tasks; yet these pre-trained representations are mostly on 2D images, neglecting the inherent 3D nature of the world. However, due to the scarcity of large-scale 3D data, it is still hard to extract a universal 3D representation from web datasets. Instead, we are seeking a general visual pre-training framework that could improve all 3D representations as an alternative. Our framework, called FVP, is a novel 4D Visual Pre-training framework for real-world robot learning. FVP frames the visual pre-training objective as a next-point-cloud-prediction problem, models the prediction model as a diffusion model, and pre-trains the model on the larger public datasets directly. Across twelve real-world manipulation tasks, FVP boosts the average success rate of 3D Diffusion Policy (DP3) for these tasks by 28%. The FVP pre-trained DP3 achieves state-of-the-art performance across imitation learning methods. Moreover, the efficacy of FVP adapts across various point cloud encoders and datasets. Finally, we apply FVP to the RDT-1B, a larger Vision-Language-Action robotic model, enhancing its performance on various robot tasks. Our project page is available at: https://4d- visual-pretraining.github.io/.", "AI": {"tldr": "FVP\u662f\u4e00\u4e2a\u65b0\u9896\u76844D\u89c6\u89c9\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u70b9\u4e91\u9884\u6d4b\u6269\u6563\u6a21\u578b\u63d0\u53473D\u8868\u793a\u6027\u80fd\uff0c\u572812\u4e2a\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5c063D Diffusion Policy\u7684\u6210\u529f\u7387\u5e73\u5747\u63d0\u534728%", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u9884\u8bad\u7ec3\u8868\u793a\u4e3b\u8981\u57fa\u4e8e2D\u56fe\u50cf\uff0c\u5ffd\u7565\u4e86\u4e16\u754c\u76843D\u672c\u8d28\uff0c\u4f46\u7531\u4e8e\u5927\u89c4\u6a213D\u6570\u636e\u7a00\u7f3a\uff0c\u96be\u4ee5\u4ece\u7f51\u7edc\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u901a\u75283D\u8868\u793a", "method": "\u5c06\u89c6\u89c9\u9884\u8bad\u7ec3\u76ee\u6807\u6784\u5efa\u4e3a\u4e0b\u4e00\u4e2a\u70b9\u4e91\u9884\u6d4b\u95ee\u9898\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u9884\u6d4b\u6a21\u578b\uff0c\u76f4\u63a5\u5728\u5927\u578b\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3", "result": "\u572812\u4e2a\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cFVP\u5c063D Diffusion Policy\u7684\u5e73\u5747\u6210\u529f\u7387\u63d0\u534728%\uff0c\u8fbe\u5230\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u80fd\u9002\u914d\u4e0d\u540c\u70b9\u4e91\u7f16\u7801\u5668\u548c\u6570\u636e\u96c6", "conclusion": "FVP\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e863D\u8868\u793a\u6027\u80fd\uff0c\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u4e0d\u540c\u6a21\u578b\u548c\u6570\u636e\u96c6\uff0c\u5305\u62ec\u589e\u5f3aRDT-1B\u7b49\u5927\u578b\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u673a\u5668\u4eba\u6a21\u578b\u5728\u5404\u79cd\u673a\u5668\u4eba\u4efb\u52a1\u4e0a\u7684\u8868\u73b0"}}
{"id": "2508.17299", "pdf": "https://arxiv.org/pdf/2508.17299", "abs": "https://arxiv.org/abs/2508.17299", "authors": ["Zhihao Chen", "Qi Gao", "Zilong Li", "Junping Zhang", "Yi Zhang", "Jun Zhao", "Hongming Shan"], "title": "FoundDiff: Foundational Diffusion Model for Generalizable Low-Dose CT Denoising", "categories": ["cs.CV"], "comment": "10 pages, 7 figures", "summary": "Low-dose computed tomography (CT) denoising is crucial for reduced radiation exposure while ensuring diagnostically acceptable image quality. Despite significant advancements driven by deep learning (DL) in recent years, existing DL-based methods, typically trained on a specific dose level and anatomical region, struggle to handle diverse noise characteristics and anatomical heterogeneity during varied scanning conditions, limiting their generalizability and robustness in clinical scenarios. In this paper, we propose FoundDiff, a foundational diffusion model for unified and generalizable LDCT denoising across various dose levels and anatomical regions. FoundDiff employs a two-stage strategy: (i) dose-anatomy perception and (ii) adaptive denoising. First, we develop a dose- and anatomy-aware contrastive language image pre-training model (DA-CLIP) to achieve robust dose and anatomy perception by leveraging specialized contrastive learning strategies to learn continuous representations that quantify ordinal dose variations and identify salient anatomical regions. Second, we design a dose- and anatomy-aware diffusion model (DA-Diff) to perform adaptive and generalizable denoising by synergistically integrating the learned dose and anatomy embeddings from DACLIP into diffusion process via a novel dose and anatomy conditional block (DACB) based on Mamba. Extensive experiments on two public LDCT datasets encompassing eight dose levels and three anatomical regions demonstrate superior denoising performance of FoundDiff over existing state-of-the-art methods and the remarkable generalization to unseen dose levels. The codes and models are available at https://github.com/hao1635/FoundDiff.", "AI": {"tldr": "FoundDiff\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u57fa\u7840\u6027\u4f4e\u5242\u91cfCT\u53bb\u566a\u65b9\u6cd5\uff0c\u80fd\u591f\u7edf\u4e00\u5904\u7406\u4e0d\u540c\u5242\u91cf\u6c34\u5e73\u548c\u89e3\u5256\u533a\u57df\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u7b56\u7565\u5b9e\u73b0\u5242\u91cf-\u89e3\u5256\u611f\u77e5\u548c\u81ea\u9002\u5e94\u53bb\u566a\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4f4e\u5242\u91cfCT\u53bb\u566a\u65b9\u6cd5\u901a\u5e38\u5728\u7279\u5b9a\u5242\u91cf\u6c34\u5e73\u548c\u89e3\u5256\u533a\u57df\u8bad\u7ec3\uff0c\u96be\u4ee5\u5904\u7406\u4e0d\u540c\u626b\u63cf\u6761\u4ef6\u4e0b\u7684\u566a\u58f0\u7279\u5f81\u548c\u89e3\u5256\u5f02\u8d28\u6027\uff0c\u9650\u5236\u4e86\u5728\u4e34\u5e8a\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u7b56\u7565\uff1a1) \u5242\u91cf-\u89e3\u5256\u611f\u77e5\u7684\u5bf9\u6bd4\u8bed\u8a00\u56fe\u50cf\u9884\u8bad\u7ec3\u6a21\u578b(DA-CLIP)\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5b66\u4e60\u8fde\u7eed\u8868\u793a\u6765\u91cf\u5316\u5242\u91cf\u53d8\u5316\u548c\u8bc6\u522b\u89e3\u5256\u533a\u57df\uff1b2) \u5242\u91cf-\u89e3\u5256\u611f\u77e5\u7684\u6269\u6563\u6a21\u578b(DA-Diff)\uff0c\u901a\u8fc7\u57fa\u4e8eMamba\u7684\u5242\u91cf\u89e3\u5256\u6761\u4ef6\u5757\u6574\u5408\u5b66\u4e60\u5230\u7684\u5d4c\u5165\u4fe1\u606f\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5171LDCT\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFoundDiff\u5728\u516b\u4e2a\u5242\u91cf\u6c34\u5e73\u548c\u4e09\u4e2a\u89e3\u5256\u533a\u57df\u4e0a\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u53bb\u566a\u6027\u80fd\uff0c\u5e76\u5bf9\u672a\u89c1\u5242\u91cf\u6c34\u5e73\u5177\u6709\u663e\u8457\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "FoundDiff\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u6cdb\u5316\u7684\u4f4e\u5242\u91cfCT\u53bb\u566a\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5242\u91cf-\u89e3\u5256\u611f\u77e5\u673a\u5236\u548c\u6269\u6563\u6a21\u578b\u96c6\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u4e0d\u540c\u4e34\u5e8a\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2508.17302", "pdf": "https://arxiv.org/pdf/2508.17302", "abs": "https://arxiv.org/abs/2508.17302", "authors": ["Peilin Xiong", "Junwen Chen", "Honghui Yuan", "Keiji Yanai"], "title": "PosBridge: Multi-View Positional Embedding Transplant for Identity-Aware Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "Localized subject-driven image editing aims to seamlessly integrate user-specified objects into target scenes. As generative models continue to scale, training becomes increasingly costly in terms of memory and computation, highlighting the need for training-free and scalable editing frameworks.To this end, we propose PosBridge an efficient and flexible framework for inserting custom objects. A key component of our method is positional embedding transplant, which guides the diffusion model to faithfully replicate the structural characteristics of reference objects.Meanwhile, we introduce the Corner Centered Layout, which concatenates reference images and the background image as input to the FLUX.1-Fill model. During progressive denoising, positional embedding transplant is applied to guide the noise distribution in the target region toward that of the reference object. In this way, Corner Centered Layout effectively directs the FLUX.1-Fill model to synthesize identity-consistent content at the desired location. Extensive experiments demonstrate that PosBridge outperforms mainstream baselines in structural consistency, appearance fidelity, and computational efficiency, showcasing its practical value and potential for broad adoption.", "AI": {"tldr": "PosBridge\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u6548\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u4f4d\u7f6e\u5d4c\u5165\u79fb\u690d\u548c\u89d2\u70b9\u4e2d\u5fc3\u5e03\u5c40\u6280\u672f\uff0c\u5728\u76ee\u6807\u573a\u666f\u4e2d\u7cbe\u786e\u63d2\u5165\u7528\u6237\u6307\u5b9a\u7684\u5bf9\u8c61", "motivation": "\u968f\u7740\u751f\u6210\u6a21\u578b\u7684\u89c4\u6a21\u4e0d\u65ad\u6269\u5927\uff0c\u8bad\u7ec3\u6210\u672c\u6025\u5267\u589e\u52a0\uff0c\u9700\u8981\u5f00\u53d1\u65e0\u9700\u8bad\u7ec3\u4e14\u53ef\u6269\u5c55\u7684\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\u6765\u5b9e\u73b0\u5b9a\u5236\u5316\u5bf9\u8c61\u7684\u65e0\u7f1d\u63d2\u5165", "method": "\u91c7\u7528\u4f4d\u7f6e\u5d4c\u5165\u79fb\u690d\u6280\u672f\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u590d\u5236\u53c2\u8003\u5bf9\u8c61\u7684\u7ed3\u6784\u7279\u5f81\uff0c\u7ed3\u5408\u89d2\u70b9\u4e2d\u5fc3\u5e03\u5c40\u5c06\u53c2\u8003\u56fe\u50cf\u548c\u80cc\u666f\u56fe\u50cf\u62fc\u63a5\u4f5c\u4e3aFLUX.1-Fill\u6a21\u578b\u7684\u8f93\u5165\uff0c\u5728\u6e10\u8fdb\u53bb\u566a\u8fc7\u7a0b\u4e2d\u6307\u5bfc\u76ee\u6807\u533a\u57df\u7684\u566a\u58f0\u5206\u5e03", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660ePosBridge\u5728\u7ed3\u6784\u4e00\u81f4\u6027\u3001\u5916\u89c2\u4fdd\u771f\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u4e3b\u6d41\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "PosBridge\u6846\u67b6\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u548c\u5e7f\u6cdb\u91c7\u7528\u6f5c\u529b\uff0c\u4e3a\u65e0\u9700\u8bad\u7ec3\u7684\u5b9a\u5236\u5316\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.17356", "pdf": "https://arxiv.org/pdf/2508.17356", "abs": "https://arxiv.org/abs/2508.17356", "authors": ["Jiazi Bu", "Pengyang Ling", "Yujie Zhou", "Yibin Wang", "Yuhang Zang", "Tong Wu", "Dahua Lin", "Jiaqi Wang"], "title": "DiCache: Let Diffusion Model Determine Its Own Cache", "categories": ["cs.CV"], "comment": null, "summary": "Recent years have witnessed the rapid development of acceleration techniques for diffusion models, especially caching-based acceleration methods. These studies seek to answer two fundamental questions: \"When to cache\" and \"How to use cache\", typically relying on predefined empirical laws or dataset-level priors to determine the timing of caching and utilizing handcrafted rules for leveraging multi-step caches. However, given the highly dynamic nature of the diffusion process, they often exhibit limited generalizability and fail on outlier samples. In this paper, a strong correlation is revealed between the variation patterns of the shallow-layer feature differences in the diffusion model and those of final model outputs. Moreover, we have observed that the features from different model layers form similar trajectories. Based on these observations, we present DiCache, a novel training-free adaptive caching strategy for accelerating diffusion models at runtime, answering both when and how to cache within a unified framework. Specifically, DiCache is composed of two principal components: (1) Online Probe Profiling Scheme leverages a shallow-layer online probe to obtain a stable prior for the caching error in real time, enabling the model to autonomously determine caching schedules. (2) Dynamic Cache Trajectory Alignment combines multi-step caches based on shallow-layer probe feature trajectory to better approximate the current feature, facilitating higher visual quality. Extensive experiments validate DiCache's capability in achieving higher efficiency and improved visual fidelity over state-of-the-art methods on various leading diffusion models including WAN 2.1, HunyuanVideo for video generation, and Flux for image generation.", "AI": {"tldr": "DiCache\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u81ea\u9002\u5e94\u7f13\u5b58\u7b56\u7565\uff0c\u901a\u8fc7\u6d45\u5c42\u5728\u7ebf\u63a2\u9488\u5b9e\u65f6\u83b7\u53d6\u7f13\u5b58\u8bef\u5dee\u5148\u9a8c\uff0c\u7ed3\u5408\u52a8\u6001\u7f13\u5b58\u8f68\u8ff9\u5bf9\u9f50\u6280\u672f\uff0c\u5728\u6269\u6563\u6a21\u578b\u4e2d\u5b9e\u73b0\u9ad8\u6548\u52a0\u901f\u548c\u66f4\u597d\u7684\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u7f13\u5b58\u52a0\u901f\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u7ecf\u9a8c\u6cd5\u5219\u6216\u6570\u636e\u96c6\u7ea7\u5148\u9a8c\uff0c\u5728\u9ad8\u5ea6\u52a8\u6001\u7684\u6269\u6563\u8fc7\u7a0b\u4e2d\u6cdb\u5316\u6027\u6709\u9650\uff0c\u65e0\u6cd5\u5904\u7406\u5f02\u5e38\u6837\u672c\u3002\u7814\u7a76\u53d1\u73b0\u6d45\u5c42\u7279\u5f81\u5dee\u5f02\u53d8\u5316\u6a21\u5f0f\u4e0e\u6700\u7ec8\u8f93\u51fa\u53d8\u5316\u6a21\u5f0f\u5b58\u5728\u5f3a\u76f8\u5173\u6027\u3002", "method": "DiCache\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u5728\u7ebf\u63a2\u9488\u5206\u6790\u65b9\u6848\uff0c\u5229\u7528\u6d45\u5c42\u5728\u7ebf\u63a2\u9488\u5b9e\u65f6\u83b7\u53d6\u7a33\u5b9a\u7f13\u5b58\u8bef\u5dee\u5148\u9a8c\uff1b2) \u52a8\u6001\u7f13\u5b58\u8f68\u8ff9\u5bf9\u9f50\uff0c\u57fa\u4e8e\u6d45\u5c42\u63a2\u9488\u7279\u5f81\u8f68\u8ff9\u7ec4\u5408\u591a\u6b65\u7f13\u5b58\u6765\u66f4\u597d\u8fd1\u4f3c\u5f53\u524d\u7279\u5f81\u3002", "result": "\u5728WAN 2.1\u3001HunyuanVideo\u89c6\u9891\u751f\u6210\u548cFlux\u56fe\u50cf\u751f\u6210\u7b49\u591a\u79cd\u9886\u5148\u6269\u6563\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0cDiCache\u5728\u6548\u7387\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "DiCache\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u89e3\u51b3\u4f55\u65f6\u7f13\u5b58\u548c\u5982\u4f55\u4f7f\u7528\u7f13\u5b58\u7684\u95ee\u9898\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u81ea\u9002\u5e94\u7f13\u5b58\u7b56\u7565\uff0c\u5728\u6269\u6563\u6a21\u578b\u52a0\u901f\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.17364", "pdf": "https://arxiv.org/pdf/2508.17364", "abs": "https://arxiv.org/abs/2508.17364", "authors": ["Guoqing Zhang", "Xingtong Ge", "Lu Shi", "Xin Zhang", "Muqing Xue", "Wanru Xu", "Yigang Cen"], "title": "Condition Weaving Meets Expert Modulation: Towards Universal and Controllable Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The image-to-image generation task aims to produce controllable images by leveraging conditional inputs and prompt instructions. However, existing methods often train separate control branches for each type of condition, leading to redundant model structures and inefficient use of computational resources. To address this, we propose a Unified image-to-image Generation (UniGen) framework that supports diverse conditional inputs while enhancing generation efficiency and expressiveness. Specifically, to tackle the widely existing parameter redundancy and computational inefficiency in controllable conditional generation architectures, we propose the Condition Modulated Expert (CoMoE) module. This module aggregates semantically similar patch features and assigns them to dedicated expert modules for visual representation and conditional modeling. By enabling independent modeling of foreground features under different conditions, CoMoE effectively mitigates feature entanglement and redundant computation in multi-condition scenarios. Furthermore, to bridge the information gap between the backbone and control branches, we propose WeaveNet, a dynamic, snake-like connection mechanism that enables effective interaction between global text-level control from the backbone and fine-grained control from conditional branches. Extensive experiments on the Subjects-200K and MultiGen-20M datasets across various conditional image generation tasks demonstrate that our method consistently achieves state-of-the-art performance, validating its advantages in both versatility and effectiveness. The code has been uploaded to https://github.com/gavin-gqzhang/UniGen.", "AI": {"tldr": "\u63d0\u51fa\u4e86UniGen\u7edf\u4e00\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7CoMoE\u6a21\u5757\u89e3\u51b3\u591a\u6761\u4ef6\u751f\u6210\u4e2d\u7684\u53c2\u6570\u5197\u4f59\u95ee\u9898\uff0c\u4f7f\u7528WeaveNet\u673a\u5236\u589e\u5f3a\u4e3b\u5e72\u7f51\u7edc\u4e0e\u6761\u4ef6\u5206\u652f\u7684\u4ea4\u4e92\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3a\u6bcf\u79cd\u6761\u4ef6\u7c7b\u578b\u8bad\u7ec3\u5355\u72ec\u7684\u63a7\u5236\u5206\u652f\uff0c\u5bfc\u81f4\u6a21\u578b\u7ed3\u6784\u5197\u4f59\u548c\u8ba1\u7b97\u8d44\u6e90\u4f7f\u7528\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u7edf\u4e00\u7684\u6846\u67b6\u6765\u652f\u6301\u591a\u6837\u5316\u6761\u4ef6\u8f93\u5165\u3002", "method": "\u63d0\u51faCondition Modulated Expert (CoMoE)\u6a21\u5757\u805a\u5408\u8bed\u4e49\u76f8\u4f3c\u7684\u8865\u4e01\u7279\u5f81\u5e76\u5206\u914d\u7ed9\u4e13\u7528\u4e13\u5bb6\u6a21\u5757\uff0c\u4ee5\u53caWeaveNet\u52a8\u6001\u8fde\u63a5\u673a\u5236\u5b9e\u73b0\u4e3b\u5e72\u7f51\u7edc\u4e0e\u6761\u4ef6\u5206\u652f\u7684\u6709\u6548\u4ea4\u4e92\u3002", "result": "\u5728Subjects-200K\u548cMultiGen-20M\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u59cb\u7ec8\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "UniGen\u6846\u67b6\u5728\u591a\u529f\u80fd\u6027\u548c\u6709\u6548\u6027\u65b9\u9762\u90fd\u5177\u6709\u4f18\u52bf\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3\u591a\u6761\u4ef6\u573a\u666f\u4e2d\u7684\u7279\u5f81\u7ea0\u7f20\u548c\u5197\u4f59\u8ba1\u7b97\u95ee\u9898\u3002"}}
{"id": "2508.17397", "pdf": "https://arxiv.org/pdf/2508.17397", "abs": "https://arxiv.org/abs/2508.17397", "authors": ["Aoqi Li", "Yanghui Song", "Jichao Dao", "Chengfu Yang"], "title": "Enhancing Underwater Images via Deep Learning: A Comparative Study of VGG19 and ResNet50-Based Approaches", "categories": ["cs.CV", "eess.IV"], "comment": "7 pages, 6 figures,2025 IEEE 3rd International Conference on Image   Processing and Computer Applications (ICIPCA 2025)", "summary": "This paper addresses the challenging problem of image enhancement in complex underwater scenes by proposing a solution based on deep learning. The proposed method skillfully integrates two deep convolutional neural network models, VGG19 and ResNet50, leveraging their powerful feature extraction capabilities to perform multi-scale and multi-level deep feature analysis of underwater images. By constructing a unified model, the complementary advantages of the two models are effectively integrated, achieving a more comprehensive and accurate image enhancement effect.To objectively evaluate the enhancement effect, this paper introduces image quality assessment metrics such as PSNR, UCIQE, and UIQM to quantitatively compare images before and after enhancement and deeply analyzes the performance of different models in different scenarios.Furthermore, to improve the practicality and stability of the underwater visual enhancement system, this paper also provides practical suggestions from aspects such as model optimization, multi-model fusion, and hardware selection, aiming to provide strong technical support for visual enhancement tasks in complex underwater environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8eVGG19\u548cResNet50\u53cc\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u878d\u5408\u7684\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u5206\u6790\u548c\u7edf\u4e00\u6a21\u578b\u6574\u5408\u5b9e\u73b0\u66f4\u5168\u9762\u7684\u56fe\u50cf\u589e\u5f3a\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u6c34\u4e0b\u573a\u666f\u4e2d\u56fe\u50cf\u589e\u5f3a\u7684\u6311\u6218\u6027\u95ee\u9898\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u63d0\u5347\u6c34\u4e0b\u89c6\u89c9\u8d28\u91cf\u3002", "method": "\u96c6\u6210VGG19\u548cResNet50\u4e24\u4e2a\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u8fdb\u884c\u591a\u5c3a\u5ea6\u591a\u5c42\u6b21\u6df1\u5ea6\u7279\u5f81\u5206\u6790\uff0c\u6784\u5efa\u7edf\u4e00\u6a21\u578b\u6574\u5408\u4e24\u8005\u7684\u4e92\u8865\u4f18\u52bf\u3002", "result": "\u4f7f\u7528PSNR\u3001UCIQE\u3001UIQM\u7b49\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u8fdb\u884c\u5b9a\u91cf\u6bd4\u8f83\uff0c\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u5206\u6790\u4e0d\u540c\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u6a21\u578b\u4f18\u5316\u3001\u591a\u6a21\u578b\u878d\u5408\u548c\u786c\u4ef6\u9009\u62e9\u7b49\u5b9e\u7528\u5efa\u8bae\uff0c\u4e3a\u590d\u6742\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u589e\u5f3a\u4efb\u52a1\u63d0\u4f9b\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2508.17404", "pdf": "https://arxiv.org/pdf/2508.17404", "abs": "https://arxiv.org/abs/2508.17404", "authors": ["Haoyu Wang", "Hao Tang", "Donglin Di", "Zhilu Zhang", "Wangmeng Zuo", "Feng Gao", "Siwei Ma", "Shiliang Zhang"], "title": "MoCo: Motion-Consistent Human Video Generation via Structure-Appearance Decoupling", "categories": ["cs.CV"], "comment": "Project: https://hywang2002.github.io/MoCo", "summary": "Generating human videos with consistent motion from text prompts remains a significant challenge, particularly for whole-body or long-range motion. Existing video generation models prioritize appearance fidelity, resulting in unrealistic or physically implausible human movements with poor structural coherence. Additionally, most existing human video datasets primarily focus on facial or upper-body motions, or consist of vertically oriented dance videos, limiting the scope of corresponding generation methods to simple movements. To overcome these challenges, we propose MoCo, which decouples the process of human video generation into two components: structure generation and appearance generation. Specifically, our method first employs an efficient 3D structure generator to produce a human motion sequence from a text prompt. The remaining video appearance is then synthesized under the guidance of the generated structural sequence. To improve fine-grained control over sparse human structures, we introduce Human-Aware Dynamic Control modules and integrate dense tracking constraints during training. Furthermore, recognizing the limitations of existing datasets, we construct a large-scale whole-body human video dataset featuring complex and diverse motions. Extensive experiments demonstrate that MoCo outperforms existing approaches in generating realistic and structurally coherent human videos.", "AI": {"tldr": "MoCo\u662f\u4e00\u4e2a\u65b0\u7684\u4eba\u7c7b\u89c6\u9891\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u751f\u6210\u8fc7\u7a0b\u89e3\u8026\u4e3a\u7ed3\u6784\u751f\u6210\u548c\u5916\u89c2\u751f\u6210\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5168\u8eab\u8fd0\u52a8\u548c\u957f\u8ddd\u79bb\u8fd0\u52a8\u65b9\u9762\u7684\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u8fc7\u4e8e\u6ce8\u91cd\u5916\u89c2\u4fdd\u771f\u5ea6\uff0c\u5bfc\u81f4\u4eba\u4f53\u8fd0\u52a8\u4e0d\u771f\u5b9e\u3001\u7269\u7406\u4e0a\u4e0d\u53ef\u884c\u4e14\u7ed3\u6784\u8fde\u8d2f\u6027\u5dee\u3002\u73b0\u6709\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u9762\u90e8\u6216\u4e0a\u534a\u8eab\u8fd0\u52a8\uff0c\u9650\u5236\u4e86\u751f\u6210\u65b9\u6cd5\u7684\u9002\u7528\u8303\u56f4\u3002", "method": "\u63d0\u51faMoCo\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u9ad8\u6548\u76843D\u7ed3\u6784\u751f\u6210\u5668\u4ece\u6587\u672c\u63d0\u793a\u751f\u6210\u4eba\u4f53\u8fd0\u52a8\u5e8f\u5217\uff1b2\uff09\u5728\u751f\u6210\u7684\u7ed3\u6784\u5e8f\u5217\u6307\u5bfc\u4e0b\u5408\u6210\u89c6\u9891\u5916\u89c2\uff1b3\uff09\u5f15\u5165Human-Aware Dynamic Control\u6a21\u5757\u548c\u6539\u8fdb\u7684\u5bc6\u96c6\u8ddf\u8e2a\u7ea6\u675f\uff1b4\uff09\u6784\u5efa\u5927\u89c4\u6a21\u5168\u8eab\u4eba\u4f53\u89c6\u9891\u6570\u636e\u96c6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cMoCo\u5728\u751f\u6210\u771f\u5b9e\u4e14\u7ed3\u6784\u8fde\u8d2f\u7684\u4eba\u4f53\u89c6\u9891\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MoCo\u901a\u8fc7\u89e3\u8026\u7ed3\u6784\u548c\u5916\u89c2\u751f\u6210\u8fc7\u7a0b\uff0c\u7ed3\u5408\u65b0\u7684\u63a7\u5236\u6a21\u5757\u548c\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u4f53\u89c6\u9891\u751f\u6210\u7684\u8d28\u91cf\u548c\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u4e3a\u590d\u6742\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.17434", "pdf": "https://arxiv.org/pdf/2508.17434", "abs": "https://arxiv.org/abs/2508.17434", "authors": ["Linwei Dong", "Qingnan Fan", "Yuhang Yu", "Qi Zhang", "Jinwei Chen", "Yawei Luo", "Changqing Zou"], "title": "TinySR: Pruning Diffusion for Real-World Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Real-world image super-resolution (Real-ISR) focuses on recovering high-quality images from low-resolution inputs that suffer from complex degradations like noise, blur, and compression. Recently, diffusion models (DMs) have shown great potential in this area by leveraging strong generative priors to restore fine details. However, their iterative denoising process incurs high computational overhead, posing challenges for real-time applications. Although one-step distillation methods, such as OSEDiff and TSD-SR, offer faster inference, they remain fundamentally constrained by their large, over-parameterized model architectures. In this work, we present TinySR, a compact yet effective diffusion model specifically designed for Real-ISR that achieves real-time performance while maintaining perceptual quality. We introduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy to facilitate more effective decision-making in depth pruning. We achieve VAE compression through channel pruning, attention removal and lightweight SepConv. We eliminate time- and prompt-related modules and perform pre-caching techniques to further speed up the model. TinySR significantly reduces computational cost and model size, achieving up to 5.68x speedup and 83% parameter reduction compared to its teacher TSD-SR, while still providing high quality results.", "AI": {"tldr": "TinySR\u662f\u4e00\u4e2a\u7d27\u51d1\u9ad8\u6548\u7684\u6269\u6563\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u5b9e\u65f6\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff0c\u901a\u8fc7\u6df1\u5ea6\u526a\u679d\u3001VAE\u538b\u7f29\u548c\u9884\u7f13\u5b58\u6280\u672f\uff0c\u5728\u4fdd\u6301\u611f\u77e5\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e865.68\u500d\u52a0\u901f\u548c83%\u53c2\u6570\u51cf\u5c11", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\u867d\u7136\u6548\u679c\u597d\uff0c\u4f46\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u800c\u4e00\u6b65\u84b8\u998f\u65b9\u6cd5\u53c8\u53d7\u9650\u4e8e\u5927\u6a21\u578b\u67b6\u6784\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u5e94\u7528\u9700\u6c42", "method": "\u63d0\u51fa\u52a8\u6001\u5757\u95f4\u6fc0\u6d3b\u548c\u6269\u5c55-\u8150\u8680\u7b56\u7565\u8fdb\u884c\u6df1\u5ea6\u526a\u679d\uff0c\u901a\u8fc7\u901a\u9053\u526a\u679d\u3001\u6ce8\u610f\u529b\u79fb\u9664\u548c\u8f7b\u91cfSepConv\u5b9e\u73b0VAE\u538b\u7f29\uff0c\u6d88\u9664\u65f6\u95f4\u548c\u63d0\u793a\u76f8\u5173\u6a21\u5757\uff0c\u5e76\u91c7\u7528\u9884\u7f13\u5b58\u6280\u672f\u52a0\u901f", "result": "\u76f8\u6bd4\u6559\u5e08\u6a21\u578bTSD-SR\uff0c\u5b9e\u73b0\u4e865.68\u500d\u52a0\u901f\u548c83%\u53c2\u6570\u51cf\u5c11\uff0c\u540c\u65f6\u4ecd\u80fd\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u8d85\u5206\u8fa8\u7387\u7ed3\u679c", "conclusion": "TinySR\u8bc1\u660e\u4e86\u5728\u4fdd\u6301\u6269\u6563\u6a21\u578b\u611f\u77e5\u8d28\u91cf\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u6709\u6548\u7684\u6a21\u578b\u538b\u7f29\u548c\u4f18\u5316\u6280\u672f\u53ef\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.17435", "pdf": "https://arxiv.org/pdf/2508.17435", "abs": "https://arxiv.org/abs/2508.17435", "authors": ["Zihan Liang", "Jiahao Sun", "Haoran Ma"], "title": "An LLM-LVLM Driven Agent for Iterative and Fine-Grained Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "Despite the remarkable capabilities of text-to-image (T2I) generation models, real-world applications often demand fine-grained, iterative image editing that existing methods struggle to provide. Key challenges include granular instruction understanding, robust context preservation during modifications, and the lack of intelligent feedback mechanisms for iterative refinement. This paper introduces RefineEdit-Agent, a novel, training-free intelligent agent framework designed to address these limitations by enabling complex, iterative, and context-aware image editing. RefineEdit-Agent leverages the powerful planning capabilities of Large Language Models (LLMs) and the advanced visual understanding and evaluation prowess of Vision-Language Large Models (LVLMs) within a closed-loop system. Our framework comprises an LVLM-driven instruction parser and scene understanding module, a multi-level LLM-driven editing planner for goal decomposition, tool selection, and sequence generation, an iterative image editing module, and a crucial LVLM-driven feedback and evaluation loop. To rigorously evaluate RefineEdit-Agent, we propose LongBench-T2I-Edit, a new benchmark featuring 500 initial images with complex, multi-turn editing instructions across nine visual dimensions. Extensive experiments demonstrate that RefineEdit-Agent significantly outperforms state-of-the-art baselines, achieving an average score of 3.67 on LongBench-T2I-Edit, compared to 2.29 for Direct Re-Prompting, 2.91 for InstructPix2Pix, 3.16 for GLIGEN-based Edit, and 3.39 for ControlNet-XL. Ablation studies, human evaluations, and analyses of iterative refinement, backbone choices, tool usage, and robustness to instruction complexity further validate the efficacy of our agentic design in delivering superior edit fidelity and context preservation.", "AI": {"tldr": "RefineEdit-Agent\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u667a\u80fd\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7LLM\u89c4\u5212\u80fd\u529b\u548cLVLM\u89c6\u89c9\u7406\u89e3\u5b9e\u73b0\u590d\u6742\u8fed\u4ee3\u5f0f\u56fe\u50cf\u7f16\u8f91\uff0c\u5728LongBench-T2I-Edit\u57fa\u51c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u8fed\u4ee3\u7f16\u8f91\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u5305\u62ec\u6307\u4ee4\u7406\u89e3\u4e0d\u7cbe\u786e\u3001\u4e0a\u4e0b\u6587\u4fdd\u6301\u56f0\u96be\u4ee5\u53ca\u7f3a\u4e4f\u667a\u80fd\u53cd\u9988\u673a\u5236\u3002", "method": "\u6784\u5efa\u95ed\u73af\u7cfb\u7edf\uff0c\u5305\u542bLVLM\u9a71\u52a8\u7684\u6307\u4ee4\u89e3\u6790\u548c\u573a\u666f\u7406\u89e3\u6a21\u5757\u3001\u591a\u7ea7LLM\u9a71\u52a8\u7684\u7f16\u8f91\u89c4\u5212\u5668\u3001\u8fed\u4ee3\u56fe\u50cf\u7f16\u8f91\u6a21\u5757\u4ee5\u53caLVLM\u9a71\u52a8\u7684\u53cd\u9988\u8bc4\u4f30\u5faa\u73af\u3002", "result": "\u5728500\u5f20\u521d\u59cb\u56fe\u50cf\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u5f97\u52063.67\uff0c\u663e\u8457\u4f18\u4e8eDirect Re-Prompting(2.29)\u3001InstructPix2Pix(2.91)\u3001GLIGEN-based Edit(3.16)\u548cControlNet-XL(3.39)\u3002", "conclusion": "RefineEdit-Agent\u901a\u8fc7\u667a\u80fd\u4ee3\u7406\u8bbe\u8ba1\u6709\u6548\u63d0\u5347\u4e86\u7f16\u8f91\u4fdd\u771f\u5ea6\u548c\u4e0a\u4e0b\u6587\u4fdd\u6301\u80fd\u529b\uff0c\u6d88\u878d\u5b9e\u9a8c\u548c\u4eba\u5de5\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.17437", "pdf": "https://arxiv.org/pdf/2508.17437", "abs": "https://arxiv.org/abs/2508.17437", "authors": ["Long Le", "Ryan Lucas", "Chen Wang", "Chuhao Chen", "Dinesh Jayaraman", "Eric Eaton", "Lingjie Liu"], "title": "Pixie: Fast and Generalizable Supervised Learning of 3D Physics from Pixels", "categories": ["cs.CV"], "comment": "Website: https://pixie-3d.github.io/", "summary": "Inferring the physical properties of 3D scenes from visual information is a critical yet challenging task for creating interactive and realistic virtual worlds. While humans intuitively grasp material characteristics such as elasticity or stiffness, existing methods often rely on slow, per-scene optimization, limiting their generalizability and application. To address this problem, we introduce PIXIE, a novel method that trains a generalizable neural network to predict physical properties across multiple scenes from 3D visual features purely using supervised losses. Once trained, our feed-forward network can perform fast inference of plausible material fields, which coupled with a learned static scene representation like Gaussian Splatting enables realistic physics simulation under external forces. To facilitate this research, we also collected PIXIEVERSE, one of the largest known datasets of paired 3D assets and physic material annotations. Extensive evaluations demonstrate that PIXIE is about 1.46-4.39x better and orders of magnitude faster than test-time optimization methods. By leveraging pretrained visual features like CLIP, our method can also zero-shot generalize to real-world scenes despite only ever been trained on synthetic data. https://pixie-3d.github.io/", "AI": {"tldr": "PIXIE\u662f\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u80fd\u591f\u4ece3D\u89c6\u89c9\u7279\u5f81\u5feb\u901f\u63a8\u65ad\u7269\u7406\u6750\u8d28\u5c5e\u6027\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u5e76\u4e14\u80fd\u591f\u96f6\u6837\u672c\u6cdb\u5316\u5230\u771f\u5b9e\u573a\u666f\u3002", "motivation": "\u4ece\u89c6\u89c9\u4fe1\u606f\u63a8\u65ad3D\u573a\u666f\u7269\u7406\u5c5e\u6027\u662f\u521b\u5efa\u4ea4\u4e92\u5f0f\u865a\u62df\u4e16\u754c\u7684\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7f13\u6162\u7684\u9010\u573a\u666f\u4f18\u5316\uff0c\u9650\u5236\u4e86\u901a\u7528\u6027\u548c\u5e94\u7528\u3002", "method": "\u8bad\u7ec3\u901a\u7528\u795e\u7ecf\u7f51\u7edc\u4ece3D\u89c6\u89c9\u7279\u5f81\u9884\u6d4b\u7269\u7406\u5c5e\u6027\uff0c\u4f7f\u7528\u76d1\u7763\u635f\u5931\uff0c\u7ed3\u5408\u9ad8\u65af\u6cfc\u6e85\u7b49\u9759\u6001\u573a\u666f\u8868\u793a\uff0c\u652f\u6301\u5feb\u901f\u524d\u5411\u63a8\u7406\u3002", "result": "PIXIE\u6bd4\u6d4b\u8bd5\u65f6\u4f18\u5316\u65b9\u6cd5\u597d1.46-4.39\u500d\uff0c\u901f\u5ea6\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u901a\u8fc7CLIP\u7b49\u9884\u8bad\u7ec3\u89c6\u89c9\u7279\u5f81\u80fd\u591f\u96f6\u6837\u672c\u6cdb\u5316\u5230\u771f\u5b9e\u573a\u666f\u3002", "conclusion": "PIXIE\u63d0\u4f9b\u4e86\u4e00\u79cd\u5feb\u901f\u3001\u901a\u7528\u7684\u7269\u7406\u5c5e\u6027\u63a8\u65ad\u65b9\u6cd5\uff0c\u4e3a\u521b\u5efa\u903c\u771f\u7684\u4ea4\u4e92\u5f0f\u865a\u62df\u4e16\u754c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u53d1\u5e03\u4e86\u5927\u578b\u6570\u636e\u96c6PIXIEVERSE\u652f\u6301\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2508.17468", "pdf": "https://arxiv.org/pdf/2508.17468", "abs": "https://arxiv.org/abs/2508.17468", "authors": ["Pedro Antonio Rabelo Saraiva", "Enzo Ferreira de Souza", "Joao Manoel Herrera Pinheiro", "Thiago H. Segreto", "Ricardo V. Godoy", "Marcelo Becker"], "title": "A Synthetic Dataset for Manometry Recognition in Robotic Applications", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "This work addresses the challenges of data scarcity and high acquisition costs for training robust object detection models in complex industrial environments, such as offshore oil platforms. The practical and economic barriers to collecting real-world data in these hazardous settings often hamper the development of autonomous inspection systems. To overcome this, in this work we propose and validate a hybrid data synthesis pipeline that combines procedural rendering with AI-driven video generation. Our methodology leverages BlenderProc to create photorealistic images with precise annotations and controlled domain randomization, and integrates NVIDIA's Cosmos-Predict2 world-foundation model to synthesize physically plausible video sequences with temporal diversity, capturing rare viewpoints and adverse conditions. We demonstrate that a YOLO-based detection network trained on a composite dataset, blending real images with our synthetic data, achieves superior performance compared to models trained exclusively on real-world data. Notably, a 1:1 mixture of real and synthetic data yielded the highest accuracy, surpassing the real-only baseline. These findings highlight the viability of a synthetic-first approach as an efficient, cost-effective, and safe alternative for developing reliable perception systems in safety-critical and resource-constrained industrial applications.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u6570\u636e\u5408\u6210\u7ba1\u9053\uff0c\u7ed3\u5408\u7a0b\u5e8f\u5316\u6e32\u67d3\u548cAI\u89c6\u9891\u751f\u6210\uff0c\u89e3\u51b3\u5de5\u4e1a\u73af\u5883\u4e2d\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u8bc1\u660e\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u6df7\u5408\u8bad\u7ec3\u80fd\u663e\u8457\u63d0\u5347\u76ee\u6807\u68c0\u6d4b\u6027\u80fd", "motivation": "\u89e3\u51b3\u590d\u6742\u5de5\u4e1a\u73af\u5883\uff08\u5982\u6d77\u4e0a\u77f3\u6cb9\u5e73\u53f0\uff09\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u9ad8\u91c7\u96c6\u6210\u672c\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u5371\u9669\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u548c\u7ecf\u6d4e\u969c\u788d\u963b\u788d\u4e86\u81ea\u4e3b\u68c0\u6d4b\u7cfb\u7edf\u7684\u53d1\u5c55", "method": "\u4f7f\u7528BlenderProc\u521b\u5efa\u5177\u6709\u7cbe\u786e\u6807\u6ce8\u548c\u53ef\u63a7\u57df\u968f\u673a\u5316\u7684\u7167\u7247\u7ea7\u771f\u5b9e\u56fe\u50cf\uff0c\u6574\u5408NVIDIA\u7684Cosmos-Predict2\u4e16\u754c\u57fa\u7840\u6a21\u578b\u5408\u6210\u7269\u7406\u4e0a\u5408\u7406\u7684\u89c6\u9891\u5e8f\u5217\uff0c\u91c7\u7528YOLO\u68c0\u6d4b\u7f51\u7edc\u5728\u6df7\u5408\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3", "result": "\u5728\u771f\u5b9e\u56fe\u50cf\u4e0e\u5408\u6210\u6570\u636e\u76841:1\u6df7\u5408\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u8fbe\u5230\u4e86\u6700\u9ad8\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4e86\u4ec5\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u7684\u57fa\u7ebf\u6a21\u578b", "conclusion": "\u5408\u6210\u4f18\u5148\u65b9\u6cd5\u662f\u5728\u5b89\u5168\u5173\u952e\u548c\u8d44\u6e90\u53d7\u9650\u7684\u5de5\u4e1a\u5e94\u7528\u4e2d\u5f00\u53d1\u53ef\u9760\u611f\u77e5\u7cfb\u7edf\u7684\u9ad8\u6548\u3001\u7ecf\u6d4e\u4e14\u5b89\u5168\u7684\u53ef\u884c\u66ff\u4ee3\u65b9\u6848"}}
{"id": "2508.17472", "pdf": "https://arxiv.org/pdf/2508.17472", "abs": "https://arxiv.org/abs/2508.17472", "authors": ["Kaiyue Sun", "Rongyao Fang", "Chengqi Duan", "Xian Liu", "Xihui Liu"], "title": "T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image Generation", "categories": ["cs.CV"], "comment": "Code: https://github.com/KaiyueSun98/T2I-ReasonBench", "summary": "We propose T2I-ReasonBench, a benchmark evaluating reasoning capabilities of text-to-image (T2I) models. It consists of four dimensions: Idiom Interpretation, Textual Image Design, Entity-Reasoning and Scientific-Reasoning. We propose a two-stage evaluation protocol to assess the reasoning accuracy and image quality. We benchmark various T2I generation models, and provide comprehensive analysis on their performances.", "AI": {"tldr": "T2I-ReasonBench\u662f\u4e00\u4e2a\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u56db\u4e2a\u7ef4\u5ea6\uff1a\u6210\u8bed\u89e3\u91ca\u3001\u6587\u672c\u56fe\u50cf\u8bbe\u8ba1\u3001\u5b9e\u4f53\u63a8\u7406\u548c\u79d1\u5b66\u63a8\u7406\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bc4\u4f30\u534f\u8bae", "motivation": "\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u8fd9\u65b9\u9762\u5b58\u5728\u4e0d\u8db3", "method": "\u63d0\u51fa\u5305\u542b\u56db\u4e2a\u7ef4\u5ea6\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e24\u9636\u6bb5\u8bc4\u4f30\u534f\u8bae\uff08\u63a8\u7406\u51c6\u786e\u6027\u548c\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff09", "result": "\u5bf9\u5404\u79cdT2I\u751f\u6210\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6027\u80fd\u5206\u6790", "conclusion": "T2I-ReasonBench\u4e3a\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177"}}
{"id": "2508.17579", "pdf": "https://arxiv.org/pdf/2508.17579", "abs": "https://arxiv.org/abs/2508.17579", "authors": ["Meida Chen", "Luis Leal", "Yue Hu", "Rong Liu", "Butian Xiong", "Andrew Feng", "Jiuyi Xu", "Yangming Shi"], "title": "IDU: Incremental Dynamic Update of Existing 3D Virtual Environments with New Imagery Data", "categories": ["cs.CV"], "comment": null, "summary": "For simulation and training purposes, military organizations have made substantial investments in developing high-resolution 3D virtual environments through extensive imaging and 3D scanning. However, the dynamic nature of battlefield conditions-where objects may appear or vanish over time-makes frequent full-scale updates both time-consuming and costly. In response, we introduce the Incremental Dynamic Update (IDU) pipeline, which efficiently updates existing 3D reconstructions, such as 3D Gaussian Splatting (3DGS), with only a small set of newly acquired images. Our approach starts with camera pose estimation to align new images with the existing 3D model, followed by change detection to pinpoint modifications in the scene. A 3D generative AI model is then used to create high-quality 3D assets of the new elements, which are seamlessly integrated into the existing 3D model. The IDU pipeline incorporates human guidance to ensure high accuracy in object identification and placement, with each update focusing on a single new object at a time. Experimental results confirm that our proposed IDU pipeline significantly reduces update time and labor, offering a cost-effective and targeted solution for maintaining up-to-date 3D models in rapidly evolving military scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u589e\u91cf\u52a8\u6001\u66f4\u65b0\uff08IDU\uff09\u7ba1\u9053\uff0c\u901a\u8fc7\u5c11\u91cf\u65b0\u56fe\u50cf\u9ad8\u6548\u66f4\u65b03D\u9ad8\u65af\u6e85\u5c04\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c11\u519b\u4e8b\u573a\u666f\u4e2d3D\u6a21\u578b\u66f4\u65b0\u7684\u65f6\u95f4\u548c\u6210\u672c", "motivation": "\u519b\u4e8b\u7ec4\u7ec7\u5f00\u53d1\u9ad8\u5206\u8fa8\u73873D\u865a\u62df\u73af\u5883\u6210\u672c\u9ad8\u6602\uff0c\u6218\u573a\u73af\u5883\u52a8\u6001\u53d8\u5316\u9700\u8981\u9891\u7e41\u66f4\u65b0\uff0c\u4f20\u7edf\u5168\u89c4\u6a21\u66f4\u65b0\u65b9\u6cd5\u8017\u65f6\u8017\u529b", "method": "IDU\u7ba1\u9053\u5305\u542b\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u3001\u53d8\u5316\u68c0\u6d4b\u30013D\u751f\u6210AI\u6a21\u578b\u521b\u5efa\u65b0\u8d44\u4ea7\u3001\u4eba\u5de5\u6307\u5bfc\u4e0b\u7684\u65e0\u7f1d\u96c6\u6210\uff0c\u6bcf\u6b21\u66f4\u65b0\u4e13\u6ce8\u4e8e\u5355\u4e2a\u65b0\u5bf9\u8c61", "result": "\u5b9e\u9a8c\u8bc1\u5b9eIDU\u7ba1\u9053\u663e\u8457\u51cf\u5c11\u66f4\u65b0\u65f6\u95f4\u548c\u4eba\u5de5\u6210\u672c", "conclusion": "IDU\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u3001\u6709\u9488\u5bf9\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5feb\u901f\u6f14\u53d8\u7684\u519b\u4e8b\u573a\u666f\u4e2d\u4fdd\u63013D\u6a21\u578b\u7684\u6700\u65b0\u72b6\u6001"}}
{"id": "2508.17588", "pdf": "https://arxiv.org/pdf/2508.17588", "abs": "https://arxiv.org/abs/2508.17588", "authors": ["Quanjian Song", "Xinyu Wang", "Donghao Zhou", "Jingyu Lin", "Cunjian Chen", "Yue Ma", "Xiu Li"], "title": "HERO: Hierarchical Extrapolation and Refresh for Efficient World Models", "categories": ["cs.CV"], "comment": "12 pages in total", "summary": "Generation-driven world models create immersive virtual environments but suffer slow inference due to the iterative nature of diffusion models. While recent advances have improved diffusion model efficiency, directly applying these techniques to world models introduces limitations such as quality degradation. In this paper, we present HERO, a training-free hierarchical acceleration framework tailored for efficient world models. Owing to the multi-modal nature of world models, we identify a feature coupling phenomenon, wherein shallow layers exhibit high temporal variability, while deeper layers yield more stable feature representations. Motivated by this, HERO adopts hierarchical strategies to accelerate inference: (i) In shallow layers, a patch-wise refresh mechanism efficiently selects tokens for recomputation. With patch-wise sampling and frequency-aware tracking, it avoids extra metric computation and remain compatible with FlashAttention. (ii) In deeper layers, a linear extrapolation scheme directly estimates intermediate features. This completely bypasses the computations in attention modules and feed-forward networks. Our experiments show that HERO achieves a 1.73$\\times$ speedup with minimal quality degradation, significantly outperforming existing diffusion acceleration methods.", "AI": {"tldr": "HERO\u662f\u4e00\u4e2a\u9488\u5bf9\u4e16\u754c\u6a21\u578b\u7684\u65e0\u8bad\u7ec3\u5206\u5c42\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u6d45\u5c42\u8865\u4e01\u5237\u65b0\u673a\u5236\u548c\u6df1\u5c42\u7ebf\u6027\u5916\u63a8\u65b9\u6848\uff0c\u5b9e\u73b01.73\u500d\u52a0\u901f\u4e14\u8d28\u91cf\u635f\u5931\u6700\u5c0f", "motivation": "\u751f\u6210\u5f0f\u4e16\u754c\u6a21\u578b\u7531\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8fed\u4ee3\u7279\u6027\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u6162\uff0c\u76f4\u63a5\u5e94\u7528\u73b0\u6709\u52a0\u901f\u6280\u672f\u4f1a\u5bfc\u81f4\u8d28\u91cf\u4e0b\u964d\u3002\u7814\u7a76\u53d1\u73b0\u4e16\u754c\u6a21\u578b\u4e2d\u5b58\u5728\u7279\u5f81\u8026\u5408\u73b0\u8c61\uff1a\u6d45\u5c42\u7279\u5f81\u65f6\u95f4\u53d8\u5316\u5927\uff0c\u6df1\u5c42\u7279\u5f81\u66f4\u7a33\u5b9a", "method": "\u5206\u5c42\u52a0\u901f\u7b56\u7565\uff1a\u6d45\u5c42\u4f7f\u7528\u8865\u4e01\u5237\u65b0\u673a\u5236\u9009\u62e9\u9700\u8981\u91cd\u65b0\u8ba1\u7b97\u7684token\uff0c\u907f\u514d\u989d\u5916\u5ea6\u91cf\u8ba1\u7b97\uff1b\u6df1\u5c42\u4f7f\u7528\u7ebf\u6027\u5916\u63a8\u65b9\u6848\u76f4\u63a5\u4f30\u8ba1\u4e2d\u95f4\u7279\u5f81\uff0c\u5b8c\u5168\u7ed5\u8fc7\u6ce8\u610f\u529b\u6a21\u5757\u548c\u524d\u9988\u7f51\u7edc\u7684\u8ba1\u7b97", "result": "\u5b9e\u9a8c\u663e\u793aHERO\u5b9e\u73b0\u4e861.73\u500d\u7684\u52a0\u901f\uff0c\u8d28\u91cf\u9000\u5316\u6781\u5c0f\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6269\u6563\u52a0\u901f\u65b9\u6cd5", "conclusion": "HERO\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4e16\u754c\u6a21\u578b\u52a0\u901f\u4e2d\u7684\u8d28\u91cf\u9000\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u5c42\u5904\u7406\u7279\u5f81\u8026\u5408\u73b0\u8c61\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u63a8\u7406\u52a0\u901f"}}
{"id": "2508.17614", "pdf": "https://arxiv.org/pdf/2508.17614", "abs": "https://arxiv.org/abs/2508.17614", "authors": ["Aowen Wang", "Wei Li", "Hao Luo", "Mengxing Ao", "Chenyu Zhu", "Xinyang Li", "Fan Wang"], "title": "JCo-MVTON: Jointly Controllable Multi-Modal Diffusion Transformer for Mask-Free Virtual Try-on", "categories": ["cs.CV"], "comment": null, "summary": "Virtual try-on systems have long been hindered by heavy reliance on human body masks, limited fine-grained control over garment attributes, and poor generalization to real-world, in-the-wild scenarios. In this paper, we propose JCo-MVTON (Jointly Controllable Multi-Modal Diffusion Transformer for Mask-Free Virtual Try-On), a novel framework that overcomes these limitations by integrating diffusion-based image generation with multi-modal conditional fusion. Built upon a Multi-Modal Diffusion Transformer (MM-DiT) backbone, our approach directly incorporates diverse control signals -- such as the reference person image and the target garment image -- into the denoising process through dedicated conditional pathways that fuse features within the self-attention layers. This fusion is further enhanced with refined positional encodings and attention masks, enabling precise spatial alignment and improved garment-person integration. To address data scarcity and quality, we introduce a bidirectional generation strategy for dataset construction: one pipeline uses a mask-based model to generate realistic reference images, while a symmetric ``Try-Off'' model, trained in a self-supervised manner, recovers the corresponding garment images. The synthesized dataset undergoes rigorous manual curation, allowing iterative improvement in visual fidelity and diversity. Experiments demonstrate that JCo-MVTON achieves state-of-the-art performance on public benchmarks including DressCode, significantly outperforming existing methods in both quantitative metrics and human evaluations. Moreover, it shows strong generalization in real-world applications, surpassing commercial systems.", "AI": {"tldr": "JCo-MVTON\u662f\u4e00\u4e2a\u65e0\u9700\u63a9\u7801\u7684\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\u865a\u62df\u8bd5\u7a7f\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6761\u4ef6\u878d\u5408\u548c\u53cc\u5411\u751f\u6210\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u63a9\u7801\u7684\u4f9d\u8d56\u3001\u63a7\u5236\u7c92\u5ea6\u4e0d\u8db3\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u865a\u62df\u8bd5\u7a7f\u7cfb\u7edf\u4e25\u91cd\u4f9d\u8d56\u4eba\u4f53\u63a9\u7801\uff0c\u5bf9\u670d\u88c5\u5c5e\u6027\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u6709\u9650\uff0c\u4e14\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u672c\u6587\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u5f00\u53d1\u4e00\u4e2a\u65e0\u9700\u63a9\u7801\u3001\u53ef\u63a7\u6027\u5f3a\u4e14\u6cdb\u5316\u80fd\u529b\u597d\u7684\u865a\u62df\u8bd5\u7a7f\u7cfb\u7edf\u3002", "method": "\u57fa\u4e8e\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668(MM-DiT)\u67b6\u6784\uff0c\u901a\u8fc7\u4e13\u7528\u6761\u4ef6\u8def\u5f84\u5c06\u53c2\u8003\u4eba\u7269\u56fe\u50cf\u548c\u76ee\u6807\u670d\u88c5\u56fe\u50cf\u76f4\u63a5\u6574\u5408\u5230\u53bb\u566a\u8fc7\u7a0b\u4e2d\u3002\u91c7\u7528\u53cc\u5411\u751f\u6210\u7b56\u7565\u6784\u5efa\u6570\u636e\u96c6\uff1a\u4e00\u4e2a\u7ba1\u9053\u4f7f\u7528\u57fa\u4e8e\u63a9\u7801\u7684\u6a21\u578b\u751f\u6210\u771f\u5b9e\u53c2\u8003\u56fe\u50cf\uff0c\u53e6\u4e00\u4e2a\u5bf9\u79f0\u7684\"Try-Off\"\u6a21\u578b\u4ee5\u81ea\u76d1\u7763\u65b9\u5f0f\u6062\u590d\u76f8\u5e94\u670d\u88c5\u56fe\u50cf\u3002", "result": "\u5728DressCode\u7b49\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u5b9a\u91cf\u6307\u6807\u548c\u4eba\u5de5\u8bc4\u4f30\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8d85\u8d8a\u4e86\u5546\u4e1a\u7cfb\u7edf\u3002", "conclusion": "JCo-MVTON\u901a\u8fc7\u591a\u6a21\u6001\u6761\u4ef6\u878d\u5408\u548c\u521b\u65b0\u7684\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u9700\u63a9\u7801\u7684\u9ad8\u8d28\u91cf\u865a\u62df\u8bd5\u7a7f\uff0c\u5728\u63a7\u5236\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7a81\u7834\uff0c\u4e3a\u865a\u62df\u8bd5\u7a7f\u6280\u672f\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.17708", "pdf": "https://arxiv.org/pdf/2508.17708", "abs": "https://arxiv.org/abs/2508.17708", "authors": ["Qinyi Tian", "Spence Cox", "Laura E. Dalton"], "title": "CATformer: Contrastive Adversarial Transformer for Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Super-resolution remains a promising technique to enhance the quality of low-resolution images. This study introduces CATformer (Contrastive Adversarial Transformer), a novel neural network integrating diffusion-inspired feature refinement with adversarial and contrastive learning. CATformer employs a dual-branch architecture combining a primary diffusion-inspired transformer, which progressively refines latent representations, with an auxiliary transformer branch designed to enhance robustness to noise through learned latent contrasts. These complementary representations are fused and decoded using deep Residual-in-Residual Dense Blocks for enhanced reconstruction quality. Extensive experiments on benchmark datasets demonstrate that CATformer outperforms recent transformer-based and diffusion-inspired methods both in efficiency and visual image quality. This work bridges the performance gap among transformer-, diffusion-, and GAN-based methods, laying a foundation for practical applications of diffusion-inspired transformers in super-resolution.", "AI": {"tldr": "CATformer\u662f\u4e00\u79cd\u7ed3\u5408\u6269\u6563\u6a21\u578b\u3001\u5bf9\u6297\u5b66\u4e60\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684\u8d85\u5206\u8fa8\u7387Transformer\u7f51\u7edc\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u67b6\u6784\u548c\u6b8b\u5dee\u5bc6\u96c6\u5757\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf", "motivation": "\u73b0\u6709\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5728transformer\u3001\u6269\u6563\u6a21\u578b\u548cGAN\u4e4b\u95f4\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6574\u5408\u8fd9\u4e9b\u65b9\u6cd5\u4f18\u52bf\u7684\u7edf\u4e00\u6846\u67b6\u6765\u63d0\u5347\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\u548c\u6548\u7387", "method": "\u63d0\u51faCATformer\u7f51\u7edc\uff0c\u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\uff1a\u4e3b\u5206\u652f\u4f7f\u7528\u6269\u6563\u542f\u53d1\u7684transformer\u9010\u6b65\u7ec6\u5316\u6f5c\u5728\u8868\u793a\uff0c\u8f85\u52a9\u5206\u652f\u901a\u8fc7\u5b66\u4e60\u7684\u6f5c\u5728\u5bf9\u6bd4\u589e\u5f3a\u566a\u58f0\u9c81\u68d2\u6027\uff0c\u6700\u540e\u901a\u8fc7\u6b8b\u5dee\u5bc6\u96c6\u5757\u8fdb\u884c\u7279\u5f81\u878d\u5408\u548c\u91cd\u5efa", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCATformer\u5728\u6548\u7387\u548c\u89c6\u89c9\u56fe\u50cf\u8d28\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u8fd1\u7684transformer\u548c\u6269\u6563\u65b9\u6cd5", "conclusion": "CATformer\u6210\u529f\u5f25\u5408\u4e86transformer\u3001\u6269\u6563\u548cGAN\u65b9\u6cd5\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u4e3a\u6269\u6563\u542f\u53d1transformer\u5728\u8d85\u5206\u8fa8\u7387\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2508.17712", "pdf": "https://arxiv.org/pdf/2508.17712", "abs": "https://arxiv.org/abs/2508.17712", "authors": ["Soham Dasgupta", "Shanthika Naik", "Preet Savalia", "Sujay Kumar Ingle", "Avinash Sharma"], "title": "NGD: Neural Gradient Based Deformation for Monocular Garment Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Dynamic garment reconstruction from monocular video is an important yet challenging task due to the complex dynamics and unconstrained nature of the garments. Recent advancements in neural rendering have enabled high-quality geometric reconstruction with image/video supervision. However, implicit representation methods that use volume rendering often provide smooth geometry and fail to model high-frequency details. While template reconstruction methods model explicit geometry, they use vertex displacement for deformation, which results in artifacts. Addressing these limitations, we propose NGD, a Neural Gradient-based Deformation method to reconstruct dynamically evolving textured garments from monocular videos. Additionally, we propose a novel adaptive remeshing strategy for modelling dynamically evolving surfaces like wrinkles and pleats of the skirt, leading to high-quality reconstruction. Finally, we learn dynamic texture maps to capture per-frame lighting and shadow effects. We provide extensive qualitative and quantitative evaluations to demonstrate significant improvements over existing SOTA methods and provide high-quality garment reconstructions.", "AI": {"tldr": "NGD\u65b9\u6cd5\u901a\u8fc7\u795e\u7ecf\u68af\u5ea6\u53d8\u5f62\u548c\u81ea\u9002\u5e94\u7f51\u683c\u91cd\u5212\u5206\u6280\u672f\uff0c\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u91cd\u5efa\u52a8\u6001\u670d\u88c5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7ec6\u8282\u5efa\u6a21\u548c\u53d8\u5f62\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\u5728\u52a8\u6001\u670d\u88c5\u91cd\u5efa\u4e2d\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u9690\u5f0f\u8868\u793a\u65b9\u6cd5\u4ea7\u751f\u8fc7\u4e8e\u5e73\u6ed1\u7684\u51e0\u4f55\u5f62\u72b6\uff0c\u65e0\u6cd5\u5efa\u6a21\u9ad8\u9891\u7ec6\u8282\uff1b\u6a21\u677f\u91cd\u5efa\u65b9\u6cd5\u4f7f\u7528\u9876\u70b9\u4f4d\u79fb\u5bfc\u81f4\u53d8\u5f62\u4f2a\u5f71\u3002", "method": "\u63d0\u51fa\u795e\u7ecf\u68af\u5ea6\u53d8\u5f62\u65b9\u6cd5(NGD)\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u7f51\u683c\u91cd\u5212\u5206\u7b56\u7565\u6765\u5efa\u6a21\u52a8\u6001\u6f14\u5316\u7684\u8868\u9762\u7ec6\u8282\uff08\u5982\u8936\u76b1\uff09\uff0c\u5e76\u5b66\u4e60\u52a8\u6001\u7eb9\u7406\u56fe\u6765\u6355\u6349\u6bcf\u5e27\u7684\u5149\u7167\u548c\u9634\u5f71\u6548\u679c\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\uff0c\u80fd\u591f\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u670d\u88c5\u91cd\u5efa\u7ed3\u679c\u3002", "conclusion": "NGD\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u52a8\u6001\u670d\u88c5\u91cd\u5efa\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u7ec6\u8282\u5efa\u6a21\u548c\u53d8\u5f62\u8d28\u91cf\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u7834\u6027\u8fdb\u5c55\u3002"}}
{"id": "2508.17718", "pdf": "https://arxiv.org/pdf/2508.17718", "abs": "https://arxiv.org/abs/2508.17718", "authors": ["Yang Li", "Songlin Yang", "Xiaoxuan Han", "Wei Wang", "Jing Dong", "Yueming Lyu", "Ziyu Xue"], "title": "Instant Preference Alignment for Text-to-Image Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "17 figures", "summary": "Text-to-image (T2I) generation has greatly enhanced creative expression, yet achieving preference-aligned generation in a real-time and training-free manner remains challenging. Previous methods often rely on static, pre-collected preferences or fine-tuning, limiting adaptability to evolving and nuanced user intents. In this paper, we highlight the need for instant preference-aligned T2I generation and propose a training-free framework grounded in multimodal large language model (MLLM) priors. Our framework decouples the task into two components: preference understanding and preference-guided generation. For preference understanding, we leverage MLLMs to automatically extract global preference signals from a reference image and enrich a given prompt using structured instruction design. Our approach supports broader and more fine-grained coverage of user preferences than existing methods. For preference-guided generation, we integrate global keyword-based control and local region-aware cross-attention modulation to steer the diffusion model without additional training, enabling precise alignment across both global attributes and local elements. The entire framework supports multi-round interactive refinement, facilitating real-time and context-aware image generation. Extensive experiments on the Viper dataset and our collected benchmark demonstrate that our method outperforms prior approaches in both quantitative metrics and human evaluations, and opens up new possibilities for dialog-based generation and MLLM-diffusion integration.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u5b9e\u73b0\u5b9e\u65f6\u504f\u597d\u5bf9\u9f50\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff0c\u901a\u8fc7\u504f\u597d\u7406\u89e3\u548c\u504f\u597d\u5f15\u5bfc\u751f\u6210\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u652f\u6301\u591a\u8f6e\u4ea4\u4e92\u5f0f\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u9884\u6536\u96c6\u504f\u597d\u6216\u5fae\u8c03\uff0c\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u7528\u6237\u610f\u56fe\uff0c\u9700\u8981\u5b9e\u73b0\u5b9e\u65f6\u3001\u8bad\u7ec3\u514d\u8d39\u7684\u504f\u597d\u5bf9\u9f50\u751f\u6210\u3002", "method": "\u5229\u7528MLLM\u4ece\u53c2\u8003\u56fe\u50cf\u81ea\u52a8\u63d0\u53d6\u5168\u5c40\u504f\u597d\u4fe1\u53f7\u5e76\u4e30\u5bcc\u63d0\u793a\u8bcd\uff0c\u7ed3\u5408\u5168\u5c40\u5173\u952e\u8bcd\u63a7\u5236\u548c\u5c40\u90e8\u533a\u57df\u611f\u77e5\u4ea4\u53c9\u6ce8\u610f\u529b\u8c03\u5236\u6765\u5f15\u5bfc\u6269\u6563\u6a21\u578b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5728Viper\u6570\u636e\u96c6\u548c\u81ea\u5efa\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u6307\u6807\u548c\u4eba\u5de5\u8bc4\u4f30\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u57fa\u4e8e\u5bf9\u8bdd\u7684\u751f\u6210\u548cMLLM-\u6269\u6563\u6a21\u578b\u96c6\u6210\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u5b9e\u65f6\u3001\u8bad\u7ec3\u514d\u8d39\u7684\u504f\u597d\u5bf9\u9f50\u56fe\u50cf\u751f\u6210\uff0c\u652f\u6301\u591a\u8f6e\u4ea4\u4e92\u4f18\u5316\uff0c\u5728\u5168\u5c40\u5c5e\u6027\u548c\u5c40\u90e8\u5143\u7d20\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.17760", "pdf": "https://arxiv.org/pdf/2508.17760", "abs": "https://arxiv.org/abs/2508.17760", "authors": ["Mingyue Yang", "Dianxi Shi", "Jialu Zhou", "Xinyu Wei", "Leqian Li", "Shaowu Yang", "Chunping Qiu"], "title": "CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text-to-Image Generation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "In Text-to-Image (T2I) generation, the complexity of entities and their intricate interactions pose a significant challenge for T2I method based on diffusion model: how to effectively control entity and their interactions to produce high-quality images. To address this, we propose CEIDM, a image generation method based on diffusion model with dual controls for entity and interaction. First, we propose an entity interactive relationships mining approach based on Large Language Models (LLMs), extracting reasonable and rich implicit interactive relationships through chain of thought to guide diffusion models to generate high-quality images that are closer to realistic logic and have more reasonable interactive relationships. Furthermore, We propose an interactive action clustering and offset method to cluster and offset the interactive action features contained in each text prompts. By constructing global and local bidirectional offsets, we enhance semantic understanding and detail supplementation of original actions, making the model's understanding of the concept of interactive \"actions\" more accurate and generating images with more accurate interactive actions. Finally, we design an entity control network which generates masks with entity semantic guidance, then leveraging multi-scale convolutional network to enhance entity feature and dynamic network to fuse feature. It effectively controls entities and significantly improves image quality. Experiments show that the proposed CEIDM method is better than the most representative existing methods in both entity control and their interaction control.", "AI": {"tldr": "CEIDM\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u91cd\u63a7\u5236\u673a\u5236\uff08\u5b9e\u4f53\u63a7\u5236\u548c\u4ea4\u4e92\u63a7\u5236\uff09\u6765\u89e3\u51b3\u590d\u6742\u5b9e\u4f53\u53ca\u5176\u4ea4\u4e92\u5173\u7cfb\u7684\u751f\u6210\u96be\u9898\u3002", "motivation": "\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\uff0c\u590d\u6742\u5b9e\u4f53\u53ca\u5176\u7cbe\u7ec6\u4ea4\u4e92\u5173\u7cfb\u7684\u63a7\u5236\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u63a7\u5236\u5b9e\u4f53\u548c\u4ea4\u4e92\u5173\u7cfb\uff0c\u5bfc\u81f4\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u4e0d\u9ad8\u3001\u4ea4\u4e92\u903b\u8f91\u4e0d\u5408\u7406\u3002", "method": "1. \u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u4f53\u4ea4\u4e92\u5173\u7cfb\u6316\u6398\u65b9\u6cd5\uff0c\u901a\u8fc7\u601d\u7ef4\u94fe\u63d0\u53d6\u4e30\u5bcc\u9690\u542b\u4ea4\u4e92\u5173\u7cfb\uff1b2. \u4ea4\u4e92\u52a8\u4f5c\u805a\u7c7b\u548c\u504f\u79fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u5c40\u90e8\u53cc\u5411\u504f\u79fb\u589e\u5f3a\u8bed\u4e49\u7406\u89e3\uff1b3. \u5b9e\u4f53\u63a7\u5236\u7f51\u7edc\uff0c\u751f\u6210\u8bed\u4e49\u5f15\u5bfc\u63a9\u7801\u5e76\u4f7f\u7528\u591a\u5c3a\u5ea6\u5377\u79ef\u7f51\u7edc\u548c\u52a8\u6001\u7f51\u7edc\u878d\u5408\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCEIDM\u65b9\u6cd5\u5728\u5b9e\u4f53\u63a7\u5236\u548c\u4ea4\u4e92\u63a7\u5236\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5177\u4ee3\u8868\u6027\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u66f4\u7b26\u5408\u73b0\u5b9e\u903b\u8f91\u3001\u4ea4\u4e92\u5173\u7cfb\u66f4\u5408\u7406\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002", "conclusion": "CEIDM\u901a\u8fc7\u53cc\u91cd\u63a7\u5236\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u5b9e\u4f53\u53ca\u5176\u4ea4\u4e92\u5173\u7cfb\u7684\u751f\u6210\u95ee\u9898\uff0c\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u4ea4\u4e92\u51c6\u786e\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff0c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.17789", "pdf": "https://arxiv.org/pdf/2508.17789", "abs": "https://arxiv.org/abs/2508.17789", "authors": ["Muhammad Aqeel", "Shakiba Sharifi", "Marco Cristani", "Francesco Setti"], "title": "Robust Anomaly Detection in Industrial Environments via Meta-Learning", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to VISION Workshop at ICCV 2025", "summary": "Anomaly detection is fundamental for ensuring quality control and operational efficiency in industrial environments, yet conventional approaches face significant challenges when training data contains mislabeled samples-a common occurrence in real-world scenarios. This paper presents RAD, a robust anomaly detection framework that integrates Normalizing Flows with Model-Agnostic Meta-Learning to address the critical challenge of label noise in industrial settings. Our approach employs a bi-level optimization strategy where meta-learning enables rapid adaptation to varying noise conditions, while uncertainty quantification guides adaptive L2 regularization to maintain model stability. The framework incorporates multiscale feature processing through pretrained feature extractors and leverages the precise likelihood estimation capabilities of Normalizing Flows for robust anomaly scoring. Comprehensive evaluation on MVTec-AD and KSDD2 datasets demonstrates superior performance, achieving I-AUROC scores of 95.4% and 94.6% respectively under clean conditions, while maintaining robust detection capabilities above 86.8% and 92.1% even when 50% of training samples are mislabeled. The results highlight RAD's exceptional resilience to noisy training conditions and its ability to detect subtle anomalies across diverse industrial scenarios, making it a practical solution for real-world anomaly detection applications where perfect data curation is challenging.", "AI": {"tldr": "RAD\u662f\u4e00\u4e2a\u7ed3\u5408\u6807\u51c6\u5316\u6d41\u548c\u6a21\u578b\u65e0\u5173\u5143\u5b66\u4e60\u7684\u9c81\u68d2\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u4e13\u95e8\u89e3\u51b3\u5de5\u4e1a\u73af\u5883\u4e2d\u8bad\u7ec3\u6570\u636e\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u572850%\u9519\u8bef\u6807\u7b7e\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u6301\u4f18\u5f02\u6027\u80fd", "motivation": "\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8bad\u7ec3\u6570\u636e\u7ecf\u5e38\u5305\u542b\u9519\u8bef\u6807\u7b7e\u6837\u672c\uff0c\u4f20\u7edf\u65b9\u6cd5\u5bf9\u6b64\u654f\u611f\uff0c\u9700\u8981\u5f00\u53d1\u5bf9\u6807\u7b7e\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\u7684\u68c0\u6d4b\u6846\u67b6", "method": "\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u7b56\u7565\uff1a\u5143\u5b66\u4e60\u5feb\u901f\u9002\u5e94\u4e0d\u540c\u566a\u58f0\u6761\u4ef6\uff0c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6307\u5bfc\u81ea\u9002\u5e94L2\u6b63\u5219\u5316\u4fdd\u6301\u6a21\u578b\u7a33\u5b9a\u6027\u3002\u7ed3\u5408\u9884\u8bad\u7ec3\u7279\u5f81\u63d0\u53d6\u5668\u8fdb\u884c\u591a\u5c3a\u5ea6\u7279\u5f81\u5904\u7406\uff0c\u5229\u7528\u6807\u51c6\u5316\u6d41\u7684\u7cbe\u786e\u4f3c\u7136\u4f30\u8ba1\u8fdb\u884c\u9c81\u68d2\u5f02\u5e38\u8bc4\u5206", "result": "\u5728MVTec-AD\u548cKSDD2\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523095.4%\u548c94.6%\u7684I-AUROC\u5206\u6570\uff08\u5e72\u51c0\u6761\u4ef6\u4e0b\uff09\uff0c\u572850%\u9519\u8bef\u6807\u7b7e\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u630186.8%\u548c92.1%\u7684\u68c0\u6d4b\u6027\u80fd", "conclusion": "RAD\u6846\u67b6\u5bf9\u566a\u58f0\u8bad\u7ec3\u6761\u4ef6\u5177\u6709\u5353\u8d8a\u7684\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u68c0\u6d4b\u5404\u79cd\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u7ec6\u5fae\u5f02\u5e38\uff0c\u662f\u5b9e\u9645\u5e94\u7528\u4e2d\u6570\u636e\u6807\u6ce8\u4e0d\u5b8c\u7f8e\u65f6\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.17817", "pdf": "https://arxiv.org/pdf/2508.17817", "abs": "https://arxiv.org/abs/2508.17817", "authors": ["Meiqi Gong", "Hao Zhang", "Xunpeng Yi", "Linfeng Tang", "Jiayi Ma"], "title": "TemCoCo: Temporally Consistent Multi-modal Video Fusion with Visual-Semantic Collaboration", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Existing multi-modal fusion methods typically apply static frame-based image fusion techniques directly to video fusion tasks, neglecting inherent temporal dependencies and leading to inconsistent results across frames. To address this limitation, we propose the first video fusion framework that explicitly incorporates temporal modeling with visual-semantic collaboration to simultaneously ensure visual fidelity, semantic accuracy, and temporal consistency. First, we introduce a visual-semantic interaction module consisting of a semantic branch and a visual branch, with Dinov2 and VGG19 employed for targeted distillation, allowing simultaneous enhancement of both the visual and semantic representations. Second, we pioneer integrate the video degradation enhancement task into the video fusion pipeline by constructing a temporal cooperative module, which leverages temporal dependencies to facilitate weak information recovery. Third, to ensure temporal consistency, we embed a temporal-enhanced mechanism into the network and devise a temporal loss to guide the optimization process. Finally, we introduce two innovative evaluation metrics tailored for video fusion, aimed at assessing the temporal consistency of the generated fused videos. Extensive experimental results on public video datasets demonstrate the superiority of our method. Our code is released at https://github.com/Meiqi-Gong/TemCoCo.", "AI": {"tldr": "\u9996\u4e2a\u89c6\u9891\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u4e49\u534f\u4f5c\u548c\u65f6\u95f4\u5efa\u6a21\uff0c\u540c\u65f6\u4fdd\u8bc1\u89c6\u89c9\u4fdd\u771f\u5ea6\u3001\u8bed\u4e49\u51c6\u786e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u76f4\u63a5\u5c06\u9759\u6001\u5e27\u7ea7\u56fe\u50cf\u878d\u5408\u6280\u672f\u5e94\u7528\u4e8e\u89c6\u9891\u878d\u5408\u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u5bfc\u81f4\u5e27\u95f4\u7ed3\u679c\u4e0d\u4e00\u81f4", "method": "1) \u89c6\u89c9-\u8bed\u4e49\u4ea4\u4e92\u6a21\u5757\uff08Dinov2\u548cVGG19\u84b8\u998f\uff09\uff1b2) \u65f6\u95f4\u534f\u4f5c\u6a21\u5757\u6574\u5408\u89c6\u9891\u9000\u5316\u589e\u5f3a\uff1b3) \u65f6\u95f4\u589e\u5f3a\u673a\u5236\u548c\u65f6\u95f4\u635f\u5931\u51fd\u6570\uff1b4) \u4e24\u4e2a\u65b0\u7684\u89c6\u9891\u878d\u5408\u8bc4\u4f30\u6307\u6807", "result": "\u5728\u516c\u5171\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027", "conclusion": "\u63d0\u51fa\u7684TemCoCo\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u7684\u65f6\u95f4\u5efa\u6a21\u548c\u89c6\u89c9-\u8bed\u4e49\u534f\u4f5c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u878d\u5408\u4e2d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e3a\u89c6\u9891\u878d\u5408\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.17844", "pdf": "https://arxiv.org/pdf/2508.17844", "abs": "https://arxiv.org/abs/2508.17844", "authors": ["Maham Nazir", "Muhammad Aqeel", "Francesco Setti"], "title": "Diffusion-Based Data Augmentation for Medical Image Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to CVAMD Workshop at ICCV 2025", "summary": "Medical image segmentation models struggle with rare abnormalities due to scarce annotated pathological data. We propose DiffAug a novel framework that combines textguided diffusion-based generation with automatic segmentation validation to address this challenge. Our proposed approach uses latent diffusion models conditioned on medical text descriptions and spatial masks to synthesize abnormalities via inpainting on normal images. Generated samples undergo dynamic quality validation through a latentspace segmentation network that ensures accurate localization while enabling single-step inference. The text prompts, derived from medical literature, guide the generation of diverse abnormality types without requiring manual annotation. Our validation mechanism filters synthetic samples based on spatial accuracy, maintaining quality while operating efficiently through direct latent estimation. Evaluated on three medical imaging benchmarks (CVC-ClinicDB, Kvasir-SEG, REFUGE2), our framework achieves state-of-the-art performance with 8-10% Dice improvements over baselines and reduces false negative rates by up to 28% for challenging cases like small polyps and flat lesions critical for early detection in screening applications.", "AI": {"tldr": "DiffAug\u662f\u4e00\u4e2a\u7ed3\u5408\u6587\u672c\u5f15\u5bfc\u6269\u6563\u751f\u6210\u548c\u81ea\u52a8\u5206\u5272\u9a8c\u8bc1\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u4e2d\u7f55\u89c1\u5f02\u5e38\u5206\u5272\u95ee\u9898\uff0c\u901a\u8fc7\u6761\u4ef6\u751f\u6210\u548c\u52a8\u6001\u9a8c\u8bc1\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u5728\u5904\u7406\u7f55\u89c1\u5f02\u5e38\u65f6\u9762\u4e34\u6807\u6ce8\u75c5\u7406\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u5f02\u5e38\u6837\u672c\u5e76\u786e\u4fdd\u51c6\u786e\u6027\u7684\u65b9\u6cd5", "method": "\u4f7f\u7528\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u6587\u672c\u5f15\u5bfc\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u533b\u5b66\u6587\u672c\u63cf\u8ff0\u548c\u7a7a\u95f4\u63a9\u7801\u6761\u4ef6\u5728\u6b63\u5e38\u56fe\u50cf\u4e0a\u8fdb\u884c\u4fee\u590d\u751f\u6210\u5f02\u5e38\u6837\u672c\uff0c\u5e76\u91c7\u7528\u6f5c\u5728\u7a7a\u95f4\u5206\u5272\u7f51\u7edc\u8fdb\u884c\u52a8\u6001\u8d28\u91cf\u9a8c\u8bc1", "result": "\u5728\u4e09\u4e2a\u533b\u5b66\u6210\u50cf\u57fa\u51c6\u6d4b\u8bd5(CVC-ClinicDB\u3001Kvasir-SEG\u3001REFUGE2)\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0cDice\u7cfb\u6570\u6bd4\u57fa\u7ebf\u63d0\u9ad88-10%\uff0c\u5047\u9634\u6027\u7387\u964d\u4f4e\u9ad8\u8fbe28%", "conclusion": "DiffAug\u6846\u67b6\u901a\u8fc7\u6587\u672c\u5f15\u5bfc\u7684\u6269\u6563\u751f\u6210\u548c\u81ea\u52a8\u9a8c\u8bc1\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u4e2d\u7f55\u89c1\u5f02\u5e38\u5206\u5272\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5728\u65e9\u671f\u7b5b\u67e5\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c"}}
{"id": "2508.17876", "pdf": "https://arxiv.org/pdf/2508.17876", "abs": "https://arxiv.org/abs/2508.17876", "authors": ["Lulu Hao", "Lipu Zhou", "Zhenzhong Wei", "Xu Wang"], "title": "Camera Pose Refinement via 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Camera pose refinement aims at improving the accuracy of initial pose estimation for applications in 3D computer vision. Most refinement approaches rely on 2D-3D correspondences with specific descriptors or dedicated networks, requiring reconstructing the scene again for a different descriptor or fully retraining the network for each scene. Some recent methods instead infer pose from feature similarity, but their lack of geometry constraints results in less accuracy. To overcome these limitations, we propose a novel camera pose refinement framework leveraging 3D Gaussian Splatting (3DGS), referred to as GS-SMC. Given the widespread usage of 3DGS, our method can employ an existing 3DGS model to render novel views, providing a lightweight solution that can be directly applied to diverse scenes without additional training or fine-tuning. Specifically, we introduce an iterative optimization approach, which refines the camera pose using epipolar geometric constraints among the query and multiple rendered images. Our method allows flexibly choosing feature extractors and matchers to establish these constraints. Extensive empirical evaluations on the 7-Scenes and the Cambridge Landmarks datasets demonstrate that our method outperforms state-of-the-art camera pose refinement approaches, achieving 53.3% and 56.9% reductions in median translation and rotation errors on 7-Scenes, and 40.7% and 53.2% on Cambridge.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u76f8\u673a\u4f4d\u59ff\u4f18\u5316\u6846\u67b6GS-SMC\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u4e0d\u540c\u573a\u666f\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u6e32\u67d3\u548c\u6781\u51e0\u4f55\u7ea6\u675f\u8fed\u4ee3\u4f18\u5316\u4f4d\u59ff\u7cbe\u5ea6", "motivation": "\u73b0\u6709\u76f8\u673a\u4f4d\u59ff\u4f18\u5316\u65b9\u6cd5\u9700\u8981\u91cd\u65b0\u6784\u5efa\u573a\u666f\u6216\u91cd\u65b0\u8bad\u7ec3\u7f51\u7edc\uff0c\u7f3a\u4e4f\u51e0\u4f55\u7ea6\u675f\u7684\u65b9\u6cd5\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u5229\u7528\u73b0\u67093DGS\u6a21\u578b\u6e32\u67d3\u65b0\u89c6\u89d2\uff0c\u901a\u8fc7\u67e5\u8be2\u56fe\u50cf\u4e0e\u591a\u4e2a\u6e32\u67d3\u56fe\u50cf\u95f4\u7684\u6781\u51e0\u4f55\u7ea6\u675f\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\uff0c\u53ef\u7075\u6d3b\u9009\u62e9\u7279\u5f81\u63d0\u53d6\u5668\u548c\u5339\u914d\u5668", "result": "\u57287-Scenes\u548cCambridge\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u79fb\u8bef\u5dee\u4e2d\u4f4d\u6570\u5206\u522b\u964d\u4f4e53.3%\u548c40.7%\uff0c\u65cb\u8f6c\u8bef\u5dee\u964d\u4f4e56.9%\u548c53.2%", "conclusion": "GS-SMC\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u9ad8\u7cbe\u5ea6\u76f8\u673a\u4f4d\u59ff\u4f18\u5316\u65b9\u6848\uff0c\u5145\u5206\u5229\u75283DGS\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u51e0\u4f55\u7ea6\u675f\u7684\u4f18\u52bf"}}
{"id": "2508.17885", "pdf": "https://arxiv.org/pdf/2508.17885", "abs": "https://arxiv.org/abs/2508.17885", "authors": ["Raul Balmez", "Alexandru Brateanu", "Ciprian Orhei", "Codruta Ancuti", "Cosmin Ancuti"], "title": "ISALux: Illumination and Segmentation Aware Transformer Employing Mixture of Experts for Low Light Image Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "We introduce ISALux, a novel transformer-based approach for Low-Light Image Enhancement (LLIE) that seamlessly integrates illumination and semantic priors. Our architecture includes an original self-attention block, Hybrid Illumination and Semantics-Aware Multi-Headed Self- Attention (HISA-MSA), which integrates illumination and semantic segmentation maps for en- hanced feature extraction. ISALux employs two self-attention modules to independently process illumination and semantic features, selectively enriching each other to regulate luminance and high- light structural variations in real-world scenarios. A Mixture of Experts (MoE)-based Feed-Forward Network (FFN) enhances contextual learning, with a gating mechanism conditionally activating the top K experts for specialized processing. To address overfitting in LLIE methods caused by distinct light patterns in benchmarking datasets, we enhance the HISA-MSA module with low-rank matrix adaptations (LoRA). Extensive qualitative and quantitative evaluations across multiple specialized datasets demonstrate that ISALux is competitive with state-of-the-art (SOTA) methods. Addition- ally, an ablation study highlights the contribution of each component in the proposed model. Code will be released upon publication.", "AI": {"tldr": "ISALux\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u5149\u7167\u548c\u8bed\u4e49\u5148\u9a8c\uff0c\u91c7\u7528\u6df7\u5408\u5149\u7167\u8bed\u4e49\u611f\u77e5\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548cMoE\u524d\u9988\u7f51\u7edc\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u56e0\u57fa\u51c6\u6570\u636e\u96c6\u5149\u7167\u6a21\u5f0f\u5dee\u5f02\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u540c\u65f6\u66f4\u597d\u5730\u6574\u5408\u5149\u7167\u548c\u8bed\u4e49\u4fe1\u606f\u6765\u63d0\u5347\u56fe\u50cf\u589e\u5f3a\u6548\u679c\u3002", "method": "\u63d0\u51faHISA-MSA\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u5206\u522b\u5904\u7406\u5149\u7167\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u91c7\u7528MoE\u524d\u9988\u7f51\u7edc\u8fdb\u884c\u6761\u4ef6\u4e13\u5bb6\u6fc0\u6d3b\uff0c\u5e76\u5f15\u5165LoRA\u4f4e\u79e9\u77e9\u9635\u9002\u5e94\u6765\u9632\u6b62\u8fc7\u62df\u5408\u3002", "result": "\u5728\u591a\u4e2a\u4e13\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u8868\u660e\uff0cISALux\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u7684\u91cd\u8981\u6027\u3002", "conclusion": "ISALux\u901a\u8fc7\u6709\u6548\u6574\u5408\u5149\u7167\u548c\u8bed\u4e49\u5148\u9a8c\uff0c\u7ed3\u5408\u521b\u65b0\u7684\u6ce8\u610f\u529b\u673a\u5236\u548cMoE\u67b6\u6784\uff0c\u4e3a\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5c06\u5728\u53d1\u8868\u540e\u5f00\u6e90\u3002"}}
{"id": "2508.17894", "pdf": "https://arxiv.org/pdf/2508.17894", "abs": "https://arxiv.org/abs/2508.17894", "authors": ["Iason Ioannis Panagos", "Giorgos Sfikas", "Christophoros Nikou"], "title": "Designing Practical Models for Isolated Word Visual Speech Recognition", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Double-column format, 13 pages with references, 2 figures", "summary": "Visual speech recognition (VSR) systems decode spoken words from an input sequence using only the video data. Practical applications of such systems include medical assistance as well as human-machine interactions. A VSR system is typically employed in a complementary role in cases where the audio is corrupt or not available. In order to accurately predict the spoken words, these architectures often rely on deep neural networks in order to extract meaningful representations from the input sequence. While deep architectures achieve impressive recognition performance, relying on such models incurs significant computation costs which translates into increased resource demands in terms of hardware requirements and results in limited applicability in real-world scenarios where resources might be constrained. This factor prevents wider adoption and deployment of speech recognition systems in more practical applications. In this work, we aim to alleviate this issue by developing architectures for VSR that have low hardware costs. Following the standard two-network design paradigm, where one network handles visual feature extraction and another one utilizes the extracted features to classify the entire sequence, we develop lightweight end-to-end architectures by first benchmarking efficient models from the image classification literature, and then adopting lightweight block designs in a temporal convolution network backbone. We create several unified models with low resource requirements but strong recognition performance. Experiments on the largest public database for English words demonstrate the effectiveness and practicality of our developed models. Code and trained models will be made publicly available.", "AI": {"tldr": "\u5f00\u53d1\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u97f3\u8bc6\u522b\u67b6\u6784\uff0c\u964d\u4f4e\u786c\u4ef6\u6210\u672c\u540c\u65f6\u4fdd\u6301\u826f\u597d\u6027\u80fd", "motivation": "\u73b0\u6709VSR\u7cfb\u7edf\u4f9d\u8d56\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u786c\u4ef6\u9700\u6c42\u5927\uff0c\u9650\u5236\u4e86\u5728\u5b9e\u9645\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e2d\u7684\u5e94\u7528\u548c\u90e8\u7f72", "method": "\u91c7\u7528\u6807\u51c6\u53cc\u7f51\u7edc\u8bbe\u8ba1\u8303\u5f0f\uff0c\u9996\u5148\u5bf9\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u9ad8\u6548\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7136\u540e\u5728\u65f6\u5e8f\u5377\u79ef\u7f51\u7edc\u9aa8\u5e72\u4e2d\u91c7\u7528\u8f7b\u91cf\u7ea7\u5757\u8bbe\u8ba1\uff0c\u521b\u5efa\u591a\u4e2a\u8d44\u6e90\u9700\u6c42\u4f4e\u4f46\u8bc6\u522b\u6027\u80fd\u5f3a\u7684\u7edf\u4e00\u6a21\u578b", "result": "\u5728\u6700\u5927\u7684\u82f1\u8bed\u5355\u8bcd\u516c\u5171\u6570\u636e\u5e93\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u5f00\u53d1\u6a21\u578b\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027", "conclusion": "\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u67b6\u6784\u80fd\u591f\u6709\u6548\u964d\u4f4eVSR\u7cfb\u7edf\u7684\u786c\u4ef6\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u8bc6\u522b\u6027\u80fd\uff0c\u63d0\u9ad8\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027"}}
{"id": "2508.18032", "pdf": "https://arxiv.org/pdf/2508.18032", "abs": "https://arxiv.org/abs/2508.18032", "authors": ["Yaqi Li", "Peng Chen", "Mingyang Han", "Bu Pi", "Haoxiang Shi", "Runzhou Zhao", "Yang Yao", "Xuan Zhang", "Jun Song"], "title": "Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Despite the promising progress of recent autoregressive models in text-to-image (T2I) generation, their ability to handle multi-attribute and ambiguous prompts remains limited. To address these limitations, existing works have applied chain-of-thought (CoT) to enable stage-aware visual synthesis and employed reinforcement learning (RL) to improve reasoning capabilities. However, most models provide reward signals only at the end of the generation stage. This monolithic final-only guidance makes it difficult to identify which stages contribute positively to the final outcome and may lead to suboptimal policies. To tackle this issue, we propose a Visual-Chain of Guidance (Visual-CoG) paradigm consisting of three stages: semantic reasoning, process refining, and outcome evaluation, with stage-aware rewards providing immediate guidance throughout the image generation pipeline. We further construct a visual cognition benchmark, VisCog-Bench, which comprises four subtasks to evaluate the effectiveness of semantic reasoning. Comprehensive evaluations on GenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%, 5%, and 19%, respectively, demonstrating the superior performance of the proposed Visual-CoG. We will release all the resources soon.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVisual-CoG\u65b9\u6cd5\uff0c\u901a\u8fc7\u9636\u6bb5\u6027\u5956\u52b1\u6307\u5bfc\u6539\u5584\u81ea\u56de\u5f52\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u591a\u5c5e\u6027\u548c\u6a21\u7cca\u63d0\u793a\u5904\u7406\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u81ea\u56de\u5f52T2I\u751f\u6210\u6a21\u578b\u5728\u5904\u7406\u591a\u5c5e\u6027\u548c\u6a21\u7cca\u63d0\u793a\u65f6\u80fd\u529b\u6709\u9650\uff0c\u800c\u4f20\u7edf\u7684\u6700\u7ec8\u5956\u52b1\u6307\u5bfc\u65b9\u5f0f\u5bfc\u81f4\u5b50\u6b65\u9aa4\u8d21\u732e\u96be\u4ee5\u8bc6\u522b\u548c\u7b56\u7565\u4f18\u5316\u56f0\u96be\u3002", "method": "\u63d0\u51faVisual-CoG\u8303\u5f0f\uff0c\u5305\u542b\u8bed\u4e49\u63a8\u7406\u3001\u8fc7\u7a0b\u7cbe\u70bc\u548c\u7ed3\u679c\u8bc4\u4f30\u4e09\u4e2a\u9636\u6bb5\uff0c\u901a\u8fc7\u9636\u6bb5\u6027\u5956\u52b1\u63d0\u4f9b\u5168\u7a0b\u7acb\u5373\u6307\u5bfc\uff0c\u5e76\u6784\u5efaVisCog-Bench\u89c6\u89c9\u8ba4\u522b\u6d4b\u8bd5\u96c6\u3002", "result": "\u5728GenEval\u3001T2I-CompBench\u548cVisCog-Bench\u4e09\u4e2a\u6d4b\u8bd5\u96c6\u4e0a\u5206\u522b\u83b7\u5f9715%\u30015%\u548c19%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u663e\u793a\u4e86\u65b9\u6cd5\u7684\u4f18\u52bf\u6027\u80fd\u3002", "conclusion": "Visual-CoG\u901a\u8fc7\u9636\u6bb5\u6027\u5956\u52b1\u6307\u5bfc\u6709\u6548\u63d0\u5347\u4e86T2I\u751f\u6210\u6a21\u578b\u7684\u8bed\u4e49\u7406\u89e3\u548c\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u5904\u7406\u590d\u6742\u63d0\u793a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.18071", "pdf": "https://arxiv.org/pdf/2508.18071", "abs": "https://arxiv.org/abs/2508.18071", "authors": ["Zhenyang Li", "Xiaoyang Bai", "Jinfan Lu", "Pengfei Shen", "Edmund Y. Lam", "Yifan Peng"], "title": "EventTracer: Fast Path Tracing-based Event Stream Rendering", "categories": ["cs.CV"], "comment": null, "summary": "Simulating event streams from 3D scenes has become a common practice in event-based vision research, as it meets the demand for large-scale, high temporal frequency data without setting up expensive hardware devices or undertaking extensive data collections. Yet existing methods in this direction typically work with noiseless RGB frames that are costly to render, and therefore they can only achieve a temporal resolution equivalent to 100-300 FPS, far lower than that of real-world event data. In this work, we propose EventTracer, a path tracing-based rendering pipeline that simulates high-fidelity event sequences from complex 3D scenes in an efficient and physics-aware manner. Specifically, we speed up the rendering process via low sample-per-pixel (SPP) path tracing, and train a lightweight event spiking network to denoise the resulting RGB videos into realistic event sequences. To capture the physical properties of event streams, the network is equipped with a bipolar leaky integrate-and-fired (BiLIF) spiking unit and trained with a bidirectional earth mover distance (EMD) loss. Our EventTracer pipeline runs at a speed of about 4 minutes per second of 720p video, and it inherits the merit of accurate spatiotemporal modeling from its path tracing backbone. We show in two downstream tasks that EventTracer captures better scene details and demonstrates a greater similarity to real-world event data than other event simulators, which establishes it as a promising tool for creating large-scale event-RGB datasets at a low cost, narrowing the sim-to-real gap in event-based vision, and boosting various application scenarios such as robotics, autonomous driving, and VRAR.", "AI": {"tldr": "EventTracer\u662f\u4e00\u4e2a\u57fa\u4e8e\u8def\u5f84\u8ffd\u8e2a\u7684\u9ad8\u6548\u4e8b\u4ef6\u6d41\u6a21\u62df\u7ba1\u9053\uff0c\u901a\u8fc7\u4f4eSPP\u8def\u5f84\u8ffd\u8e2a\u52a0\u901f\u6e32\u67d3\uff0c\u5e76\u4f7f\u7528\u8f7b\u91cf\u7ea7\u8109\u51b2\u7f51\u7edc\u53bb\u566a\u751f\u6210\u903c\u771f\u7684\u4e8b\u4ef6\u5e8f\u5217\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u62df\u901f\u5ea6\u548c\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u4e8b\u4ef6\u6d41\u6a21\u62df\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u65e0\u566a\u58f0RGB\u5e27\u6e32\u67d3\uff0c\u65f6\u95f4\u5206\u8fa8\u7387\u4ec5100-300FPS\uff0c\u8fdc\u4f4e\u4e8e\u771f\u5b9e\u4e8b\u4ef6\u6570\u636e\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5927\u89c4\u6a21\u9ad8\u9891\u7387\u6570\u636e\u9700\u6c42\u3002", "method": "\u91c7\u7528\u4f4e\u6837\u672c\u6570\u8def\u5f84\u8ffd\u8e2a\u52a0\u901f\u6e32\u67d3\uff0c\u8bad\u7ec3\u5e26\u6709\u53cc\u6781\u6027\u6cc4\u6f0f\u79ef\u5206\u53d1\u653e(BiLIF)\u5355\u5143\u7684\u8f7b\u91cf\u8109\u51b2\u7f51\u7edc\uff0c\u4f7f\u7528\u53cc\u5411\u5730\u7403\u79fb\u52a8\u8ddd\u79bb(EMD)\u635f\u5931\u8fdb\u884c\u53bb\u566a\u548c\u7269\u7406\u7279\u6027\u5efa\u6a21\u3002", "result": "EventTracer\u4ee5\u7ea64\u5206\u949f/\u79d2720p\u89c6\u9891\u7684\u901f\u5ea6\u8fd0\u884c\uff0c\u5728\u7ec6\u8282\u6355\u6349\u548c\u771f\u5b9e\u4e8b\u4ef6\u6570\u636e\u76f8\u4f3c\u6027\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u6a21\u62df\u5668\uff0c\u51c6\u786e\u7ee7\u627f\u4e86\u8def\u5f84\u8ffd\u8e2a\u7684\u65f6\u7a7a\u5efa\u6a21\u4f18\u52bf\u3002", "conclusion": "\u8be5\u5de5\u5177\u4e3a\u4f4e\u6210\u672c\u521b\u5efa\u5927\u89c4\u6a21\u4e8b\u4ef6-RGB\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7f29\u5c0f\u4e86\u4e8b\u4ef6\u89c6\u89c9\u7684\u6a21\u62df-\u73b0\u5b9e\u5dee\u8ddd\uff0c\u53ef\u4fc3\u8fdb\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u548cVR/AR\u7b49\u5e94\u7528\u53d1\u5c55\u3002"}}
{"id": "2508.18213", "pdf": "https://arxiv.org/pdf/2508.18213", "abs": "https://arxiv.org/abs/2508.18213", "authors": ["Ayce Idil Aytekin", "Helge Rhodin", "Rishabh Dabral", "Christian Theobalt"], "title": "Follow My Hold: Hand-Object Interaction Reconstruction through Geometric Guidance", "categories": ["cs.CV"], "comment": "Project page: https://aidilayce.github.io/FollowMyHold-page/", "summary": "We propose a novel diffusion-based framework for reconstructing 3D geometry of hand-held objects from monocular RGB images by leveraging hand-object interaction as geometric guidance. Our method conditions a latent diffusion model on an inpainted object appearance and uses inference-time guidance to optimize the object reconstruction, while simultaneously ensuring plausible hand-object interactions. Unlike prior methods that rely on extensive post-processing or produce low-quality reconstructions, our approach directly generates high-quality object geometry during the diffusion process by introducing guidance with an optimization-in-the-loop design. Specifically, we guide the diffusion model by applying supervision to the velocity field while simultaneously optimizing the transformations of both the hand and the object being reconstructed. This optimization is driven by multi-modal geometric cues, including normal and depth alignment, silhouette consistency, and 2D keypoint reprojection. We further incorporate signed distance field supervision and enforce contact and non-intersection constraints to ensure physical plausibility of hand-object interaction. Our method yields accurate, robust and coherent reconstructions under occlusion while generalizing well to in-the-wild scenarios.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5355\u76eeRGB\u56fe\u50cf\u624b\u6301\u7269\u4f533D\u91cd\u5efa\u6846\u67b6\uff0c\u5229\u7528\u624b-\u7269\u4ea4\u4e92\u4f5c\u4e3a\u51e0\u4f55\u6307\u5bfc\uff0c\u901a\u8fc7\u63a8\u7406\u65f6\u6307\u5bfc\u548c\u4f18\u5316\u5faa\u73af\u8bbe\u8ba1\u76f4\u63a5\u751f\u6210\u9ad8\u8d28\u91cf\u51e0\u4f55", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u540e\u5904\u7406\u6216\u4ea7\u751f\u4f4e\u8d28\u91cf\u91cd\u5efa\uff0c\u9700\u8981\u5229\u7528\u624b-\u7269\u4ea4\u4e92\u7684\u51e0\u4f55\u7ea6\u675f\u6765\u76f4\u63a5\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u7269\u7406\u5408\u7406\u76843D\u7269\u4f53\u91cd\u5efa", "method": "\u4f7f\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u4fee\u590d\u540e\u7684\u7269\u4f53\u5916\u89c2\u4e3a\u6761\u4ef6\uff0c\u901a\u8fc7\u901f\u5ea6\u573a\u76d1\u7763\u548c\u591a\u6a21\u6001\u51e0\u4f55\u7ebf\u7d22\uff08\u6cd5\u5411\u6df1\u5ea6\u5bf9\u9f50\u3001\u8f6e\u5ed3\u4e00\u81f4\u6027\u3001\u5173\u952e\u70b9\u91cd\u6295\u5f71\uff09\u8fdb\u884c\u63a8\u7406\u65f6\u6307\u5bfc\uff0c\u540c\u65f6\u4f18\u5316\u624b\u548c\u7269\u4f53\u7684\u53d8\u6362\uff0c\u5e76\u52a0\u5165SDF\u76d1\u7763\u548c\u63a5\u89e6\u7ea6\u675f", "result": "\u65b9\u6cd5\u80fd\u591f\u4ea7\u751f\u51c6\u786e\u3001\u9c81\u68d2\u4e14\u8fde\u8d2f\u7684\u91cd\u5efa\u7ed3\u679c\uff0c\u5728\u906e\u6321\u60c5\u51b5\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u5e76\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u771f\u5b9e\u573a\u666f", "conclusion": "\u8be5\u6269\u6563\u6846\u67b6\u901a\u8fc7\u624b-\u7269\u4ea4\u4e92\u51e0\u4f55\u6307\u5bfc\u548c\u4f18\u5316\u5faa\u73af\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5355\u76ee3D\u7269\u4f53\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027"}}
{"id": "2508.18242", "pdf": "https://arxiv.org/pdf/2508.18242", "abs": "https://arxiv.org/abs/2508.18242", "authors": ["Fadi Khatib", "Dror Moran", "Guy Trostianetsky", "Yoni Kasten", "Meirav Galun", "Ronen Basri"], "title": "GSVisLoc: Generalizable Visual Localization for Gaussian Splatting Scene Representations", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025 Workshops (CALIPOSE). Project page:   https://gsvisloc.github.io/", "summary": "We introduce GSVisLoc, a visual localization method designed for 3D Gaussian Splatting (3DGS) scene representations. Given a 3DGS model of a scene and a query image, our goal is to estimate the camera's position and orientation. We accomplish this by robustly matching scene features to image features. Scene features are produced by downsampling and encoding the 3D Gaussians while image features are obtained by encoding image patches. Our algorithm proceeds in three steps, starting with coarse matching, then fine matching, and finally by applying pose refinement for an accurate final estimate. Importantly, our method leverages the explicit 3DGS scene representation for visual localization without requiring modifications, retraining, or additional reference images. We evaluate GSVisLoc on both indoor and outdoor scenes, demonstrating competitive localization performance on standard benchmarks while outperforming existing 3DGS-based baselines. Moreover, our approach generalizes effectively to novel scenes without additional training.", "AI": {"tldr": "GSVisLoc\u662f\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85(3DGS)\u573a\u666f\u8868\u793a\u7684\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u7c97\u5339\u914d\u3001\u7cbe\u5339\u914d\u548c\u4f4d\u59ff\u7ec6\u5316\u4e09\u4e2a\u6b65\u9aa4\uff0c\u5b9e\u73b0\u65e0\u9700\u4fee\u6539\u3001\u91cd\u8bad\u7ec3\u6216\u989d\u5916\u53c2\u8003\u56fe\u50cf\u7684\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u590d\u6742\u7684\u573a\u666f\u8868\u793a\u6216\u5927\u91cf\u53c2\u8003\u56fe\u50cf\uff0c\u800c3DGS\u63d0\u4f9b\u4e86\u4e00\u79cd\u663e\u5f0f\u7684\u573a\u666f\u8868\u793a\u65b9\u5f0f\uff0c\u4f46\u5982\u4f55\u76f4\u63a5\u5229\u75283DGS\u8fdb\u884c\u89c6\u89c9\u5b9a\u4f4d\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u65b9\u6cd5\u5206\u4e3a\u4e09\u6b65\uff1a1) \u901a\u8fc7\u4e0b\u91c7\u6837\u548c\u7f16\u78013D\u9ad8\u65af\u751f\u6210\u573a\u666f\u7279\u5f81\uff1b2) \u7f16\u7801\u56fe\u50cf\u5757\u83b7\u53d6\u56fe\u50cf\u7279\u5f81\uff1b3) \u8fdb\u884c\u7c97\u5339\u914d\u3001\u7cbe\u5339\u914d\uff0c\u6700\u540e\u901a\u8fc7\u4f4d\u59ff\u7ec6\u5316\u83b7\u5f97\u7cbe\u786e\u7684\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u3002", "result": "\u5728\u5ba4\u5185\u5916\u573a\u666f\u7684\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u7684\u5b9a\u4f4d\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u76843DGS\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u80fd\u591f\u6709\u6548\u6cdb\u5316\u5230\u65b0\u573a\u666f\u800c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "conclusion": "GSVisLoc\u6210\u529f\u8bc1\u660e\u4e86\u76f4\u63a5\u5229\u75283DGS\u663e\u5f0f\u573a\u666f\u8868\u793a\u8fdb\u884c\u89c6\u89c9\u5b9a\u4f4d\u7684\u53ef\u884c\u6027\uff0c\u4e3a3DGS\u5728\u89c6\u89c9\u5b9a\u4f4d\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.18271", "pdf": "https://arxiv.org/pdf/2508.18271", "abs": "https://arxiv.org/abs/2508.18271", "authors": ["Haitang Feng", "Jie Liu", "Jie Tang", "Gangshan Wu", "Beiqi Chen", "Jianhuang Lai", "Guangcong Wang"], "title": "ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion Models", "categories": ["cs.CV"], "comment": "Project page: https://objfiller3d.github.io/ Code:   https://github.com/objfiller3d/ObjFiller-3D", "summary": "3D inpainting often relies on multi-view 2D image inpainting, where the inherent inconsistencies across different inpainted views can result in blurred textures, spatial discontinuities, and distracting visual artifacts. These inconsistencies pose significant challenges when striving for accurate and realistic 3D object completion, particularly in applications that demand high fidelity and structural coherence. To overcome these limitations, we propose ObjFiller-3D, a novel method designed for the completion and editing of high-quality and consistent 3D objects. Instead of employing a conventional 2D image inpainting model, our approach leverages a curated selection of state-of-the-art video editing model to fill in the masked regions of 3D objects. We analyze the representation gap between 3D and videos, and propose an adaptation of a video inpainting model for 3D scene inpainting. In addition, we introduce a reference-based 3D inpainting method to further enhance the quality of reconstruction. Experiments across diverse datasets show that compared to previous methods, ObjFiller-3D produces more faithful and fine-grained reconstructions (PSNR of 26.6 vs. NeRFiller (15.9) and LPIPS of 0.19 vs. Instant3dit (0.25)). Moreover, it demonstrates strong potential for practical deployment in real-world 3D editing applications. Project page: https://objfiller3d.github.io/ Code: https://github.com/objfiller3d/ObjFiller-3D .", "AI": {"tldr": "ObjFiller-3D\u662f\u4e00\u79cd\u65b0\u9896\u76843D\u4fee\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u89c6\u9891\u7f16\u8f91\u6a21\u578b\u800c\u975e\u4f20\u7edf2D\u56fe\u50cf\u4fee\u590d\u6765\u89e3\u51b3\u591a\u89c6\u89d2\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5728PSNR\u548cLPIPS\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u76843D\u4fee\u590d\u65b9\u6cd5\u4f9d\u8d56\u591a\u89c6\u89d22D\u56fe\u50cf\u4fee\u590d\uff0c\u5bfc\u81f4\u4e0d\u540c\u89c6\u89d2\u95f4\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\uff0c\u4ea7\u751f\u6a21\u7cca\u7eb9\u7406\u3001\u7a7a\u95f4\u4e0d\u8fde\u7eed\u6027\u548c\u89c6\u89c9\u4f2a\u5f71\uff0c\u5f71\u54cd3D\u5bf9\u8c61\u5b8c\u6210\u7684\u51c6\u786e\u6027\u548c\u771f\u5b9e\u611f\u3002", "method": "\u63d0\u51faObjFiller-3D\u65b9\u6cd5\uff0c\u4f7f\u7528\u7cbe\u9009\u7684\u89c6\u9891\u7f16\u8f91\u6a21\u578b\u6765\u586b\u51453D\u5bf9\u8c61\u7684\u63a9\u7801\u533a\u57df\uff0c\u5206\u67903D\u4e0e\u89c6\u9891\u4e4b\u95f4\u7684\u8868\u793a\u5dee\u8ddd\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u53c2\u8003\u76843D\u4fee\u590d\u65b9\u6cd5\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cObjFiller-3D\u76f8\u6bd4\u4e4b\u524d\u65b9\u6cd5\u4ea7\u751f\u66f4\u5fe0\u5b9e\u548c\u7cbe\u7ec6\u7684\u91cd\u5efa\u7ed3\u679c\uff08PSNR 26.6 vs NeRFiller 15.9\uff0cLPIPS 0.19 vs Instant3dit 0.25\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c3D\u7f16\u8f91\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\uff0c\u4e3a\u89e3\u51b33D\u4fee\u590d\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.16612", "pdf": "https://arxiv.org/pdf/2508.16612", "abs": "https://arxiv.org/abs/2508.16612", "authors": ["Aven-Le Zhou"], "title": "Negative Shanshui: Real-time Interactive Ink Painting Synthesis", "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.CY"], "comment": null, "summary": "This paper presents Negative Shanshui, a real-time interactive AI synthesis approach that reinterprets classical Chinese landscape ink painting, i.e., shanshui, to engage with ecological crises in the Anthropocene. Negative Shanshui optimizes a fine-tuned Stable Diffusion model for real-time inferences and integrates it with gaze-driven inpainting, frame interpolation; it enables dynamic morphing animations in response to the viewer's gaze and presents as an interactive virtual reality (VR) experience. The paper describes the complete technical pipeline, covering the system framework, optimization strategies, gaze-based interaction, and multimodal deployment in an art festival. Further analysis of audience feedback collected during its public exhibition highlights how participants variously engaged with the work through empathy, ambivalence, and critical reflection.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86Negative Shanshui\uff0c\u4e00\u79cd\u57fa\u4e8e\u7a33\u5b9a\u6563\u5e03\u6a21\u578b\u7684\u5b9e\u65f6\u4ea4\u4e92\u5f0fAI\u751f\u6210\u6280\u672f\uff0c\u901a\u8fc7\u91cd\u89e3\u4e2d\u56fd\u5c71\u6c34\u58a8\u7b14\u753b\u6765\u5e94\u5bf9\u751f\u6001\u5371\u673a\u3002\u7cfb\u7edf\u6574\u5408\u4e86\u89c6\u7ebf\u9a71\u52a8\u7684\u56fe\u50cf\u4fee\u590d\u548c\u5e27\u5185\u63d2\u503c\u6280\u672f\uff0c\u4ee5VR\u4f53\u9a8c\u5f62\u5f0f\u5c55\u73b0\u52a8\u6001\u53d8\u5f62\u52a8\u753b\u3002", "motivation": "\u91cd\u65b0\u89e3\u8bfb\u4f20\u7edf\u4e2d\u56fd\u5c71\u6c34\u58a8\u7b14\u753b\uff0c\u4ee5\u827a\u672f\u5f62\u5f0f\u56de\u5e94\u4eba\u7c7b\u4e16\u7eaa\u7684\u751f\u6001\u5371\u673a\u95ee\u9898\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u6280\u672f\u4f7f\u89c2\u4f17\u80fd\u591f\u6df1\u5ea6\u53c2\u4e0e\u548c\u53cd\u601d\u3002", "method": "\u4f7f\u7528\u7ec6\u8c03\u7684Stable Diffusion\u6a21\u578b\u8fdb\u884c\u5b9e\u65f6\u63a8\u7406\uff0c\u7ed3\u5408\u89c6\u7ebf\u9a71\u52a8\u7684\u56fe\u50cf\u4fee\u590d(inpainting)\u548c\u5e27\u5185\u63d2\u503c\u6280\u672f\uff0c\u6784\u5efa\u4ea4\u4e92\u5f0fVR\u7cfb\u7edf\u3002\u5305\u62ec\u5b8c\u6574\u7684\u6280\u672f\u6d41\u6c34\u7ebf\u3001\u4f18\u5316\u7b56\u7565\u548c\u591a\u6a21\u6001\u90e8\u7f72\u3002", "result": "\u5b9e\u73b0\u4e86\u6839\u636e\u89c2\u4f17\u89c6\u7ebf\u8fdb\u884c\u52a8\u6001\u53d8\u5f62\u7684\u52a8\u753b\u6548\u679c\uff0c\u6210\u529f\u5728\u827a\u672f\u8282\u4e2d\u5c55\u51fa\u3002\u89c2\u4f17\u53cd\u9988\u663e\u793a\u4e86\u901a\u8fc7\u5171\u60c5\u3001\u77db\u76fe\u548c\u6279\u5224\u6027\u53cd\u601d\u7b49\u4e0d\u540c\u65b9\u5f0f\u4e0e\u4f5c\u54c1\u4ea4\u4e92\u3002", "conclusion": "Negative Shanshui\u6210\u529f\u5c06\u4f20\u7edf\u4e2d\u56fd\u753b\u4e0e\u5148\u8fdbAI\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u521b\u9020\u4e86\u4e00\u79cd\u80fd\u591f\u5f15\u53d1\u89c2\u4f17\u5bf9\u751f\u6001\u95ee\u9898\u6df1\u5ea6\u601d\u8003\u7684\u4ea4\u4e92\u5f0f\u827a\u672f\u4f53\u9a8c\uff0c\u4e3a\u827a\u672f\u4e0e\u6280\u672f\u878d\u5408\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.16897", "pdf": "https://arxiv.org/pdf/2508.16897", "abs": "https://arxiv.org/abs/2508.16897", "authors": ["Pouya Shiri", "Xin Yi", "Neel P. Mistry", "Samaneh Javadinia", "Mohammad Chegini", "Seok-Bum Ko", "Amirali Baniasadi", "Scott J. Adams"], "title": "Generating Synthetic Contrast-Enhanced Chest CT Images from Non-Contrast Scans Using Slice-Consistent Brownian Bridge Diffusion Network", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "comment": null, "summary": "Contrast-enhanced computed tomography (CT) imaging is essential for diagnosing and monitoring thoracic diseases, including aortic pathologies. However, contrast agents pose risks such as nephrotoxicity and allergic-like reactions. The ability to generate high-fidelity synthetic contrast-enhanced CT angiography (CTA) images without contrast administration would be transformative, enhancing patient safety and accessibility while reducing healthcare costs. In this study, we propose the first bridge diffusion-based solution for synthesizing contrast-enhanced CTA images from non-contrast CT scans. Our approach builds on the Slice-Consistent Brownian Bridge Diffusion Model (SC-BBDM), leveraging its ability to model complex mappings while maintaining consistency across slices. Unlike conventional slice-wise synthesis methods, our framework preserves full 3D anatomical integrity while operating in a high-resolution 2D fashion, allowing seamless volumetric interpretation under a low memory budget. To ensure robust spatial alignment, we implement a comprehensive preprocessing pipeline that includes resampling, registration using the Symmetric Normalization method, and a sophisticated dilated segmentation mask to extract the aorta and surrounding structures. We create two datasets from the Coltea-Lung dataset: one containing only the aorta and another including both the aorta and heart, enabling a detailed analysis of anatomical context. We compare our approach against baseline methods on both datasets, demonstrating its effectiveness in preserving vascular structures while enhancing contrast fidelity.", "AI": {"tldr": "\u57fa\u4e8e\u6865\u6269\u6563\u6a21\u578b\u4ece\u975e\u5bf9\u6bd4CT\u751f\u6210\u5408\u6210\u5bf9\u6bd4\u589e\u5f3aCTA\u56fe\u50cf\uff0c\u907f\u514d\u5bf9\u6bd4\u5242\u98ce\u9669\uff0c\u4fdd\u63013D\u89e3\u5256\u5b8c\u6574\u6027", "motivation": "\u5bf9\u6bd4\u5242CT\u6210\u50cf\u5bf9\u80f8\u4e3b\u52a8\u8109\u75be\u75c5\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5bf9\u6bd4\u5242\u5b58\u5728\u80be\u6bd2\u6027\u548c\u8fc7\u654f\u53cd\u5e94\u98ce\u9669\uff0c\u9700\u8981\u65e0\u5bf9\u6bd4\u5242\u7684\u9ad8\u4fdd\u771f\u5408\u6210CTA\u56fe\u50cf", "method": "\u4f7f\u7528Slice-Consistent Brownian Bridge Diffusion Model (SC-BBDM)\uff0c\u5728\u4fdd\u6301\u5207\u7247\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5efa\u6a21\u590d\u6742\u6620\u5c04\uff0c\u901a\u8fc7\u9884\u5904\u7406\u6d41\u7a0b\u5305\u62ec\u91cd\u91c7\u6837\u3001\u5bf9\u79f0\u5f52\u4e00\u5316\u914d\u51c6\u548c\u6269\u5f20\u5206\u5272\u63a9\u6a21\u63d0\u53d6\u4e3b\u52a8\u8109\u7ed3\u6784", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\uff08\u4ec5\u4e3b\u52a8\u8109\u548c\u4e3b\u52a8\u8109+\u5fc3\u810f\uff09\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u4fdd\u6301\u8840\u7ba1\u7ed3\u6784\u5e76\u63d0\u9ad8\u5bf9\u6bd4\u5ea6\u4fdd\u771f\u5ea6", "conclusion": "\u63d0\u51fa\u7684\u6865\u6269\u6563\u65b9\u6cd5\u9996\u6b21\u5b9e\u73b0\u4e86\u4ece\u975e\u5bf9\u6bd4CT\u5408\u6210\u5bf9\u6bd4\u589e\u5f3aCTA\uff0c\u5728\u4f4e\u5185\u5b58\u9884\u7b97\u4e0b\u4fdd\u63013D\u89e3\u5256\u5b8c\u6574\u6027\uff0c\u5177\u6709\u4e34\u5e8a\u8f6c\u5316\u6f5c\u529b"}}
{"id": "2508.16930", "pdf": "https://arxiv.org/pdf/2508.16930", "abs": "https://arxiv.org/abs/2508.16930", "authors": ["Sizhe Shan", "Qiulin Li", "Yutao Cui", "Miles Yang", "Yuehai Wang", "Qun Yang", "Jin Zhou", "Zhao Zhong"], "title": "HunyuanVideo-Foley: Multimodal Diffusion with Representation Alignment for High-Fidelity Foley Audio Generation", "categories": ["eess.AS", "cs.CV", "cs.SD"], "comment": null, "summary": "Recent advances in video generation produce visually realistic content, yet the absence of synchronized audio severely compromises immersion. To address key challenges in video-to-audio generation, including multimodal data scarcity, modality imbalance and limited audio quality in existing methods, we propose HunyuanVideo-Foley, an end-to-end text-video-to-audio framework that synthesizes high-fidelity audio precisely aligned with visual dynamics and semantic context. Our approach incorporates three core innovations: (1) a scalable data pipeline curating 100k-hour multimodal datasets through automated annotation; (2) a representation alignment strategy using self-supervised audio features to guide latent diffusion training, efficiently improving audio quality and generation stability; (3) a novel multimodal diffusion transformer resolving modal competition, containing dual-stream audio-video fusion through joint attention, and textual semantic injection via cross-attention. Comprehensive evaluations demonstrate that HunyuanVideo-Foley achieves new state-of-the-art performance across audio fidelity, visual-semantic alignment, temporal alignment and distribution matching. The demo page is available at: https://szczesnys.github.io/hunyuanvideo-foley/.", "AI": {"tldr": "HunyuanVideo-Foley\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6587\u672c-\u89c6\u9891-\u97f3\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u6838\u5fc3\u521b\u65b0\u89e3\u51b3\u4e86\u89c6\u9891\u5230\u97f3\u9891\u751f\u6210\u7684\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u591a\u6a21\u6001\u6570\u636e\u7a00\u7f3a\u3001\u6a21\u6001\u4e0d\u5e73\u8861\u548c\u73b0\u6709\u65b9\u6cd5\u7684\u97f3\u9891\u8d28\u91cf\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u4e0e\u89c6\u89c9\u52a8\u6001\u548c\u8bed\u4e49\u4e0a\u4e0b\u6587\u7cbe\u786e\u5bf9\u9f50\u7684\u9ad8\u4fdd\u771f\u97f3\u9891\u5408\u6210\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u751f\u6210\u6280\u672f\u867d\u7136\u80fd\u4ea7\u751f\u89c6\u89c9\u4e0a\u771f\u5b9e\u7684\u5185\u5bb9\uff0c\u4f46\u7f3a\u4e4f\u540c\u6b65\u97f3\u9891\u4e25\u91cd\u5f71\u54cd\u4e86\u6c89\u6d78\u611f\u3002\u9700\u8981\u89e3\u51b3\u591a\u6a21\u6001\u6570\u636e\u7a00\u7f3a\u3001\u6a21\u6001\u4e0d\u5e73\u8861\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\u97f3\u9891\u8d28\u91cf\u6709\u9650\u7b49\u5173\u952e\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e09\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a(1)\u53ef\u6269\u5c55\u7684\u6570\u636e\u7ba1\u9053\uff0c\u901a\u8fc7\u81ea\u52a8\u6807\u6ce8\u7b56\u5212100k\u5c0f\u65f6\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff1b(2)\u4f7f\u7528\u81ea\u76d1\u7763\u97f3\u9891\u7279\u5f81\u7684\u8868\u5f81\u5bf9\u9f50\u7b56\u7565\u6307\u5bfc\u6f5c\u5728\u6269\u6563\u8bad\u7ec3\uff1b(3)\u65b0\u9896\u7684\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\uff0c\u901a\u8fc7\u8054\u5408\u6ce8\u610f\u529b\u5b9e\u73b0\u53cc\u6d41\u97f3\u9891-\u89c6\u9891\u878d\u5408\uff0c\u5e76\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u6ce8\u5165\u6587\u672c\u8bed\u4e49\u3002", "result": "\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cHunyuanVideo-Foley\u5728\u97f3\u9891\u4fdd\u771f\u5ea6\u3001\u89c6\u89c9-\u8bed\u4e49\u5bf9\u9f50\u3001\u65f6\u95f4\u5bf9\u9f50\u548c\u5206\u5e03\u5339\u914d\u7b49\u65b9\u9762\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u9891\u5230\u97f3\u9891\u751f\u6210\u7684\u5173\u952e\u6280\u672f\u6311\u6218\uff0c\u4e3a\u591a\u6a21\u6001\u5185\u5bb9\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891\u751f\u6210\u7684\u8d28\u91cf\u548c\u540c\u6b65\u7cbe\u5ea6\u3002"}}
{"id": "2508.16987", "pdf": "https://arxiv.org/pdf/2508.16987", "abs": "https://arxiv.org/abs/2508.16987", "authors": ["Tanvir Bhathal", "Asanshay Gupta"], "title": "WebSight: A Vision-First Architecture for Robust Web Agents", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "We introduce WebSight, a vision-based autonomous web agent, designed to interact with web environments purely through visual perception, eliminating dependence on HTML or DOM-based inputs. Central to our approach we introduce our new model, WebSight-7B, a fine-tuned vision-language model optimized for UI element interaction, trained using LoRA on a web-focused subset of the Wave-UI-25K dataset. WebSight integrates this model into a modular multi-agent architecture, comprising planning, reasoning, vision-action, and verification agents, coordinated through an episodic memory mechanism.   WebSight-7B achieves a top-1 accuracy of 58.84% on the Showdown Clicks benchmark, outperforming several larger generalist models while maintaining lower latency. The full WebSight agent achieves a 68.0% success rate on the WebVoyager benchmark, surpassing systems from labs such as OpenAI (61.0%) and HCompany (Runner H, 67.0%). Among tasks completed, WebSight answers correctly 97.14% of the time, indicating high precision. Together, WebSight and WebSight-7B establish a new standard for interpretable, robust, and efficient visual web navigation.", "AI": {"tldr": "WebSight\u662f\u4e00\u4e2a\u7eaf\u89c6\u89c9\u7684\u81ea\u4e3b\u7f51\u9875\u4ee3\u7406\uff0c\u901a\u8fc7WebSight-7B\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u65e0\u9700HTML/DOM\u8f93\u5165\u7684\u7f51\u9875\u4ea4\u4e92\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6d88\u9664\u5bf9HTML\u6216DOM\u8f93\u5165\u7684\u4f9d\u8d56\uff0c\u5f00\u53d1\u7eaf\u89c6\u89c9\u611f\u77e5\u7684\u7f51\u9875\u4ea4\u4e92\u7cfb\u7edf\uff0c\u63d0\u9ad8\u7f51\u9875\u5bfc\u822a\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5305\u62ec\u89c4\u5212\u3001\u63a8\u7406\u3001\u89c6\u89c9\u52a8\u4f5c\u548c\u9a8c\u8bc1\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u60c5\u666f\u8bb0\u5fc6\u673a\u5236\u534f\u8c03\u3002\u4f7f\u7528LoRA\u5728Wave-UI-25K\u6570\u636e\u96c6\u4e0a\u5fae\u8c03WebSight-7B\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "result": "WebSight-7B\u5728Showdown Clicks\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523058.84%\u7684top-1\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u591a\u4e2a\u5927\u578b\u901a\u7528\u6a21\u578b\u3002\u5b8c\u6574WebSight\u4ee3\u7406\u5728WebVoyager\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523068.0%\u7684\u6210\u529f\u7387\uff0c\u8d85\u8d8aOpenAI\u548cHCompany\u7684\u7cfb\u7edf\u3002", "conclusion": "WebSight\u548cWebSight-7B\u4e3a\u53ef\u89e3\u91ca\u3001\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u89c6\u89c9\u7f51\u9875\u5bfc\u822a\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2508.17223", "pdf": "https://arxiv.org/pdf/2508.17223", "abs": "https://arxiv.org/abs/2508.17223", "authors": ["Asadullah Bin Rahman", "Masud Ibn Afjal", "Md. Abdulla Al Mamun"], "title": "Deep Learning Architectures for Medical Image Denoising: A Comparative Study of CNN-DAE, CADTra, and DCMIEDNet", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical imaging modalities are inherently susceptible to noise contamination that degrades diagnostic utility and clinical assessment accuracy. This paper presents a comprehensive comparative evaluation of three state-of-the-art deep learning architectures for MRI brain image denoising: CNN-DAE, CADTra, and DCMIEDNet. We systematically evaluate these models across multiple Gaussian noise intensities ($\\sigma = 10, 15, 25$) using the Figshare MRI Brain Dataset. Our experimental results demonstrate that DCMIEDNet achieves superior performance at lower noise levels, with PSNR values of $32.921 \\pm 2.350$ dB and $30.943 \\pm 2.339$ dB for $\\sigma = 10$ and $15$ respectively. However, CADTra exhibits greater robustness under severe noise conditions ($\\sigma = 25$), achieving the highest PSNR of $27.671 \\pm 2.091$ dB. All deep learning approaches significantly outperform traditional wavelet-based methods, with improvements ranging from 5-8 dB across tested conditions. This study establishes quantitative benchmarks for medical image denoising and provides insights into architecture-specific strengths for varying noise intensities.", "AI": {"tldr": "\u672c\u6587\u5bf9\u4e09\u79cd\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784(CNN-DAE\u3001CADTra\u3001DCMIEDNet)\u5728MRI\u8111\u56fe\u50cf\u53bb\u566a\u65b9\u9762\u8fdb\u884c\u4e86\u7cfb\u7edf\u6bd4\u8f83\uff0c\u53d1\u73b0DCMIEDNet\u5728\u4f4e\u566a\u58f0\u6c34\u5e73\u8868\u73b0\u6700\u4f73\uff0cCADTra\u5728\u9ad8\u566a\u58f0\u6761\u4ef6\u4e0b\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u6240\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5747\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5c0f\u6ce2\u65b9\u6cd5\u3002", "motivation": "\u533b\u5b66\u6210\u50cf\u6a21\u5f0f\u5bb9\u6613\u53d7\u5230\u566a\u58f0\u6c61\u67d3\uff0c\u8fd9\u4f1a\u964d\u4f4e\u8bca\u65ad\u6548\u7528\u548c\u4e34\u5e8a\u8bc4\u4f30\u51c6\u786e\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u53bb\u566a\u65b9\u6cd5\u6765\u63d0\u9ad8\u533b\u5b66\u56fe\u50cf\u8d28\u91cf\u3002", "method": "\u4f7f\u7528Figshare MRI\u8111\u6570\u636e\u96c6\uff0c\u5728\u591a\u4e2a\u9ad8\u65af\u566a\u58f0\u5f3a\u5ea6(\u03c3=10,15,25)\u4e0b\u7cfb\u7edf\u8bc4\u4f30\u4e09\u79cd\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff1aCNN-DAE\u3001CADTra\u548cDCMIEDNet\uff0c\u5e76\u4e0e\u4f20\u7edf\u5c0f\u6ce2\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "DCMIEDNet\u5728\u4f4e\u566a\u58f0\u6c34\u5e73(\u03c3=10\u548c15)\u8868\u73b0\u6700\u4f18\uff0cPSNR\u5206\u522b\u4e3a32.921\u00b12.350 dB\u548c30.943\u00b12.339 dB\uff1bCADTra\u5728\u9ad8\u566a\u58f0\u6761\u4ef6(\u03c3=25)\u4e0b\u6700\u9c81\u68d2\uff0cPSNR\u8fbe27.671\u00b12.091 dB\uff1b\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u53475-8 dB\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u533b\u5b66\u56fe\u50cf\u53bb\u566a\u5efa\u7acb\u4e86\u91cf\u5316\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u67b6\u6784\u5728\u4e0d\u540c\u566a\u58f0\u5f3a\u5ea6\u4e0b\u7684\u7279\u5b9a\u4f18\u52bf\uff0c\u4e3a\u4e34\u5e8a\u5e94\u7528\u4e2d\u9009\u62e9\u5408\u9002\u7684\u53bb\u566a\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2508.17326", "pdf": "https://arxiv.org/pdf/2508.17326", "abs": "https://arxiv.org/abs/2508.17326", "authors": ["Tristan S. W. Stevens", "Ois\u00edn Nolan", "Ruud J. G. van Sloun"], "title": "Semantic Diffusion Posterior Sampling for Cardiac Ultrasound Dehazing", "categories": ["eess.IV", "cs.CV"], "comment": "10 pages, 4 figures, MICCAI challenge", "summary": "Echocardiography plays a central role in cardiac imaging, offering dynamic views of the heart that are essential for diagnosis and monitoring. However, image quality can be significantly degraded by haze arising from multipath reverberations, particularly in difficult-to-image patients. In this work, we propose a semantic-guided, diffusion-based dehazing algorithm developed for the MICCAI Dehazing Echocardiography Challenge (DehazingEcho2025). Our method integrates a pixel-wise noise model, derived from semantic segmentation of hazy inputs into a diffusion posterior sampling framework guided by a generative prior trained on clean ultrasound data. Quantitative evaluation on the challenge dataset demonstrates strong performance across contrast and fidelity metrics. Code for the submitted algorithm is available at https://github.com/tristan-deep/semantic-diffusion-echo-dehazing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u7684\u8d85\u58f0\u5fc3\u52a8\u56fe\u53bb\u96fe\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u50cf\u7d20\u7ea7\u566a\u58f0\u6a21\u578b\u548c\u751f\u6210\u5148\u9a8c\uff0c\u5728MICCAI\u6311\u6218\u8d5b\u4e2d\u8868\u73b0\u51fa\u8272", "motivation": "\u8d85\u58f0\u5fc3\u52a8\u56fe\u5728\u5fc3\u810f\u6210\u50cf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u591a\u5f84\u6df7\u54cd\u5bfc\u81f4\u7684\u96fe\u72b6\u4f2a\u5f71\u4f1a\u4e25\u91cd\u5f71\u54cd\u56fe\u50cf\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u96be\u4ee5\u6210\u50cf\u7684\u60a3\u8005\u4e2d", "method": "\u5c06\u96fe\u72b6\u8f93\u5165\u7684\u8bed\u4e49\u5206\u5272\u5f97\u5230\u7684\u50cf\u7d20\u7ea7\u566a\u58f0\u6a21\u578b\u96c6\u6210\u5230\u6269\u6563\u540e\u9a8c\u91c7\u6837\u6846\u67b6\u4e2d\uff0c\u5e76\u7531\u5728\u5e72\u51c0\u8d85\u58f0\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u751f\u6210\u5148\u9a8c\u8fdb\u884c\u5f15\u5bfc", "result": "\u5728\u6311\u6218\u6570\u636e\u96c6\u4e0a\u7684\u5b9a\u91cf\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5bf9\u6bd4\u5ea6\u548c\u4fdd\u771f\u5ea6\u6307\u6807\u4e0a\u5747\u8868\u73b0\u51fa\u5f3a\u52b2\u6027\u80fd", "conclusion": "\u8be5\u8bed\u4e49\u5f15\u5bfc\u7684\u6269\u6563\u53bb\u96fe\u7b97\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u8d85\u58f0\u5fc3\u52a8\u56fe\u56fe\u50cf\u8d28\u91cf\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u66f4\u6e05\u6670\u7684\u5f71\u50cf\u652f\u6301"}}
{"id": "2508.17389", "pdf": "https://arxiv.org/pdf/2508.17389", "abs": "https://arxiv.org/abs/2508.17389", "authors": ["Bokai Zhao", "Weiyang Shi", "Hanqing Chao", "Zijiang Yang", "Yiyang Zhang", "Ming Song", "Tianzi Jiang"], "title": "Neural Proteomics Fields for Super-resolved Spatial Proteomics Prediction", "categories": ["q-bio.QM", "cs.AI", "cs.CV"], "comment": "MICCAI 2025", "summary": "Spatial proteomics maps protein distributions in tissues, providing transformative insights for life sciences. However, current sequencing-based technologies suffer from low spatial resolution, and substantial inter-tissue variability in protein expression further compromises the performance of existing molecular data prediction methods. In this work, we introduce the novel task of spatial super-resolution for sequencing-based spatial proteomics (seq-SP) and, to the best of our knowledge, propose the first deep learning model for this task--Neural Proteomics Fields (NPF). NPF formulates seq-SP as a protein reconstruction problem in continuous space by training a dedicated network for each tissue. The model comprises a Spatial Modeling Module, which learns tissue-specific protein spatial distributions, and a Morphology Modeling Module, which extracts tissue-specific morphological features. Furthermore, to facilitate rigorous evaluation, we establish an open-source benchmark dataset, Pseudo-Visium SP, for this task. Experimental results demonstrate that NPF achieves state-of-the-art performance with fewer learnable parameters, underscoring its potential for advancing spatial proteomics research. Our code and dataset are publicly available at https://github.com/Bokai-Zhao/NPF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u7a7a\u95f4\u86cb\u767d\u8d28\u7ec4\u5b66\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bNPF\uff0c\u901a\u8fc7\u7a7a\u95f4\u5efa\u6a21\u548c\u5f62\u6001\u5efa\u6a21\u6a21\u5757\uff0c\u5728\u8fde\u7eed\u7a7a\u95f4\u4e2d\u91cd\u5efa\u86cb\u767d\u8d28\u5206\u5e03\uff0c\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6d4b\u5e8f\u7a7a\u95f4\u86cb\u767d\u8d28\u7ec4\u5b66\u6280\u672f\u7a7a\u95f4\u5206\u8fa8\u7387\u4f4e\uff0c\u4e14\u86cb\u767d\u8d28\u8868\u8fbe\u5b58\u5728\u663e\u8457\u7684\u7ec4\u7ec7\u95f4\u53d8\u5f02\u6027\uff0c\u73b0\u6709\u5206\u5b50\u6570\u636e\u9884\u6d4b\u65b9\u6cd5\u6027\u80fd\u53d7\u9650\u3002", "method": "NPF\u5c06seq-SP\u5efa\u6a21\u4e3a\u8fde\u7eed\u7a7a\u95f4\u4e2d\u7684\u86cb\u767d\u8d28\u91cd\u5efa\u95ee\u9898\uff0c\u4e3a\u6bcf\u4e2a\u7ec4\u7ec7\u8bad\u7ec3\u4e13\u7528\u7f51\u7edc\uff0c\u5305\u542b\u5b66\u4e60\u7ec4\u7ec7\u7279\u5f02\u6027\u86cb\u767d\u8d28\u7a7a\u95f4\u5206\u5e03\u7684\u7a7a\u95f4\u5efa\u6a21\u6a21\u5757\u548c\u63d0\u53d6\u7ec4\u7ec7\u7279\u5f02\u6027\u5f62\u6001\u7279\u5f81\u7684\u5f62\u6001\u5efa\u6a21\u6a21\u5757\u3002", "result": "NPF\u5728Pseudo-Visium SP\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e14\u53c2\u6570\u91cf\u66f4\u5c11\u3002", "conclusion": "NPF\u6a21\u578b\u5177\u6709\u63a8\u52a8\u7a7a\u95f4\u86cb\u767d\u8d28\u7ec4\u5b66\u7814\u7a76\u7684\u6f5c\u529b\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.17600", "pdf": "https://arxiv.org/pdf/2508.17600", "abs": "https://arxiv.org/abs/2508.17600", "authors": ["Guanxing Lu", "Baoxiong Jia", "Puhao Li", "Yixin Chen", "Ziwei Wang", "Yansong Tang", "Siyuan Huang"], "title": "GWM: Towards Scalable Gaussian World Models for Robotic Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "Published at ICCV 2025. Project page:   https://gaussian-world-model.github.io/", "summary": "Training robot policies within a learned world model is trending due to the inefficiency of real-world interactions. The established image-based world models and policies have shown prior success, but lack robust geometric information that requires consistent spatial and physical understanding of the three-dimensional world, even pre-trained on internet-scale video sources. To this end, we propose a novel branch of world model named Gaussian World Model (GWM) for robotic manipulation, which reconstructs the future state by inferring the propagation of Gaussian primitives under the effect of robot actions. At its core is a latent Diffusion Transformer (DiT) combined with a 3D variational autoencoder, enabling fine-grained scene-level future state reconstruction with Gaussian Splatting. GWM can not only enhance the visual representation for imitation learning agent by self-supervised future prediction training, but can serve as a neural simulator that supports model-based reinforcement learning. Both simulated and real-world experiments depict that GWM can precisely predict future scenes conditioned on diverse robot actions, and can be further utilized to train policies that outperform the state-of-the-art by impressive margins, showcasing the initial data scaling potential of 3D world model.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9ad8\u65af\u539f\u8bed\u4f20\u64ad\u76843D\u4e16\u754c\u6a21\u578bGWM\uff0c\u901a\u8fc7\u6269\u6563\u53d8\u6362\u5668\u548c3D\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b9e\u73b0\u7cbe\u7ec6\u573a\u666f\u91cd\u5efa\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u6027\u80fd", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u50cf\u7684\u4e16\u754c\u6a21\u578b\u7f3a\u4e4f\u7a33\u5065\u7684\u51e0\u4f55\u4fe1\u606f\uff0c\u65e0\u6cd5\u63d0\u4f9b\u4e00\u81f4\u7684\u7a7a\u95f4\u548c\u7269\u7406\u7406\u89e3\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u66f4\u597d\u7406\u89e3\u4e09\u7ef4\u4e16\u754c\u7684\u6a21\u578b", "method": "\u63d0\u51fa\u9ad8\u65af\u4e16\u754c\u6a21\u578b(GWM)\uff0c\u901a\u8fc7\u63a8\u65ad\u9ad8\u65af\u539f\u8bed\u5728\u673a\u5668\u4eba\u52a8\u4f5c\u5f71\u54cd\u4e0b\u7684\u4f20\u64ad\u6765\u91cd\u5efa\u672a\u6765\u72b6\u6001\uff0c\u6838\u5fc3\u662f\u7ed3\u54083D\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u6f5c\u5728\u6269\u6563\u53d8\u6362\u5668\uff0c\u4f7f\u7528\u9ad8\u65af\u6cfc\u6e85\u8fdb\u884c\u7cbe\u7ec6\u573a\u666f\u7ea7\u91cd\u5efa", "result": "\u6a21\u62df\u548c\u771f\u5b9e\u5b9e\u9a8c\u8868\u660eGWM\u80fd\u7cbe\u786e\u9884\u6d4b\u4e0d\u540c\u673a\u5668\u4eba\u52a8\u4f5c\u6761\u4ef6\u4e0b\u7684\u672a\u6765\u573a\u666f\uff0c\u8bad\u7ec3\u7684\u7b56\u7565\u5728\u6027\u80fd\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "GWM\u5c55\u793a\u4e863D\u4e16\u754c\u6a21\u578b\u5728\u6570\u636e\u6269\u5c55\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u65e2\u80fd\u589e\u5f3a\u6a21\u4eff\u5b66\u4e60\u7684\u89c6\u89c9\u8868\u793a\uff0c\u53c8\u80fd\u4f5c\u4e3a\u652f\u6301\u57fa\u4e8e\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684\u795e\u7ecf\u6a21\u62df\u5668"}}
{"id": "2508.17957", "pdf": "https://arxiv.org/pdf/2508.17957", "abs": "https://arxiv.org/abs/2508.17957", "authors": ["Jianhao Huang", "Qunsong Zeng", "Hongyang Du", "Kaibin Huang"], "title": "Generative Feature Imputing - A Technique for Error-resilient Semantic Communication", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Semantic communication (SemCom) has emerged as a promising paradigm for achieving unprecedented communication efficiency in sixth-generation (6G) networks by leveraging artificial intelligence (AI) to extract and transmit the underlying meanings of source data. However, deploying SemCom over digital systems presents new challenges, particularly in ensuring robustness against transmission errors that may distort semantically critical content. To address this issue, this paper proposes a novel framework, termed generative feature imputing, which comprises three key techniques. First, we introduce a spatial error concentration packetization strategy that spatially concentrates feature distortions by encoding feature elements based on their channel mappings, a property crucial for both the effectiveness and reduced complexity of the subsequent techniques. Second, building on this strategy, we propose a generative feature imputing method that utilizes a diffusion model to efficiently reconstruct missing features caused by packet losses. Finally, we develop a semantic-aware power allocation scheme that enables unequal error protection by allocating transmission power according to the semantic importance of each packet. Experimental results demonstrate that the proposed framework outperforms conventional approaches, such as Deep Joint Source-Channel Coding (DJSCC) and JPEG2000, under block fading conditions, achieving higher semantic accuracy and lower Learned Perceptual Image Patch Similarity (LPIPS) scores.", "AI": {"tldr": "\u63d0\u51fa\u751f\u6210\u5f0f\u7279\u5f81\u586b\u8865\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u9519\u8bef\u96c6\u4e2d\u6253\u5305\u3001\u6269\u6563\u6a21\u578b\u7279\u5f81\u91cd\u5efa\u548c\u8bed\u4e49\u611f\u77e5\u529f\u7387\u5206\u914d\uff0c\u63d0\u5347\u8bed\u4e49\u901a\u4fe1\u5728\u6570\u5b57\u7cfb\u7edf\u4e2d\u7684\u9c81\u68d2\u6027", "motivation": "\u8bed\u4e49\u901a\u4fe1\u57286G\u7f51\u7edc\u4e2d\u5177\u6709\u9ad8\u6548\u901a\u4fe1\u4f18\u52bf\uff0c\u4f46\u5728\u6570\u5b57\u7cfb\u7edf\u4e2d\u9762\u4e34\u4f20\u8f93\u9519\u8bef\u5bfc\u81f4\u8bed\u4e49\u5185\u5bb9\u5931\u771f\u7684\u6311\u6218\uff0c\u9700\u8981\u786e\u4fdd\u9c81\u68d2\u6027", "method": "1. \u7a7a\u95f4\u9519\u8bef\u96c6\u4e2d\u6253\u5305\u7b56\u7565\uff1a\u57fa\u4e8e\u4fe1\u9053\u6620\u5c04\u7f16\u7801\u7279\u5f81\u5143\u7d20\uff0c\u96c6\u4e2d\u7279\u5f81\u5931\u771f\uff1b2. \u751f\u6210\u5f0f\u7279\u5f81\u586b\u8865\u65b9\u6cd5\uff1a\u4f7f\u7528\u6269\u6563\u6a21\u578b\u91cd\u5efa\u4e22\u5931\u7279\u5f81\uff1b3. \u8bed\u4e49\u611f\u77e5\u529f\u7387\u5206\u914d\uff1a\u6839\u636e\u8bed\u4e49\u91cd\u8981\u6027\u8fdb\u884c\u4e0d\u7b49\u5dee\u9519\u4fdd\u62a4", "result": "\u5728\u5757\u8870\u843d\u6761\u4ef6\u4e0b\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff08DJSCC\u548cJPEG2000\uff09\uff0c\u83b7\u5f97\u66f4\u9ad8\u7684\u8bed\u4e49\u51c6\u786e\u6027\u548c\u66f4\u4f4e\u7684LPIPS\u5206\u6570", "conclusion": "\u6240\u63d0\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u5bf9\u4f20\u8f93\u9519\u8bef\u7684\u9c81\u68d2\u6027\uff0c\u4e3a6G\u7f51\u7edc\u4e2d\u7684\u8bed\u4e49\u901a\u4fe1\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.17969", "pdf": "https://arxiv.org/pdf/2508.17969", "abs": "https://arxiv.org/abs/2508.17969", "authors": ["Alexandros Gkillas", "Christos Anagnostopoulos", "Nikos Piperigkos", "Dimitris Tsiktsiris", "Theofilos Christodoulou", "Theofanis Siamatras", "Dimitrios Triantafyllou", "Christos Basdekis", "Theoktisti Marinopoulou", "Panagiotis Lepentsiotis", "Elefterios Blitsis", "Aggeliki Zacharaki", "Nearchos Stylianidis", "Leonidas Katelaris", "Lamberto Salvan", "Aris S. Lalos", "Christos Laoudias", "Antonios Lalas", "Konstantinos Votis"], "title": "A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigm", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "This paper introduces a holistic perception system for internal and external monitoring of autonomous vehicles, with the aim of demonstrating a novel AI-leveraged self-adaptive framework of advanced vehicle technologies and solutions that optimize perception and experience on-board. Internal monitoring system relies on a multi-camera setup designed for predicting and identifying driver and occupant behavior through facial recognition, exploiting in addition a large language model as virtual assistant. Moreover, the in-cabin monitoring system includes AI-empowered smart sensors that measure air-quality and perform thermal comfort analysis for efficient on and off-boarding. On the other hand, external monitoring system perceives the surrounding environment of vehicle, through a LiDAR-based cost-efficient semantic segmentation approach, that performs highly accurate and efficient super-resolution on low-quality raw 3D point clouds. The holistic perception framework is developed in the context of EU's Horizon Europe programm AutoTRUST, and has been integrated and deployed on a real electric vehicle provided by ALKE. Experimental validation and evaluation at the integration site of Joint Research Centre at Ispra, Italy, highlights increased performance and efficiency of the modular blocks of the proposed perception architecture.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u4e3b\u9a7e\u9a76\u6c7d\u8f66\u5185\u5916\u76d1\u6d4b\u7684\u6574\u4f53\u611f\u77e5\u7cfb\u7edf\uff0c\u901a\u8fc7AI\u6280\u672f\u4f18\u5316\u8f66\u5185\u4f53\u9a8c\u548c\u73af\u5883\u611f\u77e5\u6027\u80fd\u3002", "motivation": "\u4e3a\u81ea\u4e3b\u9a7e\u9a76\u6c7d\u8f66\u5f00\u53d1\u4e00\u79cd\u6574\u4f53\u6027\u7684\u611f\u77e5\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u76d1\u6d4b\u8f66\u5185\u9a7e\u9a76\u5458\u884c\u4e3a\u548c\u8f66\u5916\u73af\u5883\uff0c\u4ee5\u63d0\u5347\u5b89\u5168\u6027\u548c\u4e58\u5750\u4f53\u9a8c\u3002", "method": "\u7cfb\u7edf\u5305\u542b\u4e24\u90e8\u5206\uff1a\u5185\u90e8\u76d1\u6d4b\u7cfb\u7edf\u4f7f\u7528\u591a\u6444\u50cf\u5934\u8fdb\u884c\u9762\u90e8\u8bc6\u522b\u548c\u884c\u4e3a\u9884\u6d4b\uff0c\u96c6\u6210\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u865a\u62df\u52a9\u624b\uff0c\u4ee5\u53caAI\u667a\u80fd\u4f20\u611f\u5668\u76d1\u6d4b\u7a7a\u6c14\u8d28\u91cf\u548c\u70ed\u8212\u9002\u6027\u3002\u5916\u90e8\u76d1\u6d4b\u7cfb\u7edf\u91c7\u7528LiDAR\u6280\u672f\u8fdb\u884c\u6210\u672c\u6548\u76ca\u9ad8\u7684\u8bed\u4e49\u5206\u5272\uff0c\u5bf9\u4f4e\u8d28\u91cf3D\u70b9\u4e91\u8fdb\u884c\u8d85\u5206\u8fa8\u7387\u5904\u7406\u3002", "result": "\u8be5\u6574\u4f53\u611f\u77e5\u6846\u67b6\u5df2\u5728\u6b27\u76dfHorizon Europe\u9879\u76eeAutoTRUST\u4e2d\u5f00\u53d1\uff0c\u5e76\u90e8\u7f72\u5728ALKE\u63d0\u4f9b\u7684\u771f\u5b9e\u7535\u52a8\u6c7d\u8f66\u4e0a\u3002\u5728\u610f\u5927\u5229Ispra\u8054\u5408\u7814\u7a76\u4e2d\u5fc3\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\uff0c\u8be5\u611f\u77e5\u67b6\u6784\u7684\u6a21\u5757\u5316\u6a21\u5757\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u548c\u6548\u7387\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u521b\u65b0\u7684AI\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u611f\u77e5\u6846\u67b6\uff0c\u80fd\u591f\u5728\u771f\u5b9e\u6c7d\u8f66\u73af\u5883\u4e2d\u6709\u6548\u5730\u76d1\u6d4b\u8f66\u5185\u5916\u73af\u5883\uff0c\u4e3a\u81ea\u4e3b\u9a7e\u9a76\u6c7d\u8f66\u7684\u5b89\u5168\u6027\u548c\u7528\u6237\u4f53\u9a8c\u63d0\u4f9b\u4e86\u7efc\u5408\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
