{"id": "2508.21058", "pdf": "https://arxiv.org/pdf/2508.21058", "abs": "https://arxiv.org/abs/2508.21058", "authors": ["Shengqu Cai", "Ceyuan Yang", "Lvmin Zhang", "Yuwei Guo", "Junfei Xiao", "Ziyan Yang", "Yinghao Xu", "Zhenheng Yang", "Alan Yuille", "Leonidas Guibas", "Maneesh Agrawala", "Lu Jiang", "Gordon Wetzstein"], "title": "Mixture of Contexts for Long Video Generation", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Project page: https://primecai.github.io/moc/", "summary": "Long video generation is fundamentally a long context memory problem: models must retain and retrieve salient events across a long range without collapsing or drifting. However, scaling diffusion transformers to generate long-context videos is fundamentally limited by the quadratic cost of self-attention, which makes memory and computation intractable and difficult to optimize for long sequences. We recast long-context video generation as an internal information retrieval task and propose a simple, learnable sparse attention routing module, Mixture of Contexts (MoC), as an effective long-term memory retrieval engine. In MoC, each query dynamically selects a few informative chunks plus mandatory anchors (caption, local windows) to attend to, with causal routing that prevents loop closures. As we scale the data and gradually sparsify the routing, the model allocates compute to salient history, preserving identities, actions, and scenes over minutes of content. Efficiency follows as a byproduct of retrieval (near-linear scaling), which enables practical training and synthesis, and the emergence of memory and consistency at the scale of minutes.", "AI": {"tldr": "\u63d0\u51faMixture of Contexts (MoC)\u7a00\u758f\u6ce8\u610f\u529b\u8def\u7531\u6a21\u5757\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u4fe1\u606f\u5757\u548c\u5f3a\u5236\u951a\u70b9\u6765\u89e3\u51b3\u957f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u957f\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u95ee\u9898\uff0c\u5b9e\u73b0\u8fd1\u7ebf\u6027\u8ba1\u7b97\u7f29\u653e\u548c\u5206\u949f\u7ea7\u4e00\u81f4\u6027\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u6269\u6563\u53d8\u6362\u5668\u5728\u751f\u6210\u957f\u89c6\u9891\u65f6\u56e0\u81ea\u6ce8\u610f\u529b\u4e8c\u6b21\u8ba1\u7b97\u6210\u672c\u5bfc\u81f4\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u4e0d\u53ef\u884c\u95ee\u9898\uff0c\u4ee5\u53ca\u957f\u5e8f\u5217\u4f18\u5316\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u5c06\u957f\u4e0a\u4e0b\u6587\u89c6\u9891\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5185\u90e8\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u7a00\u758f\u6ce8\u610f\u529b\u8def\u7531\u6a21\u5757MoC\uff0c\u6bcf\u4e2a\u67e5\u8be2\u52a8\u6001\u9009\u62e9\u5c11\u91cf\u4fe1\u606f\u5757\u548c\u5f3a\u5236\u951a\u70b9\uff08\u6807\u9898\u3001\u5c40\u90e8\u7a97\u53e3\uff09\u8fdb\u884c\u5173\u6ce8\uff0c\u91c7\u7528\u56e0\u679c\u8def\u7531\u9632\u6b62\u5faa\u73af\u95ed\u5408\u3002", "result": "\u901a\u8fc7\u6269\u5c55\u6570\u636e\u548c\u9010\u6b65\u7a00\u758f\u5316\u8def\u7531\uff0c\u6a21\u578b\u80fd\u591f\u5c06\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u7ed9\u663e\u8457\u5386\u53f2\u4fe1\u606f\uff0c\u5728\u5206\u949f\u7ea7\u5185\u5bb9\u4e2d\u4fdd\u6301\u8eab\u4efd\u3001\u52a8\u4f5c\u548c\u573a\u666f\u7684\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u8fd1\u7ebf\u6027\u8ba1\u7b97\u7f29\u653e\u3002", "conclusion": "MoC\u6a21\u5757\u4f5c\u4e3a\u6709\u6548\u7684\u957f\u671f\u8bb0\u5fc6\u68c0\u7d22\u5f15\u64ce\uff0c\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u5b9e\u7528\u7684\u8bad\u7ec3\u548c\u5408\u6210\uff0c\u5e76\u5728\u5206\u949f\u5c3a\u5ea6\u4e0a\u5b9e\u73b0\u4e86\u8bb0\u5fc6\u548c\u4e00\u81f4\u6027\u7684\u6d8c\u73b0\u3002"}}
{"id": "2508.10931", "pdf": "https://arxiv.org/pdf/2508.10931", "abs": "https://arxiv.org/abs/2508.10931", "authors": ["Wenqi Guo", "Shan Du"], "title": "VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By Value Sign Flip", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "We introduce Value Sign Flip (VSF), a simple and efficient method for incorporating negative prompt guidance in few-step diffusion and flow-matching image generation models. Unlike existing approaches such as classifier-free guidance (CFG), NASA, and NAG, VSF dynamically suppresses undesired content by flipping the sign of attention values from negative prompts. Our method requires only small computational overhead and integrates effectively with MMDiT-style architectures such as Stable Diffusion 3.5 Turbo, as well as cross-attention-based models like Wan. We validate VSF on challenging datasets with complex prompt pairs and demonstrate superior performance in both static image and video generation tasks. Experimental results show that VSF significantly improves negative prompt adherence compared to prior methods in few-step models, and even CFG in non-few-step models, while maintaining competitive image quality. Code and ComfyUI node are available in https://github.com/weathon/VSF/tree/main.", "AI": {"tldr": "VSF\u662f\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u8d1f\u5411\u63d0\u793a\u5f15\u5bfc\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ffb\u8f6c\u6ce8\u610f\u529b\u503c\u7684\u7b26\u53f7\u6765\u6291\u5236\u4e0d\u9700\u8981\u7684\u5185\u5bb9\uff0c\u5728\u5c11\u6b65\u6269\u6563\u548c\u6d41\u5339\u914d\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u8d1f\u5411\u63d0\u793a\u5f15\u5bfc\u65b9\u6cd5\u5982CFG\u3001NASA\u548cNAG\u5728\u5c11\u6b65\u751f\u6210\u6a21\u578b\u4e2d\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u52a8\u6001\u6291\u5236\u4e0d\u9700\u8981\u7684\u5185\u5bb9\u3002", "method": "VSF\u901a\u8fc7\u52a8\u6001\u7ffb\u8f6c\u8d1f\u5411\u63d0\u793a\u7684\u6ce8\u610f\u529b\u503c\u7b26\u53f7\u6765\u6291\u5236\u4e0d\u9700\u8981\u7684\u5185\u5bb9\uff0c\u8ba1\u7b97\u5f00\u9500\u5c0f\uff0c\u517c\u5bb9MMDiT\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u67b6\u6784\u3002", "result": "\u5728\u590d\u6742\u63d0\u793a\u5bf9\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u663e\u793a\uff0cVSF\u5728\u5c11\u6b65\u6a21\u578b\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u975e\u5c11\u6b65\u6a21\u578b\u4e2d\u751a\u81f3\u4f18\u4e8eCFG\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "VSF\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u8d1f\u5411\u63d0\u793a\u5f15\u5bfc\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9759\u6001\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.20182", "pdf": "https://arxiv.org/pdf/2508.20182", "abs": "https://arxiv.org/abs/2508.20182", "authors": ["Yang Su", "Shunquan Tan", "Jiwu Huang"], "title": "SDiFL: Stable Diffusion-Driven Framework for Image Forgery Localization", "categories": ["cs.CV"], "comment": null, "summary": "Driven by the new generation of multi-modal large models, such as Stable Diffusion (SD), image manipulation technologies have advanced rapidly, posing significant challenges to image forensics. However, existing image forgery localization methods, which heavily rely on labor-intensive and costly annotated data, are struggling to keep pace with these emerging image manipulation technologies. To address these challenges, we are the first to integrate both image generation and powerful perceptual capabilities of SD into an image forensic framework, enabling more efficient and accurate forgery localization. First, we theoretically show that the multi-modal architecture of SD can be conditioned on forgery-related information, enabling the model to inherently output forgery localization results. Then, building on this foundation, we specifically leverage the multimodal framework of Stable DiffusionV3 (SD3) to enhance forgery localization performance.We leverage the multi-modal processing capabilities of SD3 in the latent space by treating image forgery residuals -- high-frequency signals extracted using specific highpass filters -- as an explicit modality. This modality is fused into the latent space during training to enhance forgery localization performance. Notably, our method fully preserves the latent features extracted by SD3, thereby retaining the rich semantic information of the input image. Experimental results show that our framework achieves up to 12% improvements in performance on widely used benchmarking datasets compared to current state-of-the-art image forgery localization models. Encouragingly, the model demonstrates strong performance on forensic tasks involving real-world document forgery images and natural scene forging images, even when such data were entirely unseen during training.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u5c06Stable Diffusion\u7684\u591a\u6a21\u6001\u751f\u6210\u548c\u611f\u77e5\u80fd\u529b\u6574\u5408\u5230\u56fe\u50cf\u53d6\u8bc1\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u7406\u8bba\u8bc1\u660eSD\u67b6\u6784\u53ef\u57fa\u4e8e\u4f2a\u9020\u76f8\u5173\u4fe1\u606f\u8fdb\u884c\u6761\u4ef6\u5316\u5904\u7406\uff0c\u5e76\u5229\u7528SD3\u7684\u591a\u6a21\u6001\u6846\u67b6\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5904\u7406\u56fe\u50cf\u4f2a\u9020\u6b8b\u5dee\u4f5c\u4e3a\u663e\u5f0f\u6a21\u6001\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u9ad8\u6548\u51c6\u786e\u4f2a\u9020\u5b9a\u4f4d\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u4f2a\u9020\u5b9a\u4f4d\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff0c\u96be\u4ee5\u8ddf\u4e0a\u591a\u6a21\u6001\u5927\u6a21\u578b\u9a71\u52a8\u7684\u56fe\u50cf\u5904\u7406\u6280\u672f\u53d1\u5c55\u901f\u5ea6\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u51c6\u786e\u7684\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u5229\u7528Stable Diffusion V3\u7684\u591a\u6a21\u6001\u67b6\u6784\uff0c\u5c06\u56fe\u50cf\u4f2a\u9020\u6b8b\u5dee\uff08\u901a\u8fc7\u9ad8\u901a\u6ee4\u6ce2\u5668\u63d0\u53d6\u7684\u9ad8\u9891\u4fe1\u53f7\uff09\u4f5c\u4e3a\u663e\u5f0f\u6a21\u6001\u878d\u5408\u5230\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u540c\u65f6\u5b8c\u6574\u4fdd\u7559SD3\u63d0\u53d6\u7684\u6f5c\u5728\u7279\u5f81\u548c\u8f93\u5165\u56fe\u50cf\u7684\u4e30\u5bcc\u8bed\u4e49\u4fe1\u606f\u3002", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u56fe\u50cf\u4f2a\u9020\u5b9a\u4f4d\u6a21\u578b\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe12%\uff0c\u5728\u8bad\u7ec3\u4e2d\u672a\u89c1\u8fc7\u7684\u771f\u5b9e\u6587\u6863\u4f2a\u9020\u56fe\u50cf\u548c\u81ea\u7136\u573a\u666f\u4f2a\u9020\u56fe\u50cf\u4e0a\u4e5f\u8868\u73b0\u51fa\u5f3a\u5927\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c06\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u751f\u6210\u548c\u611f\u77e5\u80fd\u529b\u6574\u5408\u5230\u56fe\u50cf\u53d6\u8bc1\u4e2d\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u9ad8\u6548\u4f2a\u9020\u5b9a\u4f4d\uff0c\u4e3a\u5e94\u5bf9\u65b0\u5174\u56fe\u50cf\u5904\u7406\u6280\u672f\u5e26\u6765\u7684\u53d6\u8bc1\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.20379", "pdf": "https://arxiv.org/pdf/2508.20379", "abs": "https://arxiv.org/abs/2508.20379", "authors": ["Hyeonyu Kim", "Seokhoon Jeong", "Seonghee Han", "Chanhyuk Choi", "Taehwan Kim"], "title": "Audio-Guided Visual Editing with Complex Multi-Modal Prompts", "categories": ["cs.CV"], "comment": "Accepted to BMVC 2025", "summary": "Visual editing with diffusion models has made significant progress but often struggles with complex scenarios that textual guidance alone could not adequately describe, highlighting the need for additional non-text editing prompts. In this work, we introduce a novel audio-guided visual editing framework that can handle complex editing tasks with multiple text and audio prompts without requiring additional training. Existing audio-guided visual editing methods often necessitate training on specific datasets to align audio with text, limiting their generalization to real-world situations. We leverage a pre-trained multi-modal encoder with strong zero-shot capabilities and integrate diverse audio into visual editing tasks, by alleviating the discrepancy between the audio encoder space and the diffusion model's prompt encoder space. Additionally, we propose a novel approach to handle complex scenarios with multiple and multi-modal editing prompts through our separate noise branching and adaptive patch selection. Our comprehensive experiments on diverse editing tasks demonstrate that our framework excels in handling complicated editing scenarios by incorporating rich information from audio, where text-only approaches fail.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u97f3\u9891\u6307\u5bfc\u89c6\u89c9\u7f16\u8f91\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u591a\u6a21\u6001\u7f16\u8f91\u4efb\u52a1\uff0c\u800c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u7684\u7edf\u4e00\u6a21\u578b\u901a\u8fc7\u6587\u672c\u6307\u5bfc\u8fdb\u884c\u89c6\u89c9\u7f16\u8f91\u5728\u590d\u6742\u573a\u666f\u4e0b\u5b58\u5728\u9650\u5236\uff0c\u9700\u8981\u989d\u5916\u7684\u975e\u6587\u672c\u7f16\u8f91\u63d0\u793a\uff0c\u800c\u73b0\u6709\u97f3\u9891\u6307\u5bfc\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u7279\u5b9a\u6570\u636e\u96c6\u8bad\u7ec3\u6765\u5bf9\u9f50\u97f3\u9891\u548c\u6587\u672c\uff0c\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u51cf\u5c11\u97f3\u9891\u7f16\u7801\u7a7a\u95f4\u4e0e\u53bb\u6563\u6a21\u578b\u63d0\u793a\u7f16\u7801\u7a7a\u95f4\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u96c6\u6210\u591a\u6837\u5316\u97f3\u9891\u5230\u89c6\u89c9\u7f16\u8f91\u4efb\u52a1\u4e2d\uff0c\u5e76\u63d0\u51fa\u5206\u79bb\u566a\u58f0\u5206\u652f\u548c\u9002\u5e94\u6027\u8865\u4e01\u9009\u62e9\u6765\u5904\u7406\u590d\u6742\u7684\u591a\u6a21\u6001\u7f16\u8f91\u63d0\u793a\u3002", "result": "\u5728\u591a\u6837\u5316\u7f16\u8f91\u4efb\u52a1\u4e0a\u8fdb\u884c\u7684\u5168\u9762\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u901a\u8fc7\u878d\u5408\u97f3\u9891\u7684\u4e30\u5bcc\u4fe1\u606f\u6765\u5904\u7406\u590d\u6742\u7684\u7f16\u8f91\u573a\u666f\uff0c\u800c\u4ec5\u4f9d\u9760\u6587\u672c\u7684\u65b9\u6cd5\u5219\u65e0\u6cd5\u5b8c\u6210\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u590d\u6742\u591a\u6a21\u6001\u89c6\u89c9\u7f16\u8f91\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891\u6307\u5bfc\u5728\u89c6\u89c9\u7f16\u8f91\u4e2d\u7684\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2508.20471", "pdf": "https://arxiv.org/pdf/2508.20471", "abs": "https://arxiv.org/abs/2508.20471", "authors": ["Jiusi Li", "Jackson Jiang", "Jinyu Miao", "Miao Long", "Tuopu Wen", "Peijin Jia", "Shengxiang Liu", "Chunlei Yu", "Maolin Liu", "Yuzhan Cai", "Kun Jiang", "Mengmeng Yang", "Diange Yang"], "title": "Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Corner cases are crucial for training and validating autonomous driving systems, yet collecting them from the real world is often costly and hazardous. Editing objects within captured sensor data offers an effective alternative for generating diverse scenarios, commonly achieved through 3D Gaussian Splatting or image generative models. However, these approaches often suffer from limited visual fidelity or imprecise pose control. To address these issues, we propose G^2Editor, a framework designed for photorealistic and precise object editing in driving videos. Our method leverages a 3D Gaussian representation of the edited object as a dense prior, injected into the denoising process to ensure accurate pose control and spatial consistency. A scene-level 3D bounding box layout is employed to reconstruct occluded areas of non-target objects. Furthermore, to guide the appearance details of the edited object, we incorporate hierarchical fine-grained features as additional conditions during generation. Experiments on the Waymo Open Dataset demonstrate that G^2Editor effectively supports object repositioning, insertion, and deletion within a unified framework, outperforming existing methods in both pose controllability and visual quality, while also benefiting downstream data-driven tasks.", "AI": {"tldr": "G^2Editor\u662f\u4e00\u4e2a\u7528\u4e8e\u9a7e\u9a76\u89c6\u9891\u4e2d\u903c\u771f\u7cbe\u786e\u7269\u4f53\u7f16\u8f91\u7684\u6846\u67b6\uff0c\u901a\u8fc73D\u9ad8\u65af\u8868\u793a\u548c\u5c42\u6b21\u5316\u7279\u5f81\u63a7\u5236\uff0c\u5b9e\u73b0\u7269\u4f53\u91cd\u65b0\u5b9a\u4f4d\u3001\u63d2\u5165\u548c\u5220\u9664\uff0c\u5728\u59ff\u6001\u63a7\u5236\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u8bad\u7ec3\u548c\u9a8c\u8bc1\u9700\u8981\u5927\u91cf\u89d2\u70b9\u6848\u4f8b\uff0c\u4f46\u771f\u5b9e\u4e16\u754c\u6536\u96c6\u6210\u672c\u9ad8\u4e14\u5371\u9669\u3002\u73b0\u6709\u7f16\u8f91\u65b9\u6cd5\u5b58\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u6709\u9650\u6216\u59ff\u6001\u63a7\u5236\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u7f16\u8f91\u5bf9\u8c61\u76843D\u9ad8\u65af\u8868\u793a\u4f5c\u4e3a\u5bc6\u96c6\u5148\u9a8c\uff0c\u6ce8\u5165\u53bb\u566a\u8fc7\u7a0b\u786e\u4fdd\u7cbe\u786e\u59ff\u6001\u63a7\u5236\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\uff1b\u4f7f\u7528\u573a\u666f\u7ea73D\u8fb9\u754c\u6846\u5e03\u5c40\u91cd\u5efa\u975e\u76ee\u6807\u5bf9\u8c61\u7684\u906e\u6321\u533a\u57df\uff1b\u5f15\u5165\u5c42\u6b21\u5316\u7ec6\u7c92\u5ea6\u7279\u5f81\u6307\u5bfc\u7f16\u8f91\u5bf9\u8c61\u7684\u5916\u89c2\u7ec6\u8282\u3002", "result": "\u5728Waymo Open Dataset\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cG^2Editor\u5728\u59ff\u6001\u53ef\u63a7\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u6709\u6548\u652f\u6301\u4e0b\u6e38\u6570\u636e\u9a71\u52a8\u4efb\u52a1\u3002", "conclusion": "G^2Editor\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u73b0\u9a7e\u9a76\u89c6\u9891\u4e2d\u903c\u771f\u4e14\u7cbe\u786e\u7684\u7269\u4f53\u7f16\u8f91\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u751f\u6210\u65b9\u6848\u3002"}}
{"id": "2508.20476", "pdf": "https://arxiv.org/pdf/2508.20476", "abs": "https://arxiv.org/abs/2508.20476", "authors": ["Jeong Hun Yeo", "Hyeongseop Rha", "Sungjune Park", "Junil Won", "Yong Man Ro"], "title": "Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding", "categories": ["cs.CV", "cs.MM", "eess.AS", "eess.IV"], "comment": "Code available at: https://github.com/JeongHun0716/UniSLA", "summary": "Audio is the primary modality for human communication and has driven the success of Automatic Speech Recognition (ASR) technologies. However, such systems remain inherently inaccessible to individuals who are deaf or hard of hearing. Visual alternatives such as sign language and lip reading offer effective substitutes, and recent advances in Sign Language Translation (SLT) and Visual Speech Recognition (VSR) have improved audio-less communication. Yet, these modalities have largely been studied in isolation, and their integration within a unified framework remains underexplored. In this paper, we introduce the first unified framework capable of handling diverse combinations of sign language, lip movements, and audio for spoken-language text generation. We focus on three main objectives: (i) designing a unified, modality-agnostic architecture capable of effectively processing heterogeneous inputs; (ii) exploring the underexamined synergy among modalities, particularly the role of lip movements as non-manual cues in sign language comprehension; and (iii) achieving performance on par with or superior to state-of-the-art models specialized for individual tasks. Building on this framework, we achieve performance on par with or better than task-specific state-of-the-art models across SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that explicitly modeling lip movements as a separate modality significantly improves SLT performance.", "AI": {"tldr": "\u9996\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u624b\u8bed\u3001\u5507\u90e8\u52a8\u4f5c\u548c\u97f3\u9891\u7b49\u591a\u6a21\u6001\u7ec4\u5408\uff0c\u7528\u4e8e\u8bed\u8a00\u6587\u672c\u751f\u6210\uff0c\u6027\u80fd\u8d85\u8fc7\u4e13\u95e8\u6a21\u578b", "motivation": "\u73b0\u6709\u8bed\u97f3\u8bc6\u522b\u6280\u672f\u5bf9\u804b\u542c\u969c\u788d\u4eba\u58eb\u4e0d\u53cb\u597d\uff0c\u800c\u624b\u8bed\u548c\u5507\u8bfb\u7b49\u89c6\u89c9\u6a21\u6001\u53c8\u901a\u5e38\u5355\u72ec\u7814\u7a76\uff0c\u9700\u8981\u7edf\u4e00\u6846\u67b6\u6765\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u6a21\u6001\u65e0\u5173\u7684\u67b6\u6784\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5f02\u6784\u8f93\u5165\uff0c\u660e\u786e\u5c06\u5507\u90e8\u52a8\u4f5c\u5efa\u6a21\u4e3a\u72ec\u7acb\u6a21\u6001", "result": "\u5728SLT\u3001VSR\u3001ASR\u548cAVSR\u7b49\u4efb\u52a1\u4e0a\u8fbe\u5230\u6216\u8d85\u8fc7\u4e86\u4e13\u95e8\u6a21\u578b\u7684\u6700\u9ad8\u6c34\u5e73\uff0c\u660e\u786e\u5efa\u6a21\u5507\u90e8\u52a8\u4f5c\u663e\u8457\u63d0\u5347\u4e86\u624b\u8bed\u7ffb\u8bd1\u6027\u80fd", "conclusion": "\u591a\u6a21\u6001\u7edf\u4e00\u6846\u67b6\u5728\u8bed\u97f3\u8bc6\u522b\u9886\u57df\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u5507\u90e8\u52a8\u4f5c\u4f5c\u4e3a\u975e\u624b\u52bf\u7d22\u5f15\u5728\u624b\u8bed\u7406\u89e3\u4e2d\u53d1\u6325\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u804b\u542c\u969c\u788d\u4eba\u58eb\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u901a\u4fe1\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.20505", "pdf": "https://arxiv.org/pdf/2508.20505", "abs": "https://arxiv.org/abs/2508.20505", "authors": ["En Ci", "Shanyan Guan", "Yanhao Ge", "Yilin Zhang", "Wei Li", "Zhenyu Zhang", "Jian Yang", "Ying Tai"], "title": "Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Despite the progress in text-to-image generation, semantic image editing remains a challenge. Inversion-based algorithms unavoidably introduce reconstruction errors, while instruction-based models mainly suffer from limited dataset quality and scale. To address these problems, we propose a descriptive-prompt-based editing framework, named DescriptiveEdit. The core idea is to re-frame `instruction-based image editing' as `reference-image-based text-to-image generation', which preserves the generative power of well-trained Text-to-Image models without architectural modifications or inversion. Specifically, taking the reference image and a prompt as input, we introduce a Cross-Attentive UNet, which newly adds attention bridges to inject reference image features into the prompt-to-edit-image generation process. Owing to its text-to-image nature, DescriptiveEdit overcomes limitations in instruction dataset quality, integrates seamlessly with ControlNet, IP-Adapter, and other extensions, and is more scalable. Experiments on the Emu Edit benchmark show it improves editing accuracy and consistency.", "AI": {"tldr": "DescriptiveEdit\u662f\u4e00\u4e2a\u57fa\u4e8e\u63cf\u8ff0\u6027\u63d0\u793a\u7684\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u5c06\u6307\u4ee4\u5f0f\u7f16\u8f91\u91cd\u65b0\u5b9a\u4e49\u4e3a\u57fa\u4e8e\u53c2\u8003\u56fe\u50cf\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff0c\u907f\u514d\u4e86\u91cd\u5efa\u8bef\u5dee\u548c\u6570\u636e\u96c6\u9650\u5236\u95ee\u9898", "motivation": "\u73b0\u6709\u7684\u8bed\u4e49\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5b58\u5728\u91cd\u5efa\u8bef\u5dee\uff08\u57fa\u4e8e\u53cd\u8f6c\u7684\u65b9\u6cd5\uff09\u548c\u6570\u636e\u96c6\u8d28\u91cf\u9650\u5236\uff08\u57fa\u4e8e\u6307\u4ee4\u7684\u65b9\u6cd5\uff09\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u53c8\u4e0d\u53d7\u6570\u636e\u96c6\u9650\u5236\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faCross-Attentive UNet\u67b6\u6784\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u6865\u63a5\u673a\u5236\u5c06\u53c2\u8003\u56fe\u50cf\u7279\u5f81\u6ce8\u5165\u5230\u63d0\u793a\u8bcd\u5230\u7f16\u8f91\u56fe\u50cf\u7684\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u4fdd\u6301\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b", "result": "\u5728Emu Edit\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u7f16\u8f91\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u7684\u63d0\u5347\uff0c\u80fd\u591f\u65e0\u7f1d\u96c6\u6210ControlNet\u3001IP-Adapter\u7b49\u6269\u5c55", "conclusion": "DescriptiveEdit\u6846\u67b6\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49\u95ee\u9898\u8303\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u4e49\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u96c6\u6210\u80fd\u529b"}}
{"id": "2508.20526", "pdf": "https://arxiv.org/pdf/2508.20526", "abs": "https://arxiv.org/abs/2508.20526", "authors": ["Matthieu Gendrin", "St\u00e9phane Pateux", "Xiaoran Jiang", "Th\u00e9o Ladune", "Luce Morin"], "title": "Adam SLAM - the last mile of camera calibration with 3DGS", "categories": ["cs.CV"], "comment": null, "summary": "The quality of the camera calibration is of major importance for evaluating progresses in novel view synthesis, as a 1-pixel error on the calibration has a significant impact on the reconstruction quality. While there is no ground truth for real scenes, the quality of the calibration is assessed by the quality of the novel view synthesis. This paper proposes to use a 3DGS model to fine tune calibration by backpropagation of novel view color loss with respect to the cameras parameters. The new calibration alone brings an average improvement of 0.4 dB PSNR on the dataset used as reference by 3DGS. The fine tuning may be long and its suitability depends on the criticity of training time, but for calibration of reference scenes, such as Mip-NeRF 360, the stake of novel view quality is the most important.", "AI": {"tldr": "\u4f7f\u75283DGS\u6a21\u578b\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u65b0\u89c6\u89d2\u989c\u8272\u635f\u5931\u6765\u5fae\u8c03\u76f8\u673a\u6807\u5b9a\uff0c\u5728\u53c2\u8003\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u53470.4 dB PSNR", "motivation": "\u76f8\u673a\u6807\u5b9a\u8d28\u91cf\u5bf9\u65b0\u9896\u89c6\u56fe\u5408\u6210\u81f3\u5173\u91cd\u8981\uff0c1\u50cf\u7d20\u7684\u6807\u5b9a\u8bef\u5dee\u4f1a\u5bf9\u91cd\u5efa\u8d28\u91cf\u4ea7\u751f\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u771f\u5b9e\u573a\u666f\u7f3a\u4e4f\u5730\u9762\u771f\u503c", "method": "\u5229\u75283DGS\u6a21\u578b\uff0c\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u65b0\u89c6\u89d2\u989c\u8272\u635f\u5931\u6765\u4f18\u5316\u76f8\u673a\u53c2\u6570\uff0c\u5b9e\u73b0\u6807\u5b9a\u5fae\u8c03", "result": "\u57283DGS\u4f7f\u7528\u7684\u53c2\u8003\u6570\u636e\u96c6\u4e0a\uff0c\u65b0\u6807\u5b9a\u65b9\u6cd5\u5e73\u5747\u5e26\u67650.4 dB PSNR\u7684\u63d0\u5347", "conclusion": "\u867d\u7136\u5fae\u8c03\u8fc7\u7a0b\u53ef\u80fd\u8017\u65f6\uff0c\u4f46\u5bf9\u4e8e\u53c2\u8003\u573a\u666f\uff08\u5982Mip-NeRF 360\uff09\u7684\u6807\u5b9a\uff0c\u65b0\u89c6\u89d2\u8d28\u91cf\u662f\u6700\u91cd\u8981\u7684\u8003\u91cf\u56e0\u7d20"}}
{"id": "2508.20586", "pdf": "https://arxiv.org/pdf/2508.20586", "abs": "https://arxiv.org/abs/2508.20586", "authors": ["Zheng Chong", "Yanwei Lei", "Shiyue Zhang", "Zhuandi He", "Zhen Wang", "Xujie Zhang", "Xiao Dong", "Yiling Wu", "Dongmei Jiang", "Xiaodan Liang"], "title": "FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models", "categories": ["cs.CV", "68T42 (Primary) 168T45 (Secondary)", "I.4.9"], "comment": "16 pages, 10 figures, 5 tables", "summary": "Despite its great potential, virtual try-on technology is hindered from real-world application by two major challenges: the inability of current methods to support multi-reference outfit compositions (including garments and accessories), and their significant inefficiency caused by the redundant re-computation of reference features in each denoising step. To address these challenges, we propose FastFit, a high-speed multi-reference virtual try-on framework based on a novel cacheable diffusion architecture. By employing a Semi-Attention mechanism and substituting traditional timestep embeddings with class embeddings for reference items, our model fully decouples reference feature encoding from the denoising process with negligible parameter overhead. This allows reference features to be computed only once and losslessly reused across all steps, fundamentally breaking the efficiency bottleneck and achieving an average 3.5x speedup over comparable methods. Furthermore, to facilitate research on complex, multi-reference virtual try-on, we introduce DressCode-MR, a new large-scale dataset. It comprises 28,179 sets of high-quality, paired images covering five key categories (tops, bottoms, dresses, shoes, and bags), constructed through a pipeline of expert models and human feedback refinement. Extensive experiments on the VITON-HD, DressCode, and our DressCode-MR datasets show that FastFit surpasses state-of-the-art methods on key fidelity metrics while offering its significant advantage in inference efficiency.", "AI": {"tldr": "FastFit\u662f\u4e00\u4e2a\u57fa\u4e8e\u53ef\u7f13\u5b58\u6269\u6563\u67b6\u6784\u7684\u9ad8\u901f\u591a\u53c2\u8003\u865a\u62df\u8bd5\u7a7f\u6846\u67b6\uff0c\u901a\u8fc7\u534a\u6ce8\u610f\u529b\u673a\u5236\u548c\u7c7b\u522b\u5d4c\u5165\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u53c2\u8003\u7279\u5f81\u7684\u4e00\u6b21\u8ba1\u7b97\u591a\u6b21\u590d\u7528\uff0c\u5e73\u5747\u52a0\u901f3.5\u500d\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u4fdd\u771f\u5ea6\u6307\u6807\u3002", "motivation": "\u5f53\u524d\u865a\u62df\u8bd5\u7a7f\u6280\u672f\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u65e0\u6cd5\u652f\u6301\u591a\u53c2\u8003\u670d\u88c5\u7ec4\u5408\uff08\u5305\u62ec\u670d\u88c5\u548c\u914d\u9970\uff09\uff0c\u4ee5\u53ca\u7531\u4e8e\u5728\u6bcf\u4e2a\u53bb\u566a\u6b65\u9aa4\u4e2d\u91cd\u590d\u8ba1\u7b97\u53c2\u8003\u7279\u5f81\u5bfc\u81f4\u7684\u663e\u8457\u4f4e\u6548\u7387\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u53ef\u7f13\u5b58\u6269\u6563\u67b6\u6784\u7684FastFit\u6846\u67b6\uff0c\u91c7\u7528\u534a\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u7c7b\u522b\u5d4c\u5165\u66ff\u4ee3\u4f20\u7edf\u65f6\u95f4\u6b65\u5d4c\u5165\uff0c\u5b9e\u73b0\u53c2\u8003\u7279\u5f81\u7f16\u7801\u4e0e\u53bb\u566a\u8fc7\u7a0b\u7684\u5b8c\u5168\u89e3\u8026\uff0c\u53ea\u9700\u8ba1\u7b97\u4e00\u6b21\u53c2\u8003\u7279\u5f81\u5373\u53ef\u5728\u6240\u6709\u6b65\u9aa4\u4e2d\u65e0\u635f\u590d\u7528\u3002", "result": "\u5728VITON-HD\u3001DressCode\u548c\u65b0\u5efa\u7684DressCode-MR\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFastFit\u5728\u5173\u952e\u4fdd\u771f\u5ea6\u6307\u6807\u4e0a\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b\u663e\u8457\u7684\u63a8\u7406\u6548\u7387\u4f18\u52bf\uff0c\u5e73\u5747\u52a0\u901f3.5\u500d\u3002", "conclusion": "FastFit\u6210\u529f\u89e3\u51b3\u4e86\u591a\u53c2\u8003\u865a\u62df\u8bd5\u7a7f\u7684\u6548\u7387\u548c\u529f\u80fd\u9650\u5236\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u591a\u53c2\u8003\u865a\u62df\u8bd5\u7a7f\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u65b9\u6848\u548c\u6570\u636e\u96c6\u652f\u6301\u3002"}}
{"id": "2508.20612", "pdf": "https://arxiv.org/pdf/2508.20612", "abs": "https://arxiv.org/abs/2508.20612", "authors": ["Aye Phyu Phyu Aung", "Lucas Lum", "Zhansen Shi", "Wen Qiu", "Bernice Zee", "JM Chin", "Yeow Kheng Lim", "J. Senthilnath"], "title": "Physics Informed Generative Models for Magnetic Field Images", "categories": ["cs.CV"], "comment": null, "summary": "In semiconductor manufacturing, defect detection and localization are critical to ensuring product quality and yield. While X-ray imaging is a reliable non-destructive testing method, it is memory-intensive and time-consuming for large-scale scanning, Magnetic Field Imaging (MFI) offers a more efficient means to localize regions of interest (ROI) for targeted X-ray scanning. However, the limited availability of MFI datasets due to proprietary concerns presents a significant bottleneck for training machine learning (ML) models using MFI. To address this challenge, we consider an ML-driven approach leveraging diffusion models with two physical constraints. We propose Physics Informed Generative Models for Magnetic Field Images (PI-GenMFI) to generate synthetic MFI samples by integrating specific physical information. We generate MFI images for the most common defect types: power shorts. These synthetic images will serve as training data for ML algorithms designed to localize defect areas efficiently. To evaluate generated MFIs, we compare our model to SOTA generative models from both variational autoencoder (VAE) and diffusion methods. We present a domain expert evaluation to assess the generated samples. In addition, we present qualitative and quantitative evaluation using various metrics used for image generation and signal processing, showing promising results to optimize the defect localization process.", "AI": {"tldr": "\u63d0\u51faPI-GenMFI\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7ed3\u5408\u7269\u7406\u7ea6\u675f\u751f\u6210\u5408\u6210\u78c1\u5834\u56fe\u50cf\uff0c\u89e3\u51b3\u534a\u5bfc\u4f53\u5236\u9020\u4e2dMFI\u6570\u636e\u96c6\u7a00\u7f3a\u95ee\u9898\uff0c\u7528\u4e8e\u7f3a\u9677\u5b9a\u4f4d\u8bad\u7ec3\u3002", "motivation": "\u534a\u5bfc\u4f53\u5236\u9020\u4e2dX\u5c04\u7ebf\u68c0\u6d4b\u5185\u5b58\u5bc6\u96c6\u4e14\u8017\u65f6\uff0cMFI\u80fd\u9ad8\u6548\u5b9a\u4f4d\u611f\u5174\u8da3\u533a\u57df\uff0c\u4f46MFI\u6570\u636e\u96c6\u56e0\u4e13\u6709\u95ee\u9898\u7a00\u7f3a\uff0c\u9650\u5236\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u7269\u7406\u4fe1\u606f\u751f\u6210\u6a21\u578bPI-GenMFI\uff0c\u96c6\u6210\u7279\u5b9a\u7269\u7406\u4fe1\u606f\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u5408\u6210MFI\u6837\u672c\uff0c\u91cd\u70b9\u5173\u6ce8\u7535\u6e90\u77ed\u8def\u7b49\u5e38\u89c1\u7f3a\u9677\u7c7b\u578b\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u7684VAE\u548c\u6269\u6563\u751f\u6210\u6a21\u578b\u6bd4\u8f83\uff0c\u901a\u8fc7\u9886\u57df\u4e13\u5bb6\u8bc4\u4f30\u548c\u591a\u79cd\u56fe\u50cf\u751f\u6210\u3001\u4fe1\u53f7\u5904\u7406\u6307\u6807\u8fdb\u884c\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\uff0c\u663e\u793a\u51fa\u4f18\u5316\u7f3a\u9677\u5b9a\u4f4d\u8fc7\u7a0b\u7684\u6f5c\u529b\u3002", "conclusion": "PI-GenMFI\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u751f\u6210\u5408\u6210MFI\u56fe\u50cf\uff0c\u4e3a\u89e3\u51b3MFI\u6570\u636e\u96c6\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u6709\u671b\u63d0\u5347\u534a\u5bfc\u4f53\u7f3a\u9677\u68c0\u6d4b\u6548\u7387\u3002"}}
{"id": "2508.20613", "pdf": "https://arxiv.org/pdf/2508.20613", "abs": "https://arxiv.org/abs/2508.20613", "authors": ["Yixiang Qiu", "Yanhan Liu", "Hongyao Yu", "Hao Fang", "Bin Chen", "Shu-Tao Xia", "Ke Xu"], "title": "Revisiting the Privacy Risks of Split Inference: A GAN-Based Data Reconstruction Attack via Progressive Feature Optimization", "categories": ["cs.CV", "cs.CR"], "comment": "10 pages, 5 figures", "summary": "The growing complexity of Deep Neural Networks (DNNs) has led to the adoption of Split Inference (SI), a collaborative paradigm that partitions computation between edge devices and the cloud to reduce latency and protect user privacy. However, recent advances in Data Reconstruction Attacks (DRAs) reveal that intermediate features exchanged in SI can be exploited to recover sensitive input data, posing significant privacy risks. Existing DRAs are typically effective only on shallow models and fail to fully leverage semantic priors, limiting their reconstruction quality and generalizability across datasets and model architectures. In this paper, we propose a novel GAN-based DRA framework with Progressive Feature Optimization (PFO), which decomposes the generator into hierarchical blocks and incrementally refines intermediate representations to enhance the semantic fidelity of reconstructed images. To stabilize the optimization and improve image realism, we introduce an L1-ball constraint during reconstruction. Extensive experiments show that our method outperforms prior attacks by a large margin, especially in high-resolution scenarios, out-of-distribution settings, and against deeper and more complex DNNs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eGAN\u7684\u8fdb\u9636\u7279\u5f81\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u5206\u5f00\u63a8\u7406\u73af\u5883\u4e2d\u66f4\u6709\u6548\u5730\u6062\u590d\u654f\u611f\u8f93\u5165\u6570\u636e\uff0c\u5728\u9ad8\u5206\u8fa8\u7387\u3001\u975e\u540c\u5206\u5e03\u548c\u6df1\u5c42\u6a21\u578b\u573a\u666f\u4e0b\u90fd\u663e\u8457\u8d85\u8fc7\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u5206\u5f00\u63a8\u7406\u6a21\u5f0f\u88ab\u5e7f\u6cdb\u91c7\u7528\u4ee5\u964d\u4f4e\u5ef6\u8fdf\u548c\u4fdd\u62a4\u9690\u79c1\u3002\u4f46\u4e2d\u95f4\u7279\u5f81\u53ef\u88ab\u5229\u7528\u6765\u6062\u590d\u654f\u611f\u6570\u636e\uff0c\u73b0\u6709\u7684\u6570\u636e\u91cd\u6784\u653b\u51fb\u65b9\u6cd5\u901a\u5e38\u53ea\u5728\u6d45\u5c42\u6a21\u578b\u4e0a\u6709\u6548\uff0c\u4e14\u6ca1\u6709\u5145\u5206\u5229\u7528\u8bed\u4e49\u5148\u9a8c\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684GAN\u57fa\u4e8e\u7684\u6570\u636e\u91cd\u6784\u653b\u51fb\u6846\u67b6\uff0c\u91c7\u7528\u8fdb\u9636\u5f0f\u7279\u5f81\u4f18\u5316(PFO)\u6280\u672f\u3002\u5c06\u751f\u6210\u5668\u5206\u89e3\u4e3a\u5c42\u6b21\u5757\uff0c\u9010\u6b65\u7cbe\u70bc\u4e2d\u95f4\u8868\u5f81\u4ee5\u63d0\u5347\u91cd\u6784\u56fe\u50cf\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\uff0c\u5e76\u5728\u91cd\u6784\u8fc7\u7a0b\u4e2d\u5f15\u5165L1\u7403\u7ea6\u675f\u6765\u7a33\u5b9a\u4f18\u5316\u548c\u63d0\u9ad8\u56fe\u50cf\u771f\u5b9e\u6027\u3002", "result": "\u7ecf\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u9ad8\u5206\u8fa8\u7387\u573a\u666f\u3001\u975e\u540c\u5206\u5e03\u8bbe\u7f6e\u4ee5\u53ca\u9762\u5bf9\u66f4\u6df1\u5c42\u548c\u66f4\u590d\u6742\u7684DNN\u6a21\u578b\u65f6\uff0c\u90fd\u663e\u8457\u8d85\u8fc7\u4e86\u4e4b\u524d\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684GAN\u57fa\u4e8ePFO\u7684\u6570\u636e\u91cd\u6784\u653b\u51fb\u6846\u67b6\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u5229\u7528\u4e2d\u95f4\u7279\u5f81\u6062\u590d\u654f\u611f\u8f93\u5165\u6570\u636e\uff0c\u5728\u591a\u79cd\u590d\u6742\u573a\u666f\u4e0b\u90fd\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u663e\u793a\u4e86\u5206\u5f00\u63a8\u7406\u6a21\u5f0f\u4e2d\u5b58\u5728\u7684\u91cd\u5927\u9690\u79c1\u98ce\u9669\u3002"}}
{"id": "2508.20623", "pdf": "https://arxiv.org/pdf/2508.20623", "abs": "https://arxiv.org/abs/2508.20623", "authors": ["Shiqi Xin", "Xiaolin Zhang", "Yanbin Liu", "Peng Zhang", "Caifeng Shan"], "title": "AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View Images", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in Gaussian Splatting have significantly boosted the reconstruction of head avatars, enabling high-quality facial modeling by representing an 3D avatar as a collection of 3D Gaussians. However, existing methods predominantly rely on frontal-view images, leaving the back-head poorly constructed. This leads to geometric inconsistencies, structural blurring, and reduced realism in the rear regions, ultimately limiting the fidelity of reconstructed avatars. To address this challenge, we propose AvatarBack, a novel plug-and-play framework specifically designed to reconstruct complete and consistent 3D Gaussian avatars by explicitly modeling the missing back-head regions. AvatarBack integrates two core technical innovations,i.e., the Subject-specific Generator (SSG) and the Adaptive Spatial Alignment Strategy (ASA). The former leverages a generative prior to synthesize identity-consistent, plausible back-view pseudo-images from sparse frontal inputs, providing robust multi-view supervision. To achieve precise geometric alignment between these synthetic views and the 3D Gaussian representation, the later employs learnable transformation matrices optimized during training, effectively resolving inherent pose and coordinate discrepancies. Extensive experiments on NeRSemble and K-hairstyle datasets, evaluated using geometric, photometric, and GPT-4o-based perceptual metrics, demonstrate that AvatarBack significantly enhances back-head reconstruction quality while preserving frontal fidelity. Moreover, the reconstructed avatars maintain consistent visual realism under diverse motions and remain fully animatable.", "AI": {"tldr": "AvatarBack\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u8eab\u4efd\u4e00\u81f4\u7684\u540e\u89c6\u56fe\u4f2a\u56fe\u50cf\u548c\u5b66\u4e60\u6027\u7a7a\u95f4\u5bf9\u9f50\u7b56\u7565\uff0c\u89e3\u51b3\u4e863D\u9ad8\u65af\u5934\u50cf\u91cd\u5efa\u4e2d\u540e\u8111\u533a\u57df\u7f3a\u5931\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b8c\u6574\u5934\u50cf\u7684\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u9ad8\u65af\u6cfc\u6e85\u5934\u50cf\u91cd\u5efa\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6b63\u9762\u89c6\u56fe\u56fe\u50cf\uff0c\u5bfc\u81f4\u540e\u8111\u533a\u57df\u91cd\u5efa\u8d28\u91cf\u5dee\uff0c\u5b58\u5728\u51e0\u4f55\u4e0d\u4e00\u81f4\u3001\u7ed3\u6784\u6a21\u7cca\u548c\u771f\u5b9e\u611f\u964d\u4f4e\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u91cd\u5efa\u5934\u50cf\u7684\u4fdd\u771f\u5ea6\u3002", "method": "\u63d0\u51faAvatarBack\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6280\u672f\uff1a1\uff09\u4e3b\u4f53\u7279\u5b9a\u751f\u6210\u5668\uff08SSG\uff09\u5229\u7528\u751f\u6210\u5148\u9a8c\u4ece\u7a00\u758f\u6b63\u9762\u8f93\u5165\u5408\u6210\u8eab\u4efd\u4e00\u81f4\u7684\u540e\u89c6\u56fe\u4f2a\u56fe\u50cf\uff1b2\uff09\u81ea\u9002\u5e94\u7a7a\u95f4\u5bf9\u9f50\u7b56\u7565\uff08ASA\uff09\u901a\u8fc7\u5b66\u4e60\u6027\u53d8\u6362\u77e9\u9635\u4f18\u5316\u5408\u6210\u89c6\u56fe\u4e0e3D\u9ad8\u65af\u8868\u793a\u4e4b\u95f4\u7684\u51e0\u4f55\u5bf9\u9f50\u3002", "result": "\u5728NeRSemble\u548cK-hairstyle\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAvatarBack\u5728\u4fdd\u6301\u6b63\u9762\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u540e\u8111\u91cd\u5efa\u8d28\u91cf\uff0c\u91cd\u5efa\u7684\u5934\u50cf\u5728\u4e0d\u540c\u8fd0\u52a8\u4e0b\u4fdd\u6301\u4e00\u81f4\u7684\u89c6\u89c9\u771f\u5b9e\u611f\u4e14\u5b8c\u5168\u53ef\u52a8\u753b\u5316\u3002", "conclusion": "AvatarBack\u6709\u6548\u89e3\u51b3\u4e863D\u9ad8\u65af\u5934\u50cf\u91cd\u5efa\u4e2d\u540e\u8111\u533a\u57df\u7f3a\u5931\u7684\u95ee\u9898\uff0c\u4e3a\u5b8c\u6574\u4e14\u4e00\u81f4\u76843D\u5934\u50cf\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u51e0\u4f55\u3001\u5149\u5ea6\u548c\u611f\u77e5\u6307\u6807\u4e0a\u5747\u8868\u73b0\u51fa\u4f18\u5f02\u6027\u80fd\u3002"}}
{"id": "2508.20640", "pdf": "https://arxiv.org/pdf/2508.20640", "abs": "https://arxiv.org/abs/2508.20640", "authors": ["Ayan Banerjee", "Fernando Vilari\u00f1o", "Josep Llad\u00f3s"], "title": "CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Preserving facial identity under extreme stylistic transformation remains a major challenge in generative art. In graffiti, a high-contrast, abstract medium, subtle distortions to the eyes, nose, or mouth can erase the subject's recognizability, undermining both personal and cultural authenticity. We present CraftGraffiti, an end-to-end text-guided graffiti generation framework designed with facial feature preservation as a primary objective. Given an input image and a style and pose descriptive prompt, CraftGraffiti first applies graffiti style transfer via LoRA-fine-tuned pretrained diffusion transformer, then enforces identity fidelity through a face-consistent self-attention mechanism that augments attention layers with explicit identity embeddings. Pose customization is achieved without keypoints, using CLIP-guided prompt extension to enable dynamic re-posing while retaining facial coherence. We formally justify and empirically validate the \"style-first, identity-after\" paradigm, showing it reduces attribute drift compared to the reverse order. Quantitative results demonstrate competitive facial feature consistency and state-of-the-art aesthetic and human preference scores, while qualitative analyses and a live deployment at the Cruilla Festival highlight the system's real-world creative impact. CraftGraffiti advances the goal of identity-respectful AI-assisted artistry, offering a principled approach for blending stylistic freedom with recognizability in creative AI applications.", "AI": {"tldr": "CraftGraffiti\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6587\u672c\u5f15\u5bfc\u6d82\u9e26\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7LoRA\u5fae\u8c03\u6269\u6563\u53d8\u6362\u5668\u548c\u9762\u90e8\u4e00\u81f4\u6027\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u6781\u7aef\u98ce\u683c\u8f6c\u6362\u4e2d\u4fdd\u6301\u9762\u90e8\u8eab\u4efd\u8bc6\u522b\u6027\u3002", "motivation": "\u89e3\u51b3\u6d82\u9e26\u827a\u672f\u4e2d\u6781\u7aef\u98ce\u683c\u8f6c\u6362\u5bfc\u81f4\u9762\u90e8\u8eab\u4efd\u8bc6\u522b\u6027\u4e27\u5931\u7684\u95ee\u9898\uff0c\u4fdd\u6301\u4e2a\u4eba\u548c\u6587\u5316\u771f\u5b9e\u6027\u3002", "method": "\u91c7\u7528\"\u98ce\u683c\u4f18\u5148\u3001\u8eab\u4efd\u540e\u5904\u7406\"\u8303\u5f0f\uff1a\u9996\u5148\u901a\u8fc7LoRA\u5fae\u8c03\u7684\u9884\u8bad\u7ec3\u6269\u6563\u53d8\u6362\u5668\u8fdb\u884c\u6d82\u9e26\u98ce\u683c\u8fc1\u79fb\uff0c\u7136\u540e\u901a\u8fc7\u9762\u90e8\u4e00\u81f4\u6027\u81ea\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u8eab\u4efd\u5d4c\u5165\u6765\u4fdd\u6301\u8eab\u4efd\u4fdd\u771f\u5ea6\uff0c\u4f7f\u7528CLIP\u5f15\u5bfc\u7684\u63d0\u793a\u6269\u5c55\u5b9e\u73b0\u65e0\u5173\u952e\u70b9\u7684\u59ff\u6001\u5b9a\u5236\u3002", "result": "\u5728\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u7684\u9762\u90e8\u7279\u5f81\u4e00\u81f4\u6027\uff0c\u83b7\u5f97\u6700\u5148\u8fdb\u7684\u5ba1\u7f8e\u548c\u4eba\u7c7b\u504f\u597d\u8bc4\u5206\uff0c\u5b9a\u6027\u5206\u6790\u548c\u5728Cruilla\u97f3\u4e50\u8282\u7684\u73b0\u573a\u90e8\u7f72\u5c55\u793a\u4e86\u7cfb\u7edf\u7684\u5b9e\u9645\u521b\u610f\u5f71\u54cd\u3002", "conclusion": "CraftGraffiti\u63a8\u8fdb\u4e86\u8eab\u4efd\u5c0a\u91cd\u7684AI\u8f85\u52a9\u827a\u672f\u76ee\u6807\uff0c\u4e3a\u521b\u610fAI\u5e94\u7528\u4e2d\u878d\u5408\u98ce\u683c\u81ea\u7531\u548c\u53ef\u8bc6\u522b\u6027\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\u3002"}}
{"id": "2508.20670", "pdf": "https://arxiv.org/pdf/2508.20670", "abs": "https://arxiv.org/abs/2508.20670", "authors": ["Anastasios Skoularikis", "Stefanos-Iordanis Papadopoulos", "Symeon Papadopoulos", "Panagiotis C. Petrantonakis"], "title": "\"Humor, Art, or Misinformation?\": A Multimodal Dataset for Intent-Aware Synthetic Image Detection", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Recent advances in multimodal AI have enabled progress in detecting synthetic and out-of-context content. However, existing efforts largely overlook the intent behind AI-generated images. To fill this gap, we introduce S-HArM, a multimodal dataset for intent-aware classification, comprising 9,576 \"in the wild\" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art, or Misinformation. Additionally, we explore three prompting strategies (image-guided, description-guided, and multimodally-guided) to construct a large-scale synthetic training dataset with Stable Diffusion. We conduct an extensive comparative study including modality fusion, contrastive learning, reconstruction networks, attention mechanisms, and large vision-language models. Our results show that models trained on image- and multimodally-guided data generalize better to \"in the wild\" content, due to preserved visual context. However, overall performance remains limited, highlighting the complexity of inferring intent and the need for specialized architectures.", "AI": {"tldr": "S-HArM\u6570\u636e\u96c6\u7528\u4e8e\u610f\u56fe\u611f\u77e5\u5206\u7c7b\uff0c\u5305\u542b9,576\u4e2a\u793e\u4ea4\u5a92\u4f53\u56fe\u50cf-\u6587\u672c\u5bf9\uff0c\u6807\u6ce8\u4e3a\u5e7d\u9ed8/\u8bbd\u523a\u3001\u827a\u672f\u6216\u865a\u5047\u4fe1\u606f\u3002\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u63d0\u793a\u7b56\u7565\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u53d1\u73b0\u56fe\u50cf\u548c\u591a\u6a21\u6001\u5f15\u5bfc\u7684\u6570\u636e\u5728\u771f\u5b9e\u5185\u5bb9\u4e0a\u6cdb\u5316\u66f4\u597d\uff0c\u4f46\u6574\u4f53\u6027\u80fd\u6709\u9650\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001AI\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u68c0\u6d4b\u5408\u6210\u548c\u8131\u8282\u5185\u5bb9\uff0c\u4f46\u5ffd\u89c6\u4e86AI\u751f\u6210\u56fe\u50cf\u80cc\u540e\u7684\u610f\u56fe\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u9700\u8981\u4e13\u95e8\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u6765\u8bc6\u522b\u5185\u5bb9\u521b\u4f5c\u610f\u56fe\u3002", "method": "\u6784\u5efaS-HArM\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u4e09\u79cd\u63d0\u793a\u7b56\u7565\uff08\u56fe\u50cf\u5f15\u5bfc\u3001\u63cf\u8ff0\u5f15\u5bfc\u3001\u591a\u6a21\u6001\u5f15\u5bfc\uff09\u901a\u8fc7Stable Diffusion\u751f\u6210\u5927\u89c4\u6a21\u5408\u6210\u8bad\u7ec3\u6570\u636e\u3002\u6bd4\u8f83\u4e86\u6a21\u6001\u878d\u5408\u3001\u5bf9\u6bd4\u5b66\u4e60\u3001\u91cd\u5efa\u7f51\u7edc\u3001\u6ce8\u610f\u529b\u673a\u5236\u548c\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7b49\u591a\u79cd\u65b9\u6cd5\u3002", "result": "\u4f7f\u7528\u56fe\u50cf\u5f15\u5bfc\u548c\u591a\u6a21\u6001\u5f15\u5bfc\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u771f\u5b9e\u5185\u5bb9\u4e0a\u6cdb\u5316\u6548\u679c\u66f4\u597d\uff0c\u56e0\u4e3a\u4fdd\u7559\u4e86\u89c6\u89c9\u4e0a\u4e0b\u6587\u3002\u4f46\u6574\u4f53\u5206\u7c7b\u6027\u80fd\u4ecd\u7136\u6709\u9650\uff0c\u8868\u660e\u63a8\u65ad\u610f\u56fe\u7684\u590d\u6742\u6027\u3002", "conclusion": "\u63a8\u65adAI\u751f\u6210\u5185\u5bb9\u7684\u610f\u56fe\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u4e13\u95e8\u7684\u67b6\u6784\u6765\u6539\u8fdb\u6027\u80fd\u3002\u56fe\u50cf\u548c\u591a\u6a21\u6001\u5f15\u5bfc\u7684\u65b9\u6cd5\u663e\u793a\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u8be5\u9886\u57df\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2508.20734", "pdf": "https://arxiv.org/pdf/2508.20734", "abs": "https://arxiv.org/abs/2508.20734", "authors": ["Reza Akbari Movahed", "Abuzar Rezaee", "Arezoo Zakeri", "Colin Berry", "Edmond S. L. Ho", "Ali Gooya"], "title": "CardioMorphNet: Cardiac Motion Prediction Using a Shape-Guided Bayesian Recurrent Deep Network", "categories": ["cs.CV"], "comment": null, "summary": "Accurate cardiac motion estimation from cine cardiac magnetic resonance (CMR) images is vital for assessing cardiac function and detecting its abnormalities. Existing methods often struggle to capture heart motion accurately because they rely on intensity-based image registration similarity losses that may overlook cardiac anatomical regions. To address this, we propose CardioMorphNet, a recurrent Bayesian deep learning framework for 3D cardiac shape-guided deformable registration using short-axis (SAX) CMR images. It employs a recurrent variational autoencoder to model spatio-temporal dependencies over the cardiac cycle and two posterior models for bi-ventricular segmentation and motion estimation. The derived loss function from the Bayesian formulation guides the framework to focus on anatomical regions by recursively registering segmentation maps without using intensity-based image registration similarity loss, while leveraging sequential SAX volumes and spatio-temporal features. The Bayesian modelling also enables computation of uncertainty maps for the estimated motion fields. Validated on the UK Biobank dataset by comparing warped mask shapes with ground truth masks, CardioMorphNet demonstrates superior performance in cardiac motion estimation, outperforming state-of-the-art methods. Uncertainty assessment shows that it also yields lower uncertainty values for estimated motion fields in the cardiac region compared with other probabilistic-based cardiac registration methods, indicating higher confidence in its predictions.", "AI": {"tldr": "CardioMorphNet\u662f\u4e00\u4e2a\u57fa\u4e8e\u8d1d\u53f6\u65af\u6df1\u5ea6\u5b66\u4e60\u76843D\u5fc3\u810f\u5f62\u72b6\u5f15\u5bfc\u53d8\u5f62\u914d\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u5faa\u73af\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5efa\u6a21\u5fc3\u810f\u5468\u671f\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e0d\u4f7f\u7528\u57fa\u4e8e\u5f3a\u5ea6\u7684\u56fe\u50cf\u914d\u51c6\u76f8\u4f3c\u6027\u635f\u5931\uff0c\u5728\u5fc3\u810f\u8fd0\u52a8\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u57fa\u4e8e\u5f3a\u5ea6\u7684\u56fe\u50cf\u914d\u51c6\u76f8\u4f3c\u6027\u635f\u5931\uff0c\u53ef\u80fd\u5ffd\u7565\u5fc3\u810f\u89e3\u5256\u533a\u57df\uff0c\u5bfc\u81f4\u5fc3\u810f\u8fd0\u52a8\u4f30\u8ba1\u4e0d\u51c6\u786e\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4e13\u6ce8\u4e8e\u89e3\u5256\u533a\u57df\u7684\u5fc3\u810f\u8fd0\u52a8\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5faa\u73af\u8d1d\u53f6\u65af\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6CardioMorphNet\uff0c\u4f7f\u7528\u5faa\u73af\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5efa\u6a21\u65f6\u7a7a\u4f9d\u8d56\uff0c\u901a\u8fc7\u4e24\u4e2a\u540e\u9a8c\u6a21\u578b\u8fdb\u884c\u53cc\u5fc3\u5ba4\u5206\u5272\u548c\u8fd0\u52a8\u4f30\u8ba1\uff0c\u5229\u7528\u5206\u5272\u56fe\u9012\u5f52\u914d\u51c6\u800c\u4e0d\u4f7f\u7528\u5f3a\u5ea6\u76f8\u4f3c\u6027\u635f\u5931\u3002", "result": "\u5728UK Biobank\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cCardioMorphNet\u5728\u5fc3\u810f\u8fd0\u52a8\u4f30\u8ba1\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e14\u76f8\u6bd4\u5176\u4ed6\u6982\u7387\u6027\u5fc3\u810f\u914d\u51c6\u65b9\u6cd5\uff0c\u5728\u5fc3\u810f\u533a\u57df\u4ea7\u751f\u66f4\u4f4e\u7684\u4e0d\u786e\u5b9a\u6027\u503c\uff0c\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u66f4\u9ad8\u3002", "conclusion": "CardioMorphNet\u901a\u8fc7\u5f62\u72b6\u5f15\u5bfc\u7684\u8d1d\u53f6\u65af\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5fc3\u810f\u8fd0\u52a8\u4f30\u8ba1\u4e2d\u5ffd\u7565\u89e3\u5256\u533a\u57df\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u548c\u53ef\u9760\u7684\u5fc3\u810f\u529f\u80fd\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2508.20751", "pdf": "https://arxiv.org/pdf/2508.20751", "abs": "https://arxiv.org/abs/2508.20751", "authors": ["Yibin Wang", "Zhimin Li", "Yuhang Zang", "Yujie Zhou", "Jiazi Bu", "Chunyu Wang", "Qinglin Lu", "Cheng Jin", "Jiaqi Wang"], "title": "Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning", "categories": ["cs.CV"], "comment": "Project Page: https://codegoat24.github.io/UnifiedReward/Pref-GRPO", "summary": "Recent advancements highlight the importance of GRPO-based reinforcement learning methods and benchmarking in enhancing text-to-image (T2I) generation. However, current methods using pointwise reward models (RM) for scoring generated images are susceptible to reward hacking. We reveal that this happens when minimal score differences between images are amplified after normalization, creating illusory advantages that drive the model to over-optimize for trivial gains, ultimately destabilizing the image generation process. To address this, we propose Pref-GRPO, a pairwise preference reward-based GRPO method that shifts the optimization objective from score maximization to preference fitting, ensuring more stable training. In Pref-GRPO, images are pairwise compared within each group using preference RM, and the win rate is used as the reward signal. Extensive experiments demonstrate that PREF-GRPO differentiates subtle image quality differences, providing more stable advantages and mitigating reward hacking. Additionally, existing T2I benchmarks are limited by coarse evaluation criteria, hindering comprehensive model assessment. To solve this, we introduce UniGenBench, a unified T2I benchmark comprising 600 prompts across 5 main themes and 20 subthemes. It evaluates semantic consistency through 10 primary and 27 sub-criteria, leveraging MLLM for benchmark construction and evaluation. Our benchmarks uncover the strengths and weaknesses of both open and closed-source T2I models and validate the effectiveness of Pref-GRPO.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Pref-GRPO\u65b9\u6cd5\u6765\u89e3\u51b3T2I\u751f\u6210\u4e2d\u7684\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0c\u901a\u8fc7\u4ece\u5206\u6570\u6700\u5927\u5316\u8f6c\u5411\u504f\u597d\u62df\u5408\u6765\u7a33\u5b9a\u8bad\u7ec3\uff0c\u5e76\u5f15\u5165\u4e86UniGenBench\u57fa\u51c6\u6d4b\u8bd5\u6765\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u70b9\u5f0f\u5956\u52b1\u6a21\u578b\u7684T2I\u751f\u6210\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\uff0c\u5373\u5fae\u5c0f\u5206\u6570\u5dee\u5f02\u5728\u5f52\u4e00\u5316\u540e\u88ab\u653e\u5927\uff0c\u5bfc\u81f4\u6a21\u578b\u8fc7\u5ea6\u4f18\u5316\u7410\u788e\u589e\u76ca\u800c\u7834\u574f\u751f\u6210\u7a33\u5b9a\u6027\u3002\u540c\u65f6\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u6807\u51c6\u7c97\u7cd9\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u3002", "method": "\u63d0\u51faPref-GRPO\u65b9\u6cd5\uff1a\u4f7f\u7528\u6210\u5bf9\u504f\u597d\u5956\u52b1\u6a21\u578b\uff0c\u5728\u6bcf\u7ec4\u56fe\u50cf\u4e2d\u8fdb\u884c\u6210\u5bf9\u6bd4\u8f83\uff0c\u4ee5\u80dc\u7387\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u3002\u5f15\u5165UniGenBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b600\u4e2a\u63d0\u793a\u8bcd\uff0c\u901a\u8fc710\u4e2a\u4e3b\u8981\u6807\u51c6\u548c27\u4e2a\u5b50\u6807\u51c6\u8bc4\u4f30\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u5229\u7528MLLM\u8fdb\u884c\u6784\u5efa\u548c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePref-GRPO\u80fd\u591f\u533a\u5206\u7ec6\u5fae\u7684\u56fe\u50cf\u8d28\u91cf\u5dee\u5f02\uff0c\u63d0\u4f9b\u66f4\u7a33\u5b9a\u7684\u4f18\u52bf\u5e76\u7f13\u89e3\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u3002UniGenBench\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u5f00\u6e90\u548c\u95ed\u6e90T2I\u6a21\u578b\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u9a8c\u8bc1\u4e86Pref-GRPO\u7684\u6709\u6548\u6027\u3002", "conclusion": "Pref-GRPO\u901a\u8fc7\u504f\u597d\u62df\u5408\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0cUniGenBench\u4e3aT2I\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4e24\u8005\u5171\u540c\u63a8\u52a8\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.20754", "pdf": "https://arxiv.org/pdf/2508.20754", "abs": "https://arxiv.org/abs/2508.20754", "authors": ["Yuxi Hu", "Jun Zhang", "Kuangyi Chen", "Zhe Zhang", "Friedrich Fraundorfer"], "title": "${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale Feature for Generalizable Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to The 36th British Machine Vision Conference (BMVC 2025),   Sheffield, UK", "summary": "Generalizable Gaussian Splatting aims to synthesize novel views for unseen scenes without per-scene optimization. In particular, recent advancements utilize feed-forward networks to predict per-pixel Gaussian parameters, enabling high-quality synthesis from sparse input views. However, existing approaches fall short in encoding discriminative, multi-view consistent features for Gaussian predictions, which struggle to construct accurate geometry with sparse views. To address this, we propose $\\mathbf{C}^{3}$-GS, a framework that enhances feature learning by incorporating context-aware, cross-dimension, and cross-scale constraints. Our architecture integrates three lightweight modules into a unified rendering pipeline, improving feature fusion and enabling photorealistic synthesis without requiring additional supervision. Extensive experiments on benchmark datasets validate that $\\mathbf{C}^{3}$-GS achieves state-of-the-art rendering quality and generalization ability. Code is available at: https://github.com/YuhsiHu/C3-GS.", "AI": {"tldr": "C3-GS\u662f\u4e00\u4e2a\u7528\u4e8e\u672a\u89c1\u573a\u666f\u65b0\u89c6\u89d2\u5408\u6210\u7684\u901a\u7528\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u3001\u8de8\u7ef4\u5ea6\u548c\u8de8\u5c3a\u5ea6\u7ea6\u675f\u589e\u5f3a\u7279\u5f81\u5b66\u4e60\uff0c\u65e0\u9700\u9010\u573a\u666f\u4f18\u5316\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u6e32\u67d3", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7f16\u7801\u5224\u522b\u6027\u3001\u591a\u89c6\u89d2\u4e00\u81f4\u7279\u5f81\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u4ece\u7a00\u758f\u8f93\u5165\u89c6\u56fe\u6784\u5efa\u51c6\u786e\u51e0\u4f55\u7ed3\u6784", "method": "\u63d0\u51faC3-GS\u6846\u67b6\uff0c\u96c6\u6210\u4e09\u4e2a\u8f7b\u91cf\u7ea7\u6a21\u5757\u5230\u7edf\u4e00\u6e32\u67d3\u6d41\u7a0b\u4e2d\uff0c\u5305\u542b\u4e0a\u4e0b\u6587\u611f\u77e5\u3001\u8de8\u7ef4\u5ea6\u548c\u8de8\u5c3a\u5ea6\u7ea6\u675f\uff0c\u6539\u8fdb\u7279\u5f81\u878d\u5408", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86C3-GS\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6e32\u67d3\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u903c\u771f\u7684\u5408\u6210\u6548\u679c\uff0c\u65e0\u9700\u989d\u5916\u76d1\u7763\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2508.20830", "pdf": "https://arxiv.org/pdf/2508.20830", "abs": "https://arxiv.org/abs/2508.20830", "authors": ["Krit Duangprom", "Tryphon Lambrou", "Binod Bhattarai"], "title": "Estimating 2D Keypoints of Surgical Tools Using Vision-Language Models with Low-Rank Adaptation", "categories": ["cs.CV"], "comment": "Accepted to MICCAI 2025", "summary": "This paper presents a novel pipeline for 2D keypoint estima- tion of surgical tools by leveraging Vision Language Models (VLMs) fine- tuned using a low rank adjusting (LoRA) technique. Unlike traditional Convolutional Neural Network (CNN) or Transformer-based approaches, which often suffer from overfitting in small-scale medical datasets, our method harnesses the generalization capabilities of pre-trained VLMs. We carefully design prompts to create an instruction-tuning dataset and use them to align visual features with semantic keypoint descriptions. Experimental results show that with only two epochs of fine tuning, the adapted VLM outperforms the baseline models, demonstrating the ef- fectiveness of LoRA in low-resource scenarios. This approach not only improves keypoint detection performance, but also paves the way for future work in 3D surgical hands and tools pose estimation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u548cLoRA\u5fae\u8c03\u6280\u672f\u7684\u5916\u79d1\u624b\u672f\u5de5\u51772D\u5173\u952e\u70b9\u4f30\u8ba1\u65b0\u65b9\u6cd5\uff0c\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u4ec5\u97002\u4e2aepoch\u5fae\u8c03\u5373\u53ef\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b", "motivation": "\u4f20\u7edfCNN\u548cTransformer\u65b9\u6cd5\u5728\u5c0f\u89c4\u6a21\u533b\u7597\u6570\u636e\u96c6\u4e0a\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u9700\u8981\u5229\u7528\u9884\u8bad\u7ec3VLMs\u7684\u6cdb\u5316\u80fd\u529b\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898", "method": "\u4f7f\u7528LoRA\u6280\u672f\u5fae\u8c03\u9884\u8bad\u7ec3VLMs\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u63d0\u793a\u8bcd\u521b\u5efa\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\uff0c\u5c06\u89c6\u89c9\u7279\u5f81\u4e0e\u8bed\u4e49\u5173\u952e\u70b9\u63cf\u8ff0\u5bf9\u9f50", "result": "\u4ec5\u97002\u4e2aepoch\u5fae\u8c03\uff0c\u9002\u5e94\u540e\u7684VLM\u5c31\u8d85\u8d8a\u4e86\u57fa\u7ebf\u6a21\u578b\uff0c\u8bc1\u660e\u4e86LoRA\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u5173\u952e\u70b9\u68c0\u6d4b\u6027\u80fd\uff0c\u8fd8\u4e3a\u672a\u67653D\u5916\u79d1\u624b\u672f\u624b\u548c\u5de5\u5177\u59ff\u6001\u4f30\u8ba1\u7814\u7a76\u94fa\u5e73\u4e86\u9053\u8def"}}
{"id": "2508.20881", "pdf": "https://arxiv.org/pdf/2508.20881", "abs": "https://arxiv.org/abs/2508.20881", "authors": ["Pushkar Shukla"], "title": "Understanding and evaluating computer vision models through the lens of counterfactuals", "categories": ["cs.CV"], "comment": null, "summary": "Counterfactual reasoning -- the practice of asking ``what if'' by varying inputs and observing changes in model behavior -- has become central to interpretable and fair AI. This thesis develops frameworks that use counterfactuals to explain, audit, and mitigate bias in vision classifiers and generative models. By systematically altering semantically meaningful attributes while holding others fixed, these methods uncover spurious correlations, probe causal dependencies, and help build more robust systems.   The first part addresses vision classifiers. CAVLI integrates attribution (LIME) with concept-level analysis (TCAV) to quantify how strongly decisions rely on human-interpretable concepts. With localized heatmaps and a Concept Dependency Score, CAVLI shows when models depend on irrelevant cues like backgrounds. Extending this, ASAC introduces adversarial counterfactuals that perturb protected attributes while preserving semantics. Through curriculum learning, ASAC fine-tunes biased models for improved fairness and accuracy while avoiding stereotype-laden artifacts.   The second part targets generative Text-to-Image (TTI) models. TIBET provides a scalable pipeline for evaluating prompt-sensitive biases by varying identity-related terms, enabling causal auditing of how race, gender, and age affect image generation. To capture interactions, BiasConnect builds causal graphs diagnosing intersectional biases. Finally, InterMit offers a modular, training-free algorithm that mitigates intersectional bias via causal sensitivity scores and user-defined fairness goals.   Together, these contributions show counterfactuals as a unifying lens for interpretability, fairness, and causality in both discriminative and generative models, establishing principled, scalable methods for socially responsible bias evaluation and mitigation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5f00\u53d1\u4e86\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u91ca\u3001\u5ba1\u8ba1\u548c\u51cf\u8f7b\u89c6\u89c9\u5206\u7c7b\u5668\u548c\u751f\u6210\u6a21\u578b\u4e2d\u7684\u504f\u89c1\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u6539\u53d8\u8bed\u4e49\u5c5e\u6027\u6765\u63ed\u793a\u865a\u5047\u76f8\u5173\u6027\u5e76\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u7cfb\u7edf\u3002", "motivation": "\u53cd\u4e8b\u5b9e\u63a8\u7406\u5df2\u6210\u4e3a\u53ef\u89e3\u91ca\u548c\u516c\u5e73AI\u7684\u6838\u5fc3\u65b9\u6cd5\uff0c\u4f46\u9700\u8981\u5f00\u53d1\u7cfb\u7edf\u5316\u6846\u67b6\u6765\u6709\u6548\u8bc6\u522b\u548c\u7f13\u89e3\u89c6\u89c9\u5206\u7c7b\u5668\u53ca\u751f\u6210\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u4e2a\u65b9\u6cd5\uff1aCAVLI\u7ed3\u5408\u5f52\u56e0\u5206\u6790\u548c\u6982\u5ff5\u7ea7\u5206\u6790\u6765\u91cf\u5316\u51b3\u7b56\u4f9d\u8d56\uff1bASAC\u901a\u8fc7\u5bf9\u6297\u6027\u53cd\u4e8b\u5b9e\u6270\u52a8\u4fdd\u62a4\u5c5e\u6027\uff1bTIBET\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u504f\u89c1\u8bc4\u4f30\u6d41\u7a0b\uff1bBiasConnect\u6784\u5efa\u56e0\u679c\u56fe\u8bca\u65ad\u504f\u89c1\uff1bInterMit\u63d0\u4f9b\u65e0\u9700\u8bad\u7ec3\u7684\u504f\u89c1\u7f13\u89e3\u7b97\u6cd5\u3002", "result": "\u5f00\u53d1\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc6\u522b\u6a21\u578b\u5bf9\u65e0\u5173\u7ebf\u7d22\u7684\u4f9d\u8d56\uff0c\u63d0\u9ad8\u6a21\u578b\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\uff0c\u907f\u514d\u523b\u677f\u5370\u8c61\uff0c\u5e76\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u7684\u793e\u4f1a\u8d23\u4efb\u504f\u89c1\u8bc4\u4f30\u548c\u7f13\u89e3\u65b9\u6cd5\u3002", "conclusion": "\u53cd\u4e8b\u5b9e\u63a8\u7406\u4e3a\u5224\u522b\u6027\u548c\u751f\u6210\u6027\u6a21\u578b\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u3001\u516c\u5e73\u6027\u548c\u56e0\u679c\u6027\u63d0\u4f9b\u4e86\u7edf\u4e00\u89c6\u89d2\uff0c\u5efa\u7acb\u4e86\u539f\u5219\u6027\u548c\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u4f53\u7cfb\u3002"}}
{"id": "2508.20920", "pdf": "https://arxiv.org/pdf/2508.20920", "abs": "https://arxiv.org/abs/2508.20920", "authors": ["Enrico Martini", "Ho Jin Choi", "Nadia Figueroa", "Nicola Bombieri"], "title": "COMETH: Convex Optimization for Multiview Estimation and Tracking of Humans", "categories": ["cs.CV", "cs.RO"], "comment": "Submitted to Information Fusion", "summary": "In the era of Industry 5.0, monitoring human activity is essential for ensuring both ergonomic safety and overall well-being. While multi-camera centralized setups improve pose estimation accuracy, they often suffer from high computational costs and bandwidth requirements, limiting scalability and real-time applicability. Distributing processing across edge devices can reduce network bandwidth and computational load. On the other hand, the constrained resources of edge devices lead to accuracy degradation, and the distribution of computation leads to temporal and spatial inconsistencies. We address this challenge by proposing COMETH (Convex Optimization for Multiview Estimation and Tracking of Humans), a lightweight algorithm for real-time multi-view human pose fusion that relies on three concepts: it integrates kinematic and biomechanical constraints to increase the joint positioning accuracy; it employs convex optimization-based inverse kinematics for spatial fusion; and it implements a state observer to improve temporal consistency. We evaluate COMETH on both public and industrial datasets, where it outperforms state-of-the-art methods in localization, detection, and tracking accuracy. The proposed fusion pipeline enables accurate and scalable human motion tracking, making it well-suited for industrial and safety-critical applications. The code is publicly available at https://github.com/PARCO-LAB/COMETH.", "AI": {"tldr": "COMETH\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u591a\u89c6\u89d2\u4eba\u4f53\u59ff\u6001\u878d\u5408\u7b97\u6cd5\uff0c\u901a\u8fc7\u51f8\u4f18\u5316\u548c\u751f\u7269\u529b\u5b66\u7ea6\u675f\u5b9e\u73b0\u5b9e\u65f6\u51c6\u786e\u7684\u4eba\u4f53\u8fd0\u52a8\u8ddf\u8e2a\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a5.0\u573a\u666f\u3002", "motivation": "\u5de5\u4e1a5.0\u65f6\u4ee3\u9700\u8981\u5b9e\u65f6\u76d1\u63a7\u4eba\u4f53\u6d3b\u52a8\u4ee5\u786e\u4fdd\u5de5\u6548\u5b89\u5168\u548c\u5065\u5eb7\uff0c\u4f46\u591a\u6444\u50cf\u5934\u96c6\u4e2d\u5f0f\u7cfb\u7edf\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u5e26\u5bbd\u9700\u6c42\u5927\uff0c\u800c\u8fb9\u7f18\u8bbe\u5907\u5904\u7406\u53c8\u5b58\u5728\u7cbe\u5ea6\u4e0b\u964d\u548c\u65f6\u7a7a\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faCOMETH\u7b97\u6cd5\uff0c\u96c6\u6210\u8fd0\u52a8\u5b66\u548c\u751f\u7269\u529b\u5b66\u7ea6\u675f\u63d0\u9ad8\u5173\u8282\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u91c7\u7528\u51f8\u4f18\u5316\u9006\u5411\u8fd0\u52a8\u5b66\u8fdb\u884c\u7a7a\u95f4\u878d\u5408\uff0c\u5e76\u5b9e\u73b0\u72b6\u6001\u89c2\u6d4b\u5668\u6539\u5584\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5728\u516c\u5171\u548c\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cCOMETH\u5728\u5b9a\u4f4d\u3001\u68c0\u6d4b\u548c\u8ddf\u8e2a\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u878d\u5408\u7ba1\u9053\u5b9e\u73b0\u4e86\u51c6\u786e\u4e14\u53ef\u6269\u5c55\u7684\u4eba\u4f53\u8fd0\u52a8\u8ddf\u8e2a\uff0c\u7279\u522b\u9002\u5408\u5de5\u4e1a\u548c\u5b89\u5168\u5173\u952e\u5e94\u7528\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.21019", "pdf": "https://arxiv.org/pdf/2508.21019", "abs": "https://arxiv.org/abs/2508.21019", "authors": ["Jiaxiang Cheng", "Bing Ma", "Xuhua Ren", "Hongyi Jin", "Kai Yu", "Peng Zhang", "Wenyue Li", "Yuan Zhou", "Tianxiang Zheng", "Qinglin Lu"], "title": "POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models", "categories": ["cs.CV"], "comment": "Project Page: https://pose-paper.github.io", "summary": "The field of video diffusion generation faces critical bottlenecks in sampling efficiency, especially for large-scale models and long sequences. Existing video acceleration methods adopt image-based techniques but suffer from fundamental limitations: they neither model the temporal coherence of video frames nor provide single-step distillation for large-scale video models. To bridge this gap, we propose POSE (Phased One-Step Equilibrium), a distillation framework that reduces the sampling steps of large-scale video diffusion models, enabling the generation of high-quality videos in a single step. POSE employs a carefully designed two-phase process to distill video models:(i) stability priming: a warm-up mechanism to stabilize adversarial distillation that adapts the high-quality trajectory of the one-step generator from high to low signal-to-noise ratio regimes, optimizing the video quality of single-step mappings near the endpoints of flow trajectories. (ii) unified adversarial equilibrium: a flexible self-adversarial distillation mechanism that promotes stable single-step adversarial training towards a Nash equilibrium within the Gaussian noise space, generating realistic single-step videos close to real videos. For conditional video generation, we propose (iii) conditional adversarial consistency, a method to improve both semantic consistency and frame consistency between conditional frames and generated frames. Comprehensive experiments demonstrate that POSE outperforms other acceleration methods on VBench-I2V by average 7.15% in semantic alignment, temporal conference and frame quality, reducing the latency of the pre-trained model by 100$\\times$, from 1000 seconds to 10 seconds, while maintaining competitive performance.", "AI": {"tldr": "POSE\u662f\u4e00\u4e2a\u5355\u6b65\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8fc7\u7a0b\uff08\u7a33\u5b9a\u6027\u9884\u70ed\u548c\u7edf\u4e00\u5bf9\u6297\u5e73\u8861\uff09\u5b9e\u73b0\u5927\u89c4\u6a21\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u9ad8\u6548\u91c7\u6837\uff0c\u5c06\u63a8\u7406\u65f6\u95f4\u4ece1000\u79d2\u51cf\u5c11\u523010\u79d2\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u9891\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u52a0\u901f\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\uff1a\u65e2\u4e0d\u80fd\u5efa\u6a21\u89c6\u9891\u5e27\u7684\u65f6\u95f4\u8fde\u8d2f\u6027\uff0c\u4e5f\u65e0\u6cd5\u4e3a\u5927\u89c4\u6a21\u89c6\u9891\u6a21\u578b\u63d0\u4f9b\u5355\u6b65\u84b8\u998f\uff0c\u5bfc\u81f4\u91c7\u6837\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u84b8\u998f\u8fc7\u7a0b\uff1a1\uff09\u7a33\u5b9a\u6027\u9884\u70ed\u673a\u5236\uff0c\u5728\u9ad8\u5230\u4f4e\u4fe1\u566a\u6bd4\u533a\u95f4\u7a33\u5b9a\u5bf9\u6297\u84b8\u998f\uff1b2\uff09\u7edf\u4e00\u5bf9\u6297\u5e73\u8861\u673a\u5236\uff0c\u5728\u9ad8\u65af\u566a\u58f0\u7a7a\u95f4\u5b9e\u73b0\u7a33\u5b9a\u7684\u5355\u6b65\u5bf9\u6297\u8bad\u7ec3\uff1b\u5bf9\u4e8e\u6761\u4ef6\u751f\u6210\u8fd8\u63d0\u51fa\u6761\u4ef6\u5bf9\u6297\u4e00\u81f4\u6027\u65b9\u6cd5\u3002", "result": "\u5728VBench-I2V\u57fa\u51c6\u4e0a\u5e73\u5747\u63d0\u53477.15%\u7684\u8bed\u4e49\u5bf9\u9f50\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u5e27\u8d28\u91cf\uff0c\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5ef6\u8fdf\u964d\u4f4e100\u500d\uff08\u4ece1000\u79d2\u523010\u79d2\uff09\u3002", "conclusion": "POSE\u6210\u529f\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u6548\u7387\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u5355\u6b65\u89c6\u9891\u751f\u6210\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2508.21032", "pdf": "https://arxiv.org/pdf/2508.21032", "abs": "https://arxiv.org/abs/2508.21032", "authors": ["Dale Decatur", "Thibault Groueix", "Wang Yifan", "Rana Hanocka", "Vladimir Kim", "Matheus Gadelha"], "title": "Reusing Computation in Text-to-Image Diffusion for Efficient Generation of Image Sets", "categories": ["cs.CV"], "comment": "ICCV 2025. Project page:   https://ddecatur.github.io/hierarchical-diffusion/", "summary": "Text-to-image diffusion models enable high-quality image generation but are computationally expensive. While prior work optimizes per-inference efficiency, we explore an orthogonal approach: reducing redundancy across correlated prompts. Our method leverages the coarse-to-fine nature of diffusion models, where early denoising steps capture shared structures among similar prompts. We propose a training-free approach that clusters prompts based on semantic similarity and shares computation in early diffusion steps. Experiments show that for models trained conditioned on image embeddings, our approach significantly reduces compute cost while improving image quality. By leveraging UnClip's text-to-image prior, we enhance diffusion step allocation for greater efficiency. Our method seamlessly integrates with existing pipelines, scales with prompt sets, and reduces the environmental and financial burden of large-scale text-to-image generation. Project page: https://ddecatur.github.io/hierarchical-diffusion/", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8bad\u7ec3\u514d\u8d39\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u7c7b\u8bed\u4e49\u76f8\u4f3c\u7684\u63d0\u793a\u8bcd\u5e76\u5728\u65e9\u671f\u6269\u6563\u6b65\u9aa4\u4e2d\u5171\u4eab\u8ba1\u7b97\uff0c\u663e\u8457\u964d\u4f4e\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u7b97\u529b\u6210\u672c\u5e76\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u867d\u7136\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u4f18\u5316\u5355\u6b21\u63a8\u7406\u6548\u7387\uff0c\u672c\u6587\u63a2\u7d22\u53e6\u4e00\u79cd\u9014\u5f84\uff1a\u51cf\u5c11\u76f8\u5173\u63d0\u793a\u8bcd\u4e4b\u95f4\u7684\u5197\u4f59\u8ba1\u7b97\u3002", "method": "\u5229\u7528\u6269\u6563\u6a21\u578b\u4ece\u7c97\u5230\u7ec6\u7684\u7279\u6027\uff0c\u5728\u65e9\u671f\u53bb\u566a\u6b65\u9aa4\u4e2d\u6355\u83b7\u76f8\u4f3c\u63d0\u793a\u8bcd\u7684\u5171\u4eab\u7ed3\u6784\u3002\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u6027\u5bf9\u63d0\u793a\u8bcd\u8fdb\u884c\u805a\u7c7b\uff0c\u5728\u65e9\u671f\u6269\u6563\u6b65\u9aa4\u4e2d\u5171\u4eab\u8ba1\u7b97\u3002\u7ed3\u5408UnClip\u7684\u6587\u672c\u5230\u56fe\u50cf\u5148\u9a8c\u4f18\u5316\u6269\u6563\u6b65\u9aa4\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5bf9\u4e8e\u57fa\u4e8e\u56fe\u50cf\u5d4c\u5165\u6761\u4ef6\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u540c\u65f6\u63d0\u9ad8\u4e86\u56fe\u50cf\u8d28\u91cf\u3002\u65b9\u6cd5\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u6d41\u7a0b\u4e2d\uff0c\u968f\u63d0\u793a\u8bcd\u96c6\u89c4\u6a21\u6269\u5c55\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u73af\u5883\u548c\u7ecf\u6d4e\u8d1f\u62c5\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8ba1\u7b97\u5171\u4eab\u7b56\u7565\u3002"}}
{"id": "2508.21040", "pdf": "https://arxiv.org/pdf/2508.21040", "abs": "https://arxiv.org/abs/2508.21040", "authors": ["Huynh Tong Dang Khoa", "Dang Hoai Nam", "Vo Nguyen Le Duy"], "title": "FW-GAN: Frequency-Driven Handwriting Synthesis with Wave-Modulated MLP Generator", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Labeled handwriting data is often scarce, limiting the effectiveness of recognition systems that require diverse, style-consistent training samples. Handwriting synthesis offers a promising solution by generating artificial data to augment training. However, current methods face two major limitations. First, most are built on conventional convolutional architectures, which struggle to model long-range dependencies and complex stroke patterns. Second, they largely ignore the crucial role of frequency information, which is essential for capturing fine-grained stylistic and structural details in handwriting. To address these challenges, we propose FW-GAN, a one-shot handwriting synthesis framework that generates realistic, writer-consistent text from a single example. Our generator integrates a phase-aware Wave-MLP to better capture spatial relationships while preserving subtle stylistic cues. We further introduce a frequency-guided discriminator that leverages high-frequency components to enhance the authenticity detection of generated samples. Additionally, we introduce a novel Frequency Distribution Loss that aligns the frequency characteristics of synthetic and real handwriting, thereby enhancing visual fidelity. Experiments on Vietnamese and English handwriting datasets demonstrate that FW-GAN generates high-quality, style-consistent handwriting, making it a valuable tool for augmenting data in low-resource handwriting recognition (HTR) pipelines. Official implementation is available at https://github.com/DAIR-Group/FW-GAN", "AI": {"tldr": "FW-GAN\u662f\u4e00\u4e2a\u57fa\u4e8e\u9891\u7387\u611f\u77e5\u7684\u5355\u6837\u672c\u624b\u5199\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7Wave-MLP\u548c\u9891\u7387\u5f15\u5bfc\u9274\u522b\u5668\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u98ce\u683c\u4e00\u81f4\u7684\u624b\u5199\u6587\u672c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u9891\u7387\u4fe1\u606f\u5efa\u6a21\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6807\u8bb0\u624b\u5199\u6570\u636e\u7a00\u7f3a\u9650\u5236\u4e86\u8bc6\u522b\u7cfb\u7edf\u7684\u6548\u679c\uff0c\u73b0\u6709\u5408\u6210\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u4f20\u7edf\u5377\u79ef\u67b6\u6784\u96be\u4ee5\u5efa\u6a21\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u590d\u6742\u7b14\u753b\u6a21\u5f0f\uff0c\u4ee5\u53ca\u5ffd\u89c6\u9891\u7387\u4fe1\u606f\u5728\u6355\u6349\u624b\u5199\u7ec6\u8282\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "method": "\u63d0\u51faFW-GAN\u6846\u67b6\uff0c\u5305\u542b\u76f8\u4f4d\u611f\u77e5Wave-MLP\u751f\u6210\u5668\u6765\u6355\u6349\u7a7a\u95f4\u5173\u7cfb\u5e76\u4fdd\u7559\u98ce\u683c\u7ebf\u7d22\uff0c\u9891\u7387\u5f15\u5bfc\u9274\u522b\u5668\u5229\u7528\u9ad8\u9891\u5206\u91cf\u589e\u5f3a\u771f\u5b9e\u6027\u68c0\u6d4b\uff0c\u4ee5\u53ca\u65b0\u9896\u7684\u9891\u7387\u5206\u5e03\u635f\u5931\u51fd\u6570\u5bf9\u9f50\u5408\u6210\u4e0e\u771f\u5b9e\u624b\u5199\u7684\u9891\u7387\u7279\u5f81\u3002", "result": "\u5728\u8d8a\u5357\u8bed\u548c\u82f1\u8bed\u624b\u5199\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFW-GAN\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u98ce\u683c\u4e00\u81f4\u7684\u624b\u5199\u6587\u672c\uff0c\u6709\u6548\u589e\u5f3a\u4f4e\u8d44\u6e90\u624b\u5199\u8bc6\u522b\u6d41\u7a0b\u7684\u6570\u636e\u3002", "conclusion": "FW-GAN\u901a\u8fc7\u9891\u7387\u611f\u77e5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u624b\u5199\u5408\u6210\u7684\u771f\u5b9e\u6027\u548c\u98ce\u683c\u4e00\u81f4\u6027\uff0c\u4e3a\u4f4e\u8d44\u6e90\u624b\u5199\u8bc6\u522b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6570\u636e\u589e\u5f3a\u5de5\u5177\u3002"}}
{"id": "2508.21066", "pdf": "https://arxiv.org/pdf/2508.21066", "abs": "https://arxiv.org/abs/2508.21066", "authors": ["Yuan Gong", "Xionghui Wang", "Jie Wu", "Shiyin Wang", "Yitong Wang", "Xinglong Wu"], "title": "OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning", "categories": ["cs.CV"], "comment": "project url: https://one-reward.github.io", "summary": "In this paper, we introduce OneReward, a unified reinforcement learning framework that enhances the model's generative capabilities across multiple tasks under different evaluation criteria using only \\textit{One Reward} model. By employing a single vision-language model (VLM) as the generative reward model, which can distinguish the winner and loser for a given task and a given evaluation criterion, it can be effectively applied to multi-task generation models, particularly in contexts with varied data and diverse task objectives. We utilize OneReward for mask-guided image generation, which can be further divided into several sub-tasks such as image fill, image extend, object removal, and text rendering, involving a binary mask as the edit area. Although these domain-specific tasks share same conditioning paradigm, they differ significantly in underlying data distributions and evaluation metrics. Existing methods often rely on task-specific supervised fine-tuning (SFT), which limits generalization and training efficiency. Building on OneReward, we develop Seedream 3.0 Fill, a mask-guided generation model trained via multi-task reinforcement learning directly on a pre-trained base model, eliminating the need for task-specific SFT. Experimental results demonstrate that our unified edit model consistently outperforms both commercial and open-source competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across multiple evaluation dimensions. Code and model are available at: https://one-reward.github.io", "AI": {"tldr": "OneReward\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u5355\u4e00\u5956\u52b1\u6a21\u578b\u63d0\u5347\u591a\u4efb\u52a1\u751f\u6210\u80fd\u529b\uff0c\u5e94\u7528\u4e8e\u63a9\u7801\u5f15\u5bfc\u56fe\u50cf\u751f\u6210\u4efb\u52a1\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u7684\u76d1\u7763\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u4efb\u52a1\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u4efb\u52a1\u8fdb\u884c\u4e13\u95e8\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8bad\u7ec3\u6548\u7387\u3002\u4e0d\u540c\u4efb\u52a1\u867d\u7136\u5171\u4eab\u76f8\u540c\u7684\u6761\u4ef6\u8303\u5f0f\uff0c\u4f46\u5728\u6570\u636e\u5206\u5e03\u548c\u8bc4\u4f30\u6307\u6807\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u5355\u4e00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u751f\u6210\u5956\u52b1\u6a21\u578b\uff0c\u80fd\u591f\u533a\u5206\u7ed9\u5b9a\u4efb\u52a1\u548c\u8bc4\u4f30\u6807\u51c6\u4e0b\u7684\u4f18\u80dc\u8005\u548c\u5931\u8d25\u8005\u3002\u57fa\u4e8eOneReward\u6846\u67b6\u5f00\u53d1Seedream 3.0 Fill\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u76f4\u63a5\u5728\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7edf\u4e00\u7684\u7f16\u8f91\u6a21\u578b\u5728\u591a\u4e2a\u8bc4\u4f30\u7ef4\u5ea6\u4e0a\u6301\u7eed\u4f18\u4e8e\u5546\u4e1a\u548c\u5f00\u6e90\u7ade\u4e89\u5bf9\u624b\uff0c\u5305\u62ecIdeogram\u3001Adobe Photoshop\u548cFLUX Fill [Pro]\u3002", "conclusion": "OneReward\u6846\u67b6\u8bc1\u660e\u4e86\u4f7f\u7528\u5355\u4e00\u5956\u52b1\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u591a\u4efb\u52a1\u751f\u6210\u6027\u80fd\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u4e3a\u591a\u4efb\u52a1\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8bad\u7ec3\u8303\u5f0f\u3002"}}
{"id": "2508.21072", "pdf": "https://arxiv.org/pdf/2508.21072", "abs": "https://arxiv.org/abs/2508.21072", "authors": ["Fahad Shamshad", "Tameem Bakr", "Yahia Shaaban", "Noor Hussein", "Karthik Nandakumar", "Nils Lukas"], "title": "First-Place Solution to NeurIPS 2024 Invisible Watermark Removal Challenge", "categories": ["cs.CV"], "comment": "Winning solution to the NeurIPS 2024 Erasing the Invisible challenge", "summary": "Content watermarking is an important tool for the authentication and copyright protection of digital media. However, it is unclear whether existing watermarks are robust against adversarial attacks. We present the winning solution to the NeurIPS 2024 Erasing the Invisible challenge, which stress-tests watermark robustness under varying degrees of adversary knowledge. The challenge consisted of two tracks: a black-box and beige-box track, depending on whether the adversary knows which watermarking method was used by the provider. For the beige-box track, we leverage an adaptive VAE-based evasion attack, with a test-time optimization and color-contrast restoration in CIELAB space to preserve the image's quality. For the black-box track, we first cluster images based on their artifacts in the spatial or frequency-domain. Then, we apply image-to-image diffusion models with controlled noise injection and semantic priors from ChatGPT-generated captions to each cluster with optimized parameter settings. Empirical evaluations demonstrate that our method successfully achieves near-perfect watermark removal (95.7%) with negligible impact on the residual image's quality. We hope that our attacks inspire the development of more robust image watermarking methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9NeurIPS 2024\u6311\u6218\u8d5b\u7684\u83b7\u80dc\u6c34\u5370\u79fb\u9664\u65b9\u6848\uff0c\u5305\u542b\u9ed1\u76d2\u548c\u767d\u76d2\u4e24\u79cd\u653b\u51fb\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e8695.7%\u7684\u6c34\u5370\u79fb\u9664\u7387\u4e14\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf", "motivation": "\u6d4b\u8bd5\u73b0\u6709\u6c34\u5370\u6280\u672f\u5bf9\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u63a8\u52a8\u66f4\u5f3a\u5927\u7684\u56fe\u50cf\u6c34\u5370\u65b9\u6cd5\u53d1\u5c55", "method": "\u767d\u76d2\u8ddf\u8e2a\u4f7f\u7528\u81ea\u9002\u5e94VAE\u89c4\u907f\u653b\u51fb\uff0c\u7ed3\u5408\u6d4b\u8bd5\u65f6\u4f18\u5316\u548cCIELAB\u8272\u5f69\u7a7a\u95f4\u5bf9\u6bd4\u5ea6\u6062\u590d\uff1b\u9ed1\u76d2\u8ddf\u8e2a\u901a\u8fc7\u56fe\u50cf\u805a\u7c7b\uff0c\u5e94\u7528\u6269\u6563\u6a21\u578b\u548cChatGPT\u751f\u6210\u7684\u8bed\u4e49\u5148\u9a8c\u8fdb\u884c\u6c34\u5370\u79fb\u9664", "result": "\u5b9e\u73b0\u4e86\u8fd1\u5b8c\u7f8e\u7684\u6c34\u5370\u79fb\u9664\u6548\u679c\uff0895.7%\uff09\uff0c\u5bf9\u5269\u4f59\u56fe\u50cf\u8d28\u91cf\u5f71\u54cd\u6781\u5c0f", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c55\u793a\u4e86\u73b0\u6709\u6c34\u5370\u6280\u672f\u7684\u8106\u5f31\u6027\uff0c\u5e0c\u671b\u6fc0\u52b1\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u56fe\u50cf\u6c34\u5370\u65b9\u6cd5"}}
{"id": "2508.20250", "pdf": "https://arxiv.org/pdf/2508.20250", "abs": "https://arxiv.org/abs/2508.20250", "authors": ["Jessica Kinnevan", "Naifa Alqahtani", "Toral Chauhan"], "title": "Efficient and Privacy-Protecting Background Removal for 2D Video Streaming using iPhone 15 Pro Max LiDAR", "categories": ["eess.IV", "cs.CV", "cs.MM", "68T45, 68U10", "I.4.6; I.4.8; H.5.1; I.2.10"], "comment": null, "summary": "Light Detection and Ranging (LiDAR) technology in consumer-grade mobile devices can be used as a replacement for traditional background removal and compositing techniques. Unlike approaches such as chroma keying and trained AI models, LiDAR's depth information is independent of subject lighting, and performs equally well in low-light and well-lit environments. We integrate the LiDAR and color cameras on the iPhone 15 Pro Max with GPU-based image processing. We use Apple's SwiftUI and Swift frameworks for user interface and backend development, and Metal Shader Language (MSL) for realtime image enhancement at the standard iPhone streaming frame rate of 60 frames per second. The only meaningful limitations of the technology are the streaming bandwidth of the depth data, which currently reduces the depth map resolution to 320x240, and any pre-existing limitations of the LiDAR IR laser to reflect accurate depth from some materials. If the LiDAR resolution on a mobile device like the iPhone can be improved to match the color image resolution, LiDAR could feasibly become the preeminent method of background removal for video applications and photography.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528iPhone 15 Pro Max\u4e0a\u7684LiDAR\u6280\u672f\u6765\u66ff\u4ee3\u4f20\u7edf\u80cc\u666f\u79fb\u9664\u6280\u672f\uff0c\u5229\u7528\u6df1\u5ea6\u4fe1\u606f\u5b9e\u73b0\u4e0d\u53d7\u5149\u7167\u6761\u4ef6\u5f71\u54cd\u7684\u9ad8\u6027\u80fd\u80cc\u666f\u79fb\u9664\u3002", "motivation": "\u4f20\u7edf\u7684\u80cc\u666f\u79fb\u9664\u6280\u672f\u5982\u84dd\u5e55\u62c9\u5e55\u548cAI\u6a21\u578b\u5b58\u5728\u5bf9\u5149\u7167\u6761\u4ef6\u654f\u611f\u3001\u6027\u80fd\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\uff0cLiDAR\u6280\u672f\u80fd\u591f\u63d0\u4f9b\u66f4\u7a33\u5b9a\u4e14\u4e0d\u53d7\u5149\u7167\u5f71\u54cd\u7684\u6df1\u5ea6\u4fe1\u606f\u3002", "method": "\u96c6\u6210iPhone 15 Pro Max\u7684LiDAR\u548c\u989c\u8272\u6444\u50cf\u5934\uff0c\u4f7f\u7528SwiftUI\u548cSwift\u6846\u67b6\u8fdb\u884c\u754c\u9762\u548c\u540e\u7aef\u5f00\u53d1\uff0c\u91c7\u7528Metal Shader Language(MSL)\u5b9e\u73b0\u5b9e\u65f6\u56fe\u50cf\u589e\u5f3a\uff0c\u8fbe\u523060fps\u7684\u6807\u51c6\u6e38\u6d41\u5e27\u7387\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u5728\u5404\u79cd\u5149\u7167\u6761\u4ef6\u4e0b\u7a33\u5b9a\u8fd0\u884c\uff0c\u4f46\u6df1\u5ea6\u6570\u636e\u6e38\u6d41\u5e26\u5bbd\u9650\u5236\u4e86\u6df1\u5ea6\u56fe\u5206\u8fa8\u7387\u4e3a320x240\uff0c\u540c\u65f6LiDAR IR\u6fc0\u5149\u5bf9\u67d0\u4e9b\u6750\u8d28\u7684\u6df1\u5ea6\u91cd\u5efa\u4e5f\u6709\u9650\u5236\u3002", "conclusion": "\u5982\u679ciPhone\u7b49\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684LiDAR\u5206\u8fa8\u7387\u80fd\u591f\u63d0\u5347\u5230\u4e0e\u989c\u8272\u56fe\u50cf\u76f8\u540c\u7684\u6c34\u5e73\uff0cLiDAR\u6280\u672f\u6709\u671b\u6210\u4e3a\u89c6\u9891\u548c\u6444\u5f71\u5e94\u7528\u4e2d\u6700\u4f18\u79f0\u7684\u80cc\u666f\u79fb\u9664\u65b9\u6cd5\u3002"}}
{"id": "2508.20547", "pdf": "https://arxiv.org/pdf/2508.20547", "abs": "https://arxiv.org/abs/2508.20547", "authors": ["Yunpeng Mei", "Hongjie Cao", "Yinqiu Xia", "Wei Xiao", "Zhaohan Feng", "Gang Wang", "Jie Chen"], "title": "SPGrasp: Spatiotemporal Prompt-driven Grasp Synthesis in Dynamic Scenes", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Real-time interactive grasp synthesis for dynamic objects remains challenging as existing methods fail to achieve low-latency inference while maintaining promptability. To bridge this gap, we propose SPGrasp (spatiotemporal prompt-driven dynamic grasp synthesis), a novel framework extending segment anything model v2 (SAMv2) for video stream grasp estimation. Our core innovation integrates user prompts with spatiotemporal context, enabling real-time interaction with end-to-end latency as low as 59 ms while ensuring temporal consistency for dynamic objects. In benchmark evaluations, SPGrasp achieves instance-level grasp accuracies of 90.6% on OCID and 93.8% on Jacquard. On the challenging GraspNet-1Billion dataset under continuous tracking, SPGrasp achieves 92.0% accuracy with 73.1 ms per-frame latency, representing a 58.5% reduction compared to the prior state-of-the-art promptable method RoG-SAM while maintaining competitive accuracy. Real-world experiments involving 13 moving objects demonstrate a 94.8% success rate in interactive grasping scenarios. These results confirm SPGrasp effectively resolves the latency-interactivity trade-off in dynamic grasp synthesis. Code is available at https://github.com/sejmoonwei/SPGrasp.", "AI": {"tldr": "SPGrasp\u662f\u4e00\u4e2a\u57fa\u4e8eSAMv2\u7684\u5b9e\u65f6\u52a8\u6001\u6293\u53d6\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u7528\u6237\u63d0\u793a\u548c\u65f6\u7a7a\u4e0a\u4e0b\u6587\uff0c\u5b9e\u73b0\u4e8659ms\u7684\u4f4e\u5ef6\u8fdf\u63a8\u7406\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u523090%\u4ee5\u4e0a\u7684\u6293\u53d6\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u65f6\u4ea4\u4e92\u5f0f\u52a8\u6001\u7269\u4f53\u6293\u53d6\u5408\u6210\u4e2d\u65e0\u6cd5\u540c\u65f6\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u63a8\u7406\u548c\u63d0\u793a\u529f\u80fd\uff0c\u9700\u8981\u89e3\u51b3\u5ef6\u8fdf\u4e0e\u4ea4\u4e92\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u6269\u5c55SAMv2\u6a21\u578b\u7528\u4e8e\u89c6\u9891\u6d41\u6293\u53d6\u4f30\u8ba1\uff0c\u6574\u5408\u7528\u6237\u63d0\u793a\u4e0e\u65f6\u7a7a\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u786e\u4fdd\u52a8\u6001\u7269\u4f53\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5728OCID\u6570\u636e\u96c6\u4e0a\u8fbe\u523090.6%\u7684\u5b9e\u4f8b\u7ea7\u6293\u53d6\u51c6\u786e\u7387\uff0cJacquard\u6570\u636e\u96c6\u4e0a93.8%\uff0cGraspNet-1Billion\u6570\u636e\u96c6\u4e0a92.0%\u51c6\u786e\u7387\uff0c\u5ef6\u8fdf\u964d\u4f4e58.5%\uff0c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d94.8%\u7684\u6210\u529f\u7387\u3002", "conclusion": "SPGrasp\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u6293\u53d6\u5408\u6210\u4e2d\u7684\u5ef6\u8fdf-\u4ea4\u4e92\u6027\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u4ea4\u4e92\u5f0f\u6293\u53d6\u5408\u6210\u3002"}}
{"id": "2508.20600", "pdf": "https://arxiv.org/pdf/2508.20600", "abs": "https://arxiv.org/abs/2508.20600", "authors": ["Kian Anvari Hamedani", "Narges Razizadeh", "Shahabedin Nabavi", "Mohsen Ebrahimi Moghaddam"], "title": "GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac MRI Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accelerated Cardiovascular Magnetic Resonance (CMR) image reconstruction remains a critical challenge due to the trade-off between scan time and image quality, particularly when generalizing across diverse acquisition settings. We propose GENRE-CMR, a generative adversarial network (GAN)-based architecture employing a residual deep unrolled reconstruction framework to enhance reconstruction fidelity and generalization. The architecture unrolls iterative optimization into a cascade of convolutional subnetworks, enriched with residual connections to enable progressive feature propagation from shallow to deeper stages. To further improve performance, we integrate two loss functions: (1) an Edge-Aware Region (EAR) loss, which guides the network to focus on structurally informative regions and helps prevent common reconstruction blurriness; and (2) a Statistical Distribution Alignment (SDA) loss, which regularizes the feature space across diverse data distributions via a symmetric KL divergence formulation. Extensive experiments confirm that GENRE-CMR surpasses state-of-the-art methods on training and unseen data, achieving 0.9552 SSIM and 38.90 dB PSNR on unseen distributions across various acceleration factors and sampling trajectories. Ablation studies confirm the contribution of each proposed component to reconstruction quality and generalization. Our framework presents a unified and robust solution for high-quality CMR reconstruction, paving the way for clinically adaptable deployment across heterogeneous acquisition protocols.", "AI": {"tldr": "GENRE-CMR\u662f\u4e00\u79cd\u57fa\u4e8eGAN\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u67b6\u6784\uff0c\u91c7\u7528\u6b8b\u5dee\u6df1\u5ea6\u5c55\u5f00\u91cd\u5efa\u6846\u67b6\u6765\u63d0\u5347\u5fc3\u810f\u78c1\u5171\u632f\u56fe\u50cf\u91cd\u5efa\u7684\u4fdd\u771f\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u8fb9\u7f18\u611f\u77e5\u533a\u57df\u635f\u5931\u548c\u7edf\u8ba1\u5206\u5e03\u5bf9\u9f50\u635f\u5931\u5b9e\u73b0\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u52a0\u901f\u5fc3\u810f\u78c1\u5171\u632f\u6210\u50cf\u4e2d\u626b\u63cf\u65f6\u95f4\u4e0e\u56fe\u50cf\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u91c7\u96c6\u8bbe\u7f6e\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u6311\u6218\u3002", "method": "\u4f7f\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc(GAN)\u67b6\u6784\uff0c\u7ed3\u5408\u6b8b\u5dee\u6df1\u5ea6\u5c55\u5f00\u91cd\u5efa\u6846\u67b6\uff0c\u5305\u542b\u8fb9\u7f18\u611f\u77e5\u533a\u57df(EAR)\u635f\u5931\u548c\u7edf\u8ba1\u5206\u5e03\u5bf9\u9f50(SDA)\u635f\u5931\u6765\u6307\u5bfc\u7f51\u7edc\u8bad\u7ec3\u3002", "result": "\u5728\u672a\u89c1\u6570\u636e\u5206\u5e03\u4e0a\u8fbe\u52300.9552 SSIM\u548c38.90 dB PSNR\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u5404\u79cd\u52a0\u901f\u56e0\u5b50\u548c\u91c7\u6837\u8f68\u8ff9\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9ad8\u8d28\u91cf\u5fc3\u810f\u78c1\u5171\u632f\u91cd\u5efa\u63d0\u4f9b\u4e86\u7edf\u4e00\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u8de8\u5f02\u6784\u91c7\u96c6\u534f\u8bae\u7684\u4e34\u5e8a\u9002\u5e94\u6027\u90e8\u7f72\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.20773", "pdf": "https://arxiv.org/pdf/2508.20773", "abs": "https://arxiv.org/abs/2508.20773", "authors": ["Christoforos N. Spartalis", "Theodoros Semertzidis", "Petros Daras", "Efstratios Gavves"], "title": "Unleashing Uncertainty: Efficient Machine Unlearning for Generative AI", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "ICML 2025 workshop on Machine Unlearning for Generative AI", "summary": "We introduce SAFEMax, a novel method for Machine Unlearning in diffusion models. Grounded in information-theoretic principles, SAFEMax maximizes the entropy in generated images, causing the model to generate Gaussian noise when conditioned on impermissible classes by ultimately halting its denoising process. Also, our method controls the balance between forgetting and retention by selectively focusing on the early diffusion steps, where class-specific information is prominent. Our results demonstrate the effectiveness of SAFEMax and highlight its substantial efficiency gains over state-of-the-art methods.", "AI": {"tldr": "SAFEMax\u662f\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u539f\u7406\u7684\u6269\u6563\u6a21\u578b\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u751f\u6210\u56fe\u50cf\u7684\u71b5\uff0c\u4f7f\u6a21\u578b\u5728\u6761\u4ef6\u4e8e\u4e0d\u5141\u8bb8\u7c7b\u522b\u65f6\u751f\u6210\u9ad8\u65af\u566a\u58f0\u5e76\u505c\u6b62\u53bb\u566a\u8fc7\u7a0b", "motivation": "\u89e3\u51b3\u6269\u6563\u6a21\u578b\u4e2d\u654f\u611f\u6216\u4e0d\u5141\u8bb8\u7c7b\u522b\u7684\u9057\u5fd8\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5176\u4ed6\u7c7b\u522b\u7684\u826f\u597d\u751f\u6210\u6027\u80fd", "method": "\u57fa\u4e8e\u4fe1\u606f\u8bba\u539f\u7406\uff0c\u5728\u65e9\u671f\u6269\u6563\u6b65\u9aa4\u4e2d\u6700\u5927\u5316\u751f\u6210\u56fe\u50cf\u7684\u71b5\uff0c\u9009\u62e9\u6027\u5173\u6ce8\u7c7b\u522b\u4fe1\u606f\u663e\u8457\u7684\u9636\u6bb5\uff0c\u63a7\u5236\u9057\u5fd8\u4e0e\u4fdd\u7559\u7684\u5e73\u8861", "result": "SAFEMax\u65b9\u6cd5\u6709\u6548\u5b9e\u73b0\u4e86\u76ee\u6807\u7c7b\u522b\u7684\u9057\u5fd8\uff0c\u5e76\u5728\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "SAFEMax\u4e3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u673a\u5668\u9057\u5fd8\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u4fe1\u606f\u8bba\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u7c7b\u522b\u9009\u62e9\u6027\u9057\u5fd8"}}
{"id": "2508.21041", "pdf": "https://arxiv.org/pdf/2508.21041", "abs": "https://arxiv.org/abs/2508.21041", "authors": ["Guillaume Balezo", "Rapha\u00ebl Bourgade", "Thomas Walter"], "title": "Efficient Fine-Tuning of DINOv3 Pretrained on Natural Images for Atypical Mitotic Figure Classification in MIDOG 2025", "categories": ["eess.IV", "cs.CV"], "comment": "3 pages. Challenge report for MIDOG 2025 (Task 2: Atypical Mitotic   Figure Classification)", "summary": "Atypical mitotic figures (AMFs) are markers of abnormal cell division associated with poor prognosis, yet their detection remains difficult due to low prevalence, subtle morphology, and inter-observer variability. The MIDOG 2025 challenge introduces a benchmark for AMF classification across multiple domains. In this work, we evaluate the recently published DINOv3-H+ vision transformer, pretrained on natural images, which we fine-tuned using low-rank adaptation (LoRA, 650k trainable parameters) and extensive augmentation. Despite the domain gap, DINOv3 transfers effectively to histopathology, achieving a balanced accuracy of 0.8871 on the preliminary test set. These results highlight the robustness of DINOv3 pretraining and show that, when combined with parameter-efficient fine-tuning, it provides a strong baseline for atypical mitosis classification in MIDOG 2025.", "AI": {"tldr": "DINOv3-H+\u89c6\u89c9\u53d8\u6362\u5668\u901a\u8fc7LoRA\u5fae\u8c03\u548c\u6570\u636e\u589e\u5f3a\uff0c\u5728MIDOG 2025\u6311\u6218\u8d5b\u7684\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e860.8871\u7684\u5e73\u8861\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e0a\u7684\u6709\u6548\u8fc1\u79fb\u80fd\u529b\u3002", "motivation": "\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u56fe\u50cf(AMFs)\u662f\u9884\u540e\u4e0d\u826f\u7684\u91cd\u8981\u6807\u5fd7\u7269\uff0c\u4f46\u7531\u4e8e\u51fa\u73b0\u9891\u7387\u4f4e\u3001\u5f62\u6001\u5b66\u7279\u5f81\u7ec6\u5fae\u4ee5\u53ca\u89c2\u5bdf\u8005\u95f4\u5dee\u5f02\u5927\uff0c\u68c0\u6d4b\u96be\u5ea6\u5f88\u9ad8\u3002MIDOG 2025\u6311\u6218\u8d5b\u4e3a\u6b64\u5efa\u7acb\u4e86\u8de8\u57df\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u4f7f\u7528\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u9884\u8bad\u7ec3\u7684DINOv3-H+\u89c6\u89c9\u53d8\u6362\u5668\uff0c\u91c7\u7528\u4f4e\u79e9\u9002\u5e94(LoRA)\u65b9\u6cd5\u8fdb\u884c\u5fae\u8c03\uff08\u4ec565\u4e07\u53ef\u8bad\u7ec3\u53c2\u6570\uff09\uff0c\u5e76\u914d\u5408\u5e7f\u6cdb\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\u3002", "result": "\u5c3d\u7ba1\u5b58\u5728\u9886\u57df\u5dee\u5f02\uff0cDINOv3\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u8fc1\u79fb\u6027\u80fd\uff0c\u5728\u521d\u6b65\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u4e860.8871\u7684\u5e73\u8861\u51c6\u786e\u7387\u3002", "conclusion": "DINOv3\u9884\u8bad\u7ec3\u6a21\u578b\u5177\u6709\u5f3a\u5927\u7684\u9c81\u68d2\u6027\uff0c\u7ed3\u5408\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u4e3aMIDOG 2025\u7684\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u57fa\u7ebf\u6a21\u578b\u3002"}}
