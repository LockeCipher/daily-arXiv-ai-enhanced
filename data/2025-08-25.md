<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 11]
- [cs.RO](#cs.RO) [Total: 1]
- [eess.IV](#eess.IV) [Total: 4]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Wavelet-Space Super-Resolution for Real-Time Rendering](https://arxiv.org/abs/2508.16024)
*Prateek Poudel,Prashant Aryal,Kirtan Kunwar,Navin Nepal,Dinesh Bania Kshatri*

Main category: cs.GR

TL;DR: 该论文提出了一种基于小波空间特征分解的神经超分辨率方法，通过小波变换分离低频和高频细节，在保持结构一致性的同时更好地保留纹理细节，相比基线方法在PSNR和LPIPS指标上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的RGB空间回归方法在神经超分辨率中难以同时保持结构一致性和精细纹理细节，需要一种能够更好处理多尺度特征的表征方法。

Method: 基于DFASR框架，引入小波域表示，使用平稳小波变换(SWT)避免空间下采样，保持子带对齐和位移不变性。模型基于空间G-buffers和时间扭曲历史帧预测小波系数，通过逆小波合成重建图像。

Result: 相比DFASR基线，PSNR提升最高达1.5 dB，LPIPS平均降低17%，计算开销增加约24ms，在高端GPU上仍能保持实时性能。

Conclusion: 小波域表示是提升图形应用中神经上采样感知质量的有效且原理性方法。

Abstract: We investigate the use of wavelet-space feature decomposition in neural super-resolution for rendering pipelines. Building on the DFASR framework, we introduce a wavelet-domain representation that separates low- and high-frequency details before reconstruction, enabling the network to better preserve fine textures while maintaining structural consistency. Unlike RGB-space regression, our approach leverages the stationary wavelet transform (SWT) to avoid spatial down-sampling, ensuring alignment across subbands and preserving shift invariance. The model predicts wavelet coefficients conditioned on spatial G-buffers and temporally warped history frames, which are then recombined through inverse wavelet synthesis. We conduct a comprehensive ablation study across wavelet families, transform types, and architectural variants, showing that incorporating SWT improves PSNR by up to 1.5 dB and reduces LPIPS by 17% on average, at a computational overhead of roughly +24 ms compared to out DFASR baseline. While absolute runtimes on our RTX 3050 mobile GPU are higher ( 141ms) than the original DFASR report on RTX 4090( 11ms), the relative overhead remains modest, suggesting that on higher-end GPUs our method would also remain real-time capable. Taken together, our results suggest that wavelet-domain representations are a principled and effective way to enhance perceptual quality in neural upscaling for graphics applications.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [Text-Driven 3D Hand Motion Generation from Sign Language Data](https://arxiv.org/abs/2508.15902)
*Léore Bensabath,Mathis Petrovich,Gül Varol*

Main category: cs.CV

TL;DR: 这篇论文通过自动化方法构建大规模的3D手势动作-文本描述数据集，训练了一个文本条件化的3D手势动作生成模型HandMDM，能够根据自然语言描述生成各种手势动作，包括不同手语言和非手语动作。


<details>
  <summary>Details</summary>
Motivation: 为了实现根据自然语言描述生成三维手势动作的目标，需要大规模的训练数据。但相关领域的标注数据缺乏，因此需要自动化方法来构建这种数据集。

Method: 利用大规模手语视频数据集和噪声伪标注，通过LLM将手语属性词典和动作-脚本线索翻译成手势动作描述，构建文本-动作数据对。然后训练文本条件化的手势动作模型HandMDM（基于扩散模型）。

Result: 训练的HandMDM模型在多个领域都表现出良好的突破性能，包括：同一手语言中未见的手语类别、另一种手语言的手语以及非手语动作。完整的实验研究验证了模型的多样性能。

Conclusion: 该研究为文本条件化的3D手势动作生成领域开达了新方向，通过自动化数据构建方法解决了数据缺乏问题。模型和数据将公开以支持该领域的未来研究。

Abstract: Our goal is to train a generative model of 3D hand motions, conditioned on natural language descriptions specifying motion characteristics such as handshapes, locations, finger/hand/arm movements. To this end, we automatically build pairs of 3D hand motions and their associated textual labels with unprecedented scale. Specifically, we leverage a large-scale sign language video dataset, along with noisy pseudo-annotated sign categories, which we translate into hand motion descriptions via an LLM that utilizes a dictionary of sign attributes, as well as our complementary motion-script cues. This data enables training a text-conditioned hand motion diffusion model HandMDM, that is robust across domains such as unseen sign categories from the same sign language, but also signs from another sign language and non-sign hand movements. We contribute extensive experimental investigation of these scenarios and will make our trained models and data publicly available to support future research in this relatively new field.

</details>


### [3] [Diverse Signer Avatars with Manual and Non-Manual Feature Modelling for Sign Language Production](https://arxiv.org/abs/2508.15988)
*Mohamed Ilyes Lakhal,Richard Bowden*

Main category: cs.CV

TL;DR: 通过潜在液化模型和新的手语特征聚合模块，该方法能够在保持语言内容的同时生成视觉质量更高、更多样化的手语视频。


<details>
  <summary>Details</summary>
Motivation: 现有的手语生成模型无法同时抓取手语表达的多样性和保持视觉质量，特别是在非手势属性（如情感表达）的建模方面。

Method: 提出了一种新的方法，利用潜在液化模型（LDM）从生成的参考图像合成超实的数字化身；设计了新的手语特征聚合模块，明确建模非手势特征（如面部）和手势特征（如手部）。

Result: 在YouTube-SL-25手语数据集上的实验显示，该方法在视觉质量方面超过了现有最先进方法，在感知指标上取得了显著改进。

Conclusion: 该方法能够有效地解决手语生成中的多样性和质量问题，为创建更自然、更包容的手语交流系统提供了新的解决方案。

Abstract: The diversity of sign representation is essential for Sign Language Production (SLP) as it captures variations in appearance, facial expressions, and hand movements. However, existing SLP models are often unable to capture diversity while preserving visual quality and modelling non-manual attributes such as emotions. To address this problem, we propose a novel approach that leverages Latent Diffusion Model (LDM) to synthesise photorealistic digital avatars from a generated reference image. We propose a novel sign feature aggregation module that explicitly models the non-manual features (\textit{e.g.}, the face) and the manual features (\textit{e.g.}, the hands). We show that our proposed module ensures the preservation of linguistic content while seamlessly using reference images with different ethnic backgrounds to ensure diversity. Experiments on the YouTube-SL-25 sign language dataset show that our pipeline achieves superior visual quality compared to state-of-the-art methods, with significant improvements on perceptual metrics.

</details>


### [4] [Two-flow Feedback Multi-scale Progressive Generative Adversarial Network](https://arxiv.org/abs/2508.16089)
*Sun Weikai,Song Shijie,Chi Wenjie*

Main category: cs.CV

TL;DR: 本文提出了一种新的双流反馈多尺度渐进生成对抗网络(MSPG-SEN)，通过四项核心技术提升GAN模型的图像质量、训练效率和稳定性，在多个数据集上达到了最先进水平。


<details>
  <summary>Details</summary>
Motivation: 虽然变分模型在图像生成领域取得了进步，但GAN因其独特优势仍有很大发展空间。本文旨在继承GAN现有优势的同时，提升图像质量、简化训练过程并降低训练成本。

Method: 提出了双流反馈多尺度渐进GAN(MSPG-SEN)模型，包括四个核心技术：1)自适应感知-行为反馈循璶(APFL)提高稳定性；2)全局连接双流动态殊差网络提升效率；3)动态嵌入式注意力机制(DEMA)提升特征表达能力；4)简化训练流程降低成本。

Result: 在5个数据集上达到了最先进水平：INKK(89.7%)、AWUN(78.3%)、IONJ(85.5%)、POKL(88.7%)、OPIN(96.4%)。模型显著提升了图像质量、训练效率、模型稳定性和泛化能力，同时降低了计算资源消耗(88.7%)。

Conclusion: MSPG-SEN模型通过创新的双流反馈多尺度渐进结构和四项核心技术，成功实现了GAN模型在图像质量、训练效率、稳定性和计算成本方面的综合提升，为GAN模型的发展提供了新的解决方案。

Abstract: Although diffusion model has made good progress in the field of image generation, GAN\cite{huang2023adaptive} still has a large development space due to its unique advantages, such as WGAN\cite{liu2021comparing}, SSGAN\cite{guibas2021adaptive} \cite{zhang2022vsa} \cite{zhou2024adapt} and so on. In this paper, we propose a novel two-flow feedback multi-scale progressive generative adversarial network (MSPG-SEN) for GAN models. This paper has four contributions: 1) : We propose a two-flow feedback multi-scale progressive Generative Adversarial network (MSPG-SEN), which not only improves image quality and human visual perception on the basis of retaining the advantages of the existing GAN model, but also simplifies the training process and reduces the training cost of GAN networks. Our experimental results show that, MSPG-SEN has achieved state-of-the-art generation results on the following five datasets,INKK The dataset is 89.7\%,AWUN The dataset is 78.3\%,IONJ The dataset is 85.5\%,POKL The dataset is 88.7\%,OPIN The dataset is 96.4\%. 2) : We propose an adaptive perception-behavioral feedback loop (APFL), which effectively improves the robustness and training stability of the model and reduces the training cost. 3) : We propose a globally connected two-flow dynamic residual network(). After ablation experiments, it can effectively improve the training efficiency and greatly improve the generalization ability, with stronger flexibility. 4) : We propose a new dynamic embedded attention mechanism (DEMA). After experiments, the attention can be extended to a variety of image processing tasks, which can effectively capture global-local information, improve feature separation capability and feature expression capabilities, and requires minimal computing resources only 88.7\% with INJK With strong cross-task capability.

</details>


### [5] [RAGSR: Regional Attention Guided Diffusion for Image Super-Resolution](https://arxiv.org/abs/2508.16158)
*Haodong He,Yancheng Bai,Rui Lan,Xu Duan,Lei Sun,Xiangxiang Chu,Gui-Song Xia*

Main category: cs.CV

TL;DR: 提出RAGSR方法，通过区域注意力机制提取细粒度区域描述信息，解决多对象场景下超分辨率细节生成问题


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成清晰准确区域细节方面面临挑战，特别是在多对象场景中，主要原因是缺乏细粒度区域描述和模型捕捉复杂提示能力不足

Method: 定位图像中的对象区域并为每个区域分配细粒度标题，形成区域-文本对作为文本先验，利用区域引导注意力机制确保每个区域-文本对在注意力过程中得到适当考虑

Result: 在基准数据集上的实验结果表明，该方法在生成感知真实视觉细节的同时保持上下文一致性方面表现出优越性能

Conclusion: RAGSR方法通过区域注意力机制有效克服了传统单图像超分辨率技术的局限性，提供了对文本和图像信息整合的更精细控制

Abstract: The rich textual information of large vision-language models (VLMs) combined with the powerful generative prior of pre-trained text-to-image (T2I) diffusion models has achieved impressive performance in single-image super-resolution (SISR). However, existing methods still face significant challenges in generating clear and accurate regional details, particularly in scenarios involving multiple objects. This challenge primarily stems from a lack of fine-grained regional descriptions and the models' insufficient ability to capture complex prompts. To address these limitations, we propose a Regional Attention Guided Super-Resolution (RAGSR) method that explicitly extracts localized fine-grained information and effectively encodes it through a novel regional attention mechanism, enabling both enhanced detail and overall visually coherent SR results. Specifically, RAGSR localizes object regions in an image and assigns fine-grained caption to each region, which are formatted as region-text pairs as textual priors for T2I models. A regional guided attention is then leveraged to ensure that each region-text pair is properly considered in the attention process while preventing unwanted interactions between unrelated region-text pairs. By leveraging this attention mechanism, our approach offers finer control over the integration of text and image information, thereby effectively overcoming limitations faced by traditional SISR techniques. Experimental results on benchmark datasets demonstrate that our approach exhibits superior performance in generating perceptually authentic visual details while maintaining contextual consistency compared to existing approaches.

</details>


### [6] [Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion Transformers](https://arxiv.org/abs/2508.16211)
*Shikang Zheng,Liang Feng,Xinyu Wang,Qinming Zhou,Peiliang Cai,Chang Zou,Jiacheng Liu,Yuqi Lin,Junjie Chen,Yue Ma,Linfeng Zhang*

Main category: cs.CV

TL;DR: FoCa方法通过将特征缓存视为特征-ODE求解问题，在保持高质量的前提下显著加速Diffusion Transformers推理，在多个任务上实现近无损的5-6倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有特征缓存方法在高加速比下难以保持生成质量，因为长步预测的不稳定性导致预测误差急剧增加。

Method: 从ODE视角建模隐藏特征序列，将特征缓存视为特征-ODE求解问题，提出Forecast-then-Calibrate (FoCa)方法，通过稳健地整合历史特征来解决大跳跃间隔下的退化问题。

Result: 在图像合成、视频生成和超分辨率任务上，FoCa实现了显著加速：FLUX 5.50倍、HunyuanVideo 6.45倍、Inf-DiT 3.17倍、DiT 4.53倍，且保持高质量。

Conclusion: FoCa方法无需额外训练即可实现近无损的高速推理，为Diffusion Transformers的实际部署提供了有效的加速解决方案。

Abstract: Diffusion Transformers (DiTs) have demonstrated exceptional performance in high-fidelity image and video generation. To reduce their substantial computational costs, feature caching techniques have been proposed to accelerate inference by reusing hidden representations from previous timesteps. However, current methods often struggle to maintain generation quality at high acceleration ratios, where prediction errors increase sharply due to the inherent instability of long-step forecasting. In this work, we adopt an ordinary differential equation (ODE) perspective on the hidden-feature sequence, modeling layer representations along the trajectory as a feature-ODE. We attribute the degradation of existing caching strategies to their inability to robustly integrate historical features under large skipping intervals. To address this, we propose FoCa (Forecast-then-Calibrate), which treats feature caching as a feature-ODE solving problem. Extensive experiments on image synthesis, video generation, and super-resolution tasks demonstrate the effectiveness of FoCa, especially under aggressive acceleration. Without additional training, FoCa achieves near-lossless speedups of 5.50 times on FLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high quality with a 4.53 times speedup on DiT.

</details>


### [7] [OmniCache: A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for Diffusion Transformer Models](https://arxiv.org/abs/2508.16212)
*Huanpeng Chu,Wei Wu,Guanyu Fen,Yutao Zhang*

Main category: cs.CV

TL;DR: OmniCache是一种无需训练的训练方法，通过分析扩散模型的采样轨迹和全局冗余性，在整个采样过程中智能分配缓存重用，并动态过滤噪声，实现加速采样同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散Transformer模型虽然性能强大，但采样步骤多、计算复杂，计算成本高，难以实时部署。现有缓存方法基于步骤间相似性，倾向于重用后期采样步骤，未能充分利用全局冗余性。

Method: 从DIT模型的采样视角出发，系统分析模型采样轨迹，在整个采样过程中策略性分配缓存重用。在缓存重用期间动态估计并过滤相应噪声，减少对采样方向的影响。

Result: 大量实验表明，该方法在保持竞争力的生成质量的同时加速了采样过程，为扩散生成模型的高效部署提供了实用解决方案。

Conclusion: OmniCache通过全局视角的缓存重用策略和噪声过滤机制，有效解决了扩散Transformer模型的高计算成本问题，实现了训练免费的加速效果，具有实际部署价值。

Abstract: Diffusion models have emerged as a powerful paradigm for generative tasks such as image synthesis and video generation, with Transformer architectures further enhancing performance. However, the high computational cost of diffusion Transformers-stemming from a large number of sampling steps and complex per-step computations-presents significant challenges for real-time deployment. In this paper, we introduce OmniCache, a training-free acceleration method that exploits the global redundancy inherent in the denoising process. Unlike existing methods that determine caching strategies based on inter-step similarities and tend to prioritize reusing later sampling steps, our approach originates from the sampling perspective of DIT models. We systematically analyze the model's sampling trajectories and strategically distribute cache reuse across the entire sampling process. This global perspective enables more effective utilization of cached computations throughout the diffusion trajectory, rather than concentrating reuse within limited segments of the sampling procedure.In addition, during cache reuse, we dynamically estimate the corresponding noise and filter it out to reduce its impact on the sampling direction.Extensive experiments demonstrate that our approach accelerates the sampling process while maintaining competitive generative quality, offering a promising and practical solution for efficient deployment of diffusion-based generative models.

</details>


### [8] [PromptFlare: Prompt-Generalized Defense via Cross-Attention Decoy in Diffusion-Based Inpainting](https://arxiv.org/abs/2508.16217)
*Hohyun Na,Seunghoo Hong,Simon S. Woo*

Main category: cs.CV

TL;DR: PromptFlare是一种针对扩散模型的新型对抗保护方法，通过利用交叉注意力机制和提示嵌入的内在特性，防止恶意图像修改。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的成功使得高质量图像修改变得容易，引发了恶意滥用的担忧。现有方法主要依赖图像级不一致性，难以有效处理文本提示的影响。

Method: 利用交叉注意力机制，识别并针对提示中不变且语义无信息的共享标记，注入对抗噪声来抑制采样过程，作为交叉注意力诱饵分散模型注意力。

Result: 在EditBench数据集上的广泛实验表明，该方法在各种指标上达到最先进性能，同时显著降低计算开销和GPU内存使用。

Conclusion: PromptFlare为未经授权的图像操作提供了强大而高效的保护方案。

Abstract: The success of diffusion models has enabled effortless, high-quality image modifications that precisely align with users' intentions, thereby raising concerns about their potential misuse by malicious actors. Previous studies have attempted to mitigate such misuse through adversarial attacks. However, these approaches heavily rely on image-level inconsistencies, which pose fundamental limitations in addressing the influence of textual prompts. In this paper, we propose PromptFlare, a novel adversarial protection method designed to protect images from malicious modifications facilitated by diffusion-based inpainting models. Our approach leverages the cross-attention mechanism to exploit the intrinsic properties of prompt embeddings. Specifically, we identify and target shared token of prompts that is invariant and semantically uninformative, injecting adversarial noise to suppress the sampling process. The injected noise acts as a cross-attention decoy, diverting the model's focus away from meaningful prompt-image alignments and thereby neutralizing the effect of prompt. Extensive experiments on the EditBench dataset demonstrate that our method achieves state-of-the-art performance across various metrics while significantly reducing computational overhead and GPU memory usage. These findings highlight PromptFlare as a robust and efficient protection against unauthorized image manipulations. The code is available at https://github.com/NAHOHYUN-SKKU/PromptFlare.

</details>


### [9] [UniEM-3M: A Universal Electron Micrograph Dataset for Microstructural Segmentation and Generation](https://arxiv.org/abs/2508.16239)
*Nan wang,Zhiyi Xia,Yiming Li,Shi Tang,Zuxin Fan,Xi Fang,Haoyi Tao,Xiaochen Cai,Guolin Ke,Linfeng Zhang,Yanhui Hong*

Main category: cs.CV

TL;DR: UniEM-3M是首个大规模多模态电子显微图像数据集，包含5091张高分辨率图像、约300万个实例分割标签和图像级文本描述，旨在解决材料科学中深度学习表征的数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 电子显微图像的高分辨率表征对材料科学至关重要，但深度学习方法的进展受到大规模、多样化、专家标注数据集稀缺的限制，主要由于获取成本高、隐私问题和标注复杂性。

Method: 构建了UniEM-3M数据集，包含图像、实例分割标签和文本描述；发布了基于整个数据集训练的文本到图像扩散模型用于数据增强；提出了基于流的UniEM-Net作为强基线模型进行实例分割。

Result: 定量实验表明，基于流的UniEM-Net模型在这个具有挑战性的基准测试中优于其他先进方法。

Conclusion: 通过发布部分数据集、生成模型和全面基准测试，这项工作将显著加速自动化材料分析的进展。

Abstract: Quantitative microstructural characterization is fundamental to materials science, where electron micrograph (EM) provides indispensable high-resolution insights. However, progress in deep learning-based EM characterization has been hampered by the scarcity of large-scale, diverse, and expert-annotated datasets, due to acquisition costs, privacy concerns, and annotation complexity. To address this issue, we introduce UniEM-3M, the first large-scale and multimodal EM dataset for instance-level understanding. It comprises 5,091 high-resolution EMs, about 3 million instance segmentation labels, and image-level attribute-disentangled textual descriptions, a subset of which will be made publicly available. Furthermore, we are also releasing a text-to-image diffusion model trained on the entire collection to serve as both a powerful data augmentation tool and a proxy for the complete data distribution. To establish a rigorous benchmark, we evaluate various representative instance segmentation methods on the complete UniEM-3M and present UniEM-Net as a strong baseline model. Quantitative experiments demonstrate that this flow-based model outperforms other advanced methods on this challenging benchmark. Our multifaceted release of a partial dataset, a generative model, and a comprehensive benchmark -- available at huggingface -- will significantly accelerate progress in automated materials analysis.

</details>


### [10] [Arbitrary-Scale 3D Gaussian Super-Resolution](https://arxiv.org/abs/2508.16467)
*Huimin Zeng,Yue Bai,Yun Fu*

Main category: cs.CV

TL;DR: 提出了一种支持任意尺度（包括整数和非整数）的3D高斯溅射超分辨率框架，通过尺度感知渲染、生成先验引导优化和渐进式超分辨率技术，实现单一模型的高质量实时渲染。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS超分辨率方法只能处理固定尺度因子，不适用于资源受限场景。直接渲染任意尺度会引入混叠伪影，而添加后处理上采样器会降低渲染效率。

Method: 构建集成框架，包含尺度感知渲染、生成先验引导优化和渐进式超分辨率技术，支持整数和非整数尺度渲染。

Result: 在渲染任意尺度高质量HR视图方面表现优异（比3DGS提升6.59 dB PSNR），保持与LR视图的结构一致性，同时维持实时渲染速度（1080p下85 FPS）。

Conclusion: 该方法成功解决了3DGS在任意尺度超分辨率中的挑战，实现了单一模型的高质量、高效率渲染，为资源受限场景提供了实用解决方案。

Abstract: Existing 3D Gaussian Splatting (3DGS) super-resolution methods typically perform high-resolution (HR) rendering of fixed scale factors, making them impractical for resource-limited scenarios. Directly rendering arbitrary-scale HR views with vanilla 3DGS introduces aliasing artifacts due to the lack of scale-aware rendering ability, while adding a post-processing upsampler for 3DGS complicates the framework and reduces rendering efficiency. To tackle these issues, we build an integrated framework that incorporates scale-aware rendering, generative prior-guided optimization, and progressive super-resolving to enable 3D Gaussian super-resolution of arbitrary scale factors with a single 3D model. Notably, our approach supports both integer and non-integer scale rendering to provide more flexibility. Extensive experiments demonstrate the effectiveness of our model in rendering high-quality arbitrary-scale HR views (6.59 dB PSNR gain over 3DGS) with a single model. It preserves structural consistency with LR views and across different scales, while maintaining real-time rendering speed (85 FPS at 1080p).

</details>


### [11] [Seeing Clearly, Forgetting Deeply: Revisiting Fine-Tuned Video Generators for Driving Simulation](https://arxiv.org/abs/2508.16512)
*Chun-Peng Chang,Chen-Yu Wang,Julian Schmidt,Holger Caesar,Alain Pagani*

Main category: cs.CV

TL;DR: 视频生成模型在驾驶数据集微调时存在视觉保真度提升但空间动态精度下降的权衡问题，持续学习策略可以平衡两者


<details>
  <summary>Details</summary>
Motivation: 研究现有视频生成微调方法在结构化驾驶数据集上的效果，发现视觉质量提升可能导致动态元素空间精度下降的权衡问题

Method: 分析驾驶场景的规律性和重复性特点，使用持续学习策略（如多域回放）来平衡视觉质量和动态精度

Result: 发现微调会使模型优先考虑表面真实感而非动态精度，而持续学习策略能够在保持视觉质量的同时维持空间准确性

Conclusion: 驾驶场景的特殊性导致视觉质量与动态理解目标之间的对齐偏移，需要采用平衡策略来同时保证视觉保真度和空间准确性

Abstract: Recent advancements in video generation have substantially improved visual quality and temporal coherence, making these models increasingly appealing for applications such as autonomous driving, particularly in the context of driving simulation and so-called "world models". In this work, we investigate the effects of existing fine-tuning video generation approaches on structured driving datasets and uncover a potential trade-off: although visual fidelity improves, spatial accuracy in modeling dynamic elements may degrade. We attribute this degradation to a shift in the alignment between visual quality and dynamic understanding objectives. In datasets with diverse scene structures within temporal space, where objects or perspective shift in varied ways, these objectives tend to highly correlated. However, the very regular and repetitive nature of driving scenes allows visual quality to improve by modeling dominant scene motion patterns, without necessarily preserving fine-grained dynamic behavior. As a result, fine-tuning encourages the model to prioritize surface-level realism over dynamic accuracy. To further examine this phenomenon, we show that simple continual learning strategies, such as replay from diverse domains, can offer a balanced alternative by preserving spatial accuracy while maintaining strong visual quality.

</details>


### [12] [MV-RAG: Retrieval Augmented Multiview Diffusion](https://arxiv.org/abs/2508.16577)
*Yosef Dayani,Omer Benishu,Sagie Benaim*

Main category: cs.CV

TL;DR: MV-RAG是一个新的文本到3D生成方法，通过检索真实世界的2D图像来增强多视角扩散模型，显著提高了罕见概念和域外内容的3D一致性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到3D生成方法在处理域外或罕见概念时往往产生不一致或不准确的结果，需要一种能够利用真实世界2D图像数据来增强3D生成质量的方法。

Method: 提出MV-RAG管道：首先从大规模2D图像数据库中检索相关图像，然后用这些图像条件化多视角扩散模型。采用混合训练策略，结合多视角数据和检索到的真实2D图像，通过保持视图预测目标来从2D数据推断3D一致性。

Result: 实验表明，该方法在域外/罕见概念上显著提高了3D一致性、照片真实性和文本遵循性，同时在标准基准测试中保持竞争力。

Conclusion: MV-RAG通过检索真实2D图像来增强多视角扩散模型，有效解决了文本到3D生成中域外概念的挑战，为3D内容生成提供了新的解决方案。

Abstract: Text-to-3D generation approaches have advanced significantly by leveraging pretrained 2D diffusion priors, producing high-quality and 3D-consistent outputs. However, they often fail to produce out-of-domain (OOD) or rare concepts, yielding inconsistent or inaccurate results. To this end, we propose MV-RAG, a novel text-to-3D pipeline that first retrieves relevant 2D images from a large in-the-wild 2D database and then conditions a multiview diffusion model on these images to synthesize consistent and accurate multiview outputs. Training such a retrieval-conditioned model is achieved via a novel hybrid strategy bridging structured multiview data and diverse 2D image collections. This involves training on multiview data using augmented conditioning views that simulate retrieval variance for view-specific reconstruction, alongside training on sets of retrieved real-world 2D images using a distinctive held-out view prediction objective: the model predicts the held-out view from the other views to infer 3D consistency from 2D data. To facilitate a rigorous OOD evaluation, we introduce a new collection of challenging OOD prompts. Experiments against state-of-the-art text-to-3D, image-to-3D, and personalization baselines show that our approach significantly improves 3D consistency, photorealism, and text adherence for OOD/rare concepts, while maintaining competitive performance on standard benchmarks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [13] [UnPose: Uncertainty-Guided Diffusion Priors for Zero-Shot Pose Estimation](https://arxiv.org/abs/2508.15972)
*Zhaodong Jiang,Ashish Sinha,Tongtong Cao,Yuan Ren,Bingbing Liu,Binbin Xu*

Main category: cs.RO

TL;DR: UnPose是一个零样本、无模型的6D物体姿态估计和重建框架，利用预训练扩散模型的3D先验和不确定性估计，从单视图RGB-D图像开始，通过多视图扩散模型和3D高斯溅射表示逐步优化姿态估计和3D重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统6D姿态估计方法依赖物体CAD模型，但获取成本高且不实用。现有方法虽然尝试利用基础模型从图像重建物体，但通常需要额外训练或产生幻觉几何。

Method: 从单视图RGB-D帧开始，使用多视图扩散模型估计初始3D模型（3D高斯溅射表示）和像素级认知不确定性。随着新观测可用，在扩散模型不确定性指导下增量融合新视图，并通过位姿图优化确保全局一致性。

Result: 大量实验表明，UnPose在6D姿态估计精度和3D重建质量方面显著优于现有方法，并在真实机器人操作任务中展示了实际应用性。

Conclusion: UnPose提供了一个有效的零样本解决方案，成功利用扩散模型的3D先验和不确定性估计来实现高质量的6D姿态估计和3D重建，无需物体CAD模型或额外训练。

Abstract: Estimating the 6D pose of novel objects is a fundamental yet challenging problem in robotics, often relying on access to object CAD models. However, acquiring such models can be costly and impractical. Recent approaches aim to bypass this requirement by leveraging strong priors from foundation models to reconstruct objects from single or multi-view images, but typically require additional training or produce hallucinated geometry. To this end, we propose UnPose, a novel framework for zero-shot, model-free 6D object pose estimation and reconstruction that exploits 3D priors and uncertainty estimates from a pre-trained diffusion model. Specifically, starting from a single-view RGB-D frame, UnPose uses a multi-view diffusion model to estimate an initial 3D model using 3D Gaussian Splatting (3DGS) representation, along with pixel-wise epistemic uncertainty estimates. As additional observations become available, we incrementally refine the 3DGS model by fusing new views guided by the diffusion model's uncertainty, thereby continuously improving the pose estimation accuracy and 3D reconstruction quality. To ensure global consistency, the diffusion prior-generated views and subsequent observations are further integrated in a pose graph and jointly optimized into a coherent 3DGS field. Extensive experiments demonstrate that UnPose significantly outperforms existing approaches in both 6D pose estimation accuracy and 3D reconstruction quality. We further showcase its practical applicability in real-world robotic manipulation tasks.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [14] [Lightweight and Fast Real-time Image Enhancement via Decomposition of the Spatial-aware Lookup Tables](https://arxiv.org/abs/2508.16121)
*Wontae Kim,Keuntek Lee,Nam Ik Cho*

Main category: eess.IV

TL;DR: 通过对3D LUT进行奇异值分解和缓存优化，提出了一种空间感知且高效的图像增强方法，在保持性能的同时大幅减少参数数量和运行时间。


<details>
  <summary>Details</summary>
Motivation: 现有的3D LUT图像增强方法存在两大问题：缺乏空间信息，以及空间感知方法参数量大、运行时间随图像分辨率增加而增加。需要一种方法在保持空间感知能力的同时提高效率。

Method: 使用奇异值分解(SVD)将3D LUT分解为低维度LUT的线性组合，重点关注表格中的冗余部分。同时改进空间特征融合模块，使其更加缓存效果。通过这种分解和优化方式生成图像适配性LUT。

Result: 实验结果表明，该模型能够有效减少参数数量和运行时间，同时保持了空间感知能力和图像增强性能。

Conclusion: 该方法通过对3D LUT进行效率优化，成功解决了空间感知3D LUT方法的参数和计算效率问题，在保持高性能的前提下实现了更高效的图像增强。

Abstract: The image enhancement methods based on 3D lookup tables (3D LUTs) efficiently reduce both model size and runtime by interpolating pre-calculated values at the vertices. However, the 3D LUT methods have a limitation due to their lack of spatial information, as they convert color values on a point-by-point basis. Although spatial-aware 3D LUT methods address this limitation, they introduce additional modules that require a substantial number of parameters, leading to increased runtime as image resolution increases. To address this issue, we propose a method for generating image-adaptive LUTs by focusing on the redundant parts of the tables. Our efficient framework decomposes a 3D LUT into a linear sum of low-dimensional LUTs and employs singular value decomposition (SVD). Furthermore, we enhance the modules for spatial feature fusion to be more cache-efficient. Extensive experimental results demonstrate that our model effectively decreases both the number of parameters and runtime while maintaining spatial awareness and performance.

</details>


### [15] [Towards Diagnostic Quality Flat-Panel Detector CT Imaging Using Diffusion Models](https://arxiv.org/abs/2508.16252)
*Hélène Corbaz,Anh Nguyen,Victor Schulze-Zachau,Paul Friedrich,Alicia Durrer,Florentin Bieder,Philippe C. Cattin,Marios N Psychogios*

Main category: eess.IV

TL;DR: 使用去噪扩散概率模型(DDPM)提升平板探测器CT(FDCT)图像质量，使其接近多探测器CT(MDCT)水平，减少患者移动需求


<details>
  <summary>Details</summary>
Motivation: 机械取栓手术患者通常需要在MDCT和FDCT之间移动，FDCT图像质量较差但使用更方便，希望通过AI技术提升FDCT图像质量

Method: 采用去噪扩散概率模型(DDPM)对FDCT图像进行去噪和增强处理，消除伪影并提高解剖结构可见性

Result: DDPM成功消除了大部分伪影，提高了解剖可见性，且不影响出血检测能力（前提是输入FDCT图像质量不过低）

Conclusion: DDPM能有效提升FDCT图像质量，使其在诊断价值上接近MDCT，有望减少患者移动需求，改善临床工作流程

Abstract: Patients undergoing a mechanical thrombectomy procedure usually have a multi-detector CT (MDCT) scan before and after the intervention. The image quality of the flat panel detector CT (FDCT) present in the intervention room is generally much lower than that of a MDCT due to significant artifacts. However, using only FDCT images could improve patient management as the patient would not need to be moved to the MDCT room. Several studies have evaluated the potential use of FDCT imaging alone and the time that could be saved by acquiring the images before and/or after the intervention only with the FDCT. This study proposes using a denoising diffusion probabilistic model (DDPM) to improve the image quality of FDCT scans, making them comparable to MDCT scans. Clinicans evaluated FDCT, MDCT, and our model's predictions for diagnostic purposes using a questionnaire. The DDPM eliminated most artifacts and improved anatomical visibility without reducing bleeding detection, provided that the input FDCT image quality is not too low. Our code can be found on github.

</details>


### [16] [Decoding MGMT Methylation: A Step Towards Precision Medicine in Glioblastoma](https://arxiv.org/abs/2508.16424)
*Hafeez Ur Rehman,Sumaiya Fazal,Moutaz Alazab,Ali Baydoun*

Main category: eess.IV

TL;DR: CAMP框架通过自适应稀疏惩罚的卷积自编码器，从MRI图像预测胶质母细胞瘤MGMT甲基化状态，准确率达0.97，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 胶质母细胞瘤治疗困难，MGMT基因甲基化状态是重要生物标志物，但现有非侵入性影像技术预测准确性不足，需要更精确的预测方法

Method: 两阶段框架：1) 定制自编码器生成合成MRI切片，保留不同模态的精细组织结构；2) 卷积神经网络结合自适应稀疏惩罚预测甲基化状态，动态适应数据变化

Result: 在基准数据集上达到准确率0.97、特异性0.98、敏感性0.97，在MRI图像合成和甲基化状态预测方面均表现优异

Conclusion: CAMP框架能显著改善MRI数据解读，为胶质母细胞瘤患者提供更个性化的治疗策略，具有重要临床应用价值

Abstract: Glioblastomas, constituting over 50% of malignant brain tumors, are highly aggressive brain tumors that pose substantial treatment challenges due to their rapid progression and resistance to standard therapies. The methylation status of the O-6-Methylguanine-DNA Methyltransferase (MGMT) gene is a critical biomarker for predicting patient response to treatment, particularly with the alkylating agent temozolomide. However, accurately predicting MGMT methylation status using non-invasive imaging techniques remains challenging due to the complex and heterogeneous nature of glioblastomas, that includes, uneven contrast, variability within lesions, and irregular enhancement patterns. This study introduces the Convolutional Autoencoders for MGMT Methylation Status Prediction (CAMP) framework, which is based on adaptive sparse penalties to enhance predictive accuracy. The CAMP framework operates in two phases: first, generating synthetic MRI slices through a tailored autoencoder that effectively captures and preserves intricate tissue and tumor structures across different MRI modalities; second, predicting MGMT methylation status using a convolutional neural network enhanced by adaptive sparse penalties. The adaptive sparse penalty dynamically adjusts to variations in the data, such as contrast differences and tumor locations in MR images. Our method excels in MRI image synthesis, preserving brain tissue, fat, and individual tumor structures across all MRI modalities. Validated on benchmark datasets, CAMP achieved an accuracy of 0.97, specificity of 0.98, and sensitivity of 0.97, significantly outperforming existing methods. These results demonstrate the potential of the CAMP framework to improve the interpretation of MRI data and contribute to more personalized treatment strategies for glioblastoma patients.

</details>


### [17] [Time-Aware One Step Diffusion Network for Real-World Image Super-Resolution](https://arxiv.org/abs/2508.16557)
*Tainyi Zhang,Zheng-Peng Duan,Peng-Tao Jiang,Bo Li,Ming-Ming Cheng,Chun-Le Guo,Chongyi Li*

Main category: eess.IV

TL;DR: 提出了TADSR方法，通过时间感知机制充分利用SD模型在不同时间步的生成先验，实现单步超分辨率并支持保真度与真实性的可控权衡


<details>
  <summary>Details</summary>
Motivation: 现有基于VSD的方法使用固定时间步难以充分利用SD模型在不同噪声注入时间步的不同生成先验，导致性能次优

Method: 提出时间感知VAE编码器将图像投影到不同时间步的潜在特征，以及时间感知VSD损失来桥接学生模型和教师模型的时间步

Result: 实验结果表明该方法在单步情况下实现了最先进的性能和可控的超分辨率结果

Conclusion: TADSR方法通过时间感知机制有效利用了SD模型的生成能力，实现了高效且可控的真实图像超分辨率

Abstract: Diffusion-based real-world image super-resolution (Real-ISR) methods have demonstrated impressive performance. To achieve efficient Real-ISR, many works employ Variational Score Distillation (VSD) to distill pre-trained stable-diffusion (SD) model for one-step SR with a fixed timestep. However, due to the different noise injection timesteps, the SD will perform different generative priors. Therefore, a fixed timestep is difficult for these methods to fully leverage the generative priors in SD, leading to suboptimal performance. To address this, we propose a Time-Aware one-step Diffusion Network for Real-ISR (TADSR). We first introduce a Time-Aware VAE Encoder, which projects the same image into different latent features based on timesteps. Through joint dynamic variation of timesteps and latent features, the student model can better align with the input pattern distribution of the pre-trained SD, thereby enabling more effective utilization of SD's generative capabilities. To better activate the generative prior of SD at different timesteps, we propose a Time-Aware VSD loss that bridges the timesteps of the student model and those of the teacher model, thereby producing more consistent generative prior guidance conditioned on timesteps. Additionally, though utilizing the generative prior in SD at different timesteps, our method can naturally achieve controllable trade-offs between fidelity and realism by changing the timestep condition. Experimental results demonstrate that our method achieves both state-of-the-art performance and controllable SR results with only a single step.

</details>
