<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 5]
- [cs.CV](#cs.CV) [Total: 20]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.ET](#cs.ET) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [SOF: Sorted Opacity Fields for Fast Unbounded Surface Reconstruction](https://arxiv.org/abs/2506.19139)
*Lukas Radl,Felix Windisch,Thomas Deixelberger,Jozef Hladky,Michael Steiner,Dieter Schmalstieg,Markus Steinberger*

Main category: cs.GR

TL;DR: SOF方法通过层次化重排序和鲁棒的Gaussian深度公式，从3D高斯中恢复精细表面，提升重建精度和速度。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖近似深度估计和全局排序启发式，导致重建网格的保真度受限，SOF旨在解决这一问题。

Method: 引入层次化重排序、鲁棒的Gaussian深度公式、水平集正则化器和几何一致性损失，并开发并行化的Marching Tetrahedra算法。

Result: SOF在重建精度上表现更优，总处理时间减少超过三倍。

Conclusion: SOF将高效的高斯渲染转化为高效的几何提取，标志着该领域的进步。

Abstract: Recent advances in 3D Gaussian representations have significantly improved the quality and efficiency of image-based scene reconstruction. Their explicit nature facilitates real-time rendering and fast optimization, yet extracting accurate surfaces - particularly in large-scale, unbounded environments - remains a difficult task. Many existing methods rely on approximate depth estimates and global sorting heuristics, which can introduce artifacts and limit the fidelity of the reconstructed mesh. In this paper, we present Sorted Opacity Fields (SOF), a method designed to recover detailed surfaces from 3D Gaussians with both speed and precision. Our approach improves upon prior work by introducing hierarchical resorting and a robust formulation of Gaussian depth, which better aligns with the level-set. To enhance mesh quality, we incorporate a level-set regularizer operating on the opacity field and introduce losses that encourage geometrically-consistent primitive shapes. In addition, we develop a parallelized Marching Tetrahedra algorithm tailored to our opacity formulation, reducing meshing time by up to an order of magnitude. As demonstrated by our quantitative evaluation, SOF achieves higher reconstruction accuracy while cutting total processing time by more than a factor of three. These results mark a step forward in turning efficient Gaussian-based rendering into equally efficient geometry extraction.

</details>


### [2] [Style Transfer: A Decade Survey](https://arxiv.org/abs/2506.19278)
*Tianshan Zhang,Hao Tang*

Main category: cs.GR

TL;DR: 本文综述了AIGC技术在视觉艺术中的发展，分析了VAE、GANs和Diffusion Models三大范式，并提出了多维评估框架，揭示了其潜力与局限。


<details>
  <summary>Details</summary>
Motivation: 探讨AIGC技术在视觉艺术中的机制与美学影响，填补当前理解的不足。

Method: 系统回顾500多篇论文，分析VAE、GANs和Diffusion Models，提出多维评估框架。

Result: 揭示了AIGC系统的变革能力与局限，强调其对未来创意实践的影响。

Conclusion: 为人工智能与艺术表达的融合提供统一视角，并指出未来研究方向。

Abstract: The revolutionary advancement of Artificial Intelligence Generated Content (AIGC) has fundamentally transformed the landscape of visual content creation and artistic expression. While remarkable progress has been made in image generation and style transfer, the underlying mechanisms and aesthetic implications of these technologies remain insufficiently understood. This paper presents a comprehensive survey of AIGC technologies in visual arts, tracing their evolution from early algorithmic frameworks to contemporary deep generative models. We identify three pivotal paradigms: Variational Autoencoders (VAE), Generative Adversarial Networks (GANs), and Diffusion Models, and examine their roles in bridging the gap between human creativity and machine synthesis. To support our analysis, we systematically review over 500 research papers published in the past decade, spanning both foundational developments and state-of-the-art innovations. Furthermore, we propose a multidimensional evaluation framework that incorporates Technical Innovation, Artistic Merit, Visual Quality, Computational Efficiency, and Creative Potential. Our findings reveal both the transformative capacities and current limitations of AIGC systems, emphasizing their profound impact on the future of creative practices. Through this extensive synthesis, we offer a unified perspective on the convergence of artificial intelligence and artistic expression, while outlining key challenges and promising directions for future research in this rapidly evolving field.

</details>


### [3] [Continuous Indexed Points for Multivariate Volume Visualization](https://arxiv.org/abs/2506.19400)
*Liang Zhou,Xinyi Gou,Daniel Weiskopf*

Main category: cs.GR

TL;DR: 提出了一种基于连续索引点的多变量体积可视化方法，通过局部线性拟合和主成分分析，在平行坐标中编码多变量数据的局部相关性，支持交互式探索。


<details>
  <summary>Details</summary>
Motivation: 多变量体积数据的可视化需要更高效的方法来捕捉局部相关性，传统方法难以满足需求。

Method: 使用主成分分析进行局部线性拟合，并通过层次空间数据结构加速，生成连续索引点在平行坐标中可视化。

Result: 方法能够有效分析多变量数据的局部相关性，支持交互式探索，并通过案例研究和用户反馈验证了其有效性。

Conclusion: 该方法为多变量体积数据的可视化提供了一种高效且通用的解决方案。

Abstract: We introduce continuous indexed points for improved multivariate volume visualization. Indexed points represent linear structures in parallel coordinates and can be used to encode local correlation of multivariate (including multifield, multifaceted, and multiattribute) volume data. First, we perform local linear fitting in the spatial neighborhood of each volume sample using principal component analysis, accelerated by hierarchical spatial data structures. This local linear information is then visualized as continuous indexed points in parallel coordinates: a density representation of indexed points in a continuous domain. With our new method, multivariate volume data can be analyzed using the eigenvector information from local spatial embeddings. We utilize both 1-flat and 2-flat indexed points, allowing us to identify correlations between two variables and even three variables, respectively. An interactive occlusion shading model facilitates good spatial perception of the volume rendering of volumetric correlation characteristics. Interactive exploration is supported by specifically designed multivariate transfer function widgets working in the image plane of parallel coordinates. We show that our generic technique works for multi-attribute datasets. The effectiveness and usefulness of our new method is demonstrated through a case study, an expert user study, and domain expert feedback.

</details>


### [4] [Virtual Memory for 3D Gaussian Splatting](https://arxiv.org/abs/2506.19415)
*Jonathan Haberl,Philipp Fleck,Clemens Arth*

Main category: cs.GR

TL;DR: 提出了一种利用虚拟内存技术高效渲染大规模3D高斯泼溅场景的方法，通过动态流式传输可见高斯元素，减少内存占用并加速渲染。


<details>
  <summary>Details</summary>
Motivation: 解决大规模3D高斯泼溅场景在存储和渲染时的高内存需求问题，提升复杂场景的实时渲染效率。

Method: 结合虚拟内存和虚拟纹理技术，动态识别并流式传输可见高斯元素，同时集成细节层次（LOD）优化渲染速度。

Result: 显著减少了内存使用，加速了渲染过程，特别是在复杂场景中，并在桌面和移动设备上进行了验证。

Conclusion: 该方法为大规模3D高斯泼溅场景的实时渲染提供了高效解决方案，具有实际应用潜力。

Abstract: 3D Gaussian Splatting represents a breakthrough in the field of novel view synthesis. It establishes Gaussians as core rendering primitives for highly accurate real-world environment reconstruction. Recent advances have drastically increased the size of scenes that can be created. In this work, we present a method for rendering large and complex 3D Gaussian Splatting scenes using virtual memory. By leveraging well-established virtual memory and virtual texturing techniques, our approach efficiently identifies visible Gaussians and dynamically streams them to the GPU just in time for real-time rendering. Selecting only the necessary Gaussians for both storage and rendering results in reduced memory usage and effectively accelerates rendering, especially for highly complex scenes. Furthermore, we demonstrate how level of detail can be integrated into our proposed method to further enhance rendering speed for large-scale scenes. With an optimized implementation, we highlight key practical considerations and thoroughly evaluate the proposed technique and its impact on desktop and mobile devices.

</details>


### [5] [Uncovering Conceptual Blindspots in Generative Image Models Using Sparse Autoencoders](https://arxiv.org/abs/2506.19708)
*Matyas Bohacek,Thomas Fel,Maneesh Agrawala,Ekdeep Singh Lubana*

Main category: cs.GR

TL;DR: 论文提出了一种系统方法，通过稀疏自编码器（SAEs）识别生成图像模型中的“概念盲点”，即训练数据中存在但在生成图像中缺失或错误表现的概念。


<details>
  <summary>Details</summary>
Motivation: 尽管生成图像模型在大规模数据集上表现优异，但在生成某些简单概念（如人手或四件一组物体）时仍存在明显缺陷。这些缺陷是否反映模型的结构性限制尚不明确。

Method: 利用稀疏自编码器（SAEs）提取可解释的概念嵌入，通过量化比较真实与生成图像中概念的分布差异，识别盲点。训练了一个包含32,000个概念的RA-SAE模型。

Result: 在四种流行生成模型（Stable Diffusion 1.5/2.1、PixArt、Kandinsky）中发现了特定被抑制或夸大的盲点（如鸟食器、DVD光盘、木质背景纹理）。

Conclusion: 提出了一个理论框架，通过评估生成模型与数据生成过程的概念保真度，系统性识别概念盲点。

Abstract: Despite their impressive performance, generative image models trained on large-scale datasets frequently fail to produce images with seemingly simple concepts -- e.g., human hands or objects appearing in groups of four -- that are reasonably expected to appear in the training data. These failure modes have largely been documented anecdotally, leaving open the question of whether they reflect idiosyncratic anomalies or more structural limitations of these models. To address this, we introduce a systematic approach for identifying and characterizing "conceptual blindspots" -- concepts present in the training data but absent or misrepresented in a model's generations. Our method leverages sparse autoencoders (SAEs) to extract interpretable concept embeddings, enabling a quantitative comparison of concept prevalence between real and generated images. We train an archetypal SAE (RA-SAE) on DINOv2 features with 32,000 concepts -- the largest such SAE to date -- enabling fine-grained analysis of conceptual disparities. Applied to four popular generative models (Stable Diffusion 1.5/2.1, PixArt, and Kandinsky), our approach reveals specific suppressed blindspots (e.g., bird feeders, DVD discs, and whitespaces on documents) and exaggerated blindspots (e.g., wood background texture and palm trees). At the individual datapoint level, we further isolate memorization artifacts -- instances where models reproduce highly specific visual templates seen during training. Overall, we propose a theoretically grounded framework for systematically identifying conceptual blindspots in generative models by assessing their conceptual fidelity with respect to the underlying data-generating process.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [6] [Diffusion Transformer-to-Mamba Distillation for High-Resolution Image Generation](https://arxiv.org/abs/2506.18999)
*Yuan Yao,Yicong Hong,Difan Liu,Long Mai,Feng Liu,Jiebo Luo*

Main category: cs.CV

TL;DR: 论文提出了一种从扩散Transformer到Mamba的蒸馏方法（T2MD），以解决高分辨率图像生成中的计算成本问题，同时保持全局依赖性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决扩散Transformer中自注意力机制的高计算复杂度问题，同时探索Mamba模型在非因果视觉生成中的潜力。

Method: 提出T2MD方法，结合自注意力和Mamba的混合模型，采用层级教师强制和基于特征的知识蒸馏技术。

Result: 实验表明，该方法在512×512分辨率下训练后，可扩展到2048×2048分辨率，实现高质量图像生成。

Conclusion: T2MD证明了Mamba模型在非因果视觉生成中的可行性，为未来研究提供了方向。

Abstract: The quadratic computational complexity of self-attention in diffusion transformers (DiT) introduces substantial computational costs in high-resolution image generation. While the linear-complexity Mamba model emerges as a potential alternative, direct Mamba training remains empirically challenging. To address this issue, this paper introduces diffusion transformer-to-mamba distillation (T2MD), forming an efficient training pipeline that facilitates the transition from the self-attention-based transformer to the linear complexity state-space model Mamba. We establish a diffusion self-attention and Mamba hybrid model that simultaneously achieves efficiency and global dependencies. With the proposed layer-level teacher forcing and feature-based knowledge distillation, T2MD alleviates the training difficulty and high cost of a state space model from scratch. Starting from the distilled 512$\times$512 resolution base model, we push the generation towards 2048$\times$2048 images via lightweight adaptation and high-resolution fine-tuning. Experiments demonstrate that our training path leads to low overhead but high-quality text-to-image generation. Importantly, our results also justify the feasibility of using sequential and causal Mamba models for generating non-causal visual output, suggesting the potential for future exploration.

</details>


### [7] [HAWAII: Hierarchical Visual Knowledge Transfer for Efficient Vision-Language Models](https://arxiv.org/abs/2506.19072)
*Yimu Wang,Mozhgan Nasr Azadani,Sean Sedwards,Krzysztof Czarnecki*

Main category: cs.CV

TL;DR: HAWAII框架通过将多个视觉专家的知识蒸馏到单一视觉编码器中，减少计算开销，同时保留互补优势。


<details>
  <summary>Details</summary>
Motivation: 提升视觉语言模型的视觉理解能力，同时避免多专家系统的高计算成本。

Method: 使用教师特定的LoRA适配器和路由器，结合细粒度和粗粒度蒸馏策略。

Result: 在多种视觉语言任务中表现优于主流开源视觉语言模型。

Conclusion: HAWAII框架高效且性能优越，适用于视觉语言模型的知识蒸馏。

Abstract: Improving the visual understanding ability of vision-language models (VLMs) is crucial for enhancing their performance across various tasks. While using multiple pretrained visual experts has shown great promise, it often incurs significant computational costs during training and inference. To address this challenge, we propose HAWAII, a novel framework that distills knowledge from multiple visual experts into a single vision encoder, enabling it to inherit the complementary strengths of several experts with minimal computational overhead. To mitigate conflicts among different teachers and switch between different teacher-specific knowledge, instead of using a fixed set of adapters for multiple teachers, we propose to use teacher-specific Low-Rank Adaptation (LoRA) adapters with a corresponding router. Each adapter is aligned with a specific teacher, avoiding noisy guidance during distillation. To enable efficient knowledge distillation, we propose fine-grained and coarse-grained distillation. At the fine-grained level, token importance scores are employed to emphasize the most informative tokens from each teacher adaptively. At the coarse-grained level, we summarize the knowledge from multiple teachers and transfer it to the student using a set of general-knowledge LoRA adapters with a router. Extensive experiments on various vision-language tasks demonstrate the superiority of HAWAII, compared to the popular open-source VLMs.

</details>


### [8] [PrITTI: Primitive-based Generation of Controllable and Editable 3D Semantic Scenes](https://arxiv.org/abs/2506.19117)
*Christina Ourania Tze,Daniel Dauner,Yiyi Liao,Dzmitry Tsishkou,Andreas Geiger*

Main category: cs.CV

TL;DR: PrITTI是一种基于潜在扩散的框架，利用3D基元生成可控、可编辑的3D语义场景布局，优于传统体素方法。


<details>
  <summary>Details</summary>
Motivation: 传统体素表示内存密集、分辨率固定且难以编辑，而基元表示更紧凑、易于操作，适合大规模3D场景生成。

Method: 采用混合表示，地面以栅格化形式建模，对象以向量化3D基元编码，并引入稳定的Cholesky参数化解码对象尺寸和方向。

Result: 在KITTI-360数据集上，PrITTI生成质量优于体素基线，内存需求降低3倍，支持实例级操作和多种下游应用。

Conclusion: PrITTI证明了基元表示在3D场景生成中的高效性和灵活性，为可控场景编辑提供了新思路。

Abstract: Large-scale 3D semantic scene generation has predominantly relied on voxel-based representations, which are memory-intensive, bound by fixed resolutions, and challenging to edit. In contrast, primitives represent semantic entities using compact, coarse 3D structures that are easy to manipulate and compose, making them an ideal representation for this task. In this paper, we introduce PrITTI, a latent diffusion-based framework that leverages primitives as the main foundational elements for generating compositional, controllable, and editable 3D semantic scene layouts. Our method adopts a hybrid representation, modeling ground surfaces in a rasterized format while encoding objects as vectorized 3D primitives. This decomposition is also reflected in a structured latent representation that enables flexible scene manipulation of ground and object components. To overcome the orientation ambiguities in conventional encoding methods, we introduce a stable Cholesky-based parameterization that jointly encodes object size and orientation. Experiments on the KITTI-360 dataset show that PrITTI outperforms a voxel-based baseline in generation quality, while reducing memory requirements by up to $3\times$. In addition, PrITTI enables direct instance-level manipulation of objects in the scene and supports a range of downstream applications, including scene inpainting, outpainting, and photo-realistic street-view synthesis.

</details>


### [9] [Automated Image Recognition Framework](https://arxiv.org/abs/2506.19261)
*Quang-Binh Nguyen,Trong-Vu Hoang,Ngoc-Do Tran,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: 提出了一种基于生成AI的自动图像识别框架（AIR），通过生成高质量预标注数据集和自动训练模型，解决了数据收集和标注的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决特定任务中数据收集和标注的高成本问题，尤其是针对新颖或敏感主题时缺乏相关数据集的情况。

Method: 包括AIR-Gen（生成定制数据集）和AIR-Aug（增强现有数据集），结合自动提示工程和分布调整算法。

Result: 实验证明生成的数据能有效训练深度学习模型，用户研究显示AIR获得4.4/5的高评分。

Conclusion: AIR框架为图像识别任务提供了一种高效、自动化的解决方案，具有广泛的应用潜力。

Abstract: While the efficacy of deep learning models heavily relies on data, gathering and annotating data for specific tasks, particularly when addressing novel or sensitive subjects lacking relevant datasets, poses significant time and resource challenges. In response to this, we propose a novel Automated Image Recognition (AIR) framework that harnesses the power of generative AI. AIR empowers end-users to synthesize high-quality, pre-annotated datasets, eliminating the necessity for manual labeling. It also automatically trains deep learning models on the generated datasets with robust image recognition performance. Our framework includes two main data synthesis processes, AIR-Gen and AIR-Aug. The AIR-Gen enables end-users to seamlessly generate datasets tailored to their specifications. To improve image quality, we introduce a novel automated prompt engineering module that leverages the capabilities of large language models. We also introduce a distribution adjustment algorithm to eliminate duplicates and outliers, enhancing the robustness and reliability of generated datasets. On the other hand, the AIR-Aug enhances a given dataset, thereby improving the performance of deep classifier models. AIR-Aug is particularly beneficial when users have limited data for specific tasks. Through comprehensive experiments, we demonstrated the efficacy of our generated data in training deep learning models and showcased the system's potential to provide image recognition models for a wide range of objects. We also conducted a user study that achieved an impressive score of 4.4 out of 5.0, underscoring the AI community's positive perception of AIR.

</details>


### [10] [HoliGS: Holistic Gaussian Splatting for Embodied View Synthesis](https://arxiv.org/abs/2506.19291)
*Xiaoyuan Wang,Yizhou Zhao,Botao Ye,Xiaojun Shan,Weijie Lyu,Lu Qi,Kelvin C. K. Chan,Yinxiao Li,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: HoliGS是一种新颖的可变形高斯溅射框架，用于从长单目RGB视频中进行实体化视图合成，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有4D高斯溅射和动态NeRF方法在长时间捕获中训练开销大的问题。

Method: 将场景分解为静态背景和时变对象，通过可逆高斯溅射变形网络实现全局刚性变换、骨架驱动关节和细微非刚性变形。

Result: 在挑战性数据集上实现更高质量的重建，同时显著减少训练和渲染时间。

Conclusion: HoliGS为现实场景中的实体化视图合成提供了实用且可扩展的解决方案。

Abstract: We propose HoliGS, a novel deformable Gaussian splatting framework that addresses embodied view synthesis from long monocular RGB videos. Unlike prior 4D Gaussian splatting and dynamic NeRF pipelines, which struggle with training overhead in minute-long captures, our method leverages invertible Gaussian Splatting deformation networks to reconstruct large-scale, dynamic environments accurately. Specifically, we decompose each scene into a static background plus time-varying objects, each represented by learned Gaussian primitives undergoing global rigid transformations, skeleton-driven articulation, and subtle non-rigid deformations via an invertible neural flow. This hierarchical warping strategy enables robust free-viewpoint novel-view rendering from various embodied camera trajectories by attaching Gaussians to a complete canonical foreground shape (\eg, egocentric or third-person follow), which may involve substantial viewpoint changes and interactions between multiple actors. Our experiments demonstrate that \ourmethod~ achieves superior reconstruction quality on challenging datasets while significantly reducing both training and rendering time compared to state-of-the-art monocular deformable NeRFs. These results highlight a practical and scalable solution for EVS in real-world scenarios. The source code will be released.

</details>


### [11] [Training-Free Motion Customization for Distilled Video Generators with Adaptive Test-Time Distillation](https://arxiv.org/abs/2506.19348)
*Jintao Rong,Xin Xie,Xinyi Yu,Linlin Ou,Xinyu Zhang,Chunhua Shen,Dong Gong*

Main category: cs.CV

TL;DR: MotionEcho是一种无需训练的方法，通过教师模型指导学生模型，提升蒸馏视频生成模型在运动定制上的表现。


<details>
  <summary>Details</summary>
Motivation: 蒸馏视频生成模型在快速合成方面表现优异，但在参考视频引导的运动定制上存在困难，尤其是在无需训练的情况下。现有方法无法适应蒸馏模型的加速生成过程和大去噪步长。

Method: 提出MotionEcho框架，利用扩散教师强制技术，通过端点预测和插值，用高质量慢速教师模型指导快速学生模型的推理。动态分配计算资源以保持效率。

Result: 在多种蒸馏视频生成模型和基准数据集上的实验表明，该方法显著提高了运动保真度和生成质量，同时保持了高效率。

Conclusion: MotionEcho在无需训练的情况下，有效解决了蒸馏视频生成模型的运动定制问题，提升了生成效果和效率。

Abstract: Distilled video generation models offer fast and efficient synthesis but struggle with motion customization when guided by reference videos, especially under training-free settings. Existing training-free methods, originally designed for standard diffusion models, fail to generalize due to the accelerated generative process and large denoising steps in distilled models. To address this, we propose MotionEcho, a novel training-free test-time distillation framework that enables motion customization by leveraging diffusion teacher forcing. Our approach uses high-quality, slow teacher models to guide the inference of fast student models through endpoint prediction and interpolation. To maintain efficiency, we dynamically allocate computation across timesteps according to guidance needs. Extensive experiments across various distilled video generation models and benchmark datasets demonstrate that our method significantly improves motion fidelity and generation quality while preserving high efficiency. Project page: https://euminds.github.io/motionecho/

</details>


### [12] [Generate the Forest before the Trees -- A Hierarchical Diffusion model for Climate Downscaling](https://arxiv.org/abs/2506.19391)
*Declan J. Curran,Sanaa Hobeichi,Hira Saleem,Hao Xue,Flora D. Salim*

Main category: cs.CV

TL;DR: 论文提出了一种名为HDD的分层扩散降尺度模型，显著降低了计算负担，同时保持了高精度。


<details>
  <summary>Details</summary>
Motivation: 传统降尺度方法计算量大，AI降尺度模型（如扩散模型）虽有效但仍计算密集，需要更轻量化的解决方案。

Method: 引入分层采样过程，采用从粗到细的层次结构，通过简单的下采样方案实现。

Result: 在ERA5和CMIP6数据集上表现优异，计算负载显著降低，且模型可跨不同分辨率迁移。

Conclusion: HDD为概率气候降尺度提供了轻量化替代方案，支持大规模高分辨率气候预测。

Abstract: Downscaling is essential for generating the high-resolution climate data needed for local planning, but traditional methods remain computationally demanding. Recent years have seen impressive results from AI downscaling models, particularly diffusion models, which have attracted attention due to their ability to generate ensembles and overcome the smoothing problem common in other AI methods. However, these models typically remain computationally intensive. We introduce a Hierarchical Diffusion Downscaling (HDD) model, which introduces an easily-extensible hierarchical sampling process to the diffusion framework. A coarse-to-fine hierarchy is imposed via a simple downsampling scheme. HDD achieves competitive accuracy on ERA5 reanalysis datasets and CMIP6 models, significantly reducing computational load by running on up to half as many pixels with competitive results. Additionally, a single model trained at 0.25{\deg} resolution transfers seamlessly across multiple CMIP6 models with much coarser resolution. HDD thus offers a lightweight alternative for probabilistic climate downscaling, facilitating affordable large-ensemble high-resolution climate projections. See a full code implementation at: https://github.com/HDD-Hierarchical-Diffusion-Downscaling/HDD-Hierarchical-Diffusion-Downscaling.

</details>


### [13] [A Global-Local Cross-Attention Network for Ultra-high Resolution Remote Sensing Image Semantic Segmentation](https://arxiv.org/abs/2506.19406)
*Chen Yi,Shan LianLei*

Main category: cs.CV

TL;DR: GLCANet是一种轻量级分割框架，用于超高分辨率遥感图像，通过双流架构和注意力机制提升语义分割的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 超高分辨率遥感技术的发展对准确高效的语义分割需求增加，现有方法在计算效率和多尺度特征融合方面存在挑战。

Method: GLCANet采用双流架构和自注意力机制，结合全局语义与局部细节，并通过掩码交叉注意力机制自适应融合特征。

Result: 实验表明GLCANet在准确性和计算效率上优于现有方法，能高效处理大尺寸高分辨率图像。

Conclusion: GLCANet为实际遥感应用提供了高效的解决方案。

Abstract: With the rapid development of ultra-high resolution (UHR) remote sensing technology, the demand for accurate and efficient semantic segmentation has increased significantly. However, existing methods face challenges in computational efficiency and multi-scale feature fusion. To address these issues, we propose GLCANet (Global-Local Cross-Attention Network), a lightweight segmentation framework designed for UHR remote sensing imagery.GLCANet employs a dual-stream architecture to efficiently fuse global semantics and local details while minimizing GPU usage. A self-attention mechanism enhances long-range dependencies, refines global features, and preserves local details for better semantic consistency. A masked cross-attention mechanism also adaptively fuses global-local features, selectively enhancing fine-grained details while exploiting global context to improve segmentation accuracy. Experimental results show that GLCANet outperforms state-of-the-art methods regarding accuracy and computational efficiency. The model effectively processes large, high-resolution images with a small memory footprint, providing a promising solution for real-world remote sensing applications.

</details>


### [14] [Deblurring in the Wild: A Real-World Dataset from Smartphone High-Speed Videos](https://arxiv.org/abs/2506.19445)
*Mahdi Mohd Hossain Noki,Syed Mumtahin Mahmud,Prothito Shovon Majumder,Abdul Mohaimen Al Radi,Md. Haider Ali,Md. Mosaddek Khan*

Main category: cs.CV

TL;DR: 该论文提出了一个基于智能手机慢动作视频构建的大规模真实图像去模糊数据集，包含42,000对高分辨率模糊-清晰图像，用于评估现有去模糊模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有去模糊数据集规模小且多样性不足，难以评估模型在真实复杂场景中的表现。

Method: 通过平均240帧慢动作视频模拟长曝光模糊，生成模糊图像，并使用中间帧作为清晰参考。

Result: 数据集规模是现有常用数据集的10倍，场景多样性更高，现有SOTA模型在其上表现显著下降。

Conclusion: 该数据集为开发鲁棒且泛化性强的去模糊模型提供了新的挑战性基准。

Abstract: We introduce the largest real-world image deblurring dataset constructed from smartphone slow-motion videos. Using 240 frames captured over one second, we simulate realistic long-exposure blur by averaging frames to produce blurry images, while using the temporally centered frame as the sharp reference. Our dataset contains over 42,000 high-resolution blur-sharp image pairs, making it approximately 10 times larger than widely used datasets, with 8 times the amount of different scenes, including indoor and outdoor environments, with varying object and camera motions. We benchmark multiple state-of-the-art (SOTA) deblurring models on our dataset and observe significant performance degradation, highlighting the complexity and diversity of our benchmark. Our dataset serves as a challenging new benchmark to facilitate robust and generalizable deblurring models.

</details>


### [15] [Stylized Structural Patterns for Improved Neural Network Pre-training](https://arxiv.org/abs/2506.19465)
*Farnood Salehi,Vandit Sharma,Amirhossein Askari Farsangi,Tunç Ozan Aydın*

Main category: cs.CV

TL;DR: 本文提出了一种两步法来缩小合成数据与真实数据之间的性能差距：改进的神经分形公式和反向风格化技术，显著降低了分布差距，并在多个任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型依赖大规模真实图像数据集的问题，这些数据集难以获取且存在隐私和法律风险，而现有合成数据训练效果不佳。

Method: 1. 提出改进的神经分形公式生成新类别的合成数据；2. 提出反向风格化技术，将小规模真实图像的特征迁移到合成数据中。

Result: 通过KID指标验证了合成数据与真实数据的分布差距显著降低；在多个任务中表现优异，如EDM2扩散模型的FID降低11%，ViT-S分类模型在ImageNet-100上准确率提升10%。

Conclusion: 该方法为在缺乏大规模真实数据集时训练实用模型提供了新思路。

Abstract: Modern deep learning models in computer vision require large datasets of real images, which are difficult to curate and pose privacy and legal concerns, limiting their commercial use. Recent works suggest synthetic data as an alternative, yet models trained with it often underperform. This paper proposes a two-step approach to bridge this gap. First, we propose an improved neural fractal formulation through which we introduce a new class of synthetic data. Second, we propose reverse stylization, a technique that transfers visual features from a small, license-free set of real images onto synthetic datasets, enhancing their effectiveness. We analyze the domain gap between our synthetic datasets and real images using Kernel Inception Distance (KID) and show that our method achieves a significantly lower distributional gap compared to existing synthetic datasets. Furthermore, our experiments across different tasks demonstrate the practical impact of this reduced gap. We show that pretraining the EDM2 diffusion model on our synthetic dataset leads to an 11% reduction in FID during image generation, compared to models trained on existing synthetic datasets, and a 20% decrease in autoencoder reconstruction error, indicating improved performance in data representation. Furthermore, a ViT-S model trained for classification on this synthetic data achieves over a 10% improvement in ImageNet-100 accuracy. Our work opens up exciting possibilities for training practical models when sufficiently large real training sets are not available.

</details>


### [16] [Self-Supervised Multimodal NeRF for Autonomous Driving](https://arxiv.org/abs/2506.19615)
*Gaurav Sharma,Ravi Kothari,Josef Schmid*

Main category: cs.CV

TL;DR: 提出了一种基于NeRF的自监督多模态动态场景新视角合成框架（NVSF），无需3D标签，在LiDAR和相机领域均表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决现有动态NeRF方法对3D标签的依赖问题，并提升多模态场景下的新视角合成性能。

Method: 联合学习时空场景的隐式神经表示，采用启发式图像像素采样和双梯度掩码优化训练。

Result: 在KITTI-360数据集上表现优于基线模型，LiDAR和相机领域均取得最佳性能。

Conclusion: NVSF是一种高效、自监督的多模态动态场景新视角合成框架，具有实际应用潜力。

Abstract: In this paper, we propose a Neural Radiance Fields (NeRF) based framework, referred to as Novel View Synthesis Framework (NVSF). It jointly learns the implicit neural representation of space and time-varying scene for both LiDAR and Camera. We test this on a real-world autonomous driving scenario containing both static and dynamic scenes. Compared to existing multimodal dynamic NeRFs, our framework is self-supervised, thus eliminating the need for 3D labels. For efficient training and faster convergence, we introduce heuristic-based image pixel sampling to focus on pixels with rich information. To preserve the local features of LiDAR points, a Double Gradient based mask is employed. Extensive experiments on the KITTI-360 dataset show that, compared to the baseline models, our framework has reported best performance on both LiDAR and Camera domain. Code of the model is available at https://github.com/gaurav00700/Selfsupervised-NVSF

</details>


### [17] [Video Compression for Spatiotemporal Earth System Data](https://arxiv.org/abs/2506.19656)
*Oscar J. Pellicer-Valero,Cesar Aybar,Gustau Camps Valls*

Main category: cs.CV

TL;DR: xarrayvideo是一个Python库，利用视频压缩技术高效压缩多通道时空数据集，压缩比高达250倍，同时保持高保真度。


<details>
  <summary>Details</summary>
Motivation: 随着地球观测数据集规模的快速增长，需要高效的压缩方法来处理这些数据。

Method: 通过ffmpeg利用标准视频编解码器将多通道时空数据集编码为视频。

Result: 在四个真实数据集上实现了高压缩比（如250倍）和高保真度（PSNR 55.86-65.91 dB）。

Conclusion: xarrayvideo为地球科学社区提供了一种高效的数据压缩解决方案，适用于深度学习任务。

Abstract: Large-scale Earth system datasets, from high-resolution remote sensing imagery to spatiotemporal climate model outputs, exhibit characteristics analogous to those of standard videos. Their inherent spatial, temporal, and spectral redundancies can thus be readily exploited by established video compression techniques. Here, we present xarrayvideo, a Python library for compressing multichannel spatiotemporal datasets by encoding them as videos. Our approach achieves compression ratios of up to 250x while maintaining high fidelity by leveraging standard, well-optimized video codecs through ffmpeg. We demonstrate the library's effectiveness on four real-world multichannel spatiotemporal datasets: DynamicEarthNet (very high resolution Planet images), DeepExtremeCubes (high resolution Sentinel-2 images), ERA5 (weather reanalysis data), and the SimpleS2 dataset (high resolution multichannel Sentinel-2 images), achieving Peak Signal-to-Noise Ratios (PSNRs) of 55.86, 40.60, 46.58, and 43.23 dB at 0.1 bits per pixel per band (bpppb) and 65.91, 54.28, 62.90, and 55.04 dB at 1 bpppb. We are redistributing two of these datasets, DeepExtremeCubes (2.3 Tb) and DynamicEarthNet (525 Gb), in the machine-learning-ready and cloud-ready TACO format through HuggingFace at significantly reduced sizes (270 Gb and 8.5 Gb, respectively) without compromising quality (PSNR 55.77-56.65 and 60.15). No performance loss is observed when the compressed versions of these datasets are used in their respective deep learning-based downstream tasks (next step reflectance prediction and landcover segmentation). In conclusion, xarrayvideo presents an efficient solution for handling the rapidly growing size of Earth observation datasets, making advanced compression techniques accessible and practical to the Earth science community. The library is available for use at https://github.com/IPL-UV/xarrayvideo

</details>


### [18] [SAM2-SGP: Enhancing SAM2 for Medical Image Segmentation via Support-Set Guided Prompting](https://arxiv.org/abs/2506.19658)
*Yang Xing,Jiong Wu,Yuheng Bu,Kuang Gong*

Main category: cs.CV

TL;DR: SAM2-SGP框架通过支持集引导提示和低秩适应策略，解决了SAM2在医学图像分割中的手动提示依赖和领域偏移问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决SAM2在医学图像分割中依赖人工提示和领域偏移的问题。

Method: 提出SAM2-SGP框架，包括伪掩模生成模块（PMG）和伪掩模注意力模块（PMA），并采用低秩适应（LoRA）策略。

Result: 在多种医学影像模态上显著优于现有模型（如nnUNet、SwinUNet、SAM2和MedSAM2）。

Conclusion: SAM2-SGP框架有效解决了医学图像分割中的关键问题，性能优越且代码开源。

Abstract: Although new vision foundation models such as Segment Anything Model 2 (SAM2) have significantly enhanced zero-shot image segmentation capabilities, reliance on human-provided prompts poses significant challenges in adapting SAM2 to medical image segmentation tasks. Moreover, SAM2's performance in medical image segmentation was limited by the domain shift issue, since it was originally trained on natural images and videos. To address these challenges, we proposed SAM2 with support-set guided prompting (SAM2-SGP), a framework that eliminated the need for manual prompts. The proposed model leveraged the memory mechanism of SAM2 to generate pseudo-masks using image-mask pairs from a support set via a Pseudo-mask Generation (PMG) module. We further introduced a novel Pseudo-mask Attention (PMA) module, which used these pseudo-masks to automatically generate bounding boxes and enhance localized feature extraction by guiding attention to relevant areas. Furthermore, a low-rank adaptation (LoRA) strategy was adopted to mitigate the domain shift issue. The proposed framework was evaluated on both 2D and 3D datasets across multiple medical imaging modalities, including fundus photography, X-ray, computed tomography (CT), magnetic resonance imaging (MRI), positron emission tomography (PET), and ultrasound. The results demonstrated a significant performance improvement over state-of-the-art models, such as nnUNet and SwinUNet, as well as foundation models, such as SAM2 and MedSAM2, underscoring the effectiveness of the proposed approach. Our code is publicly available at https://github.com/astlian9/SAM_Support.

</details>


### [19] [CoCo4D: Comprehensive and Complex 4D Scene Generation](https://arxiv.org/abs/2506.19798)
*Junwei Zhou,Xueting Li,Lu Qi,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: CoCo4D是一个从文本提示生成动态4D场景的框架，支持多视角一致性和沉浸感，通过分离动态前景和背景优化合成效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有4D合成方法在多视角一致性和动态场景生成上的局限性。

Method: 利用视频扩散模型生成初始运动序列，通过渐进式外推方案合成动态前景和背景，并优化前景轨迹以实现无缝融合。

Result: 实验表明CoCo4D在4D场景生成中性能优于或与现有方法相当。

Conclusion: CoCo4D有效且高效地生成了多视角一致的动态4D场景。

Abstract: Existing 4D synthesis methods primarily focus on object-level generation or dynamic scene synthesis with limited novel views, restricting their ability to generate multi-view consistent and immersive dynamic 4D scenes. To address these constraints, we propose a framework (dubbed as CoCo4D) for generating detailed dynamic 4D scenes from text prompts, with the option to include images. Our method leverages the crucial observation that articulated motion typically characterizes foreground objects, whereas background alterations are less pronounced. Consequently, CoCo4D divides 4D scene synthesis into two responsibilities: modeling the dynamic foreground and creating the evolving background, both directed by a reference motion sequence. Given a text prompt and an optional reference image, CoCo4D first generates an initial motion sequence utilizing video diffusion models. This motion sequence then guides the synthesis of both the dynamic foreground object and the background using a novel progressive outpainting scheme. To ensure seamless integration of the moving foreground object within the dynamic background, CoCo4D optimizes a parametric trajectory for the foreground, resulting in realistic and coherent blending. Extensive experiments show that CoCo4D achieves comparable or superior performance in 4D scene generation compared to existing methods, demonstrating its effectiveness and efficiency. More results are presented on our website https://colezwhy.github.io/coco4d/.

</details>


### [20] [Bind-Your-Avatar: Multi-Talking-Character Video Generation with Dynamic 3D-mask-based Embedding Router](https://arxiv.org/abs/2506.19833)
*Yubo Huang,Weiqiang Wang,Sirui Zhao,Tong Xu,Lin Liu,Enhong Chen*

Main category: cs.CV

TL;DR: 论文提出了一种名为Bind-Your-Avatar的模型，用于生成同一场景中多角色对话视频，解决了音频与角色对应控制的挑战，并提供了首个多角色对话数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注单角色场景，而多角色在同一空间环境中的对话视频生成问题尚未解决，尤其是音频与角色对应控制和缺乏相关数据集。

Method: 提出基于MM-DiT的模型，包括细粒度Embedding Router、3D-mask嵌入路由器和掩码优化策略，并构建了首个多角色对话数据集。

Result: 实验表明，该方法在双角色对话视频生成任务中表现优于现有技术。

Conclusion: Bind-Your-Avatar为多角色对话视频生成提供了有效解决方案，并填补了数据集空白。

Abstract: Recent years have witnessed remarkable advances in audio-driven talking head generation. However, existing approaches predominantly focus on single-character scenarios. While some methods can create separate conversation videos between two individuals, the critical challenge of generating unified conversation videos with multiple physically co-present characters sharing the same spatial environment remains largely unaddressed. This setting presents two key challenges: audio-to-character correspondence control and the lack of suitable datasets featuring multi-character talking videos within the same scene. To address these challenges, we introduce Bind-Your-Avatar, an MM-DiT-based model specifically designed for multi-talking-character video generation in the same scene. Specifically, we propose (1) A novel framework incorporating a fine-grained Embedding Router that binds `who' and `speak what' together to address the audio-to-character correspondence control. (2) Two methods for implementing a 3D-mask embedding router that enables frame-wise, fine-grained control of individual characters, with distinct loss functions based on observed geometric priors and a mask refinement strategy to enhance the accuracy and temporal smoothness of the predicted masks. (3) The first dataset, to the best of our knowledge, specifically constructed for multi-talking-character video generation, and accompanied by an open-source data processing pipeline, and (4) A benchmark for the dual-talking-characters video generation, with extensive experiments demonstrating superior performance over multiple state-of-the-art methods.

</details>


### [21] [SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution](https://arxiv.org/abs/2506.19838)
*Liangbin Xie,Yu Li,Shian Du,Menghan Xia,Xintao Wang,Fanghua Yu,Ziyan Chen,Pengfei Wan,Jiantao Zhou,Chao Dong*

Main category: cs.CV

TL;DR: 论文提出了一种两阶段视频生成方法，通过分离语义内容生成和细节合成，重点研究了轻量级级联视频超分辨率（VSR）模型的设计原则。


<details>
  <summary>Details</summary>
Motivation: 随着用户对高分辨率视频生成的需求增加，仅依赖潜在计算已不足，需探索更高效的方法。

Method: 提出两种退化策略生成训练对，分析时间步采样策略和噪声增强效果，并引入交错时间单元和稀疏局部注意力以降低计算开销。

Result: 实验证明该方法优于现有技术，消融研究验证了设计选择的有效性。

Conclusion: 研究为级联视频超分辨率生成提供了简单有效的基线，为未来高效级联合成系统的进步提供了实用见解。

Abstract: Latent diffusion models have emerged as a leading paradigm for efficient video generation. However, as user expectations shift toward higher-resolution outputs, relying solely on latent computation becomes inadequate. A promising approach involves decoupling the process into two stages: semantic content generation and detail synthesis. The former employs a computationally intensive base model at lower resolutions, while the latter leverages a lightweight cascaded video super-resolution (VSR) model to achieve high-resolution output. In this work, we focus on studying key design principles for latter cascaded VSR models, which are underexplored currently. First, we propose two degradation strategies to generate training pairs that better mimic the output characteristics of the base model, ensuring alignment between the VSR model and its upstream generator. Second, we provide critical insights into VSR model behavior through systematic analysis of (1) timestep sampling strategies, (2) noise augmentation effects on low-resolution (LR) inputs. These findings directly inform our architectural and training innovations. Finally, we introduce interleaving temporal unit and sparse local attention to achieve efficient training and inference, drastically reducing computational overhead. Extensive experiments demonstrate the superiority of our framework over existing methods, with ablation studies confirming the efficacy of each design choice. Our work establishes a simple yet effective baseline for cascaded video super-resolution generation, offering practical insights to guide future advancements in efficient cascaded synthesis systems.

</details>


### [22] [Improving Progressive Generation with Decomposable Flow Matching](https://arxiv.org/abs/2506.19839)
*Moayed Haji-Ali,Willi Menapace,Ivan Skorokhodov,Arpit Sahni,Sergey Tulyakov,Vicente Ordonez,Aliaksandr Siarohin*

Main category: cs.CV

TL;DR: Decomposable Flow Matching (DFM) 是一种简单有效的渐进式生成视觉媒体的框架，通过独立应用 Flow Matching 在多尺度表示中，显著提升了图像和视频的生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决高维视觉模态生成的计算密集型问题，避免现有多阶段架构的复杂性和额外需求。

Method: DFM 在用户定义的多尺度表示（如拉普拉斯金字塔）的每个层级上独立应用 Flow Matching。

Result: 在 Imagenet-1k 512px 上，DFM 的 FDD 分数比基础架构提高了 35.2%，比最佳基线提高了 26.4%，且在大型模型微调中收敛更快。

Conclusion: DFM 通过单一模型、架构简单性和对现有训练流程的最小修改，实现了显著的性能提升。

Abstract: Generating high-dimensional visual modalities is a computationally intensive task. A common solution is progressive generation, where the outputs are synthesized in a coarse-to-fine spectral autoregressive manner. While diffusion models benefit from the coarse-to-fine nature of denoising, explicit multi-stage architectures are rarely adopted. These architectures have increased the complexity of the overall approach, introducing the need for a custom diffusion formulation, decomposition-dependent stage transitions, add-hoc samplers, or a model cascade. Our contribution, Decomposable Flow Matching (DFM), is a simple and effective framework for the progressive generation of visual media. DFM applies Flow Matching independently at each level of a user-defined multi-scale representation (such as Laplacian pyramid). As shown by our experiments, our approach improves visual quality for both images and videos, featuring superior results compared to prior multistage frameworks. On Imagenet-1k 512px, DFM achieves 35.2% improvements in FDD scores over the base architecture and 26.4% over the best-performing baseline, under the same training compute. When applied to finetuning of large models, such as FLUX, DFM shows faster convergence speed to the training distribution. Crucially, all these advantages are achieved with a single model, architectural simplicity, and minimal modifications to existing training pipelines.

</details>


### [23] [GenHSI: Controllable Generation of Human-Scene Interaction Videos](https://arxiv.org/abs/2506.19840)
*Zekun Li,Rui Zhou,Rahul Sajnani,Xiaoyan Cong,Daniel Ritchie,Srinath Sridhar*

Main category: cs.CV

TL;DR: GenHSI是一种无需训练的方法，用于生成长时间的人类-场景交互视频（HSI），通过分阶段处理解决现有模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模预训练视频扩散模型在生成长电影式视频时面临人类-场景交互不真实、主体身份保存不足以及训练成本高等问题。

Method: GenHSI将长视频生成任务分为三个阶段：脚本编写、预视觉化和动画。利用场景图像、用户描述和人物图像生成3D关键帧，并通过现成的视频扩散模型渲染动画。

Result: 实验表明，GenHSI能生成保留场景内容和角色身份的长视频，且具有合理的人类-场景交互。

Conclusion: GenHSI无需训练即可生成长视频序列，解决了现有方法的多个挑战。

Abstract: Large-scale pre-trained video diffusion models have exhibited remarkable capabilities in diverse video generation. However, existing solutions face several challenges in using these models to generate long movie-like videos with rich human-object interactions that include unrealistic human-scene interaction, lack of subject identity preservation, and require expensive training. We propose GenHSI, a training-free method for controllable generation of long human-scene interaction videos (HSI). Taking inspiration from movie animation, our key insight is to overcome the limitations of previous work by subdividing the long video generation task into three stages: (1) script writing, (2) pre-visualization, and (3) animation. Given an image of a scene, a user description, and multiple images of a person, we use these three stages to generate long-videos that preserve human-identity and provide rich human-scene interactions. Script writing converts complex human tasks into simple atomic tasks that are used in the pre-visualization stage to generate 3D keyframes (storyboards). These 3D keyframes are rendered and animated by off-the-shelf video diffusion models for consistent long video generation with rich contacts in a 3D-aware manner. A key advantage of our work is that we alleviate the need for scanned, accurate scenes and create 3D keyframes from single-view images. We are the first to generate a long video sequence with a consistent camera pose that contains arbitrary numbers of character actions without training. Experiments demonstrate that our method can generate long videos that effectively preserve scene content and character identity with plausible human-scene interaction from a single image scene. Visit our project homepage https://kunkun0w0.github.io/project/GenHSI/ for more information.

</details>


### [24] [A Comparative Study of NAFNet Baselines for Image Restoration](https://arxiv.org/abs/2506.19845)
*Vladislav Esaulov,M. Moein Esfahani*

Main category: cs.CV

TL;DR: NAFNet是一种简单高效的图像修复深度学习基线，通过实验验证其核心组件（SimpleGate激活、SCA和LayerNorm）的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究NAFNet的核心组件对图像修复性能的影响，验证其设计合理性。

Method: 使用CIFAR10噪声和模糊图像进行消融实验，比较不同组件替换或移除后的效果。

Result: SimpleGate和简化注意力机制优于传统方法，LayerNorm对训练稳定性至关重要。

Conclusion: 支持NAFNet设计，并提出改进建议和未来研究方向。

Abstract: We study NAFNet (Nonlinear Activation Free Network), a simple and efficient deep learning baseline for image restoration. By using CIFAR10 images corrupted with noise and blur, we conduct an ablation study of NAFNet's core components. Our baseline model implements SimpleGate activation, Simplified Channel Activation (SCA), and LayerNormalization. We compare this baseline to different variants that replace or remove components. Quantitative results (PSNR, SSIM) and examples illustrate how each modification affects restoration performance. Our findings support the NAFNet design: the SimpleGate and simplified attention mechanisms yield better results than conventional activations and attention, while LayerNorm proves to be important for stable training. We conclude with recommendations for model design, discuss potential improvements, and future work.

</details>


### [25] [Radial Attention: $O(n\log n)$ Sparse Attention with Energy Decay for Long Video Generation](https://arxiv.org/abs/2506.19852)
*Xingyang Li,Muyang Li,Tianle Cai,Haocheng Xi,Shuo Yang,Yujun Lin,Lvmin Zhang,Songlin Yang,Jinbo Hu,Kelly Peng,Maneesh Agrawala,Ion Stoica,Kurt Keutzer,Song Han*

Main category: cs.CV

TL;DR: 论文提出Radial Attention，一种高效的稀疏注意力机制，通过模拟时空能量衰减现象，显著降低视频扩散模型的计算成本。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型的高计算成本限制了长视频的生成，研究发现时空注意力分数随距离衰减的现象，启发设计更高效的注意力机制。

Method: 提出Radial Attention，利用静态注意力掩码，使每个token仅关注空间邻近的token，且窗口大小随时间距离缩小，复杂度为O(n log n)。

Result: 实验表明，Radial Attention在多个数据集上保持视频质量，计算速度提升1.9倍，支持生成长度4倍的视频，训练成本降低4.4倍，推理加速3.7倍。

Conclusion: Radial Attention通过模拟能量衰减，显著提升视频扩散模型的效率和可扩展性，适用于长视频生成。

Abstract: Recent advances in diffusion models have enabled high-quality video generation, but the additional temporal dimension significantly increases computational costs, making training and inference on long videos prohibitively expensive. In this paper, we identify a phenomenon we term Spatiotemporal Energy Decay in video diffusion models: post-softmax attention scores diminish as spatial and temporal distance between tokens increase, akin to the physical decay of signal or waves over space and time in nature. Motivated by this, we propose Radial Attention, a scalable sparse attention mechanism with $O(n \log n)$ complexity that translates energy decay into exponentially decaying compute density, which is significantly more efficient than standard $O(n^2)$ dense attention and more expressive than linear attention. Specifically, Radial Attention employs a simple, static attention mask where each token attends to spatially nearby tokens, with the attention window size shrinking with temporal distance. Moreover, it allows pre-trained video diffusion models to extend their generation length with efficient LoRA-based fine-tuning. Extensive experiments show that Radial Attention maintains video quality across Wan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9$\times$ speedup over the original dense attention. With minimal tuning, it enables video generation up to 4$\times$ longer while reducing training costs by up to 4.4$\times$ compared to direct fine-tuning and accelerating inference by up to 3.7$\times$ compared to dense attention inference.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [26] [Noise Consistency Training: A Native Approach for One-Step Generator in Learning Additional Controls](https://arxiv.org/abs/2506.19741)
*Yihong Luo,Shuchen Xue,Tianyang Hu,Jing Tang*

Main category: cs.LG

TL;DR: 本文提出了一种名为Noise Consistency Training (NCT)的轻量级方法，用于将新的控制信号直接集成到预训练的一步生成器中，无需原始训练图像或重新训练基础扩散模型。


<details>
  <summary>Details</summary>
Motivation: 高效且可控的高质量内容生成是AIGC的核心挑战，现有方法在适应新控制条件时计算成本高昂。

Method: NCT通过引入适配器模块和噪声一致性损失，在生成器的噪声空间中操作，隐式引导模型遵循新控制。

Result: 实验表明，NCT在单次前向传播中实现了最先进的可控生成，超越了现有的多步和基于蒸馏的方法。

Conclusion: NCT是一种模块化、数据高效且易于部署的方法，显著提升了生成质量和计算效率。

Abstract: The pursuit of efficient and controllable high-quality content generation remains a central challenge in artificial intelligence-generated content (AIGC). While one-step generators, enabled by diffusion distillation techniques, offer excellent generation quality and computational efficiency, adapting them to new control conditions--such as structural constraints, semantic guidelines, or external inputs--poses a significant challenge. Conventional approaches often necessitate computationally expensive modifications to the base model and subsequent diffusion distillation. This paper introduces Noise Consistency Training (NCT), a novel and lightweight approach to directly integrate new control signals into pre-trained one-step generators without requiring access to original training images or retraining the base diffusion model. NCT operates by introducing an adapter module and employs a noise consistency loss in the noise space of the generator. This loss aligns the adapted model's generation behavior across noises that are conditionally dependent to varying degrees, implicitly guiding it to adhere to the new control. Theoretically, this training objective can be understood as minimizing the distributional distance between the adapted generator and the conditional distribution induced by the new conditions. NCT is modular, data-efficient, and easily deployable, relying only on the pre-trained one-step generator and a control signal model. Extensive experiments demonstrate that NCT achieves state-of-the-art controllable generation in a single forward pass, surpassing existing multi-step and distillation-based methods in both generation quality and computational efficiency. Code is available at https://github.com/Luo-Yihong/NCT

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [27] [SoK: Can Synthetic Images Replace Real Data? A Survey of Utility and Privacy of Synthetic Image Generation](https://arxiv.org/abs/2506.19360)
*Yunsung Chung,Yunbei Zhang,Nassir Marrouche,Jihun Hamm*

Main category: cs.CR

TL;DR: 本文对隐私保护数据合成（PPDS）中的合成图像生成方法进行了系统分类和比较，并提出了一个基准测试框架，以评估不同方法的隐私风险和效用。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对合成图像生成方法的全面调查和比较，尤其是在训练分类器时如何平衡效用和隐私的问题。

Method: 通过系统分类现有图像合成方法、隐私攻击和缓解措施，并使用模型无关的成员推理攻击（MIA）作为隐私风险衡量标准。

Result: 研究提供了关于合成数据是否能替代真实数据、最佳发布策略以及不同生成模型表现的实证结果。

Conclusion: 研究为合成数据生成方法的效用-隐私权衡提供了实用见解，并指导实际应用中的数据发布策略。

Abstract: Advances in generative models have transformed the field of synthetic image generation for privacy-preserving data synthesis (PPDS). However, the field lacks a comprehensive survey and comparison of synthetic image generation methods across diverse settings. In particular, when we generate synthetic images for the purpose of training a classifier, there is a pipeline of generation-sampling-classification which takes private training as input and outputs the final classifier of interest. In this survey, we systematically categorize existing image synthesis methods, privacy attacks, and mitigations along this generation-sampling-classification pipeline. To empirically compare diverse synthesis approaches, we provide a benchmark with representative generative methods and use model-agnostic membership inference attacks (MIAs) as a measure of privacy risk. Through this study, we seek to answer critical questions in PPDS: Can synthetic data effectively replace real data? Which release strategy balances utility and privacy? Do mitigations improve the utility-privacy tradeoff? Which generative models perform best across different scenarios? With a systematic evaluation of diverse methods, our study provides actionable insights into the utility-privacy tradeoffs of synthetic data generation methods and guides the decision on optimal data releasing strategies for real-world applications.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [28] [NAADA: A Noise-Aware Attention Denoising Autoencoder for Dental Panoramic Radiographs](https://arxiv.org/abs/2506.19387)
*Khuram Naveed,Bruna Neves de Freitas,Ruben Pauwels*

Main category: eess.IV

TL;DR: 论文提出了一种噪声感知自注意力方法，用于改进卷积去噪自编码器在牙科X光图像中的高频细节恢复能力。


<details>
  <summary>Details</summary>
Motivation: 卷积去噪自编码器在图像恢复中表现优异，但倾向于恢复低频特征，导致高频细节丢失，这在牙科X光图像中尤为关键。传统注意力方法可能忽略噪声区域的重要特征。

Method: 提出噪声感知自注意力方法，并基于此构建噪声感知注意力增强去噪自编码器（NAADA）网络，用于增强噪声牙科全景X光图像。

Result: 与现有方法（如Uformer、MResDNN等）相比，NAADA能更好地重建细节，提升图像质量和诊断准确性。

Conclusion: 噪声感知自注意力方法有效解决了高频细节恢复问题，为牙科X光图像的去噪提供了更优解决方案。

Abstract: Convolutional denoising autoencoders (DAEs) are powerful tools for image restoration. However, they inherit a key limitation of convolutional neural networks (CNNs): they tend to recover low-frequency features, such as smooth regions, more effectively than high-frequency details. This leads to the loss of fine details, which is particularly problematic in dental radiographs where preserving subtle anatomical structures is crucial. While self-attention mechanisms can help mitigate this issue by emphasizing important features, conventional attention methods often prioritize features corresponding to cleaner regions and may overlook those obscured by noise. To address this limitation, we propose a noise-aware self-attention method, which allows the model to effectively focus on and recover key features even within noisy regions. Building on this approach, we introduce the noise-aware attention-enhanced denoising autoencoder (NAADA) network for enhancing noisy panoramic dental radiographs. Compared with the recent state of the art (and much heavier) methods like Uformer, MResDNN etc., our method improves the reconstruction of fine details, ensuring better image quality and diagnostic accuracy.

</details>


### [29] [Angio-Diff: Learning a Self-Supervised Adversarial Diffusion Model for Angiographic Geometry Generation](https://arxiv.org/abs/2506.19455)
*Zhifeng Wang,Renjiao Yi,Xin Wen,Chenyang Zhu,Kai Xu,Kunlun He*

Main category: eess.IV

TL;DR: 提出一种基于扩散模型的自监督方法，将非血管造影X射线转换为血管造影X射线，解决数据不足问题，提升血管几何结构合成的准确性。


<details>
  <summary>Details</summary>
Motivation: 血管造影X射线诊断辐射高，非血管造影数据丰富但缺乏配对数据集，现有方法在血管几何结构合成上表现不佳。

Method: 结合扩散模型学习血管数据分布，生成器合成血管，掩码对抗模块提升质量，引入参数化血管模型增强几何准确性。

Result: 实验表明，Angio-Diff在合成血管造影图像质量和几何结构准确性上达到最优性能。

Conclusion: 该方法为血管造影合成提供了新思路和数据集，显著提升了合成质量。

Abstract: Vascular diseases pose a significant threat to human health, with X-ray angiography established as the gold standard for diagnosis, allowing for detailed observation of blood vessels. However, angiographic X-rays expose personnel and patients to higher radiation levels than non-angiographic X-rays, which are unwanted. Thus, modality translation from non-angiographic to angiographic X-rays is desirable. Data-driven deep approaches are hindered by the lack of paired large-scale X-ray angiography datasets. While making high-quality vascular angiography synthesis crucial, it remains challenging. We find that current medical image synthesis primarily operates at pixel level and struggles to adapt to the complex geometric structure of blood vessels, resulting in unsatisfactory quality of blood vessel image synthesis, such as disconnections or unnatural curvatures. To overcome this issue, we propose a self-supervised method via diffusion models to transform non-angiographic X-rays into angiographic X-rays, mitigating data shortages for data-driven approaches. Our model comprises a diffusion model that learns the distribution of vascular data from diffusion latent, a generator for vessel synthesis, and a mask-based adversarial module. To enhance geometric accuracy, we propose a parametric vascular model to fit the shape and distribution of blood vessels. The proposed method contributes a pipeline and a synthetic dataset for X-ray angiography. We conducted extensive comparative and ablation experiments to evaluate the Angio-Diff. The results demonstrate that our method achieves state-of-the-art performance in synthetic angiography image quality and more accurately synthesizes the geometric structure of blood vessels. The code is available at https://github.com/zfw-cv/AngioDiff.

</details>


### [30] [NeRF-based CBCT Reconstruction needs Normalization and Initialization](https://arxiv.org/abs/2506.19742)
*Zhuowei Xu,Han Li,Dai Sun,Zhicheng Li,Yujia Li,Qingpeng Kong,Zhiwei Cheng,Nassir Navab,S. Kevin Zhou*

Main category: eess.IV

TL;DR: 论文提出了一种归一化哈希编码器和映射一致性初始化策略，以解决CBCT重建中局部-全局训练不匹配问题，提升训练稳定性和重建质量。


<details>
  <summary>Details</summary>
Motivation: CBCT重建因投影数据有限而成为病态问题，现有NeRF方法因哈希编码器与神经网络的局部-全局训练不匹配导致特征不一致、训练不稳定。

Method: 引入归一化哈希编码器增强特征一致性，并提出映射一致性初始化策略，通过预训练模型初始化神经网络以提升早期训练稳定性。

Result: 方法在128个CT案例上验证，覆盖7个解剖区域，显著提升训练效率和重建性能。

Conclusion: 提出的方法简单有效，仅需少量代码即可显著改善CBCT重建的训练稳定性和质量。

Abstract: Cone Beam Computed Tomography (CBCT) is widely used in medical imaging. However, the limited number and intensity of X-ray projections make reconstruction an ill-posed problem with severe artifacts. NeRF-based methods have achieved great success in this task. However, they suffer from a local-global training mismatch between their two key components: the hash encoder and the neural network. Specifically, in each training step, only a subset of the hash encoder's parameters is used (local sparse), whereas all parameters in the neural network participate (global dense). Consequently, hash features generated in each step are highly misaligned, as they come from different subsets of the hash encoder. These misalignments from different training steps are then fed into the neural network, causing repeated inconsistent global updates in training, which leads to unstable training, slower convergence, and degraded reconstruction quality. Aiming to alleviate the impact of this local-global optimization mismatch, we introduce a Normalized Hash Encoder, which enhances feature consistency and mitigates the mismatch. Additionally, we propose a Mapping Consistency Initialization(MCI) strategy that initializes the neural network before training by leveraging the global mapping property from a well-trained model. The initialized neural network exhibits improved stability during early training, enabling faster convergence and enhanced reconstruction performance. Our method is simple yet effective, requiring only a few lines of code while substantially improving training efficiency on 128 CT cases collected from 4 different datasets, covering 7 distinct anatomical regions.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [31] [Experimental Assessment of Neural 3D Reconstruction for Small UAV-based Applications](https://arxiv.org/abs/2506.19491)
*Genís Castillo Gómez-Raya,Álmos Veres-Vitályos,Filip Lemic,Pablo Royo,Mario Montagud,Sergi Fernández,Sergi Abadal,Xavier Costa-Pérez*

Main category: cs.ET

TL;DR: 论文提出了一种将神经3D重建（N3DR）集成到小型无人机系统中的方法，以提升对静态小物体的精细3D重建质量，显著优于传统SfM算法。


<details>
  <summary>Details</summary>
Motivation: 小型无人机在室内和难以到达区域的部署潜力增加，但其飞行动力学和功耗问题限制了自主性和任务能力。

Method: 设计并评估了一个基于N3DR的流程，利用Instant-ngp、Nerfacto和Splatfacto等先进模型，通过小型无人机拍摄的图像提升3D重建质量。

Result: 实验表明，N3DR增强的流程显著提高了重建质量，适用于高精度3D映射和受限环境中的异常检测。

Conclusion: N3DR技术有望推动小型无人机系统的能力发展。

Abstract: The increasing miniaturization of Unmanned Aerial Vehicles (UAVs) has expanded their deployment potential to indoor and hard-to-reach areas. However, this trend introduces distinct challenges, particularly in terms of flight dynamics and power consumption, which limit the UAVs' autonomy and mission capabilities. This paper presents a novel approach to overcoming these limitations by integrating Neural 3D Reconstruction (N3DR) with small UAV systems for fine-grained 3-Dimensional (3D) digital reconstruction of small static objects. Specifically, we design, implement, and evaluate an N3DR-based pipeline that leverages advanced models, i.e., Instant-ngp, Nerfacto, and Splatfacto, to improve the quality of 3D reconstructions using images of the object captured by a fleet of small UAVs. We assess the performance of the considered models using various imagery and pointcloud metrics, comparing them against the baseline Structure from Motion (SfM) algorithm. The experimental results demonstrate that the N3DR-enhanced pipeline significantly improves reconstruction quality, making it feasible for small UAVs to support high-precision 3D mapping and anomaly detection in constrained environments. In more general terms, our results highlight the potential of N3DR in advancing the capabilities of miniaturized UAV systems.

</details>
