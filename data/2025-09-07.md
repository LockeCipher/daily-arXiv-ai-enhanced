<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 5]
- [cs.CV](#cs.CV) [Total: 8]
- [quant-ph](#quant-ph) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [LuxDiT: Lighting Estimation with Video Diffusion Transformer](https://arxiv.org/abs/2509.03680)
*Ruofan Liang,Kai He,Zan Gojcic,Igor Gilitschenski,Sanja Fidler,Nandita Vijaykumar,Zian Wang*

Main category: cs.GR

TL;DR: LuxDiT是一个基于视频扩散变换器的数据驱动方法，通过微调生成HDR环境光照图，能够从单张图像或视频中准确估计场景光照条件。


<details>
  <summary>Details</summary>
Motivation: 现有的学习型光照估计方法受限于真实HDR环境图的稀缺性，这些数据获取成本高且多样性有限。虽然生成模型在图像合成方面表现出色，但光照估计仍面临间接视觉线索依赖、全局上下文推断和高动态范围输出恢复等挑战。

Method: 提出LuxDiT方法，通过在大规模合成数据集上微调视频扩散变换器，使其能够根据视觉输入生成HDR环境图。采用低秩适应微调策略来提高输入与预测环境图之间的语义对齐。

Result: 该方法能够产生具有真实角度高频细节的准确光照预测，在定量和定性评估中都优于现有的最先进技术，并能有效泛化到真实世界场景。

Conclusion: LuxDiT通过结合视频扩散变换器和精心设计的微调策略，成功解决了单图像/视频光照估计的挑战，为计算机视觉和图形学中的光照估计问题提供了有效的解决方案。

Abstract: Estimating scene lighting from a single image or video remains a longstanding challenge in computer vision and graphics. Learning-based approaches are constrained by the scarcity of ground-truth HDR environment maps, which are expensive to capture and limited in diversity. While recent generative models offer strong priors for image synthesis, lighting estimation remains difficult due to its reliance on indirect visual cues, the need to infer global (non-local) context, and the recovery of high-dynamic-range outputs. We propose LuxDiT, a novel data-driven approach that fine-tunes a video diffusion transformer to generate HDR environment maps conditioned on visual input. Trained on a large synthetic dataset with diverse lighting conditions, our model learns to infer illumination from indirect visual cues and generalizes effectively to real-world scenes. To improve semantic alignment between the input and the predicted environment map, we introduce a low-rank adaptation finetuning strategy using a collected dataset of HDR panoramas. Our method produces accurate lighting predictions with realistic angular high-frequency details, outperforming existing state-of-the-art techniques in both quantitative and qualitative evaluations.

</details>


### [2] [ContraGS: Codebook-Condensed and Trainable Gaussian Splatting for Fast, Memory-Efficient Reconstruction](https://arxiv.org/abs/2509.03775)
*Sankeerth Durvasula,Sharanshangar Muhunthan,Zain Moustafa,Richard Chen,Ruofan Liang,Yushi Guan,Nilesh Ahuja,Nilesh Jain,Selvakumar Panneer,Nandita Vijaykumar*

Main category: cs.GR

TL;DR: ContraGS是一种直接在压缩的3D高斯分布表示上训练的方法，通过使用码本存储高斯参数向量，显著减少内存消耗和加速训练渲染，同时保持接近SOTA的质量。


<details>
  <summary>Details</summary>
Motivation: 3D高斯分布技术需要大量高斯分布来实现高质量表示，但这会显著增加GPU内存消耗和训练/渲染延迟。需要一种方法能够在保持高斯数量的同时减少内存使用。

Method: 使用码本紧凑存储高斯参数向量，将参数估计建模为贝叶斯推断问题，采用MCMC采样在压缩表示的后验分布上进行采样。

Result: 训练峰值内存平均减少3.49倍，训练和渲染速度分别平均加速1.36倍和1.88倍，同时保持接近SOTA的质量。

Conclusion: ContraGS成功解决了在码本压缩表示中训练不可微分参数的挑战，为3D高斯分布技术提供了高效的内存优化解决方案。

Abstract: 3D Gaussian Splatting (3DGS) is a state-of-art technique to model real-world scenes with high quality and real-time rendering. Typically, a higher quality representation can be achieved by using a large number of 3D Gaussians. However, using large 3D Gaussian counts significantly increases the GPU device memory for storing model parameters. A large model thus requires powerful GPUs with high memory capacities for training and has slower training/rendering latencies due to the inefficiencies of memory access and data movement. In this work, we introduce ContraGS, a method to enable training directly on compressed 3DGS representations without reducing the Gaussian Counts, and thus with a little loss in model quality. ContraGS leverages codebooks to compactly store a set of Gaussian parameter vectors throughout the training process, thereby significantly reducing memory consumption. While codebooks have been demonstrated to be highly effective at compressing fully trained 3DGS models, directly training using codebook representations is an unsolved challenge. ContraGS solves the problem of learning non-differentiable parameters in codebook-compressed representations by posing parameter estimation as a Bayesian inference problem. To this end, ContraGS provides a framework that effectively uses MCMC sampling to sample over a posterior distribution of these compressed representations. With ContraGS, we demonstrate that ContraGS significantly reduces the peak memory during training (on average 3.49X) and accelerated training and rendering (1.36X and 1.88X on average, respectively), while retraining close to state-of-art quality.

</details>


### [3] [TensoIS: A Step Towards Feed-Forward Tensorial Inverse Subsurface Scattering for Perlin Distributed Heterogeneous Media](https://arxiv.org/abs/2509.04047)
*Ashish Tiwari,Satyam Bhardwaj,Yash Bachwana,Parag Sarvoday Sahu,T. M. Feroz Ali,Bhargava Chintalapati,Shanmuganathan Raman*

Main category: cs.GR

TL;DR: 这篇论文提出了一种基于学习的前向框架TensoIS，通过Perlin噪声模型和低秩张量表示来估计异质介质的散射参数，解决了从稀疏多视角图像中逆向散射的挑战性问题。


<details>
  <summary>Details</summary>
Motivation: 异质介质的散射参数估计是个极其具有挑战性的问题，现有方法多假设均质介质，而真实世界中缺少明确的异质性散射参数分布模型。

Method: 首先创建了HeteroSynth合成数据集，使用Fractal Perlin噪声模型异质散射参数；提出TensoIS框架，通过学习低秩张量组件来表示3D散射参数体积，从稀疏多视角图像中进行反向估计。

Result: 在HeteroSynth测试集、有趣的烟雾和云层几何体以及实际样品上评估了TensoIS的效果，证明其在逆向散射中的有效性。

Conclusion: 这项研究探索了Perlin噪声分布在模拟真实世界异质散射中的潜力，为前向式异质散射参数估计提供了新的解决方案。

Abstract: Estimating scattering parameters of heterogeneous media from images is a severely under-constrained and challenging problem. Most of the existing approaches model BSSRDF either through an analysis-by-synthesis approach, approximating complex path integrals, or using differentiable volume rendering techniques to account for heterogeneity. However, only a few studies have applied learning-based methods to estimate subsurface scattering parameters, but they assume homogeneous media. Interestingly, no specific distribution is known to us that can explicitly model the heterogeneous scattering parameters in the real world. Notably, procedural noise models such as Perlin and Fractal Perlin noise have been effective in representing intricate heterogeneities of natural, organic, and inorganic surfaces. Leveraging this, we first create HeteroSynth, a synthetic dataset comprising photorealistic images of heterogeneous media whose scattering parameters are modeled using Fractal Perlin noise. Furthermore, we propose Tensorial Inverse Scattering (TensoIS), a learning-based feed-forward framework to estimate these Perlin-distributed heterogeneous scattering parameters from sparse multi-view image observations. Instead of directly predicting the 3D scattering parameter volume, TensoIS uses learnable low-rank tensor components to represent the scattering volume. We evaluate TensoIS on unseen heterogeneous variations over shapes from the HeteroSynth test set, smoke and cloud geometries obtained from open-source realistic volumetric simulations, and some real-world samples to establish its effectiveness for inverse scattering. Overall, this study is an attempt to explore Perlin noise distribution, given the lack of any such well-defined distribution in literature, to potentially model real-world heterogeneous scattering in a feed-forward manner.

</details>


### [4] [SMooGPT: Stylized Motion Generation using Large Language Models](https://arxiv.org/abs/2509.04058)
*Lei Zhong,Yi Yang,Changjian Li*

Main category: cs.GR

TL;DR: 通过将LLM作为理解器、组合器和生成器，SMooGPT方法在身体部位文本空间中生成高解释性、细粒度控制的风格化动作，充分利用了LLM的开政词能力和语义理解能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有风格化动作生成方法在解释性、控制性、普遍性方面的不足，特别是对新风格的适应能力差以及生成动作类型偏差问题。

Method: 提出理由-组合-生成的新视角，使用身体部位文本空间作为中间表示，并细调LLM构建SMooGPT模型，该模型同时扮演理解器、组合器和生成器的角色。

Result: 综合实验、评估和用户感知研究证明了该方法的有效性，在纯文本驱动的风格化动作生成任务中表现特别突出。

Conclusion: SMooGPT通过利用LLM在身体部位文本空间的强大理解和组合能力，实现了高解释性、细粒度控制和良好普遍性的风格化动作生成。

Abstract: Stylized motion generation is actively studied in computer graphics, especially benefiting from the rapid advances in diffusion models. The goal of this task is to produce a novel motion respecting both the motion content and the desired motion style, e.g., ``walking in a loop like a Monkey''. Existing research attempts to address this problem via motion style transfer or conditional motion generation. They typically embed the motion style into a latent space and guide the motion implicitly in a latent space as well. Despite the progress, their methods suffer from low interpretability and control, limited generalization to new styles, and fail to produce motions other than ``walking'' due to the strong bias in the public stylization dataset. In this paper, we propose to solve the stylized motion generation problem from a new perspective of reasoning-composition-generation, based on our observations: i) human motion can often be effectively described using natural language in a body-part centric manner, ii) LLMs exhibit a strong ability to understand and reason about human motion, and iii) human motion has an inherently compositional nature, facilitating the new motion content or style generation via effective recomposing. We thus propose utilizing body-part text space as an intermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a reasoner, composer, and generator when generating the desired stylized motion. Our method executes in the body-part text space with much higher interpretability, enabling fine-grained motion control, effectively resolving potential conflicts between motion content and style, and generalizes well to new styles thanks to the open-vocabulary ability of LLMs. Comprehensive experiments and evaluations, and a user perceptual study, demonstrate the effectiveness of our approach, especially under the pure text-driven stylized motion generation.

</details>


### [5] [Hyper Diffusion Avatars: Dynamic Human Avatar Generation using Network Weight Space Diffusion](https://arxiv.org/abs/2509.04145)
*Dongliang Cao,Guoxing Sun,Marc Habermann,Florian Bernard*

Main category: cs.GR

TL;DR: 通过结合人员特定渲染与双向模型的优势，提出了一种新的动态人类演员生成方法，能够实现高实体感和真实姿势依赖变形的渲染效果。


<details>
  <summary>Details</summary>
Motivation: 解决人员特定渲染模型无法模拟不同身份的问题，以及生成式方法渲染质量低且无法抓取姿势依赖变形的限制。

Method: 采用两阶段管线：首先优化一组人员特定UNet网络来抓取细腻的姿势依赖变形，然后训练一个超双向模型来生成网络权重，实现实时可控渲染。

Result: 在大规模跨身份多视角视频数据集上，该方法超越了现有的最先进人类演员生成方法。

Conclusion: 该方法成功结合了人员特定渲染和生成式模型的优势，能够生成具有高实体感和真实姿势依赖变形的动态人类演员。

Abstract: Creating human avatars is a highly desirable yet challenging task. Recent advancements in radiance field rendering have achieved unprecedented photorealism and real-time performance for personalized dynamic human avatars. However, these approaches are typically limited to person-specific rendering models trained on multi-view video data for a single individual, limiting their ability to generalize across different identities. On the other hand, generative approaches leveraging prior knowledge from pre-trained 2D diffusion models can produce cartoonish, static human avatars, which are animated through simple skeleton-based articulation. Therefore, the avatars generated by these methods suffer from lower rendering quality compared to person-specific rendering methods and fail to capture pose-dependent deformations such as cloth wrinkles. In this paper, we propose a novel approach that unites the strengths of person-specific rendering and diffusion-based generative modeling to enable dynamic human avatar generation with both high photorealism and realistic pose-dependent deformations. Our method follows a two-stage pipeline: first, we optimize a set of person-specific UNets, with each network representing a dynamic human avatar that captures intricate pose-dependent deformations. In the second stage, we train a hyper diffusion model over the optimized network weights. During inference, our method generates network weights for real-time, controllable rendering of dynamic human avatars. Using a large-scale, cross-identity, multi-view video dataset, we demonstrate that our approach outperforms state-of-the-art human avatar generation methods.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [6] [Singular Value Few-shot Adaptation of Vision-Language Models](https://arxiv.org/abs/2509.03740)
*Taha Koleilat,Hassan Rivaz,Yiming Xiao*

Main category: cs.CV

TL;DR: CLIP-SVD是一种基于奇异值分解的多模态参数高效适应技术，仅需调整CLIP模型参数矩阵的奇异值，用0.04%的参数实现领域适应，在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型适应方法依赖提示工程和完整微调，成本高且可能破坏预训练知识。需要一种参数高效且能保持模型泛化能力的适应方法。

Method: 使用奇异值分解(SVD)修改CLIP内部参数空间，仅微调参数矩阵的奇异值来重新缩放基向量，不注入额外模块。

Result: 在11个自然数据集和10个生物医学数据集上达到最先进分类结果，在少样本设置下准确率和泛化能力均优于先前方法。

Conclusion: CLIP-SVD通过SVD实现参数高效的领域适应，在保持预训练知识的同时显著提升性能，并提供可解释性分析。

Abstract: Vision-language models (VLMs) like CLIP have shown impressive zero-shot and few-shot learning capabilities across diverse applications. However, adapting these models to new fine-grained domains remains difficult due to reliance on prompt engineering and the high cost of full model fine-tuning. Existing adaptation approaches rely on augmented components, such as prompt tokens and adapter modules, which could limit adaptation quality, destabilize the model, and compromise the rich knowledge learned during pretraining. In this work, we present \textbf{CLIP-SVD}, a novel \textit{multi-modal} and \textit{parameter-efficient} adaptation technique that leverages Singular Value Decomposition (SVD) to modify the internal parameter space of CLIP without injecting additional modules. Specifically, we fine-tune only the singular values of the CLIP parameter matrices to rescale the basis vectors for domain adaptation while retaining the pretrained model. This design enables enhanced adaptation performance using only \textbf{0.04\%} of the model's total parameters and better preservation of its generalization ability. CLIP-SVD achieves state-of-the-art classification results on 11 natural and 10 biomedical datasets, outperforming previous methods in both accuracy and generalization under few-shot settings. Additionally, we leverage a natural language-based approach to analyze the effectiveness and dynamics of the CLIP adaptation to allow interpretability of CLIP-SVD. The code is publicly available at https://github.com/HealthX-Lab/CLIP-SVD.

</details>


### [7] [Causality-guided Prompt Learning for Vision-language Models via Visual Granulation](https://arxiv.org/abs/2509.03803)
*Mengyu Gao,Qiulei Dong*

Main category: cs.CV

TL;DR: CaPL是一种基于因果推理的视觉粒度化文本提示学习方法，通过属性解耦和粒度学习模块，显著提升了CLIP在细粒度数据集上的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有的CLIP提示学习方法在细粒度数据集上表现有限，需要一种能够捕捉细粒度类别间细微差异的方法。

Method: 提出CaPL方法，包含属性解耦模块（使用布朗桥扩散模型分解视觉特征）和粒度学习模块（通过因果推理策略构建视觉粒度）。

Result: 在15个数据集上的实验表明，CaPL显著优于最先进的提示学习方法，特别是在细粒度数据集上。

Conclusion: 通过视觉粒度化和因果推理，CaPL能够学习更具区分性的文本提示，有效提升CLIP在细粒度识别任务中的性能。

Abstract: Prompt learning has recently attracted much attention for adapting pre-trained vision-language models (e.g., CLIP) to downstream recognition tasks. However, most of the existing CLIP-based prompt learning methods only show a limited ability for handling fine-grained datasets. To address this issue, we propose a causality-guided text prompt learning method via visual granulation for CLIP, called CaPL, where the explored visual granulation technique could construct sets of visual granules for the text prompt to capture subtle discrepancies among different fine-grained classes through casual inference. The CaPL method contains the following two modules: (1) An attribute disentanglement module is proposed to decompose visual features into non-individualized attributes (shared by some classes) and individualized attributes (specific to single classes) using a Brownian Bridge Diffusion Model; (2) A granule learning module is proposed to construct visual granules by integrating the aforementioned attributes for recognition under two causal inference strategies. Thanks to the learned visual granules, more discriminative text prompt is expected to be learned. Extensive experimental results on 15 datasets demonstrate that our CaPL method significantly outperforms the state-of-the-art prompt learning methods, especially on fine-grained datasets.

</details>


### [8] [Human Motion Video Generation: A Survey](https://arxiv.org/abs/2509.03883)
*Haiwei Xue,Xiangyang Luo,Zhanghao Hu,Xin Zhang,Xunzhi Xiang,Yuqin Dai,Jianzhuang Liu,Zhensong Zhang,Minglei Li,Jian Yang,Fei Ma,Zhiyong Wu,Changpeng Yang,Zonghong Dai,Fei Richard Yu*

Main category: cs.CV

TL;DR: 这是一份关于人体运动视频生成的综述性论文，系统分析了该领域的生成过程五大阶段、三种主要模态，并首次讨论了大语言模型在该领域的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的调查太过集中于单个方法，缺乏对整个人体运动视频生成过程的系统性概览。本文弥补了这一空白，提供了全面的视角。

Method: 论文将生成过程分为五个关键阶段：输入、运动规划、运动视频生成、精炼和输出。综述了视觉、文本和音频三种主要模态，涵盖了超过200篇论文。

Result: 该综述提供了人体运动视频生成领域的全面概览，标记了重要的里程碑性研究，并首次探讨了大语言模型在该领域的应用。

Conclusion: 这份综述揭示了人体运动视频生成的幻想前景，并为推进数字人类的全面应用提供了价值较高的资源。

Abstract: Human motion video generation has garnered significant research interest due to its broad applications, enabling innovations such as photorealistic singing heads or dynamic avatars that seamlessly dance to music. However, existing surveys in this field focus on individual methods, lacking a comprehensive overview of the entire generative process. This paper addresses this gap by providing an in-depth survey of human motion video generation, encompassing over ten sub-tasks, and detailing the five key phases of the generation process: input, motion planning, motion video generation, refinement, and output. Notably, this is the first survey that discusses the potential of large language models in enhancing human motion video generation. Our survey reviews the latest developments and technological trends in human motion video generation across three primary modalities: vision, text, and audio. By covering over two hundred papers, we offer a thorough overview of the field and highlight milestone works that have driven significant technological breakthroughs. Our goal for this survey is to unveil the prospects of human motion video generation and serve as a valuable resource for advancing the comprehensive applications of digital humans. A complete list of the models examined in this survey is available in Our Repository https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.

</details>


### [9] [MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image Generation](https://arxiv.org/abs/2509.04126)
*Yuan Zhao,Liu Lin*

Main category: cs.CV

TL;DR: 提出了MEPG框架，通过位置-风格感知LLM和多专家扩散模块，解决了文本到图像扩散模型在复杂多元素提示和风格多样性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型在处理复杂多元素提示时表现不佳，且风格多样性有限，需要一种能够精确控制空间布局和风格的专业化解决方案。

Method: 使用监督微调的LLM分解输入提示为精确空间坐标和风格编码语义指令，通过注意力门控机制在局部区域和全局区域动态路由专家模型（如写实专家、风格化专家）。

Result: 实验表明MEPG在图像质量和风格多样性方面显著优于相同骨干网络的基线模型。

Conclusion: MEPG框架通过专业化模块的协同整合，有效提升了复杂多元素图像生成的质量和风格控制能力，具有轻量级集成和强扩展性优势。

Abstract: Text-to-image diffusion models have achieved remarkable image quality, but they still struggle with complex, multiele ment prompts, and limited stylistic diversity. To address these limitations, we propose a Multi-Expert Planning and Gen eration Framework (MEPG) that synergistically integrates position- and style-aware large language models (LLMs) with spatial-semantic expert modules. The framework comprises two core components: (1) a Position-Style-Aware (PSA) module that utilizes a supervised fine-tuned LLM to decom pose input prompts into precise spatial coordinates and style encoded semantic instructions; and (2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera tion through dynamic expert routing across both local regions and global areas. During the generation process for each lo cal region, specialized models (e.g., realism experts, styliza tion specialists) are selectively activated for each spatial par tition via attention-based gating mechanisms. The architec ture supports lightweight integration and replacement of ex pert models, providing strong extensibility. Additionally, an interactive interface enables real-time spatial layout editing and per-region style selection from a portfolio of experts. Ex periments show that MEPG significantly outperforms base line models with the same backbone in both image quality   and style diversity.

</details>


### [10] [TauGenNet: Plasma-Driven Tau PET Image Synthesis via Text-Guided 3D Diffusion Models](https://arxiv.org/abs/2509.04269)
*Yuxin Gong,Se-in Jang,Wei Shao,Yi Su,Kuang Gong*

Main category: cs.CV

TL;DR: 提出基于文本引导的3D扩散模型，利用结构MRI和血浆p-tau217测量值合成3D tau PET图像，为阿尔茨海默病诊断提供低成本替代方案


<details>
  <summary>Details</summary>
Motivation: tau PET成像成本高且可用性有限，而结构MRI和血浆生物标志物具有非侵入性和广泛可用性，需要开发一种经济有效的tau病理可视化方法

Method: 使用文本引导的3D扩散模型，以血浆p-tau217测量值作为文本提示，结构MRI提供解剖结构约束，在ADNI数据库的AV1451 tau PET数据上进行训练和评估

Result: 能够生成跨不同疾病阶段的真实、临床有意义的3D tau PET图像，支持数据增强和疾病进展模拟

Conclusion: 该框架可作为tau PET的非侵入性、经济有效替代方案，有助于在不同血浆生物标志物水平和认知条件下可视化tau病理和模拟疾病进展

Abstract: Accurate quantification of tau pathology via tau positron emission tomography (PET) scan is crucial for diagnosing and monitoring Alzheimer's disease (AD). However, the high cost and limited availability of tau PET restrict its widespread use. In contrast, structural magnetic resonance imaging (MRI) and plasma-based biomarkers provide non-invasive and widely available complementary information related to brain anatomy and disease progression. In this work, we propose a text-guided 3D diffusion model for 3D tau PET image synthesis, leveraging multimodal conditions from both structural MRI and plasma measurement. Specifically, the textual prompt is from the plasma p-tau217 measurement, which is a key indicator of AD progression, while MRI provides anatomical structure constraints. The proposed framework is trained and evaluated using clinical AV1451 tau PET data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. Experimental results demonstrate that our approach can generate realistic, clinically meaningful 3D tau PET across a range of disease stages. The proposed framework can help perform tau PET data augmentation under different settings, provide a non-invasive, cost-effective alternative for visualizing tau pathology, and support the simulation of disease progression under varying plasma biomarker levels and cognitive conditions.

</details>


### [11] [SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer](https://arxiv.org/abs/2509.04379)
*Jimin Xu,Bosheng Qin,Tao Jin,Zhou Zhao,Zhenhui Ye,Jun Yu,Fei Wu*

Main category: cs.CV

TL;DR: 一种新的3D风格转换流水线，利用预训练2D扩散模型的知识，通过跨视图风格对齐和实例级风格转换，实现了更结构化、视觉一致的3D场景风格化效果。


<details>
  <summary>Details</summary>
Motivation: 现有的3D风格转换方法无法有效提取和转换高级别的风格语义，并且风格化结果缺乏结构清晰度和分离性，导致不同实例或物体难以区分。

Method: 提出两阶段流水线：首先利用扩散模型生成关键视点的风格化渲染，然后将风格化的关键视图转换到3D表示中。包含跨视图风格对齐（在UNet最后一个上采样块中插入跨视图注意力）和实例级风格转换两个创新设计。

Result: 实验结果显示，该方法在各种场景上（从前向视角到具有挑战性的360度环境）都显著超过了现有的最先进方法。

Conclusion: 该研究提出的新方3D风格转换流水线通过利用扩散模型的知识，实现了更结构化、视觉一致和艺术丰富的3D场景风格化效果，解决了现有方法在高级风格语义提取和实例区分方面的问题。

Abstract: Recent advancements in neural representations, such as Neural Radiance Fields and 3D Gaussian Splatting, have increased interest in applying style transfer to 3D scenes. While existing methods can transfer style patterns onto 3D-consistent neural representations, they struggle to effectively extract and transfer high-level style semantics from the reference style image. Additionally, the stylized results often lack structural clarity and separation, making it difficult to distinguish between different instances or objects within the 3D scene. To address these limitations, we propose a novel 3D style transfer pipeline that effectively integrates prior knowledge from pretrained 2D diffusion models. Our pipeline consists of two key stages: First, we leverage diffusion priors to generate stylized renderings of key viewpoints. Then, we transfer the stylized key views onto the 3D representation. This process incorporates two innovative designs. The first is cross-view style alignment, which inserts cross-view attention into the last upsampling block of the UNet, allowing feature interactions across multiple key views. This ensures that the diffusion model generates stylized key views that maintain both style fidelity and instance-level consistency. The second is instance-level style transfer, which effectively leverages instance-level consistency across stylized key views and transfers it onto the 3D representation. This results in a more structured, visually coherent, and artistically enriched stylization. Extensive qualitative and quantitative experiments demonstrate that our 3D style transfer pipeline significantly outperforms state-of-the-art methods across a wide range of scenes, from forward-facing to challenging 360-degree environments. Visit our project page https://jm-xu.github.io/SSGaussian for immersive visualization.

</details>


### [12] [Durian: Dual Reference-guided Portrait Animation with Attribute Transfer](https://arxiv.org/abs/2509.04434)
*Hyunsoo Cha,Byungjun Kim,Hanbyul Joo*

Main category: cs.CV

TL;DR: Durian是首个零样本肖像动画生成方法，能够从参考图像向目标肖像进行面部属性迁移，通过双参考网络和扩散模型实现高保真度、空间一致的跨帧属性迁移。


<details>
  <summary>Details</summary>
Motivation: 现有的肖像动画生成方法缺乏有效的零样本面部属性迁移能力，需要开发一种能够在不同帧间保持空间一致性的高保真属性迁移技术。

Method: 采用双参考网络将肖像和属性图像的空间特征注入扩散模型去噪过程；使用自重建训练框架，从同一视频采样帧作为属性和目标参考；提出基于关键点的掩码扩展策略和空间外观变换增强方法。

Result: Durian在肖像动画属性迁移任务上达到了最先进的性能，其双参考设计支持单次生成过程中的多属性组合，无需额外训练。

Conclusion: 该方法通过创新的双参考网络设计和训练策略，成功实现了零样本条件下的高质量肖像动画生成和属性迁移，具有良好的泛化能力和实用性。

Abstract: We present Durian, the first method for generating portrait animation videos with facial attribute transfer from a given reference image to a target portrait in a zero-shot manner. To enable high-fidelity and spatially consistent attribute transfer across frames, we introduce dual reference networks that inject spatial features from both the portrait and attribute images into the denoising process of a diffusion model. We train the model using a self-reconstruction formulation, where two frames are sampled from the same portrait video: one is treated as the attribute reference and the other as the target portrait, and the remaining frames are reconstructed conditioned on these inputs and their corresponding masks. To support the transfer of attributes with varying spatial extent, we propose a mask expansion strategy using keypoint-conditioned image generation for training. In addition, we further augment the attribute and portrait images with spatial and appearance-level transformations to improve robustness to positional misalignment between them. These strategies allow the model to effectively generalize across diverse attributes and in-the-wild reference combinations, despite being trained without explicit triplet supervision. Durian achieves state-of-the-art performance on portrait animation with attribute transfer, and notably, its dual reference design enables multi-attribute composition in a single generation pass without additional training.

</details>


### [13] [Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual Try-On from a Single Image -- Technical Preview](https://arxiv.org/abs/2509.04450)
*Jun-Kun Chen,Aayush Bansal,Minh Phuoc Vo,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: VFR是一个创新的视频生成模型，能够生成任意长度的虚拟试穿视频，通过分段自回归生成方式解决长视频生成问题，确保局部平滑性和全局时间一致性。


<details>
  <summary>Details</summary>
Motivation: 传统虚拟试穿视频生成需要大量计算资源和长视频数据，且难以生成任意长度的视频。需要解决相邻片段间的局部平滑性和不同片段间的全局时间一致性挑战。

Method: 采用自回归的分段生成过程，使用前缀视频条件确保局部平滑性，通过锚点视频（360度全身视频）来保持全局时间一致性。

Result: 成功生成了分钟级别的虚拟试穿视频，在各种动作下都能保持局部平滑性和全局时间一致性。

Conclusion: VFR是长虚拟试穿视频生成领域的开创性工作，为任意长度视频生成提供了有效解决方案。

Abstract: We introduce the Virtual Fitting Room (VFR), a novel video generative model that produces arbitrarily long virtual try-on videos. Our VFR models long video generation tasks as an auto-regressive, segment-by-segment generation process, eliminating the need for resource-intensive generation and lengthy video data, while providing the flexibility to generate videos of arbitrary length. The key challenges of this task are twofold: ensuring local smoothness between adjacent segments and maintaining global temporal consistency across different segments. To address these challenges, we propose our VFR framework, which ensures smoothness through a prefix video condition and enforces consistency with the anchor video -- a 360-degree video that comprehensively captures the human's wholebody appearance. Our VFR generates minute-scale virtual try-on videos with both local smoothness and global temporal consistency under various motions, making it a pioneering work in long virtual try-on video generation.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [14] [The Chaotic Art: Quantum Representation and Manipulation of Color](https://arxiv.org/abs/2509.03542)
*Guosheng Hu*

Main category: quant-ph

TL;DR: 量子计算技术通过颜色卡利表示、颜色通道处理和量子图像生成的方式，为颜色计算提供了新技术路径，建立了经典色彩学与量子图形学之间的桥梁。


<details>
  <summary>Details</summary>
Motivation: 利用量子计算的独特计算原理，深刻改变颜色艺术的景观，为颜色计算提供新的技术可能性。

Method: 通过实验探索颜色卡利表示、颜色通道处理和量子图像生成，在Qiskit和IBM Q中进行编程实验，将数字颜色用量子位表示、操作和测量，然后恢复为经典计算机的计算结果。

Result: 该方法已经被证明作为颜色卡利表示和量子计算的艺术技术是可行的，并为信息可视化、图像处理和更多颜色计算任务提供了量子计算机的应用可能。

Conclusion: 量子计算有望推动新的颜色理论和艺术概念的发展，通过建立经典色彩学与量子图形学之间的联系，为颜色计算领域带来革命性的变化。

Abstract: Due to its unique computing principles, quantum computing technology will profoundly change the spectacle of color art. Focusing on experimental exploration of color qubit representation, color channel processing, and color image generation via quantum computing, this article proposes a new technical path for color computing in quantum computing environment, by which digital color is represented, operated, and measured in quantum bits, and then restored for classical computers as computing results. This method has been proved practicable as an artistic technique of color qubit representation and quantum computing via programming experiments in Qiskit and IBM Q. By building a bridge between classical chromatics and quantum graphics, quantum computers can be used for information visualization, image processing, and more color computing tasks. Furthermore, quantum computing can be expected to facilitate new color theories and artistic concepts.

</details>
