<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 3]
- [cs.CV](#cs.CV) [Total: 46]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [eess.IV](#eess.IV) [Total: 2]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [ControlHair: Physically-based Video Diffusion for Controllable Dynamic Hair Rendering](https://arxiv.org/abs/2509.21541)
*Weikai Lin,Haoxiang Li,Yuhao Zhu*

Main category: cs.GR

TL;DR: ControlHair是一个混合框架，结合物理模拟器和条件视频扩散模型，实现可控的动态头发渲染。


<details>
  <summary>Details</summary>
Motivation: 现有的视频扩散模型缺乏对头发动态的精细控制，而头发模拟和渲染面临复杂的动力学、材料属性和光照交互挑战。

Method: 采用三阶段流程：1）用物理模拟器将物理参数编码为逐帧几何；2）提取逐帧控制信号；3）将控制信号输入视频扩散模型生成具有所需头发动态的视频。

Result: 在10K视频数据集上训练，ControlHair在头发动态控制方面优于基于文本和姿态的基线方法。

Conclusion: ControlHair是首个物理信息视频扩散框架，支持动态发型试戴、子弹时间效果和电影图形等应用。

Abstract: Hair simulation and rendering are challenging due to complex strand dynamics, diverse material properties, and intricate light-hair interactions. Recent video diffusion models can generate high-quality videos, but they lack fine-grained control over hair dynamics. We present ControlHair, a hybrid framework that integrates a physics simulator with conditional video diffusion to enable controllable dynamic hair rendering. ControlHair adopts a three-stage pipeline: it first encodes physics parameters (e.g., hair stiffness, wind) into per-frame geometry using a simulator, then extracts per-frame control signals, and finally feeds control signals into a video diffusion model to generate videos with desired hair dynamics. This cascaded design decouples physics reasoning from video generation, supports diverse physics, and makes training the video diffusion model easy. Trained on a curated 10K video dataset, ControlHair outperforms text- and pose-conditioned baselines, delivering precisely controlled hair dynamics. We further demonstrate three use cases of ControlHair: dynamic hairstyle try-on, bullet-time effects, and cinemagraphic. ControlHair introduces the first physics-informed video diffusion framework for controllable dynamics. We provide a teaser video and experimental results on our website.

</details>


### [2] [PowerGS: Display-Rendering Power Co-Optimization for Neural Rendering in Power-Constrained XR Systems](https://arxiv.org/abs/2509.21702)
*Weikai Lin,Sushant Kondguli,Carl Marshall,Yuhao Zhu*

Main category: cs.GR

TL;DR: PowerGS是一个针对3D高斯泼溅(3DGS)的功耗优化框架，在保持质量约束下联合最小化渲染和显示功耗，可实现高达86%的总功耗降低。


<details>
  <summary>Details</summary>
Motivation: 3DGS系列模型在功耗受限的XR设备上效率低下，这些设备需要在瓦特级别运行，因此需要开发功耗优化方案。

Method: 提出了通用问题公式化，通过识别显示和渲染功耗景观中的等质量曲线，并找到给定曲线上功耗最小的点（在适当参数化下具有闭式解），支持注视点渲染以进一步节省功耗。

Result: 广泛的实验和用户研究表明，PowerGS相比最先进的3DGS模型实现了高达86%的总功耗降低，在主观和客观质量上损失最小。

Conclusion: PowerGS是首个在质量约束下联合优化3DGS渲染和显示功耗的框架，为功耗受限的XR设备提供了有效的解决方案。

Abstract: 3D Gaussian Splatting (3DGS) combines classic image-based rendering, pointbased graphics, and modern differentiable techniques, and offers an interesting alternative to traditional physically-based rendering. 3DGS-family models are far from efficient for power-constrained Extended Reality (XR) devices, which need to operate at a Watt-level. This paper introduces PowerGS, the first framework to jointly minimize the rendering and display power in 3DGS under a quality constraint. We present a general problem formulation and show that solving the problem amounts to 1) identifying the iso-quality curve(s) in the landscape subtended by the display and rendering power and 2) identifying the power-minimal point on a given curve, which has a closed-form solution given a proper parameterization of the curves. PowerGS also readily supports foveated rendering for further power savings. Extensive experiments and user studies show that PowerGS achieves up to 86% total power reduction compared to state-of-the-art 3DGS models, with minimal loss in both subjective and objective quality. Code is available at https://github.com/horizon-research/PowerGS.

</details>


### [3] [Rigidity-Aware 3D Gaussian Deformation from a Single Image](https://arxiv.org/abs/2509.22222)
*Jinhyeok Kim,Jaehun Bang,Seunghyun Seo,Kyungdon Joo*

Main category: cs.GR

TL;DR: DeformSplat：从单张图像重建3D高斯变形的新框架，通过高斯-像素匹配和刚性部分分割技术实现


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多视角视频重建变形，限制了在受限场景下的应用，需要从单张图像恢复物体变形的解决方案

Method: 提出高斯-像素匹配技术弥合3D高斯表示与2D像素观测的领域差距；开发刚性部分分割（初始化和细化）识别刚性区域以保持几何一致性

Result: 实验表明该方法显著优于现有方法，并能自然扩展到帧插值和交互式物体操作等应用

Conclusion: DeformSplat框架成功实现了从单张图像重建一致变形，解决了现有方法对多视角数据的依赖问题

Abstract: Reconstructing object deformation from a single image remains a significant challenge in computer vision and graphics. Existing methods typically rely on multi-view video to recover deformation, limiting their applicability under constrained scenarios. To address this, we propose DeformSplat, a novel framework that effectively guides 3D Gaussian deformation from only a single image. Our method introduces two main technical contributions. First, we present Gaussian-to-Pixel Matching which bridges the domain gap between 3D Gaussian representations and 2D pixel observations. This enables robust deformation guidance from sparse visual cues. Second, we propose Rigid Part Segmentation consisting of initialization and refinement. This segmentation explicitly identifies rigid regions, crucial for maintaining geometric coherence during deformation. By combining these two techniques, our approach can reconstruct consistent deformations from a single image. Extensive experiments demonstrate that our approach significantly outperforms existing methods and naturally extends to various applications,such as frame interpolation and interactive object manipulation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [Automated Prompt Generation for Creative and Counterfactual Text-to-image Synthesis](https://arxiv.org/abs/2509.21375)
*Aleksa Jelaca,Ying Jiao,Chang Tian,Marie-Francine Moens*

Main category: cs.CV

TL;DR: 提出了一个自动提示工程框架，用于生成反事实尺寸的图像（如微小的海象和巨大的按钮），通过图像评估器、监督提示重写器和DPO训练的排序器来优化提示，构建了首个反事实尺寸文本-图像数据集。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像生成中细粒度可控性的挑战，特别是反事实可控性（生成违背常识模式的图像），这对于创意和探索性应用至关重要。

Method: 开发了三组件框架：图像评估器指导数据集构建，监督提示重写器生成修订提示，DPO训练的排序器选择最佳修订提示。扩展了Grounded SAM以改进图像评估器。

Result: 图像评估器性能比其骨干网络提升了114%，实验表明该方法优于最先进的基线和ChatGPT-4o。

Conclusion: 该方法为反事实可控性的未来研究奠定了基础，在反事实尺寸生成方面表现出色。

Abstract: Text-to-image generation has advanced rapidly with large-scale multimodal training, yet fine-grained controllability remains a critical challenge. Counterfactual controllability, defined as the capacity to deliberately generate images that contradict common-sense patterns, remains a major challenge but plays a crucial role in enabling creativity and exploratory applications. In this work, we address this gap with a focus on counterfactual size (e.g., generating a tiny walrus beside a giant button) and propose an automatic prompt engineering framework that adapts base prompts into revised prompts for counterfactual images. The framework comprises three components: an image evaluator that guides dataset construction by identifying successful image generations, a supervised prompt rewriter that produces revised prompts, and a DPO-trained ranker that selects the optimal revised prompt. We construct the first counterfactual size text-image dataset and enhance the image evaluator by extending Grounded SAM with refinements, achieving a 114 percent improvement over its backbone. Experiments demonstrate that our method outperforms state-of-the-art baselines and ChatGPT-4o, establishing a foundation for future research on counterfactual controllability.

</details>


### [5] [In silico Deep Learning Protocols for Label-Free Super-Resolution Microscopy: A Comparative Study of Network Architectures and SNR Dependence](https://arxiv.org/abs/2509.21376)
*Shiraz S Kaderuppan,Jonathan Mar,Andrew Irvine,Anurag Sharma,Muhammad Ramadan Saifuddin,Wai Leong Eugene Wong,Wai Lok Woo*

Main category: cs.CV

TL;DR: 本研究评估了使用深度学习神经网络（O-Net和Theta-Net）对非荧光相位调制显微镜图像进行超分辨率重建的方法，发现两种网络在不同信噪比条件下具有互补性。


<details>
  <summary>Details</summary>
Motivation: 解决传统光学显微镜分辨率限制（约200nm）的问题，避免使用昂贵的专用设备或荧光超分辨率技术，为普通显微镜用户提供经济可行的超分辨率解决方案。

Method: 使用两种自研的深度神经网络架构（O-Net和Theta-Net），对Zernike相衬和微分干涉相衬显微镜图像进行超分辨率重建，通过原子力显微镜校准的纳米级测试靶标进行评估。

Result: 两种网络在超分辨率重建中表现良好但具有互补性：高信噪比图像适合使用O-Net模型，低信噪比图像更适合Theta-Net模型。

Conclusion: 模型架构和源图像信噪比对深度学习模型在非荧光光学纳米显微镜中的性能至关重要，即使使用相同的训练数据和训练周期，不同架构在不同条件下表现各异。

Abstract: The field of optical microscopy spans across numerous industries and research domains, ranging from education to healthcare, quality inspection and analysis. Nonetheless, a key limitation often cited by optical microscopists refers to the limit of its lateral resolution (typically defined as ~200nm), with potential circumventions involving either costly external modules (e.g. confocal scan heads, etc) and/or specialized techniques [e.g. super-resolution (SR) fluorescent microscopy]. Addressing these challenges in a normal (non-specialist) context thus remains an aspect outside the scope of most microscope users & facilities. This study thus seeks to evaluate an alternative & economical approach to achieving SR optical microscopy, involving non-fluorescent phase-modulated microscopical modalities such as Zernike phase contrast (PCM) and differential interference contrast (DIC) microscopy. Two in silico deep neural network (DNN) architectures which we developed previously (termed O-Net and Theta-Net) are assessed on their abilities to resolve a custom-fabricated test target containing nanoscale features calibrated via atomic force microscopy (AFM). The results of our study demonstrate that although both O-Net and Theta-Net seemingly performed well when super-resolving these images, they were complementary (rather than competing) approaches to be considered for image SR, particularly under different image signal-to-noise ratios (SNRs). High image SNRs favoured the application of O-Net models, while low SNRs inclined preferentially towards Theta-Net models. These findings demonstrate the importance of model architectures (in conjunction with the source image SNR) on model performance and the SR quality of the generated images where DNN models are utilized for non-fluorescent optical nanoscopy, even where the same training dataset & number of epochs are being used.

</details>


### [6] [Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation](https://arxiv.org/abs/2509.21377)
*Yinfeng Yu,Hailong Zhang,Meiling Zhu*

Main category: cs.CV

TL;DR: 提出DMTF-AVN方法，通过多目标架构和Transformer机制动态融合视听信息，在机器人导航中实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有视听融合方法往往忽视更深层次的感知上下文，需要更有效地利用多模态线索来指导导航

Method: 使用多目标架构配合精炼的Transformer机制，过滤和选择性融合跨模态信息

Result: 在Replica和Matterport3D数据集上实现最先进性能，在成功率、路径效率和场景适应性方面优于现有方法

Conclusion: 模型展现出强大的可扩展性和泛化能力，为机器人导航中的先进多模态融合策略铺平道路

Abstract: Audiovisual embodied navigation enables robots to locate audio sources by dynamically integrating visual observations from onboard sensors with the auditory signals emitted by the target. The core challenge lies in effectively leveraging multimodal cues to guide navigation. While prior works have explored basic fusion of visual and audio data, they often overlook deeper perceptual context. To address this, we propose the Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation (DMTF-AVN). Our approach uses a multi-target architecture coupled with a refined Transformer mechanism to filter and selectively fuse cross-modal information. Extensive experiments on the Replica and Matterport3D datasets demonstrate that DMTF-AVN achieves state-of-the-art performance, outperforming existing methods in success rate (SR), path efficiency (SPL), and scene adaptation (SNA). Furthermore, the model exhibits strong scalability and generalizability, paving the way for advanced multimodal fusion strategies in robotic navigation. The code and videos are available at   https://github.com/zzzmmm-svg/DMTF.

</details>


### [7] [Downscaling climate projections to 1 km with single-image super resolution](https://arxiv.org/abs/2509.21399)
*Petr Košťál,Pavel Kordík,Ondřej Podsztavek*

Main category: cs.CV

TL;DR: 使用单图像超分辨率模型将12.5公里分辨率的气候预测统计降尺度到1公里分辨率，通过气候指标评估方法验证降尺度效果。


<details>
  <summary>Details</summary>
Motivation: 现有气候预测空间分辨率较低（12.5公里），限制了在地方决策中的可用性，需要高分辨率气候预测来支持本地决策。

Method: 在观测网格数据集上训练单图像超分辨率模型，然后应用于低分辨率气候预测；提出基于气候指标的评估方法，使用气象站观测数据计算气候指标来评估降尺度结果。

Result: 日平均温度实验表明，单图像超分辨率模型能够降尺度气候预测，且不会增加气候指标误差。

Conclusion: 单图像超分辨率模型可以有效降尺度气候预测，为地方决策提供高分辨率气候数据支持。

Abstract: High-resolution climate projections are essential for local decision-making. However, available climate projections have low spatial resolution (e.g. 12.5 km), which limits their usability. We address this limitation by leveraging single-image super-resolution models to statistically downscale climate projections to 1-km resolution. Since high-resolution climate projections are unavailable for training, we train models on a high-resolution observational gridded data set and apply them to low-resolution climate projections. We propose a climate indicator-based assessment using observed climate indices computed at weather station locations to evaluate the downscaled climate projections without ground-truth high-resolution climate projections. Experiments on daily mean temperature demonstrate that single-image super-resolution models can downscale climate projections without increasing the error of climate indicators compared to low-resolution climate projections.

</details>


### [8] [DyME: Dynamic Multi-Concept Erasure in Diffusion Models with Bi-Level Orthogonal LoRA Adaptation](https://arxiv.org/abs/2509.21433)
*Jiaqi Liu,Lan Zhang,Xiaoyong Yuan*

Main category: cs.CV

TL;DR: DyME是一个按需概念擦除框架，通过训练轻量级LoRA适配器并动态组合来实现灵活的多概念擦除，解决了现有静态擦除方法在多概念和冲突概念场景下的局限性。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型会无意中复制受版权保护的风格和视觉概念，引发法律和伦理问题。现有概念擦除方法无法扩展到实际场景中需要擦除多个可能冲突概念的情况。

Method: 提出DyME框架：训练概念特定的LoRA适配器，在推理时动态组合所需适配器；引入双层正交性约束（特征级和参数级）来解耦表示偏移并强制正交适配器子空间。

Result: 在ErasureBench-H和标准数据集上的实验表明，DyME持续优于最先进的基线方法，实现了更高的多概念擦除保真度，同时最小化了对非目标内容的附带损害。

Conclusion: DyME通过模块化设计和正交性约束，有效解决了多概念擦除中的干扰问题，为实际应用提供了灵活且高效的解决方案。

Abstract: Text-to-image diffusion models (DMs) inadvertently reproduce copyrighted styles and protected visual concepts, raising legal and ethical concerns. Concept erasure has emerged as a safeguard, aiming to selectively suppress such concepts through fine-tuning. However, existing methods do not scale to practical settings where providers must erase multiple and possibly conflicting concepts. The core bottleneck is their reliance on static erasure: a single checkpoint is fine-tuned to remove all target concepts, regardless of the actual erasure needs at inference. This rigid design mismatches real-world usage, where requests vary per generation, leading to degraded erasure success and reduced fidelity for non-target content. We propose DyME, an on-demand erasure framework that trains lightweight, concept-specific LoRA adapters and dynamically composes only those needed at inference. This modular design enables flexible multi-concept erasure, but naive composition causes interference among adapters, especially when many or semantically related concepts are suppressed. To overcome this, we introduce bi-level orthogonality constraints at both the feature and parameter levels, disentangling representation shifts and enforcing orthogonal adapter subspaces. We further develop ErasureBench-H, a new hierarchical benchmark with brand-series-character structure, enabling principled evaluation across semantic granularities and erasure set sizes. Experiments on ErasureBench-H and standard datasets (e.g., CIFAR-100, Imagenette) demonstrate that DyME consistently outperforms state-of-the-art baselines, achieving higher multi-concept erasure fidelity with minimal collateral degradation.

</details>


### [9] [X-Streamer: Unified Human World Modeling with Audiovisual Interaction](https://arxiv.org/abs/2509.21574)
*You Xie,Tianpei Gu,Zenan Li,Chenxu Zhang,Guoxian Song,Xiaochen Zhao,Chao Liang,Jianwen Jiang,Hongyi Xu,Linjie Luo*

Main category: cs.CV

TL;DR: X-Streamer是一个端到端多模态人类世界建模框架，能够从单张肖像生成实时、开放式的视频通话，支持文本、语音和视频的无限交互。


<details>
  <summary>Details</summary>
Motivation: 构建能够在单一统一架构中实现文本、语音和视频无限交互的数字人类代理，将静态肖像转变为持久且智能的视听交互体验。

Method: 采用Thinker-Actor双变换器架构：Thinker模块基于预训练的大语言-语音模型感知和推理流式用户输入；Actor模块使用分块自回归扩散模型，通过交叉注意力机制将Thinker的隐藏状态转换为时间对齐的多模态响应。

Result: X-Streamer在两个A100 GPU上实时运行，能够从任意肖像维持数小时一致的视频聊天体验，实现了交互式数字人类的统一世界建模。

Conclusion: X-Streamer为构建统一世界建模的交互式数字人类铺平了道路，实现了从静态肖像到实时多模态交互的突破。

Abstract: We introduce X-Streamer, an end-to-end multimodal human world modeling framework for building digital human agents capable of infinite interactions across text, speech, and video within a single unified architecture. Starting from a single portrait, X-Streamer enables real-time, open-ended video calls driven by streaming multimodal inputs. At its core is a Thinker-Actor dual-transformer architecture that unifies multimodal understanding and generation, turning a static portrait into persistent and intelligent audiovisual interactions. The Thinker module perceives and reasons over streaming user inputs, while its hidden states are translated by the Actor into synchronized multimodal streams in real time. Concretely, the Thinker leverages a pretrained large language-speech model, while the Actor employs a chunk-wise autoregressive diffusion model that cross-attends to the Thinker's hidden states to produce time-aligned multimodal responses with interleaved discrete text and audio tokens and continuous video latents. To ensure long-horizon stability, we design inter- and intra-chunk attentions with time-aligned multimodal positional embeddings for fine-grained cross-modality alignment and context retention, further reinforced by chunk-wise diffusion forcing and global identity referencing. X-Streamer runs in real time on two A100 GPUs, sustaining hours-long consistent video chat experiences from arbitrary portraits and paving the way toward unified world modeling of interactive digital humans.

</details>


### [10] [FantasyWorld: Geometry-Consistent World Modeling via Unified Video and 3D Prediction](https://arxiv.org/abs/2509.21657)
*Yixiang Dai,Fan Jiang,Chiyu Wang,Mu Xu,Yonggang Qi*

Main category: cs.CV

TL;DR: FantasyWorld是一个几何增强框架，通过为冻结的视频基础模型添加可训练的几何分支，实现视频潜在空间和隐式3D场的联合建模，从而获得具有3D感知能力的视频表示。


<details>
  <summary>Details</summary>
Motivation: 当前视频基础模型虽然具有强大的想象力先验，但缺乏明确的3D基础能力，导致空间一致性不足，限制了在3D推理任务中的应用。

Method: 引入跨分支监督机制，几何线索指导视频生成，视频先验正则化3D预测，在单次前向传播中联合建模视频潜在空间和隐式3D场。

Result: 实验表明FantasyWorld有效桥接了视频想象和3D感知，在多视角一致性和风格一致性方面优于现有几何一致基线。

Conclusion: 该框架通过统一骨干网络和跨分支信息交换，实现了3D感知视频表示，为下游3D任务提供了无需逐场景优化的通用表示。

Abstract: High-quality 3D world models are pivotal for embodied intelligence and Artificial General Intelligence (AGI), underpinning applications such as AR/VR content creation and robotic navigation. Despite the established strong imaginative priors, current video foundation models lack explicit 3D grounding capabilities, thus being limited in both spatial consistency and their utility for downstream 3D reasoning tasks. In this work, we present FantasyWorld, a geometry-enhanced framework that augments frozen video foundation models with a trainable geometric branch, enabling joint modeling of video latents and an implicit 3D field in a single forward pass. Our approach introduces cross-branch supervision, where geometry cues guide video generation and video priors regularize 3D prediction, thus yielding consistent and generalizable 3D-aware video representations. Notably, the resulting latents from the geometric branch can potentially serve as versatile representations for downstream 3D tasks such as novel view synthesis and navigation, without requiring per-scene optimization or fine-tuning. Extensive experiments show that FantasyWorld effectively bridges video imagination and 3D perception, outperforming recent geometry-consistent baselines in multi-view coherence and style consistency. Ablation studies further confirm that these gains stem from the unified backbone and cross-branch information exchange.

</details>


### [11] [MORPH: Shape-agnostic PDE Foundation Models](https://arxiv.org/abs/2509.21670)
*Mahindra Singh Rautela,Alexander Most,Siddharth Mansingh,Bradley C. Love,Ayan Biswas,Diane Oyen,Earl Lawrence*

Main category: cs.CV

TL;DR: MORPH是一个形状无关的自回归基础模型，用于偏微分方程(PDEs)。它基于卷积视觉变换器架构，能处理异构时空数据集，支持不同维度(1D-3D)、分辨率和混合标量/矢量场的PDE数据。


<details>
  <summary>Details</summary>
Motivation: 解决科学机器学习中处理异构、多模态PDE数据的挑战，开发一个灵活且强大的基础模型来处理不同维度、分辨率和物理场的PDE问题。

Method: 采用卷积视觉变换器架构，包含三个关键组件：(i)分量卷积联合处理标量和矢量通道；(ii)场间交叉注意力建模不同物理场间信息传播；(iii)轴向注意力分解时空自注意力以降低计算负担。

Result: 在多种异构PDE数据集上预训练，并在下游预测任务中评估。使用全模型微调和LoRA适配器，MORPH在零样本和全样本泛化中均优于从头训练的模型，匹配或超越强基线和最新SOTA模型。

Conclusion: MORPH为学习科学观测的异构和多模态特性提供了灵活强大的基础架构，为实现可扩展和数据高效的科学机器学习开辟了道路。

Abstract: We introduce MORPH, a shape-agnostic, autoregressive foundation model for partial differential equations (PDEs). MORPH is built on a convolutional vision transformer backbone that seamlessly handles heterogeneous spatiotemporal datasets of varying data dimensionality (1D--3D) at different resolutions, multiple fields with mixed scalar and vector components. The architecture combines (i) component-wise convolution, which jointly processes scalar and vector channels to capture local interactions, (ii) inter-field cross-attention, which models and selectively propagates information between different physical fields, (iii) axial attentions, which factorizes full spatiotemporal self-attention along individual spatial and temporal axes to reduce computational burden while retaining expressivity. We pretrain multiple model variants on a diverse collection of heterogeneous PDE datasets and evaluate transfer to a range of downstream prediction tasks. Using both full-model fine-tuning and parameter-efficient low-rank adapters (LoRA), MORPH outperforms models trained from scratch in both zero-shot and full-shot generalization. Across extensive evaluations, MORPH matches or surpasses strong baselines and recent state-of-the-art models. Collectively, these capabilities present a flexible and powerful backbone for learning from heterogeneous and multimodal nature of scientific observations, charting a path toward scalable and data-efficient scientific machine learning.

</details>


### [12] [DeLiVR: Differential Spatiotemporal Lie Bias for Efficient Video Deraining](https://arxiv.org/abs/2509.21719)
*Shuning Sun,Jialang Lu,Xiang Chen,Jichao Wang,Dianjie Lu,Guijuan Zhang,Guangwei Gao,Zhuoran Zheng*

Main category: cs.CV

TL;DR: DeLiVR是一种高效的视频去雨方法，通过将时空李群微分偏置直接注入网络注意力分数中，实现几何一致的帧对齐和雨纹方向匹配。


<details>
  <summary>Details</summary>
Motivation: 解决野外拍摄视频中的雨纹、模糊和噪声问题，以及相机姿态微小变化导致的跨帧不匹配和时间伪影。现有方法依赖光流或启发式对齐，计算成本高且鲁棒性差。

Method: 提出两个互补组件：1) 旋转有界李相对偏置，通过紧凑预测模块预测每帧平面内角度，实现几何一致对齐；2) 微分群位移，计算相邻帧间角度差估计速度，结合时间衰减和注意力掩码聚焦帧间关系。

Result: 在公开基准测试上的广泛实验结果表明该方法的有效性。

Conclusion: 李群为视频建模中的空间和时间一致性提供了原则性表示方法，DeLiVR通过注入李群微分偏置实现了高效鲁棒的视频去雨。

Abstract: Videos captured in the wild often suffer from rain streaks, blur, and noise. In addition, even slight changes in camera pose can amplify cross-frame mismatches and temporal artifacts. Existing methods rely on optical flow or heuristic alignment, which are computationally expensive and less robust. To address these challenges, Lie groups provide a principled way to represent continuous geometric transformations, making them well-suited for enforcing spatial and temporal consistency in video modeling. Building on this insight, we propose DeLiVR, an efficient video deraining method that injects spatiotemporal Lie-group differential biases directly into attention scores of the network. Specifically, the method introduces two complementary components. First, a rotation-bounded Lie relative bias predicts the in-plane angle of each frame using a compact prediction module, where normalized coordinates are rotated and compared with base coordinates to achieve geometry-consistent alignment before feature aggregation. Second, a differential group displacement computes angular differences between adjacent frames to estimate a velocity. This bias computation combines temporal decay and attention masks to focus on inter-frame relationships while precisely matching the direction of rain streaks. Extensive experimental results demonstrate the effectiveness of our method on publicly available benchmarks.

</details>


### [13] [UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models](https://arxiv.org/abs/2509.21760)
*Lan Chen,Yuchao Gu,Qi Mao*

Main category: cs.CV

TL;DR: UniVid框架通过微调预训练的视频扩散变换器，将各种视觉任务统一表示为视觉句子，无需任务特定修改即可处理多模态和多源任务。


<details>
  <summary>Details</summary>
Motivation: 受大语言模型统一处理多种任务的启发，希望将类似范式扩展到视觉领域，但现有方法需要跨模态和来源的任务特定预训练，成本高且难以扩展到未见任务。

Method: 提出UniVid框架，将任务表示为视觉句子，使用预训练的视频扩散变换器进行微调，通过上下文序列定义任务和输出模态。

Result: UniVid在跨模态推理（图像和视频混合）和跨源任务（自然数据到标注数据）上都表现出良好的泛化能力，尽管仅使用自然视频数据训练。

Conclusion: 预训练的视频生成模型有潜力作为视觉建模的可扩展统一基础，理解和生成任务可以通过简单反转视觉句子顺序来切换。

Abstract: Large language models, trained on extensive corpora, successfully unify diverse linguistic tasks within a single generative framework. Inspired by this, recent works like Large Vision Model (LVM) extend this paradigm to vision by organizing tasks into sequential visual sentences, where visual prompts serve as the context to guide outputs. However, such modeling requires task-specific pre-training across modalities and sources, which is costly and limits scalability to unseen tasks. Given that pre-trained video generation models inherently capture temporal sequence dependencies, we explore a more unified and scalable alternative: can a pre-trained video generation model adapt to diverse image and video tasks? To answer this, we propose UniVid, a framework that fine-tunes a video diffusion transformer to handle various vision tasks without task-specific modifications. Tasks are represented as visual sentences, where the context sequence defines both the task and the expected output modality. We evaluate the generalization of UniVid from two perspectives: (1) cross-modal inference with contexts composed of both images and videos, extending beyond LVM's uni-modal setting; (2) cross-source tasks from natural to annotated data, without multi-source pre-training. Despite being trained solely on natural video data, UniVid generalizes well in both settings. Notably, understanding and generation tasks can easily switch by simply reversing the visual sentence order in this paradigm. These findings highlight the potential of pre-trained video generation models to serve as a scalable and unified foundation for vision modeling. Our code will be released at https://github.com/CUC-MIPG/UniVid.

</details>


### [14] [DeHate: A Stable Diffusion-based Multimodal Approach to Mitigate Hate Speech in Images](https://arxiv.org/abs/2509.21787)
*Dwip Dalal,Gautam Vashishtha,Anku Ranui,Aishwarya Reganti,Parth Patwa,Mohd Sarique,Chandan Gupta,Keshav Nath,Viswanatha Reddy,Vinija Jain,Aman Chadha,Amitava Das,Amit Sheth,Asif Ekbal*

Main category: cs.CV

TL;DR: 本文提出了一个用于识别数字内容中仇恨的多模态数据集，结合水印稳定扩散技术和数字注意力分析模块，生成仇恨注意力图并模糊仇恨区域。同时介绍了DeHater视觉语言模型和dehate共享任务。


<details>
  <summary>Details</summary>
Motivation: 有害在线内容的增加扭曲了公共话语，对维护健康数字环境构成挑战，需要开发有效的仇恨内容检测和去除方法。

Method: 采用水印稳定增强的稳定扩散技术结合数字注意力分析模块(DAAM)，识别图像中的仇恨元素并生成仇恨注意力图，通过模糊这些区域来去除仇恨内容。

Result: 开发了多模态仇恨检测数据集和DeHater视觉语言模型，为AI驱动的图像仇恨检测设立了新标准。

Conclusion: 该方法有助于开发更符合伦理的AI应用，特别是在社交媒体内容审核方面，通过技术手段促进更健康的数字环境。

Abstract: The rise in harmful online content not only distorts public discourse but also poses significant challenges to maintaining a healthy digital environment. In response to this, we introduce a multimodal dataset uniquely crafted for identifying hate in digital content. Central to our methodology is the innovative application of watermarked, stability-enhanced, stable diffusion techniques combined with the Digital Attention Analysis Module (DAAM). This combination is instrumental in pinpointing the hateful elements within images, thereby generating detailed hate attention maps, which are used to blur these regions from the image, thereby removing the hateful sections of the image. We release this data set as a part of the dehate shared task. This paper also describes the details of the shared task. Furthermore, we present DeHater, a vision-language model designed for multimodal dehatification tasks. Our approach sets a new standard in AI-driven image hate detection given textual prompts, contributing to the development of more ethical AI applications in social media.

</details>


### [15] [LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE](https://arxiv.org/abs/2509.21790)
*Yu Shang,Lei Jin,Yiding Ma,Xin Zhang,Chen Gao,Wei Wu,Yong Li*

Main category: cs.CV

TL;DR: LongScape是一个混合视频生成框架，通过动作引导的分块机制和上下文感知专家混合模型，实现了稳定、一致的长序列视频生成。


<details>
  <summary>Details</summary>
Motivation: 解决当前视频生成方法在长序列生成中的问题：扩散方法存在时间不一致性和视觉漂移，自回归方法则牺牲视觉细节质量。

Method: 结合块内扩散去噪和块间自回归因果生成，采用动作引导的变长分块机制，并引入上下文感知专家混合框架。

Result: 在扩展的rollout中实现了稳定一致的长序列生成，实验结果表明方法有效。

Conclusion: LongScape框架成功解决了长序列视频生成的稳定性问题，代码已开源。

Abstract: Video-based world models hold significant potential for generating high-quality embodied manipulation data. However, current video generation methods struggle to achieve stable long-horizon generation: classical diffusion-based approaches often suffer from temporal inconsistency and visual drift over multiple rollouts, while autoregressive methods tend to compromise on visual detail. To solve this, we introduce LongScape, a hybrid framework that adaptively combines intra-chunk diffusion denoising with inter-chunk autoregressive causal generation. Our core innovation is an action-guided, variable-length chunking mechanism that partitions video based on the semantic context of robotic actions. This ensures each chunk represents a complete, coherent action, enabling the model to flexibly generate diverse dynamics. We further introduce a Context-aware Mixture-of-Experts (CMoE) framework that adaptively activates specialized experts for each chunk during generation, guaranteeing high visual quality and seamless chunk transitions. Extensive experimental results demonstrate that our method achieves stable and consistent long-horizon generation over extended rollouts. Our code is available at: https://github.com/tsinghua-fib-lab/Longscape.

</details>


### [16] [MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation](https://arxiv.org/abs/2509.21797)
*Yu Shang,Yangcheng Yu,Xin Zhang,Xin Jin,Haisheng Su,Wei Wu,Yong Li*

Main category: cs.CV

TL;DR: 提出了MoWM框架，通过融合潜在世界模型和像素空间模型的表示来改进具身动作规划，在CALVIN基准上取得了最先进的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成世界模型存在像素级重建带来的视觉冗余问题，而潜在世界模型虽然紧凑但忽略了精细细节。需要一种能同时利用运动感知表示和精细视觉特征的方法。

Method: 使用混合世界模型框架，将潜在模型提供的运动感知表示作为高级先验，指导从像素空间模型提取精细视觉特征，突出对动作解码有信息量的视觉细节。

Result: 在CALVIN基准测试中实现了最先进的任务成功率和优异的泛化性能。

Conclusion: MoWM框架有效融合了不同特征空间的优势，为具身规划研究提供了有价值的见解。

Abstract: Embodied action planning is a core challenge in robotics, requiring models to generate precise actions from visual observations and language instructions. While video generation world models are promising, their reliance on pixel-level reconstruction often introduces visual redundancies that hinder action decoding and generalization. Latent world models offer a compact, motion-aware representation, but overlook the fine-grained details critical for precise manipulation. To overcome these limitations, we propose MoWM, a mixture-of-world-model framework that fuses representations from hybrid world models for embodied action planning. Our approach uses motion-aware representations from a latent model as a high-level prior, which guides the extraction of fine-grained visual features from the pixel space model. This design allows MoWM to highlight the informative visual details needed for action decoding. Extensive evaluations on the CALVIN benchmark demonstrate that our method achieves state-of-the-art task success rates and superior generalization. We also provide a comprehensive analysis of the strengths of each feature space, offering valuable insights for future research in embodied planning. The code is available at: https://github.com/tsinghua-fib-lab/MoWM.

</details>


### [17] [DiTraj: training-free trajectory control for video diffusion transformer](https://arxiv.org/abs/2509.21839)
*Cheng Lei,Jiayu Zhang,Yue Ma,Xinyu Wang,Long Chen,Liang Tang,Yiqiang Yan,Fei Su,Zhicheng Zhao*

Main category: cs.CV

TL;DR: DiTraj是一个无需训练的视频轨迹控制框架，专为扩散变换器(DiT)设计，通过前景-背景分离引导和空间-时间解耦3D-RoPE实现精确的轨迹控制。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹控制方法要么需要大量训练资源，要么专为U-Net设计，未能充分利用DiT的优越性能。

Method: 使用LLM将用户提示转换为前景和背景提示进行分离引导；提出STD-RoPE通过修改前景token的位置嵌入消除跨帧空间差异；通过调节位置嵌入密度实现3D感知轨迹控制。

Result: 实验表明该方法在视频质量和轨迹可控性方面优于先前方法。

Conclusion: DiTraj为DiT-based视频生成模型提供了一种简单有效的训练free轨迹控制解决方案。

Abstract: Diffusion Transformers (DiT)-based video generation models with 3D full attention exhibit strong generative capabilities. Trajectory control represents a user-friendly task in the field of controllable video generation. However, existing methods either require substantial training resources or are specifically designed for U-Net, do not take advantage of the superior performance of DiT. To address these issues, we propose DiTraj, a simple but effective training-free framework for trajectory control in text-to-video generation, tailored for DiT. Specifically, first, to inject the object's trajectory, we propose foreground-background separation guidance: we use the Large Language Model (LLM) to convert user-provided prompts into foreground and background prompts, which respectively guide the generation of foreground and background regions in the video. Then, we analyze 3D full attention and explore the tight correlation between inter-token attention scores and position embedding. Based on this, we propose inter-frame Spatial-Temporal Decoupled 3D-RoPE (STD-RoPE). By modifying only foreground tokens' position embedding, STD-RoPE eliminates their cross-frame spatial discrepancies, strengthening cross-frame attention among them and thus enhancing trajectory control. Additionally, we achieve 3D-aware trajectory control by regulating the density of position embedding. Extensive experiments demonstrate that our method outperforms previous methods in both video quality and trajectory controllability.

</details>


### [18] [Dynamic Novel View Synthesis in High Dynamic Range](https://arxiv.org/abs/2509.21853)
*Kaixuan Zhang,Zhipeng Xiong,Minxian Li,Mingwu Ren,Jiankang Deng,Xiatian Zhu*

Main category: cs.CV

TL;DR: 提出HDR-4DGS方法解决HDR动态新视角合成问题，通过动态色调映射模块连接HDR和LDR域，实现时空辐射一致性和准确色彩转换。


<details>
  <summary>Details</summary>
Motivation: 现有HDR新视角合成方法主要关注静态场景，但真实世界常包含动态元素如移动物体和变化光照，需要同时建模时间辐射变化和HDR/LDR转换。

Method: 基于高斯泼溅架构，引入动态色调映射模块，根据时间维度上辐射分布的变化动态调整色调映射函数，保持时间辐射一致性。

Result: HDR-4DGS在定量性能和视觉保真度上均超越现有最先进方法，能够从任意视角和时间点生成逼真的HDR渲染。

Conclusion: 该方法成功解决了HDR动态新视角合成的复杂挑战，实现了时空辐射一致性和准确色彩转换，为动态场景的HDR建模提供了有效解决方案。

Abstract: High Dynamic Range Novel View Synthesis (HDR NVS) seeks to learn an HDR 3D model from Low Dynamic Range (LDR) training images captured under conventional imaging conditions. Current methods primarily focus on static scenes, implicitly assuming all scene elements remain stationary and non-living. However, real-world scenarios frequently feature dynamic elements, such as moving objects, varying lighting conditions, and other temporal events, thereby presenting a significantly more challenging scenario. To address this gap, we propose a more realistic problem named HDR Dynamic Novel View Synthesis (HDR DNVS), where the additional dimension ``Dynamic'' emphasizes the necessity of jointly modeling temporal radiance variations alongside sophisticated 3D translation between LDR and HDR. To tackle this complex, intertwined challenge, we introduce HDR-4DGS, a Gaussian Splatting-based architecture featured with an innovative dynamic tone-mapping module that explicitly connects HDR and LDR domains, maintaining temporal radiance coherence by dynamically adapting tone-mapping functions according to the evolving radiance distributions across the temporal dimension. As a result, HDR-4DGS achieves both temporal radiance consistency and spatially accurate color translation, enabling photorealistic HDR renderings from arbitrary viewpoints and time instances. Extensive experiments demonstrate that HDR-4DGS surpasses existing state-of-the-art methods in both quantitative performance and visual fidelity. Source code will be released.

</details>


### [19] [SRHand: Super-Resolving Hand Images and 3D Shapes via View/Pose-aware Neural Image Representations and Explicit 3D Meshes](https://arxiv.org/abs/2509.21859)
*Minje Kim,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: SRHand是一种从低分辨率图像重建详细3D手部几何和纹理的方法，通过结合隐式图像表示和显式手部网格，实现多视角一致的高质量手部重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖高分辨率多视角图像输入，难以在低分辨率图像上泛化，且多视角超分辨率方法不适用于可变形的手部结构。

Method: 提出几何感知隐式图像函数(GIIF)，通过联合优化隐式图像函数和显式3D手部形状，在提升图像分辨率的同时保持多视角和姿态一致性。

Result: 在InterHand2.6M和Goliath数据集上，SRHand在定量和定性评估中显著优于现有图像超分辨率和3D手部重建方法。

Conclusion: SRHand能够从低分辨率输入重建包含皱纹、指甲等精细细节的3D手部几何和纹理图像。

Abstract: Reconstructing detailed hand avatars plays a crucial role in various applications. While prior works have focused on capturing high-fidelity hand geometry, they heavily rely on high-resolution multi-view image inputs and struggle to generalize on low-resolution images. Multi-view image super-resolution methods have been proposed to enforce 3D view consistency. These methods, however, are limited to static objects/scenes with fixed resolutions and are not applicable to articulated deformable hands. In this paper, we propose SRHand (Super-Resolution Hand), the method for reconstructing detailed 3D geometry as well as textured images of hands from low-resolution images. SRHand leverages the advantages of implicit image representation with explicit hand meshes. Specifically, we introduce a geometric-aware implicit image function (GIIF) that learns detailed hand prior by upsampling the coarse input images. By jointly optimizing the implicit image function and explicit 3D hand shapes, our method preserves multi-view and pose consistency among upsampled hand images, and achieves fine-detailed 3D reconstruction (wrinkles, nails). In experiments using the InterHand2.6M and Goliath datasets, our method significantly outperforms state-of-the-art image upsampling methods adapted to hand datasets, and 3D hand reconstruction methods, quantitatively and qualitatively. Project page: https://yunminjin2.github.io/projects/srhand

</details>


### [20] [Deepfakes: we need to re-think the concept of "real" images](https://arxiv.org/abs/2509.21864)
*Janis Keuper,Margret Keuper*

Main category: cs.CV

TL;DR: 该论文认为当前假图像检测研究过于关注生成算法和"假"样本，而忽视了"真实"图像的明确定义和数据收集。作者指出现代智能手机拍摄的照片也使用神经网络算法处理，与假图像生成器类似，因此需要重新思考"真实"图像的概念。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成模型的普及，检测假图像的需求日益增长。但当前研究主要关注生成算法，而忽视了"真实"图像的明确定义和数据收集问题。现代智能手机拍摄的照片也使用神经网络处理，这与假图像生成器相似，因此需要重新审视"真实"图像的概念。

Method: 这是一篇立场论文，通过分析当前假图像检测领域的现状，指出研究方法的局限性。作者强调需要重新定义"真实"图像的概念，并创建新的基准数据集。

Result: 分析发现当前假图像检测方法的开发和评估主要依赖少数老旧的低分辨率数据集（如ImageNet），而现代图像采集技术已发生巨大变化。超过90%的照片由智能手机拍摄，这些设备使用神经网络算法处理图像，与假图像生成器密切相关。

Conclusion: 需要重新思考"真实"图像的概念，制定清晰的技术定义，并创建新的基准数据集。作者质疑检测假图像是否是一个合理的目标，呼吁在该研究领域进行开放讨论。

Abstract: The wide availability and low usability barrier of modern image generation models has triggered the reasonable fear of criminal misconduct and negative social implications. The machine learning community has been engaging this problem with an extensive series of publications proposing algorithmic solutions for the detection of "fake", e.g. entirely generated or partially manipulated images. While there is undoubtedly some progress towards technical solutions of the problem, we argue that current and prior work is focusing too much on generative algorithms and "fake" data-samples, neglecting a clear definition and data collection of "real" images. The fundamental question "what is a real image?" might appear to be quite philosophical, but our analysis shows that the development and evaluation of basically all current "fake"-detection methods is relying on only a few, quite old low-resolution datasets of "real" images like ImageNet. However, the technology for the acquisition of "real" images, aka taking photos, has drastically evolved over the last decade: Today, over 90% of all photographs are produced by smartphones which typically use algorithms to compute an image from multiple inputs (over time) from multiple sensors. Based on the fact that these image formation algorithms are typically neural network architectures which are closely related to "fake"-image generators, we state the position that today, we need to re-think the concept of "real" images. The purpose of this position paper is to raise the awareness of the current shortcomings in this active field of research and to trigger an open discussion whether the detection of "fake" images is a sound objective at all. At the very least, we need a clear technical definition of "real" images and new benchmark datasets.

</details>


### [21] [StableDub: Taming Diffusion Prior for Generalized and Efficient Visual Dubbing](https://arxiv.org/abs/2509.21887)
*Liyang Chen,Tianze Zhou,Xu He,Boshi Tang,Zhiyong Wu,Yang Huang,Yang Wu,Zhongqian Sun,Wei Yang,Helen Meng*

Main category: cs.CV

TL;DR: StableDub是一个视觉配音框架，通过唇部习惯感知建模和遮挡鲁棒合成，解决了现有方法在唇部习惯相似性和遮挡处理方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视觉配音方法存在两个关键缺陷：(1)仅基于音频的驱动范式无法捕捉说话者特定的唇部习惯；(2)传统盲修复方法在处理遮挡时经常产生视觉伪影。

Method: 基于Stable-Diffusion骨干网络，开发了唇部习惯调制机制，联合建模音视频同步和说话者特定的口面部动态；引入遮挡感知训练策略，显式地将遮挡对象暴露给修复过程；采用混合Mamba-Transformer架构优化训练效率。

Result: 实验结果表明StableDub在唇部习惯相似性和遮挡鲁棒性方面表现优异，在音频-唇部同步、视频质量和分辨率一致性方面超越其他方法。

Conclusion: StableDub消除了对昂贵先验的需求，在计算密集的扩散模型上展现出优越的训练效率，扩展了视觉配音方法的适用性。

Abstract: The visual dubbing task aims to generate mouth movements synchronized with the driving audio, which has seen significant progress in recent years. However, two critical deficiencies hinder their wide application: (1) Audio-only driving paradigms inadequately capture speaker-specific lip habits, which fail to generate lip movements similar to the target avatar; (2) Conventional blind-inpainting approaches frequently produce visual artifacts when handling obstructions (e.g., microphones, hands), limiting practical deployment. In this paper, we propose StableDub, a novel and concise framework integrating lip-habit-aware modeling with occlusion-robust synthesis. Specifically, building upon the Stable-Diffusion backbone, we develop a lip-habit-modulated mechanism that jointly models phonemic audio-visual synchronization and speaker-specific orofacial dynamics. To achieve plausible lip geometries and object appearances under occlusion, we introduce the occlusion-aware training strategy by explicitly exposing the occlusion objects to the inpainting process. By incorporating the proposed designs, the model eliminates the necessity for cost-intensive priors in previous methods, thereby exhibiting superior training efficiency on the computationally intensive diffusion-based backbone. To further optimize training efficiency from the perspective of model architecture, we introduce a hybrid Mamba-Transformer architecture, which demonstrates the enhanced applicability in low-resource research scenarios. Extensive experimental results demonstrate that StableDub achieves superior performance in lip habit resemblance and occlusion robustness. Our method also surpasses other methods in audio-lip sync, video quality, and resolution consistency. We expand the applicability of visual dubbing methods from comprehensive aspects, and demo videos can be found at https://stabledub.github.io.

</details>


### [22] [Drag4D: Align Your Motion with Text-Driven 3D Scene Generation](https://arxiv.org/abs/2509.21888)
*Minjun Kang,Inkyu Shin,Taeyeop Lee,In So Kweon,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: Drag4D是一个交互式框架，通过三阶段流程实现文本驱动的3D场景生成与物体运动控制，支持用户定义3D轨迹来动画化物体。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在文本驱动的3D场景生成中实现精确的物体运动控制，需要一种能够将物体无缝集成到高质量3D背景中并支持交互式轨迹定义的系统。

Method: 三阶段流程：1) 使用2D高斯泼溅和全景图像增强文本到3D背景生成；2) 通过3D复制粘贴将目标物体网格集成到场景中，使用物理感知位置学习进行空间对齐；3) 使用部件增强的运动条件视频扩散模型沿用户定义的3D轨迹进行时间动画。

Result: 实现了在高质量3D背景中用户控制物体运动的协调对齐，展示了每个阶段和最终结果的有效性。

Conclusion: Drag4D提供了一个统一的架构，成功实现了文本驱动3D场景生成与交互式物体运动控制的集成，解决了运动幻觉和视图一致性问题。

Abstract: We introduce Drag4D, an interactive framework that integrates object motion control within text-driven 3D scene generation. This framework enables users to define 3D trajectories for the 3D objects generated from a single image, seamlessly integrating them into a high-quality 3D background. Our Drag4D pipeline consists of three stages. First, we enhance text-to-3D background generation by applying 2D Gaussian Splatting with panoramic images and inpainted novel views, resulting in dense and visually complete 3D reconstructions. In the second stage, given a reference image of the target object, we introduce a 3D copy-and-paste approach: the target instance is extracted in a full 3D mesh using an off-the-shelf image-to-3D model and seamlessly composited into the generated 3D scene. The object mesh is then positioned within the 3D scene via our physics-aware object position learning, ensuring precise spatial alignment. Lastly, the spatially aligned object is temporally animated along a user-defined 3D trajectory. To mitigate motion hallucination and ensure view-consistent temporal alignment, we develop a part-augmented, motion-conditioned video diffusion model that processes multiview image pairs together with their projected 2D trajectories. We demonstrate the effectiveness of our unified architecture through evaluations at each stage and in the final results, showcasing the harmonized alignment of user-controlled object motion within a high-quality 3D background.

</details>


### [23] [Syncphony: Synchronized Audio-to-Video Generation with Diffusion Transformers](https://arxiv.org/abs/2509.21893)
*Jibin Song,Mingi Kwon,Jaeseok Jeong,Youngjung Uh*

Main category: cs.CV

TL;DR: Syncphony是一个音频到视频生成模型，通过运动感知损失和音频同步引导技术，能够生成与音频精确同步的高质量视频。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到视频和图像到视频生成方法在控制运动时序方面存在局限，而音频提供了与视频运动对齐的时间线索，是时间控制视频生成的有前景条件。

Method: 基于预训练视频骨干网络，引入两个关键组件：运动感知损失（强调高运动区域学习）和音频同步引导（使用视觉对齐的无音频模型引导全模型）。

Result: 在AVSync15和The Greatest Hits数据集上的实验表明，Syncphony在同步精度和视觉质量方面均优于现有方法。

Conclusion: Syncphony通过创新的同步机制成功实现了音频与视频的高质量同步生成，为时序控制视频生成提供了有效解决方案。

Abstract: Text-to-video and image-to-video generation have made rapid progress in visual quality, but they remain limited in controlling the precise timing of motion. In contrast, audio provides temporal cues aligned with video motion, making it a promising condition for temporally controlled video generation. However, existing audio-to-video (A2V) models struggle with fine-grained synchronization due to indirect conditioning mechanisms or limited temporal modeling capacity. We present Syncphony, which generates 380x640 resolution, 24fps videos synchronized with diverse audio inputs. Our approach builds upon a pre-trained video backbone and incorporates two key components to improve synchronization: (1) Motion-aware Loss, which emphasizes learning at high-motion regions; (2) Audio Sync Guidance, which guides the full model using a visually aligned off-sync model without audio layers to better exploit audio cues at inference while maintaining visual quality. To evaluate synchronization, we propose CycleSync, a video-to-audio-based metric that measures the amount of motion cues in the generated video to reconstruct the original audio. Experiments on AVSync15 and The Greatest Hits datasets demonstrate that Syncphony outperforms existing methods in both synchronization accuracy and visual quality. Project page is available at: https://jibin86.github.io/syncphony_project_page

</details>


### [24] [TDEdit: A Unified Diffusion Framework for Text-Drag Guided Image Manipulation](https://arxiv.org/abs/2509.21905)
*Qihang Wang,Yaxiong Wang,Lechao Cheng,Zhun Zhong*

Main category: cs.CV

TL;DR: 提出了一种联合文本和拖拽控制的图像编辑框架，结合了文本驱动的纹理编辑和拖拽驱动的空间控制优势，实现了高保真的联合编辑。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动方法缺乏精确空间控制，而拖拽驱动方法缺少细粒度纹理指导，需要结合两者优势来解决互补性限制。

Method: 基于扩散模型的统一框架，包含点云确定性拖拽（通过3D特征映射增强潜在空间布局控制）和拖拽文本引导去噪（动态平衡拖拽和文本条件的影响）。

Result: 实验表明该方法不仅实现了高保真联合编辑，而且在纯文本或纯拖拽模式下也能达到或超越专门方法的性能。

Conclusion: 该方法提供了一个多功能且可推广的可控图像操作解决方案，支持灵活编辑模式（纯文本、纯拖拽或联合条件）。

Abstract: This paper explores image editing under the joint control of text and drag interactions. While recent advances in text-driven and drag-driven editing have achieved remarkable progress, they suffer from complementary limitations: text-driven methods excel in texture manipulation but lack precise spatial control, whereas drag-driven approaches primarily modify shape and structure without fine-grained texture guidance. To address these limitations, we propose a unified diffusion-based framework for joint drag-text image editing, integrating the strengths of both paradigms. Our framework introduces two key innovations: (1) Point-Cloud Deterministic Drag, which enhances latent-space layout control through 3D feature mapping, and (2) Drag-Text Guided Denoising, dynamically balancing the influence of drag and text conditions during denoising. Notably, our model supports flexible editing modes - operating with text-only, drag-only, or combined conditions - while maintaining strong performance in each setting. Extensive quantitative and qualitative experiments demonstrate that our method not only achieves high-fidelity joint editing but also matches or surpasses the performance of specialized text-only or drag-only approaches, establishing a versatile and generalizable solution for controllable image manipulation. Code will be made publicly available to reproduce all results presented in this work.

</details>


### [25] [Taming Flow-based I2V Models for Creative Video Editing](https://arxiv.org/abs/2509.21917)
*Xianghao Kong,Hansheng Chen,Yuwei Guo,Lvmin Zhang,Gordon Wetzstein,Maneesh Agrawala,Anyi Rao*

Main category: cs.CV

TL;DR: 提出IF-V2V方法，无需反转即可将现成的基于流匹配的图像到视频模型适配用于视频编辑，通过向量场校正和结构运动保持初始化确保编辑质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法要么需要特定模型的反转，要么需要大量优化，限制了利用最新图像到视频模型将图像编辑能力迁移到视频领域的能力。

Method: 采用向量场校正与样本偏差来绕过反转，引入偏差项到去噪向量场；使用结构运动保持初始化生成运动感知的时间相关噪声；通过偏差缓存机制最小化计算开销。

Result: 评估显示该方法在编辑质量和一致性方面优于现有方法，提供了轻量级的即插即用解决方案。

Conclusion: IF-V2V方法成功实现了无需反转的高质量视频编辑，能够有效利用现有图像到视频模型进行视频编辑任务。

Abstract: Although image editing techniques have advanced significantly, video editing, which aims to manipulate videos according to user intent, remains an emerging challenge. Most existing image-conditioned video editing methods either require inversion with model-specific design or need extensive optimization, limiting their capability of leveraging up-to-date image-to-video (I2V) models to transfer the editing capability of image editing models to the video domain. To this end, we propose IF-V2V, an Inversion-Free method that can adapt off-the-shelf flow-matching-based I2V models for video editing without significant computational overhead. To circumvent inversion, we devise Vector Field Rectification with Sample Deviation to incorporate information from the source video into the denoising process by introducing a deviation term into the denoising vector field. To further ensure consistency with the source video in a model-agnostic way, we introduce Structure-and-Motion-Preserving Initialization to generate motion-aware temporally correlated noise with structural information embedded. We also present a Deviation Caching mechanism to minimize the additional computational cost for denoising vector rectification without significantly impacting editing quality. Evaluations demonstrate that our method achieves superior editing quality and consistency over existing approaches, offering a lightweight plug-and-play solution to realize visual creativity.

</details>


### [26] [SemanticControl: A Training-Free Approach for Handling Loosely Aligned Visual Conditions in ControlNet](https://arxiv.org/abs/2509.21938)
*Woosung Joung,Daewon Chae,Jinkyu Kim*

Main category: cs.CV

TL;DR: 提出SemanticControl方法，通过使用语义相关但未对齐的视觉条件来增强文本到图像生成的控制能力，在视觉条件与文本提示不完全匹配时仍能保持高文本保真度。


<details>
  <summary>Details</summary>
Motivation: 现有ControlNet模型严重依赖与文本提示精确对齐的视觉条件，但在实践中这种对齐条件往往不可得，特别是在不常见或想象场景中。当使用语义相关但未精确对齐的视觉条件时，现有方法会产生低文本保真度或视觉伪影。

Method: 提出训练无关的SemanticControl方法：首先使用与视觉条件对齐的代理提示进行辅助去噪过程，提取信息性注意力掩码；然后在实际目标提示的去噪过程中利用这些掩码，自适应地抑制视觉条件与提示冲突区域的影响，同时加强文本引导。

Result: 实验结果表明，该方法在各种视觉条件（深度图、边缘图、人体骨架）下，在松散对齐条件下均能提升性能，优于现有基线方法。

Conclusion: SemanticControl能够有效利用语义相关但未精确对齐的视觉条件，解决了现有ControlNet模型在实际应用中的局限性，为文本到图像生成提供了更灵活的空间控制能力。

Abstract: ControlNet has enabled detailed spatial control in text-to-image diffusion models by incorporating additional visual conditions such as depth or edge maps. However, its effectiveness heavily depends on the availability of visual conditions that are precisely aligned with the generation goal specified by text prompt-a requirement that often fails in practice, especially for uncommon or imaginative scenes. For example, generating an image of a cat cooking in a specific pose may be infeasible due to the lack of suitable visual conditions. In contrast, structurally similar cues can often be found in more common settings-for instance, poses of humans cooking are widely available and can serve as rough visual guides. Unfortunately, existing ControlNet models struggle to use such loosely aligned visual conditions, often resulting in low text fidelity or visual artifacts. To address this limitation, we propose SemanticControl, a training-free method for effectively leveraging misaligned but semantically relevant visual conditions. Our approach adaptively suppresses the influence of the visual condition where it conflicts with the prompt, while strengthening guidance from the text. The key idea is to first run an auxiliary denoising process using a surrogate prompt aligned with the visual condition (e.g., "a human playing guitar" for a human pose condition) to extract informative attention masks, and then utilize these masks during the denoising of the actual target prompt (e.g., cat playing guitar). Experimental results demonstrate that our method improves performance under loosely aligned conditions across various conditions, including depth maps, edge maps, and human skeletons, outperforming existing baselines. Our code is available at https://mung3477.github.io/semantic-control.

</details>


### [27] [MultiCrafter: High-Fidelity Multi-Subject Generation via Spatially Disentangled Attention and Identity-Aware Reinforcement Learning](https://arxiv.org/abs/2509.21953)
*Tao Wu,Yibo Jiang,Yehao Lu,Zhizhong Wang,Zeyi Huang,Zequn Qin,Xi Li*

Main category: cs.CV

TL;DR: MultiCrafter是一个多主体图像生成框架，通过位置监督解决属性泄漏问题，使用专家混合架构增强模型能力，并采用在线强化学习来对齐人类偏好。


<details>
  <summary>Details</summary>
Motivation: 现有基于上下文学习的方法存在严重属性泄漏问题，损害主体保真度，且无法对齐人类偏好。

Method: 引入显式位置监督分离不同主体的注意力区域；使用专家混合架构增强模型能力；设计在线强化学习框架对齐人类偏好。

Result: 实验验证该框架显著提高了主体保真度，并更好地对齐了人类偏好。

Conclusion: MultiCrafter通过位置监督、专家混合架构和强化学习有效解决了多主体图像生成中的属性泄漏和偏好对齐问题。

Abstract: Multi-subject image generation aims to synthesize user-provided subjects in a single image while preserving subject fidelity, ensuring prompt consistency, and aligning with human aesthetic preferences. However, existing methods, particularly those built on the In-Context-Learning paradigm, are limited by their reliance on simple reconstruction-based objectives, leading to both severe attribute leakage that compromises subject fidelity and failing to align with nuanced human preferences. To address this, we propose MultiCrafter, a framework that ensures high-fidelity, preference-aligned generation. First, we find that the root cause of attribute leakage is a significant entanglement of attention between different subjects during the generation process. Therefore, we introduce explicit positional supervision to explicitly separate attention regions for each subject, effectively mitigating attribute leakage. To enable the model to accurately plan the attention region of different subjects in diverse scenarios, we employ a Mixture-of-Experts architecture to enhance the model's capacity, allowing different experts to focus on different scenarios. Finally, we design a novel online reinforcement learning framework to align the model with human preferences, featuring a scoring mechanism to accurately assess multi-subject fidelity and a more stable training strategy tailored for the MoE architecture. Experiments validate that our framework significantly improves subject fidelity while aligning with human preferences better.

</details>


### [28] [Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation](https://arxiv.org/abs/2509.21989)
*Abdelrahman Eldesokey,Aleksandar Cvejic,Bernard Ghanem,Peter Wonka*

Main category: cs.CV

TL;DR: 提出了一种从预训练扩散模型骨干网络中解耦视觉和语义特征的新方法，能够实现视觉对应关系，类似于语义对应关系。


<details>
  <summary>Details</summary>
Motivation: 扩散模型骨干网络已知编码了丰富的语义特征，但也必须包含视觉特征以支持图像合成能力。然而，由于缺乏标注数据集，分离这些视觉特征具有挑战性。

Method: 引入自动化流水线构建具有标注语义和视觉对应关系的图像对，基于现有主题驱动图像生成数据集，并设计对比架构来分离两种特征类型。

Result: 利用解耦表示提出新指标VSM，在量化主题驱动图像生成中的视觉不一致性方面优于CLIP、DINO和视觉语言模型等全局特征指标，同时能够空间定位不一致区域。

Conclusion: 这是首个支持主题驱动生成中不一致性量化和定位的方法，为推进该任务提供了有价值的工具。

Abstract: We propose a novel approach for disentangling visual and semantic features from the backbones of pre-trained diffusion models, enabling visual correspondence in a manner analogous to the well-established semantic correspondence. While diffusion model backbones are known to encode semantically rich features, they must also contain visual features to support their image synthesis capabilities. However, isolating these visual features is challenging due to the absence of annotated datasets. To address this, we introduce an automated pipeline that constructs image pairs with annotated semantic and visual correspondences based on existing subject-driven image generation datasets, and design a contrastive architecture to separate the two feature types. Leveraging the disentangled representations, we propose a new metric, Visual Semantic Matching (VSM), that quantifies visual inconsistencies in subject-driven image generation. Empirical results show that our approach outperforms global feature-based metrics such as CLIP, DINO, and vision--language models in quantifying visual inconsistencies while also enabling spatial localization of inconsistent regions. To our knowledge, this is the first method that supports both quantification and localization of inconsistencies in subject-driven generation, offering a valuable tool for advancing this task. Project Page:https://abdo-eldesokey.github.io/mind-the-glitch/

</details>


### [29] [FailureAtlas:Mapping the Failure Landscape of T2I Models via Active Exploration](https://arxiv.org/abs/2509.21995)
*Muxi Chen,Zhaohua Zhang,Chenchen Zhao,Mingyang Chen,Wenyu Jiang,Tianwen Jiang,Jianhuan Zhuo,Yu Tang,Qiuyong Xiao,Jihong Zhang,Qiang Xu*

Main category: cs.CV

TL;DR: FailureAtlas是一个主动探索文本到图像模型失败模式的框架，通过结构化搜索发现最小化失败概念，揭示了大量未知错误切片并建立了与训练数据稀缺的联系。


<details>
  <summary>Details</summary>
Motivation: 静态基准测试在比较文本到图像模型时诊断能力有限，难以发现系统性失败的全貌或隔离根本原因，需要主动探索的补充方法。

Method: 将错误发现构建为对最小化失败诱导概念的结构化搜索，采用新颖的加速技术处理计算爆炸问题。

Result: 在Stable Diffusion模型上发现了超过247,000个未知错误切片，首次大规模证据表明这些失败与训练数据稀缺相关。

Conclusion: FailureAtlas建立了诊断优先的新方法学，为开发更稳健的生成式AI提供了原则性和可扩展的深度模型审计引擎。

Abstract: Static benchmarks have provided a valuable foundation for comparing Text-to-Image (T2I) models. However, their passive design offers limited diagnostic power, struggling to uncover the full landscape of systematic failures or isolate their root causes. We argue for a complementary paradigm: active exploration. We introduce FailureAtlas, the first framework designed to autonomously explore and map the vast failure landscape of T2I models at scale. FailureAtlas frames error discovery as a structured search for minimal, failure-inducing concepts. While it is a computationally explosive problem, we make it tractable with novel acceleration techniques. When applied to Stable Diffusion models, our method uncovers hundreds of thousands of previously unknown error slices (over 247,000 in SD1.5 alone) and provides the first large-scale evidence linking these failures to data scarcity in the training set. By providing a principled and scalable engine for deep model auditing, FailureAtlas establishes a new, diagnostic-first methodology to guide the development of more robust generative AI. The code is available at https://github.com/cure-lab/FailureAtlas

</details>


### [30] [High-Quality Sound Separation Across Diverse Categories via Visually-Guided Generative Modeling](https://arxiv.org/abs/2509.22063)
*Chao Huang,Susan Liang,Yapeng Tian,Anurag Kumar,Chenliang Xu*

Main category: cs.CV

TL;DR: DAVIS是一个基于扩散模型的音频-视觉分离框架，通过生成学习方法解决音频-视觉声源分离任务，超越了传统掩码回归方法，在多个数据集上表现出优越的分离质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法将声音分离视为基于掩码的回归问题，在捕捉复杂数据分布以高质量分离多样化声音类别方面存在局限性。

Method: DAVIS利用去噪扩散概率模型和流匹配等生成建模范式，通过专门的分离U-Net架构直接从噪声分布合成分离的声音频谱图，同时以混合音频输入和相关视觉信息为条件。

Result: 在AVE和MUSIC数据集上的比较评估表明，DAVIS的两个变体（DDPM和Flow Matching）在分离质量上都超越了现有方法。

Conclusion: DAVIS生成框架在解决音频-视觉声源分离任务方面表现出高效性，特别擅长为多样化声音类别生成高质量的分离结果。

Abstract: We propose DAVIS, a Diffusion-based Audio-VIsual Separation framework that solves the audio-visual sound source separation task through generative learning. Existing methods typically frame sound separation as a mask-based regression problem, achieving significant progress. However, they face limitations in capturing the complex data distribution required for high-quality separation of sounds from diverse categories. In contrast, DAVIS circumvents these issues by leveraging potent generative modeling paradigms, specifically Denoising Diffusion Probabilistic Models (DDPM) and the more recent Flow Matching (FM), integrated within a specialized Separation U-Net architecture. Our framework operates by synthesizing the desired separated sound spectrograms directly from a noise distribution, conditioned concurrently on the mixed audio input and associated visual information. The inherent nature of its generative objective makes DAVIS particularly adept at producing high-quality sound separations for diverse sound categories. We present comparative evaluations of DAVIS, encompassing both its DDPM and Flow Matching variants, against leading methods on the standard AVE and MUSIC datasets. The results affirm that both variants surpass existing approaches in separation quality, highlighting the efficacy of our generative framework for tackling the audio-visual source separation task.

</details>


### [31] [Large Material Gaussian Model for Relightable 3D Generation](https://arxiv.org/abs/2509.22112)
*Jingrui Ye,Lingting Zhu,Runze Zhang,Zeyu Hu,Yingda Yin,Lanjiong Li,Lequan Yu,Qingmin Liao*

Main category: cs.CV

TL;DR: 提出了大型材质高斯模型(MGM)，能够生成具有PBR材质属性的3D内容，支持动态重光照，相比现有方法在视觉质量和材质建模方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有3D重建模型无法生成材质属性，而材质属性对于在不同光照环境下实现真实渲染至关重要。

Method: 首先微调基于深度和法线图输入的多视角材质扩散模型，然后利用生成的多视角PBR图像探索高斯材质表示，该表示与2D高斯泼溅对齐并建模PBR材质的各个通道。

Result: 实验表明，该方法生成的材质在视觉上比基线方法更具吸引力，并增强了材质建模能力，实现了实用的下游渲染应用。

Conclusion: MGM框架成功生成了具有PBR材质属性的高质量3D内容，支持动态重光照，为3D内容创建提供了更实用的解决方案。

Abstract: The increasing demand for 3D assets across various industries necessitates efficient and automated methods for 3D content creation. Leveraging 3D Gaussian Splatting, recent large reconstruction models (LRMs) have demonstrated the ability to efficiently achieve high-quality 3D rendering by integrating multiview diffusion for generation and scalable transformers for reconstruction. However, existing models fail to produce the material properties of assets, which is crucial for realistic rendering in diverse lighting environments. In this paper, we introduce the Large Material Gaussian Model (MGM), a novel framework designed to generate high-quality 3D content with Physically Based Rendering (PBR) materials, ie, albedo, roughness, and metallic properties, rather than merely producing RGB textures with uncontrolled light baking. Specifically, we first fine-tune a new multiview material diffusion model conditioned on input depth and normal maps. Utilizing the generated multiview PBR images, we explore a Gaussian material representation that not only aligns with 2D Gaussian Splatting but also models each channel of the PBR materials. The reconstructed point clouds can then be rendered to acquire PBR attributes, enabling dynamic relighting by applying various ambient light maps. Extensive experiments demonstrate that the materials produced by our method not only exhibit greater visual appeal compared to baseline methods but also enhance material modeling, thereby enabling practical downstream rendering applications.

</details>


### [32] [REFINE-CONTROL: A Semi-supervised Distillation Method For Conditional Image Generation](https://arxiv.org/abs/2509.22139)
*Yicheng Jiang,Jin Yuan,Hua Yuan,Yao Zhang,Yong Rui*

Main category: cs.CV

TL;DR: Refine-Control是一个半监督蒸馏框架，通过三层次知识融合损失和利用标记/未标记数据，在降低计算成本的同时保持高质量的条件图像生成能力。


<details>
  <summary>Details</summary>
Motivation: 解决条件图像生成模型在边缘设备部署时的高资源需求和标注数据稀缺问题，避免将用户数据发送给第三方带来的成本和隐私担忧。

Method: 提出半监督蒸馏框架，引入三层次知识融合损失来传递不同层次的知识，同时利用标记和未标记数据增强泛化能力。

Result: 实验表明Refine-Control显著降低了计算成本和延迟，同时保持了高保真度的生成能力和可控性。

Conclusion: 该框架有效解决了条件图像生成模型在资源受限环境下的部署挑战，平衡了性能与效率。

Abstract: Conditional image generation models have achieved remarkable results by leveraging text-based control to generate customized images. However, the high resource demands of these models and the scarcity of well-annotated data have hindered their deployment on edge devices, leading to enormous costs and privacy concerns, especially when user data is sent to a third party. To overcome these challenges, we propose Refine-Control, a semi-supervised distillation framework. Specifically, we improve the performance of the student model by introducing a tri-level knowledge fusion loss to transfer different levels of knowledge. To enhance generalization and alleviate dataset scarcity, we introduce a semi-supervised distillation method utilizing both labeled and unlabeled data. Our experiments reveal that Refine-Control achieves significant reductions in computational cost and latency, while maintaining high-fidelity generation capabilities and controllability, as quantified by comparative metrics.

</details>


### [33] [DragGANSpace: Latent Space Exploration and Control for GANs](https://arxiv.org/abs/2509.22169)
*Kirsten Odendaal,Neela Kaushik,Spencer Halverson*

Main category: cs.CV

TL;DR: 该研究将StyleGAN、DragGAN和PCA相结合，提升GAN生成图像的潜在空间效率和可控性，在AFHQ数据集上验证了方法在保持性能的同时提高优化效率。


<details>
  <summary>Details</summary>
Motivation: 旨在增强GAN生成图像的潜在空间效率和可控性，通过整合多种技术实现更高效、可解释的图像合成与编辑。

Method: 结合StyleGAN的结构化潜在空间、DragGAN的直观图像操作和PCA的降维能力，在AFHQ数据集上进行实验，特别在W+层引入PCA降维。

Result: PCA集成显著减少了总优化时间，同时保持良好的视觉质量，在较浅潜在空间（W+层=3）中甚至提升了SSIM指标，并实现了跨模型图像对齐和可控操作。

Conclusion: 该方法为广泛的图像合成和编辑应用提供了高效且可解释的潜在空间控制可能性。

Abstract: This work integrates StyleGAN, DragGAN and Principal Component Analysis (PCA) to enhance the latent space efficiency and controllability of GAN-generated images. Style-GAN provides a structured latent space, DragGAN enables intuitive image manipulation, and PCA reduces dimensionality and facilitates cross-model alignment for more streamlined and interpretable exploration of latent spaces. We apply our techniques to the Animal Faces High Quality (AFHQ) dataset, and find that our approach of integrating PCA-based dimensionality reduction with the Drag-GAN framework for image manipulation retains performance while improving optimization efficiency. Notably, introducing PCA into the latent W+ layers of DragGAN can consistently reduce the total optimization time while maintaining good visual quality and even boosting the Structural Similarity Index Measure (SSIM) of the optimized image, particularly in shallower latent spaces (W+ layers = 3). We also demonstrate capability for aligning images generated by two StyleGAN models trained on similar but distinct data domains (AFHQ-Dog and AFHQ-Cat), and show that we can control the latent space of these aligned images to manipulate the images in an intuitive and interpretable manner. Our findings highlight the possibility for efficient and interpretable latent space control for a wide range of image synthesis and editing applications.

</details>


### [34] [Polysemous Language Gaussian Splatting via Matching-based Mask Lifting](https://arxiv.org/abs/2509.22225)
*Jiayu Ding,Xinpeng Liu,Zhiyi Pan,Shiqiang Long,Ge Li*

Main category: cs.CV

TL;DR: MUSplat是一个无需训练的框架，将2D开放词汇理解提升到3D高斯泼溅场景中，解决了现有方法需要逐场景训练、单语义限制和跨视图语义不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有主流方法存在三个关键缺陷：(i)依赖昂贵的逐场景重新训练，无法即插即用；(ii)限制性的单语义设计无法表示复杂的多概念语义；(iii)易受跨视图语义不一致影响，破坏最终语义表示。

Method: 利用预训练的2D分割模型生成并提升多粒度2D掩码到3D，为每个高斯点估计前景概率形成初始对象组。使用语义熵和几何不透明度优化初始组的模糊边界，然后通过视觉语言模型在最具代表性的视图中解释对象外观，提炼鲁棒的文本特征以协调视觉不一致，实现开放词汇查询。

Result: 通过消除昂贵的逐场景训练过程，MUSplat将场景适应时间从数小时减少到几分钟。在开放词汇3D对象选择和语义分割的基准任务中，MUSplat优于已建立的基于训练的框架，同时解决了它们的单语义限制。

Conclusion: MUSplat成功克服了现有方法的局限性，提供了一个无需训练、支持多语义表示且能处理跨视图语义不一致的高效解决方案，在3D开放词汇理解任务中表现出色。

Abstract: Lifting 2D open-vocabulary understanding into 3D Gaussian Splatting (3DGS) scenes is a critical challenge. However, mainstream methods suffer from three key flaws: (i) their reliance on costly per-scene retraining prevents plug-and-play application; (ii) their restrictive monosemous design fails to represent complex, multi-concept semantics; and (iii) their vulnerability to cross-view semantic inconsistencies corrupts the final semantic representation. To overcome these limitations, we introduce MUSplat, a training-free framework that abandons feature optimization entirely. Leveraging a pre-trained 2D segmentation model, our pipeline generates and lifts multi-granularity 2D masks into 3D, where we estimate a foreground probability for each Gaussian point to form initial object groups. We then optimize the ambiguous boundaries of these initial groups using semantic entropy and geometric opacity. Subsequently, by interpreting the object's appearance across its most representative viewpoints, a Vision-Language Model (VLM) distills robust textual features that reconciles visual inconsistencies, enabling open-vocabulary querying via semantic matching. By eliminating the costly per-scene training process, MUSplat reduces scene adaptation time from hours to mere minutes. On benchmark tasks for open-vocabulary 3D object selection and semantic segmentation, MUSplat outperforms established training-based frameworks while simultaneously addressing their monosemous limitations.

</details>


### [35] [GS-2M: Gaussian Splatting for Joint Mesh Reconstruction and Material Decomposition](https://arxiv.org/abs/2509.22276)
*Dinh Minh Nguyen,Malte Avenhaus,Thomas Lindemeier*

Main category: cs.CV

TL;DR: GS-2M是一个基于3D高斯泼溅的统一框架，能够从多视角图像同时进行网格重建和材质分解，特别擅长处理高反射表面且无需依赖外部模型先验。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将网格重建和材质分解分开处理，难以重建高反射表面，且依赖外部模型先验来提升分解效果。同时，现有联合方法使用复杂的神经组件，限制了其在大规模场景下的性能。

Method: 基于3D高斯泼溅，联合优化与渲染深度和法线质量相关的属性，提出基于多视角光度变化的新型粗糙度监督策略，结合精心设计的损失函数和优化过程。

Result: 该方法能够生成与最先进方法相当的网格重建结果，提供三角形网格及其关联的材质组件，适用于下游任务。在广泛使用的数据集上验证了有效性。

Conclusion: GS-2M提供了一个统一的解决方案，能够同时处理网格重建和材质分解，对高反射表面具有鲁棒性，且避免了复杂神经组件的使用，具有良好的可扩展性。

Abstract: We propose a unified solution for mesh reconstruction and material decomposition from multi-view images based on 3D Gaussian Splatting, referred to as GS-2M. Previous works handle these tasks separately and struggle to reconstruct highly reflective surfaces, often relying on priors from external models to enhance the decomposition results. Conversely, our method addresses these two problems by jointly optimizing attributes relevant to the quality of rendered depth and normals, maintaining geometric details while being resilient to reflective surfaces. Although contemporary works effectively solve these tasks together, they often employ sophisticated neural components to learn scene properties, which hinders their performance at scale. To further eliminate these neural components, we propose a novel roughness supervision strategy based on multi-view photometric variation. When combined with a carefully designed loss and optimization process, our unified framework produces reconstruction results comparable to state-of-the-art methods, delivering triangle meshes and their associated material components for downstream tasks. We validate the effectiveness of our approach with widely used datasets from previous works and qualitative comparisons with state-of-the-art surface reconstruction methods.

</details>


### [36] [Jailbreaking on Text-to-Video Models via Scene Splitting Strategy](https://arxiv.org/abs/2509.22292)
*Wonjun Lee,Haon Park,Doehyeon Lee,Bumsub Ham,Suhyun Kim*

Main category: cs.CV

TL;DR: SceneSplit是一种新颖的黑盒越狱方法，通过将有害叙述分解为多个独立良性的场景来绕过文本到视频模型的安全机制。


<details>
  <summary>Details</summary>
Motivation: 随着文本到视频模型的快速发展，其安全风险日益凸显。虽然已有研究探索了LLMs、VLMs和文本到图像模型的漏洞，但文本到视频模型的安全问题尚未得到充分研究。

Method: 将有害叙述分解为多个独立良性的场景，利用场景组合约束生成空间；通过迭代场景操纵绕过安全过滤器；使用策略库复用成功攻击模式。

Result: 在11个安全类别上评估，SceneSplit在Luma Ray2、Hailuo和Veo2模型上分别达到77.2%、84.1%和78.2%的平均攻击成功率，显著优于现有基线。

Conclusion: 当前文本到视频安全机制容易受到利用叙事结构的攻击，这为理解和改进文本到视频模型安全性提供了新见解。

Abstract: Along with the rapid advancement of numerous Text-to-Video (T2V) models, growing concerns have emerged regarding their safety risks. While recent studies have explored vulnerabilities in models like LLMs, VLMs, and Text-to-Image (T2I) models through jailbreak attacks, T2V models remain largely unexplored, leaving a significant safety gap. To address this gap, we introduce SceneSplit, a novel black-box jailbreak method that works by fragmenting a harmful narrative into multiple scenes, each individually benign. This approach manipulates the generative output space, the abstract set of all potential video outputs for a given prompt, using the combination of scenes as a powerful constraint to guide the final outcome. While each scene individually corresponds to a wide and safe space where most outcomes are benign, their sequential combination collectively restricts this space, narrowing it to an unsafe region and significantly increasing the likelihood of generating a harmful video. This core mechanism is further enhanced through iterative scene manipulation, which bypasses the safety filter within this constrained unsafe region. Additionally, a strategy library that reuses successful attack patterns further improves the attack's overall effectiveness and robustness. To validate our method, we evaluate SceneSplit across 11 safety categories on T2V models. Our results show that it achieves a high average Attack Success Rate (ASR) of 77.2% on Luma Ray2, 84.1% on Hailuo, and 78.2% on Veo2, significantly outperforming the existing baseline. Through this work, we demonstrate that current T2V safety mechanisms are vulnerable to attacks that exploit narrative structure, providing new insights for understanding and improving the safety of T2V models.

</details>


### [37] [HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models](https://arxiv.org/abs/2509.22300)
*Seyedmorteza Sadat,Farnood Salehi,Romann M. Weber*

Main category: cs.CV

TL;DR: 提出了一种基于动量的采样技术HiGS，通过整合近期模型预测来提升扩散模型采样质量和效率，无需额外计算或训练。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中仍有不真实和缺乏细节的问题，特别是在使用较少NFEs或较低引导尺度时。

Method: 利用当前预测与过去预测加权平均之间的差异来引导采样过程，实现更真实、细节更丰富的输出。

Result: HiGS在各种模型和架构下都能一致提升图像质量，使用SiT模型在30步采样下实现了1.61的FID新纪录。

Conclusion: HiGS是一个即插即用的扩散采样增强方法，能够实现更快速度、更高保真度的生成。

Abstract: While diffusion models have made remarkable progress in image generation, their outputs can still appear unrealistic and lack fine details, especially when using fewer number of neural function evaluations (NFEs) or lower guidance scales. To address this issue, we propose a novel momentum-based sampling technique, termed history-guided sampling (HiGS), which enhances quality and efficiency of diffusion sampling by integrating recent model predictions into each inference step. Specifically, HiGS leverages the difference between the current prediction and a weighted average of past predictions to steer the sampling process toward more realistic outputs with better details and structure. Our approach introduces practically no additional computation and integrates seamlessly into existing diffusion frameworks, requiring neither extra training nor fine-tuning. Extensive experiments show that HiGS consistently improves image quality across diverse models and architectures and under varying sampling budgets and guidance scales. Moreover, using a pretrained SiT model, HiGS achieves a new state-of-the-art FID of 1.61 for unguided ImageNet generation at 256$\times$256 with only 30 sampling steps (instead of the standard 250). We thus present HiGS as a plug-and-play enhancement to standard diffusion sampling that enables faster generation with higher fidelity.

</details>


### [38] [RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion Transformer](https://arxiv.org/abs/2509.22323)
*Wangbo Zhao,Yizeng Han,Zhiwei Tang,Jiasheng Tang,Pengfei Zhou,Kai Wang,Bohan Zhuang,Zhangyang Wang,Fan Wang,Yang You*

Main category: cs.CV

TL;DR: RAPID3是一个零更新基础生成器的三层次强化加速框架，通过步长跳过、缓存重用和稀疏注意力三个轻量级策略头实现图像级自适应加速，在保持生成质量的同时实现近3倍采样加速。


<details>
  <summary>Details</summary>
Motivation: 现有扩散变换器加速方法要么使用统一启发式策略导致质量损失，要么需要高成本的微调，缺乏既高效又无需更新基础模型的图像级自适应加速方案。

Method: 提出三个轻量级策略头：步长跳过、缓存重用和稀疏注意力，通过组相对策略优化在线训练策略参数，同时使用对抗性判别器增强奖励信号防止奖励攻击。

Result: 在包括Stable Diffusion 3和FLUX在内的最先进DiT骨干网络上，RAPID3实现了近3倍采样加速，同时保持竞争力的生成质量。

Conclusion: RAPID3框架成功实现了无需更新基础生成器的图像级自适应加速，为扩散变换器的高效采样提供了有效解决方案。

Abstract: Diffusion Transformers (DiTs) excel at visual generation yet remain hampered by slow sampling. Existing training-free accelerators - step reduction, feature caching, and sparse attention - enhance inference speed but typically rely on a uniform heuristic or a manually designed adaptive strategy for all images, leaving quality on the table. Alternatively, dynamic neural networks offer per-image adaptive acceleration, but their high fine-tuning costs limit broader applicability. To address these limitations, we introduce RAPID3: Tri-Level Reinforced Acceleration Policies for Diffusion Transformers, a framework that delivers image-wise acceleration with zero updates to the base generator. Specifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and Sparse-Attention - observe the current denoising state and independently decide their corresponding speed-up at each timestep. All policy parameters are trained online via Group Relative Policy Optimization (GRPO) while the generator remains frozen. Meanwhile, an adversarially learned discriminator augments the reward signal, discouraging reward hacking by boosting returns only when generated samples stay close to the original model's distribution. Across state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX, RAPID3 achieves nearly 3x faster sampling with competitive generation quality.

</details>


### [39] [Effectiveness of Large Multimodal Models in Detecting Disinformation: Experimental Results](https://arxiv.org/abs/2509.22377)
*Yasmina Kheddache,Marc Lalonde*

Main category: cs.CV

TL;DR: 本研究利用GPT-4o模型开发了一个多模态虚假信息检测框架，通过优化的提示工程、结构化分析方法和六项评估标准，在多个数据集上评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 多模态环境下的虚假信息传播日益严重，需要利用大型多模态模型的先进能力来检测和缓解这一问题。

Method: 采用GPT-4o模型，开发优化提示工程，建立结构化多模态分析框架，定义六项评估标准，并在多个数据集上进行性能分析。

Result: 在Gossipcop、Politifact、Fakeddit、MMFakeBench和AMMEBA等数据集上进行了全面性能分析，揭示了GPT-4o在虚假信息检测中的优势和局限性。

Conclusion: 该研究提供了一个稳健且可复现的多模态虚假信息自动化分析方法论框架。

Abstract: The proliferation of disinformation, particularly in multimodal contexts combining text and images, presents a significant challenge across digital platforms. This study investigates the potential of large multimodal models (LMMs) in detecting and mitigating false information. We propose to approach multimodal disinformation detection by leveraging the advanced capabilities of the GPT-4o model. Our contributions include: (1) the development of an optimized prompt incorporating advanced prompt engineering techniques to ensure precise and consistent evaluations; (2) the implementation of a structured framework for multimodal analysis, including a preprocessing methodology for images and text to comply with the model's token limitations; (3) the definition of six specific evaluation criteria that enable a fine-grained classification of content, complemented by a self-assessment mechanism based on confidence levels; (4) a comprehensive performance analysis of the model across multiple heterogeneous datasets Gossipcop, Politifact, Fakeddit, MMFakeBench, and AMMEBA highlighting GPT-4o's strengths and limitations in disinformation detection; (5) an investigation of prediction variability through repeated testing, evaluating the stability and reliability of the model's classifications; and (6) the introduction of confidence-level and variability-based evaluation methods. These contributions provide a robust and reproducible methodological framework for automated multimodal disinformation analysis.

</details>


### [40] [Text Adversarial Attacks with Dynamic Outputs](https://arxiv.org/abs/2509.22393)
*Wenqiang Wang,Siyuan Liang,Xiao Yan,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出TDOA方法，通过聚类训练代理模型将动态输出场景转换为静态单输出场景，实现文本对抗攻击，在动态和静态场景下均取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有文本对抗攻击方法主要针对静态场景，无法处理动态输出标签空间的问题，需要设计适用于动态输出场景的攻击方法。

Method: 采用基于聚类的代理模型训练方法，将动态输出转换为静态单输出；提出最远标签目标攻击策略，选择偏离模型粗粒度标签最远的对抗向量来最大化干扰效果。

Result: 在4个数据集和8个受害者模型上评估，单次查询最大攻击成功率50.81%；在静态场景下最大ASR达82.68%；在生成式设置中超越先前结果0.64 RDBLEU和0.62 RDchrF。

Conclusion: TDOA方法有效解决了动态输出场景的文本对抗攻击问题，在有限访问条件下具有破坏大型语言模型的强大潜力。

Abstract: Text adversarial attack methods are typically designed for static scenarios with fixed numbers of output labels and a predefined label space, relying on extensive querying of the victim model (query-based attacks) or the surrogate model (transfer-based attacks). To address this gap, we introduce the Textual Dynamic Outputs Attack (TDOA) method, which employs a clustering-based surrogate model training approach to convert the dynamic-output scenario into a static single-output scenario. To improve attack effectiveness, we propose the farthest-label targeted attack strategy, which selects adversarial vectors that deviate most from the model's coarse-grained labels, thereby maximizing disruption. We extensively evaluate TDOA on four datasets and eight victim models (e.g., ChatGPT-4o, ChatGPT-4.1), showing its effectiveness in crafting adversarial examples and its strong potential to compromise large language models with limited access. With a single query per text, TDOA achieves a maximum attack success rate of 50.81\%. Additionally, we find that TDOA also achieves state-of-the-art performance in conventional static output scenarios, reaching a maximum ASR of 82.68\%. Meanwhile, by conceptualizing translation tasks as classification problems with unbounded output spaces, we extend the TDOA framework to generative settings, surpassing prior results by up to 0.64 RDBLEU and 0.62 RDchrF.

</details>


### [41] [Closing the Safety Gap: Surgical Concept Erasure in Visual Autoregressive Models](https://arxiv.org/abs/2509.22400)
*Xinhao Zhong,Yimin Zhou,Zhiqi Zhang,Junhao Li,Yi Sun,Bin Chen,Shu-Tao Xia,Ke Xu*

Main category: cs.CV

TL;DR: 提出了VARE框架和S-VARE方法，专门针对视觉自回归模型的概念擦除问题，通过辅助视觉令牌和过滤交叉熵损失实现精准概念移除，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 视觉自回归模型的快速发展带来了文本到图像生成的新机遇，但也加剧了安全担忧。现有的概念擦除技术主要针对扩散模型，无法适用于VAR模型的下一尺度令牌预测范式。

Method: 首先提出VARE框架，利用辅助视觉令牌降低微调强度；然后引入S-VARE方法，结合过滤交叉熵损失精确识别和最小化调整不安全视觉令牌，以及保持损失来维持语义保真度。

Result: 大量实验证明，该方法实现了外科手术式的概念擦除，同时保持了生成质量，弥补了早期方法在自回归文本到图像生成中的安全差距。

Conclusion: 提出的VARE和S-VARE方法有效解决了VAR模型的概念擦除问题，在保持生成质量的同时实现了精准安全控制。

Abstract: The rapid progress of visual autoregressive (VAR) models has brought new opportunities for text-to-image generation, but also heightened safety concerns. Existing concept erasure techniques, primarily designed for diffusion models, fail to generalize to VARs due to their next-scale token prediction paradigm. In this paper, we first propose a novel VAR Erasure framework VARE that enables stable concept erasure in VAR models by leveraging auxiliary visual tokens to reduce fine-tuning intensity. Building upon this, we introduce S-VARE, a novel and effective concept erasure method designed for VAR, which incorporates a filtered cross entropy loss to precisely identify and minimally adjust unsafe visual tokens, along with a preservation loss to maintain semantic fidelity, addressing the issues such as language drift and reduced diversity introduce by na\"ive fine-tuning. Extensive experiments demonstrate that our approach achieves surgical concept erasure while preserving generation quality, thereby closing the safety gap in autoregressive text-to-image generation by earlier methods.

</details>


### [42] [LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale Diffusion Transformer](https://arxiv.org/abs/2509.22414)
*Song Fei,Tian Ye,Lujia Wang,Lei Zhu*

Main category: cs.CV

TL;DR: LucidFlux是一个无需图像描述的通用图像修复框架，通过双分支条件器和自适应调制策略，在保持全局结构的同时恢复纹理细节，无需文本提示即可实现语义对齐。


<details>
  <summary>Details</summary>
Motivation: 解决现有图像修复方法在未知混合退化情况下容易产生过度平滑、幻觉或语义漂移的问题，特别是避免对图像描述或文本提示的依赖。

Method: 使用大型扩散变换器(Flux.1)，引入轻量级双分支条件器注入退化输入和轻度修复代理信号，设计时间步和层级自适应调制策略，通过SigLIP特征实现无描述语义对齐。

Result: 在合成和真实世界基准测试中，LucidFlux始终优于强大的开源和商业基线，消融研究验证了各组件的必要性。

Conclusion: 对于大型DiT模型，何时、何地以及如何注入条件信息——而非增加参数或依赖文本提示——是实现鲁棒且无需描述的通用图像修复的关键因素。

Abstract: Universal image restoration (UIR) aims to recover images degraded by unknown mixtures while preserving semantics -- conditions under which discriminative restorers and UNet-based diffusion priors often oversmooth, hallucinate, or drift. We present LucidFlux, a caption-free UIR framework that adapts a large diffusion transformer (Flux.1) without image captions. LucidFlux introduces a lightweight dual-branch conditioner that injects signals from the degraded input and a lightly restored proxy to respectively anchor geometry and suppress artifacts. Then, a timestep- and layer-adaptive modulation schedule is designed to route these cues across the backbone's hierarchy, in order to yield coarse-to-fine and context-aware updates that protect the global structure while recovering texture. After that, to avoid the latency and instability of text prompts or MLLM captions, we enforce caption-free semantic alignment via SigLIP features extracted from the proxy. A scalable curation pipeline further filters large-scale data for structure-rich supervision. Across synthetic and in-the-wild benchmarks, LucidFlux consistently outperforms strong open-source and commercial baselines, and ablation studies verify the necessity of each component. LucidFlux shows that, for large DiTs, when, where, and what to condition on -- rather than adding parameters or relying on text prompts -- is the governing lever for robust and caption-free universal image restoration in the wild.

</details>


### [43] [Bézier Meets Diffusion: Robust Generation Across Domains for Medical Image Segmentation](https://arxiv.org/abs/2509.22476)
*Chen Li,Meilong Xu,Xiaoling Hu,Weimin Lyu,Chao Chen*

Main category: cs.CV

TL;DR: 提出Bézier Meets Diffusion框架，通过Bézier曲线风格迁移和条件扩散模型生成高质量目标域标注图像，提升跨域分割性能


<details>
  <summary>Details</summary>
Motivation: 解决医学影像中不同模态间域差距大的问题，现有GAN方法在高度可变区域难以捕捉跨域映射

Method: 1. 基于Bézier曲线的风格迁移策略减少域差距；2. 使用分割模型生成伪标签；3. 训练条件扩散模型合成目标域图像；4. 不确定性引导的分数匹配方法提高鲁棒性

Result: 在公共数据集上的实验表明，该方法能生成逼真的标注图像，显著增强目标域数据并改善分割性能

Conclusion: 提出的统一框架能有效解决跨域医学图像生成问题，为域自适应分割提供了新思路

Abstract: Training robust learning algorithms across different medical imaging modalities is challenging due to the large domain gap. Unsupervised domain adaptation (UDA) mitigates this problem by using annotated images from the source domain and unlabeled images from the target domain to train the deep models. Existing approaches often rely on GAN-based style transfer, but these methods struggle to capture cross-domain mappings in regions with high variability. In this paper, we propose a unified framework, B\'ezier Meets Diffusion, for cross-domain image generation. First, we introduce a B\'ezier-curve-based style transfer strategy that effectively reduces the domain gap between source and target domains. The transferred source images enable the training of a more robust segmentation model across domains. Thereafter, using pseudo-labels generated by this segmentation model on the target domain, we train a conditional diffusion model (CDM) to synthesize high-quality, labeled target-domain images. To mitigate the impact of noisy pseudo-labels, we further develop an uncertainty-guided score matching method that improves the robustness of CDM training. Extensive experiments on public datasets demonstrate that our approach generates realistic labeled images, significantly augmenting the target domain and improving segmentation performance.

</details>


### [44] [PSTTS: A Plug-and-Play Token Selector for Efficient Event-based Spatio-temporal Representation Learning](https://arxiv.org/abs/2509.22481)
*Xiangmo Zhao,Nan Yang,Yang Wang,Zhanwen Liu*

Main category: cs.CV

TL;DR: 提出PSTTS模块，通过空间令牌净化和时间令牌选择消除事件数据中的时空冗余，在保持精度的同时显著提升计算效率


<details>
  <summary>Details</summary>
Motivation: 现有基于事件帧的方法忽略了事件帧序列的高空间稀疏性和帧间运动冗余，导致计算开销大；现有RGB视频的令牌稀疏化方法不适用于事件数据

Method: PSTTS包含两个阶段：空间令牌净化通过评估事件帧内事件的时空一致性来丢弃噪声和非事件区域；时间令牌选择通过评估相邻事件帧间的运动模式相似性来识别和移除冗余时间信息

Result: 在HARDVS、DailyDVS-200和SeACT数据集上，PSTTS显著提升效率：在DailyDVS-200上减少FLOPs 29-43.6%，提升FPS 21.6-41.3%，同时保持任务精度

Conclusion: PSTTS利用事件数据的时空分布特性有效识别和丢弃时空冗余令牌，实现了精度和效率的最佳平衡，是一个无需额外参数的即插即用模块

Abstract: Mainstream event-based spatio-temporal representation learning methods typically process event streams by converting them into sequences of event frames, achieving remarkable performance. However, they neglect the high spatial sparsity and inter-frame motion redundancy inherent in event frame sequences, leading to significant computational overhead. Existing token sparsification methods for RGB videos rely on unreliable intermediate token representations and neglect the influence of event noise, making them ineffective for direct application to event data. In this paper, we propose Progressive Spatio-Temporal Token Selection (PSTTS), a Plug-and-Play module for event data without introducing any additional parameters. PSTTS exploits the spatio-temporal distribution characteristics embedded in raw event data to effectively identify and discard spatio-temporal redundant tokens, achieving an optimal trade-off between accuracy and efficiency. Specifically, PSTTS consists of two stages, Spatial Token Purification and Temporal Token Selection. Spatial Token Purification discards noise and non-event regions by assessing the spatio-temporal consistency of events within each event frame to prevent interference with subsequent temporal redundancy evaluation. Temporal Token Selection evaluates the motion pattern similarity between adjacent event frames, precisely identifying and removing redundant temporal information. We apply PSTTS to four representative backbones UniformerV2, VideoSwin, EVMamba, and ExACT on the HARDVS, DailyDVS-200, and SeACT datasets. Experimental results demonstrate that PSTTS achieves significant efficiency improvements. Specifically, PSTTS reduces FLOPs by 29-43.6% and increases FPS by 21.6-41.3% on the DailyDVS-200 dataset, while maintaining task accuracy. Our code will be available.

</details>


### [45] [Group Critical-token Policy Optimization for Autoregressive Image Generation](https://arxiv.org/abs/2509.22485)
*Guohui Zhang,Hu Yu,Xiaoxiao Ma,JingHao Zhang,Yaning Pan,Mingde Yao,Jie Xiao,Linjiang Huang,Feng Zhao*

Main category: cs.CV

TL;DR: 提出了GCPO方法，通过识别关键图像token并进行针对性优化，在AR视觉生成中仅使用30%的token就能达到比全token优化更好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法对所有图像token进行均匀优化，但不同token对RLVR训练的贡献不同，需要识别关键token并实施有效的token级优化。

Method: 从三个角度识别关键token：因果依赖、熵诱导的空间结构、RLVR聚焦的token多样性。引入动态token级优势权重来鼓励探索。

Result: 在多个文本到图像基准测试中，GCPO仅使用30%的token就比GRPO全token优化表现更好。

Conclusion: GCPO通过关键token识别和针对性优化，有效提升了AR视觉生成的性能。

Abstract: Recent studies have extended Reinforcement Learning with Verifiable Rewards (RLVR) to autoregressive (AR) visual generation and achieved promising progress. However, existing methods typically apply uniform optimization across all image tokens, while the varying contributions of different image tokens for RLVR's training remain unexplored. In fact, the key obstacle lies in how to identify more critical image tokens during AR generation and implement effective token-wise optimization for them. To tackle this challenge, we propose $\textbf{G}$roup $\textbf{C}$ritical-token $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{GCPO}$), which facilitates effective policy optimization on critical tokens. We identify the critical tokens in RLVR-based AR generation from three perspectives, specifically: $\textbf{(1)}$ Causal dependency: early tokens fundamentally determine the later tokens and final image effect due to unidirectional dependency; $\textbf{(2)}$ Entropy-induced spatial structure: tokens with high entropy gradients correspond to image structure and bridges distinct visual regions; $\textbf{(3)}$ RLVR-focused token diversity: tokens with low visual similarity across a group of sampled images contribute to richer token-level diversity. For these identified critical tokens, we further introduce a dynamic token-wise advantage weight to encourage exploration, based on confidence divergence between the policy model and reference model. By leveraging 30\% of the image tokens, GCPO achieves better performance than GRPO with full tokens. Extensive experiments on multiple text-to-image benchmarks for both AR models and unified multimodal models demonstrate the effectiveness of GCPO for AR visual generation.

</details>


### [46] [Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting](https://arxiv.org/abs/2509.22615)
*Yasmine Omri,Connor Ding,Tsachy Weissman,Thierry Tambe*

Main category: cs.CV

TL;DR: 该论文探索使用2D高斯泼溅(2DGS)作为视觉语言模型的替代视觉表示，相比传统RGB图像，2DGS能显著压缩输入数据(3-20倍)并减少序列长度，同时保持有意义的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统RGB视觉编码器的两个结构效率问题：(i)从边缘设备传输密集RGB图像到云端能耗高、成本大；(ii)基于补丁的标记化导致序列长度爆炸，给注意力机制带来压力。

Method: 开发可扩展的2DGS流水线，包括结构化初始化、亮度感知剪枝和批处理CUDA内核；通过轻量级splat感知输入主干和感知器重采样器，将对比语言图像预训练(CLIP)适配到2DGS表示。

Result: 2DGS编码器在DataComp子集上实现了有意义的零样本ImageNet-1K性能，输入压缩比达到3-20倍，仅训练总参数的约7%，拟合速度比先前实现快90倍以上，GPU利用率约97%。

Conclusion: 2DGS被确立为可行的多模态基础，虽然当前准确性仍落后于RGB编码器，但为开发既语义强大又传输高效的边缘-云端学习表示开辟了新路径。

Abstract: Modern vision language pipelines are driven by RGB vision encoders trained on massive image text corpora. While these pipelines have enabled impressive zero shot capabilities and strong transfer across tasks, they still inherit two structural inefficiencies from the pixel domain: (i) transmitting dense RGB images from edge devices to the cloud is energy intensive and costly, and (ii) patch based tokenization explodes sequence length, stressing attention budgets and context limits. We explore 2D Gaussian Splatting (2DGS) as an alternative visual substrate for alignment: a compact, spatially adaptive representation that parameterizes images by a set of colored anisotropic Gaussians. We develop a scalable 2DGS pipeline with structured initialization, luminance aware pruning, and batched CUDA kernels, achieving over 90x faster fitting and about 97% GPU utilization compared to prior implementations. We further adapt contrastive language image pretraining (CLIP) to 2DGS by reusing a frozen RGB-based transformer backbone with a lightweight splat aware input stem and a perceiver resampler, training only about 7% of the total parameters. On large DataComp subsets, GS encoders yield meaningful zero shot ImageNet-1K performance while compressing inputs 3 to 20x relative to pixels. While accuracy currently trails RGB encoders, our results establish 2DGS as a viable multimodal substrate, pinpoint architectural bottlenecks, and open a path toward representations that are both semantically powerful and transmission efficient for edge cloud learning.

</details>


### [47] [LongLive: Real-time Interactive Long Video Generation](https://arxiv.org/abs/2509.22622)
*Shuai Yang,Wei Huang,Ruihang Chu,Yicheng Xiao,Yuyang Zhao,Xianbang Wang,Muyang Li,Enze Xie,Yingcong Chen,Yao Lu,Song Han,Yukang Chen*

Main category: cs.CV

TL;DR: LongLive是一个实时交互式长视频生成框架，采用帧级自回归设计，通过KV重缓存机制、流式长调优和帧级注意力汇聚技术，在单H100 GPU上实现20.7 FPS的推理速度，支持240秒长视频生成。


<details>
  <summary>Details</summary>
Motivation: 解决长视频生成在效率和质量上的挑战：扩散模型效率低，因果注意力模型在长视频上质量下降，同时需要支持实时交互式提示输入以增强动态内容创作能力。

Method: 采用因果帧级自回归设计，集成KV重缓存机制实现平滑提示切换，流式长调优对齐训练与推理，短窗口注意力配合帧级注意力汇聚保持长程一致性。

Result: 仅用32 GPU天微调1.3B参数短片段模型实现分钟级生成，单H100 GPU推理速度达20.7 FPS，支持240秒视频，INT8量化推理质量损失微小，在VBench上短长视频均表现优异。

Conclusion: LongLive成功解决了长视频生成的效率、质量和交互性挑战，为实时动态内容创作提供了可行解决方案。

Abstract: We present LongLive, a frame-level autoregressive (AR) framework for real-time and interactive long video generation. Long video generation presents challenges in both efficiency and quality. Diffusion and Diffusion-Forcing models can produce high-quality videos but suffer from low efficiency due to bidirectional attention. Causal attention AR models support KV caching for faster inference, but often degrade in quality on long videos due to memory challenges during long-video training. In addition, beyond static prompt-based generation, interactive capabilities, such as streaming prompt inputs, are critical for dynamic content creation, enabling users to guide narratives in real time. This interactive requirement significantly increases complexity, especially in ensuring visual consistency and semantic coherence during prompt transitions. To address these challenges, LongLive adopts a causal, frame-level AR design that integrates a KV-recache mechanism that refreshes cached states with new prompts for smooth, adherent switches; streaming long tuning to enable long video training and to align training and inference (train-long-test-long); and short window attention paired with a frame-level attention sink, shorten as frame sink, preserving long-range consistency while enabling faster generation. With these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model to minute-long generation in just 32 GPU-days. At inference, LongLive sustains 20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both short and long videos. LongLive supports up to 240-second videos on a single H100 GPU. LongLive further supports INT8-quantized inference with only marginal quality loss.

</details>


### [48] [Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance](https://arxiv.org/abs/2509.22635)
*Luc Boudier,Loris Manganelli,Eleftherios Tsonis,Nicolas Dufour,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: 提出DIPSY方法，利用IP-Adapter进行图像到图像转换，无需模型微调即可生成判别性合成图像，在少样本图像分类中实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 解决少样本图像分类中标记数据有限的问题，避免现有方法需要模型微调或外部信息源的依赖

Method: 使用IP-Adapter进行图像到图像转换，引入扩展的无分类器引导方案、基于类别相似度的采样策略，无需模型微调或外部工具

Result: 在十个基准数据集上达到最先进或可比性能，特别在细粒度分类任务中表现突出

Conclusion: DIPSY通过正负引导的双重图像提示有效生成类别判别特征，为少样本分类提供高效训练免费解决方案

Abstract: Few-shot image classification remains challenging due to the limited availability of labeled examples. Recent approaches have explored generating synthetic training data using text-to-image diffusion models, but often require extensive model fine-tuning or external information sources. We present a novel training-free approach, called DIPSY, that leverages IP-Adapter for image-to-image translation to generate highly discriminative synthetic images using only the available few-shot examples. DIPSY introduces three key innovations: (1) an extended classifier-free guidance scheme that enables independent control over positive and negative image conditioning; (2) a class similarity-based sampling strategy that identifies effective contrastive examples; and (3) a simple yet effective pipeline that requires no model fine-tuning or external captioning and filtering. Experiments across ten benchmark datasets demonstrate that our approach achieves state-of-the-art or comparable performance, while eliminating the need for generative model adaptation or reliance on external tools for caption generation and image filtering. Our results highlight the effectiveness of leveraging dual image prompting with positive-negative guidance for generating class-discriminative features, particularly for fine-grained classification tasks.

</details>


### [49] [Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs](https://arxiv.org/abs/2509.22646)
*Xingyu Fu,Siyi Liu,Yinuo Xu,Pan Lu,Guangqiuse Hu,Tianbo Yang,Taran Anantasagar,Christopher Shen,Yikai Mao,Yuanzhe Liu,Keyush Shah,Chung Un Lee,Yejin Choi,James Zou,Dan Roth,Chris Callison-Burch*

Main category: cs.CV

TL;DR: DeeptraceReward是首个细粒度、时空感知的基准数据集，用于标注人类感知的深度伪造痕迹，包含4.3K个详细标注，训练多模态语言模型作为奖励模型来模仿人类判断和定位。


<details>
  <summary>Details</summary>
Motivation: 视频生成模型快速发展，但人类能否检测生成视频中的深度伪造痕迹这一关键维度被忽视，需要研究人类感知的时空视觉伪影。

Method: 构建包含4.3K详细标注的数据集，每个标注提供自然语言解释、边界框区域和精确时间戳，将标注整合为9类深度伪造痕迹，训练多模态语言模型作为奖励模型。

Result: 7B奖励模型在DeeptraceReward上平均比GPT-5表现好34.7%，观察到一致的难度梯度：二元分类最容易，细粒度检测更难，其中自然语言解释最容易，空间定位次之，时间标注最难。

Conclusion: 通过突出人类感知的深度伪造痕迹，DeeptraceReward为社会意识和可信视频生成提供了严格的测试平台和训练信号。

Abstract: Can humans identify AI-generated (fake) videos and provide grounded reasons? While video generation models have advanced rapidly, a critical dimension -- whether humans can detect deepfake traces within a generated video, i.e., spatiotemporal grounded visual artifacts that reveal a video as machine generated -- has been largely overlooked. We introduce DeeptraceReward, the first fine-grained, spatially- and temporally- aware benchmark that annotates human-perceived fake traces for video generation reward. The dataset comprises 4.3K detailed annotations across 3.3K high-quality generated videos. Each annotation provides a natural-language explanation, pinpoints a bounding-box region containing the perceived trace, and marks precise onset and offset timestamps. We consolidate these annotations into 9 major categories of deepfake traces that lead humans to identify a video as AI-generated, and train multimodal language models (LMs) as reward models to mimic human judgments and localizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by 34.7% on average across fake clue identification, grounding, and explanation. Interestingly, we observe a consistent difficulty gradient: binary fake v.s. real classification is substantially easier than fine-grained deepfake trace detection; within the latter, performance degrades from natural language explanations (easiest), to spatial grounding, to temporal labeling (hardest). By foregrounding human-perceived deepfake traces, DeeptraceReward provides a rigorous testbed and training signal for socially aware and trustworthy video generation.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [50] [Guidance Watermarking for Diffusion Models](https://arxiv.org/abs/2509.22126)
*Enoal Gesny,Eva Giboulot,Teddy Furon,Vivien Chappelier*

Main category: cs.CR

TL;DR: 提出了一种基于梯度引导的扩散模型水印方法，可将后验水印方案转换为生成过程中的嵌入，无需重新训练或微调即可增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 将后验水印方案有效转换为扩散生成过程中的嵌入，增强水印的鲁棒性而不影响生成质量。

Method: 使用现成水印解码器计算梯度来引导扩散过程，结合图像增强技术提高对未训练攻击的鲁棒性。

Result: 在不同扩散模型和检测器上验证有效，水印引导不会显著改变给定种子和提示的生成图像，保持生成多样性和质量。

Conclusion: 该方法与变分自编码器修改技术互补，为扩散模型提供了一种有效的水印嵌入方案。

Abstract: This paper introduces a novel watermarking method for diffusion models. It is based on guiding the diffusion process using the gradient computed from any off-the-shelf watermark decoder. The gradient computation encompasses different image augmentations, increasing robustness to attacks against which the decoder was not originally robust, without retraining or fine-tuning. Our method effectively convert any \textit{post-hoc} watermarking scheme into an in-generation embedding along the diffusion process. We show that this approach is complementary to watermarking techniques modifying the variational autoencoder at the end of the diffusion process. We validate the methods on different diffusion models and detectors. The watermarking guidance does not significantly alter the generated image for a given seed and prompt, preserving both the diversity and quality of generation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [51] [VISION: Prompting Ocean Vertical Velocity Reconstruction from Incomplete Observations](https://arxiv.org/abs/2509.21477)
*Yuan Gao,Hao Wu,Qingsong Wen,Kun Wang,Xian Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: 提出了VISION模型，通过动态提示机制从部分海面观测数据重建海洋次表层动力学，并在新构建的KD48基准数据集上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 解决从不完备海面观测数据重建海洋次表层动力学的挑战，该领域长期缺乏标准化基准，阻碍了系统研究进展。

Method: 构建KD48高分辨率海洋动力学基准数据集，提出基于动态提示的VISION重建范式，通过状态条件提示模块将视觉提示注入通用主干网络，实现自适应计算策略调整。

Result: 在KD48基准上的大量实验表明，VISION显著优于现有最优模型，在极端数据缺失场景下表现出强泛化能力。

Conclusion: 通过提供高质量基准和鲁棒模型，为数据不确定性下的海洋科学研究建立了坚实基础设施。

Abstract: Reconstructing subsurface ocean dynamics, such as vertical velocity fields, from incomplete surface observations poses a critical challenge in Earth science, a field long hampered by the lack of standardized, analysis-ready benchmarks. To systematically address this issue and catalyze research, we first build and release KD48, a high-resolution ocean dynamics benchmark derived from petascale simulations and curated with expert-driven denoising. Building on this benchmark, we introduce VISION, a novel reconstruction paradigm based on Dynamic Prompting designed to tackle the core problem of missing data in real-world observations. The essence of VISION lies in its ability to generate a visual prompt on-the-fly from any available subset of observations, which encodes both data availability and the ocean's physical state. More importantly, we design a State-conditioned Prompting module that efficiently injects this prompt into a universal backbone, endowed with geometry- and scale-aware operators, to guide its adaptive adjustment of computational strategies. This mechanism enables VISION to precisely handle the challenges posed by varying input combinations. Extensive experiments on the KD48 benchmark demonstrate that VISION not only substantially outperforms state-of-the-art models but also exhibits strong generalization under extreme data missing scenarios. By providing a high-quality benchmark and a robust model, our work establishes a solid infrastructure for ocean science research under data uncertainty. Our codes are available at: https://github.com/YuanGao-YG/VISION.

</details>


### [52] [SlimDiff: Training-Free, Activation-Guided Hands-free Slimming of Diffusion Models](https://arxiv.org/abs/2509.21498)
*Arani Roy,Shristi Das Biswas,Kaushik Roy*

Main category: cs.LG

TL;DR: SlimDiff是一个无需梯度的扩散模型结构压缩框架，通过激活感知的谱近似方法动态修剪注意力机制和前馈网络维度，实现35%加速和约1亿参数减少，且无需微调。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成性能优秀，但存在计算成本高的问题。现有效率技术依赖于微调或重新训练来恢复性能，存在瓶颈。

Method: 将扩散模型压缩重新定义为谱近似任务，利用去噪时间步的激活协方差定义低秩子空间，在固定压缩预算下指导动态修剪，采用模块级分解而非孤立矩阵分解。

Result: 相比基线实现35%加速和约1亿参数减少，生成质量与未压缩模型相当，仅需约500个校准样本，比之前方法少70倍以上。

Conclusion: 这是第一个基于闭式解、激活引导的扩散模型结构压缩方法，完全无需训练，提供了理论清晰性和实际效率。

Abstract: Diffusion models (DMs), lauded for their generative performance, are computationally prohibitive due to their billion-scale parameters and iterative denoising dynamics. Existing efficiency techniques, such as quantization, timestep reduction, or pruning, offer savings in compute, memory, or runtime but are strictly bottlenecked by reliance on fine-tuning or retraining to recover performance. In this work, we introduce SlimDiff, an automated activation-informed structural compression framework that reduces both attention and feedforward dimensionalities in DMs, while being entirely gradient-free. SlimDiff reframes DM compression as a spectral approximation task, where activation covariances across denoising timesteps define low-rank subspaces that guide dynamic pruning under a fixed compression budget. This activation-aware formulation mitigates error accumulation across timesteps by applying module-wise decompositions over functional weight groups: query--key interactions, value--output couplings, and feedforward projections, rather than isolated matrix factorizations, while adaptively allocating sparsity across modules to respect the non-uniform geometry of diffusion trajectories. SlimDiff achieves up to 35\% acceleration and $\sim$100M parameter reduction over baselines, with generation quality on par with uncompressed models without any backpropagation. Crucially, our approach requires only about 500 calibration samples, over 70$\times$ fewer than prior methods. To our knowledge, this is the first closed-form, activation-guided structural compression of DMs that is entirely training-free, providing both theoretical clarity and practical efficiency.

</details>


### [53] [DistillKac: Few-Step Image Generation via Damped Wave Equations](https://arxiv.org/abs/2509.21513)
*Weiqiao Han,Chenlin Meng,Christopher D. Manning,Stefano Ermon*

Main category: cs.LG

TL;DR: DistillKac是一种快速图像生成器，使用阻尼波动方程及其随机Kac表示来以有限速度移动概率质量。相比扩散模型，它强制执行有限速度传输并产生全局有界动能。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型中反向时间速度可能变得僵硬且隐含允许无界传播速度的问题，通过Kac动力学强制执行有限速度传输。

Method: 使用阻尼波动方程和随机Kac表示，在速度空间中引入无分类器引导，提出仅端点蒸馏方法训练学生模型匹配冻结的教师模型。

Result: 实验表明DistillKac能够以极少的函数评估次数生成高质量样本，同时保持有限速度概率流的数值稳定性优势。

Conclusion: DistillKac在保持数值稳定性的同时实现了高效的图像生成，证明了有限速度概率流方法的有效性。

Abstract: We present DistillKac, a fast image generator that uses the damped wave equation and its stochastic Kac representation to move probability mass at finite speed. In contrast to diffusion models whose reverse time velocities can become stiff and implicitly allow unbounded propagation speed, Kac dynamics enforce finite speed transport and yield globally bounded kinetic energy. Building on this structure, we introduce classifier-free guidance in velocity space that preserves square integrability under mild conditions. We then propose endpoint only distillation that trains a student to match a frozen teacher over long intervals. We prove a stability result that promotes supervision at the endpoints to closeness along the entire path. Experiments demonstrate DistillKac delivers high quality samples with very few function evaluations while retaining the numerical stability benefits of finite speed probability flows.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [54] [Comparative Analysis of GAN and Diffusion for MRI-to-CT translation](https://arxiv.org/abs/2509.22049)
*Emily Honey,Anders Helbo,Jens Petersen*

Main category: eess.IV

TL;DR: 比较两种MRI到CT图像转换方法：条件生成对抗网络(cGAN)和条件去噪扩散概率模型(cDDPM)，发现cDDPM架构和多通道条件输入能获得更好的性能。


<details>
  <summary>Details</summary>
Motivation: CT图像在治疗和诊断中很重要，但有时难以获取，因此需要从MRI生成合成CT(sCT)图像的方法。需要确定哪种MRI到CT转换策略最有效。

Method: 使用Pix2Pix代表cGAN，Palette代表cDDPM，将3D转换问题分解为2D横向平面转换序列以减少计算成本。研究单切片和多切片条件输入的影响。

Result: cDDPM架构在MRI到CT转换中表现更好，多通道条件输入能提升性能。提出了新的切片相似度指标SIMOS来评估3D重建的连续性。

Conclusion: MRI到CT生成模型受益于多通道条件输入和cDDPM架构的使用。

Abstract: Computed tomography (CT) is essential for treatment and diagnostics; In case CT are missing or otherwise difficult to obtain, methods for generating synthetic CT (sCT) images from magnetic resonance imaging (MRI) images are sought after. Therefore, it is valuable to establish a reference for what strategies are most effective for MRI-to-CT translation. In this paper, we compare the performance of two frequently used architectures for MRI-to-CT translation: a conditional generative adversarial network (cGAN) and a conditional denoising diffusion probabilistic model (cDDPM). We chose well-established implementations to represent each architecture: Pix2Pix for cGAN, and Palette for cDDPM. We separate the classical 3D translation problem into a sequence of 2D translations on the transverse plane, to investigate the viability of a strategy that reduces the computational cost. We also investigate the impact of conditioning the generative process on a single MRI image/slice and on multiple MRI slices. The performance is assessed using a thorough evaluation protocol, including a novel slice-wise metric Similarity Of Slices (SIMOS), which measures the continuity between transverse slices when compiling the sCTs into 3D format. Our comparative analysis revealed that MRI-to-CT generative models benefit from multi-channel conditional input and using cDDPM as an architecture.

</details>


### [55] [Deep Learning-Based Cross-Anatomy CT Synthesis Using Adapted nnResU-Net with Anatomical Feature Prioritized Loss](https://arxiv.org/abs/2509.22394)
*Javier Sequeiro González,Arthur Longuefosse,Miguel Díaz Benito,Álvaro García Martín,Fabien Baldacci*

Main category: eess.IV

TL;DR: 提出基于nnUNet的3D图像翻译方法，用于MR到CT和CBCT到CT转换，引入解剖特征优先损失函数(AFP)提升临床相关结构重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决多中心医学图像跨模态合成问题，特别是在头颈部、胸部和腹部区域，需要提升解剖结构的重建保真度。

Method: 采用标准UNet和残差UNet两种网络配置，引入AFP损失函数结合紧凑分割网络特征，使用3D补丁训练，并进行AFP微调。

Result: 残差网络结合AFP损失能产生更清晰的重建结果，特别是在MR到CT的骨结构和CBCT到CT的病灶重建方面表现更好。

Conclusion: 该方法为跨模态医学图像合成提供了稳定解决方案，证明了nnUNet自动管道与残差学习及解剖引导特征损失结合的有效性。

Abstract: We present a patch-based 3D nnUNet adaptation for MR to CT and CBCT to CT image translation using the multicenter SynthRAD2025 dataset, covering head and neck (HN), thorax (TH), and abdomen (AB) regions. Our approach leverages two main network configurations: a standard UNet and a residual UNet, both adapted from nnUNet for image synthesis. The Anatomical Feature-Prioritized (AFP) loss was introduced, which compares multilayer features extracted from a compact segmentation network trained on TotalSegmentator labels, enhancing reconstruction of clinically relevant structures. Input volumes were normalized per-case using zscore normalization for MRIs, and clipping plus dataset level zscore normalization for CBCT and CT. Training used 3D patches tailored to each anatomical region without additional data augmentation. Models were trained for 1000 and 1500 epochs, with AFP fine-tuning performed for 500 epochs using a combined L1+AFP objective. During inference, overlapping patches were aggregated via mean averaging with step size of 0.3, and postprocessing included reverse zscore normalization. Both network configurations were applied across all regions, allowing consistent model design while capturing local adaptations through residual learning and AFP loss. Qualitative and quantitative evaluation revealed that residual networks combined with AFP yielded sharper reconstructions and improved anatomical fidelity, particularly for bone structures in MR to CT and lesions in CBCT to CT, while L1only networks achieved slightly better intensity-based metrics. This methodology provides a stable solution for cross modality medical image synthesis, demonstrating the effectiveness of combining the automatic nnUNet pipeline with residual learning and anatomically guided feature losses.

</details>
