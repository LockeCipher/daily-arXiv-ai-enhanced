{"id": "2506.20875", "pdf": "https://arxiv.org/pdf/2506.20875", "abs": "https://arxiv.org/abs/2506.20875", "authors": ["Chengan He", "Junxuan Li", "Tobias Kirschstein", "Artem Sevastopolsky", "Shunsuke Saito", "Qingyang Tan", "Javier Romero", "Chen Cao", "Holly Rushmeier", "Giljoo Nam"], "title": "3DGH: 3D Head Generation with Composable Hair and Face", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted to SIGGRAPH 2025. Project page:   https://c-he.github.io/projects/3dgh/", "summary": "We present 3DGH, an unconditional generative model for 3D human heads with composable hair and face components. Unlike previous work that entangles the modeling of hair and face, we propose to separate them using a novel data representation with template-based 3D Gaussian Splatting, in which deformable hair geometry is introduced to capture the geometric variations across different hairstyles. Based on this data representation, we design a 3D GAN-based architecture with dual generators and employ a cross-attention mechanism to model the inherent correlation between hair and face. The model is trained on synthetic renderings using carefully designed objectives to stabilize training and facilitate hair-face separation. We conduct extensive experiments to validate the design choice of 3DGH, and evaluate it both qualitatively and quantitatively by comparing with several state-of-the-art 3D GAN methods, demonstrating its effectiveness in unconditional full-head image synthesis and composable 3D hairstyle editing. More details will be available on our project page: https://c-he.github.io/projects/3dgh/.", "AI": {"tldr": "3DGH\u662f\u4e00\u79cd\u65e0\u6761\u4ef6\u751f\u62103D\u4eba\u5934\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u652f\u6301\u5934\u53d1\u548c\u8138\u90e8\u7684\u53ef\u7ec4\u5408\u6027\u3002\u901a\u8fc7\u5206\u79bb\u5934\u53d1\u548c\u8138\u90e8\u7684\u5efa\u6a21\uff0c\u91c7\u7528\u57fa\u4e8e\u6a21\u677f\u76843D\u9ad8\u65af\u6cfc\u6e85\u6570\u636e\u8868\u793a\u548c\u53d8\u5f62\u5934\u53d1\u51e0\u4f55\u4f53\uff0c\u5b9e\u73b0\u4e86\u4e0d\u540c\u53d1\u578b\u7684\u51e0\u4f55\u53d8\u5316\u6355\u6349\u3002\u6a21\u578b\u91c7\u7528\u53cc\u751f\u6210\u5668\u67b6\u6784\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8bad\u7ec3\u65f6\u4f7f\u7528\u5408\u6210\u6e32\u67d3\u6570\u636e\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u5168\u5934\u56fe\u50cf\u5408\u6210\u548c\u53ef\u7ec4\u5408\u53d1\u578b\u7f16\u8f91\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5934\u53d1\u548c\u8138\u90e8\u5efa\u6a21\u4e0a\u5b58\u5728\u7ea0\u7f20\u95ee\u9898\uff0c\u9650\u5236\u4e86\u751f\u6210\u6a21\u578b\u7684\u7075\u6d3b\u6027\u548c\u7f16\u8f91\u80fd\u529b\u30023DGH\u65e8\u5728\u901a\u8fc7\u5206\u79bb\u8fd9\u4e24\u90e8\u5206\u7684\u5efa\u6a21\uff0c\u63d0\u5347\u751f\u6210\u8d28\u91cf\u548c\u53ef\u7f16\u8f91\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6a21\u677f\u76843D\u9ad8\u65af\u6cfc\u6e85\u6570\u636e\u8868\u793a\u548c\u53d8\u5f62\u5934\u53d1\u51e0\u4f55\u4f53\uff0c\u8bbe\u8ba1\u53cc\u751f\u6210\u5668\u67b6\u6784\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8bad\u7ec3\u65f6\u4f7f\u7528\u5408\u6210\u6e32\u67d3\u6570\u636e\u548c\u7279\u5b9a\u76ee\u6807\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e863DGH\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u5728\u65e0\u6761\u4ef6\u5168\u5934\u56fe\u50cf\u5408\u6210\u548c\u53ef\u7ec4\u5408\u53d1\u578b\u7f16\u8f91\u4e0a\u4f18\u4e8e\u73b0\u67093D GAN\u65b9\u6cd5\u3002", "conclusion": "3DGH\u901a\u8fc7\u5206\u79bb\u5934\u53d1\u548c\u8138\u90e8\u5efa\u6a21\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u76843D\u4eba\u5934\u751f\u6210\u548c\u7075\u6d3b\u7684\u7f16\u8f91\u80fd\u529b\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.20946", "pdf": "https://arxiv.org/pdf/2506.20946", "abs": "https://arxiv.org/abs/2506.20946", "authors": ["Donggoo Kang", "Jangyeong Kim", "Dasol Jeong", "Junyoung Choi", "Jeonga Wi", "Hyunmin Lee", "Joonho Gwon", "Joonki Paik"], "title": "Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware Diffusion and Temporal Video Models", "categories": ["cs.GR", "cs.AI", "cs.CV", "68T45, 68U05", "I.3.7; I.4.10; I.2.10"], "comment": null, "summary": "Current texture synthesis methods, which generate textures from fixed viewpoints, suffer from inconsistencies due to the lack of global context and geometric understanding. Meanwhile, recent advancements in video generation models have demonstrated remarkable success in achieving temporally consistent videos. In this paper, we introduce VideoTex, a novel framework for seamless texture synthesis that leverages video generation models to address both spatial and temporal inconsistencies in 3D textures. Our approach incorporates geometry-aware conditions, enabling precise utilization of 3D mesh structures. Additionally, we propose a structure-wise UV diffusion strategy, which enhances the generation of occluded areas by preserving semantic information, resulting in smoother and more coherent textures. VideoTex not only achieves smoother transitions across UV boundaries but also ensures high-quality, temporally stable textures across video frames. Extensive experiments demonstrate that VideoTex outperforms existing methods in texture fidelity, seam blending, and stability, paving the way for dynamic real-time applications that demand both visual quality and temporal coherence.", "AI": {"tldr": "VideoTex\u5229\u7528\u89c6\u9891\u751f\u6210\u6a21\u578b\u89e3\u51b33D\u7eb9\u7406\u5408\u6210\u4e2d\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u7ed3\u5408\u51e0\u4f55\u611f\u77e5\u6761\u4ef6\u548cUV\u6269\u6563\u7b56\u7565\uff0c\u751f\u6210\u66f4\u5e73\u6ed1\u3001\u4e00\u81f4\u7684\u7eb9\u7406\u3002", "motivation": "\u73b0\u6709\u7eb9\u7406\u5408\u6210\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u51e0\u4f55\u7406\u89e3\u5bfc\u81f4\u4e0d\u4e00\u81f4\uff0c\u800c\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "method": "\u7ed3\u5408\u51e0\u4f55\u611f\u77e5\u6761\u4ef6\u548c\u7ed3\u6784UV\u6269\u6563\u7b56\u7565\uff0c\u5229\u75283D\u7f51\u683c\u7ed3\u6784\u751f\u6210\u7eb9\u7406\u3002", "result": "VideoTex\u5728\u7eb9\u7406\u4fdd\u771f\u5ea6\u3001\u63a5\u7f1d\u878d\u5408\u548c\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "VideoTex\u4e3a\u52a8\u6001\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u4e14\u65f6\u95f4\u4e00\u81f4\u7684\u7eb9\u7406\u5408\u6210\u65b9\u6848\u3002"}}
{"id": "2506.21272", "pdf": "https://arxiv.org/pdf/2506.21272", "abs": "https://arxiv.org/abs/2506.21272", "authors": ["Jiayi Zheng", "Xiaodong Cun"], "title": "FairyGen: Storied Cartoon Video from a Single Child-Drawn Character", "categories": ["cs.GR", "cs.CV", "cs.MM"], "comment": "Project Page: https://jayleejia.github.io/FairyGen/ ; Code:   https://github.com/GVCLab/FairyGen", "summary": "We propose FairyGen, an automatic system for generating story-driven cartoon videos from a single child's drawing, while faithfully preserving its unique artistic style. Unlike previous storytelling methods that primarily focus on character consistency and basic motion, FairyGen explicitly disentangles character modeling from stylized background generation and incorporates cinematic shot design to support expressive and coherent storytelling. Given a single character sketch, we first employ an MLLM to generate a structured storyboard with shot-level descriptions that specify environment settings, character actions, and camera perspectives. To ensure visual consistency, we introduce a style propagation adapter that captures the character's visual style and applies it to the background, faithfully retaining the character's full visual identity while synthesizing style-consistent scenes. A shot design module further enhances visual diversity and cinematic quality through frame cropping and multi-view synthesis based on the storyboard. To animate the story, we reconstruct a 3D proxy of the character to derive physically plausible motion sequences, which are then used to fine-tune an MMDiT-based image-to-video diffusion model. We further propose a two-stage motion customization adapter: the first stage learns appearance features from temporally unordered frames, disentangling identity from motion; the second stage models temporal dynamics using a timestep-shift strategy with frozen identity weights. Once trained, FairyGen directly renders diverse and coherent video scenes aligned with the storyboard. Extensive experiments demonstrate that our system produces animations that are stylistically faithful, narratively structured natural motion, highlighting its potential for personalized and engaging story animation. The code will be available at https://github.com/GVCLab/FairyGen", "AI": {"tldr": "FairyGen \u662f\u4e00\u4e2a\u4ece\u513f\u7ae5\u7ed8\u753b\u81ea\u52a8\u751f\u6210\u6545\u4e8b\u9a71\u52a8\u5361\u901a\u89c6\u9891\u7684\u7cfb\u7edf\uff0c\u4fdd\u6301\u539f\u753b\u7684\u72ec\u7279\u827a\u672f\u98ce\u683c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u89d2\u8272\u4e00\u81f4\u6027\u548c\u57fa\u672c\u52a8\u4f5c\uff0c\u800c FairyGen \u65e8\u5728\u901a\u8fc7\u5206\u79bb\u89d2\u8272\u5efa\u6a21\u4e0e\u98ce\u683c\u5316\u80cc\u666f\u751f\u6210\uff0c\u5e76\u878d\u5165\u7535\u5f71\u955c\u5934\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u66f4\u5177\u8868\u73b0\u529b\u548c\u8fde\u8d2f\u6027\u7684\u6545\u4e8b\u53d9\u8ff0\u3002", "method": "\u7cfb\u7edf\u4f7f\u7528 MLLM \u751f\u6210\u7ed3\u6784\u5316\u6545\u4e8b\u677f\uff0c\u98ce\u683c\u4f20\u64ad\u9002\u914d\u5668\u786e\u4fdd\u89c6\u89c9\u4e00\u81f4\u6027\uff0c\u955c\u5934\u8bbe\u8ba1\u6a21\u5757\u589e\u5f3a\u89c6\u89c9\u591a\u6837\u6027\u548c\u7535\u5f71\u8d28\u91cf\uff0c3D \u4ee3\u7406\u548c MMDiT \u6a21\u578b\u7528\u4e8e\u52a8\u753b\u751f\u6210\uff0c\u4e24\u9636\u6bb5\u8fd0\u52a8\u5b9a\u5236\u9002\u914d\u5668\u5206\u79bb\u8eab\u4efd\u4e0e\u8fd0\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFairyGen \u751f\u6210\u7684\u52a8\u753b\u98ce\u683c\u5fe0\u5b9e\u3001\u53d9\u4e8b\u7ed3\u6784\u81ea\u7136\uff0c\u9002\u5408\u4e2a\u6027\u5316\u6545\u4e8b\u52a8\u753b\u3002", "conclusion": "FairyGen \u5c55\u793a\u4e86\u5728\u4fdd\u6301\u827a\u672f\u98ce\u683c\u7684\u540c\u65f6\u751f\u6210\u9ad8\u8d28\u91cf\u6545\u4e8b\u52a8\u753b\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.20756", "pdf": "https://arxiv.org/pdf/2506.20756", "abs": "https://arxiv.org/abs/2506.20756", "authors": ["Haodong Li", "Chen Wang", "Jiahui Lei", "Kostas Daniilidis", "Lingjie Liu"], "title": "StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation", "categories": ["cs.CV"], "comment": "Work done in Nov. 2024. Project page: https://stereodiff.github.io/", "summary": "Recent video depth estimation methods achieve great performance by following the paradigm of image depth estimation, i.e., typically fine-tuning pre-trained video diffusion models with massive data. However, we argue that video depth estimation is not a naive extension of image depth estimation. The temporal consistency requirements for dynamic and static regions in videos are fundamentally different. Consistent video depth in static regions, typically backgrounds, can be more effectively achieved via stereo matching across all frames, which provides much stronger global 3D cues. While the consistency for dynamic regions still should be learned from large-scale video depth data to ensure smooth transitions, due to the violation of triangulation constraints. Based on these insights, we introduce StereoDiff, a two-stage video depth estimator that synergizes stereo matching for mainly the static areas with video depth diffusion for maintaining consistent depth transitions in dynamic areas. We mathematically demonstrate how stereo matching and video depth diffusion offer complementary strengths through frequency domain analysis, highlighting the effectiveness of their synergy in capturing the advantages of both. Experimental results on zero-shot, real-world, dynamic video depth benchmarks, both indoor and outdoor, demonstrate StereoDiff's SoTA performance, showcasing its superior consistency and accuracy in video depth estimation.", "AI": {"tldr": "StereoDiff\u7ed3\u5408\u7acb\u4f53\u5339\u914d\u548c\u89c6\u9891\u6df1\u5ea6\u6269\u6563\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u63d0\u5347\u89c6\u9891\u6df1\u5ea6\u4f30\u8ba1\u7684\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u89c6\u9891\u6df1\u5ea6\u4f30\u8ba1\u4e0d\u662f\u56fe\u50cf\u6df1\u5ea6\u4f30\u8ba1\u7684\u7b80\u5355\u6269\u5c55\uff0c\u52a8\u6001\u548c\u9759\u6001\u533a\u57df\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u9700\u6c42\u4e0d\u540c\u3002\u7acb\u4f53\u5339\u914d\u66f4\u9002\u5408\u9759\u6001\u533a\u57df\uff0c\u800c\u89c6\u9891\u6df1\u5ea6\u6269\u6563\u66f4\u9002\u5408\u52a8\u6001\u533a\u57df\u3002", "method": "\u63d0\u51faStereoDiff\uff0c\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7acb\u4f53\u5339\u914d\u5904\u7406\u9759\u6001\u533a\u57df\uff0c\u89c6\u9891\u6df1\u5ea6\u6269\u6563\u5904\u7406\u52a8\u6001\u533a\u57df\u3002\u901a\u8fc7\u9891\u57df\u5206\u6790\u8bc1\u660e\u4e24\u8005\u7684\u4e92\u8865\u6027\u3002", "result": "\u5728\u96f6\u6837\u672c\u3001\u771f\u5b9e\u4e16\u754c\u52a8\u6001\u89c6\u9891\u6df1\u5ea6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cStereoDiff\u8868\u73b0\u6700\u4f18\uff0c\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u663e\u8457\u63d0\u5347\u3002", "conclusion": "StereoDiff\u901a\u8fc7\u7ed3\u5408\u7acb\u4f53\u5339\u914d\u548c\u89c6\u9891\u6df1\u5ea6\u6269\u6563\uff0c\u5b9e\u73b0\u4e86\u89c6\u9891\u6df1\u5ea6\u4f30\u8ba1\u7684SoTA\u6027\u80fd\u3002"}}
{"id": "2506.20832", "pdf": "https://arxiv.org/pdf/2506.20832", "abs": "https://arxiv.org/abs/2506.20832", "authors": ["Cansu Korkmaz", "Ahmet Murat Tekalp", "Zafer Dogan"], "title": "Leveraging Vision-Language Models to Select Trustworthy Super-Resolution Samples Generated by Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages, 9 figures, 5 tables, accepted to IEEE Transactions on   Circuits and Systems for Video Technology", "summary": "Super-resolution (SR) is an ill-posed inverse problem with many feasible solutions consistent with a given low-resolution image. On one hand, regressive SR models aim to balance fidelity and perceptual quality to yield a single solution, but this trade-off often introduces artifacts that create ambiguity in information-critical applications such as recognizing digits or letters. On the other hand, diffusion models generate a diverse set of SR images, but selecting the most trustworthy solution from this set remains a challenge. This paper introduces a robust, automated framework for identifying the most trustworthy SR sample from a diffusion-generated set by leveraging the semantic reasoning capabilities of vision-language models (VLMs). Specifically, VLMs such as BLIP-2, GPT-4o, and their variants are prompted with structured queries to assess semantic correctness, visual quality, and artifact presence. The top-ranked SR candidates are then ensembled to yield a single trustworthy output in a cost-effective manner. To rigorously assess the validity of VLM-selected samples, we propose a novel Trustworthiness Score (TWS) a hybrid metric that quantifies SR reliability based on three complementary components: semantic similarity via CLIP embeddings, structural integrity using SSIM on edge maps, and artifact sensitivity through multi-level wavelet decomposition. We empirically show that TWS correlates strongly with human preference in both ambiguous and natural images, and that VLM-guided selections consistently yield high TWS values. Compared to conventional metrics like PSNR, LPIPS, which fail to reflect information fidelity, our approach offers a principled, scalable, and generalizable solution for navigating the uncertainty of the diffusion SR space. By aligning outputs with human expectations and semantic correctness, this work sets a new benchmark for trustworthiness in generative SR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4ece\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u9009\u62e9\u6700\u53ef\u4fe1\u6837\u672c\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u4e00\u79cd\u65b0\u7684\u53ef\u4fe1\u5ea6\u8bc4\u5206\uff08TWS\uff09\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "motivation": "\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u95ee\u9898\u5b58\u5728\u591a\u4e2a\u53ef\u884c\u89e3\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u5e73\u8861\u4fdd\u771f\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\u65f6\u53ef\u80fd\u5f15\u5165\u4f2a\u5f71\uff0c\u800c\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u6837\u89e3\u4f46\u96be\u4ee5\u9009\u62e9\u6700\u53ef\u4fe1\u7684\u6837\u672c\u3002", "method": "\u5229\u7528VLM\uff08\u5982BLIP-2\u3001GPT-4o\uff09\u901a\u8fc7\u7ed3\u6784\u5316\u67e5\u8be2\u8bc4\u4f30\u8bed\u4e49\u6b63\u786e\u6027\u3001\u89c6\u89c9\u8d28\u91cf\u548c\u4f2a\u5f71\u5b58\u5728\uff0c\u5e76\u8bbe\u8ba1TWS\u8bc4\u5206\uff08\u7ed3\u5408\u8bed\u4e49\u76f8\u4f3c\u6027\u3001\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u4f2a\u5f71\u654f\u611f\u6027\uff09\u9a8c\u8bc1\u9009\u62e9\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTWS\u4e0e\u4eba\u7c7b\u504f\u597d\u9ad8\u5ea6\u76f8\u5173\uff0c\u4e14VLM\u5f15\u5bfc\u7684\u9009\u62e9\u80fd\u6301\u7eed\u83b7\u5f97\u9ad8TWS\u503c\uff0c\u4f18\u4e8e\u4f20\u7edf\u6307\u6807\u5982PSNR\u548cLPIPS\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6269\u6563SR\u7a7a\u95f4\u7684\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u751f\u6210SR\u7684\u53ef\u4fe1\u5ea6\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2506.20879", "pdf": "https://arxiv.org/pdf/2506.20879", "abs": "https://arxiv.org/abs/2506.20879", "authors": ["Shubhankar Borse", "Seokeon Choi", "Sunghyun Park", "Jeongho Kim", "Shreya Kadambi", "Risheek Garrepalli", "Sungrack Yun", "Munawar Hayat", "Fatih Porikli"], "title": "MultiHuman-Testbench: Benchmarking Image Generation for Multiple Humans", "categories": ["cs.CV"], "comment": null, "summary": "Generation of images containing multiple humans, performing complex actions, while preserving their facial identities, is a significant challenge. A major factor contributing to this is the lack of a a dedicated benchmark. To address this, we introduce MultiHuman-Testbench, a novel benchmark for rigorously evaluating generative models for multi-human generation. The benchmark comprises 1800 samples, including carefully curated text prompts, describing a range of simple to complex human actions. These prompts are matched with a total of 5,550 unique human face images, sampled uniformly to ensure diversity across age, ethnic background, and gender. Alongside captions, we provide human-selected pose conditioning images which accurately match the prompt. We propose a multi-faceted evaluation suite employing four key metrics to quantify face count, ID similarity, prompt alignment, and action detection. We conduct a thorough evaluation of a diverse set of models, including zero-shot approaches and training-based methods, with and without regional priors. We also propose novel techniques to incorporate image and region isolation using human segmentation and Hungarian matching, significantly improving ID similarity. Our proposed benchmark and key findings provide valuable insights and a standardized tool for advancing research in multi-human image generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MultiHuman-Testbench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u4eba\u751f\u6210\u6a21\u578b\uff0c\u5305\u542b1800\u4e2a\u6837\u672c\u548c5550\u5f20\u4eba\u8138\u56fe\u50cf\uff0c\u63d0\u51fa\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u6539\u8fdb\u4e86ID\u76f8\u4f3c\u6027\u6280\u672f\u3002", "motivation": "\u591a\u4eba\u751f\u6210\u56fe\u50cf\u65f6\u4fdd\u6301\u9762\u90e8\u8eab\u4efd\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u7f3a\u4e4f\u4e13\u7528\u57fa\u51c6\u3002", "method": "\u5f15\u5165MultiHuman-Testbench\u57fa\u51c6\uff0c\u5305\u542b\u591a\u6837\u5316\u7684\u6587\u672c\u63d0\u793a\u548c\u4eba\u8138\u56fe\u50cf\uff0c\u63d0\u51fa\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u6539\u8fdbID\u76f8\u4f3c\u6027\u6280\u672f\u3002", "result": "\u57fa\u51c6\u548c\u8bc4\u4f30\u65b9\u6cd5\u4e3a\u591a\u4eba\u751f\u6210\u56fe\u50cf\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u5de5\u5177\u548c\u91cd\u8981\u89c1\u89e3\u3002", "conclusion": "MultiHuman-Testbench\u4e3a\u591a\u4eba\u751f\u6210\u56fe\u50cf\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u548c\u6280\u672f\u6539\u8fdb\u3002"}}
{"id": "2506.20922", "pdf": "https://arxiv.org/pdf/2506.20922", "abs": "https://arxiv.org/abs/2506.20922", "authors": ["Ju-Hyeon Nam", "Dong-Hyun Moon", "Sang-Chul Lee"], "title": "M2SFormer: Multi-Spectral and Multi-Scale Attention with Edge-Aware Difficulty Guidance for Image Forgery Localization", "categories": ["cs.CV"], "comment": "Accepted in International Conference on Computer Vision (ICCV) 2025", "summary": "Image editing techniques have rapidly advanced, facilitating both innovative use cases and malicious manipulation of digital images. Deep learning-based methods have recently achieved high accuracy in pixel-level forgery localization, yet they frequently struggle with computational overhead and limited representation power, particularly for subtle or complex tampering. In this paper, we propose M2SFormer, a novel Transformer encoder-based framework designed to overcome these challenges. Unlike approaches that process spatial and frequency cues separately, M2SFormer unifies multi-frequency and multi-scale attentions in the skip connection, harnessing global context to better capture diverse forgery artifacts. Additionally, our framework addresses the loss of fine detail during upsampling by utilizing a global prior map, a curvature metric indicating the difficulty of forgery localization, which then guides a difficulty-guided attention module to preserve subtle manipulations more effectively. Extensive experiments on multiple benchmark datasets demonstrate that M2SFormer outperforms existing state-of-the-art models, offering superior generalization in detecting and localizing forgeries across unseen domains.", "AI": {"tldr": "M2SFormer\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u591a\u9891\u7387\u548c\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408\u5168\u5c40\u5148\u9a8c\u56fe\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u7be1\u6539\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u50cf\u7d20\u7ea7\u7be1\u6539\u5b9a\u4f4d\u4e2d\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u548c\u8868\u5f81\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5bf9\u590d\u6742\u6216\u7ec6\u5fae\u7be1\u6539\u7684\u5904\u7406\u6548\u679c\u4e0d\u4f73\u3002", "method": "M2SFormer\u5728\u8df3\u8dc3\u8fde\u63a5\u4e2d\u7edf\u4e00\u4e86\u591a\u9891\u7387\u548c\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u5229\u7528\u5168\u5c40\u5148\u9a8c\u56fe\u6307\u5bfc\u96be\u5ea6\u611f\u77e5\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u4ee5\u4fdd\u7559\u7ec6\u5fae\u7be1\u6539\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cM2SFormer\u5728\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7be1\u6539\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "M2SFormer\u901a\u8fc7\u521b\u65b0\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u5168\u5c40\u5148\u9a8c\u56fe\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7be1\u6539\u5b9a\u4f4d\u7684\u6027\u80fd\uff0c\u4e3a\u56fe\u50cf\u7f16\u8f91\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.20967", "pdf": "https://arxiv.org/pdf/2506.20967", "abs": "https://arxiv.org/abs/2506.20967", "authors": ["Lingling Cai", "Kang Zhao", "Hangjie Yuan", "Xiang Wang", "Yingya Zhang", "Kejie Huang"], "title": "DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing", "categories": ["cs.CV", "cs.AI"], "comment": "Zero-shot video editing", "summary": "The advent of Video Diffusion Transformers (Video DiTs) marks a milestone in video generation. However, directly applying existing video editing methods to Video DiTs often incurs substantial computational overhead, due to resource-intensive attention modification or finetuning. To alleviate this problem, we present DFVEdit, an efficient zero-shot video editing method tailored for Video DiTs. DFVEdit eliminates the need for both attention modification and fine-tuning by directly operating on clean latents via flow transformation. To be more specific, we observe that editing and sampling can be unified under the continuous flow perspective. Building upon this foundation, we propose the Conditional Delta Flow Vector (CDFV) -- a theoretically unbiased estimation of DFV -- and integrate Implicit Cross Attention (ICA) guidance as well as Embedding Reinforcement (ER) to further enhance editing quality. DFVEdit excels in practical efficiency, offering at least 20x inference speed-up and 85\\% memory reduction on Video DiTs compared to attention-engineering-based editing methods. Extensive quantitative and qualitative experiments demonstrate that DFVEdit can be seamlessly applied to popular Video DiTs (e.g., CogVideoX and Wan2.1), attaining state-of-the-art performance on structural fidelity, spatial-temporal consistency, and editing quality.", "AI": {"tldr": "DFVEdit\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u96f6\u6837\u672c\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\uff0c\u4e13\u4e3a\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668\uff08Video DiTs\uff09\u8bbe\u8ba1\uff0c\u907f\u514d\u4e86\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u6ce8\u610f\u529b\u4fee\u6539\u6216\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8eVideo DiTs\u65f6\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u6d41\u53d8\u6362\u76f4\u63a5\u64cd\u4f5c\u5e72\u51c0\u6f5c\u5728\u7a7a\u95f4\uff0c\u63d0\u51fa\u6761\u4ef6\u589e\u91cf\u6d41\u5411\u91cf\uff08CDFV\uff09\u548c\u96c6\u6210\u9690\u5f0f\u4ea4\u53c9\u6ce8\u610f\u529b\uff08ICA\uff09\u6307\u5bfc\u4e0e\u5d4c\u5165\u5f3a\u5316\uff08ER\uff09\u3002", "result": "DFVEdit\u5728\u63a8\u7406\u901f\u5ea6\u4e0a\u63d0\u534720\u500d\uff0c\u5185\u5b58\u51cf\u5c1185%\uff0c\u5e76\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "DFVEdit\u4e3aVideo DiTs\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u96f6\u6837\u672c\u89c6\u9891\u7f16\u8f91\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.20977", "pdf": "https://arxiv.org/pdf/2506.20977", "abs": "https://arxiv.org/abs/2506.20977", "authors": ["Tao Liu", "Dafeng Zhang", "Gengchen Li", "Shizhuo Liu", "Yongqi Song", "Senmao Li", "Shiqi Yang", "Boqian Li", "Kai Wang", "Yaxing Wang"], "title": "From Cradle to Cane: A Two-Pass Framework for High-Fidelity Lifespan Face Aging", "categories": ["cs.CV", "cs.AI"], "comment": "30 pages, 12 figures", "summary": "Face aging has become a crucial task in computer vision, with applications ranging from entertainment to healthcare. However, existing methods struggle with achieving a realistic and seamless transformation across the entire lifespan, especially when handling large age gaps or extreme head poses. The core challenge lies in balancing age accuracy and identity preservation--what we refer to as the Age-ID trade-off. Most prior methods either prioritize age transformation at the expense of identity consistency or vice versa. In this work, we address this issue by proposing a two-pass face aging framework, named Cradle2Cane, based on few-step text-to-image (T2I) diffusion models. The first pass focuses on solving age accuracy by introducing an adaptive noise injection (AdaNI) mechanism. This mechanism is guided by including prompt descriptions of age and gender for the given person as the textual condition. Also, by adjusting the noise level, we can control the strength of aging while allowing more flexibility in transforming the face. However, identity preservation is weakly ensured here to facilitate stronger age transformations. In the second pass, we enhance identity preservation while maintaining age-specific features by conditioning the model on two identity-aware embeddings (IDEmb): SVR-ArcFace and Rotate-CLIP. This pass allows for denoising the transformed image from the first pass, ensuring stronger identity preservation without compromising the aging accuracy. Both passes are jointly trained in an end-to-end way. Extensive experiments on the CelebA-HQ test dataset, evaluated through Face++ and Qwen-VL protocols, show that our Cradle2Cane outperforms existing face aging methods in age accuracy and identity consistency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCradle2Cane\u7684\u53cc\u9636\u6bb5\u4eba\u8138\u8001\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u566a\u58f0\u6ce8\u5165\u548c\u8eab\u4efd\u611f\u77e5\u5d4c\u5165\u89e3\u51b3\u4e86\u5e74\u9f84\u51c6\u786e\u6027\u4e0e\u8eab\u4efd\u4fdd\u6301\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u73b0\u771f\u5b9e\u4e14\u65e0\u7f1d\u7684\u4eba\u8138\u8001\u5316\u65f6\uff0c\u96be\u4ee5\u5e73\u8861\u5e74\u9f84\u51c6\u786e\u6027\u4e0e\u8eab\u4efd\u4fdd\u6301\uff0c\u5c24\u5176\u662f\u5728\u5927\u5e74\u9f84\u8de8\u5ea6\u6216\u6781\u7aef\u5934\u90e8\u59ff\u6001\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u53cc\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u81ea\u9002\u5e94\u566a\u58f0\u6ce8\u5165\uff08AdaNI\uff09\u89e3\u51b3\u5e74\u9f84\u51c6\u786e\u6027\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u8eab\u4efd\u611f\u77e5\u5d4c\u5165\uff08IDEmb\uff09\u589e\u5f3a\u8eab\u4efd\u4fdd\u6301\u3002", "result": "\u5728CelebA-HQ\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\uff0cCradle2Cane\u5728\u5e74\u9f84\u51c6\u786e\u6027\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Cradle2Cane\u901a\u8fc7\u53cc\u9636\u6bb5\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u5e74\u9f84\u4e0e\u8eab\u4efd\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u4eba\u8138\u8001\u5316\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.20983", "pdf": "https://arxiv.org/pdf/2506.20983", "abs": "https://arxiv.org/abs/2506.20983", "authors": ["Wenjie Xuan", "Jing Zhang", "Juhua Liu", "Bo Du", "Dacheng Tao"], "title": "Rethink Sparse Signals for Pose-guided Text-to-image Generation", "categories": ["cs.CV"], "comment": "accepted by ICCV 2025", "summary": "Recent works favored dense signals (e.g., depth, DensePose), as an alternative to sparse signals (e.g., OpenPose), to provide detailed spatial guidance for pose-guided text-to-image generation. However, dense representations raised new challenges, including editing difficulties and potential inconsistencies with textual prompts. This fact motivates us to revisit sparse signals for pose guidance, owing to their simplicity and shape-agnostic nature, which remains underexplored. This paper proposes a novel Spatial-Pose ControlNet(SP-Ctrl), equipping sparse signals with robust controllability for pose-guided image generation. Specifically, we extend OpenPose to a learnable spatial representation, making keypoint embeddings discriminative and expressive. Additionally, we introduce keypoint concept learning, which encourages keypoint tokens to attend to the spatial positions of each keypoint, thus improving pose alignment. Experiments on animal- and human-centric image generation tasks demonstrate that our method outperforms recent spatially controllable T2I generation approaches under sparse-pose guidance and even matches the performance of dense signal-based methods. Moreover, SP-Ctrl shows promising capabilities in diverse and cross-species generation through sparse signals. Codes will be available at https://github.com/DREAMXFAR/SP-Ctrl.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u4fe1\u53f7\uff08\u5982OpenPose\uff09\u7684\u65b0\u578b\u7a7a\u95f4\u59ff\u6001\u63a7\u5236\u7f51\u7edc\uff08SP-Ctrl\uff09\uff0c\u7528\u4e8e\u59ff\u6001\u5f15\u5bfc\u7684\u56fe\u50cf\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u5bc6\u96c6\u4fe1\u53f7\uff08\u5982\u6df1\u5ea6\u3001DensePose\uff09\u5e26\u6765\u7684\u7f16\u8f91\u56f0\u96be\u548c\u4e0e\u6587\u672c\u63d0\u793a\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u5bc6\u96c6\u4fe1\u53f7\u5728\u59ff\u6001\u5f15\u5bfc\u7684\u56fe\u50cf\u751f\u6210\u4e2d\u867d\u7136\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u7a7a\u95f4\u6307\u5bfc\uff0c\u4f46\u5e26\u6765\u4e86\u7f16\u8f91\u56f0\u96be\u548c\u4e0e\u6587\u672c\u63d0\u793a\u4e0d\u4e00\u81f4\u7684\u6311\u6218\u3002\u7a00\u758f\u4fe1\u53f7\u56e0\u5176\u7b80\u5355\u6027\u548c\u5f62\u72b6\u65e0\u5173\u6027\u88ab\u91cd\u65b0\u63a2\u7d22\u3002", "method": "\u6269\u5c55OpenPose\u4e3a\u53ef\u5b66\u4e60\u7684\u7a7a\u95f4\u8868\u793a\uff0c\u5f15\u5165\u5173\u952e\u70b9\u6982\u5ff5\u5b66\u4e60\uff0c\u4f7f\u5173\u952e\u70b9\u5d4c\u5165\u66f4\u5177\u533a\u5206\u6027\u548c\u8868\u8fbe\u529b\uff0c\u5e76\u63d0\u9ad8\u59ff\u6001\u5bf9\u9f50\u3002", "result": "\u5728\u4eba\u7c7b\u548c\u52a8\u7269\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0cSP-Ctrl\u5728\u7a00\u758f\u59ff\u6001\u5f15\u5bfc\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751a\u81f3\u4e0e\u5bc6\u96c6\u4fe1\u53f7\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u7269\u79cd\u751f\u6210\u7684\u6f5c\u529b\u3002", "conclusion": "\u7a00\u758f\u4fe1\u53f7\u5728\u59ff\u6001\u5f15\u5bfc\u7684\u56fe\u50cf\u751f\u6210\u4e2d\u5177\u6709\u6f5c\u529b\uff0cSP-Ctrl\u901a\u8fc7\u6539\u8fdb\u7684\u7a00\u758f\u4fe1\u53f7\u63a7\u5236\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u4e00\u81f4\u7684\u751f\u6210\u3002"}}
{"id": "2506.20998", "pdf": "https://arxiv.org/pdf/2506.20998", "abs": "https://arxiv.org/abs/2506.20998", "authors": ["Yeon-Ji Song", "Jaein Kim", "Byung-Ju Kim", "Byoung-Tak Zhang"], "title": "DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting", "categories": ["cs.CV"], "comment": "CVPRW 2025, Neural Fields Beyond Conventional Cameras", "summary": "Novel view synthesis is a task of generating scenes from unseen perspectives; however, synthesizing dynamic scenes from blurry monocular videos remains an unresolved challenge that has yet to be effectively addressed. Existing novel view synthesis methods are often constrained by their reliance on high-resolution images or strong assumptions about static geometry and rigid scene priors. Consequently, their approaches lack robustness in real-world environments with dynamic object and camera motion, leading to instability and degraded visual fidelity. To address this, we propose Motion-aware Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting (DBMovi-GS), a method designed for dynamic view synthesis from blurry monocular videos. Our model generates dense 3D Gaussians, restoring sharpness from blurry videos and reconstructing detailed 3D geometry of the scene affected by dynamic motion variations. Our model achieves robust performance in novel view synthesis under dynamic blurry scenes and sets a new benchmark in realistic novel view synthesis for blurry monocular video inputs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDBMovi-GS\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u6a21\u7cca\u5355\u76ee\u89c6\u9891\u4e2d\u5408\u6210\u52a8\u6001\u573a\u666f\u7684\u65b0\u89c6\u89d2\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u548c\u6a21\u7cca\u573a\u666f\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u6216\u9759\u6001\u51e0\u4f55\u5047\u8bbe\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u52a8\u6001\u548c\u6a21\u7cca\u573a\u666f\u3002", "method": "\u901a\u8fc7\u7a00\u758f\u63a7\u5236\u7684\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u751f\u6210\u5bc6\u96c63D\u9ad8\u65af\uff0c\u4ece\u6a21\u7cca\u89c6\u9891\u4e2d\u6062\u590d\u6e05\u6670\u5ea6\u5e76\u91cd\u5efa\u52a8\u6001\u573a\u666f\u76843D\u51e0\u4f55\u3002", "result": "\u6a21\u578b\u5728\u52a8\u6001\u6a21\u7cca\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u65b0\u89c6\u89d2\u5408\u6210\u6027\u80fd\uff0c\u5e76\u8bbe\u5b9a\u4e86\u65b0\u7684\u57fa\u51c6\u3002", "conclusion": "DBMovi-GS\u4e3a\u6a21\u7cca\u5355\u76ee\u89c6\u9891\u8f93\u5165\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u52a8\u6001\u65b0\u89c6\u89d2\u5408\u6210\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21006", "pdf": "https://arxiv.org/pdf/2506.21006", "abs": "https://arxiv.org/abs/2506.21006", "authors": ["Tyler Ward", "Xiaoqin Wang", "Braxton McFarland", "Md Atik Ahamed", "Sahar Nozad", "Talal Arshad", "Hafsa Nebbache", "Jin Chen", "Abdullah Imran"], "title": "Detection of Breast Cancer Lumpectomy Margin with SAM-incorporated Forward-Forward Contrastive Learning", "categories": ["cs.CV"], "comment": "19 pages, 7 figures, 3 tables", "summary": "Complete removal of cancer tumors with a negative specimen margin during lumpectomy is essential in reducing breast cancer recurrence. However, 2D specimen radiography (SR), the current method used to assess intraoperative specimen margin status, has limited accuracy, resulting in nearly a quarter of patients requiring additional surgery. To address this, we propose a novel deep learning framework combining the Segment Anything Model (SAM) with Forward-Forward Contrastive Learning (FFCL), a pre-training strategy leveraging both local and global contrastive learning for patch-level classification of SR images. After annotating SR images with regions of known maligancy, non-malignant tissue, and pathology-confirmed margins, we pre-train a ResNet-18 backbone with FFCL to classify margin status, then reconstruct coarse binary masks to prompt SAM for refined tumor margin segmentation. Our approach achieved an AUC of 0.8455 for margin classification and segmented margins with a 27.4% improvement in Dice similarity over baseline models, while reducing inference time to 47 milliseconds per image. These results demonstrate that FFCL-SAM significantly enhances both the speed and accuracy of intraoperative margin assessment, with strong potential to reduce re-excision rates and improve surgical outcomes in breast cancer treatment. Our code is available at https://github.com/tbwa233/FFCL-SAM/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Segment Anything Model (SAM)\u548cForward-Forward Contrastive Learning (FFCL)\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u4e73\u817a\u764c\u672f\u4e2d\u6807\u672c\u8fb9\u7f18\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u76842D\u6807\u672c\u653e\u5c04\u6210\u50cf\uff08SR\uff09\u5728\u8bc4\u4f30\u672f\u4e2d\u6807\u672c\u8fb9\u7f18\u72b6\u6001\u65f6\u51c6\u786e\u6027\u6709\u9650\uff0c\u5bfc\u81f4\u8fd1\u56db\u5206\u4e4b\u4e00\u7684\u60a3\u8005\u9700\u8981\u989d\u5916\u624b\u672f\u3002", "method": "\u7ed3\u5408FFCL\u9884\u8bad\u7ec3\u7b56\u7565\u548cSAM\u6a21\u578b\uff0c\u901a\u8fc7\u6807\u6ce8SR\u56fe\u50cf\u4e2d\u7684\u6076\u6027\u3001\u975e\u6076\u6027\u7ec4\u7ec7\u548c\u75c5\u7406\u786e\u8ba4\u7684\u8fb9\u7f18\u533a\u57df\uff0c\u9884\u8bad\u7ec3ResNet-18\u9aa8\u5e72\u7f51\u7edc\uff0c\u5e76\u5229\u7528SAM\u8fdb\u884c\u7cbe\u7ec6\u8fb9\u7f18\u5206\u5272\u3002", "result": "AUC\u8fbe\u52300.8455\uff0cDice\u76f8\u4f3c\u5ea6\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u9ad827.4%\uff0c\u63a8\u7406\u65f6\u95f4\u7f29\u77ed\u81f347\u6beb\u79d2/\u56fe\u50cf\u3002", "conclusion": "FFCL-SAM\u663e\u8457\u63d0\u9ad8\u4e86\u672f\u4e2d\u8fb9\u7f18\u8bc4\u4f30\u7684\u901f\u5ea6\u548c\u51c6\u786e\u6027\uff0c\u6709\u671b\u964d\u4f4e\u4e73\u817a\u764c\u6cbb\u7597\u4e2d\u7684\u518d\u5207\u9664\u7387\u3002"}}
{"id": "2506.21009", "pdf": "https://arxiv.org/pdf/2506.21009", "abs": "https://arxiv.org/abs/2506.21009", "authors": ["Ayaka Yasunaga", "Hideo Saito", "Shohei Mori"], "title": "User-in-the-Loop View Sampling with Error Peaking Visualization", "categories": ["cs.CV"], "comment": "Accepted at IEEE ICIP 2025, Project Page:   https://mediated-reality.github.io/projects/yasunaga_icip25/", "summary": "Augmented reality (AR) provides ways to visualize missing view samples for novel view synthesis. Existing approaches present 3D annotations for new view samples and task users with taking images by aligning the AR display. This data collection task is known to be mentally demanding and limits capture areas to pre-defined small areas due to the ideal but restrictive underlying sampling theory. To free users from 3D annotations and limited scene exploration, we propose using locally reconstructed light fields and visualizing errors to be removed by inserting new views. Our results show that the error-peaking visualization is less invasive, reduces disappointment in final results, and is satisfactory with fewer view samples in our mobile view synthesis system. We also show that our approach can contribute to recent radiance field reconstruction for larger scenes, such as 3D Gaussian splatting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c40\u90e8\u91cd\u5efa\u5149\u573a\u548c\u53ef\u89c6\u5316\u8bef\u5dee\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u589e\u5f3a\u73b0\u5b9e\u4e2d\u7684\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u51cf\u5c11\u7528\u6237\u8d1f\u62c5\u5e76\u6269\u5c55\u573a\u666f\u63a2\u7d22\u8303\u56f4\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u7528\u6237\u901a\u8fc7AR\u663e\u793a\u5bf9\u9f503D\u6807\u6ce8\uff0c\u4efb\u52a1\u7e41\u91cd\u4e14\u573a\u666f\u53d7\u9650\uff0c\u56e0\u6b64\u63d0\u51fa\u66f4\u81ea\u7531\u3001\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u5c40\u90e8\u91cd\u5efa\u5149\u573a\u548c\u53ef\u89c6\u5316\u8bef\u5dee\uff0c\u6307\u5bfc\u7528\u6237\u63d2\u5165\u65b0\u89c6\u89d2\u4ee5\u6d88\u9664\u8bef\u5dee\u3002", "result": "\u8bef\u5dee\u5cf0\u503c\u53ef\u89c6\u5316\u4fb5\u5165\u6027\u66f4\u4f4e\uff0c\u51cf\u5c11\u7528\u6237\u5931\u671b\uff0c\u4e14\u6240\u9700\u89c6\u89d2\u6837\u672c\u66f4\u5c11\uff0c\u9002\u7528\u4e8e\u5927\u573a\u666f\u91cd\u5efa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u79fb\u52a8\u89c6\u89d2\u5408\u6210\u7cfb\u7edf\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u652f\u6301\u66f4\u5927\u573a\u666f\u7684\u8f90\u5c04\u573a\u91cd\u5efa\u3002"}}
{"id": "2506.21015", "pdf": "https://arxiv.org/pdf/2506.21015", "abs": "https://arxiv.org/abs/2506.21015", "authors": ["Qingyue Jiao", "Kangyu Zheng", "Yiyu Shi", "Zhiding Liang"], "title": "HybridQ: Hybrid Classical-Quantum Generative Adversarial Network for Skin Disease Image Generation", "categories": ["cs.CV", "cs.LG", "quant-ph"], "comment": null, "summary": "Machine learning-assisted diagnosis is gaining traction in skin disease detection, but training effective models requires large amounts of high-quality data. Skin disease datasets often suffer from class imbalance, privacy concerns, and object bias, making data augmentation essential. While classical generative models are widely used, they demand extensive computational resources and lengthy training time. Quantum computing offers a promising alternative, but existing quantum-based image generation methods can only yield grayscale low-quality images. Through a novel classical-quantum latent space fusion technique, our work overcomes this limitation and introduces the first classical-quantum generative adversarial network (GAN) capable of generating color medical images. Our model outperforms classical deep convolutional GANs and existing hybrid classical-quantum GANs in both image generation quality and classification performance boost when used as data augmentation. Moreover, the performance boost is comparable with that achieved using state-of-the-art classical generative models, yet with over 25 times fewer parameters and 10 times fewer training epochs. Such results suggest a promising future for quantum image generation as quantum hardware advances. Finally, we demonstrate the robust performance of our model on real IBM quantum machine with hardware noise.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ecf\u5178-\u91cf\u5b50\u6df7\u5408\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\uff0c\u80fd\u591f\u751f\u6210\u5f69\u8272\u533b\u5b66\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u91cf\u5b50\u56fe\u50cf\u751f\u6210\u7684\u8d28\u91cf\u95ee\u9898\uff0c\u5e76\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u76ae\u80a4\u75be\u75c5\u6570\u636e\u96c6\u5b58\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u9690\u79c1\u95ee\u9898\u548c\u5bf9\u8c61\u504f\u5dee\u7b49\u95ee\u9898\uff0c\u6570\u636e\u589e\u5f3a\u662f\u5173\u952e\u3002\u4f20\u7edf\u751f\u6210\u6a21\u578b\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\uff0c\u800c\u73b0\u6709\u91cf\u5b50\u65b9\u6cd5\u53ea\u80fd\u751f\u6210\u4f4e\u8d28\u91cf\u7070\u5ea6\u56fe\u50cf\u3002", "method": "\u91c7\u7528\u7ecf\u5178-\u91cf\u5b50\u6f5c\u5728\u7a7a\u95f4\u878d\u5408\u6280\u672f\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u6df7\u5408\u7ecf\u5178-\u91cf\u5b50GAN\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u5f69\u8272\u533b\u5b66\u56fe\u50cf\u3002", "result": "\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u548c\u5206\u7c7b\u6027\u80fd\u63d0\u5347\u4e0a\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5377\u79efGAN\u548c\u73b0\u6709\u6df7\u5408\u91cf\u5b50GAN\uff0c\u4e14\u53c2\u6570\u548c\u8bad\u7ec3\u5468\u671f\u5927\u5e45\u51cf\u5c11\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u91cf\u5b50\u56fe\u50cf\u751f\u6210\u7684\u6f5c\u529b\uff0c\u968f\u7740\u91cf\u5b50\u786c\u4ef6\u7684\u53d1\u5c55\uff0c\u672a\u6765\u524d\u666f\u5e7f\u9614\u3002"}}
{"id": "2506.21022", "pdf": "https://arxiv.org/pdf/2506.21022", "abs": "https://arxiv.org/abs/2506.21022", "authors": ["Ze Wang", "Hao Chen", "Benran Hu", "Jiang Liu", "Ximeng Sun", "Jialian Wu", "Yusheng Su", "Xiaodong Yu", "Emad Barsoum", "Zicheng Liu"], "title": "Instella-T2I: Pushing the Limits of 1D Discrete Latent Space Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Image tokenization plays a critical role in reducing the computational demands of modeling high-resolution images, significantly improving the efficiency of image and multimodal understanding and generation. Recent advances in 1D latent spaces have reduced the number of tokens required by eliminating the need for a 2D grid structure. In this paper, we further advance compact discrete image representation by introducing 1D binary image latents. By representing each image as a sequence of binary vectors, rather than using traditional one-hot codebook tokens, our approach preserves high-resolution details while maintaining the compactness of 1D latents. To the best of our knowledge, our text-to-image models are the first to achieve competitive performance in both diffusion and auto-regressive generation using just 128 discrete tokens for images up to 1024x1024, demonstrating up to a 32-fold reduction in token numbers compared to standard VQ-VAEs. The proposed 1D binary latent space, coupled with simple model architectures, achieves marked improvements in speed training and inference speed. Our text-to-image models allow for a global batch size of 4096 on a single GPU node with 8 AMD MI300X GPUs, and the training can be completed within 200 GPU days. Our models achieve competitive performance compared to modern image generation models without any in-house private training data or post-training refinements, offering a scalable and efficient alternative to conventional tokenization methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e1D\u4e8c\u8fdb\u5236\u6f5c\u5728\u7a7a\u95f4\u7684\u56fe\u50cf\u6807\u8bb0\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5efa\u6a21\u6240\u9700\u7684\u6807\u8bb0\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u7ec6\u8282\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf2D\u7f51\u683c\u7ed3\u6784\u7684\u56fe\u50cf\u6807\u8bb0\u5316\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\uff0c\u800c1D\u6f5c\u5728\u7a7a\u95f4\u867d\u51cf\u5c11\u4e86\u6807\u8bb0\u6570\u91cf\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc71D\u4e8c\u8fdb\u5236\u6f5c\u5728\u7a7a\u95f4\u5b9e\u73b0\u66f4\u7d27\u51d1\u7684\u56fe\u50cf\u8868\u793a\u3002", "method": "\u91c7\u75281D\u4e8c\u8fdb\u5236\u5411\u91cf\u5e8f\u5217\u8868\u793a\u56fe\u50cf\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u4e00\u70ed\u7f16\u7801\u6807\u8bb0\uff0c\u7ed3\u5408\u7b80\u5355\u6a21\u578b\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u3002", "result": "\u57281024x1024\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u4ec5\u9700128\u4e2a\u6807\u8bb0\uff0c\u6bd4\u6807\u51c6VQ-VAEs\u51cf\u5c1132\u500d\u6807\u8bb0\u6570\u91cf\uff0c\u4e14\u6027\u80fd\u4e0e\u4e3b\u6d41\u56fe\u50cf\u751f\u6210\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "1D\u4e8c\u8fdb\u5236\u6f5c\u5728\u7a7a\u95f4\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u56fe\u50cf\u6807\u8bb0\u5316\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u8bad\u7ec3\u548c\u63a8\u7406\u3002"}}
{"id": "2506.21045", "pdf": "https://arxiv.org/pdf/2506.21045", "abs": "https://arxiv.org/abs/2506.21045", "authors": ["Hansam Cho", "Seoung Bum Kim"], "title": "Improving Diffusion-Based Image Editing Faithfulness via Guidance and Scheduling", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "preprint", "summary": "Text-guided diffusion models have become essential for high-quality image synthesis, enabling dynamic image editing. In image editing, two crucial aspects are editability, which determines the extent of modification, and faithfulness, which reflects how well unaltered elements are preserved. However, achieving optimal results is challenging because of the inherent trade-off between editability and faithfulness. To address this, we propose Faithfulness Guidance and Scheduling (FGS), which enhances faithfulness with minimal impact on editability. FGS incorporates faithfulness guidance to strengthen the preservation of input image information and introduces a scheduling strategy to resolve misalignment between editability and faithfulness. Experimental results demonstrate that FGS achieves superior faithfulness while maintaining editability. Moreover, its compatibility with various editing methods enables precise, high-quality image edits across diverse tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFGS\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5fe0\u5b9e\u5ea6\u6307\u5bfc\u548c\u8c03\u5ea6\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u53ef\u7f16\u8f91\u6027\u7684\u540c\u65f6\u63d0\u5347\u56fe\u50cf\u7f16\u8f91\u7684\u5fe0\u5b9e\u5ea6\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u7f16\u8f91\u4e2d\u5b58\u5728\u53ef\u7f16\u8f91\u6027\u4e0e\u5fe0\u5b9e\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u96be\u4ee5\u540c\u65f6\u4f18\u5316\u4e24\u8005\u3002", "method": "FGS\u7ed3\u5408\u4e86\u5fe0\u5b9e\u5ea6\u6307\u5bfc\u4ee5\u589e\u5f3a\u8f93\u5165\u56fe\u50cf\u4fe1\u606f\u7684\u4fdd\u7559\uff0c\u5e76\u91c7\u7528\u8c03\u5ea6\u7b56\u7565\u89e3\u51b3\u53ef\u7f16\u8f91\u6027\u4e0e\u5fe0\u5b9e\u5ea6\u7684\u4e0d\u5339\u914d\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFGS\u5728\u4fdd\u6301\u53ef\u7f16\u8f91\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u5fe0\u5b9e\u5ea6\uff0c\u4e14\u517c\u5bb9\u591a\u79cd\u7f16\u8f91\u65b9\u6cd5\u3002", "conclusion": "FGS\u4e3a\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u5e73\u8861\u53ef\u7f16\u8f91\u6027\u4e0e\u5fe0\u5b9e\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21091", "pdf": "https://arxiv.org/pdf/2506.21091", "abs": "https://arxiv.org/abs/2506.21091", "authors": ["Mahmoud Tahmasebi", "Saif Huq", "Kevin Meehan", "Marion McAfee"], "title": "ESMStereo: Enhanced ShuffleMixer Disparity Upsampling for Real-Time and Accurate Stereo Matching", "categories": ["cs.CV"], "comment": "Under peer review", "summary": "Stereo matching has become an increasingly important component of modern autonomous systems. Developing deep learning-based stereo matching models that deliver high accuracy while operating in real-time continues to be a major challenge in computer vision. In the domain of cost-volume-based stereo matching, accurate disparity estimation depends heavily on large-scale cost volumes. However, such large volumes store substantial redundant information and also require computationally intensive aggregation units for processing and regression, making real-time performance unattainable. Conversely, small-scale cost volumes followed by lightweight aggregation units provide a promising route for real-time performance, but lack sufficient information to ensure highly accurate disparity estimation. To address this challenge, we propose the Enhanced Shuffle Mixer (ESM) to mitigate information loss associated with small-scale cost volumes. ESM restores critical details by integrating primary features into the disparity upsampling unit. It quickly extracts features from the initial disparity estimation and fuses them with image features. These features are mixed by shuffling and layer splitting then refined through a compact feature-guided hourglass network to recover more detailed scene geometry. The ESM focuses on local contextual connectivity with a large receptive field and low computational cost, leading to the reconstruction of a highly accurate disparity map at real-time. The compact version of ESMStereo achieves an inference speed of 116 FPS on high-end GPUs and 91 FPS on the AGX Orin.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u578b\u6df7\u6d17\u6df7\u5408\u5668\uff08ESM\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u57fa\u4e8e\u5c0f\u89c4\u6a21\u6210\u672c\u4f53\u79ef\u7684\u7acb\u4f53\u5339\u914d\u4e2d\u4fe1\u606f\u4e22\u5931\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u7acb\u4f53\u5339\u914d\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u80fd\u3002\u5c0f\u89c4\u6a21\u6210\u672c\u4f53\u79ef\u867d\u80fd\u63d0\u5347\u901f\u5ea6\uff0c\u4f46\u4fe1\u606f\u4e0d\u8db3\u5bfc\u81f4\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "\u63d0\u51faESM\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u521d\u7ea7\u7279\u5f81\u6574\u5408\u5230\u89c6\u5dee\u4e0a\u91c7\u6837\u5355\u5143\u4e2d\uff0c\u5feb\u901f\u63d0\u53d6\u7279\u5f81\u5e76\u4e0e\u56fe\u50cf\u7279\u5f81\u878d\u5408\uff0c\u901a\u8fc7\u6df7\u6d17\u548c\u5206\u5c42\u5206\u5272\u6df7\u5408\u7279\u5f81\uff0c\u518d\u901a\u8fc7\u7d27\u51d1\u7684\u7279\u5f81\u5f15\u5bfc\u6c99\u6f0f\u7f51\u7edc\u7ec6\u5316\u3002", "result": "ESM\u5728\u9ad8\u7aefGPU\u4e0a\u8fbe\u5230116 FPS\uff0c\u5728AGX Orin\u4e0a\u8fbe\u523091 FPS\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\u7684\u89c6\u5dee\u56fe\u91cd\u5efa\u3002", "conclusion": "ESM\u901a\u8fc7\u5c40\u90e8\u4e0a\u4e0b\u6587\u8fde\u63a5\u548c\u5927\u611f\u53d7\u91ce\uff0c\u4ee5\u4f4e\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u5b9e\u65f6\u7acb\u4f53\u5339\u914d\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21117", "pdf": "https://arxiv.org/pdf/2506.21117", "abs": "https://arxiv.org/abs/2506.21117", "authors": ["Jan Ackermann", "Jonas Kulhanek", "Shengqu Cai", "Haofei Xu", "Marc Pollefeys", "Gordon Wetzstein", "Leonidas Guibas", "Songyou Peng"], "title": "CL-Splats: Continual Learning of Gaussian Splatting with Local Optimization", "categories": ["cs.CV"], "comment": "ICCV 2025, Project Page: https://cl-splats.github.io", "summary": "In dynamic 3D environments, accurately updating scene representations over time is crucial for applications in robotics, mixed reality, and embodied AI. As scenes evolve, efficient methods to incorporate changes are needed to maintain up-to-date, high-quality reconstructions without the computational overhead of re-optimizing the entire scene. This paper introduces CL-Splats, which incrementally updates Gaussian splatting-based 3D representations from sparse scene captures. CL-Splats integrates a robust change-detection module that segments updated and static components within the scene, enabling focused, local optimization that avoids unnecessary re-computation. Moreover, CL-Splats supports storing and recovering previous scene states, facilitating temporal segmentation and new scene-analysis applications. Our extensive experiments demonstrate that CL-Splats achieves efficient updates with improved reconstruction quality over the state-of-the-art. This establishes a robust foundation for future real-time adaptation in 3D scene reconstruction tasks.", "AI": {"tldr": "CL-Splats\u662f\u4e00\u79cd\u52a8\u60013D\u573a\u666f\u8868\u793a\u66f4\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u91cf\u66f4\u65b0\u9ad8\u65af\u70b9\u4e91\u8868\u793a\uff0c\u7ed3\u5408\u53d8\u5316\u68c0\u6d4b\u6a21\u5757\uff0c\u5b9e\u73b0\u9ad8\u6548\u5c40\u90e8\u4f18\u5316\uff0c\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u5728\u52a8\u60013D\u73af\u5883\u4e2d\uff0c\u5b9e\u65f6\u66f4\u65b0\u573a\u666f\u8868\u793a\u5bf9\u673a\u5668\u4eba\u3001\u6df7\u5408\u73b0\u5b9e\u548c\u5177\u8eabAI\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u91cd\u65b0\u4f18\u5316\u6574\u4e2a\u573a\u666f\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\u3002", "method": "\u63d0\u51faCL-Splats\uff0c\u901a\u8fc7\u7a00\u758f\u573a\u666f\u6355\u83b7\u589e\u91cf\u66f4\u65b0\u9ad8\u65af\u70b9\u4e91\u8868\u793a\uff0c\u96c6\u6210\u53d8\u5316\u68c0\u6d4b\u6a21\u5757\uff0c\u533a\u5206\u52a8\u6001\u4e0e\u9759\u6001\u90e8\u5206\uff0c\u5b9e\u73b0\u5c40\u90e8\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCL-Splats\u5728\u9ad8\u6548\u66f4\u65b0\u7684\u540c\u65f6\uff0c\u91cd\u5efa\u8d28\u91cf\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CL-Splats\u4e3a\u672a\u6765\u5b9e\u65f63D\u573a\u666f\u91cd\u5efa\u4efb\u52a1\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2506.21132", "pdf": "https://arxiv.org/pdf/2506.21132", "abs": "https://arxiv.org/abs/2506.21132", "authors": ["Hai Jiang", "Binhao Guan", "Zhen Liu", "Xiaohong Liu", "Jian Yu", "Zheng Liu", "Songchen Han", "Shuaicheng Liu"], "title": "Learning to See in the Extremely Dark", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Learning-based methods have made promising advances in low-light RAW image enhancement, while their capability to extremely dark scenes where the environmental illuminance drops as low as 0.0001 lux remains to be explored due to the lack of corresponding datasets. To this end, we propose a paired-to-paired data synthesis pipeline capable of generating well-calibrated extremely low-light RAW images at three precise illuminance ranges of 0.01-0.1 lux, 0.001-0.01 lux, and 0.0001-0.001 lux, together with high-quality sRGB references to comprise a large-scale paired dataset named See-in-the-Extremely-Dark (SIED) to benchmark low-light RAW image enhancement approaches. Furthermore, we propose a diffusion-based framework that leverages the generative ability and intrinsic denoising property of diffusion models to restore visually pleasing results from extremely low-SNR RAW inputs, in which an Adaptive Illumination Correction Module (AICM) and a color consistency loss are introduced to ensure accurate exposure correction and color restoration. Extensive experiments on the proposed SIED and publicly available benchmarks demonstrate the effectiveness of our method. The code and dataset are available at https://github.com/JianghaiSCU/SIED.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u6781\u4f4e\u5149RAW\u56fe\u50cf\u7684\u6570\u636e\u5408\u6210\u6d41\u7a0b\u548c\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u6781\u6697\u573a\u666f\u4e0b\u7684\u56fe\u50cf\u589e\u5f3a\uff0c\u5e76\u53d1\u5e03\u4e86SIED\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u5b66\u4e60\u578b\u65b9\u6cd5\u5728\u6781\u4f4e\u5149\uff080.0001 lux\uff09\u573a\u666f\u4e0b\u7684RAW\u56fe\u50cf\u589e\u5f3a\u80fd\u529b\u5c1a\u672a\u63a2\u7d22\uff0c\u7f3a\u4e4f\u5bf9\u5e94\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u4e86\u6570\u636e\u5408\u6210\u6d41\u7a0b\u751f\u6210\u6781\u4f4e\u5149RAW\u56fe\u50cf\u53casRGB\u53c2\u8003\uff0c\u5e76\u8bbe\u8ba1\u4e86\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u5305\u542b\u81ea\u9002\u5e94\u5149\u7167\u6821\u6b63\u6a21\u5757\u548c\u989c\u8272\u4e00\u81f4\u6027\u635f\u5931\u3002", "result": "\u5728SIED\u6570\u636e\u96c6\u548c\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8bba\u6587\u586b\u8865\u4e86\u6781\u4f4e\u5149RAW\u56fe\u50cf\u589e\u5f3a\u7684\u6570\u636e\u96c6\u7a7a\u767d\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6781\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2506.21152", "pdf": "https://arxiv.org/pdf/2506.21152", "abs": "https://arxiv.org/abs/2506.21152", "authors": ["Pufan Li", "Bi'an Du", "Wei Hu"], "title": "Geometry and Perception Guided Gaussians for Multiview-consistent 3D Generation from a Single Image", "categories": ["cs.CV", "68", "I.4.0"], "comment": "10 pages, 5 figures", "summary": "Generating realistic 3D objects from single-view images requires natural appearance, 3D consistency, and the ability to capture multiple plausible interpretations of unseen regions. Existing approaches often rely on fine-tuning pretrained 2D diffusion models or directly generating 3D information through fast network inference or 3D Gaussian Splatting, but their results generally suffer from poor multiview consistency and lack geometric detail. To takle these issues, we present a novel method that seamlessly integrates geometry and perception priors without requiring additional model training to reconstruct detailed 3D objects from a single image. Specifically, we train three different Gaussian branches initialized from the geometry prior, perception prior and Gaussian noise, respectively. The geometry prior captures the rough 3D shapes, while the perception prior utilizes the 2D pretrained diffusion model to enhance multiview information. Subsequently, we refine 3D Gaussian branches through mutual interaction between geometry and perception priors, further enhanced by a reprojection-based strategy that enforces depth consistency. Experiments demonstrate the higher-fidelity reconstruction results of our method, outperforming existing methods on novel view synthesis and 3D reconstruction, demonstrating robust and consistent 3D object generation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u51e0\u4f55\u548c\u611f\u77e5\u5148\u9a8c\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u4ece\u5355\u5f20\u56fe\u50cf\u91cd\u5efa\u9ad8\u4fdd\u771f3D\u7269\u4f53\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u51e0\u4f55\u7ec6\u8282\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u8bad\u7ec3\u4e09\u4e2a\u9ad8\u65af\u5206\u652f\uff08\u51e0\u4f55\u5148\u9a8c\u3001\u611f\u77e5\u5148\u9a8c\u548c\u9ad8\u65af\u566a\u58f0\uff09\uff0c\u901a\u8fc7\u4ea4\u4e92\u548c\u91cd\u6295\u5f71\u7b56\u7565\u4f18\u53163D\u9ad8\u65af\u5206\u652f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u65b9\u6cd5\u5728\u89c6\u89d2\u5408\u6210\u548c3D\u91cd\u5efa\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u7ed3\u679c\u66f4\u4e00\u81f4\u548c\u8be6\u7ec6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u51e0\u4f55\u7ec6\u8282\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u76843D\u7269\u4f53\u751f\u6210\u3002"}}
{"id": "2506.21209", "pdf": "https://arxiv.org/pdf/2506.21209", "abs": "https://arxiv.org/abs/2506.21209", "authors": ["Louis Kerner", "Michel Meintz", "Bihe Zhao", "Franziska Boenisch", "Adam Dziedzic"], "title": "BitMark for Infinity: Watermarking Bitwise Autoregressive Image Generative Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "State-of-the-art text-to-image models like Infinity generate photorealistic images at an unprecedented speed. These models operate in a bitwise autoregressive manner over a discrete set of tokens that is practically infinite in size. However, their impressive generative power comes with a growing risk: as their outputs increasingly populate the Internet, they are likely to be scraped and reused as training data-potentially by the very same models. This phenomenon has been shown to lead to model collapse, where repeated training on generated content, especially from the models' own previous versions, causes a gradual degradation in performance. A promising mitigation strategy is watermarking, which embeds human-imperceptible yet detectable signals into generated images-enabling the identification of generated content. In this work, we introduce BitMark, a robust bitwise watermarking framework for Infinity. Our method embeds a watermark directly at the bit level of the token stream across multiple scales (also referred to as resolutions) during Infinity's image generation process. Our bitwise watermark subtly influences the bits to preserve visual fidelity and generation speed while remaining robust against a spectrum of removal techniques. Furthermore, it exhibits high radioactivity, i.e., when watermarked generated images are used to train another image generative model, this second model's outputs will also carry the watermark. The radioactive traces remain detectable even when only fine-tuning diffusion or image autoregressive models on images watermarked with our BitMark. Overall, our approach provides a principled step toward preventing model collapse in image generative models by enabling reliable detection of generated outputs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faBitMark\uff0c\u4e00\u79cd\u9488\u5bf9Infinity\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u9c81\u68d2\u4f4d\u7ea7\u6c34\u5370\u6846\u67b6\uff0c\u65e8\u5728\u9632\u6b62\u6a21\u578b\u5d29\u6e83\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6a21\u578b\u8f93\u51fa\u5728\u4e92\u8054\u7f51\u4e0a\u7684\u5e7f\u6cdb\u4f20\u64ad\uff0c\u8fd9\u4e9b\u5185\u5bb9\u53ef\u80fd\u88ab\u91cd\u65b0\u7528\u4f5c\u8bad\u7ec3\u6570\u636e\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u9010\u6e10\u9000\u5316\uff08\u6a21\u578b\u5d29\u6e83\uff09\u3002\u6c34\u5370\u6280\u672f\u53ef\u4ee5\u8bc6\u522b\u751f\u6210\u5185\u5bb9\uff0c\u4ece\u800c\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "BitMark\u901a\u8fc7\u5728Infinity\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u5728\u591a\u4e2a\u5c3a\u5ea6\uff08\u5206\u8fa8\u7387\uff09\u7684\u4ee4\u724c\u6d41\u4e2d\u5d4c\u5165\u4f4d\u7ea7\u6c34\u5370\uff0c\u4fdd\u6301\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u751f\u6210\u901f\u5ea6\uff0c\u540c\u65f6\u62b5\u6297\u53bb\u9664\u6280\u672f\u3002", "result": "BitMark\u5177\u6709\u9ad8\u653e\u5c04\u6027\uff0c\u5373\u4f7f\u7528\u6c34\u5370\u751f\u6210\u56fe\u50cf\u8bad\u7ec3\u5176\u4ed6\u6a21\u578b\u65f6\uff0c\u65b0\u6a21\u578b\u7684\u8f93\u51fa\u4e5f\u4f1a\u643a\u5e26\u6c34\u5370\uff0c\u4e14\u6c34\u5370\u5728\u5fae\u8c03\u540e\u4ecd\u53ef\u68c0\u6d4b\u3002", "conclusion": "BitMark\u4e3a\u56fe\u50cf\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9632\u6b62\u6a21\u578b\u5d29\u6e83\u7684\u53ef\u9760\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u751f\u6210\u5185\u5bb9\u5b9e\u73b0\u3002"}}
{"id": "2506.21270", "pdf": "https://arxiv.org/pdf/2506.21270", "abs": "https://arxiv.org/abs/2506.21270", "authors": ["Cheng Zou", "Senlin Cheng", "Bolei Xu", "Dandan Zheng", "Xiaobo Li", "Jingdong Chen", "Ming Yang"], "title": "Video Virtual Try-on with Conditional Diffusion Transformer Inpainter", "categories": ["cs.CV"], "comment": "10 pages, 6 figures", "summary": "Video virtual try-on aims to naturally fit a garment to a target person in consecutive video frames. It is a challenging task, on the one hand, the output video should be in good spatial-temporal consistency, on the other hand, the details of the given garment need to be preserved well in all the frames. Naively using image-based try-on methods frame by frame can get poor results due to severe inconsistency. Recent diffusion-based video try-on methods, though very few, happen to coincide with a similar solution: inserting temporal attention into image-based try-on model to adapt it for video try-on task, which have shown improvements but there still exist inconsistency problems. In this paper, we propose ViTI (Video Try-on Inpainter), formulate and implement video virtual try-on as a conditional video inpainting task, which is different from previous methods. In this way, we start with a video generation problem instead of an image-based try-on problem, which from the beginning has a better spatial-temporal consistency. Specifically, at first we build a video inpainting framework based on Diffusion Transformer with full 3D spatial-temporal attention, and then we progressively adapt it for video garment inpainting, with a collection of masking strategies and multi-stage training. After these steps, the model can inpaint the masked garment area with appropriate garment pixels according to the prompt with good spatial-temporal consistency. Finally, as other try-on methods, garment condition is added to the model to make sure the inpainted garment appearance and details are as expected. Both quantitative and qualitative experimental results show that ViTI is superior to previous works.", "AI": {"tldr": "ViTI\u5c06\u89c6\u9891\u865a\u62df\u8bd5\u7a7f\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6761\u4ef6\u89c6\u9891\u4fee\u590d\u4efb\u52a1\uff0c\u901a\u8fc7\u57fa\u4e8eDiffusion Transformer\u76843D\u65f6\u7a7a\u6ce8\u610f\u529b\u6846\u67b6\u548c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u7a7a\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u4fdd\u7559\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\u5b58\u5728\u65f6\u7a7a\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u800c\u57fa\u4e8e\u56fe\u50cf\u7684\u9010\u5e27\u5904\u7406\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\u3002ViTI\u65e8\u5728\u901a\u8fc7\u89c6\u9891\u751f\u6210\u800c\u975e\u56fe\u50cf\u8bd5\u7a7f\u7684\u601d\u8def\uff0c\u4ece\u6e90\u5934\u89e3\u51b3\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faViTI\u6846\u67b6\uff0c\u57fa\u4e8eDiffusion Transformer\u6784\u5efa3D\u65f6\u7a7a\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u91c7\u7528\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u548c\u591a\u9636\u6bb5\u63a9\u7801\u7b56\u7565\uff0c\u9010\u6b65\u9002\u5e94\u89c6\u9891\u670d\u88c5\u4fee\u590d\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cViTI\u5728\u65f6\u7a7a\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u4fdd\u7559\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ViTI\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49\u4efb\u52a1\u4e3a\u89c6\u9891\u4fee\u590d\uff0c\u7ed3\u54083D\u65f6\u7a7a\u6ce8\u610f\u529b\u548c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u865a\u62df\u8bd5\u7a7f\u7684\u6548\u679c\u3002"}}
