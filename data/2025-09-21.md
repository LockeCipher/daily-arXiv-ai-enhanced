<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 24]
- [cs.LG](#cs.LG) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance](https://arxiv.org/abs/2509.15130)
*Chenxi Song,Yanming Yang,Tong Zhao,Ruibo Li,Chi Zhang*

Main category: cs.GR

TL;DR: WorldForge是一个无需训练的推理时框架，通过三个耦合模块实现精确的运动轨迹注入，解决了视频扩散模型在3D/4D任务中的可控性和几何一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型虽然具有丰富的空间先验知识，但受限于可控性和几何不一致性，难以直接应用于3D/4D任务。传统方法需要重新训练或微调，这会破坏预训练知识且计算成本高昂。

Method: 提出三个紧密耦合的模块：1）步内递归细化机制在推理时优化网络预测；2）流门控潜在融合利用光流相似性分离运动和外观；3）双路径自校正指导通过比较有指导和无指导路径来校正轨迹漂移。

Result: 在多个基准测试上的广泛实验验证了该方法在真实性、轨迹一致性和视觉保真度方面的优越性，能够实现精确的运动控制和逼真的内容生成。

Conclusion: WorldForge为可控视频合成引入了一种新颖的即插即用范式，为利用生成先验进行空间智能任务提供了新视角。

Abstract: Recent video diffusion models demonstrate strong potential in spatial intelligence tasks due to their rich latent world priors. However, this potential is hindered by their limited controllability and geometric inconsistency, creating a gap between their strong priors and their practical use in 3D/4D tasks. As a result, current approaches often rely on retraining or fine-tuning, which risks degrading pretrained knowledge and incurs high computational costs. To address this, we propose WorldForge, a training-free, inference-time framework composed of three tightly coupled modules. Intra-Step Recursive Refinement introduces a recursive refinement mechanism during inference, which repeatedly optimizes network predictions within each denoising step to enable precise trajectory injection. Flow-Gated Latent Fusion leverages optical flow similarity to decouple motion from appearance in the latent space and selectively inject trajectory guidance into motion-related channels. Dual-Path Self-Corrective Guidance compares guided and unguided denoising paths to adaptively correct trajectory drift caused by noisy or misaligned structural signals. Together, these components inject fine-grained, trajectory-aligned guidance without training, achieving both accurate motion control and photorealistic content generation. Extensive experiments across diverse benchmarks validate our method's superiority in realism, trajectory consistency, and visual fidelity. This work introduces a novel plug-and-play paradigm for controllable video synthesis, offering a new perspective on leveraging generative priors for spatial intelligence.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [AToken: A Unified Tokenizer for Vision](https://arxiv.org/abs/2509.14476)
*Jiasen Lu,Liangchen Song,Mingze Xu,Byeongjoo Ahn,Yanjun Wang,Chen Chen,Afshin Dehghan,Yinfei Yang*

Main category: cs.CV

TL;DR: AToken是首个统一的视觉分词器，能够在图像、视频和3D资产上同时实现高保真重建和语义理解，通过4D潜在空间统一多种模态和任务。


<details>
  <summary>Details</summary>
Motivation: 现有分词器通常专注于单一模态的重建或理解任务，缺乏跨模态的统一框架。AToken旨在解决这一局限性，为下一代多模态AI系统提供统一的视觉分词基础。

Method: 采用纯Transformer架构和4D旋转位置编码，处理任意分辨率和时长的视觉输入。使用无对抗训练目标结合感知和Gram矩阵损失，通过渐进式训练课程从单图像扩展到视频和3D，支持连续和离散潜在token。

Result: 在图像上达到0.21 rFID和82.2% ImageNet准确率；视频上达到3.01 rFVD和32.6% MSRVTT检索率；3D上达到28.19 PSNR和90.9%分类准确率。在下游任务中表现出色。

Conclusion: AToken展示了统一视觉分词在构建下一代多模态AI系统中的潜力，为视觉生成和理解任务提供了强大的基础框架。

Abstract: We present AToken, the first unified visual tokenizer that achieves both high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets. Unlike existing tokenizers that specialize in either reconstruction or understanding for single modalities, AToken encodes these diverse visual inputs into a shared 4D latent space, unifying both tasks and modalities in a single framework. Specifically, we introduce a pure transformer architecture with 4D rotary position embeddings to process visual inputs of arbitrary resolutions and temporal durations. To ensure stable training, we introduce an adversarial-free training objective that combines perceptual and Gram matrix losses, achieving state-of-the-art reconstruction quality. By employing a progressive training curriculum, AToken gradually expands from single images, videos, and 3D, and supports both continuous and discrete latent tokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01 rFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9% classification accuracy for 3D. In downstream applications, AToken enables both visual generation tasks (e.g., image generation with continuous and discrete tokens, text-to-video generation, image-to-3D synthesis) and understanding tasks (e.g., multimodal LLMs), achieving competitive performance across all benchmarks. These results shed light on the next-generation multimodal AI systems built upon unified visual tokenization.

</details>


### [3] [Edge-Aware Normalized Attention for Efficient and Detail-Preserving Single Image Super-Resolution](https://arxiv.org/abs/2509.14550)
*Penghao Rao,Tieyong Zeng*

Main category: cs.CV

TL;DR: 提出了一种边缘引导的注意力机制，通过自适应调制图来增强结构显著性区域并抑制虚假纹理，结合轻量级残差设计和复合损失函数，在保持模型复杂度的情况下提升了超分辨率的结构清晰度和感知质量。


<details>
  <summary>Details</summary>
Motivation: 现有的边缘感知方法往往将边缘先验或注意力分支附加到复杂的骨干网络上，但临时融合经常引入冗余、优化不稳定或结构增益有限的问题。需要一种更有效的方法来注入边缘先验并提升超分辨率的结构保真度。

Method: 提出了边缘引导的注意力机制，从联合编码的边缘特征和中间特征激活中推导自适应调制图，用于归一化和重新加权响应。将该机制集成到轻量级残差设计中，并使用结合像素级、感知和对抗项的复合目标函数进行训练。

Result: 在标准SISR基准测试中，相比SRGAN、ESRGAN和先前的边缘注意力基线方法，在可比较的模型复杂度下，结构清晰度和感知质量都有持续改进。

Conclusion: 该方法提供了参数高效的边缘先验注入路径，通过定制的多术语损失实现稳定的对抗细化，无需依赖更深或过度参数化的架构即可增强边缘保真度，证明了基于原则的边缘条件调制在推进感知超分辨率方面的有效性。

Abstract: Single-image super-resolution (SISR) remains highly ill-posed because recovering structurally faithful high-frequency content from a single low-resolution observation is ambiguous. Existing edge-aware methods often attach edge priors or attention branches onto increasingly complex backbones, yet ad hoc fusion frequently introduces redundancy, unstable optimization, or limited structural gains. We address this gap with an edge-guided attention mechanism that derives an adaptive modulation map from jointly encoded edge features and intermediate feature activations, then applies it to normalize and reweight responses, selectively amplifying structurally salient regions while suppressing spurious textures. In parallel, we integrate this mechanism into a lightweight residual design trained under a composite objective combining pixel-wise, perceptual, and adversarial terms to balance fidelity, perceptual realism, and training stability. Extensive experiments on standard SISR benchmarks demonstrate consistent improvements in structural sharpness and perceptual quality over SRGAN, ESRGAN, and prior edge-attention baselines at comparable model complexity. The proposed formulation provides (i) a parameter-efficient path to inject edge priors, (ii) stabilized adversarial refinement through a tailored multiterm loss, and (iii) enhanced edge fidelity without resorting to deeper or heavily overparameterized architectures. These results highlight the effectiveness of principled edge-conditioned modulation for advancing perceptual super-resolution.

</details>


### [4] [Adaptive and Iterative Point Cloud Denoising with Score-Based Diffusion Model](https://arxiv.org/abs/2509.14560)
*Zhaonan Wang,Manyi Li,ShiQing Xin,Changhe Tu*

Main category: cs.CV

TL;DR: 基于评分模型的适应性迭代点云去噪方法，通过噪声估计和适应步长调整，实现更好的去噪效果和细节保持


<details>
  <summary>Details</summary>
Motivation: 现有点云去噪方法通常需要经验性地重复迭代去噪过程，无法高效地处理不同水平或模式的噪声

Method: 使用评分模型框架，先估计噪声变异度并确定适应性去噪调度，然后迭代调用训练好的网络更新点云，设计了两阶段采样策略支持迭代去噪

Result: 在合成和真实扫描数据集上都获得了更优的质量和数量结果，能够产生清漏平滑的去噪点云，同时更好地保持形状边界和细节

Conclusion: 该方法通过适应性迭代去噪机制，在点云去噪任务中实现了更优的性能，特别是在处理不同模式噪声时显示出良好的适应性

Abstract: Point cloud denoising task aims to recover the clean point cloud from the scanned data coupled with different levels or patterns of noise. The recent state-of-the-art methods often train deep neural networks to update the point locations towards the clean point cloud, and empirically repeat the denoising process several times in order to obtain the denoised results. It is not clear how to efficiently arrange the iterative denoising processes to deal with different levels or patterns of noise. In this paper, we propose an adaptive and iterative point cloud denoising method based on the score-based diffusion model. For a given noisy point cloud, we first estimate the noise variation and determine an adaptive denoising schedule with appropriate step sizes, then invoke the trained network iteratively to update point clouds following the adaptive schedule. To facilitate this adaptive and iterative denoising process, we design the network architecture and a two-stage sampling strategy for the network training to enable feature fusion and gradient fusion for iterative denoising. Compared to the state-of-the-art point cloud denoising methods, our approach obtains clean and smooth denoised point clouds, while preserving the shape boundary and details better. Our results not only outperform the other methods both qualitatively and quantitatively, but also are preferable on the synthetic dataset with different patterns of noises, as well as the real-scanned dataset.

</details>


### [5] [DiffVL: Diffusion-Based Visual Localization on 2D Maps via BEV-Conditioned GPS Denoising](https://arxiv.org/abs/2509.14565)
*Li Gao,Hongyang Sun,Liu Liu,Yunhao Li,Yang Cai*

Main category: cs.CV

TL;DR: DiffVL是一个基于扩散模型的视觉定位框架，将视觉定位重新定义为GPS去噪任务，通过联合建模GPS、标准地图和视觉信号，实现亚米级精度而无需高精地图。


<details>
  <summary>Details</summary>
Motivation: 现有视觉定位方法面临高精地图成本高、标准地图方法忽略GPS噪声的问题。GPS在城市场景中存在多路径误差，但仍然是普遍可用的信号。

Method: 使用扩散模型，将噪声GPS轨迹与视觉BEV特征和标准地图结合，通过迭代扩散细化恢复真实位姿分布，逆向学习GPS噪声扰动。

Result: 在多个数据集上达到最先进精度，相比BEV匹配基线方法表现更优，实现亚米级定位精度。

Conclusion: 扩散模型能够通过将噪声GPS作为生成先验来实现可扩展的定位，这是对传统基于匹配方法的范式转变。

Abstract: Accurate visual localization is crucial for autonomous driving, yet existing methods face a fundamental dilemma: While high-definition (HD) maps provide high-precision localization references, their costly construction and maintenance hinder scalability, which drives research toward standard-definition (SD) maps like OpenStreetMap. Current SD-map-based approaches primarily focus on Bird's-Eye View (BEV) matching between images and maps, overlooking a ubiquitous signal-noisy GPS. Although GPS is readily available, it suffers from multipath errors in urban environments. We propose DiffVL, the first framework to reformulate visual localization as a GPS denoising task using diffusion models. Our key insight is that noisy GPS trajectory, when conditioned on visual BEV features and SD maps, implicitly encode the true pose distribution, which can be recovered through iterative diffusion refinement. DiffVL, unlike prior BEV-matching methods (e.g., OrienterNet) or transformer-based registration approaches, learns to reverse GPS noise perturbations by jointly modeling GPS, SD map, and visual signals, achieving sub-meter accuracy without relying on HD maps. Experiments on multiple datasets demonstrate that our method achieves state-of-the-art accuracy compared to BEV-matching baselines. Crucially, our work proves that diffusion models can enable scalable localization by treating noisy GPS as a generative prior-making a paradigm shift from traditional matching-based methods.

</details>


### [6] [MultiEdit: Advancing Instruction-based Image Editing on Diverse and Challenging Tasks](https://arxiv.org/abs/2509.14638)
*Mingsong Li,Lin Liu,Hongjun Wang,Haoxing Chen,Xijun Gu,Shizhan Liu,Dong Gong,Junbo Zhao,Zhenzhong Lan,Jianguo Li*

Main category: cs.CV

TL;DR: MultiEdit是一个包含10.7万个高质量图像编辑样本的综合数据集，涵盖6个挑战性编辑任务，通过创新的多模态大语言模型流水线生成视觉自适应编辑指令和高质量编辑图像。


<details>
  <summary>Details</summary>
Motivation: 解决当前基于指令的图像编辑方法在处理挑战性编辑任务时的局限性，现有数据集编辑类型和样本数量有限，且传统构建方法存在噪声图像-标题对，导致模型在复杂编辑场景中能力受限。

Method: 采用新颖的数据集构建流水线，利用两个多模态大语言模型分别生成视觉自适应编辑指令和制作高保真度编辑图像，涵盖18种非风格转换编辑类型和38种风格转换操作。

Result: 实验表明，使用MultiEdit-Train集微调基础开源模型显著提升了在复杂编辑任务上的性能，同时有效保持了在标准编辑基准上的能力。

Conclusion: MultiEdit为推进更多样化、更具挑战性的基于指令的图像编辑能力研究提供了宝贵资源，数据集已公开可用。

Abstract: Current instruction-based image editing (IBIE) methods struggle with challenging editing tasks, as both editing types and sample counts of existing datasets are limited. Moreover, traditional dataset construction often contains noisy image-caption pairs, which may introduce biases and limit model capabilities in complex editing scenarios. To address these limitations, we introduce MultiEdit, a comprehensive dataset featuring over 107K high-quality image editing samples. It encompasses 6 challenging editing tasks through a diverse collection of 18 non-style-transfer editing types and 38 style transfer operations, covering a spectrum from sophisticated style transfer to complex semantic operations like person reference editing and in-image text editing. We employ a novel dataset construction pipeline that utilizes two multi-modal large language models (MLLMs) to generate visual-adaptive editing instructions and produce high-fidelity edited images, respectively. Extensive experiments demonstrate that fine-tuning foundational open-source models with our MultiEdit-Train set substantially improves models' performance on sophisticated editing tasks in our proposed MultiEdit-Test benchmark, while effectively preserving their capabilities on the standard editing benchmark. We believe MultiEdit provides a valuable resource for advancing research into more diverse and challenging IBIE capabilities. Our dataset is available at https://huggingface.co/datasets/inclusionAI/MultiEdit.

</details>


### [7] [DACoN: DINO for Anime Paint Bucket Colorization with Any Number of Reference Images](https://arxiv.org/abs/2509.14685)
*Kazuma Nagata,Naoshi Kaneko*

Main category: cs.CV

TL;DR: DACoN是一个利用基础模型进行动漫线稿自动上色的框架，通过融合语义特征和空间特征，支持任意数量参考图像，解决了遮挡、姿态变化等挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习自动上色方法在处理遮挡、姿态变化和视角变化时存在困难，且通常只能支持1-2张参考图像，限制了上色效果。

Method: 提出DACoN框架，利用基础模型提取部件级语义特征，与CNN提取的高分辨率空间特征融合，实现细粒度且鲁棒的特征提取，并移除了参考图像数量限制。

Result: 定量和定性评估表明，使用多个参考图像能显著提升上色性能，该方法在颜色化效果上优于先前方法。

Conclusion: DACoN通过结合基础模型的语义理解能力和多参考图像策略，有效解决了动漫线稿自动上色中的关键挑战，实现了更优的颜色化效果。

Abstract: Automatic colorization of line drawings has been widely studied to reduce the labor cost of hand-drawn anime production. Deep learning approaches, including image/video generation and feature-based correspondence, have improved accuracy but struggle with occlusions, pose variations, and viewpoint changes. To address these challenges, we propose DACoN, a framework that leverages foundation models to capture part-level semantics, even in line drawings. Our method fuses low-resolution semantic features from foundation models with high-resolution spatial features from CNNs for fine-grained yet robust feature extraction. In contrast to previous methods that rely on the Multiplex Transformer and support only one or two reference images, DACoN removes this constraint, allowing any number of references. Quantitative and qualitative evaluations demonstrate the benefits of using multiple reference images, achieving superior colorization performance. Our code and model are available at https://github.com/kzmngt/DACoN.

</details>


### [8] [FMGS-Avatar: Mesh-Guided 2D Gaussian Splatting with Foundation Model Priors for 3D Monocular Avatar Reconstruction](https://arxiv.org/abs/2509.14739)
*Jinlong Fan,Bingyu Hu,Xingguang Li,Yuxiang Yang,Jing Zhang*

Main category: cs.CV

TL;DR: FMGS-Avatar是一种从单目视频重建高保真可动画人体化身的创新方法，通过网格引导的2D高斯泼溅和基础模型知识蒸馏，解决了现有方法在表面细节保持和信息不足方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 从单目视频重建高质量可动画人体化身面临几何信息不足和现有3D高斯泼溅方法表面细节保持能力有限的问题。

Method: 提出网格引导的2D高斯泼溅技术，将2D高斯基元直接附着到模板网格面上，并利用基础模型（如Sapiens）补充单目视频的有限视觉线索，采用选择性梯度隔离的协调训练策略解决多模态知识蒸馏中的优化冲突。

Result: 实验评估显示该方法在重建质量上显著优于现有方法，在几何精度和外观保真度方面有显著提升，同时提供丰富的语义信息，能够在共享规范空间中实现空间和时间一致的新视角和姿态渲染。

Conclusion: FMGS-Avatar通过结合增强的表征方法和协调的信息蒸馏策略，显著推进了3D单目人体化身重建技术，实现了高质量的几何细节保持和外观保真度。

Abstract: Reconstructing high-fidelity animatable human avatars from monocular videos remains challenging due to insufficient geometric information in single-view observations. While recent 3D Gaussian Splatting methods have shown promise, they struggle with surface detail preservation due to the free-form nature of 3D Gaussian primitives. To address both the representation limitations and information scarcity, we propose a novel method, \textbf{FMGS-Avatar}, that integrates two key innovations. First, we introduce Mesh-Guided 2D Gaussian Splatting, where 2D Gaussian primitives are attached directly to template mesh faces with constrained position, rotation, and movement, enabling superior surface alignment and geometric detail preservation. Second, we leverage foundation models trained on large-scale datasets, such as Sapiens, to complement the limited visual cues from monocular videos. However, when distilling multi-modal prior knowledge from foundation models, conflicting optimization objectives can emerge as different modalities exhibit distinct parameter sensitivities. We address this through a coordinated training strategy with selective gradient isolation, enabling each loss component to optimize its relevant parameters without interference. Through this combination of enhanced representation and coordinated information distillation, our approach significantly advances 3D monocular human avatar reconstruction. Experimental evaluation demonstrates superior reconstruction quality compared to existing methods, with notable gains in geometric accuracy and appearance fidelity while providing rich semantic information. Additionally, the distilled prior knowledge within a shared canonical space naturally enables spatially and temporally consistent rendering under novel views and poses.

</details>


### [9] [Dataset Distillation for Super-Resolution without Class Labels and Pre-trained Models](https://arxiv.org/abs/2509.14777)
*Sunwoo Cho,Yejin Jung,Nam Ik Cho,Jae Woong Soh*

Main category: cs.CV

TL;DR: 通过新的数据蓄粮方法，使用很少的训练数据和计算时间就能达到超分辨率图像复原的独立性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络训练需要大量数据和计算资源的问题，特别是在超分辨率图像复原领域，并免除对预训练模型和类别标签的依赖性。

Method: 首先提取高梯度补丁并使用CLIP特征对图像分类，然后微调扩散模型学习选择补丁的分布来合成蓄粮训练图像。

Result: 仅使用0.68%的原始数据集训练转换器模型，性能下降仅0.3dB，微调模型小小时训练，SR模型训练仅需1小时，远少于全数据集的11小时。

Conclusion: 该方法实现了独立于预训练模型和类别标签的高效数据蓄粮，显著提升了数据利用效率和计算效率。

Abstract: Training deep neural networks has become increasingly demanding, requiring large datasets and significant computational resources, especially as model complexity advances. Data distillation methods, which aim to improve data efficiency, have emerged as promising solutions to this challenge. In the field of single image super-resolution (SISR), the reliance on large training datasets highlights the importance of these techniques. Recently, a generative adversarial network (GAN) inversion-based data distillation framework for SR was proposed, showing potential for better data utilization. However, the current method depends heavily on pre-trained SR networks and class-specific information, limiting its generalizability and applicability. To address these issues, we introduce a new data distillation approach for image SR that does not need class labels or pre-trained SR models. In particular, we first extract high-gradient patches and categorize images based on CLIP features, then fine-tune a diffusion model on the selected patches to learn their distribution and synthesize distilled training images. Experimental results show that our method achieves state-of-the-art performance while using significantly less training data and requiring less computational time. Specifically, when we train a baseline Transformer model for SR with only 0.68\% of the original dataset, the performance drop is just 0.3 dB. In this case, diffusion model fine-tuning takes 4 hours, and SR model training completes within 1 hour, much shorter than the 11-hour training time with the full dataset.

</details>


### [10] [Radiology Report Conditional 3D CT Generation with Multi Encoder Latent diffusion Model](https://arxiv.org/abs/2509.14780)
*Sina Amirrajab,Zohaib Salahuddin,Sheng Kuang,Henry C. Woodruff,Philippe Lambin*

Main category: cs.CV

TL;DR: Report2CT是一个基于放射学报告的3D CT生成框架，使用多文本编码器和潜在扩散模型，在MICCAI 2025挑战赛中取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用简化提示，忽略了放射学报告中的丰富语义细节，导致文本-图像对齐和临床保真度降低。

Method: 整合三个预训练医学文本编码器(BiomedVLP CXR BERT、MedEmbed、ClinicalBERT)，使用放射学报告和体素间距信息条件化3D潜在扩散模型，在CT RATE数据集的20000个CT体积上训练。

Result: 生成解剖结构一致的CT体积，具有优异的视觉质量和文本-图像对齐，多编码器条件化提高了CLIP分数，在VLM3D挑战赛中排名第一。

Conclusion: 通过利用完整放射学报告和多编码器文本条件化，Report2CT推进了3D CT合成，产生临床忠实的高质量合成数据。

Abstract: Text to image latent diffusion models have recently advanced medical image synthesis, but applications to 3D CT generation remain limited. Existing approaches rely on simplified prompts, neglecting the rich semantic detail in full radiology reports, which reduces text image alignment and clinical fidelity. We propose Report2CT, a radiology report conditional latent diffusion framework for synthesizing 3D chest CT volumes directly from free text radiology reports, incorporating both findings and impression sections using multiple text encoder. Report2CT integrates three pretrained medical text encoders (BiomedVLP CXR BERT, MedEmbed, and ClinicalBERT) to capture nuanced clinical context. Radiology reports and voxel spacing information condition a 3D latent diffusion model trained on 20000 CT volumes from the CT RATE dataset. Model performance was evaluated using Frechet Inception Distance (FID) for real synthetic distributional similarity and CLIP based metrics for semantic alignment, with additional qualitative and quantitative comparisons against GenerateCT model. Report2CT generated anatomically consistent CT volumes with excellent visual quality and text image alignment. Multi encoder conditioning improved CLIP scores, indicating stronger preservation of fine grained clinical details in the free text radiology reports. Classifier free guidance further enhanced alignment with only a minor trade off in FID. We ranked first in the VLM3D Challenge at MICCAI 2025 on Text Conditional CT Generation and achieved state of the art performance across all evaluation metrics. By leveraging complete radiology reports and multi encoder text conditioning, Report2CT advances 3D CT synthesis, producing clinically faithful and high quality synthetic data.

</details>


### [11] [Not All Degradations Are Equal: A Targeted Feature Denoising Framework for Generalizable Image Super-Resolution](https://arxiv.org/abs/2509.14841)
*Hongjun Wang,Jiyuan Chen,Zhengwei Yin,Xuan Song,Yinqiang Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种针对性的特征去噪框架，专门处理超分辨中模型对噪声逆合的问题，而非所有降级类型。


<details>
  <summary>Details</summary>
Motivation: 通过仔细研究发现，超分辨模型主要逆合的是噪声降级，而非所有降级类型，因此需要针对性的解决方案。

Method: 提出包含噪声检测和去噪模块的目标特征去噪框架，可以无缝集成到现有超分辨模型中而无需改动网络结构。

Result: 在5个传统测试集和数据集上表现出艰济的性能，包括合成和真实场景，超越了之前的正则化方法。

Conclusion: 该框架为通用性图像超分辨提供了一种高效的解决方案，能够有效抑制模型对噪声降级的逆合问题。

Abstract: Generalizable Image Super-Resolution aims to enhance model generalization capabilities under unknown degradations. To achieve this goal, the models are expected to focus only on image content-related features instead of overfitting degradations. Recently, numerous approaches such as Dropout and Feature Alignment have been proposed to suppress models' natural tendency to overfit degradations and yield promising results. Nevertheless, these works have assumed that models overfit to all degradation types (e.g., blur, noise, JPEG), while through careful investigations in this paper, we discover that models predominantly overfit to noise, largely attributable to its distinct degradation pattern compared to other degradation types. In this paper, we propose a targeted feature denoising framework, comprising noise detection and denoising modules. Our approach presents a general solution that can be seamlessly integrated with existing super-resolution models without requiring architectural modifications. Our framework demonstrates superior performance compared to previous regularization-based methods across five traditional benchmarks and datasets, encompassing both synthetic and real-world scenarios.

</details>


### [12] [Controllable Localized Face Anonymization Via Diffusion Inpainting](https://arxiv.org/abs/2509.14866)
*Ali Salar,Qing Liu,Guoying Zhao*

Main category: cs.CV

TL;DR: 基于潜在扩散模型的统一匿名化框架，通过自适应属性引导模块在去噪过程中控制面部属性，支持局部匿名化，无需额外训练即可超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着肖像图像在计算机视觉中的广泛应用，需要保护个人身份隐私，同时确保匿名化后的图像仍能有效用于下游视觉任务。

Method: 利用潜在扩散模型的修复能力，设计自适应属性引导模块，在反向去噪过程中应用梯度校正，使生成图像的面部属性与合成目标图像对齐，支持局部匿名化。

Result: 在CelebA-HQ和FFHQ数据集上的大量实验表明，该方法优于最先进的方法，且不需要额外的模型训练。

Conclusion: 提出的统一框架能够有效生成逼真的匿名化图像，在保护隐私的同时保持图像对下游任务的实用性，代码已开源。

Abstract: The growing use of portrait images in computer vision highlights the need to protect personal identities. At the same time, anonymized images must remain useful for downstream computer vision tasks. In this work, we propose a unified framework that leverages the inpainting ability of latent diffusion models to generate realistic anonymized images. Unlike prior approaches, we have complete control over the anonymization process by designing an adaptive attribute-guidance module that applies gradient correction during the reverse denoising process, aligning the facial attributes of the generated image with those of the synthesized target image. Our framework also supports localized anonymization, allowing users to specify which facial regions are left unchanged. Extensive experiments conducted on the public CelebA-HQ and FFHQ datasets show that our method outperforms state-of-the-art approaches while requiring no additional model training. The source code is available on our page.

</details>


### [13] [NeRF-based Visualization of 3D Cues Supporting Data-Driven Spacecraft Pose Estimation](https://arxiv.org/abs/2509.14890)
*Antoine Legrand,Renaud Detry,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: 通过训练NeRF图像生成器来可视化计算机视觉定位网络依赖的3D视觉线索，提高它们在实际宇宙任务中的可信过程


<details>
  <summary>Details</summary>
Motivation: 解决数据驱动的宇宙器定位方法因缺乏决策过程可解释性而在实际任务中采用遇刻的问题

Method: 训练基于NeRF的图像生成器，通过定位网络的渐度反向传播来强化生成宇宙器定位网络主要利用的3D特征

Result: 实验证明该方法能够恢复关键的3D视觉线索，并提供了定位网络监督与其隐式表征之间关系的更深入见解

Conclusion: 该可视化方法有助于理解宇宙器定位网络的决策过程，为实际宇宙任务中的可靠应用提供支持

Abstract: On-orbit operations require the estimation of the relative 6D pose, i.e., position and orientation, between a chaser spacecraft and its target. While data-driven spacecraft pose estimation methods have been developed, their adoption in real missions is hampered by the lack of understanding of their decision process. This paper presents a method to visualize the 3D visual cues on which a given pose estimator relies. For this purpose, we train a NeRF-based image generator using the gradients back-propagated through the pose estimation network. This enforces the generator to render the main 3D features exploited by the spacecraft pose estimation network. Experiments demonstrate that our method recovers the relevant 3D cues. Furthermore, they offer additional insights on the relationship between the pose estimation network supervision and its implicit representation of the target spacecraft.

</details>


### [14] [SPATIALGEN: Layout-guided 3D Indoor Scene Generation](https://arxiv.org/abs/2509.14981)
*Chuan Fang,Heng Li,Yixun Liang,Jia Zheng,Yongsen Mao,Yuan Liu,Rui Tang,Zihan Zhou,Ping Tan*

Main category: cs.CV

TL;DR: 提出了SpatialGen多视角多模态扩散模型，使用大规模合成数据集生成高质量、语义一致的3D室内场景


<details>
  <summary>Details</summary>
Motivation: 解决手动3D建模耗时费力的问题，现有生成方法在视觉质量、多样性、语义一致性和用户控制方面存在挑战，缺乏专门的大规模高质量数据集

Method: 构建包含12,328个结构化标注场景的大规模合成数据集，开发多视角多模态扩散模型SpatialGen，基于3D布局和参考图像生成外观、几何和语义信息

Result: 在实验中持续生成优于先前方法的结果，能够从任意视角合成保持跨模态空间一致性的场景

Conclusion: 通过开源数据和模型推动室内场景理解和生成领域发展，为设计、虚拟现实和机器人应用提供高效解决方案

Abstract: Creating high-fidelity 3D models of indoor environments is essential for applications in design, virtual reality, and robotics. However, manual 3D modeling remains time-consuming and labor-intensive. While recent advances in generative AI have enabled automated scene synthesis, existing methods often face challenges in balancing visual quality, diversity, semantic consistency, and user control. A major bottleneck is the lack of a large-scale, high-quality dataset tailored to this task. To address this gap, we introduce a comprehensive synthetic dataset, featuring 12,328 structured annotated scenes with 57,440 rooms, and 4.7M photorealistic 2D renderings. Leveraging this dataset, we present SpatialGen, a novel multi-view multi-modal diffusion model that generates realistic and semantically consistent 3D indoor scenes. Given a 3D layout and a reference image (derived from a text prompt), our model synthesizes appearance (color image), geometry (scene coordinate map), and semantic (semantic segmentation map) from arbitrary viewpoints, while preserving spatial consistency across modalities. SpatialGen consistently generates superior results to previous methods in our experiments. We are open-sourcing our data and models to empower the community and advance the field of indoor scene understanding and generation.

</details>


### [15] [Sea-ing Through Scattered Rays: Revisiting the Image Formation Model for Realistic Underwater Image Generation](https://arxiv.org/abs/2509.15011)
*Vasiliki Ismiroglou,Malte Pedersen,Stefan H. Bengtson,Andreas Aakerberg,Thomas B. Moeslund*

Main category: cs.CV

TL;DR: 本文提出了一种改进的水下合成数据生成流程，包含前向散射项和非均匀媒质，并收集了BUCKET数据集。结果显示在高润渍条件下质量显著提升，获得182.5%的选择率。


<details>
  <summary>Details</summary>
Motivation: 现有水下图像形成模型多关注颜色失真，而忽视了高润渍环境中复杂的距离依赖性可见性损失问题。

Method: 提出改进的合成数据生成流程，包含常被忽略的前向散射项，考虑非均匀媒质，并收集BUCKET数据集获取真实润渍镜头和对应参考图像。

Result: 结果显示在逐渐增加的润渍度条件下，新模型比参考模型在质量上有显著改善，调查参与者的选择率达到82.5%。

Conclusion: 该研究提供了一种更全面的水下合成数据生成方法，特别在高润渍环境中显示出优势，为水下视觉研究提供了有价值的数据资源和技术方案。

Abstract: In recent years, the underwater image formation model has found extensive use in the generation of synthetic underwater data. Although many approaches focus on scenes primarily affected by discoloration, they often overlook the model's ability to capture the complex, distance-dependent visibility loss present in highly turbid environments. In this work, we propose an improved synthetic data generation pipeline that includes the commonly omitted forward scattering term, while also considering a nonuniform medium. Additionally, we collected the BUCKET dataset under controlled turbidity conditions to acquire real turbid footage with the corresponding reference images. Our results demonstrate qualitative improvements over the reference model, particularly under increasing turbidity, with a selection rate of 82. 5\% by survey participants. Data and code can be accessed on the project page: vap.aau.dk/sea-ing-through-scattered-rays.

</details>


### [16] [AutoEdit: Automatic Hyperparameter Tuning for Image Editing](https://arxiv.org/abs/2509.15031)
*Chau Pham,Quan Dao,Mahesh Bhosale,Yunjie Tian,Dimitris Metaxas,David Doermann*

Main category: cs.CV

TL;DR: 通过强化学习框架动态调整反布模型图像编辑的超参数，避免暴力搜索并大幅降低计算成本


<details>
  <summary>Details</summary>
Motivation: 现有反布模型图像编辑方法需要用户手动暴力调整多个相互依赖的超参数，计算成本高，影响实际部署

Method: 将超参数搜索模型化为马尔可夫决策过程，使用近端策略优化强化学习框架动态调整各去噪步骤的超参数

Result: 实验结果显示方法大幅缩短了搜索时间和计算开销，显著优于现有暴力搜索方法

Conclusion: 该强化学习框架提高了反布模型图像编辑的实用性，促进了该技术在实际应用中的部署

Abstract: Recent advances in diffusion models have revolutionized text-guided image editing, yet existing editing methods face critical challenges in hyperparameter identification. To get the reasonable editing performance, these methods often require the user to brute-force tune multiple interdependent hyperparameters, such as inversion timesteps and attention modification, \textit{etc.} This process incurs high computational costs due to the huge hyperparameter search space. We consider searching optimal editing's hyperparameters as a sequential decision-making task within the diffusion denoising process. Specifically, we propose a reinforcement learning framework, which establishes a Markov Decision Process that dynamically adjusts hyperparameters across denoising steps, integrating editing objectives into a reward function. The method achieves time efficiency through proximal policy optimization while maintaining optimal hyperparameter configurations. Experiments demonstrate significant reduction in search time and computational overhead compared to existing brute-force approaches, advancing the practical deployment of a diffusion-based image editing framework in the real world.

</details>


### [17] [RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes](https://arxiv.org/abs/2509.15123)
*Fang Li,Hao Zhang,Narendra Ahuja*

Main category: cs.CV

TL;DR: 这篇论文提出了一种仅需单个RGB视频作为监督的动态场景摄像机参数优化方法，解决了COLMAP在动态场景中运行时间长和依赖真实运动掩码的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法如COLMAP在动态场景中存在运行时间长、依赖真实运动掩码等限制，而且常需要充分的预知信息如真实焦距、运动掩码、3D点云等，这些在普通RGB视频中通常无法获得。

Method: 方法包含三个关键组件：(1)表面块追踪滤波器，建立稳健的极稀疏齐关系；(2)离群点感知的聚合优化，通过自适应降权移动离群点来优化摄像机参数；(3)两阶段优化策略，通过调节Softplus限制和凸最小值来提高稳定性和优化速度。

Result: 在4个实际数据集(NeRF-DS, DAVIS, iPhone, TUM-dynamics)和1个合成数据集(MPI-Sintel)上进行实验，方法能够仅使用单个RGB视频就更高效、更准确地估计摄像机参数。通过将摄像机估计输入到4D重建方法进一步验证了准确性。

Conclusion: 该方法提供了一种无需外部预知信息的动态场景摄像机参数优化解决方案，在准确性和效率方面都超过了现有方法。

Abstract: Although COLMAP has long remained the predominant method for camera parameter optimization in static scenes, it is constrained by its lengthy runtime and reliance on ground truth (GT) motion masks for application to dynamic scenes. Many efforts attempted to improve it by incorporating more priors as supervision such as GT focal length, motion masks, 3D point clouds, camera poses, and metric depth, which, however, are typically unavailable in casually captured RGB videos. In this paper, we propose a novel method for more accurate and efficient camera parameter optimization in dynamic scenes solely supervised by a single RGB video. Our method consists of three key components: (1) Patch-wise Tracking Filters, to establish robust and maximally sparse hinge-like relations across the RGB video. (2) Outlier-aware Joint Optimization, for efficient camera parameter optimization by adaptive down-weighting of moving outliers, without reliance on motion priors. (3) A Two-stage Optimization Strategy, to enhance stability and optimization speed by a trade-off between the Softplus limits and convex minima in losses. We visually and numerically evaluate our camera estimates. To further validate accuracy, we feed the camera estimates into a 4D reconstruction method and assess the resulting 3D scenes, and rendered 2D RGB and depth maps. We perform experiments on 4 real-world datasets (NeRF-DS, DAVIS, iPhone, and TUM-dynamics) and 1 synthetic dataset (MPI-Sintel), demonstrating that our method estimates camera parameters more efficiently and accurately with a single RGB video as the only supervision.

</details>


### [18] [AIP: Subverting Retrieval-Augmented Generation via Adversarial Instructional Prompt](https://arxiv.org/abs/2509.15159)
*Saket S. Chaturvedi,Gaurav Bagwe,Lan Zhang,Xiaoyong Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种针对检索增强生成(RAG)系统的新型攻击方法AIP，通过操纵指令提示来隐秘地改变检索行为，攻击成功率高达95.23%，同时保持良性功能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG攻击主要依赖操纵用户查询，但在实践中往往不可行，因为用户输入通常是固定或受保护的。指令提示被广泛重用、公开分享且很少审计，其隐含的信任使其成为攻击者操纵RAG行为的理想目标。

Method: 提出基于遗传算法的联合优化方法，通过多样化查询生成策略模拟真实语言变化，演化对抗性提示以平衡攻击成功率、清洁任务效用和隐蔽性。

Result: AIP攻击在保持良性功能的同时，攻击成功率可达95.23%，揭示了RAG系统中先前被忽视的关键漏洞。

Conclusion: 研究结果强调了重新评估共享指令提示的必要性，揭示了可信但看似良性的接口组件如何被武器化以降低系统完整性。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by retrieving relevant documents from external sources to improve factual accuracy and verifiability. However, this reliance introduces new attack surfaces within the retrieval pipeline, beyond the LLM itself. While prior RAG attacks have exposed such vulnerabilities, they largely rely on manipulating user queries, which is often infeasible in practice due to fixed or protected user inputs. This narrow focus overlooks a more realistic and stealthy vector: instructional prompts, which are widely reused, publicly shared, and rarely audited. Their implicit trust makes them a compelling target for adversaries to manipulate RAG behavior covertly.   We introduce a novel attack for Adversarial Instructional Prompt (AIP) that exploits adversarial instructional prompts to manipulate RAG outputs by subtly altering retrieval behavior. By shifting the attack surface to the instructional prompts, AIP reveals how trusted yet seemingly benign interface components can be weaponized to degrade system integrity. The attack is crafted to achieve three goals: (1) naturalness, to evade user detection; (2) utility, to encourage use of prompts; and (3) robustness, to remain effective across diverse query variations. We propose a diverse query generation strategy that simulates realistic linguistic variation in user queries, enabling the discovery of prompts that generalize across paraphrases and rephrasings. Building on this, a genetic algorithm-based joint optimization is developed to evolve adversarial prompts by balancing attack success, clean-task utility, and stealthiness. Experimental results show that AIP achieves up to 95.23% ASR while preserving benign functionality. These findings uncover a critical and previously overlooked vulnerability in RAG systems, emphasizing the need to reassess the shared instructional prompts.

</details>


### [19] [A Race Bias Free Face Aging Model for Reliable Kinship Verification](https://arxiv.org/abs/2509.15177)
*Ali Nazari,Bardiya Kariminia,Mohsen Ebrahimi Moghaddam*

Main category: cs.CV

TL;DR: 这篇论文提出了一种叫RA-GAN的无种族偏见面部老化模型，通过RACEpSp和特征混合器模块，在亲缘验证中生成无偏见的同龄化图像，显著提升了验证准确率。


<details>
  <summary>Details</summary>
Motivation: 亲缘验证中家长与孩子照片存在年龄差距，而现有面部老化模型存在种族偏见，影响图像相似性评估。

Method: 设计RA-GAN模型，包含RACEpSp模块和特征混合器，生成无种族偏见的面部老化图像，并在KinFaceW-I和KinFaceW-II数据集上进行亲缘验证实验。

Result: RA-GAN在所有年龄段比SAM-GAN平均提升13.14%，在60+年龄段比CUSP-GAN提升9.1%种族准确性，同时更好保留主体身份。亲缘验证准确率在多个关系类型上都有显著提升。

Conclusion: 该研究证明了通过无偏见面部老化技术将家长与孩子图像转换到同一年龄可以有效提高亲缘验证的准确性，RA-GAN模型在性能和偏见消除方面都显示出优势。

Abstract: The age gap in kinship verification addresses the time difference between the photos of the parent and the child. Moreover, their same-age photos are often unavailable, and face aging models are racially biased, which impacts the likeness of photos. Therefore, we propose a face aging GAN model, RA-GAN, consisting of two new modules, RACEpSp and a feature mixer, to produce racially unbiased images. The unbiased synthesized photos are used in kinship verification to investigate the results of verifying same-age parent-child images. The experiments demonstrate that our RA-GAN outperforms SAM-GAN on an average of 13.14\% across all age groups, and CUSP-GAN in the 60+ age group by 9.1\% in terms of racial accuracy. Moreover, RA-GAN can preserve subjects' identities better than SAM-GAN and CUSP-GAN across all age groups. Additionally, we demonstrate that transforming parent and child images from the KinFaceW-I and KinFaceW-II datasets to the same age can enhance the verification accuracy across all age groups. The accuracy increases with our RA-GAN for the kinship relationships of father-son and father-daughter, mother-son, and mother-daughter, which are 5.22, 5.12, 1.63, and 0.41, respectively, on KinFaceW-I. Additionally, the accuracy for the relationships of father-daughter, father-son, and mother-son is 2.9, 0.39, and 1.6 on KinFaceW-II, respectively. The code is available at~\href{https://github.com/bardiya2254kariminia/An-Age-Transformation-whitout-racial-bias-for-Kinship-verification}{Github}

</details>


### [20] [Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding](https://arxiv.org/abs/2509.15178)
*Zaiquan Yang,Yuhao Liu,Gerhard Hancke,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: 基于MLLM的零样本时空视频基准方法，通过解构化空间-时间突出(DSTH)和时间增强组装(TAS)策略，解决MLLM在视频基准中的次优问题，在多个标准测试集上超越现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 发现MLLM在视频基准中存在两个关键问题：(1)动态分配基准标记但基准效果次优；(2)无法充分整合文本查询中的类型、动作等线索进行推理。需要开发一种零样本方法来充分发挥MLLM的推理能力。

Method: 提出DSTH策略：将原始查询解构为属性和动作子查询，使用logit导向重关注(LRA)模块学习空间和时间提示，分别突出属性和动作线索。TAS策略：使用原始视频帧和时间增强帧进行预测组装，提高时间一致性。

Result: 在三个常见的STVG标准测试集上，该方法在多个MLLM上都表现出色，超越了现有的SOTA方法。

Conclusion: 通过DSTH和TAS策略，成功地解改了MLLM在视频基准中的限制，充分发挥了其多模态推理能力，为零样本时空视频基准提供了有效解决方案。

Abstract: Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal tube of a video, as specified by the input text query. In this paper, we utilize multimodal large language models (MLLMs) to explore a zero-shot solution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to dynamically assign special tokens, referred to as \textit{grounding tokens}, for grounding the text query; and (2) MLLMs often suffer from suboptimal grounding due to the inability to fully integrate the cues in the text query (\textit{e.g.}, attributes, actions) for inference. Based on these insights, we propose a MLLM-based zero-shot framework for STVG, which includes novel decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH strategy first decouples the original query into attribute and action sub-queries for inquiring the existence of the target both spatially and temporally. It then uses a novel logit-guided re-attention (LRA) module to learn latent variables as spatial and temporal prompts, by regularizing token predictions for each sub-query. These prompts highlight attribute and action cues, respectively, directing the model's attention to reliable spatial and temporal related visual regions. In addition, as the spatial grounding by the attribute sub-query should be temporally consistent, we introduce the TAS strategy to assemble the predictions using the original video frames and the temporal-augmented frames as inputs to help improve temporal consistency. We evaluate our method on various MLLMs, and show that it outperforms SOTA methods on three common STVG benchmarks.   The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG.

</details>


### [21] [Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation](https://arxiv.org/abs/2509.15185)
*Xiaoyu Yue,Zidong Wang,Yuqing Wang,Wenlong Zhang,Xihui Liu,Wanli Ouyang,Lei Bai,Luping Zhou*

Main category: cs.CV

TL;DR: 本文系统研究了自回归模型在视觉领域的应用机制，发现了三个阻碍高级视觉语义学习的关键问题，并提出了自引导训练框架ST-AR来有效解决这些问题，显著提升了图像理解和生成质量。


<details>
  <summary>Details</summary>
Motivation: 最近的研究表明高质量视觉表示在图像生成中的重要性，并指出了生成模型在图像理解方面的局限性。作为最初为自然语言设计的生成范式，自回归模型在视觉领域面临类似挑战。

Method: 提出了自引导训练框架ST-AR，通过在训练过程中引入自监督目标来解决自回归模型在视觉领域面临的三个关键问题：局部条件依赖、步间语义不一致性和空间不变性缺失。

Result: ST-AR显著提升了自回归模型的图像理解能力并改善了生成质量。具体而言，ST-AR为LlamaGen-L带来约42%的FID改进，为LlamaGen-XL带来49%的FID改进，同时保持相同的采样策略。

Conclusion: 自引导训练框架ST-AR能够有效解决自回归模型在视觉领域的语义学习问题，在不依赖预训练表示模型的情况下显著提升图像理解和生成性能。

Abstract: Recent studies have demonstrated the importance of high-quality visual representations in image generation and have highlighted the limitations of generative models in image understanding. As a generative paradigm originally designed for natural language, autoregressive models face similar challenges. In this work, we present the first systematic investigation into the mechanisms of applying the next-token prediction paradigm to the visual domain. We identify three key properties that hinder the learning of high-level visual semantics: local and conditional dependence, inter-step semantic inconsistency, and spatial invariance deficiency. We show that these issues can be effectively addressed by introducing self-supervised objectives during training, leading to a novel training framework, Self-guided Training for AutoRegressive models (ST-AR). Without relying on pre-trained representation models, ST-AR significantly enhances the image understanding ability of autoregressive models and leads to improved generation quality. Specifically, ST-AR brings approximately 42% FID improvement for LlamaGen-L and 49% FID improvement for LlamaGen-XL, while maintaining the same sampling strategy.

</details>


### [22] [RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation](https://arxiv.org/abs/2509.15212)
*Yuming Jiang,Siteng Huang,Shengke Xue,Yaxi Zhao,Jun Cen,Sicong Leng,Kehan Li,Jiayan Guo,Kexiang Wang,Mingxiu Chen,Fan Wang,Deli Zhao,Xin Li*

Main category: cs.CV

TL;DR: RynnVLA-001是一个基于大规模视频生成预训练的视觉-语言-动作模型，采用两阶段预训练方法：自我中心视频生成预训练和人类中心轨迹感知建模，通过ActionVAE压缩动作序列，在机器人数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决视觉-语言-动作模型中动作预测的复杂性，并建立视觉帧预测与动作预测之间的有效桥梁，需要开发更有效的预训练策略来提升模型性能。

Method: 提出两阶段预训练方法：第一阶段在1200万个自我中心操作视频上训练图像到视频模型，预测基于初始帧和语言指令的未来帧；第二阶段联合预测未来关键点轨迹，并使用ActionVAE变分自编码器压缩动作序列为紧凑潜在嵌入。

Result: 在相同下游机器人数据集上微调后，RynnVLA-001超越了最先进的基线方法，表现出优越的性能。

Conclusion: 所提出的预训练策略为视觉-语言-动作模型提供了更有效的初始化方法，证明了视频生成预训练在VLA模型中的重要性。

Abstract: This paper presents RynnVLA-001, a vision-language-action(VLA) model built upon large-scale video generative pretraining from human demonstrations. We propose a novel two-stage pretraining methodology. The first stage, Ego-Centric Video Generative Pretraining, trains an Image-to-Video model on 12M ego-centric manipulation videos to predict future frames conditioned on an initial frame and a language instruction. The second stage, Human-Centric Trajectory-Aware Modeling, extends this by jointly predicting future keypoint trajectories, thereby effectively bridging visual frame prediction with action prediction. Furthermore, to enhance action representation, we propose ActionVAE, a variational autoencoder that compresses sequences of actions into compact latent embeddings, reducing the complexity of the VLA output space. When finetuned on the same downstream robotics datasets, RynnVLA-001 achieves superior performance over state-of-the-art baselines, demonstrating that the proposed pretraining strategy provides a more effective initialization for VLA models.

</details>


### [23] [Out-of-Sight Trajectories: Tracking, Fusion, and Prediction](https://arxiv.org/abs/2509.15219)
*Haichao Zhang,Yi Xu,Yun Fu*

Main category: cs.CV

TL;DR: 提出OST任务，通过视觉-定位映射和去噪模块处理视线外物体的噪声传感器数据，实现无监督去噪和轨迹预测，在多个数据集上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖完整无噪声观测数据，忽略了视线外物体和传感器噪声的挑战，这在现实场景中造成安全风险并阻碍可靠预测

Method: 增强的视觉-定位去噪模块利用相机标定建立视觉-定位映射，以无监督方式有效去噪噪声传感器数据，扩展应用到行人和车辆

Result: 在Vi-Fi和JRDB数据集上实现轨迹去噪和预测的最先进性能，显著超越先前基线，并与卡尔曼滤波等传统方法进行比较

Conclusion: 这是首个集成视觉-定位投影来去噪视线外智能体噪声传感器轨迹的工作，为未来进展铺平了道路

Abstract: Trajectory prediction is a critical task in computer vision and autonomous systems, playing a key role in autonomous driving, robotics, surveillance, and virtual reality. Existing methods often rely on complete and noise-free observational data, overlooking the challenges associated with out-of-sight objects and the inherent noise in sensor data caused by limited camera coverage, obstructions, and the absence of ground truth for denoised trajectories. These limitations pose safety risks and hinder reliable prediction in real-world scenarios. In this extended work, we present advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the noise-free visual trajectories of out-of-sight objects using noisy sensor data. Building on our previous research, we broaden the scope of Out-of-Sight Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending its applicability to autonomous driving, robotics, surveillance, and virtual reality. Our enhanced Vision-Positioning Denoising Module leverages camera calibration to establish a vision-positioning mapping, addressing the lack of visual references, while effectively denoising noisy sensor data in an unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB datasets, our approach achieves state-of-the-art performance in both trajectory denoising and prediction, significantly surpassing previous baselines. Additionally, we introduce comparisons with traditional denoising methods, such as Kalman filtering, and adapt recent trajectory prediction models to our task, providing a comprehensive benchmark. This work represents the first initiative to integrate vision-positioning projection for denoising noisy sensor trajectories of out-of-sight agents, paving the way for future advances. The code and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST

</details>


### [24] [Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model](https://arxiv.org/abs/2509.15220)
*Fangjinhua Wang,Qingshan Xu,Yew-Soon Ong,Marc Pollefeys*

Main category: cs.CV

TL;DR: 提出基于扩散模型的多视角立体视觉框架，将深度图细化建模为条件扩散过程，实现高效且高性能的3D重建


<details>
  <summary>Details</summary>
Motivation: 传统多视角立体视觉方法计算效率低，而扩散模型在生成任务中表现出色，希望通过引入扩散模型提升MVS的效率和性能

Method: 设计条件编码器指导扩散过程，结合轻量级2D U-Net和卷积GRU的扩散网络，提出基于置信度的自适应采样策略

Result: DiffMVS在运行时间和GPU内存方面达到SOTA效率，CasDiffMVS在DTU、Tanks & Temples和ETH3D数据集上达到SOTA性能

Conclusion: 扩散模型成功应用于MVS任务，在保持高效的同时显著提升了重建质量，为3D几何重建提供了新思路

Abstract: To reconstruct the 3D geometry from calibrated images, learning-based multi-view stereo (MVS) methods typically perform multi-view depth estimation and then fuse depth maps into a mesh or point cloud. To improve the computational efficiency, many methods initialize a coarse depth map and then gradually refine it in higher resolutions. Recently, diffusion models achieve great success in generation tasks. Starting from a random noise, diffusion models gradually recover the sample with an iterative denoising process. In this paper, we propose a novel MVS framework, which introduces diffusion models in MVS. Specifically, we formulate depth refinement as a conditional diffusion process. Considering the discriminative characteristic of depth estimation, we design a condition encoder to guide the diffusion process. To improve efficiency, we propose a novel diffusion network combining lightweight 2D U-Net and convolutional GRU. Moreover, we propose a novel confidence-based sampling strategy to adaptively sample depth hypotheses based on the confidence estimated by diffusion model. Based on our novel MVS framework, we propose two novel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitive performance with state-of-the-art efficiency in run-time and GPU memory. CasDiffMVS achieves state-of-the-art performance on DTU, Tanks & Temples and ETH3D. Code is available at: https://github.com/cvg/diffmvs.

</details>


### [25] [Lost in Translation? Vocabulary Alignment for Source-Free Domain Adaptation in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2509.15225)
*Silvio Mazzucco,Carl Persson,Mattia Segu,Pier Luigi Dovesi,Federico Tombari,Luc Van Gool,Matteo Poggi*

Main category: cs.CV

TL;DR: VocAlign是一个针对开放词汇语义分割的源自由领域自适应框架，采用师生范式增强词汇对齐策略，通过LoRA微调和Top-K类别选择机制实现高效适配，在CityScapes数据集上取得6.11 mIoU显著提升


<details>
  <summary>Details</summary>
Motivation: 针对视觉语言模型在开放词汇语义分割中的源自由领域自适应需求，旨在提升伪标签生成质量并降低计算开销

Method: 采用师生范式结合词汇对齐策略，使用LoRA进行高效微调，并提出Top-K类别选择机制减少内存需求

Result: 在CityScapes数据集上实现6.11 mIoU显著提升，在零样本分割基准测试中表现优异

Conclusion: VocAlign为开放词汇设置下的源自由自适应设立了新标准，在保持模型原始能力的同时实现了高效适配

Abstract: We introduce VocAlign, a novel source-free domain adaptation framework specifically designed for VLMs in open-vocabulary semantic segmentation. Our method adopts a student-teacher paradigm enhanced with a vocabulary alignment strategy, which improves pseudo-label generation by incorporating additional class concepts. To ensure efficiency, we use Low-Rank Adaptation (LoRA) to fine-tune the model, preserving its original capabilities while minimizing computational overhead. In addition, we propose a Top-K class selection mechanism for the student model, which significantly reduces memory requirements while further improving adaptation performance. Our approach achieves a notable 6.11 mIoU improvement on the CityScapes dataset and demonstrates superior performance on zero-shot segmentation benchmarks, setting a new standard for source-free adaptation in the open-vocabulary setting.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [26] [Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models](https://arxiv.org/abs/2509.15076)
*Mohammad Saleh Vahdatpour,Maryam Eyvazi,Yanqing Zhang*

Main category: cs.LG

TL;DR: 基于天空图像的AI空气质量预测与可视化系统，结合统计纹理分析和生成模型，提供可解释的污染场景可视化


<details>
  <summary>Details</summary>
Motivation: 传统空气质量监测系统空间覆盖有限且可访问性差，需要开发更直观、透明的空气质量可视化方法

Method: 结合统计纹理分析和监督学习进行污染分类，利用视觉语言模型引导的图像生成技术合成污染场景可视化

Result: 在城市场景数据集上验证了污染水平估计和语义一致视觉合成的有效性

Conclusion: 该系统为面向用户的智能应用提供了基础，未来将集成绿色CNN架构和FPGA增量学习以实现实时边缘计算部署

Abstract: Air pollution remains a critical threat to public health and environmental sustainability, yet conventional monitoring systems are often constrained by limited spatial coverage and accessibility. This paper proposes an AI-driven agent that predicts ambient air pollution levels from sky images and synthesizes realistic visualizations of pollution scenarios using generative modeling. Our approach combines statistical texture analysis with supervised learning for pollution classification, and leverages vision-language model (VLM)-guided image generation to produce interpretable representations of air quality conditions. The generated visuals simulate varying degrees of pollution, offering a foundation for user-facing interfaces that improve transparency and support informed environmental decision-making. These outputs can be seamlessly integrated into intelligent applications aimed at enhancing situational awareness and encouraging behavioral responses based on real-time forecasts. We validate our method using a dataset of urban sky images and demonstrate its effectiveness in both pollution level estimation and semantically consistent visual synthesis. The system design further incorporates human-centered user experience principles to ensure accessibility, clarity, and public engagement in air quality forecasting. To support scalable and energy-efficient deployment, future iterations will incorporate a green CNN architecture enhanced with FPGA-based incremental learning, enabling real-time inference on edge platforms.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [27] [Learning Mechanistic Subtypes of Neurodegeneration with a Physics-Informed Variational Autoencoder Mixture Model](https://arxiv.org/abs/2509.15124)
*Sanduni Pinnawala,Annabelle Hartanto,Ivor J. A. Simpson,Peter A. Wijeratne*

Main category: eess.IV

TL;DR: 一种深度生成模型，通过结合偏微方程物理知识和变分自动编码器混合模型，从神经影像数据中学习多种潜在动态模型混合，用于发现神经退行性疾病的机制子型


<details>
  <summary>Details</summary>
Motivation: 现有物理集成机器学习方法只考虑单个偏微方程，无法处理神经退行性疾病中多重机制导致的异质性动态，存在模型误规范和退化问题

Method: 将反应-扩散偏微方程集成到变分自动编码器混合模型框架中，支持从神经影像数据推断可解释潜变量（如扩散率和反应速率）的子型

Result: 在合成数据集上评估了方法的性能，并展示了其从正电子发射断展图数据中发现阿尔茫海默病进展机制子型的潜力

Conclusion: 该方法能够超越传统单一偏微方程假设，通过学习多种物理基础的动态模型混合，为神经退行性疾病的异质性动态模拟提供了更好的解释和应用价值

Abstract: Modelling the underlying mechanisms of neurodegenerative diseases demands methods that capture heterogeneous and spatially varying dynamics from sparse, high-dimensional neuroimaging data. Integrating partial differential equation (PDE) based physics knowledge with machine learning provides enhanced interpretability and utility over classic numerical methods. However, current physics-integrated machine learning methods are limited to considering a single PDE, severely limiting their application to diseases where multiple mechanisms are responsible for different groups (i.e., subtypes) and aggravating problems with model misspecification and degeneracy. Here, we present a deep generative model for learning mixtures of latent dynamic models governed by physics-based PDEs, going beyond traditional approaches that assume a single PDE structure. Our method integrates reaction-diffusion PDEs within a variational autoencoder (VAE) mixture model framework, supporting inference of subtypes of interpretable latent variables (e.g. diffusivity and reaction rates) from neuroimaging data. We evaluate our method on synthetic benchmarks and demonstrate its potential for uncovering mechanistic subtypes of Alzheimer's disease progression from positron emission tomography (PET) data.

</details>
