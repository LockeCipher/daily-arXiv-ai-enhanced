<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 6]
- [cs.CV](#cs.CV) [Total: 62]
- [eess.IV](#eess.IV) [Total: 8]
- [cs.LG](#cs.LG) [Total: 2]
- [math.OC](#math.OC) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [VoteSplat: Hough Voting Gaussian Splatting for 3D Scene Understanding](https://arxiv.org/abs/2506.22799)
*Minchao Jiang,Shunyu Jia,Jiaming Gu,Xiaoyuan Lu,Guangming Zhu,Anqi Dong,Liang Zhang*

Main category: cs.GR

TL;DR: VoteSplat结合3D高斯泼溅与霍夫投票，提升3D场景理解，降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法缺乏深度场景理解且训练成本高。

Method: 利用SAM实例分割生成2D投票图，嵌入空间偏移向量至高斯基元，结合深度约束优化定位。

Result: 在开放词汇3D实例定位、点云理解等任务中表现优异。

Conclusion: VoteSplat高效且多功能，适用于多种3D场景理解任务。

Abstract: 3D Gaussian Splatting (3DGS) has become horsepower in high-quality, real-time rendering for novel view synthesis of 3D scenes. However, existing methods focus primarily on geometric and appearance modeling, lacking deeper scene understanding while also incurring high training costs that complicate the originally streamlined differentiable rendering pipeline. To this end, we propose VoteSplat, a novel 3D scene understanding framework that integrates Hough voting with 3DGS. Specifically, Segment Anything Model (SAM) is utilized for instance segmentation, extracting objects, and generating 2D vote maps. We then embed spatial offset vectors into Gaussian primitives. These offsets construct 3D spatial votes by associating them with 2D image votes, while depth distortion constraints refine localization along the depth axis. For open-vocabulary object localization, VoteSplat maps 2D image semantics to 3D point clouds via voting points, reducing training costs associated with high-dimensional CLIP features while preserving semantic unambiguity. Extensive experiments demonstrate effectiveness of VoteSplat in open-vocabulary 3D instance localization, 3D point cloud understanding, click-based 3D object localization, hierarchical segmentation, and ablation studies. Our code is available at https://sy-ja.github.io/votesplat/

</details>


### [2] [DOBB-BVH: Efficient Ray Traversal by Transforming Wide BVHs into Oriented Bounding Box Trees using Discrete Rotations](https://arxiv.org/abs/2506.22849)
*Michael A. Kern,Alain Galvan,David Oldcorn,Daniel Skinner,Rohan Mehalwal,Leo Reyes Lozano,Matthäus G. Chajdas*

Main category: cs.GR

TL;DR: 提出了一种新的OBB构建技术，通过固定离散旋转集实现高效编码和低计算复杂度，显著提升光线追踪性能。


<details>
  <summary>Details</summary>
Motivation: 在薄长和任意旋转几何场景中，OBB比AABB更精确，但传统方法计算成本高且内存需求大。

Method: 采用固定离散旋转集统一内部节点的OBB变换，结合k-DOPs扩展至多子节点层次结构，作为后处理步骤集成到现有流程。

Result: 构建时间增加12.6%，但主光线性能提升18.5%，次光线提升32.4%，最大光线相交性能提升65%。

Conclusion: 该方法在实时应用中显著提升光线追踪性能，具有实际应用潜力。

Abstract: Oriented bounding box (OBB) bounding volume hierarchies offer a more precise fit than axis-aligned bounding box hierarchies in scenarios with thin elongated and arbitrarily rotated geometry, enhancing intersection test performance in ray tracing. However, determining optimally oriented bounding boxes can be computationally expensive and have high memory requirements. Recent research has shown that pre-built hierarchies can be efficiently converted to OBB hierarchies on the GPU in a bottom-up pass, yielding significant ray tracing traversal improvements. In this paper, we introduce a novel OBB construction technique where all internal node children share a consistent OBB transform, chosen from a fixed set of discrete quantized rotations. This allows for efficient encoding and reduces the computational complexity of OBB transformations. We further extend our approach to hierarchies with multiple children per node by leveraging Discrete Orientation Polytopes (k-DOPs), demonstrating improvements in traversal performance while limiting the build time impact for real-time applications. Our method is applied as a post-processing step, integrating seamlessly into existing hierarchy construction pipelines. Despite a 12.6% increase in build time, our experimental results demonstrate an average improvement of 18.5% in primary, 32.4% in secondary rays, and maximum gain of 65% in ray intersection performance, highlighting its potential for advancing real-time applications.

</details>


### [3] [Confident Splatting: Confidence-Based Compression of 3D Gaussian Splatting via Learnable Beta Distributions](https://arxiv.org/abs/2506.22973)
*AmirHossein Naghi Razlighi,Elaheh Badali Golezani,Shohreh Kasaei*

Main category: cs.GR

TL;DR: 提出一种基于可学习置信分数的3D高斯泼溅压缩方法，通过优化置信分数实现高效压缩和视觉保真。


<details>
  <summary>Details</summary>
Motivation: 解决3D高斯泼溅技术中因生成数百万个泼溅点导致的存储和计算开销过高的问题。

Method: 提出一种基于Beta分布的可学习置信分数模型，通过重建感知损失优化每个泼溅点的置信分数，实现低置信度泼溅点的剪枝。

Result: 实验表明，该方法在压缩和保真度之间取得了优于先前工作的平衡。

Conclusion: 该方法架构无关，适用于任何高斯泼溅变体，同时置信平均值可作为场景质量的新评估指标。

Abstract: 3D Gaussian Splatting enables high-quality real-time rendering but often produces millions of splats, resulting in excessive storage and computational overhead. We propose a novel lossy compression method based on learnable confidence scores modeled as Beta distributions. Each splat's confidence is optimized through reconstruction-aware losses, enabling pruning of low-confidence splats while preserving visual fidelity. The proposed approach is architecture-agnostic and can be applied to any Gaussian Splatting variant. In addition, the average confidence values serve as a new metric to assess the quality of the scene. Extensive experiments demonstrate favorable trade-offs between compression and fidelity compared to prior work. Our code and data are publicly available at https://github.com/amirhossein-razlighi/Confident-Splatting

</details>


### [4] [GaVS: 3D-Grounded Video Stabilization via Temporally-Consistent Local Reconstruction and Rendering](https://arxiv.org/abs/2506.23957)
*Zinuo You,Stamatios Georgoulis,Anpei Chen,Siyu Tang,Dengxin Dai*

Main category: cs.GR

TL;DR: 论文提出了一种基于3D的视频稳定方法GaVS，通过局部重建和渲染范式解决现有方法的几何失真、过度裁剪和泛化性差等问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频稳定方法存在几何失真、过度裁剪和泛化性差等问题，影响用户体验。

Method: 采用3D相机姿态，通过高斯散射基元预测和测试时微调，结合多视角动态感知光度监督和跨帧正则化，实现时间一致的局部重建和渲染。

Result: 在多样化的相机运动和场景动态数据集上，GaVS在传统任务指标和几何一致性上优于现有2D和2.5D方法。

Conclusion: GaVS在定量和定性评估中均表现出色，用户研究验证了其优越性。

Abstract: Video stabilization is pivotal for video processing, as it removes unwanted shakiness while preserving the original user motion intent. Existing approaches, depending on the domain they operate, suffer from several issues (e.g. geometric distortions, excessive cropping, poor generalization) that degrade the user experience. To address these issues, we introduce \textbf{GaVS}, a novel 3D-grounded approach that reformulates video stabilization as a temporally-consistent `local reconstruction and rendering' paradigm. Given 3D camera poses, we augment a reconstruction model to predict Gaussian Splatting primitives, and finetune it at test-time, with multi-view dynamics-aware photometric supervision and cross-frame regularization, to produce temporally-consistent local reconstructions. The model are then used to render each stabilized frame. We utilize a scene extrapolation module to avoid frame cropping. Our method is evaluated on a repurposed dataset, instilled with 3D-grounded information, covering samples with diverse camera motions and scene dynamics. Quantitatively, our method is competitive with or superior to state-of-the-art 2D and 2.5D approaches in terms of conventional task metrics and new geometry consistency. Qualitatively, our method produces noticeably better results compared to alternatives, validated by the user study.

</details>


### [5] [Navigating with Annealing Guidance Scale in Diffusion Space](https://arxiv.org/abs/2506.24108)
*Shai Yehezkel,Omer Dahary,Andrey Voynov,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: 提出了一种动态调整指导尺度的退火指导调度器，显著提升了文本到图像生成的质量和提示对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的无分类器指导（CFG）在采样过程中对指导尺度的选择敏感，影响图像质量和提示对齐。

Method: 通过基于条件噪声信号动态调整指导尺度的退火指导调度器，学习调度策略。

Result: 实验结果表明，该方法显著提升了图像质量和文本提示对齐，且无需额外激活或内存消耗。

Conclusion: 提出的调度器可无缝替代常见的无分类器指导，优化了提示对齐与质量之间的权衡。

Abstract: Denoising diffusion models excel at generating high-quality images conditioned on text prompts, yet their effectiveness heavily relies on careful guidance during the sampling process. Classifier-Free Guidance (CFG) provides a widely used mechanism for steering generation by setting the guidance scale, which balances image quality and prompt alignment. However, the choice of the guidance scale has a critical impact on the convergence toward a visually appealing and prompt-adherent image. In this work, we propose an annealing guidance scheduler which dynamically adjusts the guidance scale over time based on the conditional noisy signal. By learning a scheduling policy, our method addresses the temperamental behavior of CFG. Empirical results demonstrate that our guidance scheduler significantly enhances image quality and alignment with the text prompt, advancing the performance of text-to-image generation. Notably, our novel scheduler requires no additional activations or memory consumption, and can seamlessly replace the common classifier-free guidance, offering an improved trade-off between prompt alignment and quality.

</details>


### [6] [ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded Scenes](https://arxiv.org/abs/2506.21629)
*Chenhao Zhang,Yezhi Shen,Fengqing Zhu*

Main category: cs.GR

TL;DR: 提出了一种结合ICP与优化细化的大规模场景相机姿态估计方法，并引入体素场景稠密化技术，显著提升了NeRF和3DGS在户外场景中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有神经渲染方法（如NeRF和3DGS）依赖预处理相机姿态和3D结构先验的问题，特别是在户外场景中难以获取这些数据。

Method: 结合ICP与优化细化进行相机姿态估计，并采用体素场景稠密化技术指导大规模场景重建。

Result: 实验表明，ICP-3DGS在相机姿态估计和新视角合成方面优于现有方法，适用于不同规模的室内外场景。

Conclusion: 提出的方法有效解决了大规模场景中相机姿态估计的挑战，并提升了神经渲染的性能。

Abstract: In recent years, neural rendering methods such as NeRFs and 3D Gaussian Splatting (3DGS) have made significant progress in scene reconstruction and novel view synthesis. However, they heavily rely on preprocessed camera poses and 3D structural priors from structure-from-motion (SfM), which are challenging to obtain in outdoor scenarios. To address this challenge, we propose to incorporate Iterative Closest Point (ICP) with optimization-based refinement to achieve accurate camera pose estimation under large camera movements. Additionally, we introduce a voxel-based scene densification approach to guide the reconstruction in large-scale scenes. Experiments demonstrate that our approach ICP-3DGS outperforms existing methods in both camera pose estimation and novel view synthesis across indoor and outdoor scenes of various scales. Source code is available at https://github.com/Chenhao-Z/ICP-3DGS.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [7] [Neural Cellular Automata: From Cells to Pixels](https://arxiv.org/abs/2506.22899)
*Ehsan Pajouheshgar,Yitao Xu,Ali Abbasi,Alexander Mordvintsev,Wenzel Jakob,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 论文提出了一种结合隐式解码器的神经细胞自动机（NCA）方法，解决了高分辨率网格下的训练和推理效率问题，实现了实时生成高清输出。


<details>
  <summary>Details</summary>
Motivation: NCA在低分辨率网格上表现优异，但在高分辨率下因计算和内存需求剧增、信息传播受限而难以扩展。

Method: 通过引入轻量级隐式解码器，在粗网格上进行NCA演化后渲染任意分辨率图像，并设计了针对高分辨率输出的新型损失函数。

Result: 该方法显著提升了NCA在高分辨率下的质量、效率和性能，实现了实时生成全高清输出。

Conclusion: 结合隐式解码器的NCA框架能够高效扩展至高分辨率任务，同时保持自组织和涌现特性。

Abstract: Neural Cellular Automata (NCAs) are bio-inspired systems in which identical cells self-organize to form complex and coherent patterns by repeatedly applying simple local rules. NCAs display striking emergent behaviors including self-regeneration, generalization and robustness to unseen situations, and spontaneous motion. Despite their success in texture synthesis and morphogenesis, NCAs remain largely confined to low-resolution grids. This limitation stems from (1) training time and memory requirements that grow quadratically with grid size, (2) the strictly local propagation of information which impedes long-range cell communication, and (3) the heavy compute demands of real-time inference at high resolution. In this work, we overcome this limitation by pairing NCA with a tiny, shared implicit decoder, inspired by recent advances in implicit neural representations. Following NCA evolution on a coarse grid, a lightweight decoder renders output images at arbitrary resolution. We also propose novel loss functions for both morphogenesis and texture synthesis tasks, specifically tailored for high-resolution output with minimal memory and computation overhead. Combining our proposed architecture and loss functions brings substantial improvement in quality, efficiency, and performance. NCAs equipped with our implicit decoder can generate full-HD outputs in real time while preserving their self-organizing, emergent properties. Moreover, because each MLP processes cell states independently, inference remains highly parallelizable and efficient. We demonstrate the applicability of our approach across multiple NCA variants (on 2D, 3D grids, and 3D meshes) and multiple tasks, including texture generation and morphogenesis (growing patterns from a seed), showing that with our proposed framework, NCAs seamlessly scale to high-resolution outputs with minimal computational overhead.

</details>


### [8] [FreeDNA: Endowing Domain Adaptation of Diffusion-Based Dense Prediction with Training-Free Domain Noise Alignment](https://arxiv.org/abs/2506.22509)
*Hang Xu,Jie Huang,Linjiang Huang,Dong Li,Yidi Liu,Feng Zhao*

Main category: cs.CV

TL;DR: 提出了一种无需训练的领域适应方法（DNA），通过调整扩散采样过程中的噪声统计量，增强扩散密集预测模型的跨域性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在建模包含领域信息的分布转换中表现优异，但噪声统计偏差会导致领域偏移，因此需要一种方法来解决这一问题。

Method: 提出Domain Noise Alignment (DNA)方法，通过对齐目标域和源域的噪声统计量实现领域适应；在无源域情况下，利用高置信区域的统计量逐步调整噪声。

Result: 在四种密集预测任务中验证了DNA方法的有效性，显著提升了模型的领域适应能力。

Conclusion: DNA是一种高效且无需训练的领域适应方法，适用于扩散密集预测模型。

Abstract: Domain Adaptation(DA) for dense prediction tasks is an important topic, which enhances the dense prediction model's performance when tested on its unseen domain. Recently, with the development of Diffusion-based Dense Prediction (DDP) models, the exploration of DA designs tailored to this framework is worth exploring, since the diffusion model is effective in modeling the distribution transformation that comprises domain information. In this work, we propose a training-free mechanism for DDP frameworks, endowing them with DA capabilities. Our motivation arises from the observation that the exposure bias (e.g., noise statistics bias) in diffusion brings domain shift, and different domains in conditions of DDP models can also be effectively captured by the noise prediction statistics. Based on this, we propose a training-free Domain Noise Alignment (DNA) approach, which alleviates the variations of noise statistics to domain changes during the diffusion sampling process, thereby achieving domain adaptation. Specifically, when the source domain is available, we directly adopt the DNA method to achieve domain adaptation by aligning the noise statistics of the target domain with those of the source domain. For the more challenging source-free DA, inspired by the observation that regions closer to the source domain exhibit higher confidence meeting variations of sampling noise, we utilize the statistics from the high-confidence regions progressively to guide the noise statistic adjustment during the sampling process. Notably, our method demonstrates the effectiveness of enhancing the DA capability of DDP models across four common dense prediction tasks. Code is available at \href{https://github.com/xuhang07/FreeDNA}{https://github.com/xuhang07/FreeDNA}.

</details>


### [9] [HiNeuS: High-fidelity Neural Surface Mitigating Low-texture and Reflective Ambiguity](https://arxiv.org/abs/2506.23854)
*Yida Wang,Xueyang Zhang,Kun Zhan,Peng Jia,Xianpeng Lang*

Main category: cs.CV

TL;DR: HiNeuS是一个统一的神经表面重建框架，解决了多视角辐射不一致、无纹理区域关键点缺失和Eikonal约束过强导致的结构退化问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂场景下难以同时保证几何保真度和光度一致性，HiNeuS旨在通过统一框架解决这些核心问题。

Method: 1) 基于SDF的射线追踪解决反射模糊；2) 平面共形正则化保持局部表面一致性；3) 动态调整Eikonal约束以保留细节。

Result: 在合成和真实数据集上表现优异，Chamfer距离降低21.4%，PSNR提升2.32 dB，并能恢复高光物体和无纹理表面。

Conclusion: HiNeuS通过协同优化实现了几何与外观约束的统一，适用于逆向渲染任务，如材质分解和视角一致重光照。

Abstract: Neural surface reconstruction faces persistent challenges in reconciling geometric fidelity with photometric consistency under complex scene conditions. We present HiNeuS, a unified framework that holistically addresses three core limitations in existing approaches: multi-view radiance inconsistency, missing keypoints in textureless regions, and structural degradation from over-enforced Eikonal constraints during joint optimization. To resolve these issues through a unified pipeline, we introduce: 1) Differential visibility verification through SDF-guided ray tracing, resolving reflection ambiguities via continuous occlusion modeling; 2) Planar-conformal regularization via ray-aligned geometry patches that enforce local surface coherence while preserving sharp edges through adaptive appearance weighting; and 3) Physically-grounded Eikonal relaxation that dynamically modulates geometric constraints based on local radiance gradients, enabling detail preservation without sacrificing global regularity. Unlike prior methods that handle these aspects through sequential optimizations or isolated modules, our approach achieves cohesive integration where appearance-geometry constraints evolve synergistically throughout training. Comprehensive evaluations across synthetic and real-world datasets demonstrate state-of-the-art performance, including a 21.4% reduction in Chamfer distance over reflection-aware baselines and 2.32 dB PSNR improvement against neural rendering counterparts. Qualitative analyses reveal superior capability in recovering specular instruments, urban layouts with centimeter-scale infrastructure, and low-textured surfaces without local patch collapse. The method's generalizability is further validated through successful application to inverse rendering tasks, including material decomposition and view-consistent relighting.

</details>


### [10] [Preserve Anything: Controllable Image Synthesis with Object Preservation](https://arxiv.org/abs/2506.22531)
*Prasen Kumar Sharma,Neeraj Matiyali,Siddharth Srivastava,Gaurav Sharma*

Main category: cs.CV

TL;DR: 本文提出了一种名为“Preserve Anything”的新方法，用于解决文本到图像生成中的对象保留和语义一致性问题，通过N通道ControlNet实现多对象保留、语义对齐和场景控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多对象保留、语义对齐和场景控制方面存在不足，本文旨在解决这些问题。

Method: 采用N通道ControlNet，结合对象保留模块、背景引导模块和高频覆盖模块，实现多对象保留、语义一致性和用户控制。

Result: 方法在特征空间保真度（FID 15.26）和语义对齐（CLIP-S 32.85）上达到最优性能，用户研究显示显著提升。

Conclusion: 该方法在多对象保留、语义一致性和用户控制方面表现优异，显著优于现有方法。

Abstract: We introduce \textit{Preserve Anything}, a novel method for controlled image synthesis that addresses key limitations in object preservation and semantic consistency in text-to-image (T2I) generation. Existing approaches often fail (i) to preserve multiple objects with fidelity, (ii) maintain semantic alignment with prompts, or (iii) provide explicit control over scene composition. To overcome these challenges, the proposed method employs an N-channel ControlNet that integrates (i) object preservation with size and placement agnosticism, color and detail retention, and artifact elimination, (ii) high-resolution, semantically consistent backgrounds with accurate shadows, lighting, and prompt adherence, and (iii) explicit user control over background layouts and lighting conditions. Key components of our framework include object preservation and background guidance modules, enforcing lighting consistency and a high-frequency overlay module to retain fine details while mitigating unwanted artifacts. We introduce a benchmark dataset consisting of 240K natural images filtered for aesthetic quality and 18K 3D-rendered synthetic images with metadata such as lighting, camera angles, and object relationships. This dataset addresses the deficiencies of existing benchmarks and allows a complete evaluation. Empirical results demonstrate that our method achieves state-of-the-art performance, significantly improving feature-space fidelity (FID 15.26) and semantic alignment (CLIP-S 32.85) while maintaining competitive aesthetic quality. We also conducted a user study to demonstrate the efficacy of the proposed work on unseen benchmark and observed a remarkable improvement of $\sim25\%$, $\sim19\%$, $\sim13\%$, and $\sim14\%$ in terms of prompt alignment, photorealism, the presence of AI artifacts, and natural aesthetics over existing works.

</details>


### [11] [LightBSR: Towards Lightweight Blind Super-Resolution via Discriminative Implicit Degradation Representation Learning](https://arxiv.org/abs/2506.22710)
*Jiang Yuan,JI Ma,Bo Wang,Guanzhou Ke,Weiming Hu*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的盲超分辨率模型LightBSR，通过优化隐式退化表示（IDR）的区分性，采用知识蒸馏框架提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了IDR区分性对盲超分辨率的重要性，且过度复杂化适应过程，导致模型参数和计算量显著增加。

Method: 使用知识蒸馏框架，包括退化先验约束对比学习技术和特征对齐技术，以提升IDR区分性。

Result: LightBSR在多种盲超分辨率任务中表现出色，且复杂度极低。

Conclusion: 优化IDR区分性可显著提升盲超分辨率模型的性能，LightBSR是一种高效且轻量的解决方案。

Abstract: Implicit degradation estimation-based blind super-resolution (IDE-BSR) hinges on extracting the implicit degradation representation (IDR) of the LR image and adapting it to LR image features to guide HR detail restoration. Although IDE-BSR has shown potential in dealing with noise interference and complex degradations, existing methods ignore the importance of IDR discriminability for BSR and instead over-complicate the adaptation process to improve effect, resulting in a significant increase in the model's parameters and computations. In this paper, we focus on the discriminability optimization of IDR and propose a new powerful and lightweight BSR model termed LightBSR. Specifically, we employ a knowledge distillation-based learning framework. We first introduce a well-designed degradation-prior-constrained contrastive learning technique during teacher stage to make the model more focused on distinguishing different degradation types. Then we utilize a feature alignment technique to transfer the degradation-related knowledge acquired by the teacher to the student for practical inferencing. Extensive experiments demonstrate the effectiveness of IDR discriminability-driven BSR model design. The proposed LightBSR can achieve outstanding performance with minimal complexity across a range of blind SR tasks. Our code is accessible at: https://github.com/MJ-NCEPU/LightBSR.

</details>


### [12] [UniFuse: A Unified All-in-One Framework for Multi-Modal Medical Image Fusion Under Diverse Degradations and Misalignments](https://arxiv.org/abs/2506.22736)
*Dayong Su,Yafei Zhang,Huafeng Li,Jinxing Li,Yu Liu*

Main category: cs.CV

TL;DR: UniFuse是一个通用的多模态医学图像融合框架，通过嵌入退化感知提示学习模块和Omni统一特征表示方案，实现了对齐、恢复和融合的联合优化。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在处理未对齐或退化医学图像时效果下降的问题。

Method: 提出UniFuse框架，包括退化感知提示学习模块、Omni统一特征表示方案和通用特征恢复与融合模块。

Result: 实验结果表明，UniFuse在多数据集上优于现有方法。

Conclusion: UniFuse成功地将对齐、恢复和融合统一在一个框架中，显著提升了性能。

Abstract: Current multimodal medical image fusion typically assumes that source images are of high quality and perfectly aligned at the pixel level. Its effectiveness heavily relies on these conditions and often deteriorates when handling misaligned or degraded medical images. To address this, we propose UniFuse, a general fusion framework. By embedding a degradation-aware prompt learning module, UniFuse seamlessly integrates multi-directional information from input images and correlates cross-modal alignment with restoration, enabling joint optimization of both tasks within a unified framework. Additionally, we design an Omni Unified Feature Representation scheme, which leverages Spatial Mamba to encode multi-directional features and mitigate modality differences in feature alignment. To enable simultaneous restoration and fusion within an All-in-One configuration, we propose a Universal Feature Restoration & Fusion module, incorporating the Adaptive LoRA Synergistic Network (ALSN) based on LoRA principles. By leveraging ALSN's adaptive feature representation along with degradation-type guidance, we enable joint restoration and fusion within a single-stage framework. Compared to staged approaches, UniFuse unifies alignment, restoration, and fusion within a single framework. Experimental results across multiple datasets demonstrate the method's effectiveness and significant advantages over existing approaches.

</details>


### [13] [RoboPearls: Editable Video Simulation for Robot Manipulation](https://arxiv.org/abs/2506.22756)
*Tao Tang,Likui Zhang,Youpeng Wen,Kaidong Zhang,Jia-Wang Bian,xia zhou,Tianyi Yan,Kun Zhan,Peng Jia,Hefeng Wu,Liang Lin,Xiaodan Liang*

Main category: cs.CV

TL;DR: RoboPearls是一个基于3D高斯散射的可编辑视频仿真框架，用于机器人操作，通过自动化仿真生产和性能增强解决现实数据收集的挑战。


<details>
  <summary>Details</summary>
Motivation: 现实世界演示数据收集成本高且效率低，仿真平台虽提供可控环境，但存在仿真与现实的差距问题。

Method: RoboPearls利用3D高斯散射构建逼真仿真，结合增量语义蒸馏和3D正则化NNFM损失，并通过大语言模型和视觉语言模型自动化仿真生产与分析。

Result: 在多个数据集和场景（如RLBench、COLOSSEUM等）上实验验证了RoboPearls的有效性和仿真性能。

Conclusion: RoboPearls通过自动化仿真和性能增强，为解决机器人操作中的数据收集和仿真差距问题提供了有效方案。

Abstract: The development of generalist robot manipulation policies has seen significant progress, driven by large-scale demonstration data across diverse environments. However, the high cost and inefficiency of collecting real-world demonstrations hinder the scalability of data acquisition. While existing simulation platforms enable controlled environments for robotic learning, the challenge of bridging the sim-to-real gap remains. To address these challenges, we propose RoboPearls, an editable video simulation framework for robotic manipulation. Built on 3D Gaussian Splatting (3DGS), RoboPearls enables the construction of photo-realistic, view-consistent simulations from demonstration videos, and supports a wide range of simulation operators, including various object manipulations, powered by advanced modules like Incremental Semantic Distillation (ISD) and 3D regularized NNFM Loss (3D-NNFM). Moreover, by incorporating large language models (LLMs), RoboPearls automates the simulation production process in a user-friendly manner through flexible command interpretation and execution. Furthermore, RoboPearls employs a vision-language model (VLM) to analyze robotic learning issues to close the simulation loop for performance enhancement. To demonstrate the effectiveness of RoboPearls, we conduct extensive experiments on multiple datasets and scenes, including RLBench, COLOSSEUM, Ego4D, Open X-Embodiment, and a real-world robot, which demonstrate our satisfactory simulation performance.

</details>


### [14] [VSRM: A Robust Mamba-Based Framework for Video Super-Resolution](https://arxiv.org/abs/2506.22762)
*Dinh Phu Tran,Dao Duy Hung,Daeyoung Kim*

Main category: cs.CV

TL;DR: VSRM提出了一种基于Mamba的视频超分辨率框架，通过空间到时间和时间到空间的Mamba块提取长距离时空特征，并结合可变形交叉Mamba对齐模块和频率损失函数，实现了高效且高质量的图像重建。


<details>
  <summary>Details</summary>
Motivation: 现有CNN和Transformer方法在视频超分辨率任务中存在局部感受野限制或二次复杂度问题，而Mamba因其长序列建模能力和线性复杂度成为潜在解决方案。

Method: VSRM采用空间到时间和时间到空间的Mamba块提取特征，引入可变形交叉Mamba对齐模块动态补偿帧间对齐，并使用频率Charbonnier-like损失减少频域差距。

Result: VSRM在多个基准测试中达到最先进水平，验证了其高效性和高质量重建能力。

Conclusion: VSRM为视频超分辨率任务提供了新思路，结合Mamba的优势，为未来研究奠定了坚实基础。

Abstract: Video super-resolution remains a major challenge in low-level vision tasks. To date, CNN- and Transformer-based methods have delivered impressive results. However, CNNs are limited by local receptive fields, while Transformers struggle with quadratic complexity, posing challenges for processing long sequences in VSR. Recently, Mamba has drawn attention for its long-sequence modeling, linear complexity, and large receptive fields. In this work, we propose VSRM, a novel \textbf{V}ideo \textbf{S}uper-\textbf{R}esolution framework that leverages the power of \textbf{M}amba. VSRM introduces Spatial-to-Temporal Mamba and Temporal-to-Spatial Mamba blocks to extract long-range spatio-temporal features and enhance receptive fields efficiently. To better align adjacent frames, we propose Deformable Cross-Mamba Alignment module. This module utilizes a deformable cross-mamba mechanism to make the compensation stage more dynamic and flexible, preventing feature distortions. Finally, we minimize the frequency domain gaps between reconstructed and ground-truth frames by proposing a simple yet effective Frequency Charbonnier-like loss that better preserves high-frequency content and enhances visual quality. Through extensive experiments, VSRM achieves state-of-the-art results on diverse benchmarks, establishing itself as a solid foundation for future research.

</details>


### [15] [RGE-GS: Reward-Guided Expansive Driving Scene Reconstruction via Diffusion Priors](https://arxiv.org/abs/2506.22800)
*Sicong Du,Jiarun Liu,Qifeng Chen,Hao-Xiang Chen,Tai-Jiang Mu,Sheng Yang*

Main category: cs.CV

TL;DR: RGE-GS是一种结合扩散生成和奖励引导高斯积分的新型扩展重建框架，解决了现有3D高斯泼溅技术中物理不一致和训练效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 单次驾驶片段常导致道路结构扫描不完整，需要扩展重建以支持传感器模拟器有效回归驾驶行为。现有3D高斯泼溅技术虽质量高，但直接结合扩散先验会引入物理不一致并降低效率。

Method: 提出RGE-GS框架，包含奖励网络（优先选择稳定生成模式）和差异化训练策略（根据场景收敛指标调整高斯优化进度）。

Result: 在公开数据集上，RGE-GS实现了最先进的重建质量。

Conclusion: RGE-GS通过奖励引导和差异化训练，显著提升了重建质量和训练效率。

Abstract: A single-pass driving clip frequently results in incomplete scanning of the road structure, making reconstructed scene expanding a critical requirement for sensor simulators to effectively regress driving actions. Although contemporary 3D Gaussian Splatting (3DGS) techniques achieve remarkable reconstruction quality, their direct extension through the integration of diffusion priors often introduces cumulative physical inconsistencies and compromises training efficiency. To address these limitations, we present RGE-GS, a novel expansive reconstruction framework that synergizes diffusion-based generation with reward-guided Gaussian integration. The RGE-GS framework incorporates two key innovations: First, we propose a reward network that learns to identify and prioritize consistently generated patterns prior to reconstruction phases, thereby enabling selective retention of diffusion outputs for spatial stability. Second, during the reconstruction process, we devise a differentiated training strategy that automatically adjust Gaussian optimization progress according to scene converge metrics, which achieving better convergence than baseline methods. Extensive evaluations of publicly available datasets demonstrate that RGE-GS achieves state-of-the-art performance in reconstruction quality. Our source-code will be made publicly available at https://github.com/CN-ADLab/RGE-GS. (Camera-ready version incorporating reviewer suggestions will be updated soon.)

</details>


### [16] [SemFaceEdit: Semantic Face Editing on Generative Radiance Manifolds](https://arxiv.org/abs/2506.22833)
*Shashikant Verma,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: SemFaceEdit是一种基于生成辐射流形的新方法，通过语义场实现面部图像的精确局部编辑，同时保持其他区域的完整性。


<details>
  <summary>Details</summary>
Motivation: 现有3D感知GAN技术虽然提供多视角一致性，但缺乏局部编辑能力，生成辐射流形为解决这一问题提供了高效途径。

Method: SemFaceEdit通过几何模块和外观模块联合训练，生成语义辐射和占用场，并利用潜在码解耦几何与外观。

Result: 实验表明，SemFaceEdit在语义场编辑和辐射场解耦方面表现优异，实现了精确的面部语义编辑。

Conclusion: SemFaceEdit通过语义场和潜在码的结合，显著提升了面部图像的局部编辑能力和解耦效果。

Abstract: Despite multiple view consistency offered by 3D-aware GAN techniques, the resulting images often lack the capacity for localized editing. In response, generative radiance manifolds emerge as an efficient approach for constrained point sampling within volumes, effectively reducing computational demands and enabling the learning of fine details. This work introduces SemFaceEdit, a novel method that streamlines the appearance and geometric editing process by generating semantic fields on generative radiance manifolds. Utilizing latent codes, our method effectively disentangles the geometry and appearance associated with different facial semantics within the generated image. In contrast to existing methods that can change the appearance of the entire radiance field, our method enables the precise editing of particular facial semantics while preserving the integrity of other regions. Our network comprises two key modules: the Geometry module, which generates semantic radiance and occupancy fields, and the Appearance module, which is responsible for predicting RGB radiance. We jointly train both modules in adversarial settings to learn semantic-aware geometry and appearance descriptors. The appearance descriptors are then conditioned on their respective semantic latent codes by the Appearance Module, facilitating disentanglement and enhanced control. Our experiments highlight SemFaceEdit's superior performance in semantic field-based editing, particularly in achieving improved radiance field disentanglement.

</details>


### [17] [DMD-Net: Deep Mesh Denoising Network](https://arxiv.org/abs/2506.22850)
*Aalok Gangopadhyay,Shashikant Verma,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: DMD-Net是一种端到端的深度学习框架，用于网格去噪，结合了图卷积神经网络和双流网络，通过特征引导变换器实现高效去噪。


<details>
  <summary>Details</summary>
Motivation: 解决网格去噪问题，提升在复杂噪声环境下的性能。

Method: 使用图卷积神经网络和双流网络，结合特征提取器、变换器和去噪器。

Result: 在大规模数据集上表现优异，优于现有方法，对高噪声具有鲁棒性。

Conclusion: DMD-Net在网格去噪中表现出色，适用于各种噪声场景。

Abstract: We present Deep Mesh Denoising Network (DMD-Net), an end-to-end deep learning framework, for solving the mesh denoising problem. DMD-Net consists of a Graph Convolutional Neural Network in which aggregation is performed in both the primal as well as the dual graph. This is realized in the form of an asymmetric two-stream network, which contains a primal-dual fusion block that enables communication between the primal-stream and the dual-stream. We develop a Feature Guided Transformer (FGT) paradigm, which consists of a feature extractor, a transformer, and a denoiser. The feature extractor estimates the local features, that guide the transformer to compute a transformation, which is applied to the noisy input mesh to obtain a useful intermediate representation. This is further processed by the denoiser to obtain the denoised mesh. Our network is trained on a large scale dataset of 3D objects. We perform exhaustive ablation studies to demonstrate that each component in our network is essential for obtaining the best performance. We show that our method obtains competitive or better results when compared with the state-of-the-art mesh denoising algorithms. We demonstrate that our method is robust to various kinds of noise. We observe that even in the presence of extremely high noise, our method achieves excellent performance.

</details>


### [18] [Revisiting CroPA: A Reproducibility Study and Enhancements for Cross-Prompt Adversarial Transferability in Vision-Language Models](https://arxiv.org/abs/2506.22982)
*Atharv Mittal,Agam Pandey,Amritanshu Tiwari,Sukrit Jindal,Swadesh Swain*

Main category: cs.CV

TL;DR: 本文研究了大型视觉语言模型（VLMs）对抗攻击的脆弱性，验证了跨提示攻击（CroPA）的优越性，并提出改进方法以提高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在计算机视觉任务中表现优异，但对对抗攻击高度脆弱，尤其是在多模态被操纵的场景下。本文旨在验证和改进跨提示攻击方法。

Method: 1. 提出新颖的初始化策略提高攻击成功率；2. 研究跨图像可转移性；3. 设计针对视觉编码器注意力机制的新损失函数。

Result: 改进方法在多个VLMs（如Flamingo、BLIP-2等）上验证了原始结果，并显著提升了对抗攻击效果。

Conclusion: 研究强调了VLMs对抗漏洞的重要性，提供了更鲁棒的对抗样本生成框架，对实际应用中的安全性有重要意义。

Abstract: Large Vision-Language Models (VLMs) have revolutionized computer vision, enabling tasks such as image classification, captioning, and visual question answering. However, they remain highly vulnerable to adversarial attacks, particularly in scenarios where both visual and textual modalities can be manipulated. In this study, we conduct a comprehensive reproducibility study of "An Image is Worth 1000 Lies: Adversarial Transferability Across Prompts on Vision-Language Models" validating the Cross-Prompt Attack (CroPA) and confirming its superior cross-prompt transferability compared to existing baselines. Beyond replication we propose several key improvements: (1) A novel initialization strategy that significantly improves Attack Success Rate (ASR). (2) Investigate cross-image transferability by learning universal perturbations. (3) A novel loss function targeting vision encoder attention mechanisms to improve generalization. Our evaluation across prominent VLMs -- including Flamingo, BLIP-2, and InstructBLIP as well as extended experiments on LLaVA validates the original results and demonstrates that our improvements consistently boost adversarial effectiveness. Our work reinforces the importance of studying adversarial vulnerabilities in VLMs and provides a more robust framework for generating transferable adversarial examples, with significant implications for understanding the security of VLMs in real-world applications.

</details>


### [19] [Inpainting is All You Need: A Diffusion-based Augmentation Method for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2506.23038)
*Xinrong Hu,Yiyu Shi*

Main category: cs.CV

TL;DR: AugPaint是一种数据增强框架，利用潜在扩散模型通过修复生成图像-标签对，显著提升医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学数据集的像素级标签收集耗时且昂贵，如何在有限标注数据下提升分割性能是关键挑战。

Method: AugPaint利用潜在扩散模型进行修复，无需重新训练，生成与标签掩码匹配的高质量图像。

Result: 在四个公共医学图像分割数据集上，AugPaint优于现有方法，显著提升分割性能。

Conclusion: AugPaint通过生成高质量图像-标签对，有效解决了标注数据不足的问题。

Abstract: Collecting pixel-level labels for medical datasets can be a laborious and expensive process, and enhancing segmentation performance with a scarcity of labeled data is a crucial challenge. This work introduces AugPaint, a data augmentation framework that utilizes inpainting to generate image-label pairs from limited labeled data. AugPaint leverages latent diffusion models, known for their ability to generate high-quality in-domain images with low overhead, and adapts the sampling process for the inpainting task without need for retraining. Specifically, given a pair of image and label mask, we crop the area labeled with the foreground and condition on it during reversed denoising process for every noise level. Masked background area would gradually be filled in, and all generated images are paired with the label mask. This approach ensures the accuracy of match between synthetic images and label masks, setting it apart from existing dataset generation methods. The generated images serve as valuable supervision for training downstream segmentation models, effectively addressing the challenge of limited annotations. We conducted extensive evaluations of our data augmentation method on four public medical image segmentation datasets, including CT, MRI, and skin imaging. Results across all datasets demonstrate that AugPaint outperforms state-of-the-art label-efficient methodologies, significantly improving segmentation performance.

</details>


### [20] [From Coarse to Fine: Learnable Discrete Wavelet Transforms for Efficient 3D Gaussian Splatting](https://arxiv.org/abs/2506.23042)
*Hung Nguyen,An Le,Runfa Li,Truong Nguyen*

Main category: cs.CV

TL;DR: AutoOpti3DGS通过小波变换控制高斯增殖，提升3D高斯溅射的内存效率，同时保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: 解决3D高斯溅射方法中高斯基元数量增长导致的内存和带宽压力问题。

Method: 采用可学习的正向和逆向离散小波变换，固定低通滤波器，可学习高通滤波器，并通过正交性损失逐步激活高频细节。

Result: AutoOpti3DGS仅需一个超参数，生成更稀疏的场景表示，适用于内存受限的硬件。

Conclusion: AutoOpti3DGS有效平衡了视觉质量与内存效率，适用于资源受限的环境。

Abstract: 3D Gaussian Splatting has emerged as a powerful approach in novel view synthesis, delivering rapid training and rendering but at the cost of an ever-growing set of Gaussian primitives that strains memory and bandwidth. We introduce AutoOpti3DGS, a training-time framework that automatically restrains Gaussian proliferation without sacrificing visual fidelity. The key idea is to feed the input images to a sequence of learnable Forward and Inverse Discrete Wavelet Transforms, where low-pass filters are kept fixed, high-pass filters are learnable and initialized to zero, and an auxiliary orthogonality loss gradually activates fine frequencies. This wavelet-driven, coarse-to-fine process delays the formation of redundant fine Gaussians, allowing 3DGS to capture global structure first and refine detail only when necessary. Through extensive experiments, AutoOpti3DGS requires just a single filter learning-rate hyper-parameter, integrates seamlessly with existing efficient 3DGS frameworks, and consistently produces sparser scene representations more compatible with memory or storage-constrained hardware.

</details>


### [21] [Ovis-U1 Technical Report](https://arxiv.org/abs/2506.23044)
*Guo-Hua Wang,Shanshan Zhao,Xinjie Zhang,Liangfu Cao,Pengxin Zhan,Lunhao Duan,Shiyin Lu,Minghao Fu,Xiaohao Chen,Jianshan Zhao,Yang Li,Qing-Guo Chen*

Main category: cs.CV

TL;DR: Ovis-U1是一个30亿参数的多模态统一模型，集成了理解、生成和编辑能力，性能超越现有先进模型。


<details>
  <summary>Details</summary>
Motivation: 旨在通过统一训练方法提升多模态任务（理解、生成、编辑）的性能，突破现有模型的局限性。

Method: 采用扩散式视觉解码器和双向令牌精炼器，基于语言模型进行统一训练。

Result: 在OpenCompass、DPG-Bench等基准测试中表现优异，超越Ristretto-3B等模型。

Conclusion: Ovis-U1为多模态任务提供了高效统一的解决方案，推动了该领域的边界。

Abstract: In this report, we introduce Ovis-U1, a 3-billion-parameter unified model that integrates multimodal understanding, text-to-image generation, and image editing capabilities. Building on the foundation of the Ovis series, Ovis-U1 incorporates a diffusion-based visual decoder paired with a bidirectional token refiner, enabling image generation tasks comparable to leading models like GPT-4o. Unlike some previous models that use a frozen MLLM for generation tasks, Ovis-U1 utilizes a new unified training approach starting from a language model. Compared to training solely on understanding or generation tasks, unified training yields better performance, demonstrating the enhancement achieved by integrating these two tasks. Ovis-U1 achieves a score of 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent state-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In text-to-image generation, it excels with scores of 83.72 and 0.89 on the DPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves 4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the initial version of the Ovis unified model series, Ovis-U1 pushes the boundaries of multimodal understanding, generation, and editing.

</details>


### [22] [RoboScape: Physics-informed Embodied World Model](https://arxiv.org/abs/2506.23135)
*Yu Shang,Xin Zhang,Yinzhou Tang,Lei Jin,Chen Gao,Wei Wu,Yong Li*

Main category: cs.CV

TL;DR: RoboScape是一个物理信息统一的世界模型，通过联合学习RGB视频生成和物理知识，提升3D几何一致性和运动动态建模，生成更真实且物理合理的机器人场景视频。


<details>
  <summary>Details</summary>
Motivation: 当前的世界模型在物理感知方面存在局限，特别是在3D几何和运动动态建模上，导致接触丰富的机器人场景视频生成不真实。

Method: 提出RoboScape，通过两个关键物理信息联合训练任务：时间深度预测和关键点动态学习，增强视频渲染的3D几何一致性和复杂运动建模。

Result: 实验表明，RoboScape生成的视频在视觉保真度和物理合理性上优于现有方法，并在下游应用中验证了其实用性。

Conclusion: RoboScape为构建高效的物理信息世界模型提供了新思路，推动了具身智能研究的发展。

Abstract: World models have become indispensable tools for embodied intelligence, serving as powerful simulators capable of generating realistic robotic videos while addressing critical data scarcity challenges. However, current embodied world models exhibit limited physical awareness, particularly in modeling 3D geometry and motion dynamics, resulting in unrealistic video generation for contact-rich robotic scenarios. In this paper, we present RoboScape, a unified physics-informed world model that jointly learns RGB video generation and physics knowledge within an integrated framework. We introduce two key physics-informed joint training tasks: temporal depth prediction that enhances 3D geometric consistency in video rendering, and keypoint dynamics learning that implicitly encodes physical properties (e.g., object shape and material characteristics) while improving complex motion modeling. Extensive experiments demonstrate that RoboScape generates videos with superior visual fidelity and physical plausibility across diverse robotic scenarios. We further validate its practical utility through downstream applications including robotic policy training with generated data and policy evaluation. Our work provides new insights for building efficient physics-informed world models to advance embodied intelligence research. The code is available at: https://github.com/tsinghua-fib-lab/RoboScape.

</details>


### [23] [VisualPrompter: Prompt Optimization with Visual Feedback for Text-to-Image Synthesis](https://arxiv.org/abs/2506.23138)
*Shiyu Wu,Mingzhen Sun,Weining Wang,Yequan Wang,Jing Liu*

Main category: cs.CV

TL;DR: VisualPrompter是一个无需训练的提示工程框架，通过自动反思和细粒度优化提升文本到图像生成的语义对齐。


<details>
  <summary>Details</summary>
Motivation: 现有提示工程方法虽能提升图像风格和美观度，但常忽略生成图像与用户描述的语义对齐，导致内容不满意。

Method: 提出VisualPrompter框架，包含自动反思模块识别缺失概念，以及目标特定提示优化机制细粒度修订提示。

Result: 在多个文本-图像对齐评估基准上达到新最优性能，且框架设计为即插即用，适配多种生成模型。

Conclusion: VisualPrompter有效解决了语义对齐问题，提升了生成图像的内容满意度。

Abstract: Since there exists a notable gap between user-provided and model-preferred prompts, generating high-quality and satisfactory images using diffusion models often requires prompt engineering to optimize user inputs. Current studies on text-to-image prompt engineering can effectively enhance the style and aesthetics of generated images. However, they often neglect the semantic alignment between generated images and user descriptions, resulting in visually appealing but content-wise unsatisfying outputs. In this work, we propose VisualPrompter, a novel training-free prompt engineering framework that refines user inputs to model-preferred sentences. In particular, VisualPrompter utilizes an automatic self-reflection module to identify the missing concepts in generated images and a target-specific prompt optimization mechanism to revise the prompts in a fine-grained manner. Extensive experiments demonstrate the effectiveness of our VisualPrompter, which achieves new state-of-the-art performance on multiple benchmarks for text-image alignment evaluation. Additionally, our framework features a plug-and-play design, making it highly adaptable to various generative models.

</details>


### [24] [Dynamic View Synthesis from Small Camera Motion Videos](https://arxiv.org/abs/2506.23153)
*Huiqiang Sun,Xingyi Li,Juewen Peng,Liao Shen,Zhiguo Cao,Ke Xian,Guosheng Lin*

Main category: cs.CV

TL;DR: 论文提出了一种基于分布深度正则化（DDR）的方法，解决了动态3D场景中相机运动范围有限时的几何表示和相机参数估计问题。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF方法在相机运动范围有限时表现不佳，导致几何表示错误和相机参数估计不准确。

Method: 提出DDR方法，通过Gumbel-softmax采样和几何约束改进深度正则化，并引入相机参数学习。

Result: 实验表明，该方法在小相机运动输入下优于现有方法。

Conclusion: DDR方法有效解决了小相机运动场景下的几何和相机参数问题。

Abstract: Novel view synthesis for dynamic $3$D scenes poses a significant challenge. Many notable efforts use NeRF-based approaches to address this task and yield impressive results. However, these methods rely heavily on sufficient motion parallax in the input images or videos. When the camera motion range becomes limited or even stationary (i.e., small camera motion), existing methods encounter two primary challenges: incorrect representation of scene geometry and inaccurate estimation of camera parameters. These challenges make prior methods struggle to produce satisfactory results or even become invalid. To address the first challenge, we propose a novel Distribution-based Depth Regularization (DDR) that ensures the rendering weight distribution to align with the true distribution. Specifically, unlike previous methods that use depth loss to calculate the error of the expectation, we calculate the expectation of the error by using Gumbel-softmax to differentiably sample points from discrete rendering weight distribution. Additionally, we introduce constraints that enforce the volume density of spatial points before the object boundary along the ray to be near zero, ensuring that our model learns the correct geometry of the scene. To demystify the DDR, we further propose a visualization tool that enables observing the scene geometry representation at the rendering weight level. For the second challenge, we incorporate camera parameter learning during training to enhance the robustness of our model to camera parameters. We conduct extensive experiments to demonstrate the effectiveness of our approach in representing scenes with small camera motion input, and our results compare favorably to state-of-the-art methods.

</details>


### [25] [STD-GS: Exploring Frame-Event Interaction for SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic Scene](https://arxiv.org/abs/2506.23157)
*Hanyu Zhou,Haonan Wang,Haoyue Liu,Yuxing Duan,Luxin Yan,Gim Hee Lee*

Main category: cs.CV

TL;DR: 提出了一种时空解耦的高斯泼溅框架，用于高动态场景重建，通过事件相机补偿帧相机，解决时空特征不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法采用统一表示模型（如高斯）直接匹配动态场景的时空特征，但无法处理潜在的时间不连续性和背景与对象的异质性。

Method: 引入事件相机，提出时空解耦的高斯泼溅框架，通过聚类区分背景与对象的时空特征，并利用高斯表示与事件数据的一致性指导对象高斯解耦。

Result: 实验验证了方法的优越性，提高了背景与对象的时空区分能力，实现了时间连续的动态场景渲染。

Conclusion: 该方法通过时空解耦显著改善了高动态场景的重建效果。

Abstract: High-dynamic scene reconstruction aims to represent static background with rigid spatial features and dynamic objects with deformed continuous spatiotemporal features. Typically, existing methods adopt unified representation model (e.g., Gaussian) to directly match the spatiotemporal features of dynamic scene from frame camera. However, this unified paradigm fails in the potential discontinuous temporal features of objects due to frame imaging and the heterogeneous spatial features between background and objects. To address this issue, we disentangle the spatiotemporal features into various latent representations to alleviate the spatiotemporal mismatching between background and objects. In this work, we introduce event camera to compensate for frame camera, and propose a spatiotemporal-disentangled Gaussian splatting framework for high-dynamic scene reconstruction. As for dynamic scene, we figure out that background and objects have appearance discrepancy in frame-based spatial features and motion discrepancy in event-based temporal features, which motivates us to distinguish the spatiotemporal features between background and objects via clustering. As for dynamic object, we discover that Gaussian representations and event data share the consistent spatiotemporal characteristic, which could serve as a prior to guide the spatiotemporal disentanglement of object Gaussians. Within Gaussian splatting framework, the cumulative scene-object disentanglement can improve the spatiotemporal discrimination between background and objects to render the time-continuous dynamic scene. Extensive experiments have been performed to verify the superiority of the proposed method.

</details>


### [26] [BridgeShape: Latent Diffusion Schrödinger Bridge for 3D Shape Completion](https://arxiv.org/abs/2506.23205)
*Dequan Kong,Zhe Zhu,Honghua Chen,Mingqiang Wei*

Main category: cs.CV

TL;DR: BridgeShape提出了一种基于潜在扩散Schrödinger桥的3D形状补全框架，通过显式建模最优传输路径和紧凑的潜在空间编码，解决了现有方法在全局一致性和分辨率限制上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的3D形状补全方法通过深度特征交互注入不完整形状信息，但未能显式建模最优全局传输路径，且受限于体素空间的分辨率约束。

Method: BridgeShape将形状补全建模为最优传输问题，并引入深度增强的VQ-VAE编码3D形状到紧凑潜在空间，结合多视角深度信息和DINOv2特征增强几何感知。

Result: BridgeShape在大规模3D形状补全基准测试中达到最先进性能，支持更高分辨率和未见物体类别的补全。

Conclusion: BridgeShape通过潜在扩散Schrödinger桥和深度增强编码，显著提升了3D形状补全的全局一致性和细节生成能力。

Abstract: Existing diffusion-based 3D shape completion methods typically use a conditional paradigm, injecting incomplete shape information into the denoising network via deep feature interactions (e.g., concatenation, cross-attention) to guide sampling toward complete shapes, often represented by voxel-based distance functions. However, these approaches fail to explicitly model the optimal global transport path, leading to suboptimal completions. Moreover, performing diffusion directly in voxel space imposes resolution constraints, limiting the generation of fine-grained geometric details. To address these challenges, we propose BridgeShape, a novel framework for 3D shape completion via latent diffusion Schr\"odinger bridge. The key innovations lie in two aspects: (i) BridgeShape formulates shape completion as an optimal transport problem, explicitly modeling the transition between incomplete and complete shapes to ensure a globally coherent transformation. (ii) We introduce a Depth-Enhanced Vector Quantized Variational Autoencoder (VQ-VAE) to encode 3D shapes into a compact latent space, leveraging self-projected multi-view depth information enriched with strong DINOv2 features to enhance geometric structural perception. By operating in a compact yet structurally informative latent space, BridgeShape effectively mitigates resolution constraints and enables more efficient and high-fidelity 3D shape completion. BridgeShape achieves state-of-the-art performance on large-scale 3D shape completion benchmarks, demonstrating superior fidelity at higher resolutions and for unseen object classes.

</details>


### [27] [TVG-SLAM: Robust Gaussian Splatting SLAM with Tri-view Geometric Constraints](https://arxiv.org/abs/2506.23207)
*Zhen Tan,Xieyuanli Chen,Lei Feng,Yangbing Ge,Shuaifeng Zhi,Jiaxiong Liu,Dewen Hu*

Main category: cs.CV

TL;DR: TVG-SLAM是一种基于3D高斯泼溅的RGB-only SLAM系统，通过三视图几何范式提升跟踪和建图的鲁棒性，显著减少了轨迹误差。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-only SLAM系统依赖光度渲染损失，在无边界户外环境中因视角和光照变化导致鲁棒性不足。

Method: 提出三视图匹配模块和混合几何约束，结合几何与光度损失；引入概率初始化策略和动态渲染信任衰减机制。

Result: 在多个户外数据集上表现优异，最挑战性数据集的绝对轨迹误差降低69.0%，渲染质量达到SOTA。

Conclusion: TVG-SLAM通过几何增强显著提升了RGB-only SLAM的鲁棒性和渲染质量，代码将开源。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled RGB-only SLAM systems to achieve high-fidelity scene representation. However, the heavy reliance of existing systems on photometric rendering loss for camera tracking undermines their robustness, especially in unbounded outdoor environments with severe viewpoint and illumination changes. To address these challenges, we propose TVG-SLAM, a robust RGB-only 3DGS SLAM system that leverages a novel tri-view geometry paradigm to ensure consistent tracking and high-quality mapping. We introduce a dense tri-view matching module that aggregates reliable pairwise correspondences into consistent tri-view matches, forming robust geometric constraints across frames. For tracking, we propose Hybrid Geometric Constraints, which leverage tri-view matches to construct complementary geometric cues alongside photometric loss, ensuring accurate and stable pose estimation even under drastic viewpoint shifts and lighting variations. For mapping, we propose a new probabilistic initialization strategy that encodes geometric uncertainty from tri-view correspondences into newly initialized Gaussians. Additionally, we design a Dynamic Attenuation of Rendering Trust mechanism to mitigate tracking drift caused by mapping latency. Experiments on multiple public outdoor datasets show that our TVG-SLAM outperforms prior RGB-only 3DGS-based SLAM systems. Notably, in the most challenging dataset, our method improves tracking robustness, reducing the average Absolute Trajectory Error (ATE) by 69.0\% while achieving state-of-the-art rendering quality. The implementation of our method will be released as open-source.

</details>


### [28] [PixelBoost: Leveraging Brownian Motion for Realistic-Image Super-Resolution](https://arxiv.org/abs/2506.23254)
*Aradhana Mishra,Bumshik Lee*

Main category: cs.CV

TL;DR: PixelBoost是一种新型扩散模型，通过引入布朗运动的随机性提升图像超分辨率，兼顾真实性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在减少采样步骤时导致的图像模糊和真实性下降问题。

Method: 整合受控随机性到训练中，避免局部最优，并采用sigmoidal噪声排序方法简化训练。

Result: 在LPIPS、LOE、PSNR、SSIM等指标上表现优异，边缘重建能力更强，推理速度更快。

Conclusion: PixelBoost通过随机性和自适应学习，显著提升了图像超分辨率的真实性和效率。

Abstract: Diffusion-model-based image super-resolution techniques often face a trade-off between realistic image generation and computational efficiency. This issue is exacerbated when inference times by decreasing sampling steps, resulting in less realistic and hazy images. To overcome this challenge, we introduce a novel diffusion model named PixelBoost that underscores the significance of embracing the stochastic nature of Brownian motion in advancing image super-resolution, resulting in a high degree of realism, particularly focusing on texture and edge definitions. By integrating controlled stochasticity into the training regimen, our proposed model avoids convergence to local optima, effectively capturing and reproducing the inherent uncertainty of image textures and patterns. Our proposed model demonstrates superior objective results in terms of learned perceptual image patch similarity (LPIPS), lightness order error (LOE), peak signal-to-noise ratio(PSNR), structural similarity index measure (SSIM), as well as visual quality. To determine the edge enhancement, we evaluated the gradient magnitude and pixel value, and our proposed model exhibited a better edge reconstruction capability. Additionally, our model demonstrates adaptive learning capabilities by effectively adjusting to Brownian noise patterns and introduces a sigmoidal noise sequencing method that simplifies training, resulting in faster inference speeds.

</details>


### [29] [Causal-Entity Reflected Egocentric Traffic Accident Video Synthesis](https://arxiv.org/abs/2506.23263)
*Lei-lei Li,Jianwu Fang,Junbin Xiao,Shanmin Pang,Hongkai Yu,Chen Lv,Jianru Xue,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出了一种名为Causal-VidSyn的扩散模型，用于合成以自我为中心的交通事故视频，通过原因描述和驾驶员注视点识别事故参与者和行为，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为自动驾驶汽车的安全测试提供合成事故视频，以应对现实中难以承受的事故，但现有方法难以在合成视频中融入真实视频中的因果关系。

Method: 提出Causal-VidSyn扩散模型，利用原因描述和驾驶员注视点，通过事故原因回答和注视条件选择模块识别事故参与者和行为。

Result: Causal-VidSyn在帧质量和因果敏感性方面优于现有视频扩散模型，支持事故视频编辑、正常到事故视频扩散和文本到视频生成等任务。

Conclusion: Causal-VidSyn通过结合因果关系和驾驶员注视点，显著提升了合成事故视频的质量和实用性。

Abstract: Egocentricly comprehending the causes and effects of car accidents is crucial for the safety of self-driving cars, and synthesizing causal-entity reflected accident videos can facilitate the capability test to respond to unaffordable accidents in reality. However, incorporating causal relations as seen in real-world videos into synthetic videos remains challenging. This work argues that precisely identifying the accident participants and capturing their related behaviors are of critical importance. In this regard, we propose a novel diffusion model, Causal-VidSyn, for synthesizing egocentric traffic accident videos. To enable causal entity grounding in video diffusion, Causal-VidSyn leverages the cause descriptions and driver fixations to identify the accident participants and behaviors, facilitated by accident reason answering and gaze-conditioned selection modules. To support Causal-VidSyn, we further construct Drive-Gaze, the largest driver gaze dataset (with 1.54M frames of fixations) in driving accident scenarios. Extensive experiments show that Causal-VidSyn surpasses state-of-the-art video diffusion models in terms of frame quality and causal sensitivity in various tasks, including accident video editing, normal-to-accident video diffusion, and text-to-video generation.

</details>


### [30] [Autoregressive Denoising Score Matching is a Good Video Anomaly Detector](https://arxiv.org/abs/2506.23282)
*Hanwen Zhang,Congqi Cao,Qinyi Lv,Lingtong Min,Yanning Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于生成模型的视频异常检测方法，通过噪声条件评分变换器和场景依赖的运动感知评分函数，解决了传统方法对局部异常盲区的问题。


<details>
  <summary>Details</summary>
Motivation: 视频异常检测中，传统基于似然的方法无法检测到局部模式附近的异常。本文旨在解决这一问题，关注场景、运动和外观三个方面的独特差距。

Method: 1. 构建噪声条件评分变换器进行去噪评分匹配；2. 引入场景依赖和运动感知的评分函数；3. 使用自回归去噪评分匹配机制增强异常感知。

Result: 在三个流行的视频异常检测基准测试中，该方法达到了最先进的性能。

Conclusion: 通过综合考虑场景、运动和外观三个方面的异常检测，本文方法显著提升了检测能力。

Abstract: Video anomaly detection (VAD) is an important computer vision problem. Thanks to the mode coverage capabilities of generative models, the likelihood-based paradigm is catching growing interest, as it can model normal distribution and detect out-of-distribution anomalies. However, these likelihood-based methods are blind to the anomalies located in local modes near the learned distribution. To handle these ``unseen" anomalies, we dive into three gaps uniquely existing in VAD regarding scene, motion and appearance. Specifically, we first build a noise-conditioned score transformer for denoising score matching. Then, we introduce a scene-dependent and motion-aware score function by embedding the scene condition of input sequences into our model and assigning motion weights based on the difference between key frames of input sequences. Next, to solve the problem of blindness in principle, we integrate unaffected visual information via a novel autoregressive denoising score matching mechanism for inference. Through autoregressively injecting intensifying Gaussian noise into the denoised data and estimating the corresponding score function, we compare the denoised data with the original data to get a difference and aggregate it with the score function for an enhanced appearance perception and accumulate the abnormal context. With all three gaps considered, we can compute a more comprehensive anomaly indicator. Experiments on three popular VAD benchmarks demonstrate the state-of-the-art performance of our method.

</details>


### [31] [DiffFit: Disentangled Garment Warping and Texture Refinement for Virtual Try-On](https://arxiv.org/abs/2506.23295)
*Xiang Xu*

Main category: cs.CV

TL;DR: DiffFit是一个两阶段潜在扩散框架，用于高保真虚拟试穿，通过几何感知的服装变形和跨模态条件扩散模型解决现有方法在细节保留、对齐精度和效率上的问题。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿方法在保留服装细节、精确对齐、效率和多样性方面存在不足，需要改进。

Method: DiffFit采用两阶段策略：第一阶段进行几何感知的服装变形和对齐，第二阶段通过跨模态条件扩散模型优化纹理保真度。

Result: DiffFit在定量指标和感知评估中均优于现有方法，能够保留服装细节并实现精确对齐。

Conclusion: DiffFit通过解耦几何对齐和外观优化，显著提升了虚拟试穿的生成稳定性和视觉真实感。

Abstract: Virtual try-on (VTON) aims to synthesize realistic images of a person wearing a target garment, with broad applications in e-commerce and digital fashion. While recent advances in latent diffusion models have substantially improved visual quality, existing approaches still struggle with preserving fine-grained garment details, achieving precise garment-body alignment, maintaining inference efficiency, and generalizing to diverse poses and clothing styles. To address these challenges, we propose DiffFit, a novel two-stage latent diffusion framework for high-fidelity virtual try-on. DiffFit adopts a progressive generation strategy: the first stage performs geometry-aware garment warping, aligning the garment with the target body through fine-grained deformation and pose adaptation. The second stage refines texture fidelity via a cross-modal conditional diffusion model that integrates the warped garment, the original garment appearance, and the target person image for high-quality rendering. By decoupling geometric alignment and appearance refinement, DiffFit effectively reduces task complexity and enhances both generation stability and visual realism. It excels in preserving garment-specific attributes such as textures, wrinkles, and lighting, while ensuring accurate alignment with the human body. Extensive experiments on large-scale VTON benchmarks demonstrate that DiffFit achieves superior performance over existing state-of-the-art methods in both quantitative metrics and perceptual evaluations.

</details>


### [32] [Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination Correction with Gaussian Splatting](https://arxiv.org/abs/2506.23308)
*Yiming Huang,Long Bai,Beilei Cui,Yanheng Li,Tong Chen,Jie Wang,Jinlin Wu,Zhen Lei,Hongbin Liu,Hongliang Ren*

Main category: cs.CV

TL;DR: Endo-4DGX是一种针对内窥镜场景中不均匀光照的新型重建方法，通过光照自适应高斯泼溅技术，解决了3DGS在极端光照条件下的优化问题。


<details>
  <summary>Details</summary>
Motivation: 在图像引导机器人手术中，软组织的精确重建对自动化至关重要。3DGS及其变体4DGS在动态手术场景中实现了高质量的实时渲染，但在极端光照条件下（如低光和过曝）表现不佳。

Method: Endo-4DGX结合光照嵌入、区域感知增强模块和空间感知调整模块，实现了光照自适应的重建。此外，还引入了曝光控制损失以恢复外观。

Result: 实验结果表明，Endo-4DGX在挑战性光照环境下显著优于现有重建和恢复方法的组合，同时保持了几何精度。

Conclusion: Endo-4DGX在极端光照条件下表现出色，有望推动机器人辅助手术的应用。

Abstract: Accurate reconstruction of soft tissue is crucial for advancing automation in image-guided robotic surgery. The recent 3D Gaussian Splatting (3DGS) techniques and their variants, 4DGS, achieve high-quality renderings of dynamic surgical scenes in real-time. However, 3D-GS-based methods still struggle in scenarios with varying illumination, such as low light and over-exposure. Training 3D-GS in such extreme light conditions leads to severe optimization problems and devastating rendering quality. To address these challenges, we present Endo-4DGX, a novel reconstruction method with illumination-adaptive Gaussian Splatting designed specifically for endoscopic scenes with uneven lighting. By incorporating illumination embeddings, our method effectively models view-dependent brightness variations. We introduce a region-aware enhancement module to model the sub-area lightness at the Gaussian level and a spatial-aware adjustment module to learn the view-consistent brightness adjustment. With the illumination adaptive design, Endo-4DGX achieves superior rendering performance under both low-light and over-exposure conditions while maintaining geometric accuracy. Additionally, we employ an exposure control loss to restore the appearance from adverse exposure to the normal level for illumination-adaptive optimization. Experimental results demonstrate that Endo-4DGX significantly outperforms combinations of state-of-the-art reconstruction and restoration methods in challenging lighting environments, underscoring its potential to advance robot-assisted surgical applications. Our code is available at https://github.com/lastbasket/Endo-4DGX.

</details>


### [33] [FastSeg: Efficient Training-Free Open-Vocabulary Segmentation via Hierarchical Attention Refinement Method](https://arxiv.org/abs/2506.23323)
*Quang-Huy Che,Vinh-Tiep Nguyen*

Main category: cs.CV

TL;DR: FastSeg是一种高效的训练无关框架，通过预训练扩散模型（如Stable Diffusion）的（1+1）步反向过程实现开放词汇语义分割（OVSS），并在单次运行中完成所有类别的分割。


<details>
  <summary>Details</summary>
Motivation: 解决现有对比学习模型在像素级别空间精度不足和扩散模型在迭代次数与分割质量之间难以平衡的问题。

Method: 提出FastSeg框架，包含双提示机制、分层注意力细化方法（HARD）和测试时翻转（TTF）方案，以提高分割质量和空间一致性。

Result: 在PASCAL VOC、PASCAL Context和COCO Object基准测试中，平均mIoU达到43.8%，同时保持高效推理。

Conclusion: FastSeg在分割质量和推理效率之间取得了平衡，为扩展性提供了坚实基础。

Abstract: Open-vocabulary semantic segmentation (OVSS) aims to segment objects from arbitrary text categories without requiring densely annotated datasets. Although contrastive learning based models enable zero-shot segmentation, they often lose fine spatial precision at pixel level, due to global representation bias. In contrast, diffusion-based models naturally encode fine-grained spatial features via attention mechanisms that capture both global context and local details. However, they often face challenges in balancing the number of iterations with the quality of the segmentation. In this work, we propose FastSeg, a novel and efficient training-free framework with only (1+1)-step of reverse process of a pretrained diffusion model (e.g., Stable Diffusion). Moreover, instead of running multiple times for different classes, FastSeg performs segmentation for all classes at once. To further enhance the segmentation quality, FastSeg introduces three key components: (i) a dual-prompt mechanism for discriminative, class-aware attention extraction, (ii) a Hierarchical Attention Refinement Method (HARD) that enhances fused cross-attention using scale-aligned selfattention maps, and (iii) a Test-Time Flipping (TTF) scheme designed to improve spatial consistency. Extensive experiments show that FastSeg achieves state-of-the-art training-free performance, obtaining 43.8% average mIoU across PASCAL VOC, PASCAL Context, and COCO Object benchmarks while maintaining superior inference efficiency. Our results demonstrate that FastSeg provides a strong foundation for extendability, bridging the gap between segmentation quality and inference efficiency.

</details>


### [34] [CycleVAR: Repurposing Autoregressive Model for Unsupervised One-Step Image Translation](https://arxiv.org/abs/2506.23347)
*Yi Liu,Shengqian Li,Zuzeng Lin,Feng Wang,Si Liu*

Main category: cs.CV

TL;DR: 提出了一种名为CycleVAR的新方法，通过Softmax Relaxed Quantization解决传统量化方法在图像翻译中的梯度问题，并在无监督图像翻译任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索条件自回归图像生成方法在无监督图像翻译领域的潜力，解决传统量化方法导致的梯度中断问题。

Method: 使用Softmax Relaxed Quantization保持梯度传播，并引入CycleVAR方法，通过多尺度源图像标记作为上下文提示，实现图像到图像的翻译。

Result: CycleVAR在无监督场景下表现优于现有方法（如CycleGAN-Turbo），并行单步生成模式在质量和速度上均优于串行多步模式。

Conclusion: CycleVAR通过改进量化方法和多尺度生成策略，显著提升了无监督图像翻译的性能。

Abstract: The current conditional autoregressive image generation methods have shown promising results, yet their potential remains largely unexplored in the practical unsupervised image translation domain, which operates without explicit cross-domain correspondences. A critical limitation stems from the discrete quantization inherent in traditional Vector Quantization-based frameworks, which disrupts gradient flow between the Variational Autoencoder decoder and causal Transformer, impeding end-to-end optimization during adversarial training in image space. To tackle this issue, we propose using Softmax Relaxed Quantization, a novel approach that reformulates codebook selection as a continuous probability mixing process via Softmax, thereby preserving gradient propagation. Building upon this differentiable foundation, we introduce CycleVAR, which reformulates image-to-image translation as image-conditional visual autoregressive generation by injecting multi-scale source image tokens as contextual prompts, analogous to prefix-based conditioning in language models. CycleVAR exploits two modes to generate the target image tokens, including (1) serial multi-step generation, enabling iterative refinement across scales, and (2) parallel one-step generation synthesizing all resolution outputs in a single forward pass. Experimental findings indicate that the parallel one-step generation mode attains superior translation quality with quicker inference speed than the serial multi-step mode in unsupervised scenarios. Furthermore, both quantitative and qualitative results indicate that CycleVAR surpasses previous state-of-the-art unsupervised image translation models, \textit{e}.\textit{g}., CycleGAN-Turbo.

</details>


### [35] [Layer Decomposition and Morphological Reconstruction for Task-Oriented Infrared Image Enhancement](https://arxiv.org/abs/2506.23353)
*Siyuan Chai,Xiaodong Guo,Tong Liu*

Main category: cs.CV

TL;DR: 提出了一种任务导向的红外图像增强方法，通过层分解和显著性信息提取，提升图像质量以支持目标检测和语义分割任务。


<details>
  <summary>Details</summary>
Motivation: 红外图像在复杂天气条件下（如雾、雨、低光）能提升自动驾驶的感知能力，但低对比度和噪声问题影响了高级视觉任务的性能。

Method: 方法包括层分解（增强场景细节并保留暗区特征）和基于形态重建的显著性提取（增强目标信息且不放大噪声）。

Result: 实验表明，该方法在目标检测和语义分割任务中优于现有技术。

Conclusion: 该方法有效解决了红外图像低对比度和噪声问题，提升了高级视觉任务的性能。

Abstract: Infrared image helps improve the perception capabilities of autonomous driving in complex weather conditions such as fog, rain, and low light. However, infrared image often suffers from low contrast, especially in non-heat-emitting targets like bicycles, which significantly affects the performance of downstream high-level vision tasks. Furthermore, achieving contrast enhancement without amplifying noise and losing important information remains a challenge. To address these challenges, we propose a task-oriented infrared image enhancement method. Our approach consists of two key components: layer decomposition and saliency information extraction. First, we design an layer decomposition method for infrared images, which enhances scene details while preserving dark region features, providing more features for subsequent saliency information extraction. Then, we propose a morphological reconstruction-based saliency extraction method that effectively extracts and enhances target information without amplifying noise. Our method improves the image quality for object detection and semantic segmentation tasks. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods.

</details>


### [36] [Why Settle for Mid: A Probabilistic Viewpoint to Spatial Relationship Alignment in Text-to-image Models](https://arxiv.org/abs/2506.23418)
*Parham Rezaei,Arash Marioriyad,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: 论文提出了一种基于概率优势（PoS）的新框架，用于改进文本到图像模型在空间关系生成中的准确性，并引入了新的评估指标（PSE）和生成方法（PSG）。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像模型在生成复杂空间关系时的对齐问题，尤其是空间配置的准确性。

Method: 提出PoS框架，包括PSE评估指标和PSG生成方法，通过梯度引导或搜索策略优化空间关系生成。

Result: PSE指标与人类判断更一致，PSG方法显著提升了空间关系生成的准确性，优于现有方法。

Conclusion: PoS框架有效解决了文本到图像模型的空间关系对齐问题，为复杂场景生成提供了可靠工具。

Abstract: Despite the ability of text-to-image models to generate high-quality, realistic, and diverse images, they face challenges in compositional generation, often struggling to accurately represent details specified in the input prompt. A prevalent issue in compositional generation is the misalignment of spatial relationships, as models often fail to faithfully generate images that reflect the spatial configurations specified between objects in the input prompts. To address this challenge, we propose a novel probabilistic framework for modeling the relative spatial positioning of objects in a scene, leveraging the concept of Probability of Superiority (PoS). Building on this insight, we make two key contributions. First, we introduce a novel evaluation metric, PoS-based Evaluation (PSE), designed to assess the alignment of 2D and 3D spatial relationships between text and image, with improved adherence to human judgment. Second, we propose PoS-based Generation (PSG), an inference-time method that improves the alignment of 2D and 3D spatial relationships in T2I models without requiring fine-tuning. PSG employs a Part-of-Speech PoS-based reward function that can be utilized in two distinct ways: (1) as a gradient-based guidance mechanism applied to the cross-attention maps during the denoising steps, or (2) as a search-based strategy that evaluates a set of initial noise vectors to select the best one. Extensive experiments demonstrate that the PSE metric exhibits stronger alignment with human judgment compared to traditional center-based metrics, providing a more nuanced and reliable measure of complex spatial relationship accuracy in text-image alignment. Furthermore, PSG significantly enhances the ability of text-to-image models to generate images with specified spatial configurations, outperforming state-of-the-art methods across multiple evaluation metrics and benchmarks.

</details>


### [37] [PathDiff: Histopathology Image Synthesis with Unpaired Text and Mask Conditions](https://arxiv.org/abs/2506.23440)
*Mahesh Bhosale,Abdul Wasi,Yuanhao Zhai,Yunjie Tian,Samuel Border,Nan Xi,Pinaki Sarder,Junsong Yuan,David Doermann,Xuan Gong*

Main category: cs.CV

TL;DR: PathDiff是一种扩散框架，利用未配对的掩码-文本数据生成高质量的病理学图像，提升语义和空间细节控制。


<details>
  <summary>Details</summary>
Motivation: 解决病理学图像数据稀缺问题，同时利用文本和掩码数据增强生成图像的语义和结构准确性。

Method: 提出PathDiff框架，将未配对的掩码和文本数据整合到统一的条件空间中，生成高质量图像。

Result: PathDiff在图像保真度、文本-图像对齐和下游任务（如核分割和分类）中表现优于现有方法。

Conclusion: PathDiff通过结合文本和掩码数据，显著提升了病理学图像生成的语义和结构控制能力。

Abstract: Diffusion-based generative models have shown promise in synthesizing histopathology images to address data scarcity caused by privacy constraints. Diagnostic text reports provide high-level semantic descriptions, and masks offer fine-grained spatial structures essential for representing distinct morphological regions. However, public datasets lack paired text and mask data for the same histopathological images, limiting their joint use in image generation. This constraint restricts the ability to fully exploit the benefits of combining both modalities for enhanced control over semantics and spatial details. To overcome this, we propose PathDiff, a diffusion framework that effectively learns from unpaired mask-text data by integrating both modalities into a unified conditioning space. PathDiff allows precise control over structural and contextual features, generating high-quality, semantically accurate images. PathDiff also improves image fidelity, text-image alignment, and faithfulness, enhancing data augmentation for downstream tasks like nuclei segmentation and classification. Extensive experiments demonstrate its superiority over existing methods.

</details>


### [38] [Contrastive Learning with Diffusion Features for Weakly Supervised Medical Image Segmentation](https://arxiv.org/abs/2506.23460)
*Dewen Zeng,Xinrong Hu,Yu-Jen Chen,Yawen Wu,Xiaowei Xu,Yiyu Shi*

Main category: cs.CV

TL;DR: 论文提出了一种新方法CLDF，利用对比学习和扩散模型特征改进弱监督语义分割，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统CAM方法在弱监督语义分割中存在部分激活和边界不精确的问题，而基于条件扩散模型的方法易受背景噪声干扰。

Method: 结合对比学习和扩散模型特征，通过梯度图和CAM识别前景与背景像素，训练像素解码器生成低维嵌入空间。

Result: 在两个公共医学数据集的四个分割任务中，CLDF方法显著优于现有基线。

Conclusion: CLDF通过对比学习和扩散特征有效解决了噪声和边界问题，提升了分割性能。

Abstract: Weakly supervised semantic segmentation (WSSS) methods using class labels often rely on class activation maps (CAMs) to localize objects. However, traditional CAM-based methods struggle with partial activations and imprecise object boundaries due to optimization discrepancies between classification and segmentation. Recently, the conditional diffusion model (CDM) has been used as an alternative for generating segmentation masks in WSSS, leveraging its strong image generation capabilities tailored to specific class distributions. By modifying or perturbing the condition during diffusion sampling, the related objects can be highlighted in the generated images. Yet, the saliency maps generated by CDMs are prone to noise from background alterations during reverse diffusion. To alleviate the problem, we introduce Contrastive Learning with Diffusion Features (CLDF), a novel method that uses contrastive learning to train a pixel decoder to map the diffusion features from a frozen CDM to a low-dimensional embedding space for segmentation. Specifically, we integrate gradient maps generated from CDM external classifier with CAMs to identify foreground and background pixels with fewer false positives/negatives for contrastive learning, enabling robust pixel embedding learning. Experimental results on four segmentation tasks from two public medical datasets demonstrate that our method significantly outperforms existing baselines.

</details>


### [39] [Time-variant Image Inpainting via Interactive Distribution Transition Estimation](https://arxiv.org/abs/2506.23461)
*Yun Xing,Qing Guo,Xiaoguang Li,Yihao Huang,Xiaofeng Cao,Di Lin,Ivor Tsang,Lei Ma*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的时间变异图像修复任务（TAMP），通过引入交互式分布转移估计模块（InDiTE）和扩散模型（InDiTE-Diff），有效解决了时间变异图像修复中的复杂互补问题，并在新数据集TAMP-Street上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统参考引导的图像修复方法在处理时间变异图像时效果不佳，因为参考图像与目标图像内容差异大且可能受损。本文旨在解决这一实际问题。

Method: 提出InDiTE模块交互补充时间变异图像的语义，并结合扩散模型（InDiTE-Diff）进行潜在交叉参考。

Result: 在TAMP-Street数据集上的实验表明，该方法显著优于现有参考引导图像修复方法。

Conclusion: InDiTE-Diff为时间变异图像修复提供了有效解决方案，并在实际应用中展示了潜力。

Abstract: In this work, we focus on a novel and practical task, i.e., Time-vAriant iMage inPainting (TAMP). The aim of TAMP is to restore a damaged target image by leveraging the complementary information from a reference image, where both images captured the same scene but with a significant time gap in between, i.e., time-variant images. Different from conventional reference-guided image inpainting, the reference image under TAMP setup presents significant content distinction to the target image and potentially also suffers from damages. Such an application frequently happens in our daily lives to restore a damaged image by referring to another reference image, where there is no guarantee of the reference image's source and quality. In particular, our study finds that even state-of-the-art (SOTA) reference-guided image inpainting methods fail to achieve plausible results due to the chaotic image complementation. To address such an ill-posed problem, we propose a novel Interactive Distribution Transition Estimation (InDiTE) module which interactively complements the time-variant images with adaptive semantics thus facilitate the restoration of damaged regions. To further boost the performance, we propose our TAMP solution, namely Interactive Distribution Transition Estimation-driven Diffusion (InDiTE-Diff), which integrates InDiTE with SOTA diffusion model and conducts latent cross-reference during sampling. Moreover, considering the lack of benchmarks for TAMP task, we newly assembled a dataset, i.e., TAMP-Street, based on existing image and mask datasets. We conduct experiments on the TAMP-Street datasets under two different time-variant image inpainting settings, which show our method consistently outperform SOTA reference-guided image inpainting methods for solving TAMP.

</details>


### [40] [Instant GaussianImage: A Generalizable and Self-Adaptive Image Representation via 2D Gaussian Splatting](https://arxiv.org/abs/2506.23479)
*Zhaojie Zeng,Yuesong Wang,Chao Yang,Tao Guan,Lili Ju*

Main category: cs.CV

TL;DR: 提出了一种基于2D高斯泼溅的自适应图像表示框架，显著减少训练时间并动态调整高斯点数量。


<details>
  <summary>Details</summary>
Motivation: 解决高斯泼溅方法训练慢且高斯点数量固定的问题，提升灵活性和效率。

Method: 使用网络快速生成粗略高斯表示，再通过少量微调步骤优化，动态调整高斯点数量。

Result: 在DIV2K和Kodak数据集上，训练时间减少一个数量级，渲染性能相当或更优。

Conclusion: 该方法高效且灵活，适用于实际应用。

Abstract: Implicit Neural Representation (INR) has demonstrated remarkable advances in the field of image representation but demands substantial GPU resources. GaussianImage recently pioneered the use of Gaussian Splatting to mitigate this cost, however, the slow training process limits its practicality, and the fixed number of Gaussians per image limits its adaptability to varying information entropy. To address these issues, we propose in this paper a generalizable and self-adaptive image representation framework based on 2D Gaussian Splatting. Our method employs a network to quickly generate a coarse Gaussian representation, followed by minimal fine-tuning steps, achieving comparable rendering quality of GaussianImage while significantly reducing training time. Moreover, our approach dynamically adjusts the number of Gaussian points based on image complexity to further enhance flexibility and efficiency in practice. Experiments on DIV2K and Kodak datasets show that our method matches or exceeds GaussianImage's rendering performance with far fewer iterations and shorter training times. Specifically, our method reduces the training time by up to one order of magnitude while achieving superior rendering performance with the same number of Gaussians.

</details>


### [41] [MTADiffusion: Mask Text Alignment Diffusion Model for Object Inpainting](https://arxiv.org/abs/2506.23482)
*Jun Huang,Ting Liu,Yihang Wu,Xiaochao Qu,Luoqi Liu,Xiaolin Hu*

Main category: cs.CV

TL;DR: MTADiffusion是一种用于对象修复的Mask-Text Alignment扩散模型，通过MTAPipeline自动标注掩码和详细描述，构建了包含500万图像和2500万掩码-文本对的数据集。采用多任务训练策略和风格一致性损失，显著提升了修复效果。


<details>
  <summary>Details</summary>
Motivation: 现有图像修复方法存在语义不对齐、结构扭曲和风格不一致等问题，MTADiffusion旨在解决这些问题。

Method: 提出MTAPipeline自动标注掩码和描述，构建MTADataset；采用多任务训练策略（修复和边缘预测）；引入风格一致性损失（基于VGG网络和Gram矩阵）。

Result: 在BrushBench和EditBench上，MTADiffusion表现优于其他方法，达到最先进水平。

Conclusion: MTADiffusion通过改进语义对齐、结构稳定性和风格一致性，显著提升了对象修复的效果。

Abstract: Advancements in generative models have enabled image inpainting models to generate content within specific regions of an image based on provided prompts and masks. However, existing inpainting methods often suffer from problems such as semantic misalignment, structural distortion, and style inconsistency. In this work, we present MTADiffusion, a Mask-Text Alignment diffusion model designed for object inpainting. To enhance the semantic capabilities of the inpainting model, we introduce MTAPipeline, an automatic solution for annotating masks with detailed descriptions. Based on the MTAPipeline, we construct a new MTADataset comprising 5 million images and 25 million mask-text pairs. Furthermore, we propose a multi-task training strategy that integrates both inpainting and edge prediction tasks to improve structural stability. To promote style consistency, we present a novel inpainting style-consistency loss using a pre-trained VGG network and the Gram matrix. Comprehensive evaluations on BrushBench and EditBench demonstrate that MTADiffusion achieves state-of-the-art performance compared to other methods.

</details>


### [42] [ViewPoint: Panoramic Video Generation with Pretrained Diffusion Models](https://arxiv.org/abs/2506.23513)
*Zixun Fang,Kai Zhu,Zhiheng Liu,Yu Liu,Wei Zhai,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 提出了一种利用预训练视角视频模型生成全景视频的新框架，通过ViewPoint map和Pano-Perspective注意力机制解决模态差距问题，实现了高质量全景视频合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法因全景数据与视角数据之间的模态差距，难以生成高质量全景视频。

Method: 设计了具有全局空间连续性和细粒度视觉细节的ViewPoint map，并引入Pano-Perspective注意力机制，利用预训练视角先验。

Result: 实验表明，该方法能合成动态且空间一致的全景视频，性能优于现有方法。

Conclusion: 该方法在生成全景视频方面取得了显著进展，为VR等领域提供了重要支持。

Abstract: Panoramic video generation aims to synthesize 360-degree immersive videos, holding significant importance in the fields of VR, world models, and spatial intelligence. Existing works fail to synthesize high-quality panoramic videos due to the inherent modality gap between panoramic data and perspective data, which constitutes the majority of the training data for modern diffusion models. In this paper, we propose a novel framework utilizing pretrained perspective video models for generating panoramic videos. Specifically, we design a novel panorama representation named ViewPoint map, which possesses global spatial continuity and fine-grained visual details simultaneously. With our proposed Pano-Perspective attention mechanism, the model benefits from pretrained perspective priors and captures the panoramic spatial correlations of the ViewPoint map effectively. Extensive experiments demonstrate that our method can synthesize highly dynamic and spatially consistent panoramic videos, achieving state-of-the-art performance and surpassing previous methods.

</details>


### [43] [WAVE: Warp-Based View Guidance for Consistent Novel View Synthesis Using a Single Image](https://arxiv.org/abs/2506.23518)
*Jiwoo Park,Tae Eun Choi,Youngjun Jun,Seong Jae Hwang*

Main category: cs.CV

TL;DR: 提出了一种无需额外模块的视图一致性图像生成方法，通过自适应注意力操纵和噪声重新初始化提升扩散模型的视图一致性。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在生成多视角图像时难以保持空间连续性的问题，同时避免复杂多步流程的效率低下。

Method: 利用视图引导的扭曲技术，实现训练自由的自适应注意力操纵和噪声重新初始化。

Result: 通过适用于新视角数据集的综合指标框架，验证了方法在不同扩散模型中的视图一致性提升。

Conclusion: 该方法具有广泛适用性，能够有效提升扩散模型在多视角图像生成中的表现。

Abstract: Generating high-quality novel views of a scene from a single image requires maintaining structural coherence across different views, referred to as view consistency. While diffusion models have driven advancements in novel view synthesis, they still struggle to preserve spatial continuity across views. Diffusion models have been combined with 3D models to address the issue, but such approaches lack efficiency due to their complex multi-step pipelines. This paper proposes a novel view-consistent image generation method which utilizes diffusion models without additional modules. Our key idea is to enhance diffusion models with a training-free method that enables adaptive attention manipulation and noise reinitialization by leveraging view-guided warping to ensure view consistency. Through our comprehensive metric framework suitable for novel-view datasets, we show that our method improves view consistency across various diffusion models, demonstrating its broader applicability.

</details>


### [44] [Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane Localization and Anomaly Diagnosis in 3D Ultrasound](https://arxiv.org/abs/2506.23538)
*Yuhao Huang,Yueyue Xu,Haoran Dou,Jiaxiao Deng,Xin Yang,Hongyu Zheng,Dong Ni*

Main category: cs.CV

TL;DR: 提出一种智能系统，通过3D超声自动定位平面并诊断先天性子宫异常，结合去噪扩散模型和强化学习框架，显著提升诊断效果。


<details>
  <summary>Details</summary>
Motivation: 先天性子宫异常（CUAs）可能导致不孕、流产等问题，传统2D超声难以准确评估，3D超声能提供更清晰的子宫形态可视化。

Method: 1) 使用去噪扩散模型结合局部和全局指导；2) 引入强化学习框架提取关键切片；3) 提供文本驱动的不确定性建模优化分类。

Result: 在大规模3D子宫超声数据集上验证了方法的有效性，提升了平面定位和CUA诊断的准确性。

Conclusion: 提出的智能系统在3D超声中实现了高效的平面定位和CUA诊断，为临床提供了可靠工具。

Abstract: Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage, preterm birth, and an increased risk of pregnancy complications. Compared to traditional 2D ultrasound (US), 3D US can reconstruct the coronal plane, providing a clear visualization of the uterine morphology for assessing CUAs accurately. In this paper, we propose an intelligent system for simultaneous automated plane localization and CUA diagnosis. Our highlights are: 1) we develop a denoising diffusion model with local (plane) and global (volume/text) guidance, using an adaptive weighting strategy to optimize attention allocation to different conditions; 2) we introduce a reinforcement learning-based framework with unsupervised rewards to extract the key slice summary from redundant sequences, fully integrating information across multiple planes to reduce learning difficulty; 3) we provide text-driven uncertainty modeling for coarse prediction, and leverage it to adjust the classification probability for overall performance improvement. Extensive experiments on a large 3D uterine US dataset show the efficacy of our method, in terms of plane localization and CUA diagnosis. Code is available at https://github.com/yuhoo0302/CUA-US.

</details>


### [45] [Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric Attention](https://arxiv.org/abs/2506.23542)
*Weida Wang,Changyong He,Jin Zeng,Di Qiu*

Main category: cs.CV

TL;DR: 提出了一种基于运动不变图融合的ToF深度去噪网络，通过跨帧几何注意力增强时间稳定性和空间清晰度。


<details>
  <summary>Details</summary>
Motivation: ToF传感器捕获的深度图像存在噪声，现有方法未充分考虑跨帧深度变化，导致时间不一致和空间模糊。

Method: 利用图的时序自相似性进行跨帧几何注意力融合，结合图像平滑先验和ToF噪声分布，构建最大后验问题，并通过迭代滤波器实现。

Result: 在合成DVToF数据集和真实Kinectv2数据集上均表现出最优的准确性和一致性。

Conclusion: 该方法在ToF深度去噪中实现了高性能且可解释的网络，具有鲁棒的泛化能力。

Abstract: Depth images captured by Time-of-Flight (ToF) sensors are prone to noise, requiring denoising for reliable downstream applications. Previous works either focus on single-frame processing, or perform multi-frame processing without considering depth variations at corresponding pixels across frames, leading to undesirable temporal inconsistency and spatial ambiguity. In this paper, we propose a novel ToF depth denoising network leveraging motion-invariant graph fusion to simultaneously enhance temporal stability and spatial sharpness. Specifically, despite depth shifts across frames, graph structures exhibit temporal self-similarity, enabling cross-frame geometric attention for graph fusion. Then, by incorporating an image smoothness prior on the fused graph and data fidelity term derived from ToF noise distribution, we formulate a maximum a posterior problem for ToF denoising. Finally, the solution is unrolled into iterative filters whose weights are adaptively learned from the graph-informed geometric attention, producing a high-performance yet interpretable network. Experimental results demonstrate that the proposed scheme achieves state-of-the-art performance in terms of accuracy and consistency on synthetic DVToF dataset and exhibits robust generalization on the real Kinectv2 dataset. Source code will be released at \href{https://github.com/davidweidawang/GIGA-ToF}{https://github.com/davidweidawang/GIGA-ToF}.

</details>


### [46] [Pyramidal Patchification Flow for Visual Generation](https://arxiv.org/abs/2506.23543)
*Hui Li,Baoyou Chen,Liwei Zhang,Jiaye Li,Jingdong Wang,Siyu Zhu*

Main category: cs.CV

TL;DR: PPFlow通过动态调整patch大小和线性投影，优化了Diffusion Transformers的计算成本，同时保持生成性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法对所有时间步使用固定patch大小，限制了计算效率。PPFlow旨在通过动态patch大小优化计算成本。

Method: 提出Pyramidal Patchification Flow（PPFlow），根据噪声水平动态调整patch大小，并为每个大小学习线性投影。

Result: PPFlow在推理速度上比SiT-B/2快1.6倍（2级）或2.0倍（3级），训练FLOPs略低且生成性能相近。

Conclusion: PPFlow显著提升了计算效率，同时保持了生成质量，适用于从头训练或基于预训练模型的微调。

Abstract: Diffusion transformers (DiTs) adopt Patchify, mapping patch representations to token representations through linear projections, to adjust the number of tokens input to DiT blocks and thus the computation cost. Instead of a single patch size for all the timesteps, we introduce a Pyramidal Patchification Flow (PPFlow) approach: Large patch sizes are used for high noise timesteps and small patch sizes for low noise timesteps; Linear projections are learned for each patch size; and Unpatchify is accordingly modified. Unlike Pyramidal Flow, our approach operates over full latent representations other than pyramid representations, and adopts the normal denoising process without requiring the renoising trick. We demonstrate the effectiveness of our approach through two training manners. Training from scratch achieves a $1.6\times$ ($2.0\times$) inference speed over SiT-B/2 for 2-level (3-level) pyramid patchification with slightly lower training FLOPs and similar image generation performance. Training from pretrained normal DiTs achieves even better performance with small training time. The code and checkpoint are at https://github.com/fudan-generative-vision/PPFlow.

</details>


### [47] [Oneta: Multi-Style Image Enhancement Using Eigentransformation Functions](https://arxiv.org/abs/2506.23547)
*Jiwon Kim,Soohyun Hwang,Dong-O Kim,Changsu Han,Min Kyu Park,Chang-Su Kim*

Main category: cs.CV

TL;DR: 提出了一种名为Oneta的多风格图像增强算法，通过两步操作（强度增强和颜色校正）实现高性能，并支持多种风格任务。


<details>
  <summary>Details</summary>
Motivation: 解决多风格图像增强任务，通过简单但高效的两步模型实现广泛的应用场景。

Method: 使用Y-Net和C-Net分别预测eigenTF和CCM参数，通过K个可学习令牌支持多种风格。

Result: 实验表明，Oneta能有效处理六种增强任务，覆盖30个数据集。

Conclusion: Oneta通过简洁的两步模型实现了多风格图像增强的高性能。

Abstract: The first algorithm, called Oneta, for a novel task of multi-style image enhancement is proposed in this work. Oneta uses two point operators sequentially: intensity enhancement with a transformation function (TF) and color correction with a color correction matrix (CCM). This two-step enhancement model, though simple, achieves a high performance upper bound. Also, we introduce eigentransformation function (eigenTF) to represent TF compactly. The Oneta network comprises Y-Net and C-Net to predict eigenTF and CCM parameters, respectively. To support $K$ styles, Oneta employs $K$ learnable tokens. During training, each style token is learned using image pairs from the corresponding dataset. In testing, Oneta selects one of the $K$ style tokens to enhance an image accordingly. Extensive experiments show that the single Oneta network can effectively undertake six enhancement tasks -- retouching, image signal processing, low-light image enhancement, dehazing, underwater image enhancement, and white balancing -- across 30 datasets.

</details>


### [48] [JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching](https://arxiv.org/abs/2506.23552)
*Mingi Kwon,Joonghyuk Shin,Jaeseok Jung,Jaesik Park,Youngjung Uh*

Main category: cs.CV

TL;DR: JAM-Flow是一个统一框架，通过流匹配和多模态扩散变换器（MM-DiT）同时合成面部运动和语音，支持多种输入条件。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常将面部运动合成和语音合成视为独立任务，忽略了二者的内在联系。JAM-Flow旨在提供一个统一的解决方案。

Method: 采用流匹配和MM-DiT架构，结合Motion-DiT和Audio-DiT模块，通过选择性联合注意力层实现跨模态交互。

Result: JAM-Flow支持多种输入条件（如文本、参考音频和运动），并在单一模型中实现同步任务。

Conclusion: JAM-Flow为多模态生成建模提供了实用解决方案，显著推进了音频-视觉合成的整体性。

Abstract: The intrinsic link between facial motion and speech is often overlooked in generative modeling, where talking head synthesis and text-to-speech (TTS) are typically addressed as separate tasks. This paper introduces JAM-Flow, a unified framework to simultaneously synthesize and condition on both facial motion and speech. Our approach leverages flow matching and a novel Multi-Modal Diffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT and Audio-DiT modules. These are coupled via selective joint attention layers and incorporate key architectural choices, such as temporally aligned positional embeddings and localized joint attention masking, to enable effective cross-modal interaction while preserving modality-specific strengths. Trained with an inpainting-style objective, JAM-Flow supports a wide array of conditioning inputs-including text, reference audio, and reference motion-facilitating tasks such as synchronized talking head generation from text, audio-driven animation, and much more, within a single, coherent model. JAM-Flow significantly advances multi-modal generative modeling by providing a practical solution for holistic audio-visual synthesis. project page: https://joonghyuk.com/jamflow-web

</details>


### [49] [Metadata, Wavelet, and Time Aware Diffusion Models for Satellite Image Super Resolution](https://arxiv.org/abs/2506.23566)
*Luigi Sigillo,Renato Giamba,Danilo Comminiello*

Main category: cs.CV

TL;DR: MWT-Diff是一种结合潜在扩散模型和小波变换的卫星图像超分辨率框架，通过MWT-Encoder生成嵌入特征，逐步重建高分辨率图像。


<details>
  <summary>Details</summary>
Motivation: 高分辨率卫星图像获取受限于传感器时空限制和高成本，影响环境监测、灾害响应等应用。

Method: 提出MWT-Diff框架，结合潜在扩散模型和小波变换，利用MWT-Encoder生成嵌入特征指导扩散过程。

Result: 在多个数据集上表现优于现有方法，感知质量指标（FID和LPIPS）验证了其优越性。

Conclusion: MWT-Diff能有效解决卫星图像超分辨率问题，保留关键空间特征，适用于遥感分析。

Abstract: The acquisition of high-resolution satellite imagery is often constrained by the spatial and temporal limitations of satellite sensors, as well as the high costs associated with frequent observations. These challenges hinder applications such as environmental monitoring, disaster response, and agricultural management, which require fine-grained and high-resolution data. In this paper, we propose MWT-Diff, an innovative framework for satellite image super-resolution (SR) that combines latent diffusion models with wavelet transforms to address these challenges. At the core of the framework is a novel metadata-, wavelet-, and time-aware encoder (MWT-Encoder), which generates embeddings that capture metadata attributes, multi-scale frequency information, and temporal relationships. The embedded feature representations steer the hierarchical diffusion dynamics, through which the model progressively reconstructs high-resolution satellite imagery from low-resolution inputs. This process preserves critical spatial characteristics including textural patterns, boundary discontinuities, and high-frequency spectral components essential for detailed remote sensing analysis. The comparative analysis of MWT-Diff across multiple datasets demonstrated favorable performance compared to recent approaches, as measured by standard perceptual quality metrics including FID and LPIPS.

</details>


### [50] [SG-LDM: Semantic-Guided LiDAR Generation via Latent-Aligned Diffusion](https://arxiv.org/abs/2506.23606)
*Zhengkang Xiang,Zizhao Li,Amir Khodabandeh,Kourosh Khoshelham*

Main category: cs.CV

TL;DR: SG-LDM是一种基于语义引导的激光雷达扩散模型，通过潜在对齐实现语义到激光雷达的合成，并在生成高保真点云方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有激光雷达点云生成方法缺乏语义引导的问题，提升数据增强和下游任务的性能。

Method: 提出SG-LDM模型，利用潜在对齐和显式语义条件，直接在激光雷达空间操作，并开发扩散式激光雷达翻译框架。

Result: SG-LDM在生成高保真点云方面优于现有方法，翻译框架进一步提升了激光雷达分割任务的数据增强效果。

Conclusion: SG-LDM为激光雷达点云生成和翻译提供了高效解决方案，显著提升了下游任务的性能。

Abstract: Lidar point cloud synthesis based on generative models offers a promising solution to augment deep learning pipelines, particularly when real-world data is scarce or lacks diversity. By enabling flexible object manipulation, this synthesis approach can significantly enrich training datasets and enhance discriminative models. However, existing methods focus on unconditional lidar point cloud generation, overlooking their potential for real-world applications. In this paper, we propose SG-LDM, a Semantic-Guided Lidar Diffusion Model that employs latent alignment to enable robust semantic-to-lidar synthesis. By directly operating in the native lidar space and leveraging explicit semantic conditioning, SG-LDM achieves state-of-the-art performance in generating high-fidelity lidar point clouds guided by semantic labels. Moreover, we propose the first diffusion-based lidar translation framework based on SG-LDM, which enables cross-domain translation as a domain adaptation strategy to enhance downstream perception performance. Systematic experiments demonstrate that SG-LDM significantly outperforms existing lidar diffusion models and the proposed lidar translation framework further improves data augmentation performance in the downstream lidar segmentation task.

</details>


### [51] [AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention](https://arxiv.org/abs/2506.23611)
*Ziao Liu,Zhenjia Li,Yifeng Shi,Xiangang Li*

Main category: cs.CV

TL;DR: AttentionGS提出了一种新框架，通过结构注意力直接从随机初始化进行3D重建，解决了3D高斯泼溅（3DGS）对高质量点云的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 3DGS依赖于结构从运动（SfM）生成的高质量点云，但在纹理缺失或视角受限的场景中表现不佳。

Method: AttentionGS结合几何注意力和纹理注意力，早期恢复全局结构，后期细化细节，并使用不透明度加权梯度指导高斯密集化。

Result: 实验表明，AttentionGS在点云初始化不可靠的场景中显著优于现有方法。

Conclusion: 该方法为3DGS在实际应用中提供了更鲁棒和灵活的解决方案。

Abstract: 3D Gaussian Splatting (3DGS) is a powerful alternative to Neural Radiance Fields (NeRF), excelling in complex scene reconstruction and efficient rendering. However, it relies on high-quality point clouds from Structure-from-Motion (SfM), limiting its applicability. SfM also fails in texture-deficient or constrained-view scenarios, causing severe degradation in 3DGS reconstruction. To address this limitation, we propose AttentionGS, a novel framework that eliminates the dependency on high-quality initial point clouds by leveraging structural attention for direct 3D reconstruction from randomly initialization. In the early training stage, we introduce geometric attention to rapidly recover the global scene structure. As training progresses, we incorporate texture attention to refine fine-grained details and enhance rendering quality. Furthermore, we employ opacity-weighted gradients to guide Gaussian densification, leading to improved surface reconstruction. Extensive experiments on multiple benchmark datasets demonstrate that AttentionGS significantly outperforms state-of-the-art methods, particularly in scenarios where point cloud initialization is unreliable. Our approach paves the way for more robust and flexible 3D Gaussian Splatting in real-world applications.

</details>


### [52] [TurboVSR: Fantastic Video Upscalers and Where to Find Them](https://arxiv.org/abs/2506.23618)
*Zhongdao Wang,Guodongfang Zhao,Jingjing Ren,Bailan Feng,Shifeng Zhang,Wenbo Li*

Main category: cs.CV

TL;DR: TurboVSR是一种基于扩散模型的超高效视频超分辨率方法，通过高压缩比自动编码器、分解条件和快捷模型设计，显著提升计算效率，性能与现有方法相当但速度快100倍以上。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的视频超分辨率方法在细节生成上表现优异，但计算效率低下，处理短视频耗时过长。

Method: 1. 使用高压缩比自动编码器减少token数量；2. 引入分解条件降低训练复杂度；3. 将预训练扩散模型转换为快捷模型以减少采样步骤。

Result: TurboVSR性能与现有方法相当，但速度快100倍以上，支持1080p以上分辨率，4K图像超分辨率表现出色。

Conclusion: TurboVSR通过高效设计解决了扩散模型的计算效率问题，为更高分辨率的超分辨率任务提供了可能。

Abstract: Diffusion-based generative models have demonstrated exceptional promise in the video super-resolution (VSR) task, achieving a substantial advancement in detail generation relative to prior methods. However, these approaches face significant computational efficiency challenges. For instance, current techniques may require tens of minutes to super-resolve a mere 2-second, 1080p video. In this paper, we present TurboVSR, an ultra-efficient diffusion-based video super-resolution model. Our core design comprises three key aspects: (1) We employ an autoencoder with a high compression ratio of 32$\times$32$\times$8 to reduce the number of tokens. (2) Highly compressed latents pose substantial challenges for training. We introduce factorized conditioning to mitigate the learning complexity: we first learn to super-resolve the initial frame; subsequently, we condition the super-resolution of the remaining frames on the high-resolution initial frame and the low-resolution subsequent frames. (3) We convert the pre-trained diffusion model to a shortcut model to enable fewer sampling steps, further accelerating inference. As a result, TurboVSR performs on par with state-of-the-art VSR methods, while being 100+ times faster, taking only 7 seconds to process a 2-second long 1080p video. TurboVSR also supports image resolution by considering image as a one-frame video. Our efficient design makes SR beyond 1080p possible, results on 4K (3648$\times$2048) image SR show surprising fine details.

</details>


### [53] [Blending Concepts with Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.23630)
*Lorenzo Olearo,Giorgio Longari,Alessandro Raganato,Rafael Peñaloza,Simone Melzi*

Main category: cs.CV

TL;DR: 扩散模型在零样本框架下能够将不同概念（从具体对象到抽象想法）融合成连贯的新视觉实体，展示了其创造性混合能力。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型是否能够在不进行额外训练或微调的情况下，将多个概念的关键属性融合成单一新颖图像。

Method: 研究了四种混合方法，包括提示调度、嵌入插值和分层条件等，利用扩散管道的不同方面。

Result: 实验表明扩散模型具有创造性混合能力，但不同方法在不同场景下表现各异，受提示顺序、概念距离和随机种子等因素影响。

Conclusion: 扩散模型展现出显著的组合潜力，但对输入变化的敏感性也暴露无遗。

Abstract: Diffusion models have dramatically advanced text-to-image generation in recent years, translating abstract concepts into high-fidelity images with remarkable ease. In this work, we examine whether they can also blend distinct concepts, ranging from concrete objects to intangible ideas, into coherent new visual entities under a zero-shot framework. Specifically, concept blending merges the key attributes of multiple concepts (expressed as textual prompts) into a single, novel image that captures the essence of each concept. We investigate four blending methods, each exploiting different aspects of the diffusion pipeline (e.g., prompt scheduling, embedding interpolation, or layer-wise conditioning). Through systematic experimentation across diverse concept categories, such as merging concrete concepts, synthesizing compound words, transferring artistic styles, and blending architectural landmarks, we show that modern diffusion models indeed exhibit creative blending capabilities without further training or fine-tuning. Our extensive user study, involving 100 participants, reveals that no single approach dominates in all scenarios: each blending technique excels under certain conditions, with factors like prompt ordering, conceptual distance, and random seed affecting the outcome. These findings highlight the remarkable compositional potential of diffusion models while exposing their sensitivity to seemingly minor input variations.

</details>


### [54] [VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation](https://arxiv.org/abs/2506.23641)
*Peng Huang,Junhu Fu,Bowen Guo,Zeju Li,Yuanyuan Wang,Yi Guo*

Main category: cs.CV

TL;DR: 论文提出VAP-Diffusion框架，利用多模态大语言模型（MLLMs）的外部知识生成更真实多样的医学图像，解决了医学图像生成中缺乏详细描述的问题。


<details>
  <summary>Details</summary>
Motivation: 医学图像的外观受多种因素影响，生成模型需要超越标签的丰富属性信息（如形状、大小、纹理等）来生成真实多样的图像，但这些详细描述往往难以获取。

Method: 设计了基于Chain-of-Thoughts的提示词从MLLMs中提取描述，训练时存储描述并按类别随机检索；提出原型条件机制，确保测试时对未见描述组合的鲁棒性。

Result: 在三种常见医学图像类型（皮肤、结直肠、胸部X光）的四个数据集上验证了VAP-Diffusion的有效性。

Conclusion: VAP-Diffusion通过利用MLLMs的外部知识，显著提升了医学图像生成的质量和多样性。

Abstract: As the appearance of medical images is influenced by multiple underlying factors, generative models require rich attribute information beyond labels to produce realistic and diverse images. For instance, generating an image of skin lesion with specific patterns demands descriptions that go beyond diagnosis, such as shape, size, texture, and color. However, such detailed descriptions are not always accessible. To address this, we explore a framework, termed Visual Attribute Prompts (VAP)-Diffusion, to leverage external knowledge from pre-trained Multi-modal Large Language Models (MLLMs) to improve the quality and diversity of medical image generation. First, to derive descriptions from MLLMs without hallucination, we design a series of prompts following Chain-of-Thoughts for common medical imaging tasks, including dermatologic, colorectal, and chest X-ray images. Generated descriptions are utilized during training and stored across different categories. During testing, descriptions are randomly retrieved from the corresponding category for inference. Moreover, to make the generator robust to unseen combination of descriptions at the test time, we propose a Prototype Condition Mechanism that restricts test embeddings to be similar to those from training. Experiments on three common types of medical imaging across four datasets verify the effectiveness of VAP-Diffusion.

</details>


### [55] [A Unified Framework for Stealthy Adversarial Generation via Latent Optimization and Transferability Enhancement](https://arxiv.org/abs/2506.23676)
*Gaozheng Pei,Ke Ma,Dongpeng Zhang,Chengzhi Sun,Qianqian Xu,Qingming Huang*

Main category: cs.CV

TL;DR: 提出了一种统一框架，将传统对抗样本迁移增强策略融入基于扩散模型的图像编辑方法，提升其在更广泛下游任务中的适用性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的对抗样本在传统图像分类任务中表现良好，但在Deepfake检测等任务中泛化能力不足，且传统迁移增强策略难以适配。

Method: 开发了一个统一框架，将传统迁移增强策略与扩散模型结合，通过图像编辑生成对抗样本。

Result: 方法在ACM MM25竞赛中获得第一名，验证了其有效性。

Conclusion: 该框架成功解决了扩散模型在生成对抗样本时的泛化和迁移问题，适用于更广泛的任务。

Abstract: Due to their powerful image generation capabilities, diffusion-based adversarial example generation methods through image editing are rapidly gaining popularity. However, due to reliance on the discriminative capability of the diffusion model, these diffusion-based methods often struggle to generalize beyond conventional image classification tasks, such as in Deepfake detection. Moreover, traditional strategies for enhancing adversarial example transferability are challenging to adapt to these methods. To address these challenges, we propose a unified framework that seamlessly incorporates traditional transferability enhancement strategies into diffusion model-based adversarial example generation via image editing, enabling their application across a wider range of downstream tasks. Our method won first place in the "1st Adversarial Attacks on Deepfake Detectors: A Challenge in the Era of AI-Generated Media" competition at ACM MM25, which validates the effectiveness of our approach.

</details>


### [56] [SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation](https://arxiv.org/abs/2506.23690)
*Shuai Tan,Biao Gong,Yujie Wei,Shiwei Zhang,Zhuoxin Liu,Dandan Zheng,Jingdong Chen,Yan Wang,Hao Ouyang,Kecheng Zheng,Yujun Shen*

Main category: cs.CV

TL;DR: SynMotion是一种新的运动定制视频生成模型，通过结合语义引导和视觉适应，解决了现有方法在语义对齐和视觉复杂性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视频运动定制中要么过于依赖语义对齐，导致忽略视觉复杂性，要么仅调整视觉表示，导致语义混淆。SynMotion旨在同时解决这两个问题。

Method: 提出双嵌入语义理解机制分离主题和运动表示，并集成参数高效的运动适配器以增强运动保真度。采用交替优化策略训练主题和运动嵌入。

Result: 实验表明，SynMotion在文本到视频和图像到视频任务中均优于现有基线。

Conclusion: SynMotion通过联合语义和视觉优化，实现了高质量的运动定制视频生成，并提供了新的基准数据集MotionBench。

Abstract: Diffusion-based video motion customization facilitates the acquisition of human motion representations from a few video samples, while achieving arbitrary subjects transfer through precise textual conditioning. Existing approaches often rely on semantic-level alignment, expecting the model to learn new motion concepts and combine them with other entities (e.g., ''cats'' or ''dogs'') to produce visually appealing results. However, video data involve complex spatio-temporal patterns, and focusing solely on semantics cause the model to overlook the visual complexity of motion. Conversely, tuning only the visual representation leads to semantic confusion in representing the intended action. To address these limitations, we propose SynMotion, a new motion-customized video generation model that jointly leverages semantic guidance and visual adaptation. At the semantic level, we introduce the dual-embedding semantic comprehension mechanism which disentangles subject and motion representations, allowing the model to learn customized motion features while preserving its generative capabilities for diverse subjects. At the visual level, we integrate parameter-efficient motion adapters into a pre-trained video generation model to enhance motion fidelity and temporal coherence. Furthermore, we introduce a new embedding-specific training strategy which \textbf{alternately optimizes} subject and motion embeddings, supported by the manually constructed Subject Prior Video (SPV) training dataset. This strategy promotes motion specificity while preserving generalization across diverse subjects. Lastly, we introduce MotionBench, a newly curated benchmark with diverse motion patterns. Experimental results across both T2V and I2V settings demonstrate that \method outperforms existing baselines. Project page: https://lucaria-academy.github.io/SynMotion/

</details>


### [57] [Proteus-ID: ID-Consistent and Motion-Coherent Video Customization](https://arxiv.org/abs/2506.23729)
*Guiyu Zhang,Chen Shi,Zijian Jiang,Xunzhi Xiang,Jingjing Qian,Shaoshuai Shi,Li Jiang*

Main category: cs.CV

TL;DR: Proteus-ID是一种基于扩散的框架，用于解决视频身份定制中的身份一致性和运动连贯性问题，通过多模态身份融合和时间感知身份注入等方法提升效果。


<details>
  <summary>Details</summary>
Motivation: 视频身份定制任务面临身份一致性与运动自然性的挑战，现有方法难以平衡这两点。

Method: 提出MIF模块统一视觉和文本线索，TAII机制动态调节身份条件，AML策略通过自监督学习增强运动真实性。

Result: Proteus-ID在身份保持、文本对齐和运动质量上优于现有方法，建立了新的基准。

Conclusion: Proteus-ID为视频身份定制提供了高效解决方案，代码和数据已公开。

Abstract: Video identity customization seeks to synthesize realistic, temporally coherent videos of a specific subject, given a single reference image and a text prompt. This task presents two core challenges: (1) maintaining identity consistency while aligning with the described appearance and actions, and (2) generating natural, fluid motion without unrealistic stiffness. To address these challenges, we introduce Proteus-ID, a novel diffusion-based framework for identity-consistent and motion-coherent video customization. First, we propose a Multimodal Identity Fusion (MIF) module that unifies visual and textual cues into a joint identity representation using a Q-Former, providing coherent guidance to the diffusion model and eliminating modality imbalance. Second, we present a Time-Aware Identity Injection (TAII) mechanism that dynamically modulates identity conditioning across denoising steps, improving fine-detail reconstruction. Third, we propose Adaptive Motion Learning (AML), a self-supervised strategy that reweights the training loss based on optical-flow-derived motion heatmaps, enhancing motion realism without requiring additional inputs. To support this task, we construct Proteus-Bench, a high-quality dataset comprising 200K curated clips for training and 150 individuals from diverse professions and ethnicities for evaluation. Extensive experiments demonstrate that Proteus-ID outperforms prior methods in identity preservation, text alignment, and motion quality, establishing a new benchmark for video identity customization. Codes and data are publicly available at https://grenoble-zhang.github.io/Proteus-ID/.

</details>


### [58] [Can We Challenge Open-Vocabulary Object Detectors with Generated Content in Street Scenes?](https://arxiv.org/abs/2506.23751)
*Annika Mütze,Sadia Ilyas,Christian Dörpelkus,Matthias Rottmann*

Main category: cs.CV

TL;DR: 研究者使用合成数据挑战开放词汇目标检测器，发现其性能受物体位置而非语义影响。


<details>
  <summary>Details</summary>
Motivation: 开放词汇目标检测器在安全关键应用中存在局限性，但真实数据难以系统评估其泛化能力。

Method: 设计自动化流程，利用稳定扩散技术生成多样语义的合成数据，评估多种检测器。

Result: 合成数据可挑战检测器，发现其性能依赖物体位置而非语义。

Conclusion: 合成数据为挑战和改进开放词汇检测器提供了系统方法。

Abstract: Open-vocabulary object detectors such as Grounding DINO are trained on vast and diverse data, achieving remarkable performance on challenging datasets. Due to that, it is unclear where to find their limitations, which is of major concern when using in safety-critical applications. Real-world data does not provide sufficient control, required for a rigorous evaluation of model generalization. In contrast, synthetically generated data allows to systematically explore the boundaries of model competence/generalization. In this work, we address two research questions: 1) Can we challenge open-vocabulary object detectors with generated image content? 2) Can we find systematic failure modes of those models? To address these questions, we design two automated pipelines using stable diffusion to inpaint unusual objects with high diversity in semantics, by sampling multiple substantives from WordNet and ChatGPT. On the synthetically generated data, we evaluate and compare multiple open-vocabulary object detectors as well as a classical object detector. The synthetic data is derived from two real-world datasets, namely LostAndFound, a challenging out-of-distribution (OOD) detection benchmark, and the NuImages dataset. Our results indicate that inpainting can challenge open-vocabulary object detectors in terms of overlooking objects. Additionally, we find a strong dependence of open-vocabulary models on object location, rather than on object semantics. This provides a systematic approach to challenge open-vocabulary models and gives valuable insights on how data could be acquired to effectively improve these models.

</details>


### [59] [Controllable Reference-Based Real-World Remote Sensing Image Super-Resolution with Generative Diffusion Priors](https://arxiv.org/abs/2506.23801)
*Ce Wang,Wanjie Sun*

Main category: cs.CV

TL;DR: 提出了一种名为CRefDiff的新型可控参考扩散模型，用于解决遥感图像超分辨率中的实际问题，如跨传感器分辨率差距和地表覆盖变化。


<details>
  <summary>Details</summary>
Motivation: 现有参考超分辨率方法在真实场景中表现不佳，存在生成不足或过度依赖参考图像的问题。

Method: 基于预训练的Stable Diffusion模型，引入双分支融合机制自适应整合参考图像的局部和全局信息，并提出Better Start策略加速推理。

Result: 在Real-RefRSSRD数据集上，CRefDiff在各项指标上达到最优，并提升了场景分类和语义分割等下游任务性能。

Conclusion: CRefDiff通过可控参考和高效推理，显著提升了遥感图像超分辨率的实用性和灵活性。

Abstract: Super-resolution (SR) techniques can enhance the spatial resolution of remote sensing images by utilizing low-resolution (LR) images to reconstruct high-resolution (HR) images, enabling more efficient large-scale earth observation applications. While single-image super-resolution (SISR) methods have shown progress, reference-based super-resolution (RefSR) offers superior performance by incorporating historical HR images alongside current LR observations. However, existing RefSR methods struggle with real-world complexities, such as cross-sensor resolution gap and significant land cover changes, often leading to under-generation or over-reliance on reference image. To address these challenges, we propose CRefDiff, a novel controllable reference-based diffusion model for real-world remote sensing image SR. To address the under-generation problem, CRefDiff is built upon the pretrained Stable Diffusion model, leveraging its powerful generative prior to produce accurate structures and textures. To mitigate over-reliance on the reference, we introduce a dual-branch fusion mechanism that adaptively integrates both local and global information from the reference image. Moreover, this novel dual-branch design enables reference strength control during inference, enhancing interactivity and flexibility of the model. Finally, a strategy named Better Start is proposed to significantly reduce the number of denoising steps, thereby accelerating the inference process. To support further research, we introduce Real-RefRSSRD, a new real-world RefSR dataset for remote sensing images, consisting of HR NAIP and LR Sentinel-2 image pairs with diverse land cover changes and significant temporal gaps. Extensive experiments on Real-RefRSSRD show that CRefDiff achieves state-of-the-art performance across various metrics and improves downstream tasks such as scene classification and semantic segmentation.

</details>


### [60] [VMoBA: Mixture-of-Block Attention for Video Diffusion Models](https://arxiv.org/abs/2506.23858)
*Jianzong Wu,Liang Hou,Haotian Yang,Xin Tao,Ye Tian,Pengfei Wan,Di Zhang,Yunhai Tong*

Main category: cs.CV

TL;DR: 论文提出了一种名为VMoBA的新型稀疏注意力机制，用于优化视频扩散模型（VDMs）的长序列训练和高分辨率视频生成效率。


<details>
  <summary>Details</summary>
Motivation: 全注意力机制的二次复杂度限制了VDMs在长视频生成中的效率，现有稀疏注意力方法未能充分捕捉视频数据的时空特性。

Method: VMoBA通过层递进的块分区（1D-2D-3D）、全局块选择和基于阈值的块选择，动态适应视频数据的时空注意力模式。

Result: 实验显示VMoBA显著提升了训练效率（2.92x FLOPs和1.48x延迟加速），生成质量与全注意力相当或更优。

Conclusion: VMoBA在训练和推理中均表现出高效性，为高分辨率视频生成提供了实用的解决方案。

Abstract: The quadratic complexity of full attention mechanisms poses a significant bottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration, high-resolution videos. While various sparse attention methods have been proposed, many are designed as training-free inference accelerators or do not optimally capture the unique spatio-temporal characteristics inherent in video data when trained natively. This paper introduces Video Mixture of Block Attention (VMoBA), a novel sparse attention mechanism specifically adapted for VDMs. Motivated by an in-depth analysis of attention patterns within pre-trained video transformers, which revealed strong spatio-temporal locality, varying query importance, and head-specific concentration levels, VMoBA enhances the original MoBA framework with three key modifications: (1) a layer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to diverse spatio-temporal attention patterns and improve efficiency; (2) global block selection to prioritize the most salient query-key block interactions across an entire attention head; and (3) threshold-based block selection to dynamically determine the number of attended blocks based on their cumulative similarity. Extensive experiments demonstrate that VMoBA significantly accelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and 1.48x latency speedup, while attaining comparable or even superior generation quality to full attention. Furthermore, VMoBA exhibits competitive performance in training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for high-res video generation.

</details>


### [61] [GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in Ultrasound with LoRA-Tuned Vision-Language Models](https://arxiv.org/abs/2506.23903)
*Hamza Rasaee,Taha Koleilat,Hassan Rivaz*

Main category: cs.CV

TL;DR: 提出了一种基于提示驱动的视觉语言模型（VLM），结合Grounding DINO和SAM2，用于多器官超声图像分割，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决超声图像中因解剖变异、成像协议多样性和标注数据有限导致的对象分割难题。

Method: 利用18个公共超声数据集，通过LoRA微调Grounding DINO，并在未见数据集上测试性能。

Result: 在多数数据集上优于UniverSeg、MedSAM等方法，且在未见数据集上表现良好。

Conclusion: VLM在超声图像分析中具有潜力，减少了对大规模器官特定标注数据的依赖。

Abstract: Accurate and generalizable object segmentation in ultrasound imaging remains a significant challenge due to anatomical variability, diverse imaging protocols, and limited annotated data. In this study, we propose a prompt-driven vision-language model (VLM) that integrates Grounding DINO with SAM2 to enable object segmentation across multiple ultrasound organs. A total of 18 public ultrasound datasets, encompassing the breast, thyroid, liver, prostate, kidney, and paraspinal muscle, were utilized. These datasets were divided into 15 for fine-tuning and validation of Grounding DINO using Low Rank Adaptation (LoRA) to the ultrasound domain, and 3 were held out entirely for testing to evaluate performance in unseen distributions. Comprehensive experiments demonstrate that our approach outperforms state-of-the-art segmentation methods, including UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse, and SAMUS on most seen datasets while maintaining strong performance on unseen datasets without additional fine-tuning. These results underscore the promise of VLMs in scalable and robust ultrasound image analysis, reducing dependence on large, organ-specific annotated datasets. We will publish our code on code.sonography.ai after acceptance.

</details>


### [62] [Continual Adaptation: Environment-Conditional Parameter Generation for Object Detection in Dynamic Scenarios](https://arxiv.org/abs/2506.24063)
*Deng Li,Aming Wu,Yang Li,Yaowei Wang,Yahong Han*

Main category: cs.CV

TL;DR: 论文提出了一种新的持续测试时适应机制，通过参数生成而非微调来提升目标检测器的泛化能力，解决了传统方法因少量测试图像导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现实环境中数据分布随时间空间变化，传统基于闭集假设的目标检测器难以适应。持续测试时适应方法通过微调特定参数（如BatchNorm层）提升泛化能力，但少量测试图像可能导致其他固定参数表征能力下降。

Method: 设计了双路径LoRA域感知适配器，分离域不变和域特定特征；提出基于条件扩散的参数生成机制，动态合成适配器参数；引入类中心最优传输对齐方法缓解灾难性遗忘。

Result: 在多种连续域自适应目标检测任务上的实验证明了方法的有效性，可视化结果显示生成的参数能提取更多目标相关信息并增强泛化能力。

Conclusion: 通过参数生成机制和域感知适配器，该方法显著提升了目标检测器在动态环境中的适应能力和性能。

Abstract: In practice, environments constantly change over time and space, posing significant challenges for object detectors trained based on a closed-set assumption, i.e., training and test data share the same distribution. To this end, continual test-time adaptation has attracted much attention, aiming to improve detectors' generalization by fine-tuning a few specific parameters, e.g., BatchNorm layers. However, based on a small number of test images, fine-tuning certain parameters may affect the representation ability of other fixed parameters, leading to performance degradation. Instead, we explore a new mechanism, i.e., converting the fine-tuning process to a specific-parameter generation. Particularly, we first design a dual-path LoRA-based domain-aware adapter that disentangles features into domain-invariant and domain-specific components, enabling efficient adaptation. Additionally, a conditional diffusion-based parameter generation mechanism is presented to synthesize the adapter's parameters based on the current environment, preventing the optimization from getting stuck in local optima. Finally, we propose a class-centered optimal transport alignment method to mitigate catastrophic forgetting. Extensive experiments conducted on various continuous domain adaptive object detection tasks demonstrate the effectiveness. Meanwhile, visualization results show that the representation extracted by the generated parameters can capture more object-related information and strengthen the generalization ability.

</details>


### [63] [MotionGPT3: Human Motion as a Second Modality](https://arxiv.org/abs/2506.24086)
*Bingfan Zhu,Biao Jiang,Sunyi Wang,Shixiang Tang,Tao Chen,Linjie Luo,Youyi Zheng,Xin Chen*

Main category: cs.CV

TL;DR: MotionGPT3是一个双模态运动-语言模型，通过分离参数处理运动建模，解决了运动模态与离散表示之间的重建差距和语言智能退化问题。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态模型在统一理解和生成方面表现出强大能力，但统一运动-语言模型的开发仍未被充分探索。

Method: 采用混合专家方法，MotionGPT3将运动作为第二模态，通过共享注意力机制实现跨模态交互，同时保留预训练语言模型的结构和参数。

Result: 实验表明，MotionGPT3在运动理解和生成任务上表现优异，同时保持了强大的语言能力。

Conclusion: MotionGPT3建立了一个统一的双模态运动扩散框架，为运动-语言模型的发展提供了新思路。

Abstract: Though recent advances in multimodal models have demonstrated strong capabilities and opportunities in unified understanding and generation, the development of unified motion-language models remains underexplored. To enable such models with high-fidelity human motion, two core challenges must be addressed. The first is the reconstruction gap between the continuous motion modality and discrete representation in an autoregressive manner, and the second is the degradation of language intelligence during unified training. Inspired by the mixture of experts, we propose MotionGPT3, a bimodal motion-language model that treats human motion as a second modality, decoupling motion modeling via separate model parameters and enabling both effective cross-modal interaction and efficient multimodal scaling training. To preserve language intelligence, the text branch retains the original structure and parameters of the pretrained language model, while a new motion branch is integrated via a shared attention mechanism, enabling bidirectional information flow between two modalities. We first employ a motion Variational Autoencoder (VAE) to encode raw human motion into latent representations. Based on this continuous latent space, the motion branch predicts motion latents directly from intermediate hidden states using a diffusion head, bypassing discrete tokenization. Extensive experiments show that our approach achieves competitive performance on both motion understanding and generation tasks while preserving strong language capabilities, establishing a unified bimodal motion diffusion framework within an autoregressive manner.

</details>


### [64] [WaRA: Wavelet Low Rank Adaptation](https://arxiv.org/abs/2506.24092)
*Moein Heidari,Yasamin Medghalchi,Mahdi Khoursha,Reza Rezaeian,Ilker Hacihaliloglu*

Main category: cs.CV

TL;DR: WaRA是一种新的参数高效微调方法，利用小波变换分解权重更新矩阵，实现多分辨率分析，优于传统LoRA方法。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法（如LoRA）依赖全局低秩分解，忽略了局部或多尺度结构，无法捕捉权重更新中的复杂模式。

Method: WaRA通过小波变换将权重更新矩阵分解为多分辨率表示，在频域进行低秩分解，并通过逆变换重建更新。

Result: WaRA在视觉任务（如图像生成、分类和语义分割）中表现优异，同时降低了计算复杂度，并在语言任务中展示了通用性。

Conclusion: WaRA通过多分辨率分析提供了更灵活和稀疏的表示，显著提升了生成图像质量，并具有广泛适用性。

Abstract: Parameter-efficient fine-tuning (PEFT) has gained widespread adoption across various applications. Among PEFT techniques, Low-Rank Adaptation (LoRA) and its extensions have emerged as particularly effective, allowing efficient model adaptation while significantly reducing computational overhead. However, existing approaches typically rely on global low-rank factorizations, which overlook local or multi-scale structure, failing to capture complex patterns in the weight updates. To address this, we propose WaRA, a novel PEFT method that leverages wavelet transforms to decompose the weight update matrix into a multi-resolution representation. By performing low-rank factorization in the wavelet domain and reconstructing updates through an inverse transform, WaRA obtains compressed adaptation parameters that harness multi-resolution analysis, enabling it to capture both coarse and fine-grained features while providing greater flexibility and sparser representations than standard LoRA. Through comprehensive experiments and analysis, we demonstrate that WaRA performs superior on diverse vision tasks, including image generation, classification, and semantic segmentation, significantly enhancing generated image quality while reducing computational complexity. Although WaRA was primarily designed for vision tasks, we further showcase its effectiveness in language tasks, highlighting its broader applicability and generalizability. The code is publicly available at \href{GitHub}{https://github.com/moeinheidari7829/WaRA}.

</details>


### [65] [MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient Surface Reconstruction](https://arxiv.org/abs/2506.24096)
*Antoine Guédon,Diego Gomez,Nissim Maruani,Bingchen Gong,George Drettakis,Maks Ovsjanikov*

Main category: cs.CV

TL;DR: MILo是一种新型高斯泼溅框架，通过可微分地从3D高斯中提取网格，弥合了体积和表面表示之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前方法通过昂贵的后处理步骤提取表面，导致几何细节丢失或生成密集网格，限制了最终网格保留训练期间捕获的所有几何结构的能力。

Method: 设计了一种完全可微分的过程，直接从高斯参数构建网格，包括顶点位置和连接性，并引入了双向一致性框架、自适应网格提取过程和基于高斯的有符号距离计算方法。

Result: 能够以最先进的质量重建完整场景，同时比先前方法少一个数量级的网格顶点。

Conclusion: MILo生成的网格轻量且内部为空，适用于物理模拟或动画等下游应用。

Abstract: While recent advances in Gaussian Splatting have enabled fast reconstruction of high-quality 3D scenes from images, extracting accurate surface meshes remains a challenge. Current approaches extract the surface through costly post-processing steps, resulting in the loss of fine geometric details or requiring significant time and leading to very dense meshes with millions of vertices. More fundamentally, the a posteriori conversion from a volumetric to a surface representation limits the ability of the final mesh to preserve all geometric structures captured during training. We present MILo, a novel Gaussian Splatting framework that bridges the gap between volumetric and surface representations by differentiably extracting a mesh from the 3D Gaussians. We design a fully differentiable procedure that constructs the mesh-including both vertex locations and connectivity-at every iteration directly from the parameters of the Gaussians, which are the only quantities optimized during training. Our method introduces three key technical contributions: a bidirectional consistency framework ensuring both representations-Gaussians and the extracted mesh-capture the same underlying geometry during training; an adaptive mesh extraction process performed at each training iteration, which uses Gaussians as differentiable pivots for Delaunay triangulation; a novel method for computing signed distance values from the 3D Gaussians that enables precise surface extraction while avoiding geometric erosion. Our approach can reconstruct complete scenes, including backgrounds, with state-of-the-art quality while requiring an order of magnitude fewer mesh vertices than previous methods. Due to their light weight and empty interior, our meshes are well suited for downstream applications such as physics simulations or animation.

</details>


### [66] [Epona: Autoregressive Diffusion World Model for Autonomous Driving](https://arxiv.org/abs/2506.24113)
*Kaiwen Zhang,Zhenyu Tang,Xiaotao Hu,Xingang Pan,Xiaoyang Guo,Yuan Liu,Jingwei Huang,Li Yuan,Qian Zhang,Xiao-Xiao Long,Xun Cao,Wei Yin*

Main category: cs.CV

TL;DR: Epona是一种自回归扩散世界模型，通过解耦时空因子化和模块化轨迹与视频预测，实现了高分辨率、长时长的生成，并在NAVSIM基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于视频扩散的世界模型在灵活长度、长时预测和轨迹规划集成方面表现不佳，因其依赖固定长度帧序列的全局联合分布建模。

Method: 提出Epona，采用解耦时空因子化和模块化轨迹与视频预测，结合链式前向训练策略以减少自回归循环中的误差积累。

Result: 实验显示FVD提升7.4%，预测时长显著延长，并在NAVSIM基准测试中优于端到端规划器。

Conclusion: Epona在视频生成和运动规划中表现出色，为自动驾驶世界建模提供了高效解决方案。

Abstract: Diffusion models have demonstrated exceptional visual quality in video generation, making them promising for autonomous driving world modeling. However, existing video diffusion-based world models struggle with flexible-length, long-horizon predictions and integrating trajectory planning. This is because conventional video diffusion models rely on global joint distribution modeling of fixed-length frame sequences rather than sequentially constructing localized distributions at each timestep. In this work, we propose Epona, an autoregressive diffusion world model that enables localized spatiotemporal distribution modeling through two key innovations: 1) Decoupled spatiotemporal factorization that separates temporal dynamics modeling from fine-grained future world generation, and 2) Modular trajectory and video prediction that seamlessly integrate motion planning with visual modeling in an end-to-end framework. Our architecture enables high-resolution, long-duration generation while introducing a novel chain-of-forward training strategy to address error accumulation in autoregressive loops. Experimental results demonstrate state-of-the-art performance with 7.4\% FVD improvement and minutes longer prediction duration compared to prior works. The learned world model further serves as a real-time motion planner, outperforming strong end-to-end planners on NAVSIM benchmarks. Code will be publicly available at \href{https://github.com/Kevin-thu/Epona/}{https://github.com/Kevin-thu/Epona/}.

</details>


### [67] [TextMesh4D: High-Quality Text-to-4D Mesh Generation](https://arxiv.org/abs/2506.24121)
*Sisi Dai,Xinxin Su,Boyan Wan,Ruizhen Hu,Kai Xu*

Main category: cs.CV

TL;DR: TextMesh4D是一个新的框架，用于从文本生成高质量的4D内容，通过静态对象创建和动态运动合成的两阶段方法实现。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散生成模型在图像、视频和3D内容生成方面取得了进展，但动态3D内容生成（文本到4D）仍是一个未充分探索的挑战性问题。

Method: 使用每面Jacobian作为可微分网格表示，将4D生成分为静态对象创建和动态运动合成两阶段，并引入灵活性-刚性正则化项以稳定优化。

Result: TextMesh4D在时间一致性、结构保真度和视觉真实感方面达到最先进水平，且仅需单个24GB GPU。

Conclusion: TextMesh4D为文本驱动的4D网格生成提供了高效且高质量的解决方案，代码将开源以促进未来研究。

Abstract: Recent advancements in diffusion generative models significantly advanced image, video, and 3D content creation from user-provided text prompts. However, the challenging problem of dynamic 3D content generation (text-to-4D) with diffusion guidance remains largely unexplored. In this paper, we introduce TextMesh4D, a novel framework for high-quality text-to-4D generation. Our approach leverages per-face Jacobians as a differentiable mesh representation and decomposes 4D generation into two stages: static object creation and dynamic motion synthesis. We further propose a flexibility-rigidity regularization term to stabilize Jacobian optimization under video diffusion priors, ensuring robust geometric performance. Experiments demonstrate that TextMesh4D achieves state-of-the-art results in terms of temporal consistency, structural fidelity, and visual realism. Moreover, TextMesh4D operates with a low GPU memory overhead-requiring only a single 24GB GPU-offering a cost-effective yet high-quality solution for text-driven 4D mesh generation. The code will be released to facilitate future research in text-to-4D generation.

</details>


### [68] [Calligrapher: Freestyle Text Image Customization](https://arxiv.org/abs/2506.24123)
*Yue Ma,Qingyan Bai,Hao Ouyang,Ka Leong Cheng,Qiuyu Wang,Hongyu Liu,Zichen Liu,Haofan Wang,Jingye Chen,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: Calligrapher是一个基于扩散模型的框架，结合文本定制与艺术字体设计，解决了风格控制和数据依赖问题。


<details>
  <summary>Details</summary>
Motivation: 解决数字书法和设计中精确风格控制与数据依赖的挑战。

Method: 1. 自蒸馏机制构建风格基准；2. 可训练风格编码器提取特征；3. 上下文生成机制嵌入参考图像。

Result: 在多种字体和设计场景中准确复现风格细节和字形定位，优于传统模型。

Conclusion: Calligrapher自动化高质量字体设计，为数字艺术和品牌设计提供支持。

Abstract: We introduce Calligrapher, a novel diffusion-based framework that innovatively integrates advanced text customization with artistic typography for digital calligraphy and design applications. Addressing the challenges of precise style control and data dependency in typographic customization, our framework incorporates three key technical contributions. First, we develop a self-distillation mechanism that leverages the pre-trained text-to-image generative model itself alongside the large language model to automatically construct a style-centric typography benchmark. Second, we introduce a localized style injection framework via a trainable style encoder, which comprises both Qformer and linear layers, to extract robust style features from reference images. An in-context generation mechanism is also employed to directly embed reference images into the denoising process, further enhancing the refined alignment of target styles. Extensive quantitative and qualitative evaluations across diverse fonts and design contexts confirm Calligrapher's accurate reproduction of intricate stylistic details and precise glyph positioning. By automating high-quality, visually consistent typography, Calligrapher surpasses traditional models, empowering creative practitioners in digital art, branding, and contextual typographic design.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [69] [High Resolution Isotropic 3D Cine imaging with Automated Segmentation using Concatenated 2D Real-time Imaging and Deep Learning](https://arxiv.org/abs/2506.22532)
*Mark Wrobel,Michele Pascale,Tina Yao,Ruaraidh Campbell,Elena Milano,Michael Quail,Jennifer Steeden,Vivek Muthurangu*

Main category: eess.IV

TL;DR: 利用深度学习模型将2D实时电影图像拼接并转换为3D电影数据集，验证了其在心血管磁共振中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统心血管磁共振（CMR）在儿科和先天性心脏病中使用2D呼吸门控技术和3D静态成像，但耗时较长。本研究旨在通过深度学习快速生成3D电影数据集，提高效率。

Method: 训练了四个深度学习模型，分别用于对比校正、呼吸运动校正、超分辨率和心脏结构分割。在10名患者中验证了该方法。

Result: 成功将2D图像转换为3D电影，处理时间小于1分钟，与常规成像方法在心室体积和血管直径上具有良好一致性。

Conclusion: 该方法能够快速生成3D电影数据集，有望显著提升临床CMR检查的效率。

Abstract: Background: Conventional cardiovascular magnetic resonance (CMR) in paediatric and congenital heart disease uses 2D, breath-hold, balanced steady state free precession (bSSFP) cine imaging for assessment of function and cardiac-gated, respiratory-navigated, static 3D bSSFP whole-heart imaging for anatomical assessment. Our aim is to concatenate a stack 2D free-breathing real-time cines and use Deep Learning (DL) to create an isotropic a fully segmented 3D cine dataset from these images. Methods: Four DL models were trained on open-source data that performed: a) Interslice contrast correction; b) Interslice respiratory motion correction; c) Super-resolution (slice direction); and d) Segmentation of right and left atria and ventricles (RA, LA, RV, and LV), thoracic aorta (Ao) and pulmonary arteries (PA). In 10 patients undergoing routine cardiovascular examination, our method was validated on prospectively acquired sagittal stacks of real-time cine images. Quantitative metrics (ventricular volumes and vessel diameters) and image quality of the 3D cines were compared to conventional breath hold cine and whole heart imaging. Results: All real-time data were successfully transformed into 3D cines with a total post-processing time of <1 min in all cases. There were no significant biases in any LV or RV metrics with reasonable limits of agreement and correlation. There is also reasonable agreement for all vessel diameters, although there was a small but significant overestimation of RPA diameter. Conclusion: We have demonstrated the potential of creating a 3D-cine data from concatenated 2D real-time cine images using a series of DL models. Our method has short acquisition and reconstruction times with fully segmented data being available within 2 minutes. The good agreement with conventional imaging suggests that our method could help to significantly speed up CMR in clinical practice.

</details>


### [70] [CA-Diff: Collaborative Anatomy Diffusion for Brain Tissue Segmentation](https://arxiv.org/abs/2506.22882)
*Qilong Xing,Zikai Song,Yuteng Ye,Yuke Chen,Youjia Zhang,Na Feng,Junqing Yu,Wei Yang*

Main category: eess.IV

TL;DR: 提出了一种结合解剖特征的扩散模型CA-Diff，用于提升脑MRI分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有CNN和Transformer方法在复杂脑结构分割上表现不佳，扩散模型虽在图像分割中有潜力，但直接应用于脑MRI时忽略了解剖信息。

Method: 提出CA-Diff框架，引入距离场作为解剖条件，结合协作扩散过程建模其联合分布，并设计一致性损失和时间适应通道注意力模块优化特征融合。

Result: 实验表明CA-Diff优于现有SOTA方法。

Conclusion: CA-Diff通过有效利用解剖特征，显著提升了脑MRI分割的准确性。

Abstract: Segmentation of brain structures from MRI is crucial for evaluating brain morphology, yet existing CNN and transformer-based methods struggle to delineate complex structures accurately. While current diffusion models have shown promise in image segmentation, they are inadequate when applied directly to brain MRI due to neglecting anatomical information. To address this, we propose Collaborative Anatomy Diffusion (CA-Diff), a framework integrating spatial anatomical features to enhance segmentation accuracy of the diffusion model. Specifically, we introduce distance field as an auxiliary anatomical condition to provide global spatial context, alongside a collaborative diffusion process to model its joint distribution with anatomical structures, enabling effective utilization of anatomical features for segmentation. Furthermore, we introduce a consistency loss to refine relationships between the distance field and anatomical structures and design a time adapted channel attention module to enhance the U-Net feature fusion procedure. Extensive experiments show that CA-Diff outperforms state-of-the-art (SOTA) methods.

</details>


### [71] [Hierarchical Characterization of Brain Dynamics via State Space-based Vector Quantization](https://arxiv.org/abs/2506.22952)
*Yanwu Yang,Thomas Wolfers*

Main category: eess.IV

TL;DR: 论文提出了一种名为HST的分层状态空间标记化网络，用于量化大脑状态和转换，并通过改进的VQ-VAE提升标记化性能，验证了其在疾病诊断和重建中的潜力。


<details>
  <summary>Details</summary>
Motivation: 理解大脑动态功能状态转换是神经科学的重要挑战，现有方法常忽略转换依赖性和稳定嵌入量化。

Method: 提出HST网络，结合分层状态空间模型和优化的VQ-VAE，引入量化误差反馈和聚类提升性能。

Result: 在两个公开fMRI数据集上验证了HST的有效性，展示了其在疾病诊断和重建中的潜力。

Conclusion: HST为大脑动态和稳定性的量化提供了有前景的框架。

Abstract: Understanding brain dynamics through functional Magnetic Resonance Imaging (fMRI) remains a fundamental challenge in neuroscience, particularly in capturing how the brain transitions between various functional states. Recently, metastability, which refers to temporarily stable brain states, has offered a promising paradigm to quantify complex brain signals into interpretable, discretized representations. In particular, compared to cluster-based machine learning approaches, tokenization approaches leveraging vector quantization have shown promise in representation learning with powerful reconstruction and predictive capabilities. However, most existing methods ignore brain transition dependencies and lack a quantification of brain dynamics into representative and stable embeddings. In this study, we propose a Hierarchical State space-based Tokenization network, termed HST, which quantizes brain states and transitions in a hierarchical structure based on a state space-based model. We introduce a refined clustered Vector-Quantization Variational AutoEncoder (VQ-VAE) that incorporates quantization error feedback and clustering to improve quantization performance while facilitating metastability with representative and stable token representations. We validate our HST on two public fMRI datasets, demonstrating its effectiveness in quantifying the hierarchical dynamics of the brain and its potential in disease diagnosis and reconstruction performance. Our method offers a promising framework for the characterization of brain dynamics, facilitating the analysis of metastability.

</details>


### [72] [Score-based Diffusion Model for Unpaired Virtual Histology Staining](https://arxiv.org/abs/2506.23184)
*Anran Liu,Xiaofei Wang,Jing Cai,Chao Li*

Main category: eess.IV

TL;DR: 本文提出了一种基于互信息引导的扩散模型，用于从H&E图像虚拟生成IHC图像，解决了现有方法在分解染色风格与组织结构、可控染色过程及结构一致性建模方面的挑战。


<details>
  <summary>Details</summary>
Motivation: H&E染色缺乏特异性标记，而IHC染色受限于组织可用性和抗体特异性。虚拟染色技术有望高效生成IHC图像，但现有方法在分解染色风格、可控性和结构一致性方面存在不足。

Method: 采用互信息引导的扩散模型，设计了全局互信息能量函数、时间步定制的反向扩散过程及局部互信息对比学习策略，以实现染色风格与组织结构的解耦、可控染色及细胞级结构一致性。

Result: 实验表明，该方法在虚拟染色任务上优于现有技术，展示了其生物医学应用潜力。

Conclusion: 该方法为虚拟染色提供了一种高效、可控且结构一致的解决方案，代码将在接受后开源。

Abstract: Hematoxylin and eosin (H&E) staining visualizes histology but lacks specificity for diagnostic markers. Immunohistochemistry (IHC) staining provides protein-targeted staining but is restricted by tissue availability and antibody specificity. Virtual staining, i.e., computationally translating the H&E image to its IHC counterpart while preserving the tissue structure, is promising for efficient IHC generation. Existing virtual staining methods still face key challenges: 1) effective decomposition of staining style and tissue structure, 2) controllable staining process adaptable to diverse tissue and proteins, and 3) rigorous structural consistency modelling to handle the non-pixel-aligned nature of paired H&E and IHC images. This study proposes a mutual-information (MI)-guided score-based diffusion model for unpaired virtual staining. Specifically, we design 1) a global MI-guided energy function that disentangles the tissue structure and staining characteristics across modalities, 2) a novel timestep-customized reverse diffusion process for precise control of the staining intensity and structural reconstruction, and 3) a local MI-driven contrastive learning strategy to ensure the cellular level structural consistency between H&E-IHC images. Extensive experiments demonstrate the our superiority over state-of-the-art approaches, highlighting its biomedical potential. Codes will be open-sourced upon acceptance.

</details>


### [73] [SurgTPGS: Semantic 3D Surgical Scene Understanding with Text Promptable Gaussian Splatting](https://arxiv.org/abs/2506.23309)
*Yiming Huang,Long Bai,Beilei Cui,Kun Yuan,Guankun Wang,Mobarakol Islam,Nicolas Padoy,Nassir Navab,Hongliang Ren*

Main category: eess.IV

TL;DR: SurgTPGS是一种新型的文本提示高斯泼溅方法，用于实时3D手术场景查询，结合了语义特征学习和变形跟踪，显著提升了手术场景的重建质量和语义理解。


<details>
  <summary>Details</summary>
Motivation: 当前手术研究中，缺乏支持实时文本提示的3D查询方法，而准确理解3D手术场景对手术规划和实时引导至关重要。

Method: 结合Segment Anything模型和先进视觉语言模型，提出3D语义特征学习策略、语义感知变形跟踪和语义区域感知优化。

Result: 在两个真实手术数据集上的实验表明，SurgTPGS优于现有方法，提升了重建质量和语义平滑性。

Conclusion: SurgTPGS为下一代智能手术系统的发展奠定了基础，提高了手术精度和安全性。

Abstract: In contemporary surgical research and practice, accurately comprehending 3D surgical scenes with text-promptable capabilities is particularly crucial for surgical planning and real-time intra-operative guidance, where precisely identifying and interacting with surgical tools and anatomical structures is paramount. However, existing works focus on surgical vision-language model (VLM), 3D reconstruction, and segmentation separately, lacking support for real-time text-promptable 3D queries. In this paper, we present SurgTPGS, a novel text-promptable Gaussian Splatting method to fill this gap. We introduce a 3D semantics feature learning strategy incorporating the Segment Anything model and state-of-the-art vision-language models. We extract the segmented language features for 3D surgical scene reconstruction, enabling a more in-depth understanding of the complex surgical environment. We also propose semantic-aware deformation tracking to capture the seamless deformation of semantic features, providing a more precise reconstruction for both texture and semantic features. Furthermore, we present semantic region-aware optimization, which utilizes regional-based semantic information to supervise the training, particularly promoting the reconstruction quality and semantic smoothness. We conduct comprehensive experiments on two real-world surgical datasets to demonstrate the superiority of SurgTPGS over state-of-the-art methods, highlighting its potential to revolutionize surgical practices. SurgTPGS paves the way for developing next-generation intelligent surgical systems by enhancing surgical precision and safety. Our code is available at: https://github.com/lastbasket/SurgTPGS.

</details>


### [74] [FD-DiT: Frequency Domain-Directed Diffusion Transformer for Low-Dose CT Reconstruction](https://arxiv.org/abs/2506.23466)
*Qiqing Liu,Guoquan Wei,Zekun Zhou,Yiyang Wen,Liu Shi,Qiegen Liu*

Main category: eess.IV

TL;DR: 提出了一种基于频率域导向扩散变换器（FD-DiT）的低剂量CT重建方法，通过频率解耦和混合去噪网络优化图像细节。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT（LDCT）虽减少辐射，但图像噪声和伪影影响诊断准确性，现有方法在保留细节上存在不足。

Method: FD-DiT采用扩散策略逐步引入噪声，结合频率解耦技术和高频噪声识别方法，使用混合去噪网络和动态融合策略优化重建。

Result: 实验表明，FD-DiT在相同剂量下比现有方法更好地抑制噪声和伪影。

Conclusion: FD-DiT通过频率域和动态融合策略显著提升了LDCT图像重建质量。

Abstract: Low-dose computed tomography (LDCT) reduces radiation exposure but suffers from image artifacts and loss of detail due to quantum and electronic noise, potentially impacting diagnostic accuracy. Transformer combined with diffusion models has been a promising approach for image generation. Nevertheless, existing methods exhibit limitations in preserving finegrained image details. To address this issue, frequency domain-directed diffusion transformer (FD-DiT) is proposed for LDCT reconstruction. FD-DiT centers on a diffusion strategy that progressively introduces noise until the distribution statistically aligns with that of LDCT data, followed by denoising processing. Furthermore, we employ a frequency decoupling technique to concentrate noise primarily in high-frequency domain, thereby facilitating effective capture of essential anatomical structures and fine details. A hybrid denoising network is then utilized to optimize the overall data reconstruction process. To enhance the capability in recognizing high-frequency noise, we incorporate sliding sparse local attention to leverage the sparsity and locality of shallow-layer information, propagating them via skip connections for improving feature representation. Finally, we propose a learnable dynamic fusion strategy for optimal component integration. Experimental results demonstrate that at identical dose levels, LDCT images reconstructed by FD-DiT exhibit superior noise and artifact suppression compared to state-of-the-art methods.

</details>


### [75] [AFUNet: Cross-Iterative Alignment-Fusion Synergy for HDR Reconstruction via Deep Unfolding Paradigm](https://arxiv.org/abs/2506.23537)
*Xinyue Li,Zhangkai Ni,Wenhan Yang*

Main category: eess.IV

TL;DR: AFUNet通过交替优化的对齐与融合子任务，从MAP估计角度重构HDR图像，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法依赖经验设计，缺乏理论支持，影响可靠性。

Method: 提出AFUNet，将HDR重构分解为对齐与融合子任务，通过交替优化实现协同。基于MAP估计，结合空间对应先验，设计端到端可训练的AFUNet。

Result: AFUNet在定性和定量评估中表现优异，超越现有方法。

Conclusion: AFUNet通过理论驱动的设计，显著提升了HDR图像重构的性能和可靠性。

Abstract: Existing learning-based methods effectively reconstruct HDR images from multi-exposure LDR inputs with extended dynamic range and improved detail, but they rely more on empirical design rather than theoretical foundation, which can impact their reliability. To address these limitations, we propose the cross-iterative Alignment and Fusion deep Unfolding Network (AFUNet), where HDR reconstruction is systematically decoupled into two interleaved subtasks -- alignment and fusion -- optimized through alternating refinement, achieving synergy between the two subtasks to enhance the overall performance. Our method formulates multi-exposure HDR reconstruction from a Maximum A Posteriori (MAP) estimation perspective, explicitly incorporating spatial correspondence priors across LDR images and naturally bridging the alignment and fusion subproblems through joint constraints. Building on the mathematical foundation, we reimagine traditional iterative optimization through unfolding -- transforming the conventional solution process into an end-to-end trainable AFUNet with carefully designed modules that work progressively. Specifically, each iteration of AFUNet incorporates an Alignment-Fusion Module (AFM) that alternates between a Spatial Alignment Module (SAM) for alignment and a Channel Fusion Module (CFM) for adaptive feature fusion, progressively bridging misaligned content and exposure discrepancies. Extensive qualitative and quantitative evaluations demonstrate AFUNet's superior performance, consistently surpassing state-of-the-art methods. Our code is available at: https://github.com/eezkni/AFUNet

</details>


### [76] [Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound Segmentation](https://arxiv.org/abs/2506.23664)
*Fangyijie Wang,Kevin Whelan,Félix Balado,Guénolé Silvestre,Kathleen M. Curran*

Main category: eess.IV

TL;DR: 提出了一种基于扩散模型的掩码引导生成AI方法，用于生成合成胎儿头部超声图像及其分割掩码，以增强真实数据集，提升分割模型性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像数据因隐私和监管限制难以获取，且标注成本高。合成数据生成是解决这些挑战的有效方法。

Method: 采用扩散模型生成合成胎儿头部超声图像及其分割掩码，用于监督微调Segment Anything Model (SAM)。

Result: 合成数据能有效捕捉真实图像特征，使用少量真实图像对即可达到最先进的胎儿头部分割效果（Dice分数分别为94.66%和94.38%）。

Conclusion: 该方法为医学图像分割提供了一种高效的数据增强解决方案，尤其在真实数据有限的情况下表现优异。

Abstract: Medical image data is less accessible than in other domains due to privacy and regulatory constraints. In addition, labeling requires costly, time-intensive manual image annotation by clinical experts. To overcome these challenges, synthetic medical data generation offers a promising solution. Generative AI (GenAI), employing generative deep learning models, has proven effective at producing realistic synthetic images. This study proposes a novel mask-guided GenAI approach using diffusion models to generate synthetic fetal head ultrasound images paired with segmentation masks. These synthetic pairs augment real datasets for supervised fine-tuning of the Segment Anything Model (SAM). Our results show that the synthetic data captures real image features effectively, and this approach reaches state-of-the-art fetal head segmentation, especially when trained with a limited number of real image-mask pairs. In particular, the segmentation reaches Dice Scores of 94.66\% and 94.38\% using a handful of ultrasound images from the Spanish and African cohorts, respectively. Our code, models, and data are available on GitHub.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [77] [Single Image Inpainting and Super-Resolution with Simultaneous Uncertainty Guarantees by Universal Reproducing Kernels](https://arxiv.org/abs/2506.23221)
*Bálint Horváth,Balázs Csanád Csáji*

Main category: cs.LG

TL;DR: 论文提出了一种统计学习方法SGKI，用于图像修复和超分辨率问题中的缺失像素估计，并提供不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 解决图像修复和超分辨率中缺失像素估计问题，并量化估计值的不确定性。

Method: 基于再生核希尔伯特空间（RKHS）假设，提出SGKI方法，扩展并改进了现有核方法，利用Schur补高效计算非渐近置信带。

Result: SGKI不仅能估计缺失像素，还能为所有缺失像素构建同时保证的非渐近置信带，并在合成和基准图像数据集上验证了有效性。

Conclusion: SGKI方法在图像修复和超分辨率任务中表现出色，兼具估计和不确定性量化能力。

Abstract: The paper proposes a statistical learning approach to the problem of estimating missing pixels of images, crucial for image inpainting and super-resolution problems. One of the main novelties of the method is that it also provides uncertainty quantifications together with the estimated values. Our core assumption is that the underlying data-generating function comes from a Reproducing Kernel Hilbert Space (RKHS). A special emphasis is put on band-limited functions, central to signal processing, which form Paley-Wiener type RKHSs. The proposed method, which we call Simultaneously Guaranteed Kernel Interpolation (SGKI), is an extension and refinement of a recently developed kernel method. An advantage of SGKI is that it not only estimates the missing pixels, but also builds non-asymptotic confidence bands for the unobserved values, which are simultaneously guaranteed for all missing pixels. We also show how to compute these bands efficiently using Schur complements, we discuss a generalization to vector-valued functions, and we present a series of numerical experiments on various datasets containing synthetically generated and benchmark images, as well.

</details>


### [78] [Radioactive Watermarks in Diffusion and Autoregressive Image Generative Models](https://arxiv.org/abs/2506.23731)
*Michel Meintz,Jan Dubiński,Franziska Boenisch,Adam Dziedzic*

Main category: cs.LG

TL;DR: 论文分析了扩散模型和自回归图像模型中水印的放射性，发现现有方法在扩散模型中失效，并提出了一种针对自回归模型的新型放射性水印方法。


<details>
  <summary>Details</summary>
Motivation: 训练图像生成模型需要大量数据，成本高昂。为防止未经授权使用生成图像训练新模型，需要水印具有放射性（即水印在新模型中仍可识别）。

Method: 分析了扩散模型和自回归模型的水印放射性问题，并提出了一种针对自回归模型的新型水印方法，灵感来自大语言模型。

Result: 现有扩散模型水印方法放射性不足，而提出的自回归模型水印方法能有效保留放射性，支持溯源和防止未经授权使用。

Conclusion: 自回归模型的新型水印方法解决了放射性问题，为图像生成模型的版权保护提供了有效工具。

Abstract: Image generative models have become increasingly popular, but training them requires large datasets that are costly to collect and curate. To circumvent these costs, some parties may exploit existing models by using the generated images as training data for their own models. In general, watermarking is a valuable tool for detecting unauthorized use of generated images. However, when these images are used to train a new model, watermarking can only enable detection if the watermark persists through training and remains identifiable in the outputs of the newly trained model - a property known as radioactivity. We analyze the radioactivity of watermarks in images generated by diffusion models (DMs) and image autoregressive models (IARs). We find that existing watermarking methods for DMs fail to retain radioactivity, as watermarks are either erased during encoding into the latent space or lost in the noising-denoising process (during the training in the latent space). Meanwhile, despite IARs having recently surpassed DMs in image generation quality and efficiency, no radioactive watermarking methods have been proposed for them. To overcome this limitation, we propose the first watermarking method tailored for IARs and with radioactivity in mind - drawing inspiration from techniques in large language models (LLMs), which share IARs' autoregressive paradigm. Our extensive experimental evaluation highlights our method's effectiveness in preserving radioactivity within IARs, enabling robust provenance tracking, and preventing unauthorized use of their generated images.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [79] [Denoising Multi-Color QR Codes and Stiefel-Valued Data by Relaxed Regularizations](https://arxiv.org/abs/2506.22826)
*Robert Beinert,Jonas Bresch*

Main category: math.OC

TL;DR: 论文提出了一种扩展方法，用于处理多二进制和Stiefel值数据，通过凸化技术实现高效去噪。


<details>
  <summary>Details</summary>
Motivation: 研究动机是扩展现有的流形值数据去噪方法，以处理新型数据（如多二进制和Stiefel值数据），这些数据在QR码和图像识别中有应用。

Method: 方法包括将数据嵌入欧几里得空间，通过半正定矩阵编码非凸流形，并松弛秩约束以实现凸化。针对多二进制和Stiefel值数据，提出了TV和Tikhonov去噪模型。

Result: 通过合成实验验证了所提方法的有效性。

Conclusion: 结论表明，该方法能够高效处理新型流形值数据，为实际应用提供了可行的解决方案。

Abstract: The handling of manifold-valued data, for instance, plays a central role in color restoration tasks relying on circle- or sphere-valued color models, in the study of rotational or directional information related to the special orthogonal group, and in Gaussian image processing, where the pixel statistics are interpreted as values on the hyperbolic sheet. Especially, to denoise these kind of data, there have been proposed several generalizations of total variation (TV) and Tikhonov-type denoising models incorporating the underlying manifolds. Recently, a novel, numerically efficient denoising approach has been introduced, where the data are embedded in an Euclidean ambient space, the non-convex manifolds are encoded by a series of positive semi-definite, fixed-rank matrices, and the rank constraint is relaxed to obtain a convexification that can be solved using standard algorithms from convex analysis. The aim of the present paper is to extent this approach to new kinds of data like multi-binary and Stiefel-valued data. Multi-binary data can, for instance, be used to model multi-color QR codes whereas Stiefel-valued data occur in image and video-based recognition. For both new data types, we propose TV- and Tikhonov-based denoising modelstogether with easy-to-solve convexification. All derived methods are evaluated on proof-of-concept, synthetic experiments.

</details>
