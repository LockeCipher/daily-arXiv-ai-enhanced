{"id": "2509.18497", "pdf": "https://arxiv.org/pdf/2509.18497", "abs": "https://arxiv.org/abs/2509.18497", "authors": ["Kaiwen Jiang", "Jia-Mu Sun", "Zilu Li", "Dan Wang", "Tzu-Mao Li", "Ravi Ramamoorthi"], "title": "Differentiable Light Transport with Gaussian Surfels via Adapted Radiosity for Efficient Relighting and Geometry Reconstruction", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Radiance fields have gained tremendous success with applications ranging from novel view synthesis to geometry reconstruction, especially with the advent of Gaussian splatting. However, they sacrifice modeling of material reflective properties and lighting conditions, leading to significant geometric ambiguities and the inability to easily perform relighting. One way to address these limitations is to incorporate physically-based rendering, but it has been prohibitively expensive to include full global illumination within the inner loop of the optimization. Therefore, previous works adopt simplifications that make the whole optimization with global illumination effects efficient but less accurate. In this work, we adopt Gaussian surfels as the primitives and build an efficient framework for differentiable light transport, inspired from the classic radiosity theory. The whole framework operates in the coefficient space of spherical harmonics, enabling both diffuse and specular materials. We extend the classic radiosity into non-binary visibility and semi-opaque primitives, propose novel solvers to efficiently solve the light transport, and derive the backward pass for gradient optimizations, which is more efficient than auto-differentiation. During inference, we achieve view-independent rendering where light transport need not be recomputed under viewpoint changes, enabling hundreds of FPS for global illumination effects, including view-dependent reflections using a spherical harmonics representation. Through extensive qualitative and quantitative experiments, we demonstrate superior geometry reconstruction, view synthesis and relighting than previous inverse rendering baselines, or data-driven baselines given relatively sparse datasets with known or unknown lighting conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u9762\u5143\u7684\u9ad8\u6548\u53ef\u5fae\u5206\u5149\u4f20\u8f93\u6846\u67b6\uff0c\u7ed3\u5408\u8f90\u5c04\u5ea6\u7406\u8bba\uff0c\u5728\u7403\u8c10\u7cfb\u6570\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u5168\u5c40\u5149\u7167\uff0c\u652f\u6301\u6f2b\u53cd\u5c04\u548c\u955c\u9762\u6750\u8d28\uff0c\u5b9e\u73b0\u4e86\u89c6\u56fe\u65e0\u5173\u6e32\u67d3\u548c\u5b9e\u65f6\u5168\u5c40\u5149\u7167\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u8f90\u5c04\u573a\u65b9\u6cd5\u727a\u7272\u4e86\u6750\u8d28\u53cd\u5c04\u7279\u6027\u548c\u5149\u7167\u6761\u4ef6\u7684\u5efa\u6a21\uff0c\u5bfc\u81f4\u51e0\u4f55\u6b67\u4e49\u548c\u65e0\u6cd5\u8fdb\u884c\u91cd\u5149\u7167\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u52a0\u5165\u5168\u5c40\u5149\u7167\u6210\u672c\u8fc7\u9ad8\uff0c\u901a\u5e38\u91c7\u7528\u7b80\u5316\u65b9\u6848\u4f46\u7cbe\u5ea6\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u9ad8\u65af\u9762\u5143\u4f5c\u4e3a\u57fa\u5143\uff0c\u5728\u7403\u8c10\u7cfb\u6570\u7a7a\u95f4\u4e2d\u6784\u5efa\u53ef\u5fae\u5206\u5149\u4f20\u8f93\u6846\u67b6\uff0c\u6269\u5c55\u7ecf\u5178\u8f90\u5c04\u5ea6\u7406\u8bba\u652f\u6301\u975e\u4e8c\u5143\u53ef\u89c1\u6027\u548c\u534a\u900f\u660e\u57fa\u5143\uff0c\u63d0\u51fa\u9ad8\u6548\u6c42\u89e3\u5668\u548c\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u5b9e\u73b0\u4e86\u89c6\u56fe\u65e0\u5173\u6e32\u67d3\uff0c\u5728\u89c6\u89d2\u53d8\u5316\u65f6\u65e0\u9700\u91cd\u65b0\u8ba1\u7b97\u5149\u4f20\u8f93\uff0c\u8fbe\u5230\u6570\u767eFPS\u7684\u5168\u5c40\u5149\u7167\u6548\u679c\uff0c\u5305\u62ec\u89c6\u56fe\u76f8\u5173\u7684\u53cd\u5c04\u3002\u5728\u51e0\u4f55\u91cd\u5efa\u3001\u89c6\u56fe\u5408\u6210\u548c\u91cd\u5149\u7167\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7a00\u758f\u6570\u636e\u96c6\u548c\u5df2\u77e5/\u672a\u77e5\u5149\u7167\u6761\u4ef6\u4e0b\uff0c\u5747\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u9006\u5411\u6e32\u67d3\u57fa\u7ebf\u548c\u6570\u636e\u9a71\u52a8\u57fa\u7ebf\u7684\u6027\u80fd\uff0c\u4e3a\u9ad8\u6548\u7684\u5168\u5c40\u5149\u7167\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.18831", "pdf": "https://arxiv.org/pdf/2509.18831", "abs": "https://arxiv.org/abs/2509.18831", "authors": ["Pin-Yen Chiu", "I-Sheng Fang", "Jun-Cheng Chen"], "title": "Text Slider: Efficient and Plug-and-Play Continuous Concept Control for Image/Video Synthesis via LoRA Adapters", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Recent advances in diffusion models have significantly improved image and video synthesis. In addition, several concept control methods have been proposed to enable fine-grained, continuous, and flexible control over free-form text prompts. However, these methods not only require intensive training time and GPU memory usage to learn the sliders or embeddings but also need to be retrained for different diffusion backbones, limiting their scalability and adaptability. To address these limitations, we introduce Text Slider, a lightweight, efficient and plug-and-play framework that identifies low-rank directions within a pre-trained text encoder, enabling continuous control of visual concepts while significantly reducing training time, GPU memory consumption, and the number of trainable parameters. Furthermore, Text Slider supports multi-concept composition and continuous control, enabling fine-grained and flexible manipulation in both image and video synthesis. We show that Text Slider enables smooth and continuous modulation of specific attributes while preserving the original spatial layout and structure of the input. Text Slider achieves significantly better efficiency: 5$\\times$ faster training than Concept Slider and 47$\\times$ faster than Attribute Control, while reducing GPU memory usage by nearly 2$\\times$ and 4$\\times$, respectively.", "AI": {"tldr": "Text Slider\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6548\u7684\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u6587\u672c\u7f16\u7801\u5668\u4e2d\u8bc6\u522b\u4f4e\u79e9\u65b9\u5411\uff0c\u5b9e\u73b0\u5bf9\u89c6\u89c9\u6982\u5ff5\u7684\u8fde\u7eed\u63a7\u5236\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u3001GPU\u5185\u5b58\u6d88\u8017\u548c\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u6982\u5ff5\u63a7\u5236\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u65f6\u95f4\u548cGPU\u5185\u5b58\u6765\u5b66\u4e60\u6ed1\u5757\u6216\u5d4c\u5165\uff0c\u5e76\u4e14\u9700\u8981\u4e3a\u4e0d\u540c\u7684\u6269\u6563\u4e3b\u5e72\u7f51\u7edc\u91cd\u65b0\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u5728\u9884\u8bad\u7ec3\u6587\u672c\u7f16\u7801\u5668\u4e2d\u8bc6\u522b\u4f4e\u79e9\u65b9\u5411\uff0c\u5b9e\u73b0\u8fde\u7eed\u6982\u5ff5\u63a7\u5236\uff0c\u652f\u6301\u591a\u6982\u5ff5\u7ec4\u5408\u548c\u8fde\u7eed\u63a7\u5236\u3002", "result": "Text Slider\u5b9e\u73b0\u4e86\u5e73\u6ed1\u8fde\u7eed\u7684\u6982\u5ff5\u5c5e\u6027\u8c03\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u8f93\u5165\u7684\u7a7a\u95f4\u5e03\u5c40\u548c\u7ed3\u6784\uff0c\u8bad\u7ec3\u901f\u5ea6\u6bd4Concept Slider\u5feb5\u500d\uff0c\u6bd4Attribute Control\u5feb47\u500d\uff0cGPU\u5185\u5b58\u4f7f\u7528\u5206\u522b\u51cf\u5c11\u8fd12\u500d\u548c4\u500d\u3002", "conclusion": "Text Slider\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6982\u5ff5\u63a7\u5236\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u9650\u5236\u3002"}}
{"id": "2509.18948", "pdf": "https://arxiv.org/pdf/2509.18948", "abs": "https://arxiv.org/abs/2509.18948", "authors": ["Jun Ma", "Qian He", "Gaofeng He", "Huang Chen", "Chen Liu", "Xiaogang Jin", "Huamin Wang"], "title": "One-shot Embroidery Customization via Contrastive LoRA Modulation", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted to ACM Transactions on Graphics (TOG), SIGGRAPH Asia 2025", "summary": "Diffusion models have significantly advanced image manipulation techniques, and their ability to generate photorealistic images is beginning to transform retail workflows, particularly in presale visualization. Beyond artistic style transfer, the capability to perform fine-grained visual feature transfer is becoming increasingly important. Embroidery is a textile art form characterized by intricate interplay of diverse stitch patterns and material properties, which poses unique challenges for existing style transfer methods. To explore the customization for such fine-grained features, we propose a novel contrastive learning framework that disentangles fine-grained style and content features with a single reference image, building on the classic concept of image analogy. We first construct an image pair to define the target style, and then adopt a similarity metric based on the decoupled representations of pretrained diffusion models for style-content separation. Subsequently, we propose a two-stage contrastive LoRA modulation technique to capture fine-grained style features. In the first stage, we iteratively update the whole LoRA and the selected style blocks to initially separate style from content. In the second stage, we design a contrastive learning strategy to further decouple style and content through self-knowledge distillation. Finally, we build an inference pipeline to handle image or text inputs with only the style blocks. To evaluate our method on fine-grained style transfer, we build a benchmark for embroidery customization. Our approach surpasses prior methods on this task and further demonstrates strong generalization to three additional domains: artistic style transfer, sketch colorization, and appearance transfer.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5355\u5f20\u53c2\u8003\u56fe\u50cf\u4e2d\u89e3\u8026\u7ec6\u7c92\u5ea6\u98ce\u683c\u548c\u5185\u5bb9\u7279\u5f81\uff0c\u7279\u522b\u9488\u5bf9\u523a\u7ee3\u7b49\u590d\u6742\u7eb9\u7406\u7684\u5b9a\u5236\u5316\u9700\u6c42\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u5c55\u793a\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u5904\u7406\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u523a\u7ee3\u7b49\u5177\u6709\u590d\u6742\u9488\u6cd5\u56fe\u6848\u548c\u6750\u8d28\u7279\u6027\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7279\u5f81\u8fc1\u79fb\u9700\u6c42\u3002", "method": "\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u6784\u5efa\u56fe\u50cf\u5bf9\u5b9a\u4e49\u76ee\u6807\u98ce\u683c\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u89e3\u8026\u8868\u793a\u8fdb\u884c\u98ce\u683c-\u5185\u5bb9\u5206\u79bb\uff0c\u63d0\u51fa\u4e24\u9636\u6bb5\u5bf9\u6bd4LoRA\u8c03\u5236\u6280\u672f\uff0c\u5305\u62ec\u8fed\u4ee3\u66f4\u65b0\u6574\u4e2aLoRA\u548c\u9009\u5b9a\u98ce\u683c\u5757\uff0c\u4ee5\u53ca\u901a\u8fc7\u81ea\u77e5\u8bc6\u84b8\u998f\u8fdb\u4e00\u6b65\u89e3\u8026\u98ce\u683c\u548c\u5185\u5bb9\u3002", "result": "\u5728\u523a\u7ee3\u5b9a\u5236\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u5148\u524d\u65b9\u6cd5\uff0c\u5e76\u5728\u827a\u672f\u98ce\u683c\u8fc1\u79fb\u3001\u7d20\u63cf\u4e0a\u8272\u548c\u5916\u89c2\u8fc1\u79fb\u4e09\u4e2a\u989d\u5916\u9886\u57df\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u7ec6\u7c92\u5ea6\u98ce\u683c\u8fc1\u79fb\u4efb\u52a1\uff0c\u4e3a\u590d\u6742\u7eb9\u7406\u7279\u5f81\u7684\u5b9a\u5236\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.19296", "pdf": "https://arxiv.org/pdf/2509.19296", "abs": "https://arxiv.org/abs/2509.19296", "authors": ["Sherwin Bahmani", "Tianchang Shen", "Jiawei Ren", "Jiahui Huang", "Yifeng Jiang", "Haithem Turki", "Andrea Tagliasacchi", "David B. Lindell", "Zan Gojcic", "Sanja Fidler", "Huan Ling", "Jun Gao", "Xuanchi Ren"], "title": "Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation", "categories": ["cs.CV", "cs.GR"], "comment": "Project Page: https://research.nvidia.com/labs/toronto-ai/lyra/", "summary": "The ability to generate virtual environments is crucial for applications ranging from gaming to physical AI domains such as robotics, autonomous driving, and industrial AI. Current learning-based 3D reconstruction methods rely on the availability of captured real-world multi-view data, which is not always readily available. Recent advancements in video diffusion models have shown remarkable imagination capabilities, yet their 2D nature limits the applications to simulation where a robot needs to navigate and interact with the environment. In this paper, we propose a self-distillation framework that aims to distill the implicit 3D knowledge in the video diffusion models into an explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for multi-view training data. Specifically, we augment the typical RGB decoder with a 3DGS decoder, which is supervised by the output of the RGB decoder. In this approach, the 3DGS decoder can be purely trained with synthetic data generated by video diffusion models. At inference time, our model can synthesize 3D scenes from either a text prompt or a single image for real-time rendering. Our framework further extends to dynamic 3D scene generation from a monocular input video. Experimental results show that our framework achieves state-of-the-art performance in static and dynamic 3D scene generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u84b8\u998f\u6846\u67b6\uff0c\u5c06\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u7684\u9690\u5f0f3D\u77e5\u8bc6\u84b8\u998f\u5230\u663e\u5f0f\u76843D\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u4e2d\uff0c\u65e0\u9700\u591a\u89c6\u56fe\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u751f\u62103D\u573a\u666f", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5b66\u4e60\u76843D\u91cd\u5efa\u65b9\u6cd5\u4f9d\u8d56\u771f\u5b9e\u4e16\u754c\u591a\u89c6\u56fe\u6570\u636e\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u5e76\u4e0d\u603b\u662f\u53ef\u7528\u3002\u89c6\u9891\u6269\u6563\u6a21\u578b\u5177\u6709\u51fa\u8272\u7684\u60f3\u8c61\u529b\uff0c\u4f46\u51762D\u7279\u6027\u9650\u5236\u4e86\u5728\u9700\u8981\u4e0e\u73af\u5883\u4ea4\u4e92\u7684\u6a21\u62df\u5e94\u7528\u4e2d\u7684\u4f7f\u7528", "method": "\u5728RGB\u89e3\u7801\u5668\u57fa\u7840\u4e0a\u589e\u52a03DGS\u89e3\u7801\u5668\uff0c\u901a\u8fc7RGB\u89e3\u7801\u5668\u7684\u8f93\u51fa\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3\u30023DGS\u89e3\u7801\u5668\u53ef\u4ee5\u5b8c\u5168\u4f7f\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u8fdb\u884c\u8bad\u7ec3", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u9759\u6001\u548c\u52a8\u60013D\u573a\u666f\u751f\u6210\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u4ece\u6587\u672c\u63d0\u793a\u6216\u5355\u5f20\u56fe\u50cf\u5408\u62103D\u573a\u666f\u8fdb\u884c\u5b9e\u65f6\u6e32\u67d3\uff0c\u5e76\u6269\u5c55\u5230\u4ece\u5355\u76ee\u8f93\u5165\u89c6\u9891\u751f\u6210\u52a8\u60013D\u573a\u666f"}}
{"id": "2509.18179", "pdf": "https://arxiv.org/pdf/2509.18179", "abs": "https://arxiv.org/abs/2509.18179", "authors": ["Sai Varun Kodathala", "Rakesh Vunnam"], "title": "The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image Generation Outcomes", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 7 Figures", "summary": "With the increasing integration of multimodal AI systems in creative workflows, understanding information loss in vision-language-vision pipelines has become important for evaluating system limitations. However, the degradation that occurs when visual content passes through textual intermediation remains poorly quantified. In this work, we provide empirical analysis of the describe-then-generate bottleneck, where natural language serves as an intermediate representation for visual information. We generated 150 image pairs through the describe-then-generate pipeline and applied existing metrics (LPIPS, SSIM, and color distance) to measure information preservation across perceptual, structural, and chromatic dimensions. Our evaluation reveals that 99.3% of samples exhibit substantial perceptual degradation and 91.5% demonstrate significant structural information loss, providing empirical evidence that the describe-then-generate bottleneck represents a measurable and consistent limitation in contemporary multimodal systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u91cf\u5316\u4e86\u89c6\u89c9-\u8bed\u8a00-\u89c6\u89c9\u7ba1\u9053\u4e2d\u63cf\u8ff0-\u751f\u6210\u74f6\u9888\u9020\u6210\u7684\u4fe1\u606f\u635f\u5931\uff0c\u53d1\u73b099.3%\u7684\u6837\u672c\u5b58\u5728\u663e\u8457\u611f\u77e5\u9000\u5316\uff0c91.5%\u5b58\u5728\u7ed3\u6784\u4fe1\u606f\u635f\u5931\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001AI\u7cfb\u7edf\u5728\u521b\u610f\u5de5\u4f5c\u6d41\u4e2d\u7684\u96c6\u6210\u589e\u52a0\uff0c\u7406\u89e3\u89c6\u89c9-\u8bed\u8a00-\u89c6\u89c9\u7ba1\u9053\u4e2d\u7684\u4fe1\u606f\u635f\u5931\u5bf9\u4e8e\u8bc4\u4f30\u7cfb\u7edf\u5c40\u9650\u6027\u53d8\u5f97\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u901a\u8fc7\u6587\u672c\u4e2d\u4ecb\u4f20\u9012\u89c6\u89c9\u5185\u5bb9\u65f6\u7684\u9000\u5316\u7a0b\u5ea6\u5c1a\u672a\u5f97\u5230\u5145\u5206\u91cf\u5316\u3002", "method": "\u751f\u6210\u4e86150\u5bf9\u56fe\u50cf\u901a\u8fc7\u63cf\u8ff0-\u751f\u6210\u7ba1\u9053\uff0c\u5e76\u5e94\u7528\u73b0\u6709\u6307\u6807\uff08LPIPS\u3001SSIM\u548c\u989c\u8272\u8ddd\u79bb\uff09\u6765\u6d4b\u91cf\u611f\u77e5\u3001\u7ed3\u6784\u548c\u8272\u5f69\u7ef4\u5ea6\u4e0a\u7684\u4fe1\u606f\u4fdd\u5b58\u60c5\u51b5\u3002", "result": "\u8bc4\u4f30\u663e\u793a99.3%\u7684\u6837\u672c\u8868\u73b0\u51fa\u663e\u8457\u7684\u611f\u77e5\u9000\u5316\uff0c91.5%\u7684\u6837\u672c\u663e\u793a\u51fa\u663e\u8457\u7684\u7ed3\u6784\u4fe1\u606f\u635f\u5931\u3002", "conclusion": "\u63cf\u8ff0-\u751f\u6210\u74f6\u9888\u4ee3\u8868\u4e86\u5f53\u4ee3\u591a\u6a21\u6001\u7cfb\u7edf\u4e2d\u53ef\u6d4b\u91cf\u4e14\u4e00\u81f4\u7684\u9650\u5236\uff0c\u4e3a\u7cfb\u7edf\u6539\u8fdb\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002"}}
{"id": "2509.18190", "pdf": "https://arxiv.org/pdf/2509.18190", "abs": "https://arxiv.org/abs/2509.18190", "authors": ["Junseong Shin", "Seungwoo Chung", "Yunjeong Yang", "Tae Hyun Kim"], "title": "HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Dehazing involves removing haze or fog from images to restore clarity and improve visibility by estimating atmospheric scattering effects. While deep learning methods show promise, the lack of paired real-world training data and the resulting domain gap hinder generalization to real-world scenarios. In this context, physics-grounded learning becomes crucial; however, traditional methods based on the Atmospheric Scattering Model (ASM) often fall short in handling real-world complexities and diverse haze patterns. To solve this problem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM as an ordinary differential equation (ODE). Inspired by Rectified Flow (RF), HazeFlow learns an optimal ODE trajectory to map hazy images to clean ones, enhancing real-world dehazing performance with only a single inference step. Additionally, we introduce a non-homogeneous haze generation method using Markov Chain Brownian Motion (MCBM) to address the scarcity of paired real-world data. By simulating realistic haze patterns through MCBM, we enhance the adaptability of HazeFlow to diverse real-world scenarios. Through extensive experiments, we demonstrate that HazeFlow achieves state-of-the-art performance across various real-world dehazing benchmark datasets.", "AI": {"tldr": "HazeFlow\u662f\u4e00\u4e2a\u57fa\u4e8e\u5e38\u5fae\u5206\u65b9\u7a0b\u7684\u53bb\u96fe\u6846\u67b6\uff0c\u5c06\u5927\u6c14\u6563\u5c04\u6a21\u578b\u91cd\u65b0\u8868\u8ff0\u4e3aODE\uff0c\u901a\u8fc7\u5355\u6b65\u63a8\u7406\u5b9e\u73b0\u771f\u5b9e\u4e16\u754c\u7684\u53bb\u96fe\uff0c\u5e76\u5229\u7528\u9a6c\u5c14\u53ef\u592b\u94fe\u5e03\u6717\u8fd0\u52a8\u751f\u6210\u975e\u5747\u5300\u96fe\u973e\u6570\u636e\u6765\u89e3\u51b3\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u914d\u5bf9\u7684\u771f\u5b9e\u4e16\u754c\u8bad\u7ec3\u6570\u636e\u548c\u9886\u57df\u5dee\u8ddd\uff0c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u4f20\u7edf\u57fa\u4e8e\u5927\u6c14\u6563\u5c04\u6a21\u578b\u7684\u7269\u7406\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u771f\u5b9e\u4e16\u754c\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u5316\u96fe\u973e\u6a21\u5f0f\u3002", "method": "\u63d0\u51faHazeFlow\u6846\u67b6\uff0c\u5c06\u5927\u6c14\u6563\u5c04\u6a21\u578b\u91cd\u65b0\u8868\u8ff0\u4e3aODE\uff0c\u53d7Rectified Flow\u542f\u53d1\u5b66\u4e60\u4ece\u96fe\u973e\u56fe\u50cf\u5230\u6e05\u6670\u56fe\u50cf\u7684\u6700\u4f18ODE\u8f68\u8ff9\u3002\u5f15\u5165\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u94fe\u5e03\u6717\u8fd0\u52a8\u7684\u975e\u5747\u5300\u96fe\u973e\u751f\u6210\u65b9\u6cd5\u6a21\u62df\u771f\u5b9e\u96fe\u973e\u6a21\u5f0f\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u53bb\u96fe\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cHazeFlow\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "HazeFlow\u901a\u8fc7ODE\u6846\u67b6\u548c\u771f\u5b9e\u96fe\u973e\u6a21\u62df\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u771f\u5b9e\u4e16\u754c\u53bb\u96fe\u7684\u6311\u6218\uff0c\u5728\u6cdb\u5316\u6027\u548c\u6027\u80fd\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.18473", "pdf": "https://arxiv.org/pdf/2509.18473", "abs": "https://arxiv.org/abs/2509.18473", "authors": ["Binhua Huang", "Wendong Yao", "Shaowu Chen", "Guoxin Wang", "Qingyuan Wang", "Soumyabrata Dev"], "title": "MoCrop: Training Free Motion Guided Cropping for Efficient Video Action Recognition", "categories": ["cs.CV"], "comment": "5 pages, 2 figures", "summary": "We introduce MoCrop, a motion-aware adaptive cropping module for efficient video action recognition in the compressed domain. MoCrop uses motion vectors that are available in H.264 video to locate motion-dense regions and produces a single clip-level crop that is applied to all I-frames at inference. The module is training free, adds no parameters, and can be plugged into diverse backbones. A lightweight pipeline that includes denoising & merge (DM), Monte Carlo sampling (MCS), and adaptive cropping (AC) via a motion-density submatrix search yields robust crops with negligible overhead. On UCF101, MoCrop improves accuracy or reduces compute. With ResNet-50, it delivers +3.5% Top-1 accuracy at equal FLOPs (attention setting), or +2.4% Top-1 accuracy with 26.5% fewer FLOPs (efficiency setting). Applied to CoViAR, it reaches 89.2% Top-1 accuracy at the original cost and 88.5% Top-1 accuracy while reducing compute from 11.6 to 8.5 GFLOPs. Consistent gains on MobileNet-V3, EfficientNet-B1, and Swin-B indicate strong generality and make MoCrop practical for real-time deployment in the compressed domain. Our code and models are available at https://github.com/microa/MoCrop.", "AI": {"tldr": "MoCrop\u662f\u4e00\u79cd\u57fa\u4e8e\u538b\u7f29\u57df\u7684\u8fd0\u52a8\u611f\u77e5\u81ea\u9002\u5e94\u88c1\u526a\u6a21\u5757\uff0c\u7528\u4e8e\u9ad8\u6548\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\uff0c\u5229\u7528H.264\u89c6\u9891\u4e2d\u7684\u8fd0\u52a8\u5411\u91cf\u5b9a\u4f4d\u8fd0\u52a8\u5bc6\u96c6\u533a\u57df\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u8bc6\u522b\u7cbe\u5ea6\u6216\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u7279\u522b\u662f\u5728\u538b\u7f29\u57df\u5904\u7406\u4e2d\u3002\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u53c2\u6570\u96f6\u589e\u52a0\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u538b\u7f29\u89c6\u9891\u4e2d\u5df2\u6709\u7684\u8fd0\u52a8\u5411\u91cf\u4fe1\u606f\u6765\u63d0\u9ad8\u8bc6\u522b\u6548\u7387\u3002", "method": "MoCrop\u5305\u542b\u4e09\u4e2a\u8f7b\u91cf\u7ea7\u7ec4\u4ef6\uff1a\u53bb\u566a\u4e0e\u5408\u5e76(DM)\u3001\u8499\u7279\u5361\u6d1b\u91c7\u6837(MCS)\u548c\u57fa\u4e8e\u8fd0\u52a8\u5bc6\u5ea6\u5b50\u77e9\u9635\u641c\u7d22\u7684\u81ea\u9002\u5e94\u88c1\u526a(AC)\u3002\u8be5\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u5bf9\u6240\u6709I\u5e27\u5e94\u7528\u5355\u4e00\u526a\u8f91\u7ea7\u88c1\u526a\u3002", "result": "\u5728UCF101\u6570\u636e\u96c6\u4e0a\uff0cMoCrop\u5728\u4fdd\u6301\u76f8\u540cFLOPs\u65f6\u63d0\u5347ResNet-50\u51c6\u786e\u73873.5%\uff0c\u6216\u5728\u51cf\u5c1126.5%FLOPs\u65f6\u63d0\u53472.4%\u51c6\u786e\u7387\u3002\u5e94\u7528\u4e8eCoViAR\u65f6\uff0c\u5728\u539f\u59cb\u8ba1\u7b97\u6210\u672c\u4e0b\u8fbe\u523089.2%\u51c6\u786e\u7387\uff0c\u6216\u5728\u51cf\u5c11\u8ba1\u7b97\u91cf(11.6\u21928.5 GFLOPs)\u65f6\u4fdd\u630188.5%\u51c6\u786e\u7387\u3002", "conclusion": "MoCrop\u5177\u6709\u5f3a\u6cdb\u5316\u6027\uff0c\u5728\u591a\u79cd\u9aa8\u5e72\u7f51\u7edc\u4e0a\u5747\u8868\u73b0\u4e00\u81f4\uff0c\u4e3a\u538b\u7f29\u57df\u5b9e\u65f6\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18501", "pdf": "https://arxiv.org/pdf/2509.18501", "abs": "https://arxiv.org/abs/2509.18501", "authors": ["Maximilian Fehrentz", "Alexander Winkler", "Thomas Heiliger", "Nazim Haouchine", "Christian Heiliger", "Nassir Navab"], "title": "BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting for Deformable Intraoperative Surgical Navigation", "categories": ["cs.CV"], "comment": "Accepted at MICCAI 2025", "summary": "We introduce BridgeSplat, a novel approach for deformable surgical navigation that couples intraoperative 3D reconstruction with preoperative CT data to bridge the gap between surgical video and volumetric patient data. Our method rigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian parameters and mesh deformation through photometric supervision. By parametrizing each Gaussian relative to its parent mesh triangle, we enforce alignment between Gaussians and mesh and obtain deformations that can be propagated back to update the CT. We demonstrate BridgeSplat's effectiveness on visceral pig surgeries and synthetic data of a human liver under simulation, showing sensible deformations of the preoperative CT on monocular RGB data. Code, data, and additional resources can be found at https://maxfehrentz.github.io/ct-informed-splatting/ .", "AI": {"tldr": "BridgeSplat\u662f\u4e00\u79cd\u65b0\u9896\u7684\u53ef\u53d8\u5f62\u624b\u672f\u5bfc\u822a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u672f\u4e2d3D\u91cd\u5efa\u4e0e\u672f\u524dCT\u6570\u636e\u8026\u5408\uff0c\u5728\u624b\u672f\u89c6\u9891\u548c\u4f53\u79ef\u60a3\u8005\u6570\u636e\u4e4b\u95f4\u5efa\u7acb\u6865\u6881\u3002\u8be5\u65b9\u6cd5\u5c063D\u9ad8\u65af\u51fd\u6570\u7ed1\u5b9a\u5230CT\u7f51\u683c\u4e0a\uff0c\u901a\u8fc7\u5149\u5ea6\u76d1\u7763\u8054\u5408\u4f18\u5316\u9ad8\u65af\u53c2\u6570\u548c\u7f51\u683c\u53d8\u5f62\u3002", "motivation": "\u89e3\u51b3\u624b\u672f\u5bfc\u822a\u4e2d\u624b\u672f\u89c6\u9891\u4e0e\u672f\u524dCT\u6570\u636e\u4e4b\u95f4\u7684\u5dee\u8ddd\u95ee\u9898\uff0c\u5b9e\u73b0\u672f\u4e2d\u5b9e\u65f6\u53d8\u5f62\u8ddf\u8e2a\u548cCT\u6570\u636e\u66f4\u65b0\u3002", "method": "\u5c063D\u9ad8\u65af\u51fd\u6570\u53c2\u6570\u5316\u76f8\u5bf9\u4e8e\u5176\u7236\u7f51\u683c\u4e09\u89d2\u5f62\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u9ad8\u65af\u53c2\u6570\u548c\u7f51\u683c\u53d8\u5f62\uff0c\u5f3a\u5236\u6267\u884c\u9ad8\u65af\u4e0e\u7f51\u683c\u4e4b\u95f4\u7684\u5bf9\u9f50\uff0c\u5e76\u5c06\u53d8\u5f62\u4f20\u64ad\u56deCT\u8fdb\u884c\u66f4\u65b0\u3002", "result": "\u5728\u732a\u5185\u810f\u624b\u672f\u548c\u4eba\u7c7b\u809d\u810f\u5408\u6210\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5728\u5355\u76eeRGB\u6570\u636e\u4e0a\u5bf9\u672f\u524dCT\u7684\u5408\u7406\u53d8\u5f62\u3002", "conclusion": "BridgeSplat\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u624b\u672f\u5bfc\u822a\u4e2d\u7684\u5b9e\u65f6\u53d8\u5f62\u8ddf\u8e2a\uff0c\u4e3a\u624b\u672f\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u7a7a\u95f4\u5b9a\u4f4d\u548c\u53d8\u5f62\u611f\u77e5\u3002"}}
{"id": "2509.18502", "pdf": "https://arxiv.org/pdf/2509.18502", "abs": "https://arxiv.org/abs/2509.18502", "authors": ["Wenjie Liu", "Hongmin Liu", "Lixin Zhang", "Bin Fan"], "title": "Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment", "categories": ["cs.CV"], "comment": null, "summary": "Research on unsupervised domain adaptation (UDA) for semantic segmentation of remote sensing images has been extensively conducted. However, research on how to achieve domain adaptation in practical scenarios where source domain data is inaccessible namely, source-free domain adaptation (SFDA) remains limited. Self-training has been widely used in SFDA, which requires obtaining as many high-quality pseudo-labels as possible to train models on target domain data. Most existing methods optimize the entire pseudo-label set to obtain more supervisory information. However, as pseudo-label sets often contain substantial noise, simultaneously optimizing all labels is challenging. This limitation undermines the effectiveness of optimization approaches and thus restricts the performance of self-training. To address this, we propose a novel pseudo-label optimization framework called Diffusion-Guided Label Enrichment (DGLE), which starts from a few easily obtained high-quality pseudo-labels and propagates them to a complete set of pseudo-labels while ensuring the quality of newly generated labels. Firstly, a pseudo-label fusion method based on confidence filtering and super-resolution enhancement is proposed, which utilizes cross-validation of details and contextual information to obtain a small number of high-quality pseudo-labels as initial seeds. Then, we leverage the diffusion model to propagate incomplete seed pseudo-labels with irregular distributions due to its strong denoising capability for randomly distributed noise and powerful modeling capacity for complex distributions, thereby generating complete and high-quality pseudo-labels. This method effectively avoids the difficulty of directly optimizing the complete set of pseudo-labels, significantly improves the quality of pseudo-labels, and thus enhances the model's performance in the target domain.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDGLE\u7684\u4f2a\u6807\u7b7e\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6e90\u6570\u636e\u4e0d\u53ef\u8bbf\u95ee\u7684\u8bed\u4e49\u5206\u5272\u9886\u57df\u81ea\u9002\u5e94\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u4ece\u5c11\u91cf\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u51fa\u53d1\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u4f20\u64ad\u751f\u6210\u5b8c\u6574\u7684\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u96c6\u3002", "motivation": "\u5728\u6e90\u6570\u636e\u4e0d\u53ef\u8bbf\u95ee\u7684\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u6e90\u81ea\u7531\u9886\u57df\u81ea\u9002\u5e94\uff08SFDA\uff09\u7814\u7a76\u6709\u9650\u3002\u73b0\u6709\u81ea\u8bad\u7ec3\u65b9\u6cd5\u9700\u8981\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\uff0c\u4f46\u4f2a\u6807\u7b7e\u96c6\u901a\u5e38\u5305\u542b\u5927\u91cf\u566a\u58f0\uff0c\u540c\u65f6\u4f18\u5316\u6240\u6709\u6807\u7b7e\u5177\u6709\u6311\u6218\u6027\u3002", "method": "1\uff09\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u8fc7\u6ee4\u548c\u8d85\u5206\u8fa8\u7387\u589e\u5f3a\u7684\u4f2a\u6807\u7b7e\u878d\u5408\u65b9\u6cd5\uff0c\u83b7\u5f97\u5c11\u91cf\u9ad8\u8d28\u91cf\u521d\u59cb\u79cd\u5b50\u6807\u7b7e\uff1b2\uff09\u5229\u7528\u6269\u6563\u6a21\u578b\u4f20\u64ad\u4e0d\u5b8c\u6574\u79cd\u5b50\u4f2a\u6807\u7b7e\uff0c\u751f\u6210\u5b8c\u6574\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u3002", "result": "\u8be5\u65b9\u6cd5\u6709\u6548\u907f\u514d\u4e86\u76f4\u63a5\u4f18\u5316\u5b8c\u6574\u4f2a\u6807\u7b7e\u96c6\u7684\u56f0\u96be\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4f2a\u6807\u7b7e\u8d28\u91cf\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u76ee\u6807\u57df\u7684\u6027\u80fd\u3002", "conclusion": "DGLE\u6846\u67b6\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5f15\u5bfc\u7684\u6807\u7b7e\u4e30\u5bcc\u5316\uff0c\u4e3a\u6e90\u81ea\u7531\u9886\u57df\u81ea\u9002\u5e94\u7684\u8bed\u4e49\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4f2a\u6807\u7b7e\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18566", "pdf": "https://arxiv.org/pdf/2509.18566", "abs": "https://arxiv.org/abs/2509.18566", "authors": ["Xiaoting Yin", "Hao Shi", "Kailun Yang", "Jiajun Zhai", "Shangwei Guo", "Lin Wang", "Kaiwei Wang"], "title": "Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": null, "summary": "Reconstructing dynamic humans together with static scenes from monocular videos remains difficult, especially under fast motion, where RGB frames suffer from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond temporal resolution, making them a superior sensing choice for dynamic human reconstruction. Accordingly, we present a novel event-guided human-scene reconstruction framework that jointly models human and scene from a single monocular event camera via 3D Gaussian Splatting. Specifically, a unified set of 3D Gaussians carries a learnable semantic attribute; only Gaussians classified as human undergo deformation for animation, while scene Gaussians stay static. To combat blur, we propose an event-guided loss that matches simulated brightness changes between consecutive renderings with the event stream, improving local fidelity in fast-moving regions. Our approach removes the need for external human masks and simplifies managing separate Gaussian sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers state-of-the-art human-scene reconstruction, with notable gains over strong baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u548c3D\u9ad8\u65af\u6cfc\u6e85\u7684\u5355\u76ee\u89c6\u9891\u52a8\u6001\u4eba\u4f53\u4e0e\u9759\u6001\u573a\u666f\u8054\u5408\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u4e8b\u4ef6\u5f15\u5bfc\u7684\u635f\u5931\u51fd\u6570\u89e3\u51b3\u5feb\u901f\u8fd0\u52a8\u4e0b\u7684\u8fd0\u52a8\u6a21\u7cca\u95ee\u9898", "motivation": "\u4ece\u5355\u76ee\u89c6\u9891\u91cd\u5efa\u52a8\u6001\u4eba\u4f53\u548c\u9759\u6001\u573a\u666f\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u5feb\u901f\u8fd0\u52a8\u65f6RGB\u5e27\u4f1a\u51fa\u73b0\u8fd0\u52a8\u6a21\u7cca\u3002\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u5fae\u79d2\u7ea7\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u4f18\u52bf\uff0c\u66f4\u9002\u5408\u52a8\u6001\u4eba\u4f53\u91cd\u5efa", "method": "\u4f7f\u7528\u7edf\u4e00\u76843D\u9ad8\u65af\u96c6\u5408\uff0c\u5176\u4e2d\u5305\u542b\u53ef\u5b66\u4e60\u7684\u8bed\u4e49\u5c5e\u6027\uff1b\u53ea\u6709\u88ab\u5206\u7c7b\u4e3a\u4eba\u4f53\u7684\u9ad8\u65af\u4f1a\u8fdb\u884c\u53d8\u5f62\u52a8\u753b\uff0c\u573a\u666f\u9ad8\u65af\u4fdd\u6301\u9759\u6001\u3002\u63d0\u51fa\u4e8b\u4ef6\u5f15\u5bfc\u7684\u635f\u5931\u51fd\u6570\uff0c\u5339\u914d\u8fde\u7eed\u6e32\u67d3\u4e4b\u95f4\u7684\u6a21\u62df\u4eae\u5ea6\u53d8\u5316\u4e0e\u4e8b\u4ef6\u6d41", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6ZJU-MoCap-Blur\u548cMMHPSD-Blur\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u4eba\u4f53-\u573a\u666f\u91cd\u5efa\u6548\u679c\uff0c\u5728PSNR/SSIM\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0cLPIPS\u6307\u6807\u964d\u4f4e\uff0c\u7279\u522b\u5bf9\u9ad8\u901f\u8fd0\u52a8\u4e3b\u4f53\u6548\u679c\u660e\u663e", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u5916\u90e8\u4eba\u4f53\u63a9\u7801\uff0c\u7b80\u5316\u4e86\u5355\u72ec\u9ad8\u65af\u96c6\u5408\u7684\u7ba1\u7406\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5feb\u901f\u8fd0\u52a8\u4e0b\u7684\u91cd\u5efa\u95ee\u9898"}}
{"id": "2509.18593", "pdf": "https://arxiv.org/pdf/2509.18593", "abs": "https://arxiv.org/abs/2509.18593", "authors": ["Xiaoman Wu", "Lubin Gan", "Siying Wu", "Jing Zhang", "Yunwei Ou", "Xiaoyan Sun"], "title": "SSCM: A Spatial-Semantic Consistent Model for Multi-Contrast MRI Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Multi-contrast Magnetic Resonance Imaging super-resolution (MC-MRI SR) aims to enhance low-resolution (LR) contrasts leveraging high-resolution (HR) references, shortening acquisition time and improving imaging efficiency while preserving anatomical details. The main challenge lies in maintaining spatial-semantic consistency, ensuring anatomical structures remain well-aligned and coherent despite structural discrepancies and motion between the target and reference images. Conventional methods insufficiently model spatial-semantic consistency and underuse frequency-domain information, which leads to poor fine-grained alignment and inadequate recovery of high-frequency details. In this paper, we propose the Spatial-Semantic Consistent Model (SSCM), which integrates a Dynamic Spatial Warping Module for inter-contrast spatial alignment, a Semantic-Aware Token Aggregation Block for long-range semantic consistency, and a Spatial-Frequency Fusion Block for fine structure restoration. Experiments on public and private datasets show that SSCM achieves state-of-the-art performance with fewer parameters while ensuring spatially and semantically consistent reconstructions.", "AI": {"tldr": "\u63d0\u51faSSCM\u6a21\u578b\u89e3\u51b3\u591a\u5bf9\u6bd4\u5ea6MRI\u8d85\u5206\u8fa8\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u7a7a\u95f4\u626d\u66f2\u3001\u8bed\u4e49\u611f\u77e5\u4ee4\u724c\u805a\u5408\u548c\u7a7a\u95f4\u9891\u7387\u878d\u5408\u6a21\u5757\uff0c\u5b9e\u73b0\u7a7a\u95f4\u8bed\u4e49\u4e00\u81f4\u6027\u91cd\u5efa", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u7a7a\u95f4\u8bed\u4e49\u4e00\u81f4\u6027\u5efa\u6a21\u4e0d\u8db3\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u9891\u57df\u4fe1\u606f\uff0c\u5bfc\u81f4\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u5dee\u548c\u9ad8\u9891\u7ec6\u8282\u6062\u590d\u4e0d\u8db3", "method": "SSCM\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u52a8\u6001\u7a7a\u95f4\u626d\u66f2\u6a21\u5757\u5b9e\u73b0\u8de8\u5bf9\u6bd4\u5ea6\u7a7a\u95f4\u5bf9\u9f50\uff0c\u8bed\u4e49\u611f\u77e5\u4ee4\u724c\u805a\u5408\u5757\u4fdd\u6301\u957f\u7a0b\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u7a7a\u95f4\u9891\u7387\u878d\u5408\u5757\u6062\u590d\u7cbe\u7ec6\u7ed3\u6784", "result": "\u5728\u516c\u5171\u548c\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSSCM\u4ee5\u66f4\u5c11\u53c2\u6570\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u786e\u4fdd\u7a7a\u95f4\u548c\u8bed\u4e49\u4e00\u81f4\u7684\u91cd\u5efa", "conclusion": "SSCM\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u591a\u5bf9\u6bd4\u5ea6MRI\u8d85\u5206\u8fa8\u7387\u4e2d\u7684\u7a7a\u95f4\u8bed\u4e49\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf"}}
{"id": "2509.18602", "pdf": "https://arxiv.org/pdf/2509.18602", "abs": "https://arxiv.org/abs/2509.18602", "authors": ["Xu Liu", "Yibo Lu", "Xinxian Wang", "Xinyu Wu"], "title": "Training-Free Multi-Style Fusion Through Reference-Based Adaptive Modulation", "categories": ["cs.CV"], "comment": "Accepted at ACPR 2025 (oral)", "summary": "We propose Adaptive Multi-Style Fusion (AMSF), a reference-based training-free framework that enables controllable fusion of multiple reference styles in diffusion models. Most of the existing reference-based methods are limited by (a) acceptance of only one style image, thus prohibiting hybrid aesthetics and scalability to more styles, and (b) lack of a principled mechanism to balance several stylistic influences. AMSF mitigates these challenges by encoding all style images and textual hints with a semantic token decomposition module that is adaptively injected into every cross-attention layer of an frozen diffusion model. A similarity-aware re-weighting module then recalibrates, at each denoising step, the attention allocated to every style component, yielding balanced and user-controllable blends without any fine-tuning or external adapters. Both qualitative and quantitative evaluations show that AMSF produces multi-style fusion results that consistently outperform the state-of-the-art approaches, while its fusion design scales seamlessly to two or more styles. These capabilities position AMSF as a practical step toward expressive multi-style generation in diffusion models.", "AI": {"tldr": "AMSF\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u53c2\u8003\u5f0f\u6846\u67b6\uff0c\u80fd\u591f\u5728\u6269\u6563\u6a21\u578b\u4e2d\u5b9e\u73b0\u591a\u53c2\u8003\u98ce\u683c\u7684\u53ef\u63a7\u878d\u5408\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u53ea\u80fd\u5904\u7406\u5355\u4e00\u98ce\u683c\u4e14\u7f3a\u4e4f\u5e73\u8861\u673a\u5236\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u53c2\u8003\u5f0f\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a(a) \u53ea\u80fd\u63a5\u53d7\u5355\u4e00\u98ce\u683c\u56fe\u50cf\uff0c\u65e0\u6cd5\u5b9e\u73b0\u6df7\u5408\u7f8e\u5b66\u548c\u6269\u5c55\u5230\u66f4\u591a\u98ce\u683c\uff1b(b) \u7f3a\u4e4f\u5e73\u8861\u591a\u79cd\u98ce\u683c\u5f71\u54cd\u7684\u673a\u5236\u3002", "method": "\u901a\u8fc7\u8bed\u4e49\u6807\u8bb0\u5206\u89e3\u6a21\u5757\u7f16\u7801\u6240\u6709\u98ce\u683c\u56fe\u50cf\u548c\u6587\u672c\u63d0\u793a\uff0c\u81ea\u9002\u5e94\u6ce8\u5165\u5230\u51bb\u7ed3\u6269\u6563\u6a21\u578b\u7684\u6bcf\u4e2a\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\uff0c\u7136\u540e\u901a\u8fc7\u76f8\u4f3c\u5ea6\u611f\u77e5\u91cd\u52a0\u6743\u6a21\u5757\u5728\u6bcf\u4e2a\u53bb\u566a\u6b65\u9aa4\u91cd\u65b0\u6821\u51c6\u5bf9\u6bcf\u4e2a\u98ce\u683c\u7ec4\u4ef6\u7684\u6ce8\u610f\u529b\u5206\u914d\u3002", "result": "\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u663e\u793a\uff0cAMSF\u751f\u6210\u7684\u591a\u98ce\u683c\u878d\u5408\u7ed3\u679c\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e14\u5176\u878d\u5408\u8bbe\u8ba1\u53ef\u65e0\u7f1d\u6269\u5c55\u5230\u4e24\u79cd\u6216\u66f4\u591a\u98ce\u683c\u3002", "conclusion": "AMSF\u662f\u5411\u6269\u6563\u6a21\u578b\u4e2d\u8868\u8fbe\u6027\u591a\u98ce\u683c\u751f\u6210\u8fc8\u51fa\u7684\u5b9e\u7528\u4e00\u6b65\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u5fae\u8c03\u6216\u5916\u90e8\u9002\u914d\u5668\u7684\u5e73\u8861\u4e14\u7528\u6237\u53ef\u63a7\u7684\u98ce\u683c\u6df7\u5408\u3002"}}
{"id": "2509.18619", "pdf": "https://arxiv.org/pdf/2509.18619", "abs": "https://arxiv.org/abs/2509.18619", "authors": ["Yichen Wu", "Xu Liu", "Chenxuan Zhao", "Xinyu Wu"], "title": "Prompt-Guided Dual Latent Steering for Inversion Problems", "categories": ["cs.CV"], "comment": "Accepted at DICTA 2025 (oral)", "summary": "Inverting corrupted images into the latent space of diffusion models is challenging. Current methods, which encode an image into a single latent vector, struggle to balance structural fidelity with semantic accuracy, leading to reconstructions with semantic drift, such as blurred details or incorrect attributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering (PDLS), a novel, training-free framework built upon Rectified Flow models for their stable inversion paths. PDLS decomposes the inversion process into two complementary streams: a structural path to preserve source integrity and a semantic path guided by a prompt. We formulate this dual guidance as an optimal control problem and derive a closed-form solution via a Linear Quadratic Regulator (LQR). This controller dynamically steers the generative trajectory at each step, preventing semantic drift while ensuring the preservation of fine detail without costly, per-image optimization. Extensive experiments on FFHQ-1K and ImageNet-1K under various inversion tasks, including Gaussian deblurring, motion deblurring, super-resolution and freeform inpainting, demonstrate that PDLS produces reconstructions that are both more faithful to the original image and better aligned with the semantic information than single-latent baselines.", "AI": {"tldr": "PDLS\u662f\u4e00\u79cd\u57fa\u4e8eRectified Flow\u6a21\u578b\u7684\u65e0\u9700\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u6f5c\u5728\u6d41\uff08\u7ed3\u6784\u8def\u5f84\u548c\u8bed\u4e49\u8def\u5f84\uff09\u89e3\u51b3\u6269\u6563\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u56fe\u50cf\u53cd\u6f14\u4e2d\u7684\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\uff0c\u4f7f\u7528LQR\u63a7\u5236\u5668\u52a8\u6001\u5f15\u5bfc\u751f\u6210\u8f68\u8ff9\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5c06\u56fe\u50cf\u7f16\u7801\u4e3a\u5355\u4e2a\u6f5c\u5728\u5411\u91cf\u65f6\uff0c\u96be\u4ee5\u5e73\u8861\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u8bed\u4e49\u51c6\u786e\u6027\uff0c\u5bfc\u81f4\u91cd\u5efa\u56fe\u50cf\u51fa\u73b0\u8bed\u4e49\u6f02\u79fb\uff08\u5982\u7ec6\u8282\u6a21\u7cca\u6216\u5c5e\u6027\u9519\u8bef\uff09\u3002", "method": "PDLS\u5c06\u53cd\u6f14\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e24\u4e2a\u4e92\u8865\u6d41\uff1a\u7ed3\u6784\u8def\u5f84\u4fdd\u6301\u6e90\u56fe\u50cf\u5b8c\u6574\u6027\uff0c\u8bed\u4e49\u8def\u5f84\u7531\u63d0\u793a\u8bcd\u5f15\u5bfc\u3002\u901a\u8fc7\u6700\u4f18\u63a7\u5236\u95ee\u9898\u5efa\u6a21\uff0c\u4f7f\u7528\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\u5668\uff08LQR\uff09\u83b7\u5f97\u95ed\u5f0f\u89e3\uff0c\u52a8\u6001\u5f15\u5bfc\u751f\u6210\u8f68\u8ff9\u3002", "result": "\u5728FFHQ-1K\u548cImageNet-1K\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPDLS\u5728\u5404\u79cd\u53cd\u6f14\u4efb\u52a1\uff08\u9ad8\u65af\u53bb\u6a21\u7cca\u3001\u8fd0\u52a8\u53bb\u6a21\u7cca\u3001\u8d85\u5206\u8fa8\u7387\u548c\u81ea\u7531\u5f62\u5f0f\u4fee\u590d\uff09\u4e2d\uff0c\u6bd4\u5355\u6f5c\u5728\u57fa\u7ebf\u65b9\u6cd5\u4ea7\u751f\u66f4\u5fe0\u5b9e\u4e8e\u539f\u59cb\u56fe\u50cf\u4e14\u8bed\u4e49\u4fe1\u606f\u66f4\u5bf9\u9f50\u7684\u91cd\u5efa\u7ed3\u679c\u3002", "conclusion": "PDLS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u56fe\u50cf\u53cd\u6f14\u4e2d\u7684\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\uff0c\u65e0\u9700\u6602\u8d35\u7684\u6bcf\u56fe\u50cf\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u7ec6\u8282\u7684\u540c\u65f6\u786e\u4fdd\u8bed\u4e49\u51c6\u786e\u6027\u3002"}}
{"id": "2509.18639", "pdf": "https://arxiv.org/pdf/2509.18639", "abs": "https://arxiv.org/abs/2509.18639", "authors": ["Yuanhuiyi Lyu", "Chi Kit Wong", "Chenfei Liao", "Lutao Jiang", "Xu Zheng", "Zexin Lu", "Linfeng Zhang", "Xuming Hu"], "title": "Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent works have made notable advancements in enhancing unified models for text-to-image generation through the Chain-of-Thought (CoT). However, these reasoning methods separate the processes of understanding and generation, which limits their ability to guide the reasoning of unified models in addressing the deficiencies of their generative capabilities. To this end, we propose a novel reasoning framework for unified models, Understanding-in-Generation (UiG), which harnesses the robust understanding capabilities of unified models to reinforce their performance in image generation. The core insight of our UiG is to integrate generative guidance by the strong understanding capabilities during the reasoning process, thereby mitigating the limitations of generative abilities. To achieve this, we introduce \"Image Editing\" as a bridge to infuse understanding into the generation process. Initially, we verify the generated image and incorporate the understanding of unified models into the editing instructions. Subsequently, we enhance the generated image step by step, gradually infusing the understanding into the generation process. Our UiG framework demonstrates a significant performance improvement in text-to-image generation over existing text-to-image reasoning methods, e.g., a 3.92% gain on the long prompt setting of the TIIF benchmark. The project code: https://github.com/QC-LY/UiG", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUnderstanding-in-Generation (UiG)\u7684\u65b0\u578b\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7406\u89e3\u80fd\u529b\u878d\u5165\u751f\u6210\u8fc7\u7a0b\u6765\u589e\u5f3a\u7edf\u4e00\u6a21\u578b\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u7406\u65b9\u6cd5\u5c06\u7406\u89e3\u548c\u751f\u6210\u8fc7\u7a0b\u5206\u79bb\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u6307\u5bfc\u7edf\u4e00\u6a21\u578b\u89e3\u51b3\u751f\u6210\u80fd\u529b\u4e0d\u8db3\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165\"\u56fe\u50cf\u7f16\u8f91\"\u4f5c\u4e3a\u6865\u6881\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u901a\u8fc7\u5f3a\u5927\u7684\u7406\u89e3\u80fd\u529b\u6765\u6574\u5408\u751f\u6210\u6307\u5bfc\u3002\u9996\u5148\u9a8c\u8bc1\u751f\u6210\u7684\u56fe\u50cf\u5e76\u5c06\u6a21\u578b\u7684\u7406\u89e3\u878d\u5165\u7f16\u8f91\u6307\u4ee4\u4e2d\uff0c\u7136\u540e\u9010\u6b65\u589e\u5f3a\u751f\u6210\u7684\u56fe\u50cf\u3002", "result": "\u5728TIIF\u57fa\u51c6\u6d4b\u8bd5\u7684\u957f\u63d0\u793a\u8bbe\u7f6e\u4e0a\u5b9e\u73b0\u4e863.92%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u63a8\u7406\u65b9\u6cd5\u3002", "conclusion": "UiG\u6846\u67b6\u901a\u8fc7\u5c06\u7406\u89e3\u80fd\u529b\u878d\u5165\u751f\u6210\u8fc7\u7a0b\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u751f\u6210\u80fd\u529b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u7edf\u4e00\u6a21\u578b\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18711", "pdf": "https://arxiv.org/pdf/2509.18711", "abs": "https://arxiv.org/abs/2509.18711", "authors": ["Ke Li", "Di Wang", "Ting Wang", "Fuyu Dong", "Yiming Zhang", "Luyao Zhang", "Xiangyu Wang", "Shaofeng Li", "Quan Wang"], "title": "RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Remote sensing visual grounding (RSVG) aims to localize objects in remote sensing images based on free-form natural language expressions. Existing approaches are typically constrained to closed-set vocabularies, limiting their applicability in open-world scenarios. While recent attempts to leverage generic foundation models for open-vocabulary RSVG, they overly rely on expensive high-quality datasets and time-consuming fine-tuning. To address these limitations, we propose \\textbf{RSVG-ZeroOV}, a training-free framework that aims to explore the potential of frozen generic foundation models for zero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key stages: (i) Overview: We utilize a vision-language model (VLM) to obtain cross-attention\\footnote[1]{In this paper, although decoder-only VLMs use self-attention over all tokens, we refer to the image-text interaction part as cross-attention to distinguish it from pure visual self-attention.}maps that capture semantic correlations between text queries and visual regions. (ii) Focus: By leveraging the fine-grained modeling priors of a diffusion model (DM), we fill in gaps in structural and shape information of objects, which are often overlooked by VLM. (iii) Evolve: A simple yet effective attention evolution module is introduced to suppress irrelevant activations, yielding purified segmentation masks over the referred objects. Without cumbersome task-specific training, RSVG-ZeroOV offers an efficient and scalable solution. Extensive experiments demonstrate that the proposed framework consistently outperforms existing weakly-supervised and zero-shot methods.", "AI": {"tldr": "RSVG-ZeroOV\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u96f6\u6837\u672c\u5f00\u653e\u8bcd\u6c47\u9065\u611f\u89c6\u89c9\u5b9a\u4f4d\u6846\u67b6\uff0c\u5229\u7528\u51bb\u7ed3\u7684\u901a\u7528\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e0b\u7684\u76ee\u6807\u5b9a\u4f4d", "motivation": "\u73b0\u6709\u9065\u611f\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u53d7\u9650\u4e8e\u5c01\u95ed\u8bcd\u6c47\u8868\uff0c\u800c\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u8017\u65f6\u5fae\u8c03\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u83b7\u53d6\u6587\u672c\u67e5\u8be2\u4e0e\u89c6\u89c9\u533a\u57df\u7684\u8bed\u4e49\u5173\u8054\uff1b2\uff09\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u5efa\u6a21\u80fd\u529b\u8865\u5145\u7ed3\u6784\u5f62\u72b6\u4fe1\u606f\uff1b3\uff09\u901a\u8fc7\u6ce8\u610f\u529b\u8fdb\u5316\u6a21\u5757\u6291\u5236\u65e0\u5173\u6fc0\u6d3b\uff0c\u751f\u6210\u7eaf\u51c0\u5206\u5272\u63a9\u7801", "result": "\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u6846\u67b6\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u7684\u5f31\u76d1\u7763\u548c\u96f6\u6837\u672c\u65b9\u6cd5", "conclusion": "RSVG-ZeroOV\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65e0\u9700\u7e41\u7410\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u7684\u9ad8\u6548\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u51bb\u7ed3\u901a\u7528\u57fa\u7840\u6a21\u578b\u5728\u96f6\u6837\u672c\u5f00\u653e\u8bcd\u6c47\u9065\u611f\u89c6\u89c9\u5b9a\u4f4d\u4e2d\u7684\u6f5c\u529b"}}
{"id": "2509.18738", "pdf": "https://arxiv.org/pdf/2509.18738", "abs": "https://arxiv.org/abs/2509.18738", "authors": ["Ruichao Hou", "Xingyuan Li", "Tongwei Ren", "Dongming Zhou", "Gangshan Wu", "Jinde Cao"], "title": "HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal Salient Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "RGB-thermal salient object detection (RGB-T SOD) aims to identify prominent objects by integrating complementary information from RGB and thermal modalities. However, learning the precise boundaries and complete objects remains challenging due to the intrinsic insufficient feature fusion and the extrinsic limitations of data scarcity. In this paper, we propose a novel hybrid prompt-driven segment anything model (HyPSAM), which leverages the zero-shot generalization capabilities of the segment anything model (SAM) for RGB-T SOD. Specifically, we first propose a dynamic fusion network (DFNet) that generates high-quality initial saliency maps as visual prompts. DFNet employs dynamic convolution and multi-branch decoding to facilitate adaptive cross-modality interaction, overcoming the limitations of fixed-parameter kernels and enhancing multi-modal feature representation. Moreover, we propose a plug-and-play refinement network (P2RNet), which serves as a general optimization strategy to guide SAM in refining saliency maps by using hybrid prompts. The text prompt ensures reliable modality input, while the mask and box prompts enable precise salient object localization. Extensive experiments on three public datasets demonstrate that our method achieves state-of-the-art performance. Notably, HyPSAM has remarkable versatility, seamlessly integrating with different RGB-T SOD methods to achieve significant performance gains, thereby highlighting the potential of prompt engineering in this field. The code and results of our method are available at: https://github.com/milotic233/HyPSAM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u63d0\u793a\u9a71\u52a8\u7684Segment Anything\u6a21\u578b\uff08HyPSAM\uff09\uff0c\u7528\u4e8eRGB-\u70ed\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u3002\u901a\u8fc7\u52a8\u6001\u878d\u5408\u7f51\u7edc\u751f\u6210\u9ad8\u8d28\u91cf\u521d\u59cb\u663e\u8457\u56fe\u4f5c\u4e3a\u89c6\u89c9\u63d0\u793a\uff0c\u5e76\u7ed3\u5408\u6587\u672c\u3001\u63a9\u7801\u548c\u6846\u63d0\u793a\u6765\u6307\u5bfcSAM\u6a21\u578b\u4f18\u5316\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u7ed3\u679c\u3002", "motivation": "RGB-\u70ed\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u9762\u4e34\u7279\u5f81\u878d\u5408\u4e0d\u8db3\u548c\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5b66\u4e60\u7cbe\u786e\u8fb9\u754c\u548c\u5b8c\u6574\u76ee\u6807\u3002", "method": "1. \u63d0\u51fa\u52a8\u6001\u878d\u5408\u7f51\u7edc\uff08DFNet\uff09\u4f7f\u7528\u52a8\u6001\u5377\u79ef\u548c\u591a\u5206\u652f\u89e3\u7801\u5b9e\u73b0\u81ea\u9002\u5e94\u8de8\u6a21\u6001\u4ea4\u4e92\uff1b2. \u8bbe\u8ba1\u5373\u63d2\u5373\u7528\u4f18\u5316\u7f51\u7edc\uff08P2RNet\uff09\u4f5c\u4e3a\u901a\u7528\u4f18\u5316\u7b56\u7565\uff0c\u5229\u7528\u6df7\u5408\u63d0\u793a\uff08\u6587\u672c\u3001\u63a9\u7801\u3001\u6846\uff09\u6307\u5bfcSAM\u6a21\u578b\u4f18\u5316\u663e\u8457\u56fe\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5177\u6709\u663e\u8457\u7684\u591a\u529f\u80fd\u6027\uff0c\u53ef\u4ee5\u4e0e\u4e0d\u540c\u7684RGB-T SOD\u65b9\u6cd5\u65e0\u7f1d\u96c6\u6210\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "HyPSAM\u5c55\u793a\u4e86\u63d0\u793a\u5de5\u7a0b\u5728RGB-T SOD\u9886\u57df\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u5229\u7528SAM\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u548c\u8fb9\u754c\u7cbe\u786e\u5b9a\u4f4d\u7684\u95ee\u9898\u3002"}}
{"id": "2509.18743", "pdf": "https://arxiv.org/pdf/2509.18743", "abs": "https://arxiv.org/abs/2509.18743", "authors": ["Susmit Neogi"], "title": "TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing", "categories": ["cs.CV"], "comment": "39th Conference on Neural Information Processing Systems (NeurIPS   2025) Workshop", "summary": "LiDAR-based perception is central to autonomous driving and robotics, yet raw point clouds remain highly vulnerable to noise, occlusion, and adversarial corruptions. Autoencoders offer a natural framework for denoising and reconstruction, but their performance degrades under challenging real-world conditions. In this work, we propose TriFusion-AE, a multimodal cross-attention autoencoder that integrates textual priors, monocular depth maps from multi-view images, and LiDAR point clouds to improve robustness. By aligning semantic cues from text, geometric (depth) features from images, and spatial structure from LiDAR, TriFusion-AE learns representations that are resilient to stochastic noise and adversarial perturbations. Interestingly, while showing limited gains under mild perturbations, our model achieves significantly more robust reconstruction under strong adversarial attacks and heavy noise, where CNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset to reflect realistic low-data deployment scenarios. Our multimodal fusion framework is designed to be model-agnostic, enabling seamless integration with any CNN-based point cloud autoencoder for joint representation learning.", "AI": {"tldr": "TriFusion-AE\u662f\u4e00\u79cd\u591a\u6a21\u6001\u4ea4\u53c9\u6ce8\u610f\u529b\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u6574\u5408\u6587\u672c\u5148\u9a8c\u3001\u591a\u89c6\u56fe\u56fe\u50cf\u7684\u5355\u76ee\u6df1\u5ea6\u56fe\u548cLiDAR\u70b9\u4e91\uff0c\u63d0\u9ad8LiDAR\u611f\u77e5\u5728\u566a\u58f0\u548c\u5bf9\u6297\u6027\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "LiDAR\u70b9\u4e91\u5bf9\u566a\u58f0\u3001\u906e\u6321\u548c\u5bf9\u6297\u6027\u653b\u51fb\u9ad8\u5ea6\u654f\u611f\uff0c\u73b0\u6709\u81ea\u7f16\u7801\u5668\u5728\u771f\u5b9e\u4e16\u754c\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u611f\u77e5\u65b9\u6cd5\u3002", "method": "\u63d0\u51faTriFusion-AE\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u9f50\u6587\u672c\u8bed\u4e49\u7ebf\u7d22\u3001\u56fe\u50cf\u51e0\u4f55\u7279\u5f81\u548cLiDAR\u7a7a\u95f4\u7ed3\u6784\uff0c\u5b66\u4e60\u5bf9\u566a\u58f0\u548c\u5bf9\u6297\u6027\u653b\u51fb\u5177\u6709\u5f39\u6027\u7684\u8868\u793a\u3002", "result": "\u5728nuScenes-mini\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u6a21\u578b\u5728\u5f3a\u5bf9\u6297\u6027\u653b\u51fb\u548c\u91cd\u5ea6\u566a\u58f0\u4e0b\u8868\u73b0\u51fa\u663e\u8457\u9c81\u68d2\u6027\uff0c\u800c\u4f20\u7edfCNN\u81ea\u7f16\u7801\u5668\u5728\u8fd9\u4e9b\u6761\u4ef6\u4e0b\u5d29\u6e83\u3002", "conclusion": "\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\uff0c\u53ef\u4e0e\u4efb\u4f55\u57fa\u4e8eCNN\u7684\u70b9\u4e91\u81ea\u7f16\u7801\u5668\u65e0\u7f1d\u96c6\u6210\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u6280\u672f\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u611f\u77e5\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18759", "pdf": "https://arxiv.org/pdf/2509.18759", "abs": "https://arxiv.org/abs/2509.18759", "authors": ["Zhaorui Wang", "Yi Gu", "Deming Zhou", "Renjing Xu"], "title": "FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in 3D reconstruction and novel view synthesis. However, reconstructing 3D scenes from sparse viewpoints remains highly challenging due to insufficient visual information, which results in noticeable artifacts persisting across the 3D representation. To address this limitation, recent methods have resorted to generative priors to remove artifacts and complete missing content in under-constrained areas. Despite their effectiveness, these approaches struggle to ensure multi-view consistency, resulting in blurred structures and implausible details. In this work, we propose FixingGS, a training-free method that fully exploits the capabilities of the existing diffusion model for sparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our distillation approach, which delivers more accurate and cross-view coherent diffusion priors, thereby enabling effective artifact removal and inpainting. In addition, we propose an adaptive progressive enhancement scheme that further refines reconstructions in under-constrained regions. Extensive experiments demonstrate that FixingGS surpasses existing state-of-the-art methods with superior visual quality and reconstruction performance. Our code will be released publicly.", "AI": {"tldr": "FixingGS\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u73b0\u6709\u6269\u6563\u6a21\u578b\u589e\u5f3a\u7a00\u758f\u89c6\u89d23D\u9ad8\u65af\u6e85\u5c04\u91cd\u5efa\uff0c\u901a\u8fc7\u66f4\u51c6\u786e\u7684\u8de8\u89c6\u56fe\u4e00\u81f4\u6027\u6269\u6563\u5148\u9a8c\u548c\u81ea\u9002\u5e94\u6e10\u8fdb\u589e\u5f3a\u65b9\u6848\uff0c\u6709\u6548\u53bb\u9664\u4f2a\u5f71\u5e76\u8865\u5168\u7f3a\u5931\u5185\u5bb9\u3002", "motivation": "\u7a00\u758f\u89c6\u89d2\u4e0b\u76843D\u573a\u666f\u91cd\u5efa\u7531\u4e8e\u89c6\u89c9\u4fe1\u606f\u4e0d\u8db3\u4f1a\u4ea7\u751f\u660e\u663e\u4f2a\u5f71\uff0c\u73b0\u6709\u57fa\u4e8e\u751f\u6210\u5148\u9a8c\u7684\u65b9\u6cd5\u96be\u4ee5\u4fdd\u8bc1\u591a\u89c6\u56fe\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u7ed3\u6784\u6a21\u7cca\u548c\u7ec6\u8282\u4e0d\u771f\u5b9e\u3002", "method": "\u63d0\u51fa\u84b8\u998f\u65b9\u6cd5\u63d0\u4f9b\u66f4\u51c6\u786e\u548c\u8de8\u89c6\u56fe\u4e00\u81f4\u7684\u6269\u6563\u5148\u9a8c\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u6e10\u8fdb\u589e\u5f3a\u65b9\u6848\uff0c\u5728\u8bad\u7ec3\u7ea6\u675f\u4e0d\u8db3\u533a\u57df\u8fdb\u4e00\u6b65\u4f18\u5316\u91cd\u5efa\u6548\u679c\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eFixingGS\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u91cd\u5efa\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "FixingGS\u901a\u8fc7\u5145\u5206\u5229\u7528\u6269\u6563\u6a21\u578b\u80fd\u529b\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7a00\u758f\u89c6\u89d23D\u9ad8\u65af\u6e85\u5c04\u91cd\u5efa\u4e2d\u7684\u4f2a\u5f71\u548c\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u95ee\u9898\u3002"}}
{"id": "2509.18796", "pdf": "https://arxiv.org/pdf/2509.18796", "abs": "https://arxiv.org/abs/2509.18796", "authors": ["Danush Kumar Venkatesh", "Stefanie Speidel"], "title": "Towards Application Aligned Synthetic Surgical Image Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "The scarcity of annotated surgical data poses a significant challenge for developing deep learning systems in computer-assisted interventions. While diffusion models can synthesize realistic images, they often suffer from data memorization, resulting in inconsistent or non-diverse samples that may fail to improve, or even harm, downstream performance. We introduce \\emph{Surgical Application-Aligned Diffusion} (SAADi), a new framework that aligns diffusion models with samples preferred by downstream models. Our method constructs pairs of \\emph{preferred} and \\emph{non-preferred} synthetic images and employs lightweight fine-tuning of diffusion models to align the image generation process with downstream objectives explicitly. Experiments on three surgical datasets demonstrate consistent gains of $7$--$9\\%$ in classification and $2$--$10\\%$ in segmentation tasks, with the considerable improvements observed for underrepresented classes. Iterative refinement of synthetic samples further boosts performance by $4$--$10\\%$. Unlike baseline approaches, our method overcomes sample degradation and establishes task-aware alignment as a key principle for mitigating data scarcity and advancing surgical vision applications.", "AI": {"tldr": "SAADi\u6846\u67b6\u901a\u8fc7\u5c06\u6269\u6563\u6a21\u578b\u4e0e\u4e0b\u6e38\u6a21\u578b\u504f\u597d\u7684\u6837\u672c\u5bf9\u9f50\uff0c\u89e3\u51b3\u624b\u672f\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u907f\u514d\u6570\u636e\u8bb0\u5fc6\u5316\u5bfc\u81f4\u7684\u6837\u672c\u4e0d\u4e00\u81f4\u6216\u591a\u6837\u6027\u4e0d\u8db3\u3002", "motivation": "\u624b\u672f\u6570\u636e\u6807\u6ce8\u7a00\u7f3a\u9650\u5236\u4e86\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u5728\u8ba1\u7b97\u673a\u8f85\u52a9\u5e72\u9884\u4e2d\u7684\u53d1\u5c55\uff0c\u73b0\u6709\u6269\u6563\u6a21\u578b\u5b58\u5728\u6570\u636e\u8bb0\u5fc6\u5316\u95ee\u9898\uff0c\u751f\u6210\u7684\u6837\u672c\u53ef\u80fd\u4e0d\u4e00\u81f4\u6216\u7f3a\u4e4f\u591a\u6837\u6027\uff0c\u751a\u81f3\u635f\u5bb3\u4e0b\u6e38\u6027\u80fd\u3002", "method": "\u6784\u5efa\u504f\u597d\u548c\u975e\u504f\u597d\u5408\u6210\u56fe\u50cf\u5bf9\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5fae\u8c03\u6269\u6563\u6a21\u578b\uff0c\u4f7f\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u4e0e\u4e0b\u6e38\u76ee\u6807\u660e\u786e\u5bf9\u9f50\u3002", "result": "\u5728\u4e09\u4e2a\u624b\u672f\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5206\u7c7b\u4efb\u52a1\u63d0\u53477-9%\uff0c\u5206\u5272\u4efb\u52a1\u63d0\u53472-10%\uff0c\u5bf9\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u7c7b\u522b\u6539\u5584\u663e\u8457\u3002\u8fed\u4ee3\u7ec6\u5316\u5408\u6210\u6837\u672c\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd4-10%\u3002", "conclusion": "SAADi\u65b9\u6cd5\u514b\u670d\u4e86\u6837\u672c\u9000\u5316\u95ee\u9898\uff0c\u786e\u7acb\u4e86\u4efb\u52a1\u611f\u77e5\u5bf9\u9f50\u4f5c\u4e3a\u7f13\u89e3\u6570\u636e\u7a00\u7f3a\u548c\u63a8\u8fdb\u624b\u672f\u89c6\u89c9\u5e94\u7528\u7684\u5173\u952e\u539f\u5219\u3002"}}
{"id": "2509.18801", "pdf": "https://arxiv.org/pdf/2509.18801", "abs": "https://arxiv.org/abs/2509.18801", "authors": ["Kuang Xiaodong", "Li Bingxuan", "Li Yuan", "Rao Fan", "Ma Gege", "Xie Qingguo", "Mok Greta S P", "Liu Huafeng", "Zhu Wentao"], "title": "A Kernel Space-based Multidimensional Sparse Model for Dynamic PET Image Denoising", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Achieving high image quality for temporal frames in dynamic positron emission tomography (PET) is challenging due to the limited statistic especially for the short frames. Recent studies have shown that deep learning (DL) is useful in a wide range of medical image denoising tasks. In this paper, we propose a model-based neural network for dynamic PET image denoising. The inter-frame spatial correlation and intra-frame structural consistency in dynamic PET are used to establish the kernel space-based multidimensional sparse (KMDS) model. We then substitute the inherent forms of the parameter estimation with neural networks to enable adaptive parameters optimization, forming the end-to-end neural KMDS-Net. Extensive experimental results from simulated and real data demonstrate that the neural KMDS-Net exhibits strong denoising performance for dynamic PET, outperforming previous baseline methods. The proposed method may be used to effectively achieve high temporal and spatial resolution for dynamic PET. Our source code is available at https://github.com/Kuangxd/Neural-KMDS-Net/tree/main.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u795e\u7ecf\u7f51\u7edc\u7684\u52a8\u6001PET\u56fe\u50cf\u53bb\u566a\u65b9\u6cd5KMDS-Net\uff0c\u5229\u7528\u5e27\u95f4\u7a7a\u95f4\u76f8\u5173\u6027\u548c\u5e27\u5185\u7ed3\u6784\u4e00\u81f4\u6027\u5efa\u7acb\u591a\u7ef4\u7a00\u758f\u6a21\u578b\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u81ea\u9002\u5e94\u4f18\u5316\u53c2\u6570\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u53bb\u566a\u6027\u80fd\u3002", "motivation": "\u52a8\u6001PET\u4e2d\u77ed\u65f6\u95f4\u5e27\u7684\u56fe\u50cf\u8d28\u91cf\u56e0\u7edf\u8ba1\u91cf\u6709\u9650\u800c\u96be\u4ee5\u4fdd\u8bc1\uff0c\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u5b66\u56fe\u50cf\u53bb\u566a\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u52a8\u6001PET\u7279\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u5efa\u7acb\u57fa\u4e8e\u6838\u7a7a\u95f4\u7684\u591a\u7ef4\u7a00\u758f(KMDS)\u6a21\u578b\uff0c\u5229\u7528\u52a8\u6001PET\u7684\u5e27\u95f4\u7a7a\u95f4\u76f8\u5173\u6027\u548c\u5e27\u5185\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u7136\u540e\u7528\u795e\u7ecf\u7f51\u7edc\u66ff\u4ee3\u53c2\u6570\u4f30\u8ba1\u8fc7\u7a0b\uff0c\u5f62\u6210\u7aef\u5230\u7aef\u7684KMDS-Net\u7f51\u7edc\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cKMDS-Net\u5728\u52a8\u6001PET\u53bb\u566a\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u4e4b\u524d\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u6709\u6548\u5b9e\u73b0\u52a8\u6001PET\u7684\u9ad8\u65f6\u95f4\u548c\u7a7a\u95f4\u5206\u8fa8\u7387\uff0c\u4e3a\u52a8\u6001PET\u56fe\u50cf\u8d28\u91cf\u63d0\u5347\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18824", "pdf": "https://arxiv.org/pdf/2509.18824", "abs": "https://arxiv.org/abs/2509.18824", "authors": ["Yanzuo Lu", "Xin Xia", "Manlin Zhang", "Huafeng Kuang", "Jianbin Zheng", "Yuxi Ren", "Xuefeng Xiao"], "title": "Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation", "categories": ["cs.CV"], "comment": "Technical Report", "summary": "Unified multimodal models have recently attracted considerable attention for their remarkable abilities in jointly understanding and generating diverse content. However, as contexts integrate increasingly numerous interleaved multimodal tokens, the iterative processes of diffusion denoising and autoregressive decoding impose significant computational overhead. To address this, we propose Hyper-Bagel, a unified acceleration framework designed to simultaneously speed up both multimodal understanding and generation tasks. Our approach uses a divide-and-conquer strategy, employing speculative decoding for next-token prediction and a multi-stage distillation process for diffusion denoising. The framework delivers substantial performance gains, achieving over a 2x speedup in multimodal understanding. For generative tasks, our resulting lossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a 22x speedup in image editing, all while preserving the high-quality output of the original model. We further develop a highly efficient 1-NFE model that enables near real-time interactive editing and generation. By combining advanced adversarial distillation with human feedback learning, this model achieves ultimate cost-effectiveness and responsiveness, making complex multimodal interactions seamless and instantaneous.", "AI": {"tldr": "Hyper-Bagel\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u6d4b\u89e3\u7801\u548c\u591a\u9636\u6bb5\u84b8\u998f\u6280\u672f\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u8f93\u51fa\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5305\u542b\u8d8a\u6765\u8d8a\u591a\u7684\u4ea4\u7ec7\u6807\u8bb0\uff0c\u6269\u6563\u53bb\u566a\u548c\u81ea\u56de\u5f52\u89e3\u7801\u7684\u8fed\u4ee3\u8fc7\u7a0b\u5e26\u6765\u4e86\u5de8\u5927\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u9700\u8981\u9ad8\u6548\u7684\u52a0\u901f\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5206\u6cbb\u7b56\u7565\uff0c\u4f7f\u7528\u63a8\u6d4b\u89e3\u7801\u8fdb\u884c\u4e0b\u4e00\u6807\u8bb0\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u591a\u9636\u6bb5\u84b8\u998f\u8fc7\u7a0b\u52a0\u901f\u6269\u6563\u53bb\u566a\u3002\u7ed3\u5408\u5bf9\u6297\u84b8\u998f\u548c\u4eba\u7c7b\u53cd\u9988\u5b66\u4e60\u5f00\u53d1\u9ad8\u6548\u6a21\u578b\u3002", "result": "\u5728\u591a\u6a21\u6001\u7406\u89e3\u4efb\u52a1\u4e0a\u5b9e\u73b0\u8d85\u8fc72\u500d\u52a0\u901f\uff1b\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u5b9e\u73b016.67\u500d\u52a0\u901f\uff1b\u56fe\u50cf\u7f16\u8f91\u5b9e\u73b022\u500d\u52a0\u901f\uff1b\u5f00\u53d1\u51fa\u8fd1\u4e4e\u5b9e\u65f6\u76841-NFE\u6a21\u578b\u3002", "conclusion": "Hyper-Bagel\u6846\u67b6\u901a\u8fc7\u5148\u8fdb\u7684\u52a0\u901f\u6280\u672f\u5b9e\u73b0\u4e86\u6210\u672c\u6548\u76ca\u548c\u54cd\u5e94\u6027\u7684\u663e\u8457\u63d0\u5347\uff0c\u4f7f\u590d\u6742\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u53d8\u5f97\u65e0\u7f1d\u4e14\u5373\u65f6\u3002"}}
{"id": "2509.18897", "pdf": "https://arxiv.org/pdf/2509.18897", "abs": "https://arxiv.org/abs/2509.18897", "authors": ["Jiayu Wang", "Ruizhi Wang", "Jie Song", "Haofei Zhang", "Mingli Song", "Zunlei Feng", "Li Sun"], "title": "RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing", "categories": ["cs.CV"], "comment": "26 pages, 4 figures", "summary": "In this paper, we introduce a novel benchmark designed to propel the advancement of general-purpose, large-scale 3D vision models for remote sensing imagery. While several datasets have been proposed within the realm of remote sensing, many existing collections either lack comprehensive depth information or fail to establish precise alignment between depth data and remote sensing images. To address this deficiency, we present a visual Benchmark for 3D understanding of Remotely Sensed images, dubbed RS3DBench. This dataset encompasses 54,951 pairs of remote sensing images and pixel-level aligned depth maps, accompanied by corresponding textual descriptions, spanning a broad array of geographical contexts. It serves as a tool for training and assessing 3D visual perception models within remote sensing image spatial understanding tasks. Furthermore, we introduce a remotely sensed depth estimation model derived from stable diffusion, harnessing its multimodal fusion capabilities, thereby delivering state-of-the-art performance on our dataset. Our endeavor seeks to make a profound contribution to the evolution of 3D visual perception models and the advancement of geographic artificial intelligence within the remote sensing domain. The dataset, models and code will be accessed on the https://rs3dbench.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RS3DBench\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b54,951\u5bf9\u9065\u611f\u56fe\u50cf\u4e0e\u50cf\u7d20\u7ea7\u5bf9\u9f50\u7684\u6df1\u5ea6\u56fe\uff0c\u65e8\u5728\u63a8\u52a8\u9065\u611f\u9886\u57df\u901a\u7528\u5927\u89c4\u6a213D\u89c6\u89c9\u6a21\u578b\u7684\u53d1\u5c55\u3002", "motivation": "\u73b0\u6709\u9065\u611f\u6570\u636e\u96c6\u7f3a\u4e4f\u5168\u9762\u7684\u6df1\u5ea6\u4fe1\u606f\u6216\u6df1\u5ea6\u6570\u636e\u4e0e\u9065\u611f\u56fe\u50cf\u4e4b\u95f4\u7684\u7cbe\u786e\u5bf9\u9f50\uff0c\u9650\u5236\u4e863D\u89c6\u89c9\u611f\u77e5\u6a21\u578b\u5728\u9065\u611f\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u5305\u542b54,951\u5bf9\u9065\u611f\u56fe\u50cf\u548c\u6df1\u5ea6\u56fe\u7684\u6570\u636e\u96c6\uff0c\u5e76\u57fa\u4e8e\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u5f00\u53d1\u9065\u611f\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\uff0c\u5229\u7528\u5176\u591a\u6a21\u6001\u878d\u5408\u80fd\u529b\u3002", "result": "\u63d0\u51fa\u7684\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u5728RS3DBench\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "RS3DBench\u57fa\u51c6\u5c06\u4e3a\u9065\u611f\u9886\u57df3D\u89c6\u89c9\u611f\u77e5\u6a21\u578b\u7684\u53d1\u5c55\u548c\u5730\u7406\u4eba\u5de5\u667a\u80fd\u7684\u8fdb\u6b65\u505a\u51fa\u91cd\u8981\u8d21\u732e\u3002"}}
{"id": "2509.18898", "pdf": "https://arxiv.org/pdf/2509.18898", "abs": "https://arxiv.org/abs/2509.18898", "authors": ["Pengteng Li", "Yunfan Lu", "Pinhao Song", "Weiyu Guo", "Huizai Yao", "F. Richard Yu", "Hui Xiong"], "title": "DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we propose the first Structure-from-Motion (SfM)-free deblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat. We address the motion-deblurring problem in two ways. First, we leverage the pretrained capability of the dense stereo module (DUSt3R) to directly obtain accurate initial point clouds from blurred images. Without calculating camera poses as an intermediate result, we avoid the cumulative errors transfer from inaccurate camera poses to the initial point clouds' positions. Second, we introduce the event stream into the deblur pipeline for its high sensitivity to dynamic change. By decoding the latent sharp images from the event stream and blurred images, we can provide a fine-grained supervision signal for scene reconstruction optimization. Extensive experiments across a range of scenes demonstrate that DeblurSplat not only excels in generating high-fidelity novel views but also achieves significant rendering efficiency compared to the SOTAs in deblur 3D-GS.", "AI": {"tldr": "DeblurSplat\u662f\u9996\u4e2a\u65e0\u9700Structure-from-Motion\u7684\u4e8b\u4ef6\u76f8\u673a\u53bb\u6a21\u7cca3D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff0c\u901a\u8fc7DUSt3R\u76f4\u63a5\u83b7\u53d6\u521d\u59cb\u70b9\u4e91\u5e76\u5229\u7528\u4e8b\u4ef6\u6d41\u8fdb\u884c\u7cbe\u7ec6\u76d1\u7763\uff0c\u5b9e\u73b0\u9ad8\u6548\u9ad8\u4fdd\u771f\u5ea6\u7684\u53bb\u6a21\u7cca\u6e32\u67d3\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8fd0\u52a8\u53bb\u6a21\u7cca\u65b9\u6cd5\u4e2d\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u7d2f\u79ef\u8bef\u5dee\u95ee\u9898\uff0c\u4ee5\u53ca\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u5bf9\u52a8\u6001\u53d8\u5316\u7684\u9ad8\u654f\u611f\u6027\u6765\u63d0\u5347\u53bb\u6a21\u7cca\u6548\u679c\u3002", "method": "1) \u4f7f\u7528DUSt3R\u5bc6\u96c6\u7acb\u4f53\u6a21\u5757\u76f4\u63a5\u4ece\u6a21\u7cca\u56fe\u50cf\u83b7\u53d6\u51c6\u786e\u521d\u59cb\u70b9\u4e91\uff0c\u907f\u514d\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u8bef\u5dee\u4f20\u9012\uff1b2) \u5f15\u5165\u4e8b\u4ef6\u6d41\u89e3\u7801\u6f5c\u5728\u6e05\u6670\u56fe\u50cf\uff0c\u4e3a\u573a\u666f\u91cd\u5efa\u4f18\u5316\u63d0\u4f9b\u7cbe\u7ec6\u76d1\u7763\u4fe1\u53f7\u3002", "result": "\u5728\u591a\u4e2a\u573a\u666f\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDeblurSplat\u4e0d\u4ec5\u80fd\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u65b0\u89c6\u89d2\u56fe\u50cf\uff0c\u800c\u4e14\u5728\u53bb\u6a21\u7cca3D-GS\u65b9\u9762\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6e32\u67d3\u6548\u7387\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u9700SfM\u7684\u9ad8\u6548\u53bb\u6a21\u7cca3D\u91cd\u5efa\uff0c\u7ed3\u5408\u4e8b\u4ef6\u76f8\u673a\u7684\u4f18\u52bf\u4e3a\u52a8\u6001\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18917", "pdf": "https://arxiv.org/pdf/2509.18917", "abs": "https://arxiv.org/abs/2509.18917", "authors": ["Amirhesam Aghanouri", "Cristina Olaverri-Monreal"], "title": "LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Autonomous vehicles (AVs) are expected to revolutionize transportation by improving efficiency and safety. Their success relies on 3D vision systems that effectively sense the environment and detect traffic agents. Among sensors AVs use to create a comprehensive view of surroundings, LiDAR provides high-resolution depth data enabling accurate object detection, safe navigation, and collision avoidance. However, collecting real-world LiDAR data is time-consuming and often affected by noise and sparsity due to adverse weather or sensor limitations. This work applies a denoising diffusion probabilistic model (DDPM), enhanced with novel noise scheduling and time-step embedding techniques to generate high-quality synthetic data for augmentation, thereby improving performance across a range of computer vision tasks, particularly in AV perception. These modifications impact the denoising process and the model's temporal awareness, allowing it to produce more realistic point clouds based on the projection. The proposed method was extensively evaluated under various configurations using the IAMCV and KITTI-360 datasets, with four performance metrics compared against state-of-the-art (SOTA) methods. The results demonstrate the model's superior performance over most existing baselines and its effectiveness in mitigating the effects of noisy and sparse LiDAR data, producing diverse point clouds with rich spatial relationships and structural detail.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DDPM\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u566a\u58f0\u8c03\u5ea6\u548c\u65f6\u95f4\u6b65\u5d4c\u5165\u6280\u672f\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210LiDAR\u6570\u636e\uff0c\u7528\u4e8e\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u76843D\u89c6\u89c9\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u771f\u5b9eLiDAR\u6570\u636e\u91c7\u96c6\u8017\u65f6\u4e14\u6613\u53d7\u566a\u58f0\u548c\u7a00\u758f\u6027\u5f71\u54cd\uff08\u5982\u6076\u52a3\u5929\u6c14\u6216\u4f20\u611f\u5668\u9650\u5236\uff09\uff0c\u9700\u8981\u5408\u6210\u6570\u636e\u6765\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684DDPM\u6a21\u578b\uff0c\u5f15\u5165\u65b0\u9896\u7684\u566a\u58f0\u8c03\u5ea6\u548c\u65f6\u95f4\u6b65\u5d4c\u5165\u6280\u672f\uff0c\u4f18\u5316\u53bb\u566a\u8fc7\u7a0b\u548c\u65f6\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u751f\u6210\u66f4\u771f\u5b9e\u7684\u70b9\u4e91\u6570\u636e\u3002", "result": "\u5728IAMCV\u548cKITTI-360\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u56db\u9879\u6027\u80fd\u6307\u6807\u4e0a\u4f18\u4e8e\u5927\u591a\u6570\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u7f13\u89e3\u566a\u58f0\u548c\u7a00\u758fLiDAR\u6570\u636e\u7684\u5f71\u54cd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u5177\u6709\u4e30\u5bcc\u7a7a\u95f4\u5173\u7cfb\u548c\u7ed3\u6784\u7ec6\u8282\u7684\u591a\u6837\u5316\u70b9\u4e91\uff0c\u663e\u8457\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2509.18956", "pdf": "https://arxiv.org/pdf/2509.18956", "abs": "https://arxiv.org/abs/2509.18956", "authors": ["Zijing Guo", "Yunyang Zhao", "Lin Wang"], "title": "Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Mirror-containing environments pose unique challenges for 3D reconstruction and novel view synthesis (NVS), as reflective surfaces introduce view-dependent distortions and inconsistencies. While cutting-edge methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical scenes, their performance deteriorates in the presence of mirrors. Existing solutions mainly focus on handling mirror surfaces through symmetry mapping but often overlook the rich information carried by mirror reflections. These reflections offer complementary perspectives that can fill in absent details and significantly enhance reconstruction quality. To advance 3D reconstruction in mirror-rich environments, we present MirrorScene3D, a comprehensive dataset featuring diverse indoor scenes, 1256 high-quality images, and annotated mirror masks, providing a benchmark for evaluating reconstruction methods in reflective settings. Building on this, we propose ReflectiveGS, an extension of 3D Gaussian Splatting that utilizes mirror reflections as complementary viewpoints rather than simple symmetry artifacts, enhancing scene geometry and recovering absent details. Experiments on MirrorScene3D show that ReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and training speed, setting a new benchmark for 3D reconstruction in mirror-rich environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MirrorScene3D\u6570\u636e\u96c6\u548cReflectiveGS\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u542b\u955c\u9762\u73af\u5883\u4e2d\u76843D\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u5408\u6210\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u955c\u9762\u53cd\u5c04\u4f5c\u4e3a\u8865\u5145\u89c6\u89d2\u6765\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u73b0\u67093D\u91cd\u5efa\u65b9\u6cd5\uff08\u5982NeRF\u548c3DGS\uff09\u5728\u542b\u955c\u9762\u73af\u5883\u4e2d\u6027\u80fd\u4e0b\u964d\uff0c\u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u955c\u9762\u8868\u9762\u7684\u5bf9\u79f0\u6620\u5c04\uff0c\u4f46\u5ffd\u7565\u4e86\u955c\u9762\u53cd\u5c04\u643a\u5e26\u7684\u4e30\u5bcc\u4fe1\u606f\uff0c\u8fd9\u4e9b\u53cd\u5c04\u53ef\u4ee5\u63d0\u4f9b\u8865\u5145\u89c6\u89d2\u6765\u586b\u8865\u7f3a\u5931\u7ec6\u8282\u3002", "method": "\u63d0\u51fa\u4e86ReflectiveGS\u65b9\u6cd5\uff0c\u8fd9\u662f3D\u9ad8\u65af\u6cfc\u6e85\u7684\u6269\u5c55\uff0c\u5c06\u955c\u9762\u53cd\u5c04\u4f5c\u4e3a\u8865\u5145\u89c6\u89d2\u800c\u975e\u7b80\u5355\u7684\u5bf9\u79f0\u4f2a\u5f71\uff0c\u4ece\u800c\u589e\u5f3a\u573a\u666f\u51e0\u4f55\u5e76\u6062\u590d\u7f3a\u5931\u7ec6\u8282\u3002", "result": "\u5728MirrorScene3D\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cReflectiveGS\u5728SSIM\u3001PSNR\u3001LPIPS\u6307\u6807\u548c\u8bad\u7ec3\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ReflectiveGS\u4e3a\u955c\u9762\u4e30\u5bcc\u73af\u5883\u4e2d\u76843D\u91cd\u5efa\u8bbe\u5b9a\u4e86\u65b0\u7684\u57fa\u51c6\uff0c\u8bc1\u660e\u4e86\u5229\u7528\u955c\u9762\u53cd\u5c04\u4f5c\u4e3a\u8865\u5145\u89c6\u89d2\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.18958", "pdf": "https://arxiv.org/pdf/2509.18958", "abs": "https://arxiv.org/abs/2509.18958", "authors": ["Cristina Iacono", "Mariarosaria Meola", "Federica Conte", "Laura Mecozzi", "Umberto Bracale", "Pietro Falco", "Fanny Ficuciello"], "title": "Generative data augmentation for biliary tract detection on intraoperative images", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Cholecystectomy is one of the most frequently performed procedures in gastrointestinal surgery, and the laparoscopic approach is the gold standard for symptomatic cholecystolithiasis and acute cholecystitis. In addition to the advantages of a significantly faster recovery and better cosmetic results, the laparoscopic approach bears a higher risk of bile duct injury, which has a significant impact on quality of life and survival. To avoid bile duct injury, it is essential to improve the intraoperative visualization of the bile duct. This work aims to address this problem by leveraging a deep-learning approach for the localization of the biliary tract from white-light images acquired during the surgical procedures. To this end, the construction and annotation of an image database to train the Yolo detection algorithm has been employed. Besides classical data augmentation techniques, the paper proposes Generative Adversarial Network (GAN) for the generation of a synthetic portion of the training dataset. Experimental results have been discussed along with ethical considerations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u672f\u4e2d\u80c6\u9053\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u4f7f\u7528Yolo\u68c0\u6d4b\u7b97\u6cd5\u548cGAN\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u65e8\u5728\u964d\u4f4e\u8179\u8154\u955c\u80c6\u56ca\u5207\u9664\u672f\u4e2d\u80c6\u7ba1\u635f\u4f24\u7684\u98ce\u9669\u3002", "motivation": "\u8179\u8154\u955c\u80c6\u56ca\u5207\u9664\u672f\u867d\u7136\u6062\u590d\u5feb\u3001\u7f8e\u5bb9\u6548\u679c\u597d\uff0c\u4f46\u80c6\u7ba1\u635f\u4f24\u98ce\u9669\u8f83\u9ad8\uff0c\u4e25\u91cd\u5f71\u54cd\u60a3\u8005\u751f\u6d3b\u8d28\u91cf\u548c\u751f\u5b58\u7387\u3002\u4e3a\u6539\u5584\u672f\u4e2d\u80c6\u9053\u53ef\u89c6\u5316\uff0c\u9700\u8981\u5f00\u53d1\u53ef\u9760\u7684\u80c6\u9053\u5b9a\u4f4d\u6280\u672f\u3002", "method": "\u6784\u5efa\u5e76\u6807\u6ce8\u56fe\u50cf\u6570\u636e\u5e93\u8bad\u7ec3Yolo\u68c0\u6d4b\u7b97\u6cd5\uff0c\u91c7\u7528\u7ecf\u5178\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5e76\u63d0\u51fa\u4f7f\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc(GAN)\u751f\u6210\u90e8\u5206\u5408\u6210\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5b9a\u4f4d\u80c6\u9053\u7ed3\u6784\uff0c\u4f46\u5177\u4f53\u6027\u80fd\u6307\u6807\u672a\u5728\u6458\u8981\u4e2d\u8be6\u7ec6\u8bf4\u660e\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6709\u671b\u6539\u5584\u8179\u8154\u955c\u80c6\u56ca\u5207\u9664\u672f\u4e2d\u7684\u80c6\u9053\u53ef\u89c6\u5316\uff0c\u964d\u4f4e\u80c6\u7ba1\u635f\u4f24\u98ce\u9669\uff0c\u540c\u65f6\u9700\u8981\u5173\u6ce8\u76f8\u5173\u7684\u4f26\u7406\u8003\u91cf\u3002"}}
{"id": "2509.19052", "pdf": "https://arxiv.org/pdf/2509.19052", "abs": "https://arxiv.org/abs/2509.19052", "authors": ["Jierui Qu", "Jianchun Zhao"], "title": "A DyL-Unet framework based on dynamic learning for Temporally Consistent Echocardiographic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate segmentation of cardiac anatomy in echocardiography is essential for cardiovascular diagnosis and treatment. Yet echocardiography is prone to deformation and speckle noise, causing frame-to-frame segmentation jitter. Even with high accuracy in single-frame segmentation, temporal instability can weaken functional estimates and impair clinical interpretability. To address these issues, we propose DyL-UNet, a dynamic learning-based temporal consistency U-Net segmentation architecture designed to achieve temporally stable and precise echocardiographic segmentation. The framework constructs an Echo-Dynamics Graph (EDG) through dynamic learning to extract dynamic information from videos. DyL-UNet incorporates multiple Swin-Transformer-based encoder-decoder branches for processing single-frame images. It further introduces Cardiac Phase-Dynamics Attention (CPDA) at the skip connections, which uses EDG-encoded dynamic features and cardiac-phase cues to enforce temporal consistency during segmentation. Extensive experiments on the CAMUS and EchoNet-Dynamic datasets demonstrate that DyL-UNet maintains segmentation accuracy comparable to existing methods while achieving superior temporal consistency, providing a reliable solution for automated clinical echocardiography.", "AI": {"tldr": "DyL-UNet\u662f\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u5b66\u4e60\u7684\u65f6\u95f4\u4e00\u81f4\u6027U-Net\u5206\u5272\u67b6\u6784\uff0c\u65e8\u5728\u5b9e\u73b0\u8d85\u58f0\u5fc3\u52a8\u56fe\u5206\u5272\u7684\u65f6\u95f4\u7a33\u5b9a\u6027\u548c\u7cbe\u786e\u6027\uff0c\u901a\u8fc7\u6784\u5efa\u56de\u58f0\u52a8\u6001\u56fe\u548c\u4f7f\u7528\u5fc3\u810f\u76f8\u4f4d\u52a8\u6001\u6ce8\u610f\u529b\u673a\u5236\u6765\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u8d85\u58f0\u5fc3\u52a8\u56fe\u5bb9\u6613\u53d8\u5f62\u548c\u4ea7\u751f\u6591\u70b9\u566a\u58f0\uff0c\u5bfc\u81f4\u5e27\u95f4\u5206\u5272\u6296\u52a8\uff0c\u5373\u4f7f\u5355\u5e27\u5206\u5272\u7cbe\u5ea6\u9ad8\uff0c\u65f6\u95f4\u4e0d\u7a33\u5b9a\u6027\u4e5f\u4f1a\u524a\u5f31\u529f\u80fd\u4f30\u8ba1\u5e76\u5f71\u54cd\u4e34\u5e8a\u89e3\u91ca\u6027\u3002", "method": "DyL-UNet\u901a\u8fc7\u52a8\u6001\u5b66\u4e60\u6784\u5efa\u56de\u58f0\u52a8\u6001\u56fe\u63d0\u53d6\u89c6\u9891\u52a8\u6001\u4fe1\u606f\uff0c\u91c7\u7528\u591a\u4e2a\u57fa\u4e8eSwin-Transformer\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5206\u652f\u5904\u7406\u5355\u5e27\u56fe\u50cf\uff0c\u5e76\u5728\u8df3\u8dc3\u8fde\u63a5\u5904\u5f15\u5165\u5fc3\u810f\u76f8\u4f4d\u52a8\u6001\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5229\u7528\u52a8\u6001\u7279\u5f81\u548c\u5fc3\u810f\u76f8\u4f4d\u7ebf\u7d22\u5f3a\u5236\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5728CAMUS\u548cEchoNet-Dynamic\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDyL-UNet\u5728\u4fdd\u6301\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u7684\u5206\u5272\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "conclusion": "DyL-UNet\u4e3a\u81ea\u52a8\u5316\u4e34\u5e8a\u8d85\u58f0\u5fc3\u52a8\u56fe\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5b9e\u73b0\u65f6\u95f4\u7a33\u5b9a\u4e14\u7cbe\u786e\u7684\u5206\u5272\u3002"}}
{"id": "2509.19073", "pdf": "https://arxiv.org/pdf/2509.19073", "abs": "https://arxiv.org/abs/2509.19073", "authors": ["Hung Nguyen", "Runfa Li", "An Le", "Truong Nguyen"], "title": "WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction", "categories": ["cs.CV", "eess.IV", "eess.SP"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has become a powerful representation for image-based object reconstruction, yet its performance drops sharply in sparse-view settings. Prior works address this limitation by employing diffusion models to repair corrupted renders, subsequently using them as pseudo ground truths for later optimization. While effective, such approaches incur heavy computation from the diffusion fine-tuning and repair steps. We present WaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object reconstruction. Our key idea is to shift diffusion into the wavelet domain: diffusion is applied only to the low-resolution LL subband, while high-frequency subbands are refined with a lightweight network. We further propose an efficient online random masking strategy to curate training pairs for diffusion fine-tuning, replacing the commonly used, but inefficient, leave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360 and OmniObject3D, show WaveletGaussian achieves competitive rendering quality while substantially reducing training time.", "AI": {"tldr": "WaveletGaussian\u662f\u4e00\u4e2a\u7528\u4e8e\u7a00\u758f\u89c6\u56fe3D\u9ad8\u65af\u5bf9\u8c61\u91cd\u5efa\u7684\u9ad8\u6548\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6269\u6563\u8fc7\u7a0b\u8f6c\u79fb\u5230\u5c0f\u6ce2\u57df\uff0c\u4ec5\u5728\u4f4e\u5206\u8fa8\u7387LL\u5b50\u5e26\u5e94\u7528\u6269\u6563\uff0c\u9ad8\u9891\u5b50\u5e26\u4f7f\u7528\u8f7b\u91cf\u7f51\u7edc\u4f18\u5316\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "3D\u9ad8\u65af\u6e85\u5c04\u5728\u7a00\u758f\u89c6\u56fe\u8bbe\u7f6e\u4e0b\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u6269\u6563\u6a21\u578b\u4fee\u590d\u635f\u574f\u7684\u6e32\u67d3\u56fe\u50cf\u4f5c\u4e3a\u4f2a\u5730\u9762\u771f\u503c\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51fa\u5c0f\u6ce2\u57df\u6269\u6563\u65b9\u6cd5\uff1a\u5728\u4f4e\u5206\u8fa8\u7387LL\u5b50\u5e26\u5e94\u7528\u6269\u6563\uff0c\u9ad8\u9891\u5b50\u5e26\u4f7f\u7528\u8f7b\u91cf\u7f51\u7edc\u4f18\u5316\uff1b\u91c7\u7528\u9ad8\u6548\u7684\u5728\u7ebf\u968f\u673a\u63a9\u7801\u7b56\u7565\u66ff\u4ee3\u4f4e\u6548\u7684\u7559\u4e00\u7b56\u7565\u6765\u6784\u5efa\u8bad\u7ec3\u5bf9\u3002", "result": "\u5728Mip-NeRF 360\u548cOmniObject3D\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cWaveletGaussian\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u6e32\u67d3\u8d28\u91cf\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "WaveletGaussian\u6846\u67b6\u901a\u8fc7\u5c0f\u6ce2\u57df\u7684\u9ad8\u6548\u6269\u6563\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7a00\u758f\u89c6\u56fe3D\u9ad8\u65af\u91cd\u5efa\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8d28\u91cf\u4e0e\u6548\u7387\u7684\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2509.19208", "pdf": "https://arxiv.org/pdf/2509.19208", "abs": "https://arxiv.org/abs/2509.19208", "authors": ["Earl Ranario", "Ismael Mayanja", "Heesup Yun", "Brian N. Bailey", "J. Mason Earles"], "title": "Enabling Plant Phenotyping in Weedy Environments using Multi-Modal Imagery via Synthetic and Generated Training Data", "categories": ["cs.CV"], "comment": null, "summary": "Accurate plant segmentation in thermal imagery remains a significant challenge for high throughput field phenotyping, particularly in outdoor environments where low contrast between plants and weeds and frequent occlusions hinder performance. To address this, we present a framework that leverages synthetic RGB imagery, a limited set of real annotations, and GAN-based cross-modality alignment to enhance semantic segmentation in thermal images. We trained models on 1,128 synthetic images containing complex mixtures of crop and weed plants in order to generate image segmentation masks for crop and weed plants. We additionally evaluated the benefit of integrating as few as five real, manually segmented field images within the training process using various sampling strategies. When combining all the synthetic images with a few labeled real images, we observed a maximum relative improvement of 22% for the weed class and 17% for the plant class compared to the full real-data baseline. Cross-modal alignment was enabled by translating RGB to thermal using CycleGAN-turbo, allowing robust template matching without calibration. Results demonstrated that combining synthetic data with limited manual annotations and cross-domain translation via generative models can significantly boost segmentation performance in complex field environments for multi-model imagery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5408\u6210RGB\u56fe\u50cf\u3001\u5c11\u91cf\u771f\u5b9e\u6807\u6ce8\u548cGAN\u8de8\u6a21\u6001\u5bf9\u9f50\u6765\u589e\u5f3a\u70ed\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u7684\u6846\u67b6\uff0c\u5728\u590d\u6742\u7530\u95f4\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u690d\u7269\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u70ed\u56fe\u50cf\u4e2d\u7684\u690d\u7269\u5206\u5272\u5728\u5ba4\u5916\u9ad8\u901a\u91cf\u8868\u578b\u5206\u6790\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u662f\u690d\u7269\u4e0e\u6742\u8349\u5bf9\u6bd4\u5ea6\u4f4e\u548c\u9891\u7e41\u906e\u6321\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u4f7f\u75281,128\u5f20\u5408\u6210\u56fe\u50cf\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u4f5c\u7269\u548c\u6742\u8349\u5206\u5272\u63a9\u7801\uff0c\u7ed3\u5408\u5c11\u91cf\u771f\u5b9e\u6807\u6ce8\u56fe\u50cf\uff0c\u91c7\u7528CycleGAN-turbo\u8fdb\u884cRGB\u5230\u70ed\u56fe\u50cf\u7684\u8de8\u6a21\u6001\u8f6c\u6362\u3002", "result": "\u7ed3\u5408\u6240\u6709\u5408\u6210\u56fe\u50cf\u548c\u5c11\u91cf\u771f\u5b9e\u56fe\u50cf\uff0c\u6742\u8349\u7c7b\u548c\u690d\u7269\u7c7b\u7684\u5206\u5272\u6027\u80fd\u76f8\u6bd4\u5168\u771f\u5b9e\u6570\u636e\u57fa\u7ebf\u5206\u522b\u63d0\u5347\u4e8622%\u548c17%\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u4e0e\u6709\u9650\u624b\u52a8\u6807\u6ce8\u4ee5\u53ca\u751f\u6210\u6a21\u578b\u7684\u8de8\u57df\u8f6c\u6362\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u590d\u6742\u7530\u95f4\u73af\u5883\u4e2d\u591a\u6a21\u6001\u56fe\u50cf\u7684\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2509.19230", "pdf": "https://arxiv.org/pdf/2509.19230", "abs": "https://arxiv.org/abs/2509.19230", "authors": ["Tianshuo Zhang", "Li Gao", "Siran Peng", "Xiangyu Zhu", "Zhen Lei"], "title": "DevFD: Developmental Face Forgery Detection by Learning Shared and Orthogonal LoRA Subspaces", "categories": ["cs.CV"], "comment": "Accepted by NeurIPS 2025", "summary": "The rise of realistic digital face generation and manipulation poses significant social risks. The primary challenge lies in the rapid and diverse evolution of generation techniques, which often outstrip the detection capabilities of existing models. To defend against the ever-evolving new types of forgery, we need to enable our model to quickly adapt to new domains with limited computation and data while avoiding forgetting previously learned forgery types. In this work, we posit that genuine facial samples are abundant and relatively stable in acquisition methods, while forgery faces continuously evolve with the iteration of manipulation techniques. Given the practical infeasibility of exhaustively collecting all forgery variants, we frame face forgery detection as a continual learning problem and allow the model to develop as new forgery types emerge. Specifically, we employ a Developmental Mixture of Experts (MoE) architecture that uses LoRA models as its individual experts. These experts are organized into two groups: a Real-LoRA to learn and refine knowledge of real faces, and multiple Fake-LoRAs to capture incremental information from different forgery types. To prevent catastrophic forgetting, we ensure that the learning direction of Fake-LoRAs is orthogonal to the established subspace. Moreover, we integrate orthogonal gradients into the orthogonal loss of Fake-LoRAs, preventing gradient interference throughout the training process of each task. Experimental results under both the datasets and manipulation types incremental protocols demonstrate the effectiveness of our method.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6301\u7eed\u5b66\u4e60\u7684\u6570\u5b57\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d1\u5c55\u6027\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u6765\u5e94\u5bf9\u4e0d\u65ad\u6f14\u53d8\u7684\u4f2a\u9020\u6280\u672f\uff0c\u9632\u6b62\u6a21\u578b\u9057\u5fd8\u5df2\u5b66\u4e60\u7684\u4f2a\u9020\u7c7b\u578b\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u4eba\u8138\u751f\u6210\u548c\u64cd\u7eb5\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u73b0\u6709\u7684\u68c0\u6d4b\u6a21\u578b\u96be\u4ee5\u8ddf\u4e0a\u6280\u672f\u8fed\u4ee3\u901f\u5ea6\u3002\u9700\u8981\u8ba9\u6a21\u578b\u80fd\u591f\u5feb\u901f\u9002\u5e94\u65b0\u9886\u57df\uff0c\u540c\u65f6\u907f\u514d\u9057\u5fd8\u5df2\u5b66\u4e60\u7684\u4f2a\u9020\u7c7b\u578b\u3002", "method": "\u91c7\u7528\u53d1\u5c55\u6027\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u4f7f\u7528LoRA\u6a21\u578b\u4f5c\u4e3a\u4e13\u5bb6\uff0c\u5206\u4e3aReal-LoRA\u5b66\u4e60\u771f\u5b9e\u4eba\u8138\u77e5\u8bc6\uff0c\u591a\u4e2aFake-LoRA\u6355\u83b7\u4e0d\u540c\u4f2a\u9020\u7c7b\u578b\u7684\u589e\u91cf\u4fe1\u606f\u3002\u901a\u8fc7\u6b63\u4ea4\u68af\u5ea6\u548c\u6b63\u4ea4\u635f\u5931\u9632\u6b62\u68af\u5ea6\u5e72\u6270\u548c\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u5728\u6570\u636e\u96c6\u548c\u64cd\u7eb5\u7c7b\u578b\u589e\u91cf\u534f\u8bae\u4e0b\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u6784\u5efa\u4e3a\u6301\u7eed\u5b66\u4e60\u95ee\u9898\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u4e0d\u65ad\u6f14\u53d8\u7684\u4f2a\u9020\u6280\u672f\u6311\u6218\u3002"}}
{"id": "2509.19244", "pdf": "https://arxiv.org/pdf/2509.19244", "abs": "https://arxiv.org/abs/2509.19244", "authors": ["Shufan Li", "Jiuxiang Gu", "Kangning Liu", "Zhe Lin", "Zijun Wei", "Aditya Grover", "Jason Kuen"], "title": "Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation", "categories": ["cs.CV"], "comment": "32 pages, 15 figures", "summary": "We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM) capable of image understanding and generation tasks. Unlike existing multimodal diffsion language models such as MMaDa and Muddit which only support simple image-level understanding tasks and low-resolution image generation, Lavida-O exhibits many new capabilities such as object grounding, image-editing, and high-resolution (1024px) image synthesis. It is also the first unified MDM that uses its understanding capabilities to improve image generation and editing results through planning and iterative self-reflection. To allow effective and efficient training and sampling, Lavida-O ntroduces many novel techniques such as Elastic Mixture-of-Transformer architecture, universal text conditioning, and stratified sampling. \\ours~achieves state-of-the-art performance on a wide range of benchmarks such as RefCOCO object grounding, GenEval text-to-image generation, and ImgEdit image editing, outperforming existing autoregressive and continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while offering considerable speedup at inference.", "AI": {"tldr": "Lavida-O\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u63a9\u7801\u6269\u6563\u6a21\u578b\uff0c\u652f\u6301\u56fe\u50cf\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\uff0c\u5177\u5907\u7269\u4f53\u5b9a\u4f4d\u3001\u56fe\u50cf\u7f16\u8f91\u548c\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5408\u6210\u7b49\u65b0\u80fd\u529b", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4ec5\u652f\u6301\u7b80\u5355\u7684\u56fe\u50cf\u7ea7\u7406\u89e3\u4efb\u52a1\u548c\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\uff0c\u65e0\u6cd5\u6ee1\u8db3\u590d\u6742\u4efb\u52a1\u9700\u6c42", "method": "\u91c7\u7528\u5f39\u6027\u6df7\u5408Transformer\u67b6\u6784\u3001\u901a\u7528\u6587\u672c\u6761\u4ef6\u5316\u548c\u5206\u5c42\u91c7\u6837\u7b49\u65b0\u6280\u672f\uff0c\u901a\u8fc7\u89c4\u5212\u548c\u8fed\u4ee3\u81ea\u53cd\u601d\u5229\u7528\u7406\u89e3\u80fd\u529b\u6539\u8fdb\u751f\u6210\u7ed3\u679c", "result": "\u5728RefCOCO\u7269\u4f53\u5b9a\u4f4d\u3001GenEval\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u548cImgEdit\u56fe\u50cf\u7f16\u8f91\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8d85\u8d8aQwen2.5-VL\u548cFluxKontext-dev\u7b49\u6a21\u578b", "conclusion": "Lavida-O\u662f\u9996\u4e2a\u7edf\u4e00\u7684\u63a9\u7801\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u7406\u89e3\u80fd\u529b\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u5728\u63a8\u7406\u65f6\u63d0\u4f9b\u663e\u8457\u52a0\u901f"}}
{"id": "2509.19252", "pdf": "https://arxiv.org/pdf/2509.19252", "abs": "https://arxiv.org/abs/2509.19252", "authors": ["Gabriel Maldonado", "Narges Rashvand", "Armin Danesh Pazho", "Ghazal Alinezhad Noghre", "Vinit Katariya", "Hamed Tabkhi"], "title": "Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Continuous human motion understanding remains a core challenge in computer vision due to its high dimensionality and inherent redundancy. Efficient compression and representation are crucial for analyzing complex motion dynamics. In this work, we introduce an adversarially-refined VQ-GAN framework with dense motion tokenization for compressing spatio-temporal heatmaps while preserving the fine-grained traces of human motion. Our approach combines dense motion tokenization with adversarial refinement, which eliminates reconstruction artifacts like motion smearing and temporal misalignment observed in non-adversarial baselines. Our experiments on the CMU Panoptic dataset provide conclusive evidence of our method's superiority, outperforming the dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%. Furthermore, our dense tokenization strategy enables a novel analysis of motion complexity, revealing that 2D motion can be optimally represented with a compact 128-token vocabulary, while 3D motion's complexity demands a much larger 1024-token codebook for faithful reconstruction. These results establish practical deployment feasibility across diverse motion analysis applications. The code base for this work is available at https://github.com/TeCSAR-UNCC/Pose-Quantization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6297\u6027\u7cbe\u70bc\u7684VQ-GAN\u6846\u67b6\uff0c\u901a\u8fc7\u5bc6\u96c6\u8fd0\u52a8\u6807\u8bb0\u5316\u6280\u672f\u538b\u7f29\u65f6\u7a7a\u70ed\u56fe\uff0c\u6709\u6548\u89e3\u51b3\u4eba\u4f53\u8fd0\u52a8\u7406\u89e3\u4e2d\u7684\u9ad8\u7ef4\u5ea6\u548c\u5197\u4f59\u95ee\u9898\u3002", "motivation": "\u8fde\u7eed\u4eba\u4f53\u8fd0\u52a8\u7406\u89e3\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u9762\u4e34\u9ad8\u7ef4\u5ea6\u548c\u5185\u5728\u5197\u4f59\u7684\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u7684\u538b\u7f29\u548c\u8868\u793a\u65b9\u6cd5\u6765\u5206\u6790\u590d\u6742\u7684\u8fd0\u52a8\u52a8\u6001\u3002", "method": "\u7ed3\u5408\u5bc6\u96c6\u8fd0\u52a8\u6807\u8bb0\u5316\u548c\u5bf9\u6297\u6027\u7cbe\u70bc\u7684VQ-GAN\u6846\u67b6\uff0c\u6d88\u9664\u91cd\u5efa\u4f2a\u5f71\u5982\u8fd0\u52a8\u6a21\u7cca\u548c\u65f6\u95f4\u9519\u4f4d\u3002", "result": "\u5728CMU Panoptic\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u6bd4dVAE\u57fa\u7ebf\u5728SSIM\u6307\u6807\u4e0a\u63d0\u53479.31%\uff0c\u65f6\u95f4\u4e0d\u7a33\u5b9a\u6027\u964d\u4f4e37.1%\u3002\u53d1\u73b02D\u8fd0\u52a8\u53ef\u7528128\u4e2a\u6807\u8bb0\u8bcd\u6c47\u8868\u8868\u793a\uff0c3D\u8fd0\u52a8\u9700\u89811024\u4e2a\u6807\u8bb0\u4ee3\u7801\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5404\u79cd\u8fd0\u52a8\u5206\u6790\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u6027\uff0c\u8bc1\u660e\u4e86\u5bc6\u96c6\u6807\u8bb0\u5316\u7b56\u7565\u5728\u8fd0\u52a8\u590d\u6742\u5ea6\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.19282", "pdf": "https://arxiv.org/pdf/2509.19282", "abs": "https://arxiv.org/abs/2509.19282", "authors": ["Bingnan Li", "Chen-Yu Wang", "Haiyang Xu", "Xiang Zhang", "Ethan Armand", "Divyansh Srivastava", "Xiaojun Shan", "Zeyuan Chen", "Jianwen Xie", "Zhuowen Tu"], "title": "OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps", "categories": ["cs.CV"], "comment": "Accepted to NeurIPS 2025 Dataset&Benchmark Track", "summary": "Despite steady progress in layout-to-image generation, current methods still struggle with layouts containing significant overlap between bounding boxes. We identify two primary challenges: (1) large overlapping regions and (2) overlapping instances with minimal semantic distinction. Through both qualitative examples and quantitative analysis, we demonstrate how these factors degrade generation quality. To systematically assess this issue, we introduce OverLayScore, a novel metric that quantifies the complexity of overlapping bounding boxes. Our analysis reveals that existing benchmarks are biased toward simpler cases with low OverLayScore values, limiting their effectiveness in evaluating model performance under more challenging conditions. To bridge this gap, we present OverLayBench, a new benchmark featuring high-quality annotations and a balanced distribution across different levels of OverLayScore. As an initial step toward improving performance on complex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a curated amodal mask dataset. Together, our contributions lay the groundwork for more robust layout-to-image generation under realistic and challenging scenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86OverLayScore\u6307\u6807\u548cOverLayBench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u5904\u7406\u5e03\u5c40\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u8fb9\u754c\u6846\u91cd\u53e0\u7684\u6311\u6218\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5e03\u5c40\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u5728\u5904\u7406\u8fb9\u754c\u6846\u663e\u8457\u91cd\u53e0\u7684\u5e03\u5c40\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u57fa\u51c6\u504f\u5411\u4e8e\u7b80\u5355\u91cd\u53e0\u60c5\u51b5\uff0c\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u6a21\u578b\u5728\u590d\u6742\u91cd\u53e0\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "method": "1\uff09\u63d0\u51faOverLayScore\u6307\u6807\u91cf\u5316\u8fb9\u754c\u6846\u91cd\u53e0\u590d\u6742\u5ea6\uff1b2\uff09\u6784\u5efaOverLayBench\u57fa\u51c6\uff0c\u5305\u542b\u9ad8\u8d28\u91cf\u6807\u6ce8\u548c\u5e73\u8861\u7684OverLayScore\u5206\u5e03\uff1b3\uff09\u63d0\u51faCreatiLayout-AM\u6a21\u578b\uff0c\u5728amodal\u63a9\u7801\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5206\u6790\u8868\u660e\u73b0\u6709\u57fa\u51c6\u5b58\u5728\u504f\u5411\u4f4eOverLayScore\u503c\u7684\u504f\u5dee\uff0c\u9650\u5236\u4e86\u8bc4\u4f30\u6a21\u578b\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u7684\u6709\u6548\u6027\u3002\u63d0\u51fa\u7684OverLayBench\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u73b0\u5b9e\u548c\u6311\u6218\u6027\u573a\u666f\u4e0b\u5b9e\u73b0\u66f4\u7a33\u5065\u7684\u5e03\u5c40\u5230\u56fe\u50cf\u751f\u6210\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u901a\u8fc7\u65b0\u6307\u6807\u548c\u57fa\u51c6\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.19297", "pdf": "https://arxiv.org/pdf/2509.19297", "abs": "https://arxiv.org/abs/2509.19297", "authors": ["Weijie Wang", "Yeqing Chen", "Zeyu Zhang", "Hengyu Liu", "Haoxiao Wang", "Zhiyuan Feng", "Wenkang Qin", "Zheng Zhu", "Donny Y. Chen", "Bohan Zhuang"], "title": "VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction", "categories": ["cs.CV"], "comment": "Project Page: https://lhmd.top/volsplat, Code:   https://github.com/ziplab/VolSplat", "summary": "Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective solution for novel view synthesis. Existing methods predominantly rely on a pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a 3D Gaussian. We rethink this widely adopted formulation and identify several inherent limitations: it renders the reconstructed 3D models heavily dependent on the number of input views, leads to view-biased density distributions, and introduces alignment errors, particularly when source views contain occlusions or low texture. To address these challenges, we introduce VolSplat, a new multi-view feed-forward paradigm that replaces pixel alignment with voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature matching, ensuring robust multi-view consistency. Furthermore, it enables adaptive control over Gaussian density based on 3D scene complexity, yielding more faithful Gaussian point clouds, improved geometric consistency, and enhanced novel-view rendering quality. Experiments on widely used benchmarks including RealEstate10K and ScanNet demonstrate that VolSplat achieves state-of-the-art performance while producing more plausible and view-consistent Gaussian reconstructions. In addition to superior results, our approach establishes a more scalable framework for feed-forward 3D reconstruction with denser and more robust representations, paving the way for further research in wider communities. The video results, code and trained models are available on our project page: https://lhmd.top/volsplat.", "AI": {"tldr": "VolSplat\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u89c6\u89d2\u524d\u99883D\u9ad8\u65af\u6e85\u5c04\u65b9\u6cd5\uff0c\u7528\u4f53\u7d20\u5bf9\u9f50\u7684\u9ad8\u65af\u9884\u6d4b\u53d6\u4ee3\u50cf\u7d20\u5bf9\u9f50\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5bf9\u8f93\u5165\u89c6\u89d2\u6570\u91cf\u7684\u4f9d\u8d56\u3001\u89c6\u89d2\u504f\u5dee\u5bc6\u5ea6\u5206\u5e03\u548c\u5bf9\u9f50\u8bef\u5dee\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u50cf\u7d20\u5bf9\u9f503D\u9ad8\u65af\u9884\u6d4b\u65b9\u6cd5\u5b58\u5728\u5bf9\u8f93\u5165\u89c6\u89d2\u6570\u91cf\u7684\u4f9d\u8d56\u3001\u89c6\u89d2\u504f\u5dee\u5bc6\u5ea6\u5206\u5e03\u3001\u4ee5\u53ca\u5bf9\u906e\u6321\u548c\u4f4e\u7eb9\u7406\u533a\u57df\u7684\u5bf9\u9f50\u8bef\u5dee\u7b49\u56fa\u6709\u5c40\u9650\u6027\u3002", "method": "VolSplat\u91c7\u7528\u4f53\u7d20\u5bf9\u9f50\u7684\u9ad8\u65af\u9884\u6d4b\u8303\u5f0f\uff0c\u76f4\u63a5\u4ece\u9884\u6d4b\u76843D\u4f53\u7d20\u7f51\u683c\u9884\u6d4b\u9ad8\u65af\u5206\u5e03\uff0c\u907f\u514d\u4e86\u57fa\u4e8e2D\u7279\u5f81\u5339\u914d\u7684\u50cf\u7d20\u5bf9\u9f50\u65b9\u6cd5\u7684\u95ee\u9898\u3002", "result": "\u5728RealEstate10K\u548cScanNet\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVolSplat\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4ea7\u751f\u66f4\u53ef\u4fe1\u548c\u89c6\u89d2\u4e00\u81f4\u76843D\u9ad8\u65af\u91cd\u5efa\uff0c\u5e76\u63d0\u9ad8\u4e86\u65b0\u89c6\u89d2\u6e32\u67d3\u8d28\u91cf\u3002", "conclusion": "VolSplat\u4e3a\u524d\u99883D\u91cd\u5efa\u5efa\u7acb\u4e86\u4e00\u4e2a\u66f4\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u66f4\u5bc6\u96c6\u548c\u66f4\u9c81\u68d2\u7684\u8868\u793a\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u793e\u533a\u7684\u7814\u7a76\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.19044", "pdf": "https://arxiv.org/pdf/2509.19044", "abs": "https://arxiv.org/abs/2509.19044", "authors": ["Yang Li", "Chenyu Wang", "Tingrui Wang", "Yongwei Wang", "Haonan Li", "Zhunga Liu", "Quan Pan"], "title": "Latent Danger Zone: Distilling Unified Attention for Cross-Architecture Black-box Attacks", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Black-box adversarial attacks remain challenging due to limited access to model internals. Existing methods often depend on specific network architectures or require numerous queries, resulting in limited cross-architecture transferability and high query costs. To address these limitations, we propose JAD, a latent diffusion model framework for black-box adversarial attacks. JAD generates adversarial examples by leveraging a latent diffusion model guided by attention maps distilled from both a convolutional neural network (CNN) and a Vision Transformer (ViT) models. By focusing on image regions that are commonly sensitive across architectures, this approach crafts adversarial perturbations that transfer effectively between different model types. This joint attention distillation strategy enables JAD to be architecture-agnostic, achieving superior attack generalization across diverse models. Moreover, the generative nature of the diffusion framework yields high adversarial sample generation efficiency by reducing reliance on iterative queries. Experiments demonstrate that JAD offers improved attack generalization, generation efficiency, and cross-architecture transferability compared to existing methods, providing a promising and effective paradigm for black-box adversarial attacks.", "AI": {"tldr": "JAD\u662f\u4e00\u4e2a\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u9ed1\u76d2\u5bf9\u6297\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u6ce8\u610f\u529b\u84b8\u998f\u7b56\u7565\u5b9e\u73b0\u8de8\u67b6\u6784\u7684\u5bf9\u6297\u6837\u672c\u751f\u6210\uff0c\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u6027\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u9ed1\u76d2\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u4f9d\u8d56\u7279\u5b9a\u7f51\u7edc\u67b6\u6784\u3001\u67e5\u8be2\u6b21\u6570\u591a\u3001\u8de8\u67b6\u6784\u8fc1\u79fb\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u4eceCNN\u548cViT\u6a21\u578b\u4e2d\u84b8\u998f\u6ce8\u610f\u529b\u56fe\u6765\u6307\u5bfc\u5bf9\u6297\u6837\u672c\u751f\u6210\uff0c\u805a\u7126\u4e8e\u8de8\u67b6\u6784\u654f\u611f\u7684\u56fe\u50cf\u533a\u57df\u3002", "result": "JAD\u5728\u653b\u51fb\u6cdb\u5316\u6027\u3001\u751f\u6210\u6548\u7387\u548c\u8de8\u67b6\u6784\u8fc1\u79fb\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "JAD\u4e3a\u9ed1\u76d2\u5bf9\u6297\u653b\u51fb\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u6709\u6548\u8303\u5f0f\uff0c\u5177\u6709\u67b6\u6784\u65e0\u5173\u6027\u548c\u9ad8\u751f\u6210\u6548\u7387\u3002"}}
