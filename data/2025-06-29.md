<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 3]
- [cs.CV](#cs.CV) [Total: 19]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [3DGH: 3D Head Generation with Composable Hair and Face](https://arxiv.org/abs/2506.20875)
*Chengan He,Junxuan Li,Tobias Kirschstein,Artem Sevastopolsky,Shunsuke Saito,Qingyang Tan,Javier Romero,Chen Cao,Holly Rushmeier,Giljoo Nam*

Main category: cs.GR

TL;DR: 3DGH是一种无条件生成3D人头模型的方法，支持头发和脸部的可组合性。通过分离头发和脸部的建模，采用基于模板的3D高斯泼溅数据表示和变形头发几何体，实现了不同发型的几何变化捕捉。模型采用双生成器架构和交叉注意力机制，训练时使用合成渲染数据，实验证明其在全头图像合成和可组合发型编辑上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在头发和脸部建模上存在纠缠问题，限制了生成模型的灵活性和编辑能力。3DGH旨在通过分离这两部分的建模，提升生成质量和可编辑性。

Method: 提出基于模板的3D高斯泼溅数据表示和变形头发几何体，设计双生成器架构和交叉注意力机制，训练时使用合成渲染数据和特定目标函数。

Result: 实验验证了3DGH的设计选择，在无条件全头图像合成和可组合发型编辑上优于现有3D GAN方法。

Conclusion: 3DGH通过分离头发和脸部建模，实现了高质量的3D人头生成和灵活的编辑能力，为相关领域提供了新思路。

Abstract: We present 3DGH, an unconditional generative model for 3D human heads with composable hair and face components. Unlike previous work that entangles the modeling of hair and face, we propose to separate them using a novel data representation with template-based 3D Gaussian Splatting, in which deformable hair geometry is introduced to capture the geometric variations across different hairstyles. Based on this data representation, we design a 3D GAN-based architecture with dual generators and employ a cross-attention mechanism to model the inherent correlation between hair and face. The model is trained on synthetic renderings using carefully designed objectives to stabilize training and facilitate hair-face separation. We conduct extensive experiments to validate the design choice of 3DGH, and evaluate it both qualitatively and quantitatively by comparing with several state-of-the-art 3D GAN methods, demonstrating its effectiveness in unconditional full-head image synthesis and composable 3D hairstyle editing. More details will be available on our project page: https://c-he.github.io/projects/3dgh/.

</details>


### [2] [Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware Diffusion and Temporal Video Models](https://arxiv.org/abs/2506.20946)
*Donggoo Kang,Jangyeong Kim,Dasol Jeong,Junyoung Choi,Jeonga Wi,Hyunmin Lee,Joonho Gwon,Joonki Paik*

Main category: cs.GR

TL;DR: VideoTex利用视频生成模型解决3D纹理合成中的空间和时间不一致问题，结合几何感知条件和UV扩散策略，生成更平滑、一致的纹理。


<details>
  <summary>Details</summary>
Motivation: 现有纹理合成方法因缺乏全局上下文和几何理解导致不一致，而视频生成模型在时间一致性上表现优异。

Method: 结合几何感知条件和结构UV扩散策略，利用3D网格结构生成纹理。

Result: VideoTex在纹理保真度、接缝融合和稳定性上优于现有方法。

Conclusion: VideoTex为动态实时应用提供了高质量且时间一致的纹理合成方案。

Abstract: Current texture synthesis methods, which generate textures from fixed viewpoints, suffer from inconsistencies due to the lack of global context and geometric understanding. Meanwhile, recent advancements in video generation models have demonstrated remarkable success in achieving temporally consistent videos. In this paper, we introduce VideoTex, a novel framework for seamless texture synthesis that leverages video generation models to address both spatial and temporal inconsistencies in 3D textures. Our approach incorporates geometry-aware conditions, enabling precise utilization of 3D mesh structures. Additionally, we propose a structure-wise UV diffusion strategy, which enhances the generation of occluded areas by preserving semantic information, resulting in smoother and more coherent textures. VideoTex not only achieves smoother transitions across UV boundaries but also ensures high-quality, temporally stable textures across video frames. Extensive experiments demonstrate that VideoTex outperforms existing methods in texture fidelity, seam blending, and stability, paving the way for dynamic real-time applications that demand both visual quality and temporal coherence.

</details>


### [3] [FairyGen: Storied Cartoon Video from a Single Child-Drawn Character](https://arxiv.org/abs/2506.21272)
*Jiayi Zheng,Xiaodong Cun*

Main category: cs.GR

TL;DR: FairyGen 是一个从儿童绘画自动生成故事驱动卡通视频的系统，保持原画的独特艺术风格。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注角色一致性和基本动作，而 FairyGen 旨在通过分离角色建模与风格化背景生成，并融入电影镜头设计，实现更具表现力和连贯性的故事叙述。

Method: 系统使用 MLLM 生成结构化故事板，风格传播适配器确保视觉一致性，镜头设计模块增强视觉多样性和电影质量，3D 代理和 MMDiT 模型用于动画生成，两阶段运动定制适配器分离身份与运动。

Result: 实验表明，FairyGen 生成的动画风格忠实、叙事结构自然，适合个性化故事动画。

Conclusion: FairyGen 展示了在保持艺术风格的同时生成高质量故事动画的潜力。

Abstract: We propose FairyGen, an automatic system for generating story-driven cartoon videos from a single child's drawing, while faithfully preserving its unique artistic style. Unlike previous storytelling methods that primarily focus on character consistency and basic motion, FairyGen explicitly disentangles character modeling from stylized background generation and incorporates cinematic shot design to support expressive and coherent storytelling. Given a single character sketch, we first employ an MLLM to generate a structured storyboard with shot-level descriptions that specify environment settings, character actions, and camera perspectives. To ensure visual consistency, we introduce a style propagation adapter that captures the character's visual style and applies it to the background, faithfully retaining the character's full visual identity while synthesizing style-consistent scenes. A shot design module further enhances visual diversity and cinematic quality through frame cropping and multi-view synthesis based on the storyboard. To animate the story, we reconstruct a 3D proxy of the character to derive physically plausible motion sequences, which are then used to fine-tune an MMDiT-based image-to-video diffusion model. We further propose a two-stage motion customization adapter: the first stage learns appearance features from temporally unordered frames, disentangling identity from motion; the second stage models temporal dynamics using a timestep-shift strategy with frozen identity weights. Once trained, FairyGen directly renders diverse and coherent video scenes aligned with the storyboard. Extensive experiments demonstrate that our system produces animations that are stylistically faithful, narratively structured natural motion, highlighting its potential for personalized and engaging story animation. The code will be available at https://github.com/GVCLab/FairyGen

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation](https://arxiv.org/abs/2506.20756)
*Haodong Li,Chen Wang,Jiahui Lei,Kostas Daniilidis,Lingjie Liu*

Main category: cs.CV

TL;DR: StereoDiff结合立体匹配和视频深度扩散，通过两阶段方法提升视频深度估计的一致性和准确性。


<details>
  <summary>Details</summary>
Motivation: 视频深度估计不是图像深度估计的简单扩展，动态和静态区域的时序一致性需求不同。立体匹配更适合静态区域，而视频深度扩散更适合动态区域。

Method: 提出StereoDiff，两阶段方法：立体匹配处理静态区域，视频深度扩散处理动态区域。通过频域分析证明两者的互补性。

Result: 在零样本、真实世界动态视频深度基准测试中，StereoDiff表现最优，一致性和准确性显著提升。

Conclusion: StereoDiff通过结合立体匹配和视频深度扩散，实现了视频深度估计的SoTA性能。

Abstract: Recent video depth estimation methods achieve great performance by following the paradigm of image depth estimation, i.e., typically fine-tuning pre-trained video diffusion models with massive data. However, we argue that video depth estimation is not a naive extension of image depth estimation. The temporal consistency requirements for dynamic and static regions in videos are fundamentally different. Consistent video depth in static regions, typically backgrounds, can be more effectively achieved via stereo matching across all frames, which provides much stronger global 3D cues. While the consistency for dynamic regions still should be learned from large-scale video depth data to ensure smooth transitions, due to the violation of triangulation constraints. Based on these insights, we introduce StereoDiff, a two-stage video depth estimator that synergizes stereo matching for mainly the static areas with video depth diffusion for maintaining consistent depth transitions in dynamic areas. We mathematically demonstrate how stereo matching and video depth diffusion offer complementary strengths through frequency domain analysis, highlighting the effectiveness of their synergy in capturing the advantages of both. Experimental results on zero-shot, real-world, dynamic video depth benchmarks, both indoor and outdoor, demonstrate StereoDiff's SoTA performance, showcasing its superior consistency and accuracy in video depth estimation.

</details>


### [5] [Leveraging Vision-Language Models to Select Trustworthy Super-Resolution Samples Generated by Diffusion Models](https://arxiv.org/abs/2506.20832)
*Cansu Korkmaz,Ahmet Murat Tekalp,Zafer Dogan*

Main category: cs.CV

TL;DR: 本文提出了一种利用视觉语言模型（VLM）从扩散模型生成的高分辨率图像中选择最可信样本的自动化框架，并通过一种新的可信度评分（TWS）验证其效果。


<details>
  <summary>Details</summary>
Motivation: 超分辨率（SR）问题存在多个可行解，传统方法在平衡保真度和感知质量时可能引入伪影，而扩散模型生成多样解但难以选择最可信的样本。

Method: 利用VLM（如BLIP-2、GPT-4o）通过结构化查询评估语义正确性、视觉质量和伪影存在，并设计TWS评分（结合语义相似性、结构完整性和伪影敏感性）验证选择。

Result: 实验表明TWS与人类偏好高度相关，且VLM引导的选择能持续获得高TWS值，优于传统指标如PSNR和LPIPS。

Conclusion: 该方法为扩散SR空间的不确定性提供了可扩展、通用的解决方案，为生成SR的可信度设定了新基准。

Abstract: Super-resolution (SR) is an ill-posed inverse problem with many feasible solutions consistent with a given low-resolution image. On one hand, regressive SR models aim to balance fidelity and perceptual quality to yield a single solution, but this trade-off often introduces artifacts that create ambiguity in information-critical applications such as recognizing digits or letters. On the other hand, diffusion models generate a diverse set of SR images, but selecting the most trustworthy solution from this set remains a challenge. This paper introduces a robust, automated framework for identifying the most trustworthy SR sample from a diffusion-generated set by leveraging the semantic reasoning capabilities of vision-language models (VLMs). Specifically, VLMs such as BLIP-2, GPT-4o, and their variants are prompted with structured queries to assess semantic correctness, visual quality, and artifact presence. The top-ranked SR candidates are then ensembled to yield a single trustworthy output in a cost-effective manner. To rigorously assess the validity of VLM-selected samples, we propose a novel Trustworthiness Score (TWS) a hybrid metric that quantifies SR reliability based on three complementary components: semantic similarity via CLIP embeddings, structural integrity using SSIM on edge maps, and artifact sensitivity through multi-level wavelet decomposition. We empirically show that TWS correlates strongly with human preference in both ambiguous and natural images, and that VLM-guided selections consistently yield high TWS values. Compared to conventional metrics like PSNR, LPIPS, which fail to reflect information fidelity, our approach offers a principled, scalable, and generalizable solution for navigating the uncertainty of the diffusion SR space. By aligning outputs with human expectations and semantic correctness, this work sets a new benchmark for trustworthiness in generative SR.

</details>


### [6] [MultiHuman-Testbench: Benchmarking Image Generation for Multiple Humans](https://arxiv.org/abs/2506.20879)
*Shubhankar Borse,Seokeon Choi,Sunghyun Park,Jeongho Kim,Shreya Kadambi,Risheek Garrepalli,Sungrack Yun,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: 论文提出了MultiHuman-Testbench基准，用于评估多人生成模型，包含1800个样本和5550张人脸图像，提出多维度评估指标，并改进了ID相似性技术。


<details>
  <summary>Details</summary>
Motivation: 多人生成图像时保持面部身份是一个挑战，缺乏专用基准。

Method: 引入MultiHuman-Testbench基准，包含多样化的文本提示和人脸图像，提出多维度评估指标，并改进ID相似性技术。

Result: 基准和评估方法为多人生成图像研究提供了标准化工具和重要见解。

Conclusion: MultiHuman-Testbench为多人生成图像研究提供了有价值的基准和技术改进。

Abstract: Generation of images containing multiple humans, performing complex actions, while preserving their facial identities, is a significant challenge. A major factor contributing to this is the lack of a a dedicated benchmark. To address this, we introduce MultiHuman-Testbench, a novel benchmark for rigorously evaluating generative models for multi-human generation. The benchmark comprises 1800 samples, including carefully curated text prompts, describing a range of simple to complex human actions. These prompts are matched with a total of 5,550 unique human face images, sampled uniformly to ensure diversity across age, ethnic background, and gender. Alongside captions, we provide human-selected pose conditioning images which accurately match the prompt. We propose a multi-faceted evaluation suite employing four key metrics to quantify face count, ID similarity, prompt alignment, and action detection. We conduct a thorough evaluation of a diverse set of models, including zero-shot approaches and training-based methods, with and without regional priors. We also propose novel techniques to incorporate image and region isolation using human segmentation and Hungarian matching, significantly improving ID similarity. Our proposed benchmark and key findings provide valuable insights and a standardized tool for advancing research in multi-human image generation.

</details>


### [7] [M2SFormer: Multi-Spectral and Multi-Scale Attention with Edge-Aware Difficulty Guidance for Image Forgery Localization](https://arxiv.org/abs/2506.20922)
*Ju-Hyeon Nam,Dong-Hyun Moon,Sang-Chul Lee*

Main category: cs.CV

TL;DR: M2SFormer是一种基于Transformer的框架，通过统一多频率和多尺度注意力机制，结合全局先验图，显著提升了图像篡改定位的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法在像素级篡改定位中存在计算开销大和表征能力有限的问题，尤其是对复杂或细微篡改的处理效果不佳。

Method: M2SFormer在跳跃连接中统一了多频率和多尺度注意力机制，并利用全局先验图指导难度感知注意力模块，以保留细微篡改特征。

Result: 在多个基准数据集上的实验表明，M2SFormer在检测和定位篡改方面优于现有最先进模型，具有更强的跨领域泛化能力。

Conclusion: M2SFormer通过创新的注意力机制和全局先验图，显著提升了篡改定位的性能，为图像编辑检测提供了更有效的解决方案。

Abstract: Image editing techniques have rapidly advanced, facilitating both innovative use cases and malicious manipulation of digital images. Deep learning-based methods have recently achieved high accuracy in pixel-level forgery localization, yet they frequently struggle with computational overhead and limited representation power, particularly for subtle or complex tampering. In this paper, we propose M2SFormer, a novel Transformer encoder-based framework designed to overcome these challenges. Unlike approaches that process spatial and frequency cues separately, M2SFormer unifies multi-frequency and multi-scale attentions in the skip connection, harnessing global context to better capture diverse forgery artifacts. Additionally, our framework addresses the loss of fine detail during upsampling by utilizing a global prior map, a curvature metric indicating the difficulty of forgery localization, which then guides a difficulty-guided attention module to preserve subtle manipulations more effectively. Extensive experiments on multiple benchmark datasets demonstrate that M2SFormer outperforms existing state-of-the-art models, offering superior generalization in detecting and localizing forgeries across unseen domains.

</details>


### [8] [DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing](https://arxiv.org/abs/2506.20967)
*Lingling Cai,Kang Zhao,Hangjie Yuan,Xiang Wang,Yingya Zhang,Kejie Huang*

Main category: cs.CV

TL;DR: DFVEdit是一种高效的零样本视频编辑方法，专为视频扩散变换器（Video DiTs）设计，避免了计算开销大的注意力修改或微调。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法直接应用于Video DiTs时计算开销大，需要改进。

Method: 通过流变换直接操作干净潜在空间，提出条件增量流向量（CDFV）和集成隐式交叉注意力（ICA）指导与嵌入强化（ER）。

Result: DFVEdit在推理速度上提升20倍，内存减少85%，并在多个指标上达到最先进性能。

Conclusion: DFVEdit为Video DiTs提供了一种高效、高质量的零样本视频编辑解决方案。

Abstract: The advent of Video Diffusion Transformers (Video DiTs) marks a milestone in video generation. However, directly applying existing video editing methods to Video DiTs often incurs substantial computational overhead, due to resource-intensive attention modification or finetuning. To alleviate this problem, we present DFVEdit, an efficient zero-shot video editing method tailored for Video DiTs. DFVEdit eliminates the need for both attention modification and fine-tuning by directly operating on clean latents via flow transformation. To be more specific, we observe that editing and sampling can be unified under the continuous flow perspective. Building upon this foundation, we propose the Conditional Delta Flow Vector (CDFV) -- a theoretically unbiased estimation of DFV -- and integrate Implicit Cross Attention (ICA) guidance as well as Embedding Reinforcement (ER) to further enhance editing quality. DFVEdit excels in practical efficiency, offering at least 20x inference speed-up and 85\% memory reduction on Video DiTs compared to attention-engineering-based editing methods. Extensive quantitative and qualitative experiments demonstrate that DFVEdit can be seamlessly applied to popular Video DiTs (e.g., CogVideoX and Wan2.1), attaining state-of-the-art performance on structural fidelity, spatial-temporal consistency, and editing quality.

</details>


### [9] [From Cradle to Cane: A Two-Pass Framework for High-Fidelity Lifespan Face Aging](https://arxiv.org/abs/2506.20977)
*Tao Liu,Dafeng Zhang,Gengchen Li,Shizhuo Liu,Yongqi Song,Senmao Li,Shiqi Yang,Boqian Li,Kai Wang,Yaxing Wang*

Main category: cs.CV

TL;DR: 提出了一种名为Cradle2Cane的双阶段人脸老化框架，通过自适应噪声注入和身份感知嵌入解决了年龄准确性与身份保持的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在实现真实且无缝的人脸老化时，难以平衡年龄准确性与身份保持，尤其是在大年龄跨度或极端头部姿态的情况下。

Method: 采用基于文本到图像扩散模型的双阶段框架：第一阶段通过自适应噪声注入（AdaNI）解决年龄准确性；第二阶段通过身份感知嵌入（IDEmb）增强身份保持。

Result: 在CelebA-HQ测试数据集上，Cradle2Cane在年龄准确性和身份一致性方面优于现有方法。

Conclusion: Cradle2Cane通过双阶段设计有效解决了年龄与身份的权衡问题，为人脸老化任务提供了更优的解决方案。

Abstract: Face aging has become a crucial task in computer vision, with applications ranging from entertainment to healthcare. However, existing methods struggle with achieving a realistic and seamless transformation across the entire lifespan, especially when handling large age gaps or extreme head poses. The core challenge lies in balancing age accuracy and identity preservation--what we refer to as the Age-ID trade-off. Most prior methods either prioritize age transformation at the expense of identity consistency or vice versa. In this work, we address this issue by proposing a two-pass face aging framework, named Cradle2Cane, based on few-step text-to-image (T2I) diffusion models. The first pass focuses on solving age accuracy by introducing an adaptive noise injection (AdaNI) mechanism. This mechanism is guided by including prompt descriptions of age and gender for the given person as the textual condition. Also, by adjusting the noise level, we can control the strength of aging while allowing more flexibility in transforming the face. However, identity preservation is weakly ensured here to facilitate stronger age transformations. In the second pass, we enhance identity preservation while maintaining age-specific features by conditioning the model on two identity-aware embeddings (IDEmb): SVR-ArcFace and Rotate-CLIP. This pass allows for denoising the transformed image from the first pass, ensuring stronger identity preservation without compromising the aging accuracy. Both passes are jointly trained in an end-to-end way. Extensive experiments on the CelebA-HQ test dataset, evaluated through Face++ and Qwen-VL protocols, show that our Cradle2Cane outperforms existing face aging methods in age accuracy and identity consistency.

</details>


### [10] [Rethink Sparse Signals for Pose-guided Text-to-image Generation](https://arxiv.org/abs/2506.20983)
*Wenjie Xuan,Jing Zhang,Juhua Liu,Bo Du,Dacheng Tao*

Main category: cs.CV

TL;DR: 论文提出了一种基于稀疏信号（如OpenPose）的新型空间姿态控制网络（SP-Ctrl），用于姿态引导的图像生成，解决了密集信号（如深度、DensePose）带来的编辑困难和与文本提示不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 密集信号在姿态引导的图像生成中虽然提供了详细的空间指导，但带来了编辑困难和与文本提示不一致的挑战。稀疏信号因其简单性和形状无关性被重新探索。

Method: 扩展OpenPose为可学习的空间表示，引入关键点概念学习，使关键点嵌入更具区分性和表达力，并提高姿态对齐。

Result: 在人类和动物图像生成任务中，SP-Ctrl在稀疏姿态引导下优于现有方法，甚至与密集信号方法性能相当，并展示了跨物种生成的潜力。

Conclusion: 稀疏信号在姿态引导的图像生成中具有潜力，SP-Ctrl通过改进的稀疏信号控制能力，实现了高效且一致的生成。

Abstract: Recent works favored dense signals (e.g., depth, DensePose), as an alternative to sparse signals (e.g., OpenPose), to provide detailed spatial guidance for pose-guided text-to-image generation. However, dense representations raised new challenges, including editing difficulties and potential inconsistencies with textual prompts. This fact motivates us to revisit sparse signals for pose guidance, owing to their simplicity and shape-agnostic nature, which remains underexplored. This paper proposes a novel Spatial-Pose ControlNet(SP-Ctrl), equipping sparse signals with robust controllability for pose-guided image generation. Specifically, we extend OpenPose to a learnable spatial representation, making keypoint embeddings discriminative and expressive. Additionally, we introduce keypoint concept learning, which encourages keypoint tokens to attend to the spatial positions of each keypoint, thus improving pose alignment. Experiments on animal- and human-centric image generation tasks demonstrate that our method outperforms recent spatially controllable T2I generation approaches under sparse-pose guidance and even matches the performance of dense signal-based methods. Moreover, SP-Ctrl shows promising capabilities in diverse and cross-species generation through sparse signals. Codes will be available at https://github.com/DREAMXFAR/SP-Ctrl.

</details>


### [11] [DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting](https://arxiv.org/abs/2506.20998)
*Yeon-Ji Song,Jaein Kim,Byung-Ju Kim,Byoung-Tak Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为DBMovi-GS的方法，用于从模糊单目视频中合成动态场景的新视角，解决了现有方法在动态和模糊场景中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有新视角合成方法依赖高分辨率图像或静态几何假设，无法有效处理动态和模糊场景。

Method: 通过稀疏控制的高斯泼溅技术生成密集3D高斯，从模糊视频中恢复清晰度并重建动态场景的3D几何。

Result: 模型在动态模糊场景中实现了鲁棒的新视角合成性能，并设定了新的基准。

Conclusion: DBMovi-GS为模糊单目视频输入提供了更真实的动态新视角合成解决方案。

Abstract: Novel view synthesis is a task of generating scenes from unseen perspectives; however, synthesizing dynamic scenes from blurry monocular videos remains an unresolved challenge that has yet to be effectively addressed. Existing novel view synthesis methods are often constrained by their reliance on high-resolution images or strong assumptions about static geometry and rigid scene priors. Consequently, their approaches lack robustness in real-world environments with dynamic object and camera motion, leading to instability and degraded visual fidelity. To address this, we propose Motion-aware Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting (DBMovi-GS), a method designed for dynamic view synthesis from blurry monocular videos. Our model generates dense 3D Gaussians, restoring sharpness from blurry videos and reconstructing detailed 3D geometry of the scene affected by dynamic motion variations. Our model achieves robust performance in novel view synthesis under dynamic blurry scenes and sets a new benchmark in realistic novel view synthesis for blurry monocular video inputs.

</details>


### [12] [Detection of Breast Cancer Lumpectomy Margin with SAM-incorporated Forward-Forward Contrastive Learning](https://arxiv.org/abs/2506.21006)
*Tyler Ward,Xiaoqin Wang,Braxton McFarland,Md Atik Ahamed,Sahar Nozad,Talal Arshad,Hafsa Nebbache,Jin Chen,Abdullah Imran*

Main category: cs.CV

TL;DR: 提出了一种结合Segment Anything Model (SAM)和Forward-Forward Contrastive Learning (FFCL)的深度学习框架，用于提高乳腺癌术中标本边缘评估的准确性和速度。


<details>
  <summary>Details</summary>
Motivation: 传统的2D标本放射成像（SR）在评估术中标本边缘状态时准确性有限，导致近四分之一的患者需要额外手术。

Method: 结合FFCL预训练策略和SAM模型，通过标注SR图像中的恶性、非恶性组织和病理确认的边缘区域，预训练ResNet-18骨干网络，并利用SAM进行精细边缘分割。

Result: AUC达到0.8455，Dice相似度比基线模型提高27.4%，推理时间缩短至47毫秒/图像。

Conclusion: FFCL-SAM显著提高了术中边缘评估的速度和准确性，有望降低乳腺癌治疗中的再切除率。

Abstract: Complete removal of cancer tumors with a negative specimen margin during lumpectomy is essential in reducing breast cancer recurrence. However, 2D specimen radiography (SR), the current method used to assess intraoperative specimen margin status, has limited accuracy, resulting in nearly a quarter of patients requiring additional surgery. To address this, we propose a novel deep learning framework combining the Segment Anything Model (SAM) with Forward-Forward Contrastive Learning (FFCL), a pre-training strategy leveraging both local and global contrastive learning for patch-level classification of SR images. After annotating SR images with regions of known maligancy, non-malignant tissue, and pathology-confirmed margins, we pre-train a ResNet-18 backbone with FFCL to classify margin status, then reconstruct coarse binary masks to prompt SAM for refined tumor margin segmentation. Our approach achieved an AUC of 0.8455 for margin classification and segmented margins with a 27.4% improvement in Dice similarity over baseline models, while reducing inference time to 47 milliseconds per image. These results demonstrate that FFCL-SAM significantly enhances both the speed and accuracy of intraoperative margin assessment, with strong potential to reduce re-excision rates and improve surgical outcomes in breast cancer treatment. Our code is available at https://github.com/tbwa233/FFCL-SAM/.

</details>


### [13] [User-in-the-Loop View Sampling with Error Peaking Visualization](https://arxiv.org/abs/2506.21009)
*Ayaka Yasunaga,Hideo Saito,Shohei Mori*

Main category: cs.CV

TL;DR: 提出了一种基于局部重建光场和可视化误差的方法，用于增强现实中的新视角合成，减少用户负担并扩展场景探索范围。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要用户通过AR显示对齐3D标注，任务繁重且场景受限，因此提出更自由、高效的方法。

Method: 利用局部重建光场和可视化误差，指导用户插入新视角以消除误差。

Result: 误差峰值可视化侵入性更低，减少用户失望，且所需视角样本更少，适用于大场景重建。

Conclusion: 该方法在移动视角合成系统中表现优异，并能支持更大场景的辐射场重建。

Abstract: Augmented reality (AR) provides ways to visualize missing view samples for novel view synthesis. Existing approaches present 3D annotations for new view samples and task users with taking images by aligning the AR display. This data collection task is known to be mentally demanding and limits capture areas to pre-defined small areas due to the ideal but restrictive underlying sampling theory. To free users from 3D annotations and limited scene exploration, we propose using locally reconstructed light fields and visualizing errors to be removed by inserting new views. Our results show that the error-peaking visualization is less invasive, reduces disappointment in final results, and is satisfactory with fewer view samples in our mobile view synthesis system. We also show that our approach can contribute to recent radiance field reconstruction for larger scenes, such as 3D Gaussian splatting.

</details>


### [14] [HybridQ: Hybrid Classical-Quantum Generative Adversarial Network for Skin Disease Image Generation](https://arxiv.org/abs/2506.21015)
*Qingyue Jiao,Kangyu Zheng,Yiyu Shi,Zhiding Liang*

Main category: cs.CV

TL;DR: 提出了一种经典-量子混合生成对抗网络（GAN），能够生成彩色医学图像，解决了量子图像生成的质量问题，并在性能和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 皮肤疾病数据集存在类别不平衡、隐私问题和对象偏差等问题，数据增强是关键。传统生成模型计算资源需求高，而现有量子方法只能生成低质量灰度图像。

Method: 采用经典-量子潜在空间融合技术，开发了一种混合经典-量子GAN，能够生成高质量彩色医学图像。

Result: 模型在图像生成质量和分类性能提升上优于传统深度卷积GAN和现有混合量子GAN，且参数和训练周期大幅减少。

Conclusion: 该研究展示了量子图像生成的潜力，随着量子硬件的发展，未来前景广阔。

Abstract: Machine learning-assisted diagnosis is gaining traction in skin disease detection, but training effective models requires large amounts of high-quality data. Skin disease datasets often suffer from class imbalance, privacy concerns, and object bias, making data augmentation essential. While classical generative models are widely used, they demand extensive computational resources and lengthy training time. Quantum computing offers a promising alternative, but existing quantum-based image generation methods can only yield grayscale low-quality images. Through a novel classical-quantum latent space fusion technique, our work overcomes this limitation and introduces the first classical-quantum generative adversarial network (GAN) capable of generating color medical images. Our model outperforms classical deep convolutional GANs and existing hybrid classical-quantum GANs in both image generation quality and classification performance boost when used as data augmentation. Moreover, the performance boost is comparable with that achieved using state-of-the-art classical generative models, yet with over 25 times fewer parameters and 10 times fewer training epochs. Such results suggest a promising future for quantum image generation as quantum hardware advances. Finally, we demonstrate the robust performance of our model on real IBM quantum machine with hardware noise.

</details>


### [15] [Instella-T2I: Pushing the Limits of 1D Discrete Latent Space Image Generation](https://arxiv.org/abs/2506.21022)
*Ze Wang,Hao Chen,Benran Hu,Jiang Liu,Ximeng Sun,Jialian Wu,Yusheng Su,Xiaodong Yu,Emad Barsoum,Zicheng Liu*

Main category: cs.CV

TL;DR: 提出了一种基于1D二进制潜在空间的图像标记化方法，显著减少了高分辨率图像建模所需的标记数量，同时保持细节和效率。


<details>
  <summary>Details</summary>
Motivation: 传统2D网格结构的图像标记化方法计算量大，而1D潜在空间虽减少了标记数量，但仍需进一步优化。本文旨在通过1D二进制潜在空间实现更紧凑的图像表示。

Method: 采用1D二进制向量序列表示图像，替代传统的一热编码标记，结合简单模型架构，显著提升训练和推理速度。

Result: 在1024x1024分辨率图像上仅需128个标记，比标准VQ-VAEs减少32倍标记数量，且性能与主流图像生成模型相当。

Conclusion: 1D二进制潜在空间提供了一种高效、可扩展的图像标记化方法，适用于大规模训练和推理。

Abstract: Image tokenization plays a critical role in reducing the computational demands of modeling high-resolution images, significantly improving the efficiency of image and multimodal understanding and generation. Recent advances in 1D latent spaces have reduced the number of tokens required by eliminating the need for a 2D grid structure. In this paper, we further advance compact discrete image representation by introducing 1D binary image latents. By representing each image as a sequence of binary vectors, rather than using traditional one-hot codebook tokens, our approach preserves high-resolution details while maintaining the compactness of 1D latents. To the best of our knowledge, our text-to-image models are the first to achieve competitive performance in both diffusion and auto-regressive generation using just 128 discrete tokens for images up to 1024x1024, demonstrating up to a 32-fold reduction in token numbers compared to standard VQ-VAEs. The proposed 1D binary latent space, coupled with simple model architectures, achieves marked improvements in speed training and inference speed. Our text-to-image models allow for a global batch size of 4096 on a single GPU node with 8 AMD MI300X GPUs, and the training can be completed within 200 GPU days. Our models achieve competitive performance compared to modern image generation models without any in-house private training data or post-training refinements, offering a scalable and efficient alternative to conventional tokenization methods.

</details>


### [16] [Improving Diffusion-Based Image Editing Faithfulness via Guidance and Scheduling](https://arxiv.org/abs/2506.21045)
*Hansam Cho,Seoung Bum Kim*

Main category: cs.CV

TL;DR: 提出了一种名为FGS的方法，通过引入忠实度指导和调度策略，在保持可编辑性的同时提升图像编辑的忠实度。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导扩散模型在图像编辑中存在可编辑性与忠实度之间的权衡问题，难以同时优化两者。

Method: FGS结合了忠实度指导以增强输入图像信息的保留，并采用调度策略解决可编辑性与忠实度的不匹配问题。

Result: 实验表明，FGS在保持可编辑性的同时显著提升了忠实度，且兼容多种编辑方法。

Conclusion: FGS为图像编辑提供了一种高效平衡可编辑性与忠实度的解决方案。

Abstract: Text-guided diffusion models have become essential for high-quality image synthesis, enabling dynamic image editing. In image editing, two crucial aspects are editability, which determines the extent of modification, and faithfulness, which reflects how well unaltered elements are preserved. However, achieving optimal results is challenging because of the inherent trade-off between editability and faithfulness. To address this, we propose Faithfulness Guidance and Scheduling (FGS), which enhances faithfulness with minimal impact on editability. FGS incorporates faithfulness guidance to strengthen the preservation of input image information and introduces a scheduling strategy to resolve misalignment between editability and faithfulness. Experimental results demonstrate that FGS achieves superior faithfulness while maintaining editability. Moreover, its compatibility with various editing methods enables precise, high-quality image edits across diverse tasks.

</details>


### [17] [ESMStereo: Enhanced ShuffleMixer Disparity Upsampling for Real-Time and Accurate Stereo Matching](https://arxiv.org/abs/2506.21091)
*Mahmoud Tahmasebi,Saif Huq,Kevin Meehan,Marion McAfee*

Main category: cs.CV

TL;DR: 论文提出了一种增强型混洗混合器（ESM），用于解决基于小规模成本体积的立体匹配中信息丢失的问题，实现了高精度和实时性能。


<details>
  <summary>Details</summary>
Motivation: 立体匹配在自动驾驶系统中至关重要，但现有深度学习方法难以同时实现高精度和实时性能。小规模成本体积虽能提升速度，但信息不足导致精度下降。

Method: 提出ESM方法，通过将初级特征整合到视差上采样单元中，快速提取特征并与图像特征融合，通过混洗和分层分割混合特征，再通过紧凑的特征引导沙漏网络细化。

Result: ESM在高端GPU上达到116 FPS，在AGX Orin上达到91 FPS，同时保持了高精度的视差图重建。

Conclusion: ESM通过局部上下文连接和大感受野，以低计算成本实现了高精度实时立体匹配，为自动驾驶系统提供了有效解决方案。

Abstract: Stereo matching has become an increasingly important component of modern autonomous systems. Developing deep learning-based stereo matching models that deliver high accuracy while operating in real-time continues to be a major challenge in computer vision. In the domain of cost-volume-based stereo matching, accurate disparity estimation depends heavily on large-scale cost volumes. However, such large volumes store substantial redundant information and also require computationally intensive aggregation units for processing and regression, making real-time performance unattainable. Conversely, small-scale cost volumes followed by lightweight aggregation units provide a promising route for real-time performance, but lack sufficient information to ensure highly accurate disparity estimation. To address this challenge, we propose the Enhanced Shuffle Mixer (ESM) to mitigate information loss associated with small-scale cost volumes. ESM restores critical details by integrating primary features into the disparity upsampling unit. It quickly extracts features from the initial disparity estimation and fuses them with image features. These features are mixed by shuffling and layer splitting then refined through a compact feature-guided hourglass network to recover more detailed scene geometry. The ESM focuses on local contextual connectivity with a large receptive field and low computational cost, leading to the reconstruction of a highly accurate disparity map at real-time. The compact version of ESMStereo achieves an inference speed of 116 FPS on high-end GPUs and 91 FPS on the AGX Orin.

</details>


### [18] [CL-Splats: Continual Learning of Gaussian Splatting with Local Optimization](https://arxiv.org/abs/2506.21117)
*Jan Ackermann,Jonas Kulhanek,Shengqu Cai,Haofei Xu,Marc Pollefeys,Gordon Wetzstein,Leonidas Guibas,Songyou Peng*

Main category: cs.CV

TL;DR: CL-Splats是一种动态3D场景表示更新方法，通过增量更新高斯点云表示，结合变化检测模块，实现高效局部优化，提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 在动态3D环境中，实时更新场景表示对机器人、混合现实和具身AI至关重要。传统方法需重新优化整个场景，计算开销大。

Method: 提出CL-Splats，通过稀疏场景捕获增量更新高斯点云表示，集成变化检测模块，区分动态与静态部分，实现局部优化。

Result: 实验表明，CL-Splats在高效更新的同时，重建质量优于现有方法。

Conclusion: CL-Splats为未来实时3D场景重建任务提供了坚实基础。

Abstract: In dynamic 3D environments, accurately updating scene representations over time is crucial for applications in robotics, mixed reality, and embodied AI. As scenes evolve, efficient methods to incorporate changes are needed to maintain up-to-date, high-quality reconstructions without the computational overhead of re-optimizing the entire scene. This paper introduces CL-Splats, which incrementally updates Gaussian splatting-based 3D representations from sparse scene captures. CL-Splats integrates a robust change-detection module that segments updated and static components within the scene, enabling focused, local optimization that avoids unnecessary re-computation. Moreover, CL-Splats supports storing and recovering previous scene states, facilitating temporal segmentation and new scene-analysis applications. Our extensive experiments demonstrate that CL-Splats achieves efficient updates with improved reconstruction quality over the state-of-the-art. This establishes a robust foundation for future real-time adaptation in 3D scene reconstruction tasks.

</details>


### [19] [Learning to See in the Extremely Dark](https://arxiv.org/abs/2506.21132)
*Hai Jiang,Binhao Guan,Zhen Liu,Xiaohong Liu,Jian Yu,Zheng Liu,Songchen Han,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 论文提出了一种生成极低光RAW图像的数据合成流程和扩散模型框架，用于极暗场景下的图像增强，并发布了SIED数据集。


<details>
  <summary>Details</summary>
Motivation: 现有学习型方法在极低光（0.0001 lux）场景下的RAW图像增强能力尚未探索，缺乏对应数据集。

Method: 提出了数据合成流程生成极低光RAW图像及sRGB参考，并设计了扩散模型框架，包含自适应光照校正模块和颜色一致性损失。

Result: 在SIED数据集和公开基准测试中验证了方法的有效性。

Conclusion: 论文填补了极低光RAW图像增强的数据集空白，提出的方法在极低信噪比条件下表现优异。

Abstract: Learning-based methods have made promising advances in low-light RAW image enhancement, while their capability to extremely dark scenes where the environmental illuminance drops as low as 0.0001 lux remains to be explored due to the lack of corresponding datasets. To this end, we propose a paired-to-paired data synthesis pipeline capable of generating well-calibrated extremely low-light RAW images at three precise illuminance ranges of 0.01-0.1 lux, 0.001-0.01 lux, and 0.0001-0.001 lux, together with high-quality sRGB references to comprise a large-scale paired dataset named See-in-the-Extremely-Dark (SIED) to benchmark low-light RAW image enhancement approaches. Furthermore, we propose a diffusion-based framework that leverages the generative ability and intrinsic denoising property of diffusion models to restore visually pleasing results from extremely low-SNR RAW inputs, in which an Adaptive Illumination Correction Module (AICM) and a color consistency loss are introduced to ensure accurate exposure correction and color restoration. Extensive experiments on the proposed SIED and publicly available benchmarks demonstrate the effectiveness of our method. The code and dataset are available at https://github.com/JianghaiSCU/SIED.

</details>


### [20] [Geometry and Perception Guided Gaussians for Multiview-consistent 3D Generation from a Single Image](https://arxiv.org/abs/2506.21152)
*Pufan Li,Bi'an Du,Wei Hu*

Main category: cs.CV

TL;DR: 提出一种新方法，通过整合几何和感知先验，无需额外训练即可从单张图像重建高保真3D物体。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多视角一致性和几何细节上表现不佳，需要改进。

Method: 训练三个高斯分支（几何先验、感知先验和高斯噪声），通过交互和重投影策略优化3D高斯分支。

Result: 实验显示方法在视角合成和3D重建上优于现有方法，生成结果更一致和详细。

Conclusion: 该方法有效解决了多视角一致性和几何细节问题，实现了高质量的3D物体生成。

Abstract: Generating realistic 3D objects from single-view images requires natural appearance, 3D consistency, and the ability to capture multiple plausible interpretations of unseen regions. Existing approaches often rely on fine-tuning pretrained 2D diffusion models or directly generating 3D information through fast network inference or 3D Gaussian Splatting, but their results generally suffer from poor multiview consistency and lack geometric detail. To takle these issues, we present a novel method that seamlessly integrates geometry and perception priors without requiring additional model training to reconstruct detailed 3D objects from a single image. Specifically, we train three different Gaussian branches initialized from the geometry prior, perception prior and Gaussian noise, respectively. The geometry prior captures the rough 3D shapes, while the perception prior utilizes the 2D pretrained diffusion model to enhance multiview information. Subsequently, we refine 3D Gaussian branches through mutual interaction between geometry and perception priors, further enhanced by a reprojection-based strategy that enforces depth consistency. Experiments demonstrate the higher-fidelity reconstruction results of our method, outperforming existing methods on novel view synthesis and 3D reconstruction, demonstrating robust and consistent 3D object generation.

</details>


### [21] [BitMark for Infinity: Watermarking Bitwise Autoregressive Image Generative Models](https://arxiv.org/abs/2506.21209)
*Louis Kerner,Michel Meintz,Bihe Zhao,Franziska Boenisch,Adam Dziedzic*

Main category: cs.CV

TL;DR: 论文提出BitMark，一种针对Infinity文本到图像模型的鲁棒位级水印框架，旨在防止模型崩溃。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型输出在互联网上的广泛传播，这些内容可能被重新用作训练数据，导致模型性能逐渐退化（模型崩溃）。水印技术可以识别生成内容，从而缓解这一问题。

Method: BitMark通过在Infinity图像生成过程中，在多个尺度（分辨率）的令牌流中嵌入位级水印，保持视觉保真度和生成速度，同时抵抗去除技术。

Result: BitMark具有高放射性，即使用水印生成图像训练其他模型时，新模型的输出也会携带水印，且水印在微调后仍可检测。

Conclusion: BitMark为图像生成模型提供了一种防止模型崩溃的可靠方法，通过检测生成内容实现。

Abstract: State-of-the-art text-to-image models like Infinity generate photorealistic images at an unprecedented speed. These models operate in a bitwise autoregressive manner over a discrete set of tokens that is practically infinite in size. However, their impressive generative power comes with a growing risk: as their outputs increasingly populate the Internet, they are likely to be scraped and reused as training data-potentially by the very same models. This phenomenon has been shown to lead to model collapse, where repeated training on generated content, especially from the models' own previous versions, causes a gradual degradation in performance. A promising mitigation strategy is watermarking, which embeds human-imperceptible yet detectable signals into generated images-enabling the identification of generated content. In this work, we introduce BitMark, a robust bitwise watermarking framework for Infinity. Our method embeds a watermark directly at the bit level of the token stream across multiple scales (also referred to as resolutions) during Infinity's image generation process. Our bitwise watermark subtly influences the bits to preserve visual fidelity and generation speed while remaining robust against a spectrum of removal techniques. Furthermore, it exhibits high radioactivity, i.e., when watermarked generated images are used to train another image generative model, this second model's outputs will also carry the watermark. The radioactive traces remain detectable even when only fine-tuning diffusion or image autoregressive models on images watermarked with our BitMark. Overall, our approach provides a principled step toward preventing model collapse in image generative models by enabling reliable detection of generated outputs.

</details>


### [22] [Video Virtual Try-on with Conditional Diffusion Transformer Inpainter](https://arxiv.org/abs/2506.21270)
*Cheng Zou,Senlin Cheng,Bolei Xu,Dandan Zheng,Xiaobo Li,Jingdong Chen,Ming Yang*

Main category: cs.CV

TL;DR: ViTI将视频虚拟试穿任务重新定义为条件视频修复任务，通过基于Diffusion Transformer的3D时空注意力框架和渐进式训练策略，显著提升了时空一致性和细节保留。


<details>
  <summary>Details</summary>
Motivation: 现有视频虚拟试穿方法存在时空不一致问题，而基于图像的逐帧处理方法效果不佳。ViTI旨在通过视频生成而非图像试穿的思路，从源头解决一致性问题。

Method: 提出ViTI框架，基于Diffusion Transformer构建3D时空注意力模型，采用渐进式训练和多阶段掩码策略，逐步适应视频服装修复任务。

Result: 实验表明，ViTI在时空一致性和细节保留上优于现有方法。

Conclusion: ViTI通过重新定义任务为视频修复，结合3D时空注意力和渐进式训练，显著提升了视频虚拟试穿的效果。

Abstract: Video virtual try-on aims to naturally fit a garment to a target person in consecutive video frames. It is a challenging task, on the one hand, the output video should be in good spatial-temporal consistency, on the other hand, the details of the given garment need to be preserved well in all the frames. Naively using image-based try-on methods frame by frame can get poor results due to severe inconsistency. Recent diffusion-based video try-on methods, though very few, happen to coincide with a similar solution: inserting temporal attention into image-based try-on model to adapt it for video try-on task, which have shown improvements but there still exist inconsistency problems. In this paper, we propose ViTI (Video Try-on Inpainter), formulate and implement video virtual try-on as a conditional video inpainting task, which is different from previous methods. In this way, we start with a video generation problem instead of an image-based try-on problem, which from the beginning has a better spatial-temporal consistency. Specifically, at first we build a video inpainting framework based on Diffusion Transformer with full 3D spatial-temporal attention, and then we progressively adapt it for video garment inpainting, with a collection of masking strategies and multi-stage training. After these steps, the model can inpaint the masked garment area with appropriate garment pixels according to the prompt with good spatial-temporal consistency. Finally, as other try-on methods, garment condition is added to the model to make sure the inpainted garment appearance and details are as expected. Both quantitative and qualitative experimental results show that ViTI is superior to previous works.

</details>
