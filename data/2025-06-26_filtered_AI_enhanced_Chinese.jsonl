{"id": "2506.20202", "pdf": "https://arxiv.org/pdf/2506.20202", "abs": "https://arxiv.org/abs/2506.20202", "authors": ["Da Li", "Donggang Jia", "Yousef Rajeh", "Dominik Engel", "Ivan Viola"], "title": "RaRa Clipper: A Clipper for Gaussian Splatting Based on Ray Tracer and Rasterizer", "categories": ["cs.GR"], "comment": null, "summary": "With the advancement of Gaussian Splatting techniques, a growing number of datasets based on this representation have been developed. However, performing accurate and efficient clipping for Gaussian Splatting remains a challenging and unresolved problem, primarily due to the volumetric nature of Gaussian primitives, which makes hard clipping incapable of precisely localizing their pixel-level contributions. In this paper, we propose a hybrid rendering framework that combines rasterization and ray tracing to achieve efficient and high-fidelity clipping of Gaussian Splatting data. At the core of our method is the RaRa strategy, which first leverages rasterization to quickly identify Gaussians intersected by the clipping plane, followed by ray tracing to compute attenuation weights based on their partial occlusion. These weights are then used to accurately estimate each Gaussian's contribution to the final image, enabling smooth and continuous clipping effects. We validate our approach on diverse datasets, including general Gaussians, hair strand Gaussians, and multi-layer Gaussians, and conduct user studies to evaluate both perceptual quality and quantitative performance. Experimental results demonstrate that our method delivers visually superior results while maintaining real-time rendering performance and preserving high fidelity in the unclipped regions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5149\u6805\u5316\u548c\u5149\u7ebf\u8ffd\u8e2a\u7684\u6df7\u5408\u6e32\u67d3\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u4e14\u9ad8\u4fdd\u771f\u5730\u88c1\u526a\u9ad8\u65af\u6cfc\u6e85\u6570\u636e\u3002", "motivation": "\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u7684\u8fdb\u6b65\u5e26\u6765\u4e86\u5927\u91cf\u57fa\u4e8e\u6b64\u8868\u793a\u7684\u6570\u636e\u96c6\uff0c\u4f46\u56e0\u5176\u4f53\u79ef\u7279\u6027\uff0c\u7cbe\u786e\u88c1\u526a\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528RaRa\u7b56\u7565\uff0c\u5148\u7528\u5149\u6805\u5316\u5feb\u901f\u8bc6\u522b\u88ab\u88c1\u526a\u5e73\u9762\u76f8\u4ea4\u7684\u9ad8\u65af\uff0c\u518d\u7528\u5149\u7ebf\u8ffd\u8e2a\u8ba1\u7b97\u90e8\u5206\u906e\u6321\u7684\u8870\u51cf\u6743\u91cd\uff0c\u4ee5\u51c6\u786e\u4f30\u8ba1\u8d21\u732e\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u65b9\u6cd5\u5728\u89c6\u89c9\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6e32\u67d3\u6027\u80fd\u548c\u9ad8\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5e73\u6ed1\u8fde\u7eed\u7684\u88c1\u526a\u6548\u679c\uff0c\u89e3\u51b3\u4e86\u9ad8\u65af\u6cfc\u6e85\u6570\u636e\u7684\u7cbe\u786e\u88c1\u526a\u95ee\u9898\u3002"}}
{"id": "2506.20367", "pdf": "https://arxiv.org/pdf/2506.20367", "abs": "https://arxiv.org/abs/2506.20367", "authors": ["Edoardo Alberto Dominici", "Jozef Hladky", "Floor Verhoeven", "Lukas Radl", "Thomas Deixelberger", "Stefan Ainetter", "Philipp Drescher", "Stefan Hauswiesner", "Arno Coomans", "Giacomo Nazzaro", "Konstantinos Vardis", "Markus Steinberger"], "title": "DreamAnywhere: Object-Centric Panoramic 3D Scene Generation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Recent advances in text-to-3D scene generation have demonstrated significant potential to transform content creation across multiple industries. Although the research community has made impressive progress in addressing the challenges of this complex task, existing methods often generate environments that are only front-facing, lack visual fidelity, exhibit limited scene understanding, and are typically fine-tuned for either indoor or outdoor settings. In this work, we address these issues and propose DreamAnywhere, a modular system for the fast generation and prototyping of 3D scenes. Our system synthesizes a 360{\\deg} panoramic image from text, decomposes it into background and objects, constructs a complete 3D representation through hybrid inpainting, and lifts object masks to detailed 3D objects that are placed in the virtual environment. DreamAnywhere supports immersive navigation and intuitive object-level editing, making it ideal for scene exploration, visual mock-ups, and rapid prototyping -- all with minimal manual modeling. These features make our system particularly suitable for low-budget movie production, enabling quick iteration on scene layout and visual tone without the overhead of traditional 3D workflows. Our modular pipeline is highly customizable as it allows components to be replaced independently. Compared to current state-of-the-art text and image-based 3D scene generation approaches, DreamAnywhere shows significant improvements in coherence in novel view synthesis and achieves competitive image quality, demonstrating its effectiveness across diverse and challenging scenarios. A comprehensive user study demonstrates a clear preference for our method over existing approaches, validating both its technical robustness and practical usefulness.", "AI": {"tldr": "DreamAnywhere\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7cfb\u7edf\uff0c\u7528\u4e8e\u5feb\u901f\u751f\u6210\u548c\u539f\u578b\u53163D\u573a\u666f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u3001\u573a\u666f\u7406\u89e3\u548c\u591a\u73af\u5883\u9002\u5e94\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u52303D\u573a\u666f\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u573a\u666f\u5355\u4e00\u3001\u89c6\u89c9\u4fdd\u771f\u5ea6\u4f4e\u3001\u573a\u666f\u7406\u89e3\u6709\u9650\u4e14\u4ec5\u9002\u7528\u4e8e\u7279\u5b9a\u73af\u5883\u7684\u95ee\u9898\uff0cDreamAnywhere\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u901a\u8fc7\u751f\u6210360\u00b0\u5168\u666f\u56fe\u50cf\uff0c\u5206\u89e3\u4e3a\u80cc\u666f\u548c\u5bf9\u8c61\uff0c\u901a\u8fc7\u6df7\u5408\u4fee\u590d\u6784\u5efa\u5b8c\u65743D\u8868\u793a\uff0c\u5e76\u5c06\u5bf9\u8c61\u63a9\u7801\u63d0\u5347\u4e3a\u8be6\u7ec63D\u5bf9\u8c61\uff0c\u652f\u6301\u6c89\u6d78\u5f0f\u5bfc\u822a\u548c\u5bf9\u8c61\u7ea7\u7f16\u8f91\u3002", "result": "DreamAnywhere\u5728\u65b0\u89c6\u89d2\u5408\u6210\u4e00\u81f4\u6027\u548c\u56fe\u50cf\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7528\u6237\u7814\u7a76\u663e\u793a\u5176\u6280\u672f\u7a33\u5065\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "DreamAnywhere\u4e3a\u4f4e\u6210\u672c\u7535\u5f71\u5236\u4f5c\u7b49\u573a\u666f\u63d0\u4f9b\u4e86\u5feb\u901f\u8fed\u4ee3\u548c\u9ad8\u6548\u539f\u578b\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\u4f7f\u5176\u5177\u6709\u9ad8\u5ea6\u53ef\u5b9a\u5236\u6027\u3002"}}
{"id": "2506.20652", "pdf": "https://arxiv.org/pdf/2506.20652", "abs": "https://arxiv.org/abs/2506.20652", "authors": ["Roi Bar-On", "Dana Cohen-Bar", "Daniel Cohen-Or"], "title": "EditP23: 3D Editing via Propagation of Image Prompts to Multi-View", "categories": ["cs.GR", "cs.CV", "68U05 (Primary), 68T45 (Secondary)", "I.3.7; I.3.8; I.4.9"], "comment": "Code, supplementary videos, interactive 3D visualizations, and   additional results are available at https://editp23.github.io/", "summary": "We present EditP23, a method for mask-free 3D editing that propagates 2D image edits to multi-view representations in a 3D-consistent manner. In contrast to traditional approaches that rely on text-based prompting or explicit spatial masks, EditP23 enables intuitive edits by conditioning on a pair of images: an original view and its user-edited counterpart. These image prompts are used to guide an edit-aware flow in the latent space of a pre-trained multi-view diffusion model, allowing the edit to be coherently propagated across views. Our method operates in a feed-forward manner, without optimization, and preserves the identity of the original object, in both structure and appearance. We demonstrate its effectiveness across a range of object categories and editing scenarios, achieving high fidelity to the source while requiring no manual masks.", "AI": {"tldr": "EditP23\u662f\u4e00\u79cd\u65e0\u9700\u63a9\u7801\u76843D\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc72D\u56fe\u50cf\u7f16\u8f91\u4f20\u64ad\u5230\u591a\u89c6\u89d2\u8868\u793a\uff0c\u5b9e\u73b03D\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6587\u672c\u63d0\u793a\u6216\u663e\u5f0f\u7a7a\u95f4\u63a9\u7801\uff0c\u800cEditP23\u901a\u8fc7\u4e00\u5bf9\u56fe\u50cf\uff08\u539f\u59cb\u89c6\u56fe\u548c\u7528\u6237\u7f16\u8f91\u540e\u7684\u89c6\u56fe\uff09\u5b9e\u73b0\u76f4\u89c2\u7f16\u8f91\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u7f16\u8f91\u611f\u77e5\u6d41\uff0c\u4ee5\u56fe\u50cf\u63d0\u793a\u4e3a\u6307\u5bfc\uff0c\u5b9e\u73b0\u8de8\u89c6\u56fe\u7684\u4e00\u81f4\u6027\u7f16\u8f91\u3002", "result": "\u65b9\u6cd5\u65e0\u9700\u4f18\u5316\uff0c\u524d\u9988\u64cd\u4f5c\uff0c\u4fdd\u6301\u539f\u59cb\u5bf9\u8c61\u7684\u7ed3\u6784\u548c\u5916\u89c2\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5bf9\u8c61\u7c7b\u522b\u548c\u7f16\u8f91\u573a\u666f\u3002", "conclusion": "EditP23\u5728\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\uff0c\u65e0\u9700\u624b\u52a8\u63a9\u7801\uff0c\u5c55\u793a\u4e86\u9ad8\u6548\u4e14\u76f4\u89c2\u76843D\u7f16\u8f91\u80fd\u529b\u3002"}}
{"id": "2506.20103", "pdf": "https://arxiv.org/pdf/2506.20103", "abs": "https://arxiv.org/abs/2506.20103", "authors": ["Jiahao Lin", "Weixuan Peng", "Bojia Zi", "Yifeng Gao", "Xianbiao Qi", "Xingjun Ma", "Yu-Gang Jiang"], "title": "BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos", "categories": ["cs.CV", "cs.AI", "I.4"], "comment": "7 page,4 figures,2 tables", "summary": "Recent advances in deep generative models have led to significant progress in video generation, yet the fidelity of AI-generated videos remains limited. Synthesized content often exhibits visual artifacts such as temporally inconsistent motion, physically implausible trajectories, unnatural object deformations, and local blurring that undermine realism and user trust. Accurate detection and spatial localization of these artifacts are crucial for both automated quality control and for guiding the development of improved generative models. However, the research community currently lacks a comprehensive benchmark specifically designed for artifact localization in AI generated videos. Existing datasets either restrict themselves to video or frame level detection or lack the fine-grained spatial annotations necessary for evaluating localization methods. To address this gap, we introduce BrokenVideos, a benchmark dataset of 3,254 AI-generated videos with meticulously annotated, pixel-level masks highlighting regions of visual corruption. Each annotation is validated through detailed human inspection to ensure high quality ground truth. Our experiments show that training state of the art artifact detection models and multi modal large language models (MLLMs) on BrokenVideos significantly improves their ability to localize corrupted regions. Through extensive evaluation, we demonstrate that BrokenVideos establishes a critical foundation for benchmarking and advancing research on artifact localization in generative video models. The dataset is available at: https://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86BrokenVideos\u6570\u636e\u96c6\uff0c\u7528\u4e8eAI\u751f\u6210\u89c6\u9891\u4e2d\u7684\u89c6\u89c9\u4f2a\u5f71\u5b9a\u4f4d\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u4e0d\u8db3\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u63d0\u5347\u68c0\u6d4b\u6a21\u578b\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "AI\u751f\u6210\u89c6\u9891\u4e2d\u5b58\u5728\u89c6\u89c9\u4f2a\u5f71\uff08\u5982\u8fd0\u52a8\u4e0d\u4e00\u81f4\u3001\u4e0d\u81ea\u7136\u7684\u53d8\u5f62\u7b49\uff09\uff0c\u4f46\u7f3a\u4e4f\u4e13\u95e8\u7528\u4e8e\u4f2a\u5f71\u5b9a\u4f4d\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u76f8\u5173\u7814\u7a76\u7684\u8fdb\u5c55\u3002", "method": "\u63d0\u51faBrokenVideos\u6570\u636e\u96c6\uff0c\u5305\u542b3,254\u4e2aAI\u751f\u6210\u89c6\u9891\uff0c\u5e26\u6709\u50cf\u7d20\u7ea7\u6807\u6ce8\u7684\u4f2a\u5f71\u533a\u57df\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u9a8c\u8bc1\u786e\u4fdd\u6807\u6ce8\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528BrokenVideos\u8bad\u7ec3\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u4f2a\u5f71\u5b9a\u4f4d\u80fd\u529b\u3002", "conclusion": "BrokenVideos\u4e3a\u751f\u6210\u89c6\u9891\u6a21\u578b\u7684\u4f2a\u5f71\u5b9a\u4f4d\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u6b65\u3002"}}
{"id": "2506.20151", "pdf": "https://arxiv.org/pdf/2506.20151", "abs": "https://arxiv.org/abs/2506.20151", "authors": ["Haipeng Fan", "Shiyuan Zhang", "Baohunesitu", "Zihang Guo", "Huaiwen Zhang"], "title": "EAR: Erasing Concepts from Unified Autoregressive Models", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 7 figures, 1 tables", "summary": "Autoregressive (AR) models have achieved unified and strong performance across both visual understanding and image generation tasks. However, removing undesired concepts from AR models while maintaining overall generation quality remains an open challenge. In this paper, we propose Erasure Autoregressive Model (EAR), a fine-tuning method for effective and utility-preserving concept erasure in AR models. Specifically, we introduce Windowed Gradient Accumulation (WGA) strategy to align patch-level decoding with erasure objectives, and Thresholded Loss Masking (TLM) strategy to protect content unrelated to the target concept during fine-tuning. Furthermore, we propose a novel benchmark, Erase Concept Generator and Visual Filter (ECGVF), aim at provide a more rigorous and comprehensive foundation for evaluating concept erasure in AR models. Specifically, we first employ structured templates across diverse large language models (LLMs) to pre-generate a large-scale corpus of target-replacement concept prompt pairs. Subsequently, we generate images from these prompts and subject them to rigorous filtering via a visual classifier to ensure concept fidelity and alignment. Extensive experimental results conducted on the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR achieves marked improvements in both erasure effectiveness and model utility preservation. Code is available at: https://github.com/immc-lab/ear/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEAR\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u6709\u6548\u4e14\u4fdd\u7559\u6548\u7528\u7684\u6982\u5ff5\u64e6\u9664\u3002\u901a\u8fc7WGA\u548cTLM\u7b56\u7565\uff0c\u4ee5\u53ca\u65b0\u57fa\u51c6ECGVF\uff0c\u5b9e\u9a8c\u8bc1\u660eEAR\u5728\u64e6\u9664\u6548\u679c\u548c\u6a21\u578b\u6548\u7528\u4fdd\u6301\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u81ea\u56de\u5f52\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u548c\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5982\u4f55\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u53bb\u9664\u4e0d\u60f3\u8981\u7684\u6982\u5ff5\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51faEAR\u65b9\u6cd5\uff0c\u5305\u62ecWGA\u7b56\u7565\u548cTLM\u7b56\u7565\uff0c\u5e76\u5f15\u5165ECGVF\u57fa\u51c6\u7528\u4e8e\u8bc4\u4f30\u6982\u5ff5\u64e6\u9664\u6548\u679c\u3002", "result": "\u5728Janus-Pro\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEAR\u5728\u6982\u5ff5\u64e6\u9664\u548c\u6a21\u578b\u6548\u7528\u4fdd\u6301\u4e0a\u5747\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "EAR\u4e3a\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u7684\u6982\u5ff5\u64e6\u9664\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.20222", "pdf": "https://arxiv.org/pdf/2506.20222", "abs": "https://arxiv.org/abs/2506.20222", "authors": ["Pujing Yang", "Guangyi Zhang", "Yunlong Cai", "Lei Yu", "Guanding Yu"], "title": "Dynamic Bandwidth Allocation for Hybrid Event-RGB Transmission", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "Event cameras asynchronously capture pixel-level intensity changes with extremely low latency. They are increasingly used in conjunction with RGB cameras for a wide range of vision-related applications. However, a major challenge in these hybrid systems lies in the transmission of the large volume of triggered events and RGB images. To address this, we propose a transmission scheme that retains efficient reconstruction performance of both sources while accomplishing real-time deblurring in parallel. Conventional RGB cameras and event cameras typically capture the same scene in different ways, often resulting in significant redundant information across their outputs. To address this, we develop a joint event and image (E-I) transmission framework to eliminate redundancy and thereby optimize channel bandwidth utilization. Our approach employs Bayesian modeling and the information bottleneck method to disentangle the shared and domain-specific information within the E-I inputs. This disentangled information bottleneck framework ensures both the compactness and informativeness of extracted shared and domain-specific information. Moreover, it adaptively allocates transmission bandwidth based on scene dynamics, i.e., more symbols are allocated to events for dynamic details or to images for static information. Simulation results demonstrate that the proposed scheme not only achieves superior reconstruction quality compared to conventional systems but also delivers enhanced deblurring performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u4e8b\u4ef6\u548c\u56fe\u50cf\u4f20\u8f93\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u5efa\u6a21\u548c\u4fe1\u606f\u74f6\u9888\u65b9\u6cd5\u6d88\u9664\u5197\u4f59\uff0c\u4f18\u5316\u5e26\u5bbd\u5229\u7528\uff0c\u540c\u65f6\u5b9e\u73b0\u5b9e\u65f6\u53bb\u6a21\u7cca\u3002", "motivation": "\u89e3\u51b3\u6df7\u5408\u7cfb\u7edf\u4e2d\u4e8b\u4ef6\u548cRGB\u56fe\u50cf\u4f20\u8f93\u7684\u5197\u4f59\u95ee\u9898\uff0c\u4f18\u5316\u5e26\u5bbd\u5229\u7528\u5e76\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u5efa\u6a21\u548c\u4fe1\u606f\u74f6\u9888\u65b9\u6cd5\uff0c\u5206\u79bb\u5171\u4eab\u548c\u9886\u57df\u7279\u5b9a\u4fe1\u606f\uff0c\u52a8\u6001\u5206\u914d\u4f20\u8f93\u5e26\u5bbd\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6848\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u53bb\u6a21\u7cca\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7cfb\u7edf\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u4f18\u5316\u4e86\u5e26\u5bbd\u5229\u7528\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u91cd\u5efa\u548c\u53bb\u6a21\u7cca\u6027\u80fd\u3002"}}
{"id": "2506.20254", "pdf": "https://arxiv.org/pdf/2506.20254", "abs": "https://arxiv.org/abs/2506.20254", "authors": ["Kun Yuan", "Tingxuan Chen", "Shi Li", "Joel L. Lavanchy", "Christian Heiliger", "Ege \u00d6zsoy", "Yiming Huang", "Long Bai", "Nassir Navab", "Vinkle Srivastav", "Hongliang Ren", "Nicolas Padoy"], "title": "Recognizing Surgical Phases Anywhere: Few-Shot Test-time Adaptation and Task-graph Guided Refinement", "categories": ["cs.CV"], "comment": "Accepted by MICCAI 2025", "summary": "The complexity and diversity of surgical workflows, driven by heterogeneous operating room settings, institutional protocols, and anatomical variability, present a significant challenge in developing generalizable models for cross-institutional and cross-procedural surgical understanding. While recent surgical foundation models pretrained on large-scale vision-language data offer promising transferability, their zero-shot performance remains constrained by domain shifts, limiting their utility in unseen surgical environments. To address this, we introduce Surgical Phase Anywhere (SPA), a lightweight framework for versatile surgical workflow understanding that adapts foundation models to institutional settings with minimal annotation. SPA leverages few-shot spatial adaptation to align multi-modal embeddings with institution-specific surgical scenes and phases. It also ensures temporal consistency through diffusion modeling, which encodes task-graph priors derived from institutional procedure protocols. Finally, SPA employs dynamic test-time adaptation, exploiting the mutual agreement between multi-modal phase prediction streams to adapt the model to a given test video in a self-supervised manner, enhancing the reliability under test-time distribution shifts. SPA is a lightweight adaptation framework, allowing hospitals to rapidly customize phase recognition models by defining phases in natural language text, annotating a few images with the phase labels, and providing a task graph defining phase transitions. The experimental results show that the SPA framework achieves state-of-the-art performance in few-shot surgical phase recognition across multiple institutions and procedures, even outperforming full-shot models with 32-shot labeled data. Code is available at https://github.com/CAMMA-public/SPA", "AI": {"tldr": "SPA\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u5c11\u91cf\u6807\u6ce8\u548c\u81ea\u7136\u8bed\u8a00\u5b9a\u4e49\u624b\u672f\u9636\u6bb5\uff0c\u5b9e\u73b0\u8de8\u673a\u6784\u548c\u8de8\u624b\u672f\u7684\u5de5\u4f5c\u6d41\u7406\u89e3\uff0c\u6027\u80fd\u4f18\u4e8e\u5168\u6807\u6ce8\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u624b\u672f\u5de5\u4f5c\u6d41\u591a\u6837\u6027\u5e26\u6765\u7684\u6a21\u578b\u6cdb\u5316\u6027\u5dee\u95ee\u9898\uff0c\u63d0\u5347\u5728\u672a\u89c1\u624b\u672f\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u3002", "method": "SPA\u7ed3\u5408\u5c11\u6837\u672c\u7a7a\u95f4\u9002\u5e94\u3001\u6269\u6563\u5efa\u6a21\u548c\u52a8\u6001\u6d4b\u8bd5\u65f6\u9002\u5e94\uff0c\u5229\u7528\u591a\u6a21\u6001\u5d4c\u5165\u548c\u4efb\u52a1\u56fe\u5148\u9a8c\u3002", "result": "SPA\u5728\u5c11\u6837\u672c\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u751a\u81f3\u4f18\u4e8e\u5168\u6807\u6ce8\u6a21\u578b\u3002", "conclusion": "SPA\u4e3a\u533b\u9662\u63d0\u4f9b\u4e86\u4e00\u79cd\u5feb\u901f\u5b9a\u5236\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u6a21\u578b\u7684\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.20294", "pdf": "https://arxiv.org/pdf/2506.20294", "abs": "https://arxiv.org/abs/2506.20294", "authors": ["Shunqi Mao", "Wei Guo", "Chaoyi Zhang", "Weidong Cai"], "title": "Ctrl-Z Sampling: Diffusion Sampling with Controlled Random Zigzag Explorations", "categories": ["cs.CV"], "comment": "10 pages, 3 figures, 2 tables", "summary": "Diffusion models have shown strong performance in conditional generation by progressively denoising Gaussian noise toward a target data distribution. This denoising process can be interpreted as a form of hill climbing in a learned latent space, where the model iteratively refines the sample toward regions of higher probability. However, diffusion models often converge to local optima that are locally visually coherent yet globally inconsistent or conditionally misaligned, due to latent space complexity and suboptimal initialization. Prior efforts attempted to address this by strengthening guidance signals or manipulating the initial noise distribution. We introduce Controlled Random Zigzag Sampling (Ctrl-Z Sampling), a novel sampling strategy designed to detect and escape such local maxima during conditional generation. The method first identifies potential local maxima using a reward model. Upon detection, it injects noise and reverts to a previous, noisier state to escape the current optimization plateau. The reward model then evaluates candidate trajectories, accepting only those that offer improvement, while progressively deeper retreat enables stronger escapes when nearby alternatives fail. This controlled random zigzag process allows dynamic alternation between forward refinement and backward exploration, enhancing both alignment and visual quality in the generated outputs. The proposed Ctrl-Z Sampling is model-agnostic and compatible with existing diffusion frameworks. Experimental results show that Ctrl-Z Sampling substantially improves generation quality with only around 7.6X increase in function evaluations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCtrl-Z\u91c7\u6837\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6269\u6563\u6a21\u578b\u4e2d\u68c0\u6d4b\u5e76\u9003\u79bb\u5c40\u90e8\u6700\u4f18\u89e3\uff0c\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u6761\u4ef6\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u89e3\uff0c\u5bfc\u81f4\u5168\u5c40\u4e0d\u4e00\u81f4\u6216\u6761\u4ef6\u4e0d\u5339\u914d\u3002", "method": "\u901a\u8fc7\u5956\u52b1\u6a21\u578b\u8bc6\u522b\u5c40\u90e8\u6700\u4f18\u89e3\uff0c\u6ce8\u5165\u566a\u58f0\u5e76\u56de\u9000\u5230\u66f4\u65e9\u72b6\u6001\uff0c\u52a8\u6001\u4ea4\u66ff\u524d\u5411\u4f18\u5316\u548c\u540e\u5411\u63a2\u7d22\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCtrl-Z\u91c7\u6837\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u4ec5\u589e\u52a0\u7ea67.6\u500d\u51fd\u6570\u8bc4\u4f30\u3002", "conclusion": "Ctrl-Z\u91c7\u6837\u662f\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u73b0\u6709\u6269\u6563\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5c40\u90e8\u6700\u4f18\u95ee\u9898\u3002"}}
{"id": "2506.20302", "pdf": "https://arxiv.org/pdf/2506.20302", "abs": "https://arxiv.org/abs/2506.20302", "authors": ["Abbas Anwar", "Mohammad Shullar", "Ali Arshad Nasir", "Mudassir Masood", "Saeed Anwar"], "title": "TDiR: Transformer based Diffusion for Image Restoration Tasks", "categories": ["cs.CV"], "comment": null, "summary": "Images captured in challenging environments often experience various forms of degradation, including noise, color cast, blur, and light scattering. These effects significantly reduce image quality, hindering their applicability in downstream tasks such as object detection, mapping, and classification. Our transformer-based diffusion model was developed to address image restoration tasks, aiming to improve the quality of degraded images. This model was evaluated against existing deep learning methodologies across multiple quality metrics for underwater image enhancement, denoising, and deraining on publicly available datasets. Our findings demonstrate that the diffusion model, combined with transformers, surpasses current methods in performance. The results of our model highlight the efficacy of diffusion models and transformers in improving the quality of degraded images, consequently expanding their utility in downstream tasks that require high-fidelity visual data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u56fe\u50cf\u6062\u590d\u4efb\u52a1\uff0c\u5728\u591a\u4e2a\u8d28\u91cf\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u6311\u6218\u6027\u73af\u5883\u4e2d\u6355\u83b7\u7684\u56fe\u50cf\u5e38\u56e0\u566a\u58f0\u3001\u8272\u504f\u3001\u6a21\u7cca\u548c\u5149\u6563\u5c04\u7b49\u95ee\u9898\u8d28\u91cf\u4e0b\u964d\uff0c\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u7684\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u5408Transformer\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u56fe\u50cf\u6062\u590d\u4efb\u52a1\uff0c\u5e76\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "result": "\u6a21\u578b\u5728\u56fe\u50cf\u589e\u5f3a\u3001\u53bb\u566a\u548c\u53bb\u96e8\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u4e0eTransformer\u7ed3\u5408\u80fd\u6709\u6548\u63d0\u5347\u9000\u5316\u56fe\u50cf\u8d28\u91cf\uff0c\u589e\u5f3a\u5176\u5728\u9700\u8981\u9ad8\u4fdd\u771f\u89c6\u89c9\u6570\u636e\u7684\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.20449", "pdf": "https://arxiv.org/pdf/2506.20449", "abs": "https://arxiv.org/abs/2506.20449", "authors": ["Changlu Guo", "Anders Nymark Christensen", "Morten Rieger Hannemose"], "title": "Med-Art: Diffusion Transformer for 2D Medical Text-to-Image Generation", "categories": ["cs.CV"], "comment": "The project is available at \\url{https://medart-ai.github.io}", "summary": "Text-to-image generative models have achieved remarkable breakthroughs in recent years. However, their application in medical image generation still faces significant challenges, including small dataset sizes, and scarcity of medical textual data. To address these challenges, we propose Med-Art, a framework specifically designed for medical image generation with limited data. Med-Art leverages vision-language models to generate visual descriptions of medical images which overcomes the scarcity of applicable medical textual data. Med-Art adapts a large-scale pre-trained text-to-image model, PixArt-$\\alpha$, based on the Diffusion Transformer (DiT), achieving high performance under limited data. Furthermore, we propose an innovative Hybrid-Level Diffusion Fine-tuning (HLDF) method, which enables pixel-level losses, effectively addressing issues such as overly saturated colors. We achieve state-of-the-art performance on two medical image datasets, measured by FID, KID, and downstream classification performance.", "AI": {"tldr": "Med-Art\u662f\u4e00\u4e2a\u9488\u5bf9\u5c0f\u89c4\u6a21\u533b\u5b66\u6570\u636e\u8bbe\u8ba1\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u9884\u8bad\u7ec3\u6a21\u578bPixArt-\u03b1\uff0c\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u6587\u672c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u751f\u6210\u9762\u4e34\u6570\u636e\u96c6\u5c0f\u548c\u6587\u672c\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "Med-Art\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u533b\u5b66\u56fe\u50cf\u63cf\u8ff0\uff0c\u5e76\u57fa\u4e8ePixArt-\u03b1\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u63d0\u51faHLDF\u65b9\u6cd5\u4f18\u5316\u50cf\u7d20\u7ea7\u635f\u5931\u3002", "result": "\u5728\u4e24\u4e2a\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86FID\u3001KID\u548c\u5206\u7c7b\u6027\u80fd\u7684\u6700\u4f18\u7ed3\u679c\u3002", "conclusion": "Med-Art\u5728\u6709\u9650\u6570\u636e\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u533b\u5b66\u56fe\u50cf\u751f\u6210\uff0c\u4e3a\u533b\u5b66\u9886\u57df\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2506.20452", "pdf": "https://arxiv.org/pdf/2506.20452", "abs": "https://arxiv.org/abs/2506.20452", "authors": ["Tobias Vontobel", "Seyedmorteza Sadat", "Farnood Salehi", "Romann M. Weber"], "title": "HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based Diffusion Sampling", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Diffusion models have emerged as the leading approach for image synthesis, demonstrating exceptional photorealism and diversity. However, training diffusion models at high resolutions remains computationally prohibitive, and existing zero-shot generation techniques for synthesizing images beyond training resolutions often produce artifacts, including object duplication and spatial incoherence. In this paper, we introduce HiWave, a training-free, zero-shot approach that substantially enhances visual fidelity and structural coherence in ultra-high-resolution image synthesis using pretrained diffusion models. Our method employs a two-stage pipeline: generating a base image from the pretrained model followed by a patch-wise DDIM inversion step and a novel wavelet-based detail enhancer module. Specifically, we first utilize inversion methods to derive initial noise vectors that preserve global coherence from the base image. Subsequently, during sampling, our wavelet-domain detail enhancer retains low-frequency components from the base image to ensure structural consistency, while selectively guiding high-frequency components to enrich fine details and textures. Extensive evaluations using Stable Diffusion XL demonstrate that HiWave effectively mitigates common visual artifacts seen in prior methods, achieving superior perceptual quality. A user study confirmed HiWave's performance, where it was preferred over the state-of-the-art alternative in more than 80% of comparisons, highlighting its effectiveness for high-quality, ultra-high-resolution image synthesis without requiring retraining or architectural modifications.", "AI": {"tldr": "HiWave\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u96f6\u6837\u672c\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u6d41\u7a0b\uff08\u57fa\u7840\u56fe\u50cf\u751f\u6210\u548c\u5c0f\u6ce2\u589e\u5f3a\uff09\u63d0\u5347\u8d85\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5408\u6210\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5408\u6210\u4e2d\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u751f\u6210\u56fe\u50cf\u65f6\u51fa\u73b0\u7684\u4f2a\u5f71\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a1) \u751f\u6210\u57fa\u7840\u56fe\u50cf\uff1b2) \u901a\u8fc7DDIM\u53cd\u6f14\u548c\u5c0f\u6ce2\u589e\u5f3a\u6a21\u5757\u63d0\u5347\u7ec6\u8282\u3002", "result": "HiWave\u663e\u8457\u51cf\u5c11\u4e86\u4f2a\u5f71\uff0c\u5728\u7528\u6237\u7814\u7a76\u4e2d80%\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HiWave\u4e3a\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u4fee\u6539\u67b6\u6784\u7684\u9ad8\u8d28\u91cf\u8d85\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5408\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.20590", "pdf": "https://arxiv.org/pdf/2506.20590", "abs": "https://arxiv.org/abs/2506.20590", "authors": ["Chaojun Ni", "Jie Li", "Haoyun Li", "Hengyu Liu", "Xiaofeng Wang", "Zheng Zhu", "Guosheng Zhao", "Boyuan Wang", "Chenxin Li", "Guan Huang", "Wenjun Mei"], "title": "WonderFree: Enhancing Novel View Quality and Cross-View Consistency for 3D Scene Exploration", "categories": ["cs.CV"], "comment": null, "summary": "Interactive 3D scene generation from a single image has gained significant attention due to its potential to create immersive virtual worlds. However, a key challenge in current 3D generation methods is the limited explorability, which cannot render high-quality images during larger maneuvers beyond the original viewpoint, particularly when attempting to move forward into unseen areas. To address this challenge, we propose WonderFree, the first model that enables users to interactively generate 3D worlds with the freedom to explore from arbitrary angles and directions. Specifically, we decouple this challenge into two key subproblems: novel view quality, which addresses visual artifacts and floating issues in novel views, and cross-view consistency, which ensures spatial consistency across different viewpoints. To enhance rendering quality in novel views, we introduce WorldRestorer, a data-driven video restoration model designed to eliminate floaters and artifacts. In addition, a data collection pipeline is presented to automatically gather training data for WorldRestorer, ensuring it can handle scenes with varying styles needed for 3D scene generation. Furthermore, to improve cross-view consistency, we propose ConsistView, a multi-view joint restoration mechanism that simultaneously restores multiple perspectives while maintaining spatiotemporal coherence. Experimental results demonstrate that WonderFree not only enhances rendering quality across diverse viewpoints but also significantly improves global coherence and consistency. These improvements are confirmed by CLIP-based metrics and a user study showing a 77.20% preference for WonderFree over WonderWorld enabling a seamless and immersive 3D exploration experience. The code, model, and data will be publicly available.", "AI": {"tldr": "WonderFree\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f3D\u573a\u666f\u751f\u6210\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u63a2\u7d22\u6027\u548c\u89c6\u89d2\u4e00\u81f4\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7WorldRestorer\u548cConsistView\u6280\u672f\u63d0\u5347\u6e32\u67d3\u8d28\u91cf\u548c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d3D\u751f\u6210\u65b9\u6cd5\u5728\u63a2\u7d22\u6027\u4e0a\u53d7\u9650\uff0c\u65e0\u6cd5\u5728\u5927\u8303\u56f4\u89c6\u89d2\u53d8\u5316\u4e0b\u4fdd\u6301\u9ad8\u8d28\u91cf\u6e32\u67d3\uff0c\u5c24\u5176\u662f\u5728\u672a\u89c2\u5bdf\u533a\u57df\u3002", "method": "\u63d0\u51faWonderFree\u6a21\u578b\uff0c\u5206\u89e3\u95ee\u9898\u4e3a\u89c6\u89d2\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u4e24\u4e2a\u5b50\u95ee\u9898\uff0c\u5206\u522b\u901a\u8fc7WorldRestorer\uff08\u89c6\u9891\u4fee\u590d\u6a21\u578b\uff09\u548cConsistView\uff08\u591a\u89c6\u89d2\u8054\u5408\u4fee\u590d\u673a\u5236\uff09\u89e3\u51b3\u3002", "result": "\u5b9e\u9a8c\u8868\u660eWonderFree\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u8d28\u91cf\u548c\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u7528\u6237\u504f\u597d\u7387\u8fbe77.20%\u3002", "conclusion": "WonderFree\u4e3a3D\u573a\u666f\u63a2\u7d22\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u516c\u5f00\u3002"}}
{"id": "2506.20601", "pdf": "https://arxiv.org/pdf/2506.20601", "abs": "https://arxiv.org/abs/2506.20601", "authors": ["Rui Huang", "Guangyao Zhai", "Zuria Bauer", "Marc Pollefeys", "Federico Tombari", "Leonidas Guibas", "Gao Huang", "Francis Engelmann"], "title": "Video Perception Models for 3D Scene Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Traditionally, 3D scene synthesis requires expert knowledge and significant manual effort. Automating this process could greatly benefit fields such as architectural design, robotics simulation, virtual reality, and gaming. Recent approaches to 3D scene synthesis often rely on the commonsense reasoning of large language models (LLMs) or strong visual priors of modern image generation models. However, current LLMs demonstrate limited 3D spatial reasoning ability, which restricts their ability to generate realistic and coherent 3D scenes. Meanwhile, image generation-based methods often suffer from constraints in viewpoint selection and multi-view inconsistencies. In this work, we present Video Perception models for 3D Scene synthesis (VIPScene), a novel framework that exploits the encoded commonsense knowledge of the 3D physical world in video generation models to ensure coherent scene layouts and consistent object placements across views. VIPScene accepts both text and image prompts and seamlessly integrates video generation, feedforward 3D reconstruction, and open-vocabulary perception models to semantically and geometrically analyze each object in a scene. This enables flexible scene synthesis with high realism and structural consistency. For more precise analysis, we further introduce First-Person View Score (FPVScore) for coherence and plausibility evaluation, utilizing continuous first-person perspective to capitalize on the reasoning ability of multimodal large language models. Extensive experiments show that VIPScene significantly outperforms existing methods and generalizes well across diverse scenarios. The code will be released.", "AI": {"tldr": "VIPScene\u5229\u7528\u89c6\u9891\u751f\u6210\u6a21\u578b\u76843D\u5e38\u8bc6\u77e5\u8bc6\uff0c\u7ed3\u5408\u6587\u672c\u548c\u56fe\u50cf\u63d0\u793a\uff0c\u5b9e\u73b0\u9ad8\u771f\u5b9e\u611f\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u76843D\u573a\u666f\u5408\u6210\u3002", "motivation": "\u4f20\u7edf3D\u573a\u666f\u5408\u6210\u9700\u8981\u4e13\u5bb6\u77e5\u8bc6\u548c\u5927\u91cf\u624b\u52a8\u5de5\u4f5c\uff0c\u81ea\u52a8\u5316\u6b64\u8fc7\u7a0b\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5efa\u7b51\u8bbe\u8ba1\u3001\u673a\u5668\u4eba\u6a21\u62df\u7b49\u9886\u57df\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982LLM\u6216\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff09\u57283D\u7a7a\u95f4\u63a8\u7406\u6216\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u4e0a\u5b58\u5728\u5c40\u9650\u3002", "method": "VIPScene\u6574\u5408\u89c6\u9891\u751f\u6210\u3001\u524d\u99883D\u91cd\u5efa\u548c\u5f00\u653e\u8bcd\u6c47\u611f\u77e5\u6a21\u578b\uff0c\u901a\u8fc7\u8bed\u4e49\u548c\u51e0\u4f55\u5206\u6790\u5b9e\u73b0\u573a\u666f\u5408\u6210\uff0c\u5e76\u5f15\u5165FPVScore\u8bc4\u4f30\u4e00\u81f4\u6027\u548c\u5408\u7406\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eVIPScene\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u80fd\u6cdb\u5316\u5230\u591a\u6837\u573a\u666f\u3002", "conclusion": "VIPScene\u901a\u8fc7\u89c6\u9891\u751f\u6210\u6a21\u578b\u76843D\u5e38\u8bc6\u77e5\u8bc6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u771f\u5b9e\u611f\u548c\u4e00\u81f4\u6027\u76843D\u573a\u666f\u5408\u6210\u3002"}}
{"id": "2506.20616", "pdf": "https://arxiv.org/pdf/2506.20616", "abs": "https://arxiv.org/abs/2506.20616", "authors": ["Quoc-Duy Tran", "Anh-Tuan Vo", "Dinh-Khoi Vo", "Tam V. Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "title": "Shape2Animal: Creative Animal Generation from Natural Silhouettes", "categories": ["cs.CV"], "comment": null, "summary": "Humans possess a unique ability to perceive meaningful patterns in ambiguous stimuli, a cognitive phenomenon known as pareidolia. This paper introduces Shape2Animal framework to mimics this imaginative capacity by reinterpreting natural object silhouettes, such as clouds, stones, or flames, as plausible animal forms. Our automated framework first performs open-vocabulary segmentation to extract object silhouette and interprets semantically appropriate animal concepts using vision-language models. It then synthesizes an animal image that conforms to the input shape, leveraging text-to-image diffusion model and seamlessly blends it into the original scene to generate visually coherent and spatially consistent compositions. We evaluated Shape2Animal on a diverse set of real-world inputs, demonstrating its robustness and creative potential. Our Shape2Animal can offer new opportunities for visual storytelling, educational content, digital art, and interactive media design. Our project page is here: https://shape2image.github.io", "AI": {"tldr": "Shape2Animal\u6846\u67b6\u901a\u8fc7\u91cd\u65b0\u89e3\u91ca\u81ea\u7136\u7269\u4f53\u8f6e\u5ed3\uff08\u5982\u4e91\u3001\u77f3\u5934\u6216\u706b\u7130\uff09\u4e3a\u52a8\u7269\u5f62\u6001\uff0c\u6a21\u62df\u4eba\u7c7b\u7684pareidolia\u73b0\u8c61\u3002", "motivation": "\u6a21\u4eff\u4eba\u7c7b\u5728\u6a21\u7cca\u523a\u6fc0\u4e2d\u611f\u77e5\u6709\u610f\u4e49\u6a21\u5f0f\u7684\u80fd\u529b\uff0c\u4e3a\u89c6\u89c9\u53d9\u4e8b\u3001\u6559\u80b2\u5185\u5bb9\u548c\u6570\u5b57\u827a\u672f\u63d0\u4f9b\u65b0\u673a\u4f1a\u3002", "method": "\u4f7f\u7528\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u63d0\u53d6\u7269\u4f53\u8f6e\u5ed3\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u89e3\u91ca\u52a8\u7269\u6982\u5ff5\uff0c\u5229\u7528\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5408\u6210\u52a8\u7269\u56fe\u50cf\u5e76\u878d\u5165\u539f\u59cb\u573a\u666f\u3002", "result": "\u5728\u591a\u6837\u5316\u771f\u5b9e\u8f93\u5165\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u9c81\u68d2\u6027\u548c\u521b\u9020\u529b\u3002", "conclusion": "Shape2Animal\u4e3a\u89c6\u89c9\u53d9\u4e8b\u3001\u6559\u80b2\u5185\u5bb9\u548c\u4ea4\u4e92\u5a92\u4f53\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u6027\u3002"}}
{"id": "2506.20638", "pdf": "https://arxiv.org/pdf/2506.20638", "abs": "https://arxiv.org/abs/2506.20638", "authors": ["Cl\u00e9ment Forray", "Pauline Delporte", "Nicolas Delaygue", "Florence Genin", "Dawa Derksen"], "title": "Joint attitude estimation and 3D neural reconstruction of non-cooperative space objects", "categories": ["cs.CV"], "comment": "accepted for CVPR 2025 NFBCC workshop", "summary": "Obtaining a better knowledge of the current state and behavior of objects orbiting Earth has proven to be essential for a range of applications such as active debris removal, in-orbit maintenance, or anomaly detection. 3D models represent a valuable source of information in the field of Space Situational Awareness (SSA). In this work, we leveraged Neural Radiance Fields (NeRF) to perform 3D reconstruction of non-cooperative space objects from simulated images. This scenario is challenging for NeRF models due to unusual camera characteristics and environmental conditions : mono-chromatic images, unknown object orientation, limited viewing angles, absence of diffuse lighting etc. In this work we focus primarly on the joint optimization of camera poses alongside the NeRF. Our experimental results show that the most accurate 3D reconstruction is achieved when training with successive images one-by-one. We estimate camera poses by optimizing an uniform rotation and use regularization to prevent successive poses from being too far apart.", "AI": {"tldr": "\u5229\u7528NeRF\u4ece\u6a21\u62df\u56fe\u50cf\u4e2d\u91cd\u5efa\u975e\u5408\u4f5c\u7a7a\u95f4\u7269\u4f53\u76843D\u6a21\u578b\uff0c\u91cd\u70b9\u4f18\u5316\u76f8\u673a\u59ff\u6001\uff0c\u5b9e\u9a8c\u8868\u660e\u9010\u5e27\u8bad\u7ec3\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u63d0\u5347\u7a7a\u95f4\u6001\u52bf\u611f\u77e5\u80fd\u529b\uff0c\u652f\u6301\u788e\u7247\u6e05\u9664\u3001\u5728\u8f68\u7ef4\u62a4\u7b49\u5e94\u7528\u3002", "method": "\u5229\u7528NeRF\u8fdb\u884c3D\u91cd\u5efa\uff0c\u8054\u5408\u4f18\u5316\u76f8\u673a\u59ff\u6001\uff0c\u91c7\u7528\u9010\u5e27\u8bad\u7ec3\u548c\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "result": "\u9010\u5e27\u8bad\u7ec3\u548c\u76f8\u673a\u59ff\u6001\u4f18\u5316\u5b9e\u73b0\u4e86\u6700\u51c6\u786e\u76843D\u91cd\u5efa\u3002", "conclusion": "NeRF\u7ed3\u5408\u76f8\u673a\u59ff\u6001\u4f18\u5316\u5728\u7a7a\u95f4\u7269\u4f533D\u91cd\u5efa\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2506.19935", "pdf": "https://arxiv.org/pdf/2506.19935", "abs": "https://arxiv.org/abs/2506.19935", "authors": ["Shuchen Xue", "Tianyu Xie", "Tianyang Hu", "Zijin Feng", "Jiacheng Sun", "Kenji Kawaguchi", "Zhenguo Li", "Zhi-Ming Ma"], "title": "Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and Architecture", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": null, "summary": "Large language models (LLMs) predominantly use autoregressive (AR) approaches, but masked diffusion models (MDMs) are emerging as viable alternatives. A key challenge in comparing AR and MDM paradigms is their typical architectural difference: AR models are often decoder-only, while MDMs have largely been encoder-only. This practice of changing both the modeling paradigm and architecture simultaneously makes direct comparisons unfair, as it's hard to distinguish whether observed differences stem from the paradigm itself or the architectural shift. This research evaluates MDMs within a decoder-only framework to: (1) equitably compare MDM (as Any-Order AR, or AO-AR) and standard AR paradigms. Our investigation suggests that the standard AO-AR objective, which averages over all token permutations, may benefit from refinement, as many permutations appear less informative compared to the language's inherent left-to-right structure. (2) Investigate architectural influences (decoder-only vs. encoder-only) within MDMs. We demonstrate that while encoder-only MDMs model a simpler conditional probability space, decoder-only MDMs can achieve dramatic generation speedups ($\\sim25\\times$) and comparable perplexity with temperature annealing despite modeling a vastly larger space, highlighting key trade-offs. This work thus decouples core paradigm differences from architectural influences, offering insights for future model design. Code is available at https://github.com/scxue/AO-GPT-MDM.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5c06\u63a9\u7801\u6269\u6563\u6a21\u578b\uff08MDM\uff09\u4e0e\u89e3\u7801\u5668\u67b6\u6784\u7ed3\u5408\uff0c\u516c\u5e73\u6bd4\u8f83\u4e86\u81ea\u56de\u5f52\uff08AR\uff09\u4e0eMDM\u8303\u5f0f\uff0c\u5e76\u63a2\u8ba8\u4e86\u67b6\u6784\u5bf9MDM\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u6bd4\u8f83AR\u548cMDM\u8303\u5f0f\u65f6\uff0c\u4f20\u7edf\u65b9\u6cd5\u540c\u65f6\u6539\u53d8\u67b6\u6784\u548c\u8303\u5f0f\uff0c\u5bfc\u81f4\u4e0d\u516c\u5e73\u5bf9\u6bd4\u3002\u672c\u7814\u7a76\u65e8\u5728\u5206\u79bb\u8303\u5f0f\u4e0e\u67b6\u6784\u7684\u5f71\u54cd\u3002", "method": "\u5728\u89e3\u7801\u5668\u67b6\u6784\u4e2d\u8bc4\u4f30MDM\uff08\u79f0\u4e3aAny-Order AR\uff0cAO-AR\uff09\uff0c\u5e76\u5206\u6790\u5176\u4e0e\u6807\u51c6AR\u7684\u5dee\u5f02\u3002\u540c\u65f6\u63a2\u8ba8\u89e3\u7801\u5668\u4e0e\u7f16\u7801\u5668\u67b6\u6784\u5bf9MDM\u7684\u5f71\u54cd\u3002", "result": "\u89e3\u7801\u5668MDM\u5728\u751f\u6210\u901f\u5ea6\u4e0a\u663e\u8457\u63d0\u5347\uff08\u7ea625\u500d\uff09\uff0c\u4e14\u56f0\u60d1\u5ea6\u4e0e\u7f16\u7801\u5668MDM\u76f8\u5f53\u3002AO-AR\u76ee\u6807\u53ef\u80fd\u9700\u8981\u4f18\u5316\uff0c\u56e0\u5176\u90e8\u5206\u6392\u5217\u4fe1\u606f\u8f83\u5c11\u3002", "conclusion": "\u7814\u7a76\u89e3\u8026\u4e86\u8303\u5f0f\u4e0e\u67b6\u6784\u7684\u5f71\u54cd\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002\u89e3\u7801\u5668MDM\u5728\u901f\u5ea6\u548c\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002"}}
{"id": "2506.20245", "pdf": "https://arxiv.org/pdf/2506.20245", "abs": "https://arxiv.org/abs/2506.20245", "authors": ["Yushan Zhao", "Jinyuan He", "Donglai Chen", "Weijie Luo", "Chong Xie", "Ri Zhang", "Yonghong Chen", "Yan Xu"], "title": "FedBKD: Distilled Federated Learning to Embrace Gerneralization and Personalization on Non-IID Data", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Federated learning (FL) is a decentralized collaborative machine learning (ML) technique. It provides a solution to the issues of isolated data islands and data privacy leakage in industrial ML practices. One major challenge in FL is handling the non-identical and independent distributed (non-IID) data. Current solutions either focus on constructing an all-powerful global model, or customizing personalized local models. Few of them can provide both a well-generalized global model and well-performed local models at the same time. Additionally, many FL solutions to the non-IID problem are benefited from introducing public datasets. However, this will also increase the risk of data leakage. To tackle the problems, we propose a novel data-free distillation framework, Federated Bidirectional Knowledge Distillation (FedBKD). Specifically, we train Generative Adversarial Networks (GAN) for synthetic data. During the GAN training, local models serve as discriminators and their parameters are frozen. The synthetic data is then used for bidirectional distillation between global and local models to achieve knowledge interactions so that performances for both sides are improved. We conduct extensive experiments on 4 benchmarks under different non-IID settings. The results show that FedBKD achieves SOTA performances in every case.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedBKD\u7684\u65e0\u6570\u636e\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5b9e\u73b0\u5168\u5c40\u548c\u5c40\u90e8\u6a21\u578b\u4e4b\u95f4\u7684\u53cc\u5411\u77e5\u8bc6\u84b8\u998f\uff0c\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u7684\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4e2d\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u6570\u636e\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u5f15\u5165\u516c\u5171\u6570\u636e\u96c6\u5e26\u6765\u7684\u6570\u636e\u6cc4\u9732\u98ce\u9669\u3002", "method": "\u4f7f\u7528GAN\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5c40\u90e8\u6a21\u578b\u4f5c\u4e3a\u5224\u522b\u5668\uff0c\u53c2\u6570\u51bb\u7ed3\uff1b\u901a\u8fc7\u5408\u6210\u6570\u636e\u5b9e\u73b0\u5168\u5c40\u4e0e\u5c40\u90e8\u6a21\u578b\u7684\u53cc\u5411\u77e5\u8bc6\u84b8\u998f\u3002", "result": "\u57284\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFedBKD\u5728\u4e0d\u540c\u975eIID\u8bbe\u7f6e\u4e0b\u5747\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "FedBKD\u6846\u67b6\u5728\u63d0\u5347\u5168\u5c40\u548c\u5c40\u90e8\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u907f\u514d\u4e86\u6570\u636e\u6cc4\u9732\u98ce\u9669\uff0c\u4e3a\u975eIID\u6570\u636e\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
