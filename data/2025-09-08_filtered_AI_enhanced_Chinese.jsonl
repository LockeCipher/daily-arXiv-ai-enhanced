{"id": "2509.04513", "pdf": "https://arxiv.org/pdf/2509.04513", "abs": "https://arxiv.org/abs/2509.04513", "authors": ["Ming Du", "Volker Rose", "Junjing Deng", "Dileep Singh", "Si Chen", "Mathew J. Cherukara"], "title": "Fidelity-preserving enhancement of ptychography with foundational text-to-image models", "categories": ["cs.GR", "cs.NA", "math.NA", "physics.app-ph", "65Kxx"], "comment": null, "summary": "Ptychographic phase retrieval enables high-resolution imaging of complex samples but often suffers from artifacts such as grid pathology and multislice crosstalk, which degrade reconstructed images. We propose a plug-and-play (PnP) framework that integrates physics model-based phase retrieval with text-guided image editing using foundational diffusion models. By employing the alternating direction method of multipliers (ADMM), our approach ensures consensus between data fidelity and artifact removal subproblems, maintaining physics consistency while enhancing image quality. Artifact removal is achieved using a text-guided diffusion image editing method (LEDITS++) with a pre-trained foundational diffusion model, allowing users to specify artifacts for removal in natural language. Demonstrations on simulated and experimental datasets show significant improvements in artifact suppression and structural fidelity, validated by metrics such as peak signal-to-noise ratio (PSNR) and diffraction pattern consistency. This work highlights the combination of text-guided generative models and model-based phase retrieval algorithms as a transferable and fidelity-preserving method for high-quality diffraction imaging.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u6a21\u578b\u76f8\u4f4d\u68c0\u7d22\u4e0e\u6587\u672c\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u7684\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7ADMM\u7b97\u6cd5\u5b9e\u73b0\u6570\u636e\u4fdd\u771f\u5ea6\u4e0e\u4f2a\u5f71\u53bb\u9664\u7684\u5171\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u884d\u5c04\u6210\u50cf\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3ptychographic\u76f8\u4f4d\u68c0\u7d22\u4e2d\u7684\u7f51\u683c\u75c5\u7406\u5b66\u548c\u591a\u5c42\u4e32\u6270\u7b49\u4f2a\u5f71\u95ee\u9898\uff0c\u8fd9\u4e9b\u4f2a\u5f71\u4f1a\u964d\u4f4e\u91cd\u5efa\u56fe\u50cf\u8d28\u91cf\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u7269\u7406\u4e00\u81f4\u6027\u53c8\u80fd\u6709\u6548\u53bb\u9664\u4f2a\u5f71\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5(ADMM)\u6846\u67b6\uff0c\u5c06\u7269\u7406\u6a21\u578b\u76f8\u4f4d\u68c0\u7d22\u4e0e\u6587\u672c\u5f15\u5bfc\u6269\u6563\u6a21\u578b(LEDITS++)\u76f8\u7ed3\u5408\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u5b9a\u9700\u8981\u53bb\u9664\u7684\u4f2a\u5f71\u7c7b\u578b\u3002", "result": "\u5728\u6a21\u62df\u548c\u5b9e\u9a8c\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u663e\u8457\u7684\u4f2a\u5f71\u6291\u5236\u548c\u7ed3\u6784\u4fdd\u771f\u5ea6\u63d0\u5347\uff0c\u901a\u8fc7PSNR\u548c\u884d\u5c04\u56fe\u6848\u4e00\u81f4\u6027\u7b49\u6307\u6807\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6587\u672c\u5f15\u5bfc\u751f\u6210\u6a21\u578b\u4e0e\u57fa\u4e8e\u6a21\u578b\u7684\u76f8\u4f4d\u68c0\u7d22\u7b97\u6cd5\u76f8\u7ed3\u5408\uff0c\u4e3a\u9ad8\u8d28\u91cf\u884d\u5c04\u6210\u50cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u8fc1\u79fb\u4e14\u4fdd\u771f\u5ea6\u9ad8\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.05285", "pdf": "https://arxiv.org/pdf/2509.05285", "abs": "https://arxiv.org/abs/2509.05285", "authors": ["Haruo Fujiwara", "Yusuke Mukuta", "Tatsuya Harada"], "title": "Improved 3D Scene Stylization via Text-Guided Generative Image Editing with Region-Based Control", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Recent advances in text-driven 3D scene editing and stylization, which leverage the powerful capabilities of 2D generative models, have demonstrated promising outcomes. However, challenges remain in ensuring high-quality stylization and view consistency simultaneously. Moreover, applying style consistently to different regions or objects in the scene with semantic correspondence is a challenging task. To address these limitations, we introduce techniques that enhance the quality of 3D stylization while maintaining view consistency and providing optional region-controlled style transfer. Our method achieves stylization by re-training an initial 3D representation using stylized multi-view 2D images of the source views. Therefore, ensuring both style consistency and view consistency of stylized multi-view images is crucial. We achieve this by extending the style-aligned depth-conditioned view generation framework, replacing the fully shared attention mechanism with a single reference-based attention-sharing mechanism, which effectively aligns style across different viewpoints. Additionally, inspired by recent 3D inpainting methods, we utilize a grid of multiple depth maps as a single-image reference to further strengthen view consistency among stylized images. Finally, we propose Multi-Region Importance-Weighted Sliced Wasserstein Distance Loss, allowing styles to be applied to distinct image regions using segmentation masks from off-the-shelf models. We demonstrate that this optional feature enhances the faithfulness of style transfer and enables the mixing of different styles across distinct regions of the scene. Experimental evaluations, both qualitative and quantitative, demonstrate that our pipeline effectively improves the results of text-driven 3D stylization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6587\u672c\u9a71\u52a83D\u573a\u666f\u98ce\u683c\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u8003\u6ce8\u610f\u529b\u673a\u5236\u548c\u6df1\u5ea6\u56fe\u589e\u5f3a\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u5e76\u652f\u6301\u591a\u533a\u57df\u98ce\u683c\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u9a71\u52a83D\u573a\u666f\u7f16\u8f91\u65b9\u6cd5\u5728\u4fdd\u8bc1\u9ad8\u8d28\u91cf\u98ce\u683c\u5316\u548c\u89c6\u89d2\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u573a\u666f\u4e2d\u4e0d\u540c\u533a\u57df\u8fdb\u884c\u8bed\u4e49\u5bf9\u5e94\u7684\u98ce\u683c\u8fc1\u79fb\u8f83\u4e3a\u56f0\u96be\u3002", "method": "\u6269\u5c55\u98ce\u683c\u5bf9\u9f50\u7684\u6df1\u5ea6\u6761\u4ef6\u89c6\u89d2\u751f\u6210\u6846\u67b6\uff0c\u4f7f\u7528\u53c2\u8003\u6ce8\u610f\u529b\u673a\u5236\u66ff\u4ee3\u5171\u4eab\u6ce8\u610f\u529b\uff1b\u5229\u7528\u591a\u6df1\u5ea6\u56fe\u7f51\u683c\u589e\u5f3a\u89c6\u89d2\u4e00\u81f4\u6027\uff1b\u63d0\u51fa\u591a\u533a\u57df\u91cd\u8981\u6027\u52a0\u6743\u5207\u7247Wasserstein\u8ddd\u79bb\u635f\u5931\u5b9e\u73b0\u533a\u57df\u63a7\u5236\u98ce\u683c\u8fc1\u79fb\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6587\u672c\u9a71\u52a83D\u98ce\u683c\u5316\u7684\u7ed3\u679c\u8d28\u91cf\uff0c\u5728\u4fdd\u6301\u89c6\u89d2\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u98ce\u683c\u5316\u6548\u679c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6280\u672f\u80fd\u591f\u540c\u65f6\u786e\u4fdd\u98ce\u683c\u4e00\u81f4\u6027\u548c\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u5e76\u63d0\u4f9b\u53ef\u9009\u7684\u591a\u533a\u57df\u98ce\u683c\u63a7\u5236\u529f\u80fd\uff0c\u4e3a3D\u573a\u666f\u98ce\u683c\u5316\u63d0\u4f9b\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.04548", "pdf": "https://arxiv.org/pdf/2509.04548", "abs": "https://arxiv.org/abs/2509.04548", "authors": ["Hongyang Wei", "Baixin Xu", "Hongbo Liu", "Cyrus Wu", "Jie Liu", "Yi Peng", "Peiyu Wang", "Zexiang Liu", "Jingwen He", "Yidan Xietian", "Chuanxin Tang", "Zidong Wang", "Yichen Wei", "Liang Hu", "Boyi Jiang", "William Li", "Ying He", "Yang Liu", "Xuchen Song", "Eric Li", "Yahui Zhou"], "title": "Skywork UniPic 2.0: Building Kontext Model with Online RL for Unified Multimodal Model", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in multimodal models have demonstrated impressive capabilities in unified image generation and editing. However, many prominent open-source models prioritize scaling model parameters over optimizing training strategies, limiting their efficiency and performance. In this work, we present UniPic2-SD3.5M-Kontext, a 2B-parameter DiT model based on SD3.5-Medium, which achieves state-of-the-art image generation and editing while extending seamlessly into a unified multimodal framework. Our approach begins with architectural modifications to SD3.5-Medium and large-scale pre-training on high-quality data, enabling joint text-to-image generation and editing capabilities. To enhance instruction following and editing consistency, we propose a novel Progressive Dual-Task Reinforcement strategy (PDTR), which effectively strengthens both tasks in a staged manner. We empirically validate that the reinforcement phases for different tasks are mutually beneficial and do not induce negative interference. After pre-training and reinforcement strategies, UniPic2-SD3.5M-Kontext demonstrates stronger image generation and editing capabilities than models with significantly larger generation parameters-including BAGEL (7B) and Flux-Kontext (12B). Furthermore, following the MetaQuery, we connect the UniPic2-SD3.5M-Kontext and Qwen2.5-VL-7B via a connector and perform joint training to launch a unified multimodal model UniPic2-Metaquery. UniPic2-Metaquery integrates understanding, generation, and editing, achieving top-tier performance across diverse tasks with a simple and scalable training paradigm. This consistently validates the effectiveness and generalizability of our proposed training paradigm, which we formalize as Skywork UniPic 2.0.", "AI": {"tldr": "UniPic2-SD3.5M-Kontext\u662f\u4e00\u4e2a2B\u53c2\u6570\u7684DiT\u6a21\u578b\uff0c\u57fa\u4e8eSD3.5-Medium\u67b6\u6784\u6539\u8fdb\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u521b\u65b0\u7684\u6e10\u8fdb\u5f0f\u53cc\u4efb\u52a1\u5f3a\u5316\u7b56\u7565\uff0c\u5728\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5e76\u6269\u5c55\u5230\u7edf\u4e00\u591a\u6a21\u6001\u6846\u67b6UniPic2-Metaquery\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u591a\u6a21\u6001\u6a21\u578b\u8fc7\u4e8e\u6ce8\u91cd\u53c2\u6570\u89c4\u6a21\u6269\u5c55\u800c\u5ffd\u89c6\u8bad\u7ec3\u7b56\u7565\u4f18\u5316\uff0c\u9650\u5236\u4e86\u6548\u7387\u548c\u6027\u80fd\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u8303\u5f0f\u6765\u63d0\u5347\u6a21\u578b\u80fd\u529b\u3002", "method": "1. \u5bf9SD3.5-Medium\u8fdb\u884c\u67b6\u6784\u4fee\u6539\u548c\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u9884\u8bad\u7ec3\uff1b2. \u63d0\u51fa\u6e10\u8fdb\u5f0f\u53cc\u4efb\u52a1\u5f3a\u5316\u7b56\u7565(PDTR)\u5206\u9636\u6bb5\u589e\u5f3a\u6307\u4ee4\u8ddf\u968f\u548c\u7f16\u8f91\u4e00\u81f4\u6027\uff1b3. \u901a\u8fc7\u8fde\u63a5\u5668\u5c06\u56fe\u50cf\u6a21\u578b\u4e0eQwen2.5-VL-7B\u7ed3\u5408\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\u3002", "result": "\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u80fd\u529b\u4e0a\u8d85\u8d8a\u53c2\u6570\u89c4\u6a21\u66f4\u5927\u7684\u6a21\u578b(BAGEL 7B\u548cFlux-Kontext 12B)\uff0cUniPic2-Metaquery\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u8fbe\u5230\u9876\u7ea7\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bad\u7ec3\u8303\u5f0fSkywork UniPic 2.0\u88ab\u8bc1\u660e\u6709\u6548\u4e14\u5177\u6709\u826f\u597d\u6cdb\u5316\u6027\uff0c\u80fd\u591f\u4ee5\u7b80\u5355\u53ef\u6269\u5c55\u7684\u65b9\u5f0f\u5b9e\u73b0\u7406\u89e3\u3001\u751f\u6210\u548c\u7f16\u8f91\u7684\u7edf\u4e00\u96c6\u6210\u3002"}}
{"id": "2509.04582", "pdf": "https://arxiv.org/pdf/2509.04582", "abs": "https://arxiv.org/abs/2509.04582", "authors": ["Jingyi Lu", "Kai Han"], "title": "Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing via Bidirectional Warping", "categories": ["cs.CV", "I.3.6; I.3.3"], "comment": "Accepted to ICCV 2025. Project page:   https://visual-ai.github.io/inpaint4drag/", "summary": "Drag-based image editing has emerged as a powerful paradigm for intuitive image manipulation. However, existing approaches predominantly rely on manipulating the latent space of generative models, leading to limited precision, delayed feedback, and model-specific constraints. Accordingly, we present Inpaint4Drag, a novel framework that decomposes drag-based editing into pixel-space bidirectional warping and image inpainting. Inspired by elastic object deformation in the physical world, we treat image regions as deformable materials that maintain natural shape under user manipulation. Our method achieves real-time warping previews (0.01s) and efficient inpainting (0.3s) at 512x512 resolution, significantly improving the interaction experience compared to existing methods that require minutes per edit. By transforming drag inputs directly into standard inpainting formats, our approach serves as a universal adapter for any inpainting model without architecture modification, automatically inheriting all future improvements in inpainting technology. Extensive experiments demonstrate that our method achieves superior visual quality and precise control while maintaining real-time performance. Project page: https://visual-ai.github.io/inpaint4drag/", "AI": {"tldr": "Inpaint4Drag\u662f\u4e00\u4e2a\u57fa\u4e8e\u50cf\u7d20\u7a7a\u95f4\u53cc\u5411\u53d8\u5f62\u548c\u56fe\u50cf\u4fee\u590d\u7684\u62d6\u62fd\u5f0f\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u53d8\u5f62\u9884\u89c8\u548c\u9ad8\u6548\u4fee\u590d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u4e92\u4f53\u9a8c\u3002", "motivation": "\u73b0\u6709\u62d6\u62fd\u5f0f\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u751f\u6210\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u64cd\u4f5c\uff0c\u5b58\u5728\u7cbe\u5ea6\u6709\u9650\u3001\u53cd\u9988\u5ef6\u8fdf\u548c\u6a21\u578b\u7279\u5b9a\u9650\u5236\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u76f4\u63a5\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u62d6\u62fd\u7f16\u8f91\u5206\u89e3\u4e3a\u50cf\u7d20\u7a7a\u95f4\u53cc\u5411\u53d8\u5f62\u548c\u56fe\u50cf\u4fee\u590d\u4e24\u4e2a\u6b65\u9aa4\uff0c\u5c06\u56fe\u50cf\u533a\u57df\u89c6\u4e3a\u53ef\u53d8\u5f62\u6750\u6599\uff0c\u901a\u8fc7\u76f4\u63a5\u8f6c\u6362\u62d6\u62fd\u8f93\u5165\u4e3a\u6807\u51c6\u4fee\u590d\u683c\u5f0f\uff0c\u53ef\u4f5c\u4e3a\u4efb\u4f55\u4fee\u590d\u6a21\u578b\u7684\u901a\u7528\u9002\u914d\u5668\u3002", "result": "\u5728512x512\u5206\u8fa8\u7387\u4e0b\u5b9e\u73b0\u5b9e\u65f6\u53d8\u5f62\u9884\u89c8(0.01\u79d2)\u548c\u9ad8\u6548\u4fee\u590d(0.3\u79d2)\uff0c\u76f8\u6bd4\u9700\u8981\u6570\u5206\u949f\u7f16\u8f91\u7684\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4ea4\u4e92\u4f53\u9a8c\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5177\u6709\u4f18\u8d8a\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u7cbe\u786e\u63a7\u5236\u3002", "conclusion": "Inpaint4Drag\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65e0\u9700\u67b6\u6784\u4fee\u6539\u7684\u901a\u7528\u9002\u914d\u5668\u65b9\u6848\uff0c\u80fd\u591f\u81ea\u52a8\u7ee7\u627f\u672a\u6765\u4fee\u590d\u6280\u672f\u7684\u6240\u6709\u6539\u8fdb\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u62d6\u62fd\u5f0f\u56fe\u50cf\u7f16\u8f91\u3002"}}
{"id": "2509.04775", "pdf": "https://arxiv.org/pdf/2509.04775", "abs": "https://arxiv.org/abs/2509.04775", "authors": ["R. Makharia", "J. G. Singla", "Amitabh", "N. Dube", "H. Sharma"], "title": "Comparative Evaluation of Traditional and Deep Learning Feature Matching Algorithms using Chandrayaan-2 Lunar Data", "categories": ["cs.CV"], "comment": "27 pages, 11 figures, 3 tables", "summary": "Accurate image registration is critical for lunar exploration, enabling surface mapping, resource localization, and mission planning. Aligning data from diverse lunar sensors -- optical (e.g., Orbital High Resolution Camera, Narrow and Wide Angle Cameras), hyperspectral (Imaging Infrared Spectrometer), and radar (e.g., Dual-Frequency Synthetic Aperture Radar, Selene/Kaguya mission) -- is challenging due to differences in resolution, illumination, and sensor distortion. We evaluate five feature matching algorithms: SIFT, ASIFT, AKAZE, RIFT2, and SuperGlue (a deep learning-based matcher), using cross-modality image pairs from equatorial and polar regions. A preprocessing pipeline is proposed, including georeferencing, resolution alignment, intensity normalization, and enhancements like adaptive histogram equalization, principal component analysis, and shadow correction. SuperGlue consistently yields the lowest root mean square error and fastest runtimes. Classical methods such as SIFT and AKAZE perform well near the equator but degrade under polar lighting. The results highlight the importance of preprocessing and learning-based approaches for robust lunar image registration across diverse conditions.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u8bc4\u4f30\u4e86\u4e94\u79cd\u7279\u5f81\u5339\u914d\u7b97\u6cd5\u5728\u6708\u7403\u591a\u6a21\u6001\u56fe\u50cf\u6ce8\u518c\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5SuperGlue\u8868\u73b0\u6700\u4f18\uff0c\u7ecf\u5178\u7b97\u6cd5\u5728\u6781\u5730\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u5f3a\u8c03\u9884\u5904\u7406\u6d41\u7a0b\u548c\u5b66\u4e60\u65b9\u6cd5\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u6708\u7403\u56fe\u50cf\u6ce8\u518c\u5bf9\u4e8e\u8868\u9762\u6210\u56fe\u3001\u8d44\u6e90\u5b9a\u4f4d\u548c\u4efb\u52a1\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\u7684\u5bf9\u9f50\u9047\u5230\u4e86\u5206\u8fa8\u7387\u3001\u7167\u660e\u548c\u4f20\u611f\u5668\u5f62\u53d8\u7b49\u6311\u6218\u3002", "method": "\u8bc4\u4f30SIFT\u3001ASIFT\u3001AKAZE\u3001RIFT2\u548cSuperGlue\u4e94\u79cd\u7b97\u6cd5\uff0c\u4f7f\u7528\u8d64\u9053\u548c\u6781\u5730\u533a\u57df\u7684\u4ea4\u53c9\u6a21\u6001\u56fe\u50cf\u5bf9\u3002\u63d0\u51fa\u9884\u5904\u7406\u6d41\u7a0b\u5305\u62ec\u5730\u7406\u53c2\u8003\u3001\u5206\u8fa8\u7387\u5bf9\u9f50\u3001\u5f3a\u5ea6\u5f52\u4e00\u5316\u3001\u9002\u5e94\u6027\u76f4\u65b9\u56fe\u5747\u8861\u5316\u3001\u4e3b\u6210\u5206\u5206\u6790\u548c\u9634\u5f71\u7f29\u6b63\u3002", "result": "SuperGlue\u6301\u7eed\u83b7\u5f97\u6700\u4f4e\u7684\u6839\u5747\u65b9\u8bef\u5dee\u548c\u6700\u5feb\u7684\u8fd0\u884c\u65f6\u95f4\u3002SIFT\u548cAKAZE\u7b49\u7ecf\u5178\u65b9\u6cd5\u5728\u8d64\u9053\u9644\u8fd1\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6781\u5730\u5149\u7167\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u7ed3\u679c\u5f3a\u8c03\u4e86\u9884\u5904\u7406\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u5bf9\u4e8e\u5728\u591a\u6837\u5316\u6761\u4ef6\u4e0b\u5b9e\u73b0\u7a33\u5065\u7684\u6708\u7403\u56fe\u50cf\u6ce8\u518c\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.04824", "pdf": "https://arxiv.org/pdf/2509.04824", "abs": "https://arxiv.org/abs/2509.04824", "authors": ["Haosong Liu", "Xiancheng Zhu", "Huanqiang Zeng", "Jianqing Zhu", "Jiuwen Cao", "Junhui Hou"], "title": "Exploring Non-Local Spatial-Angular Correlations with a Hybrid Mamba-Transformer Framework for Light Field Super-Resolution", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recently, Mamba-based methods, with its advantage in long-range information modeling and linear complexity, have shown great potential in optimizing both computational cost and performance of light field image super-resolution (LFSR). However, current multi-directional scanning strategies lead to inefficient and redundant feature extraction when applied to complex LF data. To overcome this challenge, we propose a Subspace Simple Scanning (Sub-SS) strategy, based on which we design the Subspace Simple Mamba Block (SSMB) to achieve more efficient and precise feature extraction. Furthermore, we propose a dual-stage modeling strategy to address the limitation of state space in preserving spatial-angular and disparity information, thereby enabling a more comprehensive exploration of non-local spatial-angular correlations. Specifically, in stage I, we introduce the Spatial-Angular Residual Subspace Mamba Block (SA-RSMB) for shallow spatial-angular feature extraction; in stage II, we use a dual-branch parallel structure combining the Epipolar Plane Mamba Block (EPMB) and Epipolar Plane Transformer Block (EPTB) for deep epipolar feature refinement. Building upon meticulously designed modules and strategies, we introduce a hybrid Mamba-Transformer framework, termed LFMT. LFMT integrates the strengths of Mamba and Transformer models for LFSR, enabling comprehensive information exploration across spatial, angular, and epipolar-plane domains. Experimental results demonstrate that LFMT significantly outperforms current state-of-the-art methods in LFSR, achieving substantial improvements in performance while maintaining low computational complexity on both real-word and synthetic LF datasets.", "AI": {"tldr": "\u63d0\u51faLFMT\u6846\u67b6\uff0c\u7ed3\u5408Mamba\u548cTransformer\u4f18\u52bf\uff0c\u901a\u8fc7\u5b50\u7a7a\u95f4\u7b80\u5355\u626b\u63cf\u7b56\u7565\u548c\u53cc\u9636\u6bb5\u5efa\u6a21\uff0c\u5728\u5149\u573a\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7279\u5f81\u63d0\u53d6\u548c\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709Mamba\u65b9\u6cd5\u5728\u5149\u573a\u8d85\u5206\u8fa8\u7387\u4e2d\u5b58\u5728\u591a\u65b9\u5411\u626b\u63cf\u7b56\u7565\u6548\u7387\u4f4e\u4e0b\u548c\u7279\u5f81\u5197\u4f59\u95ee\u9898\uff0c\u4e14\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5728\u4fdd\u6301\u7a7a\u95f4-\u89d2\u5ea6\u548c\u89c6\u5dee\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u63d0\u51fa\u5b50\u7a7a\u95f4\u7b80\u5355\u626b\u63cf\u7b56\u7565(Sub-SS)\u548c\u5b50\u7a7a\u95f4\u7b80\u5355Mamba\u5757(SSMB)\uff1b\u91c7\u7528\u53cc\u9636\u6bb5\u5efa\u6a21\uff1a\u9636\u6bb5I\u4f7f\u7528\u7a7a\u95f4-\u89d2\u5ea6\u6b8b\u5dee\u5b50\u7a7a\u95f4Mamba\u5757(SA-RSMB)\u8fdb\u884c\u6d45\u5c42\u7279\u5f81\u63d0\u53d6\uff0c\u9636\u6bb5II\u4f7f\u7528\u53cc\u5206\u652f\u5e76\u884c\u7ed3\u6784\u7ed3\u5408\u6781\u5e73\u9762Mamba\u5757(EPMB)\u548c\u6781\u5e73\u9762Transformer\u5757(EPTB)\u8fdb\u884c\u6df1\u5ea6\u7279\u5f81\u7cbe\u70bc\u3002", "result": "\u5728\u771f\u5b9e\u548c\u5408\u6210\u5149\u573a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6027\u80fd\u7684\u5b9e\u8d28\u6027\u63d0\u5347\u3002", "conclusion": "LFMT\u6846\u67b6\u6210\u529f\u6574\u5408\u4e86Mamba\u548cTransformer\u7684\u4f18\u52bf\uff0c\u80fd\u591f\u5168\u9762\u63a2\u7d22\u7a7a\u95f4\u3001\u89d2\u5ea6\u548c\u6781\u5e73\u9762\u57df\u7684\u4fe1\u606f\uff0c\u4e3a\u5149\u573a\u8d85\u5206\u8fa8\u7387\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.04859", "pdf": "https://arxiv.org/pdf/2509.04859", "abs": "https://arxiv.org/abs/2509.04859", "authors": ["Hannah Schieber", "Dominik Frischmann", "Simon Boche", "Victor Schaack", "Angela Schoellig", "Stefan Leutenegger", "Daniel Roth"], "title": "CoRe-GS: Coarse-to-Refined Gaussian Splatting with Semantic Object Focus", "categories": ["cs.CV"], "comment": null, "summary": "Mobile reconstruction for autonomous aerial robotics holds strong potential for critical applications such as tele-guidance and disaster response. These tasks demand both accurate 3D reconstruction and fast scene processing. Instead of reconstructing the entire scene in detail, it is often more efficient to focus on specific objects, i.e., points of interest (PoIs). Mobile robots equipped with advanced sensing can usually detect these early during data acquisition or preliminary analysis, reducing the need for full-scene optimization. Gaussian Splatting (GS) has recently shown promise in delivering high-quality novel view synthesis and 3D representation by an incremental learning process. Extending GS with scene editing, semantics adds useful per-splat features to isolate objects effectively.   Semantic 3D Gaussian editing can already be achieved before the full training cycle is completed, reducing the overall training time. Moreover, the semantically relevant area, the PoI, is usually already known during capturing. To balance high-quality reconstruction with reduced training time, we propose CoRe-GS. We first generate a coarse segmentation-ready scene with semantic GS and then refine it for the semantic object using our novel color-based effective filtering for effective object isolation. This is speeding up the training process to be about a quarter less than a full training cycle for semantic GS. We evaluate our approach on two datasets, SCRREAM (real-world, outdoor) and NeRDS 360 (synthetic, indoor), showing reduced runtime and higher novel-view-synthesis quality.", "AI": {"tldr": "CoRe-GS\u662f\u4e00\u79cd\u7528\u4e8e\u79fb\u52a8\u81ea\u4e3b\u7a7a\u4e2d\u673a\u5668\u4eba\u7684\u8bed\u4e493D\u9ad8\u65af\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u7c97\u7c92\u5ea6\u8bed\u4e49\u5206\u5272\u548c\u989c\u8272\u8fc7\u6ee4\u5feb\u901f\u9694\u79bb\u611f\u5174\u8da3\u5bf9\u8c61\uff0c\u5c06\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u7ea6\u56db\u5206\u4e4b\u4e00\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u65b0\u89c6\u89d2\u5408\u6210\u6548\u679c\u3002", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u5e94\u7528\u5982\u8fdc\u7a0b\u6307\u5bfc\u548c\u707e\u96be\u54cd\u5e94\u9700\u8981\u5feb\u901f\u51c6\u786e\u76843D\u91cd\u5efa\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u91cd\u5efa\u6574\u4e2a\u573a\u666f\u6548\u7387\u4f4e\u4e0b\u3002\u901a\u8fc7\u4e13\u6ce8\u4e8e\u7279\u5b9a\u611f\u5174\u8da3\u5bf9\u8c61(PoIs)\uff0c\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u9700\u6c42\u548c\u63d0\u9ad8\u5904\u7406\u901f\u5ea6\u3002", "method": "\u9996\u5148\u4f7f\u7528\u8bed\u4e49\u9ad8\u65af\u6cfc\u6e85\u751f\u6210\u7c97\u7c92\u5ea6\u53ef\u5206\u5272\u573a\u666f\uff0c\u7136\u540e\u901a\u8fc7\u65b0\u9896\u7684\u57fa\u4e8e\u989c\u8272\u7684\u6709\u6548\u8fc7\u6ee4\u65b9\u6cd5\u5bf9\u8bed\u4e49\u5bf9\u8c61\u8fdb\u884c\u7cbe\u7ec6\u5316\u5904\u7406\uff0c\u5b9e\u73b0\u6709\u6548\u7684\u5bf9\u8c61\u9694\u79bb\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6(SCRREAM\u771f\u5b9e\u6237\u5916\u548cNeRDS 360\u5408\u6210\u5ba4\u5185)\u4e0a\u8bc4\u4f30\u663e\u793a\uff0c\u8fd0\u884c\u65f6\u95f4\u51cf\u5c11\uff0c\u65b0\u89c6\u89d2\u5408\u6210\u8d28\u91cf\u63d0\u9ad8\uff0c\u8bad\u7ec3\u65f6\u95f4\u6bd4\u5b8c\u6574\u8bed\u4e49\u9ad8\u65af\u6cfc\u6e85\u8bad\u7ec3\u5468\u671f\u51cf\u5c11\u7ea6\u56db\u5206\u4e4b\u4e00\u3002", "conclusion": "CoRe-GS\u65b9\u6cd5\u5728\u4fdd\u6301\u91cd\u5efa\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\uff0c\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8bed\u4e493D\u91cd\u5efa\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.04957", "pdf": "https://arxiv.org/pdf/2509.04957", "abs": "https://arxiv.org/abs/2509.04957", "authors": ["Gehui Chen", "Guan'an Wang", "Xiaowen Huang", "Jitao Sang"], "title": "Efficient Video-to-Audio Generation via Multiple Foundation Models Mapper", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "comment": null, "summary": "Recent Video-to-Audio (V2A) generation relies on extracting semantic and temporal features from video to condition generative models. Training these models from scratch is resource intensive. Consequently, leveraging foundation models (FMs) has gained traction due to their cross-modal knowledge transfer and generalization capabilities. One prior work has explored fine-tuning a lightweight mapper network to connect a pre-trained visual encoder with a text-to-audio generation model for V2A. Inspired by this, we introduce the Multiple Foundation Model Mapper (MFM-Mapper). Compared to the previous mapper approach, MFM-Mapper benefits from richer semantic and temporal information by fusing features from dual visual encoders. Furthermore, by replacing a linear mapper with GPT-2, MFM-Mapper improves feature alignment, drawing parallels between cross-modal features mapping and autoregressive translation tasks. Our MFM-Mapper exhibits remarkable training efficiency. It achieves better performance in semantic and temporal consistency with fewer training consuming, requiring only 16\\% of the training scale compared to previous mapper-based work, yet achieves competitive performance with models trained on a much larger scale.", "AI": {"tldr": "MFM-Mapper\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u591a\u6a21\u6001\u6620\u5c04\u5668\uff0c\u901a\u8fc7\u878d\u5408\u53cc\u89c6\u89c9\u7f16\u7801\u5668\u7279\u5f81\u548c\u4f7f\u7528GPT-2\u8fdb\u884c\u7279\u5f81\u5bf9\u9f50\uff0c\u663e\u8457\u51cf\u5c11\u4e86V2A\u751f\u6210\u7684\u8bad\u7ec3\u6210\u672c\uff0c\u4ec5\u9700\u4e4b\u524d\u65b9\u6cd516%\u7684\u8bad\u7ec3\u91cf\u5c31\u80fd\u8fbe\u5230\u7ade\u4e89\u6027\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u5230\u97f3\u9891\u751f\u6210\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8d44\u6e90\u8bad\u7ec3\uff0c\u800c\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u6620\u5c04\u65b9\u6cd5\u867d\u7136\u51cf\u5c11\u4e86\u8bad\u7ec3\u6210\u672c\uff0c\u4f46\u8bed\u4e49\u548c\u65f6\u95f4\u4fe1\u606f\u63d0\u53d6\u4e0d\u591f\u5145\u5206\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u878d\u5408\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u7684\u7279\u5f81\u6765\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u8bad\u7ec3\u6548\u7387\u3002", "method": "\u63d0\u51faMFM-Mapper\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u53cc\u89c6\u89c9\u7f16\u7801\u5668\u878d\u5408\u66f4\u4e30\u5bcc\u7684\u8bed\u4e49\u548c\u65f6\u95f4\u7279\u5f81\uff1b2\uff09\u7528GPT-2\u66ff\u4ee3\u7ebf\u6027\u6620\u5c04\u5668\uff0c\u5c06\u8de8\u6a21\u6001\u7279\u5f81\u6620\u5c04\u89c6\u4e3a\u81ea\u56de\u5f52\u7ffb\u8bd1\u4efb\u52a1\uff1b3\uff09\u4fdd\u6301\u8f7b\u91cf\u7ea7\u6620\u5c04\u7f51\u7edc\u67b6\u6784\u3002", "result": "\u65b9\u6cd5\u4ec5\u9700\u4e4b\u524d\u6620\u5c04\u5668\u65b9\u6cd516%\u7684\u8bad\u7ec3\u89c4\u6a21\uff0c\u5728\u8bed\u4e49\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u8fbe\u5230\u4e86\u4e0e\u66f4\u5927\u89c4\u6a21\u8bad\u7ec3\u6a21\u578b\u76f8\u7ade\u4e89\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "MFM-Mapper\u901a\u8fc7\u6709\u6548\u5229\u7528\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u7684\u7279\u5f81\u878d\u5408\u548cGPT-2\u7684\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\uff0c\u5728\u89c6\u9891\u5230\u97f3\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\u7684\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2509.05030", "pdf": "https://arxiv.org/pdf/2509.05030", "abs": "https://arxiv.org/abs/2509.05030", "authors": ["Cong Cao", "Xianhang Cheng", "Jingyuan Liu", "Yujian Zheng", "Zhenhui Lin", "Meriem Chkir", "Hao Li"], "title": "LUIVITON: Learned Universal Interoperable VIrtual Try-ON", "categories": ["cs.CV"], "comment": null, "summary": "We present LUIVITON, an end-to-end system for fully automated virtual try-on, capable of draping complex, multi-layer clothing onto diverse and arbitrarily posed humanoid characters. To address the challenge of aligning complex garments with arbitrary and highly diverse body shapes, we use SMPL as a proxy representation and separate the clothing-to-body draping problem into two correspondence tasks: 1) clothing-to-SMPL and 2) body-to-SMPL correspondence, where each has its unique challenges. While we address the clothing-to-SMPL fitting problem using a geometric learning-based approach for partial-to-complete shape correspondence prediction, we introduce a diffusion model-based approach for body-to-SMPL correspondence using multi-view consistent appearance features and a pre-trained 2D foundation model. Our method can handle complex geometries, non-manifold meshes, and generalizes effectively to a wide range of humanoid characters -- including humans, robots, cartoon subjects, creatures, and aliens, while maintaining computational efficiency for practical adoption. In addition to offering a fully automatic fitting solution, LUIVITON supports fast customization of clothing size, allowing users to adjust clothing sizes and material properties after they have been draped. We show that our system can produce high-quality 3D clothing fittings without any human labor, even when 2D clothing sewing patterns are not available.", "AI": {"tldr": "LUIVITON\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5168\u81ea\u52a8\u865a\u62df\u8bd5\u7a7f\u7cfb\u7edf\uff0c\u80fd\u591f\u5c06\u590d\u6742\u7684\u591a\u5c42\u670d\u88c5\u8986\u76d6\u5230\u591a\u6837\u5316\u4e14\u4efb\u610f\u59ff\u6001\u7684\u4eba\u5f62\u89d2\u8272\u4e0a\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u5373\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u76843D\u670d\u88c5\u62df\u5408\u6548\u679c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u590d\u6742\u670d\u88c5\u4e0e\u4efb\u610f\u591a\u6837\u5316\u4f53\u578b\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5904\u7406\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u3001\u975e\u6d41\u5f62\u7f51\u683c\u4ee5\u53ca\u6cdb\u5316\u5230\u5e7f\u6cdb\u4eba\u5f62\u89d2\u8272\u7684\u9700\u6c42\u3002", "method": "\u4f7f\u7528SMPL\u4f5c\u4e3a\u4ee3\u7406\u8868\u793a\uff0c\u5c06\u670d\u88c5\u5230\u8eab\u4f53\u7684\u8986\u76d6\u95ee\u9898\u5206\u89e3\u4e3a\u4e24\u4e2a\u5bf9\u5e94\u4efb\u52a1\uff1a1\uff09\u57fa\u4e8e\u51e0\u4f55\u5b66\u4e60\u7684\u670d\u88c5\u5230SMPL\u5bf9\u5e94\u65b9\u6cd5\uff0c2\uff09\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8eab\u4f53\u5230SMPL\u5bf9\u5e94\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u89c6\u89d2\u4e00\u81f4\u7684\u5916\u89c2\u7279\u5f81\u548c\u9884\u8bad\u7ec3\u76842D\u57fa\u7840\u6a21\u578b\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u5904\u7406\u590d\u6742\u51e0\u4f55\u5f62\u72b6\uff0c\u6709\u6548\u6cdb\u5316\u5230\u5404\u79cd\u4eba\u5f62\u89d2\u8272\uff08\u5305\u62ec\u4eba\u7c7b\u3001\u673a\u5668\u4eba\u3001\u5361\u901a\u89d2\u8272\u3001\u751f\u7269\u548c\u5916\u661f\u4eba\uff09\uff0c\u5e76\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\uff0c\u652f\u6301\u670d\u88c5\u5c3a\u5bf8\u548c\u6750\u8d28\u7684\u5feb\u901f\u5b9a\u5236\u3002", "conclusion": "LUIVITON\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u670d\u88c5\u62df\u5408\u89e3\u51b3\u65b9\u6848\uff0c\u5373\u4f7f\u57282D\u670d\u88c5\u7f1d\u5236\u56fe\u6848\u4e0d\u53ef\u7528\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u80fd\u4ea7\u751f\u9ad8\u8d28\u91cf\u76843D\u670d\u88c5\u62df\u5408\u6548\u679c\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.05075", "pdf": "https://arxiv.org/pdf/2509.05075", "abs": "https://arxiv.org/abs/2509.05075", "authors": ["Yangming Li", "Chaoyu Liu", "Lihao Liu", "Simon Masnou", "Carola-Bibian Sch\u00f6nlieb"], "title": "GeoSplat: A Deep Dive into Geometry-Constrained Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "A few recent works explored incorporating geometric priors to regularize the optimization of Gaussian splatting, further improving its performance. However, those early studies mainly focused on the use of low-order geometric priors (e.g., normal vector), and they are also unreliably estimated by noise-sensitive methods, like local principal component analysis. To address their limitations, we first present GeoSplat, a general geometry-constrained optimization framework that exploits both first-order and second-order geometric quantities to improve the entire training pipeline of Gaussian splatting, including Gaussian initialization, gradient update, and densification. As an example, we initialize the scales of 3D Gaussian primitives in terms of principal curvatures, leading to a better coverage of the object surface than random initialization. Secondly, based on certain geometric structures (e.g., local manifold), we introduce efficient and noise-robust estimation methods that provide dynamic geometric priors for our framework. We conduct extensive experiments on multiple datasets for novel view synthesis, showing that our framework: GeoSplat, significantly improves the performance of Gaussian splatting and outperforms previous baselines.", "AI": {"tldr": "GeoSplat\u662f\u4e00\u4e2a\u51e0\u4f55\u7ea6\u675f\u4f18\u5316\u6846\u67b6\uff0c\u5229\u7528\u4e00\u9636\u548c\u4e8c\u9636\u51e0\u4f55\u91cf\u6539\u8fdb\u9ad8\u65af\u6e85\u5c04\u7684\u6574\u4e2a\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5305\u62ec\u521d\u59cb\u5316\u3001\u68af\u5ea6\u66f4\u65b0\u548c\u81f4\u5bc6\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528\u4f4e\u9636\u51e0\u4f55\u5148\u9a8c\uff08\u5982\u6cd5\u5411\u91cf\uff09\uff0c\u4e14\u901a\u8fc7\u566a\u58f0\u654f\u611f\u65b9\u6cd5\u4f30\u8ba1\u4e0d\u53ef\u9760\u3002\u9700\u8981\u66f4\u9c81\u68d2\u7684\u51e0\u4f55\u7ea6\u675f\u6765\u6539\u8fdb\u9ad8\u65af\u6e85\u5c04\u4f18\u5316", "method": "\u63d0\u51fa\u51e0\u4f55\u7ea6\u675f\u4f18\u5316\u6846\u67b6\uff0c\u5229\u7528\u4e3b\u66f2\u7387\u521d\u59cb\u5316\u9ad8\u65af\u5c3a\u5ea6\uff0c\u57fa\u4e8e\u5c40\u90e8\u6d41\u5f62\u7b49\u51e0\u4f55\u7ed3\u6784\u5f15\u5165\u9ad8\u6548\u4e14\u566a\u58f0\u9c81\u68d2\u7684\u4f30\u8ba1\u65b9\u6cd5\uff0c\u63d0\u4f9b\u52a8\u6001\u51e0\u4f55\u5148\u9a8c", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u65b0\u9896\u89c6\u56fe\u5408\u6210\u5b9e\u9a8c\u4e2d\uff0cGeoSplat\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u65af\u6e85\u5c04\u6027\u80fd\uff0c\u4f18\u4e8e\u4e4b\u524d\u7684\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "GeoSplat\u901a\u8fc7\u5f15\u5165\u9ad8\u9636\u51e0\u4f55\u7ea6\u675f\u548c\u9c81\u68d2\u4f30\u8ba1\u65b9\u6cd5\uff0c\u6709\u6548\u6539\u8fdb\u4e86\u9ad8\u65af\u6e85\u5c04\u7684\u4f18\u5316\u8fc7\u7a0b\uff0c\u4e3a3D\u91cd\u5efa\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u51e0\u4f55\u5148\u9a8c"}}
{"id": "2509.04719", "pdf": "https://arxiv.org/pdf/2509.04719", "abs": "https://arxiv.org/abs/2509.04719", "authors": ["Han Liang", "Jiahui Zhou", "Zicheng Zhou", "Xiaoxi Zhang", "Xu Chen"], "title": "STADI: Fine-Grained Step-Patch Diffusion Parallelism for Heterogeneous GPUs", "categories": ["cs.DC", "cs.CV"], "comment": null, "summary": "The escalating adoption of diffusion models for applications such as image generation demands efficient parallel inference techniques to manage their substantial computational cost. However, existing diffusion parallelism inference schemes often underutilize resources in heterogeneous multi-GPU environments, where varying hardware capabilities or background tasks cause workload imbalance. This paper introduces Spatio-Temporal Adaptive Diffusion Inference (STADI), a novel framework to accelerate diffusion model inference in such settings. At its core is a hybrid scheduler that orchestrates fine-grained parallelism across both temporal and spatial dimensions. Temporally, STADI introduces a novel computation-aware step allocator applied after warmup phases, using a least-common-multiple-minimizing quantization technique to reduce denoising steps on slower GPUs and execution synchronization. To further minimize GPU idle periods, STADI executes an elastic patch parallelism mechanism that allocates variably sized image patches to GPUs according to their computational capability, ensuring balanced workload distribution through a complementary spatial mechanism. Extensive experiments on both load-imbalanced and heterogeneous multi-GPU clusters validate STADI's efficacy, demonstrating improved load balancing and mitigation of performance bottlenecks. Compared to patch parallelism, a state-of-the-art diffusion inference framework, our method significantly reduces end-to-end inference latency by up to 45% and significantly improves resource utilization on heterogeneous GPUs.", "AI": {"tldr": "STADI\u662f\u4e00\u4e2a\u9488\u5bf9\u5f02\u6784\u591aGPU\u73af\u5883\u7684\u6269\u6563\u6a21\u578b\u63a8\u7406\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u7a7a\u81ea\u9002\u5e94\u8c03\u5ea6\u673a\u5236\u663e\u8457\u51cf\u5c11\u63a8\u7406\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u8d44\u6e90\u5229\u7528\u7387", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5e76\u884c\u63a8\u7406\u65b9\u6848\u5728\u5f02\u6784\u591aGPU\u73af\u5883\u4e2d\u8d44\u6e90\u5229\u7528\u4e0d\u8db3\uff0c\u786c\u4ef6\u80fd\u529b\u5dee\u5f02\u6216\u540e\u53f0\u4efb\u52a1\u5bfc\u81f4\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u5e73\u8861", "method": "\u91c7\u7528\u6df7\u5408\u8c03\u5ea6\u5668\u5b9e\u73b0\u65f6\u7a7a\u7ef4\u5ea6\u7684\u7ec6\u7c92\u5ea6\u5e76\u884c\uff1a\u65f6\u95f4\u7ef4\u5ea6\u4f7f\u7528\u8ba1\u7b97\u611f\u77e5\u7684\u6b65\u6570\u5206\u914d\u5668\u548c\u6700\u5c0f\u516c\u500d\u6570\u6700\u5c0f\u5316\u91cf\u5316\u6280\u672f\uff1b\u7a7a\u95f4\u7ef4\u5ea6\u4f7f\u7528\u5f39\u6027\u8865\u4e01\u5e76\u884c\u673a\u5236\u6309GPU\u8ba1\u7b97\u80fd\u529b\u5206\u914d\u4e0d\u540c\u5927\u5c0f\u7684\u56fe\u50cf\u5757", "result": "\u5728\u8d1f\u8f7d\u4e0d\u5e73\u8861\u548c\u5f02\u6784\u591aGPU\u96c6\u7fa4\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86STADI\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u8865\u4e01\u5e76\u884c\u6846\u67b6\uff0c\u7aef\u5230\u7aef\u63a8\u7406\u5ef6\u8fdf\u6700\u591a\u51cf\u5c1145%\uff0c\u5f02\u6784GPU\u8d44\u6e90\u5229\u7528\u7387\u663e\u8457\u63d0\u9ad8", "conclusion": "STADI\u901a\u8fc7\u65f6\u7a7a\u81ea\u9002\u5e94\u8c03\u5ea6\u6210\u529f\u89e3\u51b3\u4e86\u5f02\u6784\u73af\u5883\u4e2d\u6269\u6563\u6a21\u578b\u63a8\u7406\u7684\u8d1f\u8f7d\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u5e76\u884c\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.04819", "pdf": "https://arxiv.org/pdf/2509.04819", "abs": "https://arxiv.org/abs/2509.04819", "authors": ["Shuhan Ding", "Jingjing Fu", "Yu Gu", "Naiteek Sangani", "Mu Wei", "Paul Vozila", "Nan Liu", "Jiang Bian", "Hoifung Poon"], "title": "AURAD: Anatomy-Pathology Unified Radiology Synthesis with Progressive Representations", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image synthesis has become an essential strategy for augmenting datasets and improving model generalization in data-scarce clinical settings. However, fine-grained and controllable synthesis remains difficult due to limited high-quality annotations and domain shifts across datasets. Existing methods, often designed for natural images or well-defined tumors, struggle to generalize to chest radiographs, where disease patterns are morphologically diverse and tightly intertwined with anatomical structures. To address these challenges, we propose AURAD, a controllable radiology synthesis framework that jointly generates high-fidelity chest X-rays and pseudo semantic masks. Unlike prior approaches that rely on randomly sampled masks-limiting diversity, controllability, and clinical relevance-our method learns to generate masks that capture multi-pathology coexistence and anatomical-pathological consistency. It follows a progressive pipeline: pseudo masks are first generated from clinical prompts conditioned on anatomical structures, and then used to guide image synthesis. We also leverage pretrained expert medical models to filter outputs and ensure clinical plausibility. Beyond visual realism, the synthesized masks also serve as labels for downstream tasks such as detection and segmentation, bridging the gap between generative modeling and real-world clinical applications. Extensive experiments and blinded radiologist evaluations demonstrate the effectiveness and generalizability of our method across tasks and datasets. In particular, 78% of our synthesized images are classified as authentic by board-certified radiologists, and over 40% of predicted segmentation overlays are rated as clinically useful. All code, pre-trained models, and the synthesized dataset will be released upon publication.", "AI": {"tldr": "AURAD\u662f\u4e00\u4e2a\u53ef\u63a7\u7684\u653e\u5c04\u5b66\u5408\u6210\u6846\u67b6\uff0c\u80fd\u591f\u8054\u5408\u751f\u6210\u9ad8\u4fdd\u771f\u80f8\u90e8X\u5149\u7247\u548c\u4f2a\u8bed\u4e49\u63a9\u7801\uff0c\u901a\u8fc7\u4e34\u5e8a\u63d0\u793a\u751f\u6210\u89e3\u5256\u7ed3\u6784\u76f8\u5173\u7684\u591a\u75c5\u7406\u5171\u5b58\u63a9\u7801\u6765\u6307\u5bfc\u56fe\u50cf\u5408\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5408\u6210\u7684\u53ef\u63a7\u6027\u548c\u4e34\u5e8a\u76f8\u5173\u6027\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5408\u6210\u4e2d\u7531\u4e8e\u9ad8\u8d28\u91cf\u6807\u6ce8\u6709\u9650\u548c\u6570\u636e\u96c6\u95f4\u57df\u504f\u79fb\u5bfc\u81f4\u7684\u7ec6\u7c92\u5ea6\u53ef\u63a7\u5408\u6210\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u80f8\u90e8X\u5149\u7247\u4e2d\u75be\u75c5\u6a21\u5f0f\u5f62\u6001\u591a\u6837\u4e14\u4e0e\u89e3\u5256\u7ed3\u6784\u7d27\u5bc6\u4ea4\u7ec7\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u6e10\u8fdb\u5f0f\u6d41\u7a0b\uff1a\u9996\u5148\u57fa\u4e8e\u89e3\u5256\u7ed3\u6784\u6761\u4ef6\u4ece\u4e34\u5e8a\u63d0\u793a\u751f\u6210\u4f2a\u63a9\u7801\uff0c\u7136\u540e\u7528\u8fd9\u4e9b\u63a9\u7801\u6307\u5bfc\u56fe\u50cf\u5408\u6210\uff1b\u5229\u7528\u9884\u8bad\u7ec3\u4e13\u5bb6\u533b\u5b66\u6a21\u578b\u8fc7\u6ee4\u8f93\u51fa\u4ee5\u786e\u4fdd\u4e34\u5e8a\u5408\u7406\u6027\uff1b\u751f\u6210\u7684\u63a9\u7801\u8fd8\u53ef\u4f5c\u4e3a\u4e0b\u6e38\u4efb\u52a1\u7684\u6807\u7b7e\u3002", "result": "78%\u7684\u5408\u6210\u56fe\u50cf\u88ab\u8ba4\u8bc1\u653e\u5c04\u79d1\u533b\u751f\u5206\u7c7b\u4e3a\u771f\u5b9e\u56fe\u50cf\uff0c\u8d85\u8fc740%\u7684\u9884\u6d4b\u5206\u5272\u8986\u76d6\u88ab\u8bc4\u5b9a\u4e3a\u4e34\u5e8a\u6709\u7528\uff1b\u5728\u591a\u4e2a\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AURAD\u6846\u67b6\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u89c6\u89c9\u771f\u5b9e\u6027\uff0c\u8fd8\u901a\u8fc7\u751f\u6210\u7684\u63a9\u7801\u8fde\u63a5\u4e86\u751f\u6210\u5efa\u6a21\u4e0e\u771f\u5b9e\u4e16\u754c\u4e34\u5e8a\u5e94\u7528\uff0c\u4e3a\u6570\u636e\u7a00\u7f3a\u7684\u4e34\u5e8a\u73af\u5883\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.04870", "pdf": "https://arxiv.org/pdf/2509.04870", "abs": "https://arxiv.org/abs/2509.04870", "authors": ["Yuanyuan Gui", "Wei Li", "Yinjian Wang", "Xiang-Gen Xia", "Mauro Marty", "Christian Ginzler", "Zuyuan Wang"], "title": "Multi-modal Uncertainty Robust Tree Cover Segmentation For High-Resolution Remote Sensing Images", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Recent advances in semantic segmentation of multi-modal remote sensing images have significantly improved the accuracy of tree cover mapping, supporting applications in urban planning, forest monitoring, and ecological assessment. Integrating data from multiple modalities-such as optical imagery, light detection and ranging (LiDAR), and synthetic aperture radar (SAR)-has shown superior performance over single-modality methods. However, these data are often acquired days or even months apart, during which various changes may occur, such as vegetation disturbances (e.g., logging, and wildfires) and variations in imaging quality. Such temporal misalignments introduce cross-modal uncertainty, especially in high-resolution imagery, which can severely degrade segmentation accuracy. To address this challenge, we propose MURTreeFormer, a novel multi-modal segmentation framework that mitigates and leverages aleatoric uncertainty for robust tree cover mapping. MURTreeFormer treats one modality as primary and others as auxiliary, explicitly modeling patch-level uncertainty in the auxiliary modalities via a probabilistic latent representation. Uncertain patches are identified and reconstructed from the primary modality's distribution through a VAE-based resampling mechanism, producing enhanced auxiliary features for fusion. In the decoder, a gradient magnitude attention (GMA) module and a lightweight refinement head (RH) are further integrated to guide attention toward tree-like structures and to preserve fine-grained spatial details. Extensive experiments on multi-modal datasets from Shanghai and Zurich demonstrate that MURTreeFormer significantly improves segmentation performance and effectively reduces the impact of temporally induced aleatoric uncertainty.", "AI": {"tldr": "MURTreeFormer\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u6a21\u6001\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u6982\u7387\u6f5c\u5728\u8868\u793a\u548cVAE\u91cd\u91c7\u6837\u673a\u5236\u5904\u7406\u591a\u6a21\u6001\u9065\u611f\u56fe\u50cf\u4e2d\u7684\u65f6\u95f4\u9519\u4f4d\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6811\u51a0\u8986\u76d6\u5206\u5272\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u9065\u611f\u56fe\u50cf\uff08\u5149\u5b66\u3001LiDAR\u3001SAR\uff09\u901a\u5e38\u5728\u4e0d\u540c\u65f6\u95f4\u91c7\u96c6\uff0c\u65f6\u95f4\u9519\u4f4d\u4f1a\u5bfc\u81f4\u690d\u88ab\u6270\u52a8\u548c\u6210\u50cf\u8d28\u91cf\u53d8\u5316\uff0c\u5f15\u5165\u8de8\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\uff0c\u4e25\u91cd\u5f71\u54cd\u5206\u5272\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faMURTreeFormer\u6846\u67b6\uff1a1\uff09\u5c06\u4e00\u79cd\u6a21\u6001\u4f5c\u4e3a\u4e3b\u8981\u6a21\u6001\uff0c\u5176\u4ed6\u4f5c\u4e3a\u8f85\u52a9\u6a21\u6001\uff1b2\uff09\u901a\u8fc7\u6982\u7387\u6f5c\u5728\u8868\u793a\u5efa\u6a21\u8f85\u52a9\u6a21\u6001\u7684\u8865\u4e01\u7ea7\u4e0d\u786e\u5b9a\u6027\uff1b3\uff09\u4f7f\u7528VAE\u91cd\u91c7\u6837\u673a\u5236\u91cd\u5efa\u4e0d\u786e\u5b9a\u8865\u4e01\uff1b4\uff09\u5728\u89e3\u7801\u5668\u4e2d\u96c6\u6210\u68af\u5ea6\u5e45\u5ea6\u6ce8\u610f\u529b\u6a21\u5757\u548c\u8f7b\u91cf\u7ea7\u7ec6\u5316\u5934\u3002", "result": "\u5728\u4e0a\u6d77\u548c\u82cf\u9ece\u4e16\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660eMURTreeFormer\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u5272\u6027\u80fd\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u65f6\u95f4\u5f15\u8d77\u7684\u968f\u673a\u4e0d\u786e\u5b9a\u6027\u5f71\u54cd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u9065\u611f\u56fe\u50cf\u4e2d\u7684\u65f6\u95f4\u9519\u4f4d\u95ee\u9898\uff0c\u4e3a\u6811\u51a0\u8986\u76d6\u5236\u56fe\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u57ce\u89c4\u3001\u68ee\u6797\u76d1\u6d4b\u548c\u751f\u6001\u8bc4\u4f30\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.05154", "pdf": "https://arxiv.org/pdf/2509.05154", "abs": "https://arxiv.org/abs/2509.05154", "authors": ["Julia Dietlmeier", "Oluwabukola Grace Adegboro", "Vayangi Ganepola", "Claudia Mazo", "Noel E. O'Connor"], "title": "VLSM-Ensemble: Ensembling CLIP-based Vision-Language Models for Enhanced Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "Medical Imaging with Deep Learning (MIDL 2025) short paper", "summary": "Vision-language models and their adaptations to image segmentation tasks present enormous potential for producing highly accurate and interpretable results. However, implementations based on CLIP and BiomedCLIP are still lagging behind more sophisticated architectures such as CRIS. In this work, instead of focusing on text prompt engineering as is the norm, we attempt to narrow this gap by showing how to ensemble vision-language segmentation models (VLSMs) with a low-complexity CNN. By doing so, we achieve a significant Dice score improvement of 6.3% on the BKAI polyp dataset using the ensembled BiomedCLIPSeg, while other datasets exhibit gains ranging from 1% to 6%. Furthermore, we provide initial results on additional four radiology and non-radiology datasets. We conclude that ensembling works differently across these datasets (from outperforming to underperforming the CRIS model), indicating a topic for future investigation by the community. The code is available at https://github.com/juliadietlmeier/VLSM-Ensemble.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u89c6\u89c9\u8bed\u8a00\u5206\u5272\u6a21\u578b(VLSMs)\u4e0e\u4f4e\u590d\u6742\u5ea6CNN\u96c6\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\uff0c\u5728BKAI\u606f\u8089\u6570\u636e\u96c6\u4e0aDice\u5206\u6570\u63d0\u9ad8\u4e866.3%\uff0c\u5176\u4ed6\u6570\u636e\u96c6\u4e5f\u67091-6%\u7684\u63d0\u5347", "motivation": "\u5f53\u524d\u57fa\u4e8eCLIP\u548cBiomedCLIP\u7684\u89c6\u89c9\u8bed\u8a00\u5206\u5272\u6a21\u578b\u6027\u80fd\u4ecd\u843d\u540e\u4e8eCRIS\u7b49\u590d\u6742\u67b6\u6784\uff0c\u5e0c\u671b\u901a\u8fc7\u6a21\u578b\u96c6\u6210\u65b9\u6cd5\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd", "method": "\u91c7\u7528\u89c6\u89c9\u8bed\u8a00\u5206\u5272\u6a21\u578b(VLSMs)\u4e0e\u4f4e\u590d\u6742\u5ea6CNN\u8fdb\u884c\u96c6\u6210\uff0c\u800c\u975e\u4f20\u7edf\u7684\u6587\u672c\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5", "result": "\u5728BKAI\u606f\u8089\u6570\u636e\u96c6\u4e0aDice\u5206\u6570\u63d0\u53476.3%\uff0c\u5176\u4ed6\u56db\u4e2a\u653e\u5c04\u5b66\u548c\u975e\u653e\u5c04\u5b66\u6570\u636e\u96c6\u4e5f\u67091-6%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u96c6\u6210\u6548\u679c\u56e0\u6570\u636e\u96c6\u800c\u5f02", "conclusion": "\u6a21\u578b\u96c6\u6210\u65b9\u6cd5\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u5dee\u5f02\u660e\u663e\uff0c\u6709\u65f6\u4f18\u4e8eCRIS\u6a21\u578b\u6709\u65f6\u4e0d\u5982\uff0c\u8fd9\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u672a\u6765\u7814\u7a76\u7684\u91cd\u8981\u65b9\u5411"}}
{"id": "2509.05263", "pdf": "https://arxiv.org/pdf/2509.05263", "abs": "https://arxiv.org/abs/2509.05263", "authors": ["Yinglin Duan", "Zhengxia Zou", "Tongwei Gu", "Wei Jia", "Zhan Zhao", "Luyi Xu", "Xinzhu Liu", "Hao Jiang", "Kang Chen", "Shuang Qiu"], "title": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. A more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real world conveniently. While traditional manual modeling has enabled the creation of virtual 3D scenes, modern approaches have leveraged advanced machine learning algorithms for 3D world generation, with most recent advances focusing on generative methods that can create virtual worlds based on user instructions. This work explores such a research direction by proposing LatticeWorld, a simple yet effective 3D world generation framework that streamlines the industrial production pipeline of 3D environments. LatticeWorld leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed framework accepts textual descriptions and visual instructions as multimodal inputs and creates large-scale 3D interactive worlds with dynamic agents, featuring competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. We conduct comprehensive experiments to evaluate LatticeWorld, showing that it achieves superior accuracy in scene layout generation and visual fidelity. Moreover, LatticeWorld achieves over a $90\\times$ increase in industrial production efficiency while maintaining high creative quality compared with traditional manual production methods. Our demo video is available at https://youtu.be/8VWZXpERR18", "AI": {"tldr": "LatticeWorld\u662f\u4e00\u4e2a\u57fa\u4e8e\u8f7b\u91cf\u7ea7LLM\u548c\u6e38\u620f\u5f15\u64ce\u76843D\u4e16\u754c\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8f93\u5165\u521b\u5efa\u5927\u89c4\u6a21\u52a8\u6001\u4ea4\u4e92\u73af\u5883\uff0c\u663e\u8457\u63d0\u5347\u5de5\u4e1a\u751f\u4ea7\u6548\u738790\u500d\u4ee5\u4e0a", "motivation": "\u4f20\u7edf\u624b\u52a8\u5efa\u6a213D\u573a\u666f\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u57fa\u4e8e\u7528\u6237\u6307\u4ee4\u81ea\u52a8\u751f\u6210\u903c\u771f3D\u865a\u62df\u4e16\u754c\u7684\u7cfb\u7edf\uff0c\u4ee5\u7f29\u5c0f\u4eff\u771f\u4e0e\u73b0\u5b9e\u5dee\u8ddd\u5e76\u4e30\u5bcc\u771f\u5b9e\u4e16\u754c\u4fe1\u606f\u83b7\u53d6", "method": "\u7ed3\u5408\u8f7b\u91cf\u7ea7LLM(LLaMA-2-7B)\u548c\u5de5\u4e1a\u7ea7\u6e32\u67d3\u5f15\u64ce(\u5982Unreal Engine 5)\uff0c\u63a5\u53d7\u6587\u672c\u63cf\u8ff0\u548c\u89c6\u89c9\u6307\u4ee4\u4f5c\u4e3a\u591a\u6a21\u6001\u8f93\u5165\uff0c\u751f\u6210\u5305\u542b\u52a8\u6001\u4ee3\u7406\u7684\u5927\u89c4\u6a213D\u4ea4\u4e92\u4e16\u754c", "result": "\u5728\u573a\u666f\u5e03\u5c40\u751f\u6210\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u65b9\u9762\u8fbe\u5230\u4f18\u5f02\u7cbe\u5ea6\uff0c\u76f8\u6bd4\u4f20\u7edf\u624b\u52a8\u751f\u4ea7\u65b9\u5f0f\u5b9e\u73b090\u500d\u4ee5\u4e0a\u7684\u5de5\u4e1a\u751f\u4ea7\u6548\u7387\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u521b\u610f", "conclusion": "LatticeWorld\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u76843D\u4e16\u754c\u751f\u6210\u6846\u67b6\uff0c\u80fd\u591f\u9ad8\u6548\u521b\u5efa\u5177\u6709\u7ade\u4e89\u6027\u591a\u4ee3\u7406\u4ea4\u4e92\u3001\u9ad8\u4fdd\u771f\u7269\u7406\u6a21\u62df\u548c\u5b9e\u65f6\u6e32\u67d3\u7684\u52a8\u6001\u73af\u5883\uff0c\u663e\u8457\u4f18\u5316\u5de5\u4e1a\u751f\u4ea7\u6d41\u7a0b"}}
