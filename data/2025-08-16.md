<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 29]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion Learning](https://arxiv.org/abs/2508.10133)
*Thanh-Dat Truong,Christophe Bobda,Nitin Agarwal,Khoa Luu*

Main category: cs.CV

TL;DR: 该论文提出了一种新型的多模态注意力归一化流方法（MANGO），通过可逆交叉注意力层（ICA）和三种新的交叉注意力机制，显式地学习多模态数据的相关性，提升了模型的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态融合方法依赖Transformer的注意力机制隐式学习特征相关性，难以捕捉模态间的本质特征和复杂结构，因此需要一种显式、可解释且可扩展的多模态融合方法。

Method: 提出MANGO方法，包括可逆交叉注意力层（ICA）和三种新的交叉注意力机制（MMCA、IMCA、LICA），并结合归一化流模型处理高维多模态数据。

Result: 在语义分割、图像到图像翻译和电影类型分类三个任务上，MANGO方法达到了最先进的性能。

Conclusion: MANGO方法通过显式建模多模态数据的相关性，显著提升了模型的性能和可解释性，适用于高维多模态学习任务。

Abstract: Multimodal learning has gained much success in recent years. However, current multimodal fusion methods adopt the attention mechanism of Transformers to implicitly learn the underlying correlation of multimodal features. As a result, the multimodal model cannot capture the essential features of each modality, making it difficult to comprehend complex structures and correlations of multimodal inputs. This paper introduces a novel Multimodal Attention-based Normalizing Flow (MANGO) approach\footnote{The source code of this work will be publicly available.} to developing explicit, interpretable, and tractable multimodal fusion learning. In particular, we propose a new Invertible Cross-Attention (ICA) layer to develop the Normalizing Flow-based Model for multimodal data. To efficiently capture the complex, underlying correlations in multimodal data in our proposed invertible cross-attention layer, we propose three new cross-attention mechanisms: Modality-to-Modality Cross-Attention (MMCA), Inter-Modality Cross-Attention (IMCA), and Learnable Inter-Modality Cross-Attention (LICA). Finally, we introduce a new Multimodal Attention-based Normalizing Flow to enable the scalability of our proposed method to high-dimensional multimodal data. Our experimental results on three different multimodal learning tasks, i.e., semantic segmentation, image-to-image translation, and movie genre classification, have illustrated the state-of-the-art (SoTA) performance of the proposed approach.

</details>


### [2] [EntropyGS: An Efficient Entropy Coding on 3D Gaussian Splatting](https://arxiv.org/abs/2508.10227)
*Yuning Huang,Jiahao Pang,Fengqing Zhu,Dong Tian*

Main category: cs.CV

TL;DR: 3D高斯泼溅（3DGS）是一种新兴的视图合成方法，具有快速训练/渲染和高质量视觉效果的优点。本文提出了一种名为EntropyGS的因子化和参数化熵编码方法，用于压缩3DGS高斯属性，实现了30倍的比特率降低，同时保持相似的渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3DGS的高斯创建和视图渲染任务通常在时间或设备上分离，因此需要存储/传输和压缩3DGS高斯属性。本文旨在通过分析高斯属性的统计特性，提出高效的压缩方法。

Method: 首先对3DGS高斯属性进行相关性和统计分析，发现球谐AC属性遵循拉普拉斯分布，而旋转、缩放和不透明度可由高斯混合分布近似。基于此，提出了一种因子化和参数化的熵编码方法EntropyGS，根据属性类型自适应量化并进行熵编码。

Result: EntropyGS在基准数据集上实现了约30倍的比特率降低，同时保持与输入3DGS数据相似的渲染质量，且编码和解码时间较快。

Conclusion: EntropyGS是一种高效的3DGS高斯属性压缩方法，通过统计分析和自适应熵编码，显著降低了存储和传输需求，同时保持了高质量的渲染效果。

Abstract: As an emerging novel view synthesis approach, 3D Gaussian Splatting (3DGS) demonstrates fast training/rendering with superior visual quality. The two tasks of 3DGS, Gaussian creation and view rendering, are typically separated over time or devices, and thus storage/transmission and finally compression of 3DGS Gaussians become necessary. We begin with a correlation and statistical analysis of 3DGS Gaussian attributes. An inspiring finding in this work reveals that spherical harmonic AC attributes precisely follow Laplace distributions, while mixtures of Gaussian distributions can approximate rotation, scaling, and opacity. Additionally, harmonic AC attributes manifest weak correlations with other attributes except for inherited correlations from a color space. A factorized and parameterized entropy coding method, EntropyGS, is hereinafter proposed. During encoding, distribution parameters of each Gaussian attribute are estimated to assist their entropy coding. The quantization for entropy coding is adaptively performed according to Gaussian attribute types. EntropyGS demonstrates about 30x rate reduction on benchmark datasets while maintaining similar rendering quality compared to input 3DGS data, with a fast encoding and decoding time.

</details>


### [3] [High Fidelity Text to Image Generation with Contrastive Alignment and Structural Guidance](https://arxiv.org/abs/2508.10280)
*Danyi Gao*

Main category: cs.CV

TL;DR: 提出了一种结合文本-图像对比约束与结构引导机制的高保真图像生成方法，解决了现有方法在语义对齐和结构一致性上的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动图像生成方法在语义对齐精度和结构一致性上存在性能瓶颈，需要改进。

Method: 通过对比学习模块增强跨模态对齐约束，结合结构先验（如语义布局图或边缘草图）指导空间级结构建模，并联合优化对比损失、结构一致性损失和语义保留损失。

Result: 在COCO-2014数据集上的实验表明，该方法在CLIP Score、FID和SSIM指标上表现优异，有效平衡了语义对齐与结构保真度。

Conclusion: 该方法无需增加计算复杂度，即可生成语义清晰且结构完整的图像，为联合文本-图像建模提供了可行技术路径。

Abstract: This paper addresses the performance bottlenecks of existing text-driven image generation methods in terms of semantic alignment accuracy and structural consistency. A high-fidelity image generation method is proposed by integrating text-image contrastive constraints with structural guidance mechanisms. The approach introduces a contrastive learning module that builds strong cross-modal alignment constraints to improve semantic matching between text and image. At the same time, structural priors such as semantic layout maps or edge sketches are used to guide the generator in spatial-level structural modeling. This enhances the layout completeness and detail fidelity of the generated images. Within the overall framework, the model jointly optimizes contrastive loss, structural consistency loss, and semantic preservation loss. A multi-objective supervision mechanism is adopted to improve the semantic consistency and controllability of the generated content. Systematic experiments are conducted on the COCO-2014 dataset. Sensitivity analyses are performed on embedding dimensions, text length, and structural guidance strength. Quantitative metrics confirm the superior performance of the proposed method in terms of CLIP Score, FID, and SSIM. The results show that the method effectively bridges the gap between semantic alignment and structural fidelity without increasing computational complexity. It demonstrates a strong ability to generate semantically clear and structurally complete images, offering a viable technical path for joint text-image modeling and image generation.

</details>


### [4] [AtomDiffuser: Time-Aware Degradation Modeling for Drift and Beam Damage in STEM Imaging](https://arxiv.org/abs/2508.10359)
*Hao Wang,Hongkui Zheng,Kai He,Abolfazl Razi*

Main category: cs.CV

TL;DR: AtomDiffuser是一个时间感知的降解建模框架，用于分离STEM数据中的空间漂移和辐射衰减，通过预测仿射变换和空间变化衰减图来解析原子结构演化。


<details>
  <summary>Details</summary>
Motivation: 时间分辨STEM数据解释困难，主要由于空间漂移和辐射损伤的复杂纠缠效应，现有方法难以明确分离这些效应或建模原子分辨率下的材料动力学。

Method: 提出AtomDiffuser框架，利用降解作为物理启发、时间条件的过程，预测仿射变换和空间变化衰减图，解析结构演化。

Result: AtomDiffuser在合成和真实cryo-STEM数据上表现良好，支持高分辨率降解推断和漂移对齐，可视化并量化与辐射诱导原子不稳定性相关的降解模式。

Conclusion: AtomDiffuser为STEM数据提供了一种可解释的降解建模方法，有助于理解原子结构的动态演化。

Abstract: Scanning transmission electron microscopy (STEM) plays a critical role in modern materials science, enabling direct imaging of atomic structures and their evolution under external interferences. However, interpreting time-resolved STEM data remains challenging due to two entangled degradation effects: spatial drift caused by mechanical and thermal instabilities, and beam-induced signal loss resulting from radiation damage. These factors distort both geometry and intensity in complex, temporally correlated ways, making it difficult for existing methods to explicitly separate their effects or model material dynamics at atomic resolution. In this work, we present AtomDiffuser, a time-aware degradation modeling framework that disentangles sample drift and radiometric attenuation by predicting an affine transformation and a spatially varying decay map between any two STEM frames. Unlike traditional denoising or registration pipelines, our method leverages degradation as a physically heuristic, temporally conditioned process, enabling interpretable structural evolutions across time. Trained on synthetic degradation processes, AtomDiffuser also generalizes well to real-world cryo-STEM data. It further supports high-resolution degradation inference and drift alignment, offering tools for visualizing and quantifying degradation patterns that correlate with radiation-induced atomic instabilities.

</details>


### [5] [Towards Spatially Consistent Image Generation: On Incorporating Intrinsic Scene Properties into Diffusion Models](https://arxiv.org/abs/2508.10382)
*Hyundo Lee,Suhyung Choi,Byoung-Tak Zhang,Inwoo Hwang*

Main category: cs.CV

TL;DR: 该论文提出了一种通过联合生成图像和其对应的内在场景属性（如深度、分割图）来改善图像生成模型的空间一致性和真实性的方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型因缺乏对底层结构和空间布局的充分信息，常生成空间不一致和扭曲的图像。

Method: 利用预训练估计器从大型图像数据集中提取内在场景属性，并通过自编码器将其聚合为单一潜在变量；基于预训练的潜在扩散模型（LDMs），同时去噪图像和内在域，共享互信息以保持一致性。

Result: 实验表明，该方法能纠正空间不一致性，生成更自然的场景布局，同时保持基础模型（如Stable Diffusion）的保真度和文本对齐性。

Conclusion: 通过联合生成图像和内在属性，模型能隐式捕捉场景结构，生成更一致和真实的图像。

Abstract: Image generation models trained on large datasets can synthesize high-quality images but often produce spatially inconsistent and distorted images due to limited information about the underlying structures and spatial layouts. In this work, we leverage intrinsic scene properties (e.g., depth, segmentation maps) that provide rich information about the underlying scene, unlike prior approaches that solely rely on image-text pairs or use intrinsics as conditional inputs. Our approach aims to co-generate both images and their corresponding intrinsics, enabling the model to implicitly capture the underlying scene structure and generate more spatially consistent and realistic images. Specifically, we first extract rich intrinsic scene properties from a large image dataset with pre-trained estimators, eliminating the need for additional scene information or explicit 3D representations. We then aggregate various intrinsic scene properties into a single latent variable using an autoencoder. Building upon pre-trained large-scale Latent Diffusion Models (LDMs), our method simultaneously denoises the image and intrinsic domains by carefully sharing mutual information so that the image and intrinsic reflect each other without degrading image quality. Experimental results demonstrate that our method corrects spatial inconsistencies and produces a more natural layout of scenes while maintaining the fidelity and textual alignment of the base model (e.g., Stable Diffusion).

</details>


### [6] [PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver Distraction Detection](https://arxiv.org/abs/2508.10397)
*Haibin Sun,Xinghui Song*

Main category: cs.CV

TL;DR: 提出了一种基于姿态驱动的质量控制数据增强框架（PQ-DAF），用于解决驾驶员分心检测中的少样本学习和领域适应问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型在真实场景中泛化能力不足，主要源于数据标注成本高和训练与部署环境的领域差异。

Method: 利用渐进条件扩散模型（PCDMs）生成多样训练样本，并通过基于CogVLM的样本质量评估模块过滤低质量样本。

Result: PQ-DAF显著提升了少样本条件下的驾驶员分心检测性能，增强了模型的跨领域泛化能力。

Conclusion: PQ-DAF为数据稀缺条件下的驾驶员分心检测提供了一种高效且可靠的解决方案。

Abstract: Driver distraction detection is essential for improving traffic safety and reducing road accidents. However, existing models often suffer from degraded generalization when deployed in real-world scenarios. This limitation primarily arises from the few-shot learning challenge caused by the high cost of data annotation in practical environments, as well as the substantial domain shift between training datasets and target deployment conditions. To address these issues, we propose a Pose-driven Quality-controlled Data Augmentation Framework (PQ-DAF) that leverages a vision-language model for sample filtering to cost-effectively expand training data and enhance cross-domain robustness. Specifically, we employ a Progressive Conditional Diffusion Model (PCDMs) to accurately capture key driver pose features and synthesize diverse training examples. A sample quality assessment module, built upon the CogVLM vision-language model, is then introduced to filter out low-quality synthetic samples based on a confidence threshold, ensuring the reliability of the augmented dataset. Extensive experiments demonstrate that PQ-DAF substantially improves performance in few-shot driver distraction detection, achieving significant gains in model generalization under data-scarce conditions.

</details>


### [7] [NanoControl: A Lightweight Framework for Precise and Efficient Control in Diffusion Transformer](https://arxiv.org/abs/2508.10424)
*Shanyuan Liu,Jian Zhu,Junda Lu,Yue Gong,Liuzhuozheng Li,Bo Cheng,Yuhang Ma,Liebucha Wu,Xiaoyu Wu,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: NanoControl提出了一种高效的DiT可控文本到图像生成方法，通过LoRA风格控制模块和KV-Context增强机制，显著减少计算开销，同时保持高质量生成。


<details>
  <summary>Details</summary>
Motivation: 现有DiT可控生成方法依赖ControlNet范式，导致参数和计算成本高。

Method: 采用Flux作为主干网络，设计LoRA风格控制模块和KV-Context增强机制。

Result: 仅增加0.024%参数和0.029%GFLOPs，实现高效可控生成。

Conclusion: NanoControl在减少计算开销的同时，保持了生成质量和可控性。

Abstract: Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in text-to-image synthesis. However, in the domain of controllable text-to-image generation using DiTs, most existing methods still rely on the ControlNet paradigm originally designed for UNet-based diffusion models. This paradigm introduces significant parameter overhead and increased computational costs. To address these challenges, we propose the Nano Control Diffusion Transformer (NanoControl), which employs Flux as the backbone network. Our model achieves state-of-the-art controllable text-to-image generation performance while incurring only a 0.024\% increase in parameter count and a 0.029\% increase in GFLOPs, thus enabling highly efficient controllable generation. Specifically, rather than duplicating the DiT backbone for control, we design a LoRA-style (low-rank adaptation) control module that directly learns control signals from raw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation mechanism that integrates condition-specific key-value information into the backbone in a simple yet highly effective manner, facilitating deep fusion of conditional features. Extensive benchmark experiments demonstrate that NanoControl significantly reduces computational overhead compared to conventional control approaches, while maintaining superior generation quality and achieving improved controllability.

</details>


### [8] [CRISP: Contrastive Residual Injection and Semantic Prompting for Continual Video Instance Segmentation](https://arxiv.org/abs/2508.10432)
*Baichen Liu,Qi Lyu,Xudong Wang,Jiahua Dong,Lianqing Liu,Zhi Han*

Main category: cs.CV

TL;DR: CRISP方法通过对比残差注入和语义提示，解决了持续视频实例分割中的实例、类别和任务混淆问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决持续视频实例分割中实例、类别和任务混淆的问题，同时保持时间一致性。

Method: 使用实例相关损失、自适应残差语义提示框架和对比学习语义一致性损失，以及任务间查询空间初始化策略。

Result: 在YouTube-VIS数据集上显著优于现有方法，避免了灾难性遗忘，提升了分割和分类性能。

Conclusion: CRISP方法在持续视频实例分割任务中表现出色，为未来研究提供了新思路。

Abstract: Continual video instance segmentation demands both the plasticity to absorb new object categories and the stability to retain previously learned ones, all while preserving temporal consistency across frames. In this work, we introduce Contrastive Residual Injection and Semantic Prompting (CRISP), an earlier attempt tailored to address the instance-wise, category-wise, and task-wise confusion in continual video instance segmentation. For instance-wise learning, we model instance tracking and construct instance correlation loss, which emphasizes the correlation with the prior query space while strengthening the specificity of the current task query. For category-wise learning, we build an adaptive residual semantic prompt (ARSP) learning framework, which constructs a learnable semantic residual prompt pool generated by category text and uses an adjustive query-prompt matching mechanism to build a mapping relationship between the query of the current task and the semantic residual prompt. Meanwhile, a semantic consistency loss based on the contrastive learning is introduced to maintain semantic coherence between object queries and residual prompts during incremental training. For task-wise learning, to ensure the correlation at the inter-task level within the query space, we introduce a concise yet powerful initialization strategy for incremental prompts. Extensive experiments on YouTube-VIS-2019 and YouTube-VIS-2021 datasets demonstrate that CRISP significantly outperforms existing continual segmentation methods in the long-term continual video instance segmentation task, avoiding catastrophic forgetting and effectively improving segmentation and classification performance. The code is available at https://github.com/01upup10/CRISP.

</details>


### [9] [From Images to Perception: Emergence of Perceptual Properties by Reconstructing Images](https://arxiv.org/abs/2508.10450)
*Pablo Hernández-Cámara,Jesus Malo,Valero Laparra*

Main category: cs.CV

TL;DR: 论文提出了一种受生物启发的视觉模型PerceptNet，通过端到端优化在图像重建任务中表现出与人类感知判断的高度相关性。


<details>
  <summary>Details</summary>
Motivation: 探索人类视觉感知是否源于图像统计特性，并验证生物启发模型能否无需人类监督学习感知度量。

Method: 使用PerceptNet模型，在自编码、去噪、去模糊和稀疏正则化任务中进行端到端优化。

Result: 模型的V1层与人类感知判断高度相关，且在适度噪声、模糊和稀疏条件下表现最佳。

Conclusion: 视觉系统可能针对特定失真水平优化，生物启发模型可无监督学习感知度量。

Abstract: A number of scientists suggested that human visual perception may emerge from image statistics, shaping efficient neural representations in early vision. In this work, a bio-inspired architecture that can accommodate several known facts in the retina-V1 cortex, the PerceptNet, has been end-to-end optimized for different tasks related to image reconstruction: autoencoding, denoising, deblurring, and sparsity regularization. Our results show that the encoder stage (V1-like layer) consistently exhibits the highest correlation with human perceptual judgments on image distortion despite not using perceptual information in the initialization or training. This alignment exhibits an optimum for moderate noise, blur and sparsity. These findings suggest that the visual system may be tuned to remove those particular levels of distortion with that level of sparsity and that biologically inspired models can learn perceptual metrics without human supervision.

</details>


### [10] [Trajectory-aware Shifted State Space Models for Online Video Super-Resolution](https://arxiv.org/abs/2508.10453)
*Qiang Zhu,Xiandong Meng,Yuxian Jiang,Fan Zhang,David Bull,Shuyuan Zhu,Bing Zeng*

Main category: cs.CV

TL;DR: 本文提出了一种基于轨迹感知移位状态空间模型（TS-Mamba）的在线视频超分辨率方法，通过长时轨迹建模和低复杂度Mamba实现高效时空信息聚合。


<details>
  <summary>Details</summary>
Motivation: 现有在线视频超分辨率方法仅使用相邻前一帧进行时间对齐，限制了长时建模能力。状态空间模型（SSMs）具有线性计算复杂度和全局感受野，能显著提升效率和性能。

Method: TS-Mamba首先构建视频轨迹以选择最相似的令牌，然后使用轨迹感知移位Mamba聚合模块（TSMA）进行令牌聚合。移位SSMs块基于Hilbert扫描和移位操作设计，以补偿扫描损失并增强空间连续性。

Result: 在三个广泛使用的VSR测试数据集上，TS-Mamba在大多数情况下优于六个基准模型，且计算复杂度降低22.7%。

Conclusion: TS-Mamba通过长时轨迹建模和高效Mamba聚合，实现了在线视频超分辨率的性能提升和计算效率优化。

Abstract: Online video super-resolution (VSR) is an important technique for many real-world video processing applications, which aims to restore the current high-resolution video frame based on temporally previous frames. Most of the existing online VSR methods solely employ one neighboring previous frame to achieve temporal alignment, which limits long-range temporal modeling of videos. Recently, state space models (SSMs) have been proposed with linear computational complexity and a global receptive field, which significantly improve computational efficiency and performance. In this context, this paper presents a novel online VSR method based on Trajectory-aware Shifted SSMs (TS-Mamba), leveraging both long-term trajectory modeling and low-complexity Mamba to achieve efficient spatio-temporal information aggregation. Specifically, TS-Mamba first constructs the trajectories within a video to select the most similar tokens from the previous frames. Then, a Trajectory-aware Shifted Mamba Aggregation (TSMA) module consisting of proposed shifted SSMs blocks is employed to aggregate the selected tokens. The shifted SSMs blocks are designed based on Hilbert scannings and corresponding shift operations to compensate for scanning losses and strengthen the spatial continuity of Mamba. Additionally, we propose a trajectory-aware loss function to supervise the trajectory generation, ensuring the accuracy of token selection when training our model. Extensive experiments on three widely used VSR test datasets demonstrate that compared with six online VSR benchmark models, our TS-Mamba achieves state-of-the-art performance in most cases and over 22.7\% complexity reduction (in MACs). The source code for TS-Mamba will be available at https://github.com.

</details>


### [11] [TweezeEdit: Consistent and Efficient Image Editing with Path Regularization](https://arxiv.org/abs/2508.10498)
*Jianda Mao,Kaibo Wang,Yang Xiang,Kani Chen*

Main category: cs.CV

TL;DR: TweezeEdit提出了一种无需调优和反转的图像编辑框架，通过正则化整个去噪路径，显著提升了语义保留和编辑效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在图像编辑中过度依赖目标提示，导致源图像语义保留不足且编辑路径冗长。

Method: TweezeEdit采用梯度驱动正则化，通过一致性模型直接注入目标提示语义，避免依赖反转锚点。

Result: 实验表明，TweezeEdit在语义保留和目标对齐上优于现有方法，仅需12步（1.6秒/次）完成编辑。

Conclusion: TweezeEdit为实时图像编辑提供了高效且语义保留的解决方案。

Abstract: Large-scale pre-trained diffusion models empower users to edit images through text guidance. However, existing methods often over-align with target prompts while inadequately preserving source image semantics. Such approaches generate target images explicitly or implicitly from the inversion noise of the source images, termed the inversion anchors. We identify this strategy as suboptimal for semantic preservation and inefficient due to elongated editing paths. We propose TweezeEdit, a tuning- and inversion-free framework for consistent and efficient image editing. Our method addresses these limitations by regularizing the entire denoising path rather than relying solely on the inversion anchors, ensuring source semantic retention and shortening editing paths. Guided by gradient-driven regularization, we efficiently inject target prompt semantics along a direct path using a consistency model. Extensive experiments demonstrate TweezeEdit's superior performance in semantic preservation and target alignment, outperforming existing methods. Remarkably, it requires only 12 steps (1.6 seconds per edit), underscoring its potential for real-time applications.

</details>


### [12] [Multi-Sample Anti-Aliasing and Constrained Optimization for 3D Gaussian Splatting](https://arxiv.org/abs/2508.10507)
*Zheng Zhou,Jia-Chen Zhang,Yu-Jie Xiong,Chun-Ming Xia*

Main category: cs.CV

TL;DR: 提出了一种结合多重采样抗锯齿（MSAA）和双重几何约束的优化框架，显著提升了3D高斯溅射在细节保留和实时渲染方面的性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯溅射方法在优化过程中缺乏足够的几何约束，导致高频纹理和锐利不连续区域的模糊重建。

Method: 通过自适应混合四重子样本计算像素颜色，引入动态梯度分析的权重策略和梯度微分约束，优先优化关键区域。

Result: 在多个基准测试中，该方法在细节保留和高频纹理重建方面达到最先进水平，同时保持实时渲染效率。

Conclusion: 该方法通过双重几何约束和自适应优化，显著提升了3D高斯溅射的细节重建质量，并在定量指标和感知质量上优于基线方法。

Abstract: Recent advances in 3D Gaussian splatting have significantly improved real-time novel view synthesis, yet insufficient geometric constraints during scene optimization often result in blurred reconstructions of fine-grained details, particularly in regions with high-frequency textures and sharp discontinuities. To address this, we propose a comprehensive optimization framework integrating multisample anti-aliasing (MSAA) with dual geometric constraints. Our system computes pixel colors through adaptive blending of quadruple subsamples, effectively reducing aliasing artifacts in high-frequency components. The framework introduces two constraints: (a) an adaptive weighting strategy that prioritizes under-reconstructed regions through dynamic gradient analysis, and (b) gradient differential constraints enforcing geometric regularization at object boundaries. This targeted optimization enables the model to allocate computational resources preferentially to critical regions requiring refinement while maintaining global consistency. Extensive experimental evaluations across multiple benchmarks demonstrate that our method achieves state-of-the-art performance in detail preservation, particularly in preserving high-frequency textures and sharp discontinuities, while maintaining real-time rendering efficiency. Quantitative metrics and perceptual studies confirm statistically significant improvements over baseline approaches in both structural similarity (SSIM) and perceptual quality (LPIPS).

</details>


### [13] [A Segmentation-driven Editing Method for Bolt Defect Augmentation and Detection](https://arxiv.org/abs/2508.10509)
*Yangjie Xiao,Ke Zhang,Jiacun Wang,Xin Sheng,Yurong Guo,Meijuan Chen,Zehua Ren,Zhaoye Zheng,Zhenbing Zhao*

Main category: cs.CV

TL;DR: 提出了一种基于分割的螺栓缺陷编辑方法（SBDE），通过生成高质量缺陷图像增强数据集，显著提升螺栓缺陷检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决螺栓缺陷图像稀缺和数据分布不平衡对检测性能的限制。

Method: 1. 提出Bolt-SAM模型，通过CFA和MAMD增强复杂螺栓属性的分割；2. 结合MOD和LaMa构建缺陷编辑模型；3. 提出ERA策略恢复并扩展数据集。

Result: 生成的缺陷图像优于现有方法，显著提升检测性能。

Conclusion: SBDE方法有效且具有应用潜力，代码已开源。

Abstract: Bolt defect detection is critical to ensure the safety of transmission lines. However, the scarcity of defect images and imbalanced data distributions significantly limit detection performance. To address this problem, we propose a segmentationdriven bolt defect editing method (SBDE) to augment the dataset. First, a bolt attribute segmentation model (Bolt-SAM) is proposed, which enhances the segmentation of complex bolt attributes through the CLAHE-FFT Adapter (CFA) and Multipart- Aware Mask Decoder (MAMD), generating high-quality masks for subsequent editing tasks. Second, a mask optimization module (MOD) is designed and integrated with the image inpainting model (LaMa) to construct the bolt defect attribute editing model (MOD-LaMa), which converts normal bolts into defective ones through attribute editing. Finally, an editing recovery augmentation (ERA) strategy is proposed to recover and put the edited defect bolts back into the original inspection scenes and expand the defect detection dataset. We constructed multiple bolt datasets and conducted extensive experiments. Experimental results demonstrate that the bolt defect images generated by SBDE significantly outperform state-of-the-art image editing models, and effectively improve the performance of bolt defect detection, which fully verifies the effectiveness and application potential of the proposed method. The code of the project is available at https://github.com/Jay-xyj/SBDE.

</details>


### [14] [HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head Synthesis](https://arxiv.org/abs/2508.10566)
*Shiyu Liu,Kui Jiang,Xianming Liu,Hongxun Yao,Xiaocheng Feng*

Main category: cs.CV

TL;DR: HM-Talker通过结合隐式和显式运动特征，提出了一种新的音频驱动说话头视频生成框架，解决了现有方法的运动模糊和唇部抖动问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖隐式建模音频-面部运动关联，缺乏显式发音先验，导致视频质量不佳。

Method: HM-Talker采用混合运动表示，结合隐式和显式运动特征，并通过跨模态解缠模块和混合运动建模模块优化特征提取与泛化。

Result: 实验表明HM-Talker在视觉质量和唇同步准确性上优于现有方法。

Conclusion: HM-Talker通过混合运动表示和身份无关学习，显著提升了说话头视频的生成质量。

Abstract: Audio-driven talking head video generation enhances user engagement in human-computer interaction. However, current methods frequently produce videos with motion blur and lip jitter, primarily due to their reliance on implicit modeling of audio-facial motion correlations--an approach lacking explicit articulatory priors (i.e., anatomical guidance for speech-related facial movements). To overcome this limitation, we propose HM-Talker, a novel framework for generating high-fidelity, temporally coherent talking heads. HM-Talker leverages a hybrid motion representation combining both implicit and explicit motion cues. Explicit cues use Action Units (AUs), anatomically defined facial muscle movements, alongside implicit features to minimize phoneme-viseme misalignment. Specifically, our Cross-Modal Disentanglement Module (CMDM) extracts complementary implicit/explicit motion features while predicting AUs directly from audio input aligned to visual cues. To mitigate identity-dependent biases in explicit features and enhance cross-subject generalization, we introduce the Hybrid Motion Modeling Module (HMMM). This module dynamically merges randomly paired implicit/explicit features, enforcing identity-agnostic learning. Together, these components enable robust lip synchronization across diverse identities, advancing personalized talking head synthesis. Extensive experiments demonstrate HM-Talker's superiority over state-of-the-art methods in visual quality and lip-sync accuracy.

</details>


### [15] [SpaRC-AD: A Baseline for Radar-Camera Fusion in End-to-End Autonomous Driving](https://arxiv.org/abs/2508.10567)
*Philipp Wolters,Johannes Gilg,Torben Teepe,Gerhard Rigoll*

Main category: cs.CV

TL;DR: SpaRC-AD是一种基于查询的端到端相机-雷达融合框架，用于面向规划的自动驾驶，通过稀疏3D特征对齐和多普勒速度估计，显著提升了3D场景表示和任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决基于视觉的自动驾驶系统在恶劣天气、部分遮挡和精确速度估计方面的局限性，特别是在安全敏感场景中需要准确运动理解和长时程轨迹预测的需求。

Method: 采用相机和雷达融合的稀疏3D特征对齐方法，结合多普勒速度估计，优化代理锚点、地图多段线和运动建模。

Result: 在多个自动驾驶任务中表现优于现有视觉基线，包括3D检测（+4.8% mAP）、多目标跟踪（+8.3% AMOTA）、在线地图（+1.8% mAP）、运动预测（-4.0% mADE）和轨迹规划（-0.1m L2和-9% TPC）。

Conclusion: SpaRC-AD在空间一致性和时间连续性上表现优异，证明了雷达融合在安全关键场景中的有效性，特别是在需要长时程轨迹预测的情况下。

Abstract: End-to-end autonomous driving systems promise stronger performance through unified optimization of perception, motion forecasting, and planning. However, vision-based approaches face fundamental limitations in adverse weather conditions, partial occlusions, and precise velocity estimation - critical challenges in safety-sensitive scenarios where accurate motion understanding and long-horizon trajectory prediction are essential for collision avoidance. To address these limitations, we propose SpaRC-AD, a query-based end-to-end camera-radar fusion framework for planning-oriented autonomous driving. Through sparse 3D feature alignment, and doppler-based velocity estimation, we achieve strong 3D scene representations for refinement of agent anchors, map polylines and motion modelling. Our method achieves strong improvements over the state-of-the-art vision-only baselines across multiple autonomous driving tasks, including 3D detection (+4.8% mAP), multi-object tracking (+8.3% AMOTA), online mapping (+1.8% mAP), motion prediction (-4.0% mADE), and trajectory planning (-0.1m L2 and -9% TPC). We achieve both spatial coherence and temporal consistency on multiple challenging benchmarks, including real-world open-loop nuScenes, long-horizon T-nuScenes, and closed-loop simulator Bench2Drive. We show the effectiveness of radar-based fusion in safety-critical scenarios where accurate motion understanding and long-horizon trajectory prediction are essential for collision avoidance. The source code of all experiments is available at https://phi-wol.github.io/sparcad/

</details>


### [16] [Fourier-Guided Attention Upsampling for Image Super-Resolution](https://arxiv.org/abs/2508.10616)
*Daejune Choi,Youchan No,Jinhyung Lee,Duksu Kim*

Main category: cs.CV

TL;DR: 提出了一种轻量级上采样模块FGA，用于单图像超分辨率，通过频率引导注意力机制提升高频细节重建并减少伪影。


<details>
  <summary>Details</summary>
Motivation: 传统上采样方法（如Sub-Pixel Convolution）效率高但难以重建高频细节且易引入伪影，FGA旨在解决这些问题。

Method: FGA包含三个部分：(1)基于傅里叶特征的多层感知机用于位置频率编码，(2)跨分辨率相关性注意力层用于自适应空间对齐，(3)频域L1损失用于频谱保真监督。

Result: FGA仅增加0.3M参数，在五种超分辨率骨干网络上均表现优异，PSNR平均提升0.12~0.14 dB，频域一致性提升达29%。

Conclusion: FGA在减少伪影和保留细节方面表现突出，是一种实用且可扩展的传统上采样替代方案。

Abstract: We propose Frequency-Guided Attention (FGA), a lightweight upsampling module for single image super-resolution. Conventional upsamplers, such as Sub-Pixel Convolution, are efficient but frequently fail to reconstruct high-frequency details and introduce aliasing artifacts. FGA addresses these issues by integrating (1) a Fourier feature-based Multi-Layer Perceptron (MLP) for positional frequency encoding, (2) a cross-resolution Correlation Attention Layer for adaptive spatial alignment, and (3) a frequency-domain L1 loss for spectral fidelity supervision. Adding merely 0.3M parameters, FGA consistently enhances performance across five diverse super-resolution backbones in both lightweight and full-capacity scenarios. Experimental results demonstrate average PSNR gains of 0.12~0.14 dB and improved frequency-domain consistency by up to 29%, particularly evident on texture-rich datasets. Visual and spectral evaluations confirm FGA's effectiveness in reducing aliasing and preserving fine details, establishing it as a practical, scalable alternative to traditional upsampling methods.

</details>


### [17] [ChatENV: An Interactive Vision-Language Model for Sensor-Guided Environmental Monitoring and Scenario Simulation](https://arxiv.org/abs/2508.10635)
*Hosam Elgendy,Ahmed Sharshar,Ahmed Aboeitta,Mohsen Guizani*

Main category: cs.CV

TL;DR: ChatENV是一个交互式视觉语言模型，结合卫星图像对和真实世界传感器数据，用于环境监测和分析。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型忽视环境传感器的因果信号，依赖单一来源的标注，缺乏交互式场景推理能力。

Method: 构建包含177k图像的数据集，使用GPT-4和Gemini 2.0标注，并基于Qwen-2.5-VL模型进行微调。

Result: 在时间推理和假设推理任务中表现优异（BERT-F1 0.903），优于现有时间模型。

Conclusion: ChatENV是一个强大的工具，支持基于传感器的环境监测和交互式分析。

Abstract: Understanding environmental changes from aerial imagery is vital for climate resilience, urban planning, and ecosystem monitoring. Yet, current vision language models (VLMs) overlook causal signals from environmental sensors, rely on single-source captions prone to stylistic bias, and lack interactive scenario-based reasoning. We present ChatENV, the first interactive VLM that jointly reasons over satellite image pairs and real-world sensor data. Our framework: (i) creates a 177k-image dataset forming 152k temporal pairs across 62 land-use classes in 197 countries with rich sensor metadata (e.g., temperature, PM10, CO); (ii) annotates data using GPT- 4o and Gemini 2.0 for stylistic and semantic diversity; and (iii) fine-tunes Qwen-2.5-VL using efficient Low-Rank Adaptation (LoRA) adapters for chat purposes. ChatENV achieves strong performance in temporal and "what-if" reasoning (e.g., BERT-F1 0.903) and rivals or outperforms state-of-the-art temporal models, while supporting interactive scenario-based analysis. This positions ChatENV as a powerful tool for grounded, sensor-aware environmental monitoring.

</details>


### [18] [Hybrid Generative Fusion for Efficient and Privacy-Preserving Face Recognition Dataset Generation](https://arxiv.org/abs/2508.10672)
*Feiran Li,Qianqian Xu,Shilong Bao,Boyu Han,Zhiyong Yang,Qingming Huang*

Main category: cs.CV

TL;DR: 论文提出了一种构建高质量人脸数据集的方法，通过清理基线数据集、生成合成身份并结合课程学习策略，最终在竞赛中取得第一名。


<details>
  <summary>Details</summary>
Motivation: 解决构建不重叠身份的高质量人脸数据集的挑战，以提升人脸识别模型的性能。

Method: 结合MoE策略清理数据，使用Stable Diffusion生成合成身份，并通过Vec2Face扩展样本，采用课程学习策略训练模型。

Result: 构建的数据集在10K、20K和100K身份规模下均提升了模型性能，并在竞赛中获得第一名。

Conclusion: 混合真实与合成数据的策略有效构建了高质量数据集，显著提升了人脸识别模型的性能。

Abstract: In this paper, we present our approach to the DataCV ICCV Challenge, which centers on building a high-quality face dataset to train a face recognition model. The constructed dataset must not contain identities overlapping with any existing public face datasets. To handle this challenge, we begin with a thorough cleaning of the baseline HSFace dataset, identifying and removing mislabeled or inconsistent identities through a Mixture-of-Experts (MoE) strategy combining face embedding clustering and GPT-4o-assisted verification. We retain the largest consistent identity cluster and apply data augmentation up to a fixed number of images per identity. To further diversify the dataset, we generate synthetic identities using Stable Diffusion with prompt engineering. As diffusion models are computationally intensive, we generate only one reference image per identity and efficiently expand it using Vec2Face, which rapidly produces 49 identity-consistent variants. This hybrid approach fuses GAN-based and diffusion-based samples, enabling efficient construction of a diverse and high-quality dataset. To address the high visual similarity among synthetic identities, we adopt a curriculum learning strategy by placing them early in the training schedule, allowing the model to progress from easier to harder samples. Our final dataset contains 50 images per identity, and all newly generated identities are checked with mainstream face datasets to ensure no identity leakage. Our method achieves \textbf{1st place} in the competition, and experimental results show that our dataset improves model performance across 10K, 20K, and 100K identity scales. Code is available at https://github.com/Ferry-Li/datacv_fr.

</details>


### [19] [Physics-Informed Joint Multi-TE Super-Resolution with Implicit Neural Representation for Robust Fetal T2 Mapping](https://arxiv.org/abs/2508.10680)
*Busra Bulut,Maik Dannecker,Thomas Sanchez,Sara Neves Silva,Vladyslav Zalevskyi,Steven Jia,Jean-Baptiste Ledoux,Guillaume Auzias,François Rousseau,Jana Hutter,Daniel Rueckert,Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: 提出了一种联合重建跨回波时间（TE）数据的方法，用于胎儿脑MRI中的T2映射，解决了运动伪影问题，并减少了扫描时间。


<details>
  <summary>Details</summary>
Motivation: 胎儿脑MRI中的T2映射在0.55T中场强下具有潜力，但现有方法因运动伪影和长扫描时间而受限。

Method: 结合隐式神经表示和物理信息正则化，跨TE联合重建数据，保留解剖和T2定量信息。

Result: 在模拟胎儿脑和成人数据集上表现优异，并首次在0.55T下实现胎儿T2映射。

Conclusion: 该方法通过利用解剖冗余，有望减少T2映射中每个TE所需的堆栈数量。

Abstract: T2 mapping in fetal brain MRI has the potential to improve characterization of the developing brain, especially at mid-field (0.55T), where T2 decay is slower. However, this is challenging as fetal MRI acquisition relies on multiple motion-corrupted stacks of thick slices, requiring slice-to-volume reconstruction (SVR) to estimate a high-resolution (HR) 3D volume. Currently, T2 mapping involves repeated acquisitions of these stacks at each echo time (TE), leading to long scan times and high sensitivity to motion. We tackle this challenge with a method that jointly reconstructs data across TEs, addressing severe motion. Our approach combines implicit neural representations with a physics-informed regularization that models T2 decay, enabling information sharing across TEs while preserving anatomical and quantitative T2 fidelity. We demonstrate state-of-the-art performance on simulated fetal brain and in vivo adult datasets with fetal-like motion. We also present the first in vivo fetal T2 mapping results at 0.55T. Our study shows potential for reducing the number of stacks per TE in T2 mapping by leveraging anatomical redundancy.

</details>


### [20] [Novel View Synthesis using DDIM Inversion](https://arxiv.org/abs/2508.10688)
*Sehajdeep SIngh,A V Subramanyam*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级视图转换框架TUNet，利用预训练扩散模型的高保真生成能力，从单张输入图像合成新视角。通过融合策略优化潜在空间，解决了模糊重建问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要微调大型扩散模型或从头训练，成本高且重建效果模糊，泛化能力差。因此，探索一种轻量级框架，直接利用预训练扩散模型的高保真生成能力，成为研究动机。

Method: 使用DDIM反转潜在空间，通过相机姿态条件转换U-Net（TUNet）预测目标视角的潜在表示。提出融合策略利用DDIM反转中的噪声相关性，优化潜在空间以保留细节。

Result: 在MVImgNet上的实验表明，该方法优于现有方法，能够生成高质量的新视角图像。

Conclusion: 提出的轻量级框架TUNet结合融合策略，有效利用预训练扩散模型，解决了单图像新视角合成的模糊和泛化问题。

Abstract: Synthesizing novel views from a single input image is a challenging task. It requires extrapolating the 3D structure of a scene while inferring details in occluded regions, and maintaining geometric consistency across viewpoints. Many existing methods must fine-tune large diffusion backbones using multiple views or train a diffusion model from scratch, which is extremely expensive. Additionally, they suffer from blurry reconstruction and poor generalization. This gap presents the opportunity to explore an explicit lightweight view translation framework that can directly utilize the high-fidelity generative capabilities of a pretrained diffusion model while reconstructing a scene from a novel view. Given the DDIM-inverted latent of a single input image, we employ a camera pose-conditioned translation U-Net, TUNet, to predict the inverted latent corresponding to the desired target view. However, the image sampled using the predicted latent may result in a blurry reconstruction. To this end, we propose a novel fusion strategy that exploits the inherent noise correlation structure observed in DDIM inversion. The proposed fusion strategy helps preserve the texture and fine-grained details. To synthesize the novel view, we use the fused latent as the initial condition for DDIM sampling, leveraging the generative prior of the pretrained diffusion model. Extensive experiments on MVImgNet demonstrate that our method outperforms existing methods.

</details>


### [21] [Beyond conventional vision: RGB-event fusion for robust object detection in dynamic traffic scenarios](https://arxiv.org/abs/2508.10704)
*Zhanwen Liu,Yujing Sun,Yang Wang,Nan Yang,Shengbo Eben Li,Xiangmo Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种结合事件相机和RGB相机的方法（MCFNet），通过动态范围增强和跨模态特征融合，显著提升了复杂交通场景下的目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统RGB相机在动态范围上的限制导致复杂交通环境（如夜间驾驶、隧道）中的全局对比度降低和高频细节丢失，影响了目标检测的准确性。

Method: 提出MCFNet网络，包括事件校正模块（ECM）进行时间对齐，事件动态上采样模块（EDUM）进行空间对齐，以及跨模态Mamba融合模块（CMM）实现自适应特征融合。

Result: 在DSEC-Det和PKU-DAVIS-SOD数据集上，MCFNet在低光照和快速移动场景中表现优异，mAP50和mAP分别提升7.4%和1.7%。

Conclusion: MCFNet通过结合事件相机和RGB相机，有效解决了动态范围限制问题，显著提升了复杂场景下的目标检测性能。

Abstract: The dynamic range limitation of conventional RGB cameras reduces global contrast and causes loss of high-frequency details such as textures and edges in complex traffic environments (e.g., nighttime driving, tunnels), hindering discriminative feature extraction and degrading frame-based object detection. To address this, we integrate a bio-inspired event camera with an RGB camera to provide high dynamic range information and propose a motion cue fusion network (MCFNet), which achieves optimal spatiotemporal alignment and adaptive cross-modal feature fusion under challenging lighting. Specifically, an event correction module (ECM) temporally aligns asynchronous event streams with image frames via optical-flow-based warping, jointly optimized with the detection network to learn task-aware event representations. The event dynamic upsampling module (EDUM) enhances spatial resolution of event frames to match image structures, ensuring precise spatiotemporal alignment. The cross-modal mamba fusion module (CMM) uses adaptive feature fusion with a novel interlaced scanning mechanism, effectively integrating complementary information for robust detection. Experiments conducted on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate that MCFNet significantly outperforms existing methods in various poor lighting and fast moving traffic scenarios. Notably, on the DSEC-Det dataset, MCFNet achieves a remarkable improvement, surpassing the best existing methods by 7.4% in mAP50 and 1.7% in mAP metrics, respectively. The code is available at https://github.com/Charm11492/MCFNet.

</details>


### [22] [CountCluster: Training-Free Object Quantity Guidance with Cross-Attention Map Clustering for Text-to-Image Generation](https://arxiv.org/abs/2508.10710)
*Joohyeon Lee,Jin-Seop Lee,Jee-Hyong Lee*

Main category: cs.CV

TL;DR: 论文提出了一种名为CountCluster的方法，通过优化对象交叉注意力图的分簇，以更准确地生成符合输入提示中对象数量的图像。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在生成图像时难以准确反映输入提示中指定的对象数量，且现有方法依赖外部工具或训练，效果有限。

Method: CountCluster方法在推理时根据注意力分数将对象交叉注意力图分为k个簇，定义理想分布并优化潜在空间以对齐目标分布。

Result: 相比现有方法，CountCluster在对象数量准确性上平均提升了18.5%，并在多种提示下表现出优越的数量控制性能。

Conclusion: CountCluster无需外部工具或额外训练，显著提升了生成图像中对象数量的准确性。

Abstract: Diffusion-based text-to-image generation models have demonstrated strong performance in terms of image quality and diversity. However, they still struggle to generate images that accurately reflect the number of objects specified in the input prompt. Several approaches have been proposed that rely on either external counting modules for iterative refinement or quantity representations derived from learned tokens or latent features. However, they still have limitations in accurately reflecting the specified number of objects and overlook an important structural characteristic--The number of object instances in the generated image is largely determined in the early timesteps of the denoising process. To correctly reflect the object quantity for image generation, the highly activated regions in the object cross-attention map at the early timesteps should match the input object quantity, while each region should be clearly separated. To address this issue, we propose \textit{CountCluster}, a method that guides the object cross-attention map to be clustered according to the specified object count in the input, without relying on any external tools or additional training. The proposed method partitions the object cross-attention map into $k$ clusters at inference time based on attention scores, defines an ideal distribution in which each cluster is spatially well-separated, and optimizes the latent to align with this target distribution. Our method achieves an average improvement of 18.5\%p in object count accuracy compared to existing methods, and demonstrates superior quantity control performance across a variety of prompts. Code will be released at: https://github.com/JoohyeonL22/CountCluster .

</details>


### [23] [NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale](https://arxiv.org/abs/2508.10711)
*NextStep Team,Chunrui Han,Guopeng Li,Jingwei Wu,Quan Sun,Yan Cai,Yuang Peng,Zheng Ge,Deyu Zhou,Haomiao Tang,Hongyu Zhou,Kenkun Liu,Ailin Huang,Bin Wang,Changxin Miao,Deshan Sun,En Yu,Fukun Yin,Gang Yu,Hao Nie,Haoran Lv,Hanpeng Hu,Jia Wang,Jian Zhou,Jianjian Sun,Kaijun Tan,Kang An,Kangheng Lin,Liang Zhao,Mei Chen,Peng Xing,Rui Wang,Shiyu Liu,Shutao Xia,Tianhao You,Wei Ji,Xianfang Zeng,Xin Han,Xuelin Zhang,Yana Wei,Yanming Xu,Yimin Jiang,Yingming Wang,Yu Zhou,Yucheng Han,Ziyang Meng,Binxing Jiao,Daxin Jiang,Xiangyu Zhang,Yibo Zhu*

Main category: cs.CV

TL;DR: NextStep-1是一种14B自回归模型，结合157M流匹配头，通过离散文本和连续图像标记的预测目标，在文本到图像生成任务中达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有自回归模型依赖计算密集型扩散模型或量化损失的问题，推动自回归范式发展。

Method: 使用NextStep-1模型，结合流匹配头，训练离散文本和连续图像标记的预测目标。

Result: 在文本到图像生成任务中表现最优，支持高保真图像合成和编辑。

Conclusion: NextStep-1展示了统一方法的强大和多功能性，代码和模型将开源。

Abstract: Prevailing autoregressive (AR) models for text-to-image generation either rely on heavy, computationally-intensive diffusion models to process continuous image tokens, or employ vector quantization (VQ) to obtain discrete tokens with quantization loss. In this paper, we push the autoregressive paradigm forward with NextStep-1, a 14B autoregressive model paired with a 157M flow matching head, training on discrete text tokens and continuous image tokens with next-token prediction objectives. NextStep-1 achieves state-of-the-art performance for autoregressive models in text-to-image generation tasks, exhibiting strong capabilities in high-fidelity image synthesis. Furthermore, our method shows strong performance in image editing, highlighting the power and versatility of our unified approach. To facilitate open research, we will release our code and models to the community.

</details>


### [24] [Exploiting Discriminative Codebook Prior for Autoregressive Image Generation](https://arxiv.org/abs/2508.10719)
*Longxiang Tang,Ruihang Chu,Xiang Wang,Yujin Han,Pingyu Wu,Chunming He,Yingya Zhang,Shiwei Zhang,Jiaya Jia*

Main category: cs.CV

TL;DR: 论文提出了一种名为DCPE的方法，替代k-means聚类，更有效地利用代码本中的令牌相似性信息，加速自回归模型训练并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅利用令牌索引值训练自回归模型，而忽略了代码本中丰富的令牌相似性信息。k-means聚类在代码本特征空间中表现不佳，存在令牌空间差异和质心距离不准确的问题。

Method: 提出DCPE方法，采用基于实例的距离替代质心距离，通过聚合合并技术解决令牌空间差异问题。

Result: DCPE加速了LlamaGen-B的自回归模型训练42%，并提升了FID和IS性能。

Conclusion: DCPE是一种即插即用的方法，能有效挖掘和利用代码本中的令牌相似性信息，提升模型训练效率和性能。

Abstract: Advanced discrete token-based autoregressive image generation systems first tokenize images into sequences of token indices with a codebook, and then model these sequences in an autoregressive paradigm. While autoregressive generative models are trained only on index values, the prior encoded in the codebook, which contains rich token similarity information, is not exploited. Recent studies have attempted to incorporate this prior by performing naive k-means clustering on the tokens, helping to facilitate the training of generative models with a reduced codebook. However, we reveal that k-means clustering performs poorly in the codebook feature space due to inherent issues, including token space disparity and centroid distance inaccuracy. In this work, we propose the Discriminative Codebook Prior Extractor (DCPE) as an alternative to k-means clustering for more effectively mining and utilizing the token similarity information embedded in the codebook. DCPE replaces the commonly used centroid-based distance, which is found to be unsuitable and inaccurate for the token feature space, with a more reasonable instance-based distance. Using an agglomerative merging technique, it further addresses the token space disparity issue by avoiding splitting high-density regions and aggregating low-density ones. Extensive experiments demonstrate that DCPE is plug-and-play and integrates seamlessly with existing codebook prior-based paradigms. With the discriminative prior extracted, DCPE accelerates the training of autoregressive models by 42% on LlamaGen-B and improves final FID and IS performance.

</details>


### [25] [Video-BLADE: Block-Sparse Attention Meets Step Distillation for Efficient Video Generation](https://arxiv.org/abs/2508.10774)
*Youping Gu,Xiaolong Li,Yuhao Hu,Bohan Zhuang*

Main category: cs.CV

TL;DR: BLADE提出了一种数据无关的联合训练框架，结合自适应块稀疏注意力（ASA）和稀疏感知的步蒸馏，显著加速扩散变换器的视频生成，同时提升质量。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器在高质量视频生成中表现优异，但其迭代去噪过程缓慢且长序列的二次注意力成本高，导致推理瓶颈。现有加速策略（步蒸馏和稀疏注意力）单独使用效果有限，联合使用又面临训练数据昂贵的问题。

Method: BLADE框架包含：1) 自适应块稀疏注意力（ASA），动态生成内容感知的稀疏掩码；2) 基于轨迹分布匹配（TDM）的稀疏感知步蒸馏，将稀疏性直接融入蒸馏过程。

Result: 在CogVideoX-5B和Wan2.1-1.3B模型上，BLADE实现了14.10x和8.89x的端到端加速，并在VBench-2.0基准测试中提升了视频质量分数。

Conclusion: BLADE通过创新框架有效解决了扩散变换器的推理瓶颈，同时提升了生成质量，为高效高质量视频生成提供了可行方案。

Abstract: Diffusion transformers currently lead the field in high-quality video generation, but their slow iterative denoising process and prohibitive quadratic attention costs for long sequences create significant inference bottlenecks. While both step distillation and sparse attention mechanisms have shown promise as independent acceleration strategies, effectively combining these approaches presents critical challenges -- training-free integration yields suboptimal results, while separately training sparse attention after step distillation requires prohibitively expensive high-quality video data. To overcome these limitations, we propose BLADE, an innovative data-free joint training framework that introduces: (1) an Adaptive Block-Sparse Attention (ASA) mechanism for dynamically generating content-aware sparsity masks to focus computation on salient spatiotemporal features, and (2) a sparsity-aware step distillation paradigm built upon Trajectory Distribution Matching (TDM) that directly incorporates sparsity into the distillation process rather than treating it as a separate compression step, with fast convergence. We validate BLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B. Our framework demonstrates remarkable efficiency gains across different scales. On Wan2.1-1.3B, BLADE achieves a 14.10x end-to-end inference acceleration over a 50-step baseline. Moreover, on models such as CogVideoX-5B with short video sequence lengths, our framework delivers a robust 8.89x speedup. Crucially, the acceleration is accompanied by a consistent quality improvement. On the VBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from 0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further corroborated by superior ratings in human evaluations. Our code and model weights are publicly available at: http://ziplab.co/BLADE-Homepage/.

</details>


### [26] [Ultra-High-Definition Reference-Based Landmark Image Super-Resolution with Generative Diffusion Prior](https://arxiv.org/abs/2508.10779)
*Zhenning Shi,Zizheng Yan,Yuhang Yu,Clara Xue,Jingyu Zhuang,Qi Zhang,Jinwei Chen,Tao Li,Qingnan Fan*

Main category: cs.CV

TL;DR: 论文提出TriFlowSR框架，通过显式模式匹配解决LR图像与参考HR图像对齐问题，并引入首个UHD地标场景RefSR数据集Landmark-4K。


<details>
  <summary>Details</summary>
Motivation: 现有基于ControlNet的RefSR方法难以有效对齐LR与参考HR图像，且现有数据集分辨率低、质量差，无法支持高质量恢复。

Method: 提出TriFlowSR框架，设计Reference Matching Strategy实现LR与参考HR图像的有效匹配，并构建Landmark-4K数据集。

Result: 实验表明，TriFlowSR能更好地利用参考HR图像的语义和纹理信息，优于现有方法。

Conclusion: TriFlowSR是首个针对UHD地标场景的基于扩散的RefSR方法，解决了真实世界退化问题。

Abstract: Reference-based Image Super-Resolution (RefSR) aims to restore a low-resolution (LR) image by utilizing the semantic and texture information from an additional reference high-resolution (reference HR) image. Existing diffusion-based RefSR methods are typically built upon ControlNet, which struggles to effectively align the information between the LR image and the reference HR image. Moreover, current RefSR datasets suffer from limited resolution and poor image quality, resulting in the reference images lacking sufficient fine-grained details to support high-quality restoration. To overcome the limitations above, we propose TriFlowSR, a novel framework that explicitly achieves pattern matching between the LR image and the reference HR image. Meanwhile, we introduce Landmark-4K, the first RefSR dataset for Ultra-High-Definition (UHD) landmark scenarios. Considering the UHD scenarios with real-world degradation, in TriFlowSR, we design a Reference Matching Strategy to effectively match the LR image with the reference HR image. Experimental results show that our approach can better utilize the semantic and texture information of the reference HR image compared to previous methods. To the best of our knowledge, we propose the first diffusion-based RefSR pipeline for ultra-high definition landmark scenarios under real-world degradation. Our code and model will be available at https://github.com/nkicsl/TriFlowSR.

</details>


### [27] [Object Fidelity Diffusion for Remote Sensing Image Generation](https://arxiv.org/abs/2508.10801)
*Ziqi Ye,Shuran Ma,Jie Yang,Xiaoyi Yang,Ziyang Gong,Xue Yang,Haipeng Wang*

Main category: cs.CV

TL;DR: OF-Diff提出了一种高保真遥感图像生成方法，通过先验形状提取和双分支扩散模型提升生成质量，实验表明其在多项指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在遥感图像生成中难以捕捉形态细节，导致低保真度，影响目标检测模型的鲁棒性和可靠性。

Method: 1. 基于布局提取对象先验形状；2. 引入双分支扩散模型和扩散一致性损失；3. 使用DDPO微调扩散过程。

Result: OF-Diff在多项关键质量指标上优于现有方法，例如飞机、船只和车辆的mAP分别提升8.3%、7.7%和4.0%。

Conclusion: OF-Diff显著提升了遥感图像生成的保真度和多样性，尤其在多态和小目标类别上表现突出。

Abstract: High-precision controllable remote sensing image generation is both meaningful and challenging. Existing diffusion models often produce low-fidelity images due to their inability to adequately capture morphological details, which may affect the robustness and reliability of object detection models. To enhance the accuracy and fidelity of generated objects in remote sensing, this paper proposes Object Fidelity Diffusion (OF-Diff), which effectively improves the fidelity of generated objects. Specifically, we are the first to extract the prior shapes of objects based on the layout for diffusion models in remote sensing. Then, we introduce a dual-branch diffusion model with diffusion consistency loss, which can generate high-fidelity remote sensing images without providing real images during the sampling phase. Furthermore, we introduce DDPO to fine-tune the diffusion process, making the generated remote sensing images more diverse and semantically consistent. Comprehensive experiments demonstrate that OF-Diff outperforms state-of-the-art methods in the remote sensing across key quality metrics. Notably, the performance of several polymorphic and small object classes shows significant improvement. For instance, the mAP increases by 8.3%, 7.7%, and 4.0% for airplanes, ships, and vehicles, respectively.

</details>


### [28] [Hierarchical Fine-grained Preference Optimization for Physically Plausible Video Generation](https://arxiv.org/abs/2508.10858)
*Harold Haodong Chen,Haojian Huang,Qifeng Chen,Harry Yang,Ser-Nam Lim*

Main category: cs.CV

TL;DR: PhysHPO提出了一种分层跨模态直接偏好优化框架，通过四个层次的细粒度对齐提升视频生成的物理合理性和质量，并引入自动数据选择流程。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成技术虽能生成高质量视频，但难以满足物理规律的准确性需求，限制了实际应用中的真实感和准确性。

Method: PhysHPO通过四个层次（实例、状态、运动和语义）优化视频对齐，并利用自动数据选择流程从现有数据集中高效筛选优质数据。

Result: 实验表明，PhysHPO显著提升了视频生成的物理合理性和整体质量。

Conclusion: 该研究首次探索了细粒度偏好对齐和数据选择在视频生成中的应用，为更真实、符合人类偏好的视频生成奠定了基础。

Abstract: Recent advancements in video generation have enabled the creation of high-quality, visually compelling videos. However, generating videos that adhere to the laws of physics remains a critical challenge for applications requiring realism and accuracy. In this work, we propose PhysHPO, a novel framework for Hierarchical Cross-Modal Direct Preference Optimization, to tackle this challenge by enabling fine-grained preference alignment for physically plausible video generation. PhysHPO optimizes video alignment across four hierarchical granularities: a) Instance Level, aligning the overall video content with the input prompt; b) State Level, ensuring temporal consistency using boundary frames as anchors; c) Motion Level, modeling motion trajectories for realistic dynamics; and d) Semantic Level, maintaining logical consistency between narrative and visuals. Recognizing that real-world videos are the best reflections of physical phenomena, we further introduce an automated data selection pipeline to efficiently identify and utilize "good data" from existing large-scale text-video datasets, thereby eliminating the need for costly and time-intensive dataset construction. Extensive experiments on both physics-focused and general capability benchmarks demonstrate that PhysHPO significantly improves physical plausibility and overall video generation quality of advanced models. To the best of our knowledge, this is the first work to explore fine-grained preference alignment and data selection for video generation, paving the way for more realistic and human-preferred video generation paradigms.

</details>


### [29] [TexVerse: A Universe of 3D Objects with High-Resolution Textures](https://arxiv.org/abs/2508.10868)
*Yibo Zhang,Li Zhang,Rui Ma,Nan Cao*

Main category: cs.CV

TL;DR: TexVerse是一个大规模高分辨率纹理的3D数据集，填补了现有数据集中高分辨率纹理生成研究的空白。


<details>
  <summary>Details</summary>
Motivation: 当前大规模3D数据集在几何生成方面取得进展，但高分辨率纹理的端到端生成研究不足，TexVerse旨在解决这一问题。

Method: TexVerse从Sketchfab收集了858K个高分辨率3D模型，包括158K个PBR材质模型，并提供骨架和动画子集。

Result: 数据集包含1.6M个3D实例，附带详细注释，适用于纹理合成、PBR材质开发等任务。

Conclusion: TexVerse为3D视觉和图形任务提供了高质量的数据资源。

Abstract: We introduce TexVerse, a large-scale 3D dataset featuring high-resolution textures. While recent advances in large-scale 3D datasets have enhanced high-resolution geometry generation, creating high-resolution textures end-to-end remains underexplored due to the lack of suitable datasets. TexVerse fills this gap with a curated collection of over 858K unique high-resolution 3D models sourced from Sketchfab, including more than 158K models with physically based rendering (PBR) materials. Each model encompasses all of its high-resolution variants, bringing the total to 1.6M 3D instances. TexVerse also includes specialized subsets: TexVerse-Skeleton, with 69K rigged models, and TexVerse-Animation, with 54K animated models, both preserving original skeleton and animation data uploaded by the user. We also provide detailed model annotations describing overall characteristics, structural components, and intricate features. TexVerse offers a high-quality data resource with wide-ranging potential applications in texture synthesis, PBR material development, animation, and various 3D vision and graphics tasks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [30] [Invisible Watermarks, Visible Gains: Steering Machine Unlearning with Bi-Level Watermarking Design](https://arxiv.org/abs/2508.10065)
*Yuhao Sun,Yihua Zhang,Gaowen Liu,Hongtao Xie,Sijia Liu*

Main category: cs.CR

TL;DR: 论文提出了一种利用数字水印技术的数据级调整方法（Water4MU），以提升机器遗忘（MU）的效果，通过双层优化框架实现精确数据遗忘。


<details>
  <summary>Details</summary>
Motivation: 当前机器遗忘算法主要依赖模型权重调整，缺乏对数据级调整的探索，而数字水印技术可能为遗忘过程带来新优势。

Method: 提出Water4MU框架，结合数字水印和双层优化（BLO）：上层优化水印网络以减少遗忘难度，下层独立训练模型。

Result: 实验证明Water4MU在图像分类和生成任务中均有效，尤其在“挑战性遗忘”场景中优于现有方法。

Conclusion: Water4MU通过数据级调整显著提升了机器遗忘的效果，为敏感数据移除提供了新思路。

Abstract: With the increasing demand for the right to be forgotten, machine unlearning (MU) has emerged as a vital tool for enhancing trust and regulatory compliance by enabling the removal of sensitive data influences from machine learning (ML) models. However, most MU algorithms primarily rely on in-training methods to adjust model weights, with limited exploration of the benefits that data-level adjustments could bring to the unlearning process. To address this gap, we propose a novel approach that leverages digital watermarking to facilitate MU by strategically modifying data content. By integrating watermarking, we establish a controlled unlearning mechanism that enables precise removal of specified data while maintaining model utility for unrelated tasks. We first examine the impact of watermarked data on MU, finding that MU effectively generalizes to watermarked data. Building on this, we introduce an unlearning-friendly watermarking framework, termed Water4MU, to enhance unlearning effectiveness. The core of Water4MU is a bi-level optimization (BLO) framework: at the upper level, the watermarking network is optimized to minimize unlearning difficulty, while at the lower level, the model itself is trained independently of watermarking. Experimental results demonstrate that Water4MU is effective in MU across both image classification and image generation tasks. Notably, it outperforms existing methods in challenging MU scenarios, known as "challenging forgets".

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [31] [Geospatial Diffusion for Land Cover Imperviousness Change Forecasting](https://arxiv.org/abs/2508.10649)
*Debvrat Varshney,Vibhas Vats,Bhartendu Pandey,Christa Brelsford,Philipe Dias*

Main category: cs.LG

TL;DR: 论文提出了一种利用生成式AI（GenAI）预测土地覆盖变化的新方法，通过将土地覆盖变化预测视为基于历史和辅助数据的数据合成问题。实验表明，该方法在预测不透水表面变化方面优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 土地覆盖变化对地球系统过程有重要影响，但目前预测能力滞后于其他领域。生成式AI为解决这一问题提供了新思路。

Method: 将土地覆盖变化预测视为数据合成问题，利用扩散模型进行十年尺度的不透水表面预测，并与无变化基线模型对比。

Result: 在12个大都市区的实验中，模型在分辨率≥0.7×0.7km²时的平均绝对误差（MAE）低于基线模型，表明其能捕捉时空模式。

Conclusion: 生成式模型能有效预测土地覆盖变化，未来研究将结合地球物理属性和驱动变量以支持多场景模拟。

Abstract: Land cover, both present and future, has a significant effect on several important Earth system processes. For example, impervious surfaces heat up and speed up surface water runoff and reduce groundwater infiltration, with concomitant effects on regional hydrology and flood risk. While regional Earth System models have increasing skill at forecasting hydrologic and atmospheric processes at high resolution in future climate scenarios, our ability to forecast land-use and land-cover change (LULC), a critical input to risk and consequences assessment for these scenarios, has lagged behind. In this paper, we propose a new paradigm exploiting Generative AI (GenAI) for land cover change forecasting by framing LULC forecasting as a data synthesis problem conditioned on historical and auxiliary data-sources. We discuss desirable properties of generative models that fundament our research premise, and demonstrate the feasibility of our methodology through experiments on imperviousness forecasting using historical data covering the entire conterminous United States. Specifically, we train a diffusion model for decadal forecasting of imperviousness and compare its performance to a baseline that assumes no change at all. Evaluation across 12 metropolitan areas for a year held-out during training indicate that for average resolutions $\geq 0.7\times0.7km^2$ our model yields MAE lower than such a baseline. This finding corroborates that such a generative model can capture spatiotemporal patterns from historical data that are significant for projecting future change. Finally, we discuss future research to incorporate auxiliary information on physical properties about the Earth, as well as supporting simulation of different scenarios by means of driver variables.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [32] [DINOMotion: advanced robust tissue motion tracking with DINOv2 in 2D-Cine MRI-guided radiotherapy](https://arxiv.org/abs/2508.10260)
*Soorena Salari,Catherine Spino,Laurie-Anne Pharand,Fabienne Lathuiliere,Hassan Rivaz,Silvain Beriault,Yiming Xiao*

Main category: eess.IV

TL;DR: DINOMotion是一种基于DINOv2和LoRA层的新型深度学习框架，用于2D-Cine MRI引导放疗中的运动跟踪，具有鲁棒性、高效性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在大偏差和可解释性方面存在不足，需要一种更高效、鲁棒的解决方案。

Method: 结合DINOv2的特征表示和LoRA层减少参数，直接计算图像配准，并提供视觉对应关系。

Result: 在肾脏、肝脏和肺部的实验中，Dice分数分别为92.07%、90.90%和95.23%，处理时间约30ms/扫描。

Conclusion: DINOMotion在实时运动跟踪中表现出色，优于现有方法，具有临床应用潜力。

Abstract: Accurate tissue motion tracking is critical to ensure treatment outcome and safety in 2D-Cine MRI-guided radiotherapy. This is typically achieved by registration of sequential images, but existing methods often face challenges with large misalignments and lack of interpretability. In this paper, we introduce DINOMotion, a novel deep learning framework based on DINOv2 with Low-Rank Adaptation (LoRA) layers for robust, efficient, and interpretable motion tracking. DINOMotion automatically detects corresponding landmarks to derive optimal image registration, enhancing interpretability by providing explicit visual correspondences between sequential images. The integration of LoRA layers reduces trainable parameters, improving training efficiency, while DINOv2's powerful feature representations offer robustness against large misalignments. Unlike iterative optimization-based methods, DINOMotion directly computes image registration at test time. Our experiments on volunteer and patient datasets demonstrate its effectiveness in estimating both linear and nonlinear transformations, achieving Dice scores of 92.07% for the kidney, 90.90% for the liver, and 95.23% for the lung, with corresponding Hausdorff distances of 5.47 mm, 8.31 mm, and 6.72 mm, respectively. DINOMotion processes each scan in approximately 30ms and consistently outperforms state-of-the-art methods, particularly in handling large misalignments. These results highlight its potential as a robust and interpretable solution for real-time motion tracking in 2D-Cine MRI-guided radiotherapy.

</details>


### [33] [Efficient Image Denoising Using Global and Local Circulant Representation](https://arxiv.org/abs/2508.10307)
*Zhaoming Kong,Jiahuan Zhang,Xiaowei Yang*

Main category: eess.IV

TL;DR: 提出了一种基于Haar变换和t-SVD的高效图像去噪算法Haar-tSVD，结合非局部自相似性和PCA，实现快速去噪。


<details>
  <summary>Details</summary>
Motivation: 随着成像设备的进步和图像数据的爆炸式增长，高效且有效的图像去噪需求日益增加。

Method: 利用Haar变换和t-SVD统一捕捉全局和局部相关性，提出一步式并行滤波方法，无需学习局部基。结合CNN噪声估计增强鲁棒性。

Result: 在不同真实去噪任务中验证了Haar-tSVD的高效性和有效性，平衡了去噪速度与性能。

Conclusion: Haar-tSVD是一种快速、高效的去噪方法，适用于实际应用，代码和结果已开源。

Abstract: The advancement of imaging devices and countless image data generated everyday impose an increasingly high demand on efficient and effective image denoising. In this paper, we present a computationally simple denoising algorithm, termed Haar-tSVD, aiming to explore the nonlocal self-similarity prior and leverage the connection between principal component analysis (PCA) and the Haar transform under circulant representation. We show that global and local patch correlations can be effectively captured through a unified tensor-singular value decomposition (t-SVD) projection with the Haar transform. This results in a one-step, highly parallelizable filtering method that eliminates the need for learning local bases to represent image patches, striking a balance between denoising speed and performance. Furthermore, we introduce an adaptive noise estimation scheme based on a CNN estimator and eigenvalue analysis to enhance the robustness and adaptability of the proposed method. Experiments on different real-world denoising tasks validate the efficiency and effectiveness of Haar-tSVD for noise removal and detail preservation. Datasets, code and results are publicly available at https://github.com/ZhaomingKong/Haar-tSVD.

</details>
