<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 11]
- [cs.CV](#cs.CV) [Total: 40]
- [cs.LG](#cs.LG) [Total: 7]
- [physics.optics](#physics.optics) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Creative synthesis of kinematic mechanisms](https://arxiv.org/abs/2510.03308)
*Jiong Lin,Jialong Ning,Judah Goldfeder,Hod Lipson*

Main category: cs.GR

TL;DR: 将平面连杆机构的运动综合问题建模为跨域图像生成任务，使用RGB图像表示连杆机构，通过共享潜在空间的变分自编码器生成未见过的运动曲线和模拟新运动学。


<details>
  <summary>Details</summary>
Motivation: 探索图像生成模型在机械设计中的潜力，为平面连杆机构运动综合提供统一的图像表示框架，支持基于轨迹形状和速度分布的合成。

Method: 使用RGB图像表示各种连杆机构，构建包含从简单四杆机构到复杂八杆机构的数据集，采用共享潜在空间的变分自编码器，通过轨迹点绘制速度的颜色梯度编码速度信息。

Result: 在三个复杂度递增的数据集上验证了方法有效性：标准四杆机构集、四杆和曲柄滑块混合集、包含多环机构的复杂集。结果表明图像表示能有效生成机械设计。

Conclusion: 基于图像的表示方法为生成式机械设计提供了有效框架，旋转副、移动副以及凸轮和齿轮等机构都能在统一的图像生成框架中表示和合成。

Abstract: In this paper, we formulate the problem of kinematic synthesis for planar linkages as a cross-domain image generation task. We develop a planar linkages dataset using RGB image representations, covering a range of mechanisms: from simple types such as crank-rocker and crank-slider to more complex eight-bar linkages like Jansen's mechanism. A shared-latent variational autoencoder (VAE) is employed to explore the potential of image generative models for synthesizing unseen motion curves and simulating novel kinematics. By encoding the drawing speed of trajectory points as color gradients, the same architecture also supports kinematic synthesis conditioned on both trajectory shape and velocity profiles. We validate our method on three datasets of increasing complexity: a standard four-bar linkage set, a mixed set of four-bar and crank-slider mechanisms, and a complex set including multi-loop mechanisms. Preliminary results demonstrate the effectiveness of image-based representations for generative mechanical design, showing that mechanisms with revolute and prismatic joints, and potentially cams and gears, can be represented and synthesized within a unified image generation framework.

</details>


### [2] [Universal Beta Splatting](https://arxiv.org/abs/2510.03312)
*Rong Liu,Zhongpai Gao,Benjamin Planche,Meida Chen,Van Nguyen Nguyen,Meng Zheng,Anwesa Choudhuri,Terrence Chen,Yue Wang,Andrew Feng,Ziyan Wu*

Main category: cs.GR

TL;DR: Universal Beta Splatting (UBS) 是一个统一框架，将3D高斯泼溅推广到N维各向异性Beta核，用于显式辐射场渲染。Beta核能在单一表示中建模空间、角度和时间维度的可控依赖关系，优于固定高斯基元。


<details>
  <summary>Details</summary>
Motivation: 现有高斯泼溅方法使用固定高斯基元，无法有效建模复杂的光传输效应、各向异性视角依赖外观和场景动态。需要统一框架来处理这些多维依赖关系。

Method: 提出基于Beta核的统一框架，Beta核可控制空间、角度和时间维度的依赖关系建模。通过CUDA加速实现实时渲染，无需辅助网络或特定颜色编码。

Result: 在静态、视角依赖和动态基准测试中一致优于现有方法。Beta参数自然分解场景属性为可解释的：空间（表面vs纹理）、角度（漫反射vs镜面反射）、时间（静态vs动态）。

Conclusion: Beta核作为可扩展的通用基元，为辐射场渲染提供了统一解决方案，保持向后兼容性并实现实时高性能渲染。

Abstract: We introduce Universal Beta Splatting (UBS), a unified framework that generalizes 3D Gaussian Splatting to N-dimensional anisotropic Beta kernels for explicit radiance field rendering. Unlike fixed Gaussian primitives, Beta kernels enable controllable dependency modeling across spatial, angular, and temporal dimensions within a single representation. Our unified approach captures complex light transport effects, handles anisotropic view-dependent appearance, and models scene dynamics without requiring auxiliary networks or specific color encodings. UBS maintains backward compatibility by approximating to Gaussian Splatting as a special case, guaranteeing plug-in usability and lower performance bounds. The learned Beta parameters naturally decompose scene properties into interpretable without explicit supervision: spatial (surface vs. texture), angular (diffuse vs. specular), and temporal (static vs. dynamic). Our CUDA-accelerated implementation achieves real-time rendering while consistently outperforming existing methods across static, view-dependent, and dynamic benchmarks, establishing Beta kernels as a scalable universal primitive for radiance field rendering. Our project website is available at https://rongliu-leo.github.io/universal-beta-splatting/.

</details>


### [3] [Style Brush: Guided Style Transfer for 3D Objects](https://arxiv.org/abs/2510.03433)
*Áron Samuel Kovács,Pedro Hermosilla,Renata G. Raidou*

Main category: cs.GR

TL;DR: Style Brush是一种新颖的纹理网格风格迁移方法，为艺术家提供细粒度控制，通过引入捕捉风格方向性的损失函数，支持多风格图像和平滑风格过渡。


<details>
  <summary>Details</summary>
Motivation: 扩展传统3D风格迁移方法，为艺术家提供更精细的控制能力，使风格迁移过程更加灵活和用户友好。

Method: 引入新颖的损失函数来捕捉风格方向性，支持多风格图像或部分风格，使用易于生成的引导纹理简化用户交互。

Result: 通过多种网格、风格图像和轮廓形状的广泛评估，展示了方法的灵活性和生成纹理的视觉吸引力。

Conclusion: Style Brush方法在3D风格迁移中提供了精细的控制能力，生成的纹理具有视觉吸引力，且用户交互过程简化，适合广泛受众使用。

Abstract: We introduce Style Brush, a novel style transfer method for textured meshes designed to empower artists with fine-grained control over the stylization process. Our approach extends traditional 3D style transfer methods by introducing a novel loss function that captures style directionality, supports multiple style images or portions thereof, and enables smooth transitions between styles in the synthesized texture. The use of easily generated guiding textures streamlines user interaction, making our approach accessible to a broad audience. Extensive evaluations with various meshes, style images, and contour shapes demonstrate the flexibility of our method and showcase the visual appeal of the generated textures.

</details>


### [4] [Paris: A Decentralized Trained Open-Weight Diffusion Model](https://arxiv.org/abs/2510.03434)
*Zhiying Jiang,Raihan Seraj,Marcos Villagra,Bidhan Roy*

Main category: cs.GR

TL;DR: Paris是首个完全通过去中心化计算预训练的公开扩散模型，展示了无需中心化基础设施即可实现高质量文本到图像生成的能力。


<details>
  <summary>Details</summary>
Motivation: 旨在证明高质量文本到图像生成可以在没有中心协调基础设施的情况下实现，消除对专用GPU集群的需求。

Method: 使用分布式扩散训练框架，将数据划分为语义连贯的簇，8个专家扩散模型在完全隔离的环境中独立训练，无需梯度、参数或中间激活同步。

Result: Paris在保持生成质量的同时，比先前的去中心化基线减少了14倍训练数据和16倍计算量，生成质量可与中心协调基线相媲美。

Conclusion: 去中心化训练方法可行且高效，为大规模扩散模型训练提供了新的可能性，无需专用硬件互连即可在异构硬件上训练。

Abstract: We present Paris, the first publicly released diffusion model pre-trained entirely through decentralized computation. Paris demonstrates that high-quality text-to-image generation can be achieved without centrally coordinated infrastructure. Paris is open for research and commercial use. Paris required implementing our Distributed Diffusion Training framework from scratch. The model consists of 8 expert diffusion models (129M-605M parameters each) trained in complete isolation with no gradient, parameter, or intermediate activation synchronization. Rather than requiring synchronized gradient updates across thousands of GPUs, we partition data into semantically coherent clusters where each expert independently optimizes its subset while collectively approximating the full distribution. A lightweight transformer router dynamically selects appropriate experts at inference, achieving generation quality comparable to centrally coordinated baselines. Eliminating synchronization enables training on heterogeneous hardware without specialized interconnects. Empirical validation confirms that Paris's decentralized training maintains generation quality while removing the dedicated GPU cluster requirement for large-scale diffusion models. Paris achieves this using 14$\times$ less training data and 16$\times$ less compute than the prior decentralized baseline.

</details>


### [5] [Neon: Negative Extrapolation From Self-Training Improves Image Generation](https://arxiv.org/abs/2510.03597)
*Sina Alemohammad,Zhangyang Wang,Richard G. Baraniuk*

Main category: cs.GR

TL;DR: Neon是一种新的学习方法，通过负向外推从自训练中解决模型自噬障碍问题，在少量合成数据上就能显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型扩展受限于高质量训练数据的稀缺性，而使用未经验证的合成数据进行微调会导致模型自噬障碍，造成样本质量和多样性的快速退化。

Method: Neon首先在自身合成的数据上微调基础模型，然后反直觉地反转梯度更新，从退化的权重中进行负向外推，从而更好地对齐真实数据分布。

Result: Neon在ImageNet 256x256上将xAR-L模型的FID提升至1.02的新SOTA，仅需0.36%的额外训练计算量，且适用于多种架构和数据集。

Conclusion: Neon提供了一种简单有效的解决方案，能够将自训练中的退化转化为自我改进的信号，显著提升模型性能且计算成本极低。

Abstract: Scaling generative AI models is bottlenecked by the scarcity of high-quality training data. The ease of synthesizing from a generative model suggests using (unverified) synthetic data to augment a limited corpus of real data for the purpose of fine-tuning in the hope of improving performance. Unfortunately, however, the resulting positive feedback loop leads to model autophagy disorder (MAD, aka model collapse) that results in a rapid degradation in sample quality and/or diversity. In this paper, we introduce Neon (for Negative Extrapolation frOm self-traiNing), a new learning method that turns the degradation from self-training into a powerful signal for self-improvement. Given a base model, Neon first fine-tunes it on its own self-synthesized data but then, counterintuitively, reverses its gradient updates to extrapolate away from the degraded weights. We prove that Neon works because typical inference samplers that favor high-probability regions create a predictable anti-alignment between the synthetic and real data population gradients, which negative extrapolation corrects to better align the model with the true data distribution. Neon is remarkably easy to implement via a simple post-hoc merge that requires no new real data, works effectively with as few as 1k synthetic samples, and typically uses less than 1% additional training compute. We demonstrate Neon's universality across a range of architectures (diffusion, flow matching, autoregressive, and inductive moment matching models) and datasets (ImageNet, CIFAR-10, and FFHQ). In particular, on ImageNet 256x256, Neon elevates the xAR-L model to a new state-of-the-art FID of 1.02 with only 0.36% additional training compute. Code is available at https://github.com/SinaAlemohammad/Neon

</details>


### [6] [Diverse Text-to-Image Generation via Contrastive Noise Optimization](https://arxiv.org/abs/2510.03813)
*Byungjun Kim,Soobin Um,Jong Chul Ye*

Main category: cs.GR

TL;DR: 提出对比噪声优化方法，通过优化初始噪声来提升文本到图像生成模型的多样性，同时保持图像质量


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在强文本引导下输出多样性有限，现有方法对超参数敏感且效果有限

Method: 在Tweedie数据空间定义对比损失，优化一批噪声潜在变量，通过排斥机制最大化多样性并保持保真度

Result: 在多个T2I骨干模型上的实验表明，该方法实现了优越的质量-多样性帕累托边界，且对超参数选择鲁棒

Conclusion: 对比噪声优化是一种简单有效的预处理方法，能从独特角度解决文本到图像生成中的多样性问题

Abstract: Text-to-image (T2I) diffusion models have demonstrated impressive performance in generating high-fidelity images, largely enabled by text-guided inference. However, this advantage often comes with a critical drawback: limited diversity, as outputs tend to collapse into similar modes under strong text guidance. Existing approaches typically optimize intermediate latents or text conditions during inference, but these methods deliver only modest gains or remain sensitive to hyperparameter tuning. In this work, we introduce Contrastive Noise Optimization, a simple yet effective method that addresses the diversity issue from a distinct perspective. Unlike prior techniques that adapt intermediate latents, our approach shapes the initial noise to promote diverse outputs. Specifically, we develop a contrastive loss defined in the Tweedie data space and optimize a batch of noise latents. Our contrastive optimization repels instances within the batch to maximize diversity while keeping them anchored to a reference sample to preserve fidelity. We further provide theoretical insights into the mechanism of this preprocessing to substantiate its effectiveness. Extensive experiments across multiple T2I backbones demonstrate that our approach achieves a superior quality-diversity Pareto frontier while remaining robust to hyperparameter choices.

</details>


### [7] [3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs Using MCP and RAG](https://arxiv.org/abs/2510.04536)
*Shun-ichiro Hayashi,Daichi Mukunoki,Tetsuya Hoshino,Satoshi Ohshima,Takahiro Katagiri*

Main category: cs.GR

TL;DR: 3Dify是一个基于大语言模型的程序化3D计算机图形生成框架，允许用户通过自然语言指令生成3D内容，支持多种DCC工具自动化操作和图像质量优化。


<details>
  <summary>Details</summary>
Motivation: 旨在解决传统3D内容创作需要专业技能和复杂操作的问题，让用户能够通过简单的自然语言指令就能生成高质量的3D计算机图形内容。

Method: 基于Dify平台构建，采用MCP协议和RAG技术，通过MCP自动化操作DCC工具，对于不支持MCP的工具使用CUA方法自动化GUI操作，并支持用户反馈优化和本地LLM集成。

Result: 开发了一个完整的3D-CG生成框架，能够根据自然语言指令自动生成3D内容，支持多工具协同工作，并提供了图像质量优化机制。

Conclusion: 3Dify框架成功实现了通过自然语言指令生成3D计算机图形的目标，为3D内容创作提供了更便捷的解决方案，同时支持本地部署以降低成本。

Abstract: This paper proposes "3Dify," a procedural 3D computer graphics (3D-CG) generation framework utilizing Large Language Models (LLMs). The framework enables users to generate 3D-CG content solely through natural language instructions. 3Dify is built upon Dify, an open-source platform for AI application development, and incorporates several state-of-the-art LLM-related technologies such as the Model Context Protocol (MCP) and Retrieval-Augmented Generation (RAG). For 3D-CG generation support, 3Dify automates the operation of various Digital Content Creation (DCC) tools via MCP. When DCC tools do not support MCP-based interaction, the framework employs the Computer-Using Agent (CUA) method to automate Graphical User Interface (GUI) operations. Moreover, to enhance image generation quality, 3Dify allows users to provide feedback by selecting preferred images from multiple candidates. The LLM then learns variable patterns from these selections and applies them to subsequent generations. Furthermore, 3Dify supports the integration of locally deployed LLMs, enabling users to utilize custom-developed models and to reduce both time and monetary costs associated with external API calls by leveraging their own computational resources.

</details>


### [8] [C3Editor: Achieving Controllable Consistency in 2D Model for 3D Editing](https://arxiv.org/abs/2510.04539)
*Zeng Tao,Zheng Ding,Zeyuan Chen,Xiang Zhang,Leizhi Li,Zhuowen Tu*

Main category: cs.GR

TL;DR: C3Editor是一个可控且一致的基于2D提升的3D编辑框架，通过选择性建立视图一致的2D编辑模型来解决现有方法的不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于2D提升的3D编辑方法存在不一致性问题，主要源于缺乏视图一致的2D编辑模型和难以确保多视图编辑一致性。

Method: 首先选择GT视图及其编辑图像作为优化目标，然后在GT视图和多个视图上微调2D编辑模型以对齐GT编辑图像并确保多视图一致性，引入单独的LoRA模块进行针对性微调。

Result: 该方法在定性和定量评估中都优于现有的基于2D提升的方法，提供了更一致和可控的2D和3D编辑结果。

Conclusion: C3Editor框架通过选择性建立视图一致的2D编辑模型，成功解决了3D编辑中的一致性问题，实现了更优的编辑效果。

Abstract: Existing 2D-lifting-based 3D editing methods often encounter challenges related to inconsistency, stemming from the lack of view-consistent 2D editing models and the difficulty of ensuring consistent editing across multiple views. To address these issues, we propose C3Editor, a controllable and consistent 2D-lifting-based 3D editing framework. Given an original 3D representation and a text-based editing prompt, our method selectively establishes a view-consistent 2D editing model to achieve superior 3D editing results. The process begins with the controlled selection of a ground truth (GT) view and its corresponding edited image as the optimization target, allowing for user-defined manual edits. Next, we fine-tune the 2D editing model within the GT view and across multiple views to align with the GT-edited image while ensuring multi-view consistency. To meet the distinct requirements of GT view fitting and multi-view consistency, we introduce separate LoRA modules for targeted fine-tuning. Our approach delivers more consistent and controllable 2D and 3D editing results than existing 2D-lifting-based methods, outperforming them in both qualitative and quantitative evaluations.

</details>


### [9] [Social Agent: Mastering Dyadic Nonverbal Behavior Generation via Conversational LLM Agents](https://arxiv.org/abs/2510.04637)
*Zeyi Zhang,Yanju Zhou,Heyuan Yao,Tenglong Ao,Xiaohang Zhan,Libin Liu*

Main category: cs.GR

TL;DR: Social Agent是一个基于大语言模型的框架，用于生成对话中协调的非语言行为，通过智能体系统和双人手势生成模型实现自然、同步的交互。


<details>
  <summary>Details</summary>
Motivation: 解决对话中非语言行为生成的真实性和协调性问题，使虚拟角色在交互中表现出更自然的肢体语言和响应性。

Method: 开发基于LLM的智能体系统来指导对话流程和行为决策，并提出基于自回归扩散模型的双人手势生成模型，将语音信号转化为协调动作。

Result: 用户研究和定量评估表明，该模型显著提高了双人交互质量，产生了自然、同步的非语言行为。

Conclusion: Social Agent框架通过智能体系统和手势生成模型的结合，成功实现了动态、响应式的双人交互，为非语言行为合成提供了有效解决方案。

Abstract: We present Social Agent, a novel framework for synthesizing realistic and contextually appropriate co-speech nonverbal behaviors in dyadic conversations. In this framework, we develop an agentic system driven by a Large Language Model (LLM) to direct the conversation flow and determine appropriate interactive behaviors for both participants. Additionally, we propose a novel dual-person gesture generation model based on an auto-regressive diffusion model, which synthesizes coordinated motions from speech signals. The output of the agentic system is translated into high-level guidance for the gesture generator, resulting in realistic movement at both the behavioral and motion levels. Furthermore, the agentic system periodically examines the movements of interlocutors and infers their intentions, forming a continuous feedback loop that enables dynamic and responsive interactions between the two participants. User studies and quantitative evaluations show that our model significantly improves the quality of dyadic interactions, producing natural, synchronized nonverbal behaviors.

</details>


### [10] [Bridging Text and Video Generation: A Survey](https://arxiv.org/abs/2510.04999)
*Nilay Kumar,Priyansh Bhandari,G. Maragatham*

Main category: cs.GR

TL;DR: 本文对文本到视频生成模型进行了全面调查，从早期GANs和VAEs发展到混合扩散-Transformer架构，分析了模型工作原理、数据集、训练配置、评估指标及当前挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 文本到视频生成技术在教育、营销、娱乐等领域具有巨大潜力，但面临对齐、长期一致性和计算效率等挑战，需要系统梳理该领域的发展脉络和技术现状。

Method: 采用系统性调查方法，追溯从GANs、VAEs到混合Diffusion-Transformer架构的演进，详细分析模型工作原理、训练数据集、硬件配置和超参数设置。

Result: 提供了文本到视频生成模型的全面技术路线图，包括模型架构演变、性能评估指标对比，以及现有方法的局限性分析。

Conclusion: 文本到视频生成领域已取得显著进展，但仍需解决对齐、长期一致性和计算效率等核心挑战，未来应向更全面、感知对齐的评估策略发展。

Abstract: Text-to-video (T2V) generation technology holds potential to transform multiple domains such as education, marketing, entertainment, and assistive technologies for individuals with visual or reading comprehension challenges, by creating coherent visual content from natural language prompts. From its inception, the field has advanced from adversarial models to diffusion-based models, yielding higher-fidelity, temporally consistent outputs. Yet challenges persist, such as alignment, long-range coherence, and computational efficiency. Addressing this evolving landscape, we present a comprehensive survey of text-to-video generative models, tracing their development from early GANs and VAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these models work, what limitations they addressed in their predecessors, and why shifts toward new architectural paradigms were necessary to overcome challenges in quality, coherence, and control. We provide a systematic account of the datasets, which the surveyed text-to-video models were trained and evaluated on, and, to support reproducibility and assess the accessibility of training such models, we detail their training configurations, including their hardware specifications, GPU counts, batch sizes, learning rates, optimizers, epochs, and other key hyperparameters. Further, we outline the evaluation metrics commonly used for evaluating such models and present their performance across standard benchmarks, while also discussing the limitations of these metrics and the emerging shift toward more holistic, perception-aligned evaluation strategies. Finally, drawing from our analysis, we outline the current open challenges and propose a few promising future directions, laying out a perspective for future researchers to explore and build upon in advancing T2V research and applications.

</details>


### [11] [SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder](https://arxiv.org/abs/2510.05081)
*Ronen Kamenetsky,Sara Dorfman,Daniel Garibi,Roni Paiss,Or Patashnik,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: 提出了一种通过文本嵌入的token级操作实现解耦和连续编辑的方法，使用稀疏自编码器识别语义隔离的编辑方向


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像扩散模型是现代图像编辑的基础，但仅靠文本提示无法充分控制编辑过程，需要解耦和连续控制两个重要特性

Method: 通过操作文本嵌入沿特定方向实现编辑，使用稀疏自编码器识别语义隔离的维度，直接在文本嵌入上操作而不修改扩散过程

Result: 实验表明该方法能够在多种属性和领域中实现直观高效的连续控制操作

Conclusion: 该方法实现了对图像编辑的解耦和连续控制，具有模型无关性和广泛适用性

Abstract: Large-scale text-to-image diffusion models have become the backbone of modern image editing, yet text prompts alone do not offer adequate control over the editing process. Two properties are especially desirable: disentanglement, where changing one attribute does not unintentionally alter others, and continuous control, where the strength of an edit can be smoothly adjusted. We introduce a method for disentangled and continuous editing through token-level manipulation of text embeddings. The edits are applied by manipulating the embeddings along carefully chosen directions, which control the strength of the target attribute. To identify such directions, we employ a Sparse Autoencoder (SAE), whose sparse latent space exposes semantically isolated dimensions. Our method operates directly on text embeddings without modifying the diffusion process, making it model agnostic and broadly applicable to various image synthesis backbones. Experiments show that it enables intuitive and efficient manipulations with continuous control across diverse attributes and domains.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [12] [Photorealistic Inpainting for Perturbation-based Explanations in Ecological Monitoring](https://arxiv.org/abs/2510.03317)
*Günel Aghakishiyeva,Jiayi Zhou,Saagar Arya,James David Poling,Holly R. Houliston,Jamie N. Womble,David W. Johnston,Brinnae Bent*

Main category: cs.CV

TL;DR: 提出了一种基于修复引导的扰动解释技术，通过生成逼真的掩码局部编辑来揭示计算机视觉模型在生态监测中的决策依据，特别应用于海豹检测任务。


<details>
  <summary>Details</summary>
Motivation: 生态监测中自动化视觉模型的不透明预测限制了信任和实地应用，需要能够产生逼真、上下文保留的解释方法来支持专家验证和AI在生态学中的可信部署。

Method: 使用修复引导的基于扰动的解释技术，通过Segment-Anything-Model精炼的掩码支持两种干预：(i)对象移除/替换（如用冰/水或船只替换海豹），(ii)背景替换并将原始动物合成到新场景中。

Result: 该方法能够定位诊断结构，避免传统扰动中常见的删除伪影，通过重新评分扰动图像（翻转率、置信度下降）和专家评审获得生态合理性和可解释性。

Conclusion: 该方法产生领域相关的见解，支持专家验证，并为生态学中AI的更可信部署提供了有效工具。

Abstract: Ecological monitoring is increasingly automated by vision models, yet opaque predictions limit trust and field adoption. We present an inpainting-guided, perturbation-based explanation technique that produces photorealistic, mask-localized edits that preserve scene context. Unlike masking or blurring, these edits stay in-distribution and reveal which fine-grained morphological cues drive predictions in tasks such as species recognition and trait attribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for harbor seal detection in Glacier Bay drone imagery, using Segment-Anything-Model-refined masks to support two interventions: (i) object removal/replacement (e.g., replacing seals with plausible ice/water or boats) and (ii) background replacement with original animals composited onto new scenes. Explanations are assessed by re-scoring perturbed images (flip rate, confidence drop) and by expert review for ecological plausibility and interpretability. The resulting explanations localize diagnostic structures, avoid deletion artifacts common to traditional perturbations, and yield domain-relevant insights that support expert validation and more trustworthy deployment of AI in ecology.

</details>


### [13] [Textured Gaussians for Enhanced 3D Scene Appearance Modeling](https://arxiv.org/abs/2411.18625)
*Brian Chao,Hung-Yu Tseng,Lorenzo Porzi,Chen Gao,Tuotuo Li,Qinbo Li,Ayush Saraf,Jia-Bin Huang,Johannes Kopf,Gordon Wetzstein,Changil Kim*

Main category: cs.CV

TL;DR: 提出了一种增强3D高斯泼溅的方法，通过为每个高斯添加纹理映射来提升表达力，解决了原始方法中每个高斯只能表示单一颜色和简单椭球体的问题。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅在3D重建和渲染中表现出色，但每个高斯只能表示单一颜色和简单椭球体，限制了表达力。

Method: 为每个高斯添加alpha、RGB或RGBA纹理映射，使高斯能够表示空间变化的颜色和不透明度，从而增强纹理模式和几何结构的表达能力。

Result: 在多个基准数据集上验证，使用相似或更少的高斯数量实现了比现有方法更好的图像质量。

Conclusion: 通过纹理映射增强高斯表达力是有效的，特别是alpha-only纹理映射能显著提升表达力，RGB纹理映射能达到最高表达力。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a state-of-the-art 3D reconstruction and rendering technique due to its high-quality results and fast training and rendering time. However, pixels covered by the same Gaussian are always shaded in the same color up to a Gaussian falloff scaling factor. Furthermore, the finest geometric detail any individual Gaussian can represent is a simple ellipsoid. These properties of 3DGS greatly limit the expressivity of individual Gaussian primitives. To address these issues, we draw inspiration from texture and alpha mapping in traditional graphics and integrate it with 3DGS. Specifically, we propose a new generalized Gaussian appearance representation that augments each Gaussian with alpha~(A), RGB, or RGBA texture maps to model spatially varying color and opacity across the extent of each Gaussian. As such, each Gaussian can represent a richer set of texture patterns and geometric structures, instead of just a single color and ellipsoid as in naive Gaussian Splatting. Surprisingly, we found that the expressivity of Gaussians can be greatly improved by using alpha-only texture maps, and further augmenting Gaussians with RGB texture maps achieves the highest expressivity. We validate our method on a wide variety of standard benchmark datasets and our own custom captures at both the object and scene levels. We demonstrate image quality improvements over existing methods while using a similar or lower number of Gaussians.

</details>


### [14] [Inference-Time Search using Side Information for Diffusion-based Image Reconstruction](https://arxiv.org/abs/2510.03352)
*Mahdi Farahbakhsh,Vishnu Teja Kunde,Dileep Kalathil,Krishna Narayanan,Jean-Francois Chamberland*

Main category: cs.CV

TL;DR: 提出一种基于侧信息的推理时搜索算法，用于改进扩散模型在逆问题中的重建质量，平衡探索与利用，避免梯度引导的奖励黑客伪影。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型方法通常忽略侧信息，而在严重病态设置中，侧信息可显著提升重建质量。梯度引导方法容易产生奖励黑客伪影。

Method: 提出推理时搜索算法，在采样过程中利用侧信息进行引导，平衡探索与利用，可无缝集成到现有扩散基图像重建流程中。

Result: 在多种逆问题（框内修复、超分辨率、运动/高斯/非线性/盲去模糊等）上实验表明，该方法能一致提升扩散基图像重建算法的定性和定量性能，优于包括梯度引导在内的其他基线方法。

Conclusion: 所提出的侧信息搜索方法能有效改进扩散模型在逆问题中的重建性能，提供更准确可靠的重建结果，代码已开源。

Abstract: Diffusion models have emerged as powerful priors for solving inverse problems. However, existing approaches typically overlook side information that could significantly improve reconstruction quality, especially in severely ill-posed settings. In this work, we propose a novel inference-time search algorithm that guides the sampling process using the side information in a manner that balances exploration and exploitation. This enables more accurate and reliable reconstructions, providing an alternative to the gradient-based guidance that is prone to reward-hacking artifacts. Our approach can be seamlessly integrated into a wide range of existing diffusion-based image reconstruction pipelines. Through extensive experiments on a number of inverse problems, such as box inpainting, super-resolution, and various deblurring tasks including motion, Gaussian, nonlinear, and blind deblurring, we show that our approach consistently improves the qualitative and quantitative performance of diffusion-based image reconstruction algorithms. We also show the superior performance of our approach with respect to other baselines, including reward gradient-based guidance algorithms. The code is available at \href{https://github.com/mhdfb/sideinfo-search-reconstruction}{this repository}.

</details>


### [15] [Spatial-ViLT: Enhancing Visual Spatial Reasoning through Multi-Task Learning](https://arxiv.org/abs/2510.03441)
*Chashi Mahiul Islam,Oteo Mamo,Samuel Jacob Chacko,Xiuwen Liu,Weikuan Yu*

Main category: cs.CV

TL;DR: 提出了SpatialViLT，一种增强的视觉语言模型，通过整合深度图、3D坐标和边缘图等空间特征来改进3D场景和复杂物体配置的空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在多模态推理方面取得进展，但在3D场景和复杂物体配置的空间推理方面仍面临挑战。

Method: 采用多任务学习框架整合空间特征，提出了SpatialViLT和MaskedSpatialViLT两个变体，以及结合两者的SpatialEnsemble方法。

Result: 在具有挑战性的视觉空间推理数据集上，模型在方向性、拓扑和邻近关系等空间推理类别中表现出色，达到了最先进的准确率。

Conclusion: 这项工作在增强AI系统空间智能方面迈出了重要一步，对高级多模态理解和实际应用至关重要。

Abstract: Vision-language models (VLMs) have advanced multimodal reasoning but still face challenges in spatial reasoning for 3D scenes and complex object configurations. To address this, we introduce SpatialViLT, an enhanced VLM that integrates spatial features like depth maps, 3D coordinates, and edge maps through a multi-task learning framework. This approach enriches multimodal embeddings with spatial understanding. We propose two variants: SpatialViLT and MaskedSpatialViLT, focusing on full and masked object regions, respectively. Additionally, SpatialEnsemble combines both approaches, achieving state-of-the-art accuracy. Our models excel in spatial reasoning categories such as directional, topological, and proximity relations, as demonstrated on the challenging Visual Spatial Reasoning (VSR) dataset. This work represents a significant step in enhancing the spatial intelligence of AI systems, crucial for advanced multimodal understanding and real-world applications.

</details>


### [16] [Denoising of Two-Phase Optically Sectioned Structured Illumination Reconstructions Using Encoder-Decoder Networks](https://arxiv.org/abs/2510.03452)
*Allison Davis,Yezhi Shen,Xiaoyu Ji,Fengqing Zhu*

Main category: cs.CV

TL;DR: 使用编码器-解码器网络对两相光学切片结构照明显微镜中的伪影进行降噪，通过合成训练数据解决了缺乏干净地面真值数据的问题。


<details>
  <summary>Details</summary>
Motivation: 两相光学切片结构照明显微镜中，减少采集时间会引入残留伪影，传统降噪方法难以有效抑制，且监督训练缺乏干净的光学切片地面真值数据。

Method: 使用非对称降噪自编码器和U-Net网络，通过将真实伪影场应用于合成图像来创建合成训练对，然后在真实OS-SI图像上进行评估。

Result: 两种网络都提高了图像清晰度，每种网络在不同类型的伪影上表现优异。

Conclusion: 合成训练能够实现对OS-SI图像的监督降噪，编码器-解码器网络有潜力简化重建工作流程。

Abstract: Structured illumination (SI) enhances image resolution and contrast by projecting patterned light onto a sample. In two-phase optical-sectioning SI (OS-SI), reduced acquisition time introduces residual artifacts that conventional denoising struggles to suppress. Deep learning offers an alternative to traditional methods; however, supervised training is limited by the lack of clean, optically sectioned ground-truth data. We investigate encoder-decoder networks for artifact reduction in two-phase OS-SI, using synthetic training pairs formed by applying real artifact fields to synthetic images. An asymmetrical denoising autoencoder (DAE) and a U-Net are trained on the synthetic data, then evaluated on real OS-SI images. Both networks improve image clarity, with each excelling against different artifact types. These results demonstrate that synthetic training enables supervised denoising of OS-SI images and highlight the potential of encoder-decoder networks to streamline reconstruction workflows.

</details>


### [17] [SketchPlan: Diffusion Based Drone Planning From Human Sketches](https://arxiv.org/abs/2510.03545)
*Sixten Norelius,Aaron O. Feldman,Mac Schwager*

Main category: cs.CV

TL;DR: SketchPlan是一个基于扩散模型的规划器，通过2D手绘草图在深度图像上生成无人机3D飞行路径，实现零样本从仿真到现实的迁移。


<details>
  <summary>Details</summary>
Motivation: 解决如何让无人机通过人类直观的2D手绘草图来理解导航意图，并在未知真实环境中生成安全准确的3D飞行路径。

Method: 包含两个组件：SketchAdapter学习将手绘草图映射到投影2D路径，DiffPath扩散模型从2D投影和第一人称深度图像推断3D轨迹。使用32k合成数据集训练，其中872条路径标注了真实人类草图。

Result: 在真实世界无人机测试中，低/中等障碍物环境下成功率100%，未知高障碍物环境下成功率40%，比关键消融实验高出20-60%的任务完成率。

Conclusion: 混合人工标注和自动标注数据的训练，加上模块化设计，显著提升了模型正确理解人类意图和推断3D路径的能力。

Abstract: We propose SketchPlan, a diffusion-based planner that interprets 2D hand-drawn sketches over depth images to generate 3D flight paths for drone navigation. SketchPlan comprises two components: a SketchAdapter that learns to map the human sketches to projected 2D paths, and DiffPath, a diffusion model that infers 3D trajectories from 2D projections and a first person view depth image. Our model achieves zero-shot sim-to-real transfer, generating accurate and safe flight paths in previously unseen real-world environments. To train the model, we build a synthetic dataset of 32k flight paths using a diverse set of photorealistic 3D Gaussian Splatting scenes. We automatically label the data by computing 2D projections of the 3D flight paths onto the camera plane, and use this to train the DiffPath diffusion model. However, since real human 2D sketches differ significantly from ideal 2D projections, we additionally label 872 of the 3D flight paths with real human sketches and use this to train the SketchAdapter to infer the 2D projection from the human sketch. We demonstrate SketchPlan's effectiveness in both simulated and real-world experiments, and show through ablations that training on a mix of human labeled and auto-labeled data together with a modular design significantly boosts its capabilities to correctly interpret human intent and infer 3D paths. In real-world drone tests, SketchPlan achieved 100\% success in low/medium clutter and 40\% in unseen high-clutter environments, outperforming key ablations by 20-60\% in task completion.

</details>


### [18] [Diffusion-Classifier Synergy: Reward-Aligned Learning via Mutual Boosting Loop for FSCIL](https://arxiv.org/abs/2510.03608)
*Ruitao Wu,Yifan Zhao,Guangyao Chen,Jia Li*

Main category: cs.CV

TL;DR: DCS框架通过扩散模型与分类器的协同进化，在少样本类增量学习中实现了知识保留和新类学习的显著提升


<details>
  <summary>Details</summary>
Motivation: 解决FSCIL中模型在有限数据下难以泛化的问题，避免扩散模型直接应用导致的语义错位或无效指导

Method: 提出扩散-分类器协同框架，采用奖励对齐学习策略，通过特征级和logits级的多层面奖励函数指导扩散模型生成

Result: 在FSCIL基准测试中达到最先进性能，显著提升了知识保留和新类学习能力

Conclusion: DCS框架通过扩散模型与分类器的相互促进循环，有效解决了FSCIL中的数据稀缺和稳定性-可塑性困境

Abstract: Few-Shot Class-Incremental Learning (FSCIL) challenges models to sequentially learn new classes from minimal examples without forgetting prior knowledge, a task complicated by the stability-plasticity dilemma and data scarcity. Current FSCIL methods often struggle with generalization due to their reliance on limited datasets. While diffusion models offer a path for data augmentation, their direct application can lead to semantic misalignment or ineffective guidance. This paper introduces Diffusion-Classifier Synergy (DCS), a novel framework that establishes a mutual boosting loop between diffusion model and FSCIL classifier. DCS utilizes a reward-aligned learning strategy, where a dynamic, multi-faceted reward function derived from the classifier's state directs the diffusion model. This reward system operates at two levels: the feature level ensures semantic coherence and diversity using prototype-anchored maximum mean discrepancy and dimension-wise variance matching, while the logits level promotes exploratory image generation and enhances inter-class discriminability through confidence recalibration and cross-session confusion-aware mechanisms. This co-evolutionary process, where generated images refine the classifier and an improved classifier state yields better reward signals, demonstrably achieves state-of-the-art performance on FSCIL benchmarks, significantly enhancing both knowledge retention and new class learning.

</details>


### [19] [A Novel Cloud-Based Diffusion-Guided Hybrid Model for High-Accuracy Accident Detection in Intelligent Transportation Systems](https://arxiv.org/abs/2510.03675)
*Siva Sai,Saksham Gupta,Vinay Chamola,Rajkumar Buyya*

Main category: cs.CV

TL;DR: 提出了一种结合引导分类和扩散技术的混合模型，用于智能交通系统中的事故检测，在公开数据集上达到97.32%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统分类方法在理解复杂数据分布方面存在局限性，扩散模型具有有效理解复杂数据分布的内在能力，可以克服传统方法的不足。

Method: 使用微调的ExceptionNet架构输出作为扩散模型的输入，以图像张量作为条件，构建包含多个条件模块的鲁棒分类框架，通过时间嵌入和图像协变量嵌入动态调整网络行为。

Result: 在基于图像的事故检测中表现最佳，准确率达到97.32%。通过消融研究分析了扩散特性如时间步调度器、编码技术、时间步数量和架构设计变化。

Conclusion: 提出的混合扩散模型在智能交通系统事故检测中显著优于基线模型，证明了扩散模型在复杂分类任务中的有效性。

Abstract: The integration of Diffusion Models into Intelligent Transportation Systems (ITS) is a substantial improvement in the detection of accidents. We present a novel hybrid model integrating guidance classification with diffusion techniques. By leveraging fine-tuned ExceptionNet architecture outputs as input for our proposed diffusion model and processing image tensors as our conditioning, our approach creates a robust classification framework. Our model consists of multiple conditional modules, which aim to modulate the linear projection of inputs using time embeddings and image covariate embeddings, allowing the network to adapt its behavior dynamically throughout the diffusion process. To address the computationally intensive nature of diffusion models, our implementation is cloud-based, enabling scalable and efficient processing. Our strategy overcomes the shortcomings of conventional classification approaches by leveraging diffusion models inherent capacity to effectively understand complicated data distributions. We investigate important diffusion characteristics, such as timestep schedulers, timestep encoding techniques, timestep count, and architectural design changes, using a thorough ablation study, and have conducted a comprehensive evaluation of the proposed model against the baseline models on a publicly available dataset. The proposed diffusion model performs best in image-based accident detection with an accuracy of 97.32%.

</details>


### [20] [Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models](https://arxiv.org/abs/2510.03721)
*Leander Girrbach,Stephan Alaniz,Genevieve Smith,Trevor Darrell,Zeynep Akata*

Main category: cs.CV

TL;DR: 该研究通过为LAION-400M数据集创建人物中心标注，揭示了训练数据构成与下游模型偏见之间的实证联系，发现60-70%的性别偏见可由数据中的直接共现关系线性解释。


<details>
  <summary>Details</summary>
Motivation: 大规模多模态数据集训练出的视觉语言模型显示出强烈的人口统计偏见，但由于缺乏人口统计标注（如LAION-400M），训练数据在产生这些偏见中的作用尚不清楚。

Method: 通过验证的自动标注流程为完整数据集创建人物中心标注，包括超过2.76亿个边界框、感知性别和种族/民族标签，以及自动生成的标题，结合目标检测、多模态字幕生成和微调分类器。

Result: 揭示了人口统计不平衡和有害关联，如男性和被感知为黑人或中东裔的个体与犯罪相关和负面内容的不成比例关联；60-70%的CLIP和Stable Diffusion中的性别偏见可由数据中的直接共现关系线性解释。

Conclusion: 这些资源建立了数据集构成与下游模型偏见之间的首个大规模实证联系，为理解训练数据在模型偏见形成中的作用提供了重要依据。

Abstract: Vision-language models trained on large-scale multimodal datasets show strong demographic biases, but the role of training data in producing these biases remains unclear. A major barrier has been the lack of demographic annotations in web-scale datasets such as LAION-400M. We address this gap by creating person-centric annotations for the full dataset, including over 276 million bounding boxes, perceived gender and race/ethnicity labels, and automatically generated captions. These annotations are produced through validated automatic labeling pipelines combining object detection, multimodal captioning, and finetuned classifiers. Using them, we uncover demographic imbalances and harmful associations, such as the disproportionate linking of men and individuals perceived as Black or Middle Eastern with crime-related and negative content. We also show that 60-70% of gender bias in CLIP and Stable Diffusion can be linearly explained by direct co-occurrences in the data. Our resources establish the first large-scale empirical link between dataset composition and downstream model bias.

</details>


### [21] [LoRA Patching: Exposing the Fragility of Proactive Defenses against Deepfakes](https://arxiv.org/abs/2510.03747)
*Zuomin Qu,Yimao Guo,Qianyue Hu,Wei Lu*

Main category: cs.CV

TL;DR: 提出LoRA修补方法，通过向Deepfake生成器注入可插拔的LoRA补丁来绕过现有防御系统，同时提出防御性LoRA修补作为补充解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有针对Deepfake的主动防御措施缺乏鲁棒性和可靠性，需要揭示其安全漏洞并开发更强大的防御策略。

Method: 使用低秩适应(LoRA)补丁技术，结合可学习门控机制防止梯度爆炸，并引入多模态特征对齐(MMFA)损失函数进行语义级特征对齐。

Result: 仅用1000个面部样本和单轮微调，LoRA修补就能成功绕过多种主动防御系统。

Conclusion: 当前Deepfake防御范式存在严重弱点，需要开发更鲁棒的防御策略，同时提出的防御性LoRA修补可作为缓解安全漏洞的补充方案。

Abstract: Deepfakes pose significant societal risks, motivating the development of proactive defenses that embed adversarial perturbations in facial images to prevent manipulation. However, in this paper, we show that these preemptive defenses often lack robustness and reliability. We propose a novel approach, Low-Rank Adaptation (LoRA) patching, which injects a plug-and-play LoRA patch into Deepfake generators to bypass state-of-the-art defenses. A learnable gating mechanism adaptively controls the effect of the LoRA patch and prevents gradient explosions during fine-tuning. We also introduce a Multi-Modal Feature Alignment (MMFA) loss, encouraging the features of adversarial outputs to align with those of the desired outputs at the semantic level. Beyond bypassing, we present defensive LoRA patching, embedding visible warnings in the outputs as a complementary solution to mitigate this newly identified security vulnerability. With only 1,000 facial examples and a single epoch of fine-tuning, LoRA patching successfully defeats multiple proactive defenses. These results reveal a critical weakness in current paradigms and underscore the need for more robust Deepfake defense strategies. Our code is available at https://github.com/ZOMIN28/LoRA-Patching.

</details>


### [22] [Contrastive-SDE: Guiding Stochastic Differential Equations with Contrastive Learning for Unpaired Image-to-Image Translation](https://arxiv.org/abs/2510.03821)
*Venkata Narendra Kotyada,Revanth Eranki,Nagesh Bhattu Sristy*

Main category: cs.CV

TL;DR: 提出了一种基于对比学习和扩散模型的非配对图像转换方法，通过时间相关的对比学习保留领域不变特征，引导预训练SDE进行图像转换。


<details>
  <summary>Details</summary>
Motivation: 非配对图像转换需要在不使用对齐样本的情况下学习源域和目标域之间的映射。扩散模型能生成高质量多样化输出，对比学习能学习语义相似性，两者都适合非配对场景。

Method: 使用时间相关的对比学习方法，将图像与其领域不变特征作为正样本对进行SimCLR训练，然后利用学习到的对比模型指导预训练SDE进行图像转换。

Result: 在三个常见的非配对图像转换任务上与多个基线方法比较，使用四个评估指标。Contrastive-SDE在多个指标上达到与最先进方法相当的结果，收敛速度显著更快，且无需标签监督或分类器训练。

Conclusion: 该方法为非配对图像转换提供了更高效的替代方案，结合了对比学习和扩散模型的优势，在保持性能的同时提高了训练效率。

Abstract: Unpaired image-to-image translation involves learning mappings between source domain and target domain in the absence of aligned or corresponding samples. Score based diffusion models have demonstrated state-of-the-art performance in generative tasks. Their ability to approximate complex data distributions through stochastic differential equations (SDEs) enables them to generate high-fidelity and diverse outputs, making them particularly well-suited for unpaired I2I settings. In parallel, contrastive learning provides a powerful framework for learning semantic similarities without the need for explicit supervision or paired data. By pulling together representations of semantically similar samples and pushing apart dissimilar ones, contrastive methods are inherently aligned with the objectives of unpaired translation. Its ability to selectively enforce semantic consistency at the feature level makes contrastive learning particularly effective for guiding generation in unpaired scenarios. In this work, we propose a time-dependent contrastive learning approach where a model is trained with SimCLR by considering an image and its domain invarient feature as a positive pair, enabling the preservation of domain-invariant features and the discarding of domain-specific ones. The learned contrastive model then guides the inference of a pretrained SDE for the I2I translation task. We empirically compare Contrastive-SDE with several baselines across three common unpaired I2I tasks, using four metrics for evaluation. Constrastive-SDE achieves comparable results to the state-of-the-art on several metrics. Furthermore, we observe that our model converges significantly faster and requires no label supervision or classifier training, making it a more efficient alternative for this task.

</details>


### [23] [Mirage: Unveiling Hidden Artifacts in Synthetic Images with Large Vision-Language Models](https://arxiv.org/abs/2510.03840)
*Pranav Sharma,Shivank Garg,Durga Toshniwal*

Main category: cs.CV

TL;DR: 该论文提出了Mirage数据集，包含具有可见伪影的AI生成图像，并研究大型视觉语言模型在可解释AI图像检测中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成图像越来越难以被标准检测器识别，但人类仍能区分。研究者希望探索大型视觉语言模型是否能替代人类判断进行可解释的AI图像检测。

Method: 创建了Mirage数据集，包含各种具有可见伪影的AI生成图像，并在该数据集和现有基准数据集上测试大型视觉语言模型的检测能力。

Result: 大型视觉语言模型在检测具有可见伪影的AI生成图像时表现优异，但在面对缺乏此类线索的图像时性能下降。

Conclusion: 大型视觉语言模型在AI图像检测方面具有潜力，特别是在图像包含可见伪影时，但其性能受限于图像质量，需要进一步改进以处理更逼真的生成图像。

Abstract: Recent advances in image generation models have led to models that produce synthetic images that are increasingly difficult for standard AI detectors to identify, even though they often remain distinguishable by humans. To identify this discrepancy, we introduce \textbf{Mirage}, a curated dataset comprising a diverse range of AI-generated images exhibiting visible artifacts, where current state-of-the-art detection methods largely fail. Furthermore, we investigate whether Large Vision-Language Models (LVLMs), which are increasingly employed as substitutes for human judgment in various tasks, can be leveraged for explainable AI image detection. Our experiments on both Mirage and existing benchmark datasets demonstrate that while LVLMs are highly effective at detecting AI-generated images with visible artifacts, their performance declines when confronted with images lacking such cues.

</details>


### [24] [Optimized Minimal 4D Gaussian Splatting](https://arxiv.org/abs/2510.03857)
*Minseo Lee,Byeonghyeon Lee,Lucas Yunkyu Lee,Eunsoo Lee,Sangmin Kim,Seunghyeon Song,Joo Chan Lee,Jong Hwan Ko,Jaesik Park,Eunbyung Park*

Main category: cs.CV

TL;DR: OMG4是一个优化4D高斯泼溅表示的框架，通过三阶段剪枝和压缩技术，在保持重建质量的同时将模型大小减少60%以上。


<details>
  <summary>Details</summary>
Motivation: 4D高斯泼溅表示面临存储开销过大的挑战，需要数百万个高斯函数进行高保真重建，现有方法在压缩比或视觉质量方面仍有局限。

Method: 采用三阶段渐进式剪枝：高斯采样识别关键基元、高斯剪枝去除冗余、高斯合并融合相似特征；结合隐式外观压缩和广义子向量量化技术。

Result: 在标准基准数据集上的实验表明，OMG4显著优于现有最先进方法，模型大小减少超过60%的同时保持重建质量。

Conclusion: OMG4在紧凑4D场景表示方面迈出了重要一步，为广泛应用开辟了新可能性。

Abstract: 4D Gaussian Splatting has emerged as a new paradigm for dynamic scene representation, enabling real-time rendering of scenes with complex motions. However, it faces a major challenge of storage overhead, as millions of Gaussians are required for high-fidelity reconstruction. While several studies have attempted to alleviate this memory burden, they still face limitations in compression ratio or visual quality. In this work, we present OMG4 (Optimized Minimal 4D Gaussian Splatting), a framework that constructs a compact set of salient Gaussians capable of faithfully representing 4D Gaussian models. Our method progressively prunes Gaussians in three stages: (1) Gaussian Sampling to identify primitives critical to reconstruction fidelity, (2) Gaussian Pruning to remove redundancies, and (3) Gaussian Merging to fuse primitives with similar characteristics. In addition, we integrate implicit appearance compression and generalize Sub-Vector Quantization (SVQ) to 4D representations, further reducing storage while preserving quality. Extensive experiments on standard benchmark datasets demonstrate that OMG4 significantly outperforms recent state-of-the-art methods, reducing model sizes by over 60% while maintaining reconstruction quality. These results position OMG4 as a significant step forward in compact 4D scene representation, opening new possibilities for a wide range of applications. Our source code is available at https://minshirley.github.io/OMG4/.

</details>


### [25] [SDAKD: Student Discriminator Assisted Knowledge Distillation for Super-Resolution Generative Adversarial Networks](https://arxiv.org/abs/2510.03870)
*Nikolaos Kaparinos,Vasileios Mezaris*

Main category: cs.CV

TL;DR: 提出SDAKD方法，通过引入学生判别器来解决GAN知识蒸馏中的容量不匹配问题，在图像超分辨率任务上取得了优于基线方法和SOTA方法的性能。


<details>
  <summary>Details</summary>
Motivation: GANs在生成任务中表现优异，但计算需求大，难以部署在资源受限设备上。知识蒸馏是GAN压缩的有前景方向，但由于学生生成器与教师判别器之间的容量不匹配，有效训练小型学生生成器具有挑战性。

Method: 提出学生判别器辅助知识蒸馏(SDAKD)，引入学生判别器来缓解容量不匹配问题。采用三阶段训练策略，并在最后两个训练阶段集成适配的特征图蒸馏方法。

Result: 在GCFSR和Real-ESRGAN两个高性能超分辨率GAN上评估SDAKD，实验表明该方法在基线和SOTA GAN知识蒸馏方法上取得了一致的改进。

Conclusion: SDAKD通过引入学生判别器有效解决了GAN知识蒸馏中的容量不匹配问题，在超分辨率任务上表现优异，代码将在论文接受后开源。

Abstract: Generative Adversarial Networks (GANs) achieve excellent performance in generative tasks, such as image super-resolution, but their computational requirements make difficult their deployment on resource-constrained devices. While knowledge distillation is a promising research direction for GAN compression, effectively training a smaller student generator is challenging due to the capacity mismatch between the student generator and the teacher discriminator. In this work, we propose Student Discriminator Assisted Knowledge Distillation (SDAKD), a novel GAN distillation methodology that introduces a student discriminator to mitigate this capacity mismatch. SDAKD follows a three-stage training strategy, and integrates an adapted feature map distillation approach in its last two training stages. We evaluated SDAKD on two well-performing super-resolution GANs, GCFSR and Real-ESRGAN. Our experiments demonstrate consistent improvements over the baselines and SOTA GAN knowledge distillation methods. The SDAKD source code will be made openly available upon acceptance of the paper.

</details>


### [26] [DHQA-4D: Perceptual Quality Assessment of Dynamic 4D Digital Human](https://arxiv.org/abs/2510.03874)
*Yunhao Li,Sijing Wu,Yucheng Zhu,Huiyu Duan,Zicheng Zhang,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出了DHQA-4D数据集和DynaMesh-Rater模型，用于动态4D数字人网格的质量评估，通过多维度特征提取和大语言模型集成来预测质量分数。


<details>
  <summary>Details</summary>
Motivation: 随着3D扫描和重建技术的发展，基于4D网格的动态数字人化身越来越流行，但在采集、压缩和传输过程中容易受到各种噪声影响，影响用户体验，因此需要质量评估方法。

Method: 首先构建了DHQA-4D数据集，包含32个高质量4D人体网格序列和1920个失真网格。然后提出DynaMesh-Rater模型，从投影2D视频提取视觉特征，从裁剪视频片段提取运动特征，从4D网格提取几何特征，利用大语言模型集成这些多维度特征，并通过LoRA指令调优来预测质量分数。

Result: 在DHQA-4D数据集上的大量实验结果表明，DynaMesh-Rater方法优于之前的质量评估方法。

Conclusion: 该研究为动态4D数字人网格质量评估提供了有效的数据集和方法，能够同时处理带纹理和不带纹理的4D网格。

Abstract: With the rapid development of 3D scanning and reconstruction technologies, dynamic digital human avatars based on 4D meshes have become increasingly popular. A high-precision dynamic digital human avatar can be applied to various fields such as game production, animation generation, and remote immersive communication. However, these 4D human avatar meshes are prone to being degraded by various types of noise during the processes of collection, compression, and transmission, thereby affecting the viewing experience of users. In light of this fact, quality assessment of dynamic 4D digital humans becomes increasingly important. In this paper, we first propose a large-scale dynamic digital human quality assessment dataset, DHQA-4D, which contains 32 high-quality real-scanned 4D human mesh sequences, 1920 distorted textured 4D human meshes degraded by 11 textured distortions, as well as their corresponding textured and non-textured mean opinion scores (MOSs). Equipped with DHQA-4D dataset, we analyze the influence of different types of distortion on human perception for textured dynamic 4D meshes and non-textured dynamic 4D meshes. Additionally, we propose DynaMesh-Rater, a novel large multimodal model (LMM) based approach that is able to assess both textured 4D meshes and non-textured 4D meshes. Concretely, DynaMesh-Rater elaborately extracts multi-dimensional features, including visual features from a projected 2D video, motion features from cropped video clips, and geometry features from the 4D human mesh to provide comprehensive quality-related information. Then we utilize a LMM model to integrate the multi-dimensional features and conduct a LoRA-based instruction tuning technique to teach the LMM model to predict the quality scores. Extensive experimental results on the DHQA-4D dataset demonstrate the superiority of our DynaMesh-Rater method over previous quality assessment methods.

</details>


### [27] [Generating Human Motion Videos using a Cascaded Text-to-Video Framework](https://arxiv.org/abs/2510.03909)
*Hyelin Nam,Hyojun Go,Byeongjun Park,Byung-Hoon Kim,Hyungjin Chung*

Main category: cs.CV

TL;DR: CAMEO是一个级联框架，通过连接文本到动作模型和条件视频扩散模型，实现通用人体运动视频生成，解决了训练和推理过程中的对齐问题。


<details>
  <summary>Details</summary>
Motivation: 尽管视频扩散模型快速发展，但在通用人体视频生成方面仍未被充分探索，现有工作大多局限于图像到视频设置或舞蹈视频等狭窄领域。

Method: 提出CAMEO级联框架，分析并准备文本提示和视觉条件来有效训练VDM，确保运动描述、条件信号和生成视频之间的稳健对齐，并引入相机感知条件模块自动选择与输入文本对齐的视角。

Result: 在MovieGen基准和新引入的T2M-VDM组合基准上证明了方法的有效性，展示了其在多样化用例中的多功能性。

Conclusion: CAMEO框架成功弥合了文本到动作模型和条件视频扩散模型之间的差距，通过精心设计的组件提升了人体运动视频生成的质量和一致性。

Abstract: Human video generation is becoming an increasingly important task with broad applications in graphics, entertainment, and embodied AI. Despite the rapid progress of video diffusion models (VDMs), their use for general-purpose human video generation remains underexplored, with most works constrained to image-to-video setups or narrow domains like dance videos. In this work, we propose CAMEO, a cascaded framework for general human motion video generation. It seamlessly bridges Text-to-Motion (T2M) models and conditional VDMs, mitigating suboptimal factors that may arise in this process across both training and inference through carefully designed components. Specifically, we analyze and prepare both textual prompts and visual conditions to effectively train the VDM, ensuring robust alignment between motion descriptions, conditioning signals, and the generated videos. Furthermore, we introduce a camera-aware conditioning module that connects the two stages, automatically selecting viewpoints aligned with the input text to enhance coherence and reduce manual intervention. We demonstrate the effectiveness of our approach on both the MovieGen benchmark and a newly introduced benchmark tailored to the T2M-VDM combination, while highlighting its versatility across diverse use cases.

</details>


### [28] [Prompt-to-Prompt: Text-Based Image Editing Via Cross-Attention Mechanisms -- The Research of Hyperparameters and Novel Mechanisms to Enhance Existing Frameworks](https://arxiv.org/abs/2510.04034)
*Linn Bieske,Carla Lorente*

Main category: cs.CV

TL;DR: 本文研究了稳定扩散模型中的超参数优化，提出了三种方法来提高文本驱动图像编辑的精确性和可靠性：词替换方法、注意力重加权方法和CL P2P框架。


<details>
  <summary>Details</summary>
Motivation: 当前基于稳定扩散模型的图像编辑方法虽然简化了编辑过程，但结果存在可变性（如头发颜色变化不一致），需要提高精确性和可靠性。

Method: 1. 全面研究"词替换"方法；2. 开发"注意力重加权方法"以提高适应性；3. 提出"CL P2P"框架解决循环不一致等现有限制。

Result: 通过优化超参数设置与神经网络模型架构选择（特别是注意力机制）的交互，显著影响了生成图像的构图和质量。

Conclusion: 这项工作有助于理解和改进超参数设置与神经网络模型架构选择之间的相互作用，特别是注意力机制对生成图像质量和构图的重要影响。

Abstract: Recent advances in image editing have shifted from manual pixel manipulation to employing deep learning methods like stable diffusion models, which now leverage cross-attention mechanisms for text-driven control. This transition has simplified the editing process but also introduced variability in results, such as inconsistent hair color changes. Our research aims to enhance the precision and reliability of prompt-to-prompt image editing frameworks by exploring and optimizing hyperparameters. We present a comprehensive study of the "word swap" method, develop an "attention re-weight method" for better adaptability, and propose the "CL P2P" framework to address existing limitations like cycle inconsistency. This work contributes to understanding and improving the interaction between hyperparameter settings and the architectural choices of neural network models, specifically their attention mechanisms, which significantly influence the composition and quality of the generated images.

</details>


### [29] [Diffusion Low Rank Hybrid Reconstruction for Sparse View Medical Imaging](https://arxiv.org/abs/2510.04069)
*Zongyin Deng,Qing Zhou,Yuhao Fang,Zijian Wang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: TV-LoRA是一种结合扩散生成先验和多正则化约束的低剂量稀疏视图CT重建方法，在ADMM框架下实现高效3D重建


<details>
  <summary>Details</summary>
Motivation: 解决极稀疏视图下CT重建的病态问题和纹理丢失问题，结合生成先验和物理约束来提高重建质量

Method: 结合扩散生成先验(NCSN++ SDE建模)和各向异性TV、核范数(LoRA)多正则化约束，采用ADMM框架、2D切片策略、FFT加速和并行优化

Result: 在AAPM-2016、CTHD、LIDC数据集上，TV-LoRA在SSIM、纹理恢复、边缘清晰度和伪影抑制方面均优于基准方法，展示了强鲁棒性和泛化性

Conclusion: 扩散模型+TV-LoRA实现了高保真、高效的3D CT重建，在低剂量稀疏采样场景中具有广泛的临床适用性

Abstract: This work presents TV-LoRA, a novel method for low-dose sparse-view CT reconstruction that combines a diffusion generative prior (NCSN++ with SDE modeling) and multi-regularization constraints, including anisotropic TV and nuclear norm (LoRA), within an ADMM framework. To address ill-posedness and texture loss under extremely sparse views, TV-LoRA integrates generative and physical constraints, and utilizes a 2D slice-based strategy with FFT acceleration and tensor-parallel optimization for efficient inference. Experiments on AAPM-2016, CTHD, and LIDC datasets with $N_{\mathrm{view}}=8,4,2$ show that TV-LoRA consistently surpasses benchmarks in SSIM, texture recovery, edge clarity, and artifact suppression, demonstrating strong robustness and generalizability. Ablation studies confirm the complementary effects of LoRA regularization and diffusion priors, while the FFT-PCG module provides a speedup. Overall, Diffusion + TV-LoRA achieves high-fidelity, efficient 3D CT reconstruction and broad clinical applicability in low-dose, sparse-sampling scenarios.

</details>


### [30] [Learning Efficient Meshflow and Optical Flow from Event Cameras](https://arxiv.org/abs/2510.04111)
*Xinglong Luo,Ao Luo,Kunming Luo,Zhengning Wang,Ping Tan,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 本文提出了基于事件相机的网格流估计新任务，创建了高分辨率事件网格流数据集HREM，开发了轻量级EEMFlow网络，并进一步扩展到支持密集光流和自适应密度调整。


<details>
  <summary>Details</summary>
Motivation: 现有事件流估计方法存在两个关键问题：缺乏专门的网格流数据集和方法，以及未充分探索事件数据密度挑战。

Method: 生成HREM高分辨率数据集，提出轻量级EEMFlow网络架构，添加置信度诱导细节补全模块，并开发自适应密度模块来优化输入事件数据密度。

Result: EEMFlow模型在性能和运行效率上均优于现有方法（快30倍），自适应密度模块将EEMFlow和EEMFlow+性能分别提升8%和10%。

Conclusion: 该工作为事件相机网格流估计提供了首个专用数据集和高效方法，解决了数据密度适应性挑战，显著提升了模型性能。

Abstract: In this paper, we explore the problem of event-based meshflow estimation, a novel task that involves predicting a spatially smooth sparse motion field from event cameras. To start, we review the state-of-the-art in event-based flow estimation, highlighting two key areas for further research: i) the lack of meshflow-specific event datasets and methods, and ii) the underexplored challenge of event data density. First, we generate a large-scale High-Resolution Event Meshflow (HREM) dataset, which showcases its superiority by encompassing the merits of high resolution at 1280x720, handling dynamic objects and complex motion patterns, and offering both optical flow and meshflow labels. These aspects have not been fully explored in previous works. Besides, we propose Efficient Event-based MeshFlow (EEMFlow) network, a lightweight model featuring a specially crafted encoder-decoder architecture to facilitate swift and accurate meshflow estimation. Furthermore, we upgrade EEMFlow network to support dense event optical flow, in which a Confidence-induced Detail Completion (CDC) module is proposed to preserve sharp motion boundaries. We conduct comprehensive experiments to show the exceptional performance and runtime efficiency (30x faster) of our EEMFlow model compared to the recent state-of-the-art flow method. As an extension, we expand HREM into HREM+, a multi-density event dataset contributing to a thorough study of the robustness of existing methods across data with varying densities, and propose an Adaptive Density Module (ADM) to adjust the density of input event data to a more optimal range, enhancing the model's generalization ability. We empirically demonstrate that ADM helps to significantly improve the performance of EEMFlow and EEMFlow+ by 8% and 10%, respectively. Code and dataset are released at https://github.com/boomluo02/EEMFlowPlus.

</details>


### [31] [Joint Learning of Pose Regression and Denoising Diffusion with Score Scaling Sampling for Category-level 6D Pose Estimation](https://arxiv.org/abs/2510.04125)
*Seunghyun Lee,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: 提出一种新的6D物体姿态估计方法，通过预训练编码器和联合学习策略加速训练收敛，同时采用采样指导消除额外评估网络需求，在多个基准测试中达到最先进精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的6D物体姿态估计方法存在训练收敛慢、需要端到端学习编码器、以及需要额外网络来过滤低质量姿态候选的问题。

Method: 1) 预训练编码器并使用直接姿态回归头，联合学习回归头和去噪扩散头；2) 提出基于时间相关分数缩放的采样指导，有效平衡探索与利用，无需额外评估网络。

Result: 在REAL275、HouseCat6D和ROPE等多个基准测试中，该方法即使使用单姿态推理也能达到最先进精度，同时在训练和推理方面都更高效。

Conclusion: 该方法简单有效，通过预训练策略和采样指导解决了现有方法的局限性，在6D物体姿态估计任务中表现出色。

Abstract: Latest diffusion models have shown promising results in category-level 6D object pose estimation by modeling the conditional pose distribution with depth image input. The existing methods, however, suffer from slow convergence during training, learning its encoder with the diffusion denoising network in end-to-end fashion, and require an additional network that evaluates sampled pose hypotheses to filter out low-quality pose candidates. In this paper, we propose a novel pipeline that tackles these limitations by two key components. First, the proposed method pretrains the encoder with the direct pose regression head, and jointly learns the networks via the regression head and the denoising diffusion head, significantly accelerating training convergence while achieving higher accuracy. Second, sampling guidance via time-dependent score scaling is proposed s.t. the exploration-exploitation trade-off is effectively taken, eliminating the need for the additional evaluation network. The sampling guidance maintains multi-modal characteristics of symmetric objects at early denoising steps while ensuring high-quality pose generation at final steps. Extensive experiments on multiple benchmarks including REAL275, HouseCat6D, and ROPE, demonstrate that the proposed method, simple yet effective, achieves state-of-the-art accuracies even with single-pose inference, while being more efficient in both training and inference.

</details>


### [32] [Let Features Decide Their Own Solvers: Hybrid Feature Caching for Diffusion Transformers](https://arxiv.org/abs/2510.04188)
*Shikang Zheng,Guantao Chen,Qinming Zhou,Yuqi Lin,Lixuan He,Chang Zou,Peiliang Cai,Jiacheng Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: HyCa是一个基于混合ODE求解器的缓存框架，通过维度级缓存策略加速扩散变换器的采样过程，在多个模型上实现5-6倍加速且几乎无损。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器的迭代采样过程由于每步都需要昂贵的transformer前向传播而成为主要瓶颈，现有特征缓存方法对所有特征维度采用统一策略，忽略了它们异质的动态行为。

Method: 将隐藏特征演化建模为跨维度的ODE混合，引入HyCa框架应用维度级缓存策略，基于混合ODE求解器原理。

Result: 在FLUX上实现5.55倍加速，HunyuanVideo上5.56倍加速，Qwen-Image和Qwen-Image-Edit上6.24倍加速，无需重新训练。

Conclusion: HyCa通过维度级缓存策略有效解决了扩散变换器采样瓶颈，在多个领域和模型上实现了近无损的高效加速。

Abstract: Diffusion Transformers offer state-of-the-art fidelity in image and video synthesis, but their iterative sampling process remains a major bottleneck due to the high cost of transformer forward passes at each timestep. To mitigate this, feature caching has emerged as a training-free acceleration technique that reuses or forecasts hidden representations. However, existing methods often apply a uniform caching strategy across all feature dimensions, ignoring their heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by modeling hidden feature evolution as a mixture of ODEs across dimensions, and introduce HyCa, a Hybrid ODE solver inspired caching framework that applies dimension-wise caching strategies. HyCa achieves near-lossless acceleration across diverse domains and models, including 5.55 times speedup on FLUX, 5.56 times speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and Qwen-Image-Edit without retraining.

</details>


### [33] [World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge](https://arxiv.org/abs/2510.04201)
*Moo Hyun Son,Jintaek Oh,Sun Bin Mun,Jaechul Roh,Sehyun Choi*

Main category: cs.CV

TL;DR: World-To-Image框架通过代理驱动的网络搜索获取未知概念图像，进行多模态提示优化，显著提升文本到图像模型对新颖实体的生成能力，在语义对齐和视觉美学方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像模型在处理新颖或分布外实体时性能显著下降的问题，因为模型存在固有的知识截止限制。

Method: 设计代理动态搜索网络获取基础模型未知概念的图像，然后进行多模态提示优化，引导生成模型实现准确合成。

Result: 在NICE基准测试中，语义对齐准确率提升8.1%，在语义对齐和视觉美学方面显著优于最先进方法，且仅需少于3次迭代即可高效实现。

Conclusion: 该框架为文本到图像系统更好地反映不断变化的现实世界铺平了道路，实现了高效率和显著性能提升。

Abstract: While text-to-image (T2I) models can synthesize high-quality images, their performance degrades significantly when prompted with novel or out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We introduce World-To-Image, a novel framework that bridges this gap by empowering T2I generation with agent-driven world knowledge. We design an agent that dynamically searches the web to retrieve images for concepts unknown to the base model. This information is then used to perform multimodal prompt optimization, steering powerful generative backbones toward an accurate synthesis. Critically, our evaluation goes beyond traditional metrics, utilizing modern assessments like LLMGrader and ImageReward to measure true semantic fidelity. Our experiments show that World-To-Image substantially outperforms state-of-the-art methods in both semantic alignment and visual aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated NICE benchmark. Our framework achieves these results with high efficiency in less than three iterations, paving the way for T2I systems that can better reflect the ever-changing real world. Our demo code is available here\footnote{https://github.com/mhson-kyle/World-To-Image}.

</details>


### [34] [MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering](https://arxiv.org/abs/2510.04220)
*Lixuan He,Shikang Zheng,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出了MASC框架，通过构建层次化语义树来结构化视觉标记的预测空间，显著提升自回归图像生成的训练效率和生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统自回归模型将视觉标记视为扁平词汇表，忽略了标记嵌入空间的内在结构，导致预测任务复杂、训练效率低下且生成质量受限。

Method: MASC框架使用几何感知距离度量和密度驱动的凝聚构造方法，从码本内在结构构建层次化语义树，将高维扁平预测任务转化为结构化层次任务。

Result: 训练速度提升高达57%，LlamaGen-XL的FID从2.87降至2.58，显著改善生成质量，使自回归框架与最先进方法具有竞争力。

Conclusion: 结构化预测空间与架构创新同等重要，MASC证明了通过建模标记嵌入的底层流形可以显著简化学习问题，提升可扩展生成建模性能。

Abstract: Autoregressive (AR) models have shown great promise in image generation, yet they face a fundamental inefficiency stemming from their core component: a vast, unstructured vocabulary of visual tokens. This conventional approach treats tokens as a flat vocabulary, disregarding the intrinsic structure of the token embedding space where proximity often correlates with semantic similarity. This oversight results in a highly complex prediction task, which hinders training efficiency and limits final generation quality. To resolve this, we propose Manifold-Aligned Semantic Clustering (MASC), a principled framework that constructs a hierarchical semantic tree directly from the codebook's intrinsic structure. MASC employs a novel geometry-aware distance metric and a density-driven agglomerative construction to model the underlying manifold of the token embeddings. By transforming the flat, high-dimensional prediction task into a structured, hierarchical one, MASC introduces a beneficial inductive bias that significantly simplifies the learning problem for the AR model. MASC is designed as a plug-and-play module, and our extensive experiments validate its effectiveness: it accelerates training by up to 57% and significantly improves generation quality, reducing the FID of LlamaGen-XL from 2.87 to 2.58. MASC elevates existing AR frameworks to be highly competitive with state-of-the-art methods, establishing that structuring the prediction space is as crucial as architectural innovation for scalable generative modeling.

</details>


### [35] [Scaling Sequence-to-Sequence Generative Neural Rendering](https://arxiv.org/abs/2510.04236)
*Shikun Liu,Kam Woh Ng,Wonbong Jang,Jiadong Guo,Junlin Han,Haozhe Liu,Yiannis Douratsos,Juan C. Pérez,Zijian Zhou,Chi Phung,Tao Xiang,Juan-Manuel Pérez-Rúa*

Main category: cs.CV

TL;DR: Kaleido是一种生成式神经渲染模型，将3D视为视频的特殊子域，通过序列到序列的图像合成实现无显式3D表示的新视角生成。


<details>
  <summary>Details</summary>
Motivation: 解决传统神经渲染方法对显式3D表示和稀缺相机标注数据的依赖，通过统一3D和视频建模框架，利用大规模视频数据进行预训练。

Method: 采用仅解码器的整流流变换器架构，通过掩码自回归框架实现任意数量参考视图到任意数量目标视图的生成，统一处理3D和视频建模。

Result: 在多个视角合成基准测试中达到最先进水平，在少视图设置下显著优于其他生成方法，在多视图设置下首次达到逐场景优化方法的质量。

Conclusion: Kaleido证明了通过序列到序列方法统一3D和视频建模的可行性，利用大规模视频数据预训练显著提升了空间一致性和渲染质量。

Abstract: We present Kaleido, a family of generative models designed for photorealistic, unified object- and scene-level neural rendering. Kaleido operates on the principle that 3D can be regarded as a specialised sub-domain of video, expressed purely as a sequence-to-sequence image synthesis task. Through a systemic study of scaling sequence-to-sequence generative neural rendering, we introduce key architectural innovations that enable our model to: i) perform generative view synthesis without explicit 3D representations; ii) generate any number of 6-DoF target views conditioned on any number of reference views via a masked autoregressive framework; and iii) seamlessly unify 3D and video modelling within a single decoder-only rectified flow transformer. Within this unified framework, Kaleido leverages large-scale video data for pre-training, which significantly improves spatial consistency and reduces reliance on scarce, camera-labelled 3D datasets -- all without any architectural modifications. Kaleido sets a new state-of-the-art on a range of view synthesis benchmarks. Its zero-shot performance substantially outperforms other generative methods in few-view settings, and, for the first time, matches the quality of per-scene optimisation methods in many-view settings.

</details>


### [36] [The best performance in the CARE 2025 -- Liver Task (LiSeg-Contrast): Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and Test-Time Adaptation](https://arxiv.org/abs/2510.04243)
*Jincan Lou,Jingkun Chen,Haoquan Li,Hang Li,Wenjian Huang,Weihua Chen,Fan Wang,Jianguo Zhang*

Main category: cs.CV

TL;DR: 提出了CoSSeg-TTA框架，用于增强MRI肝脏分割，结合半监督学习和域适应技术解决标注数据有限和跨中心域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 解决对比增强MRI肝脏分割面临的标注数据有限、增强协议异构以及跨扫描仪和机构的域偏移等挑战。

Method: 基于nnU-Netv2构建，集成半监督均值教师方案利用未标注数据，包含随机化直方图风格外观转换和可训练对比感知网络的域适应模块，以及持续测试时适应策略。

Result: 在低标注条件下，框架持续优于nnU-Netv2基线，获得更高的Dice分数和Hausdorff距离，并对未见域表现出强泛化能力。

Conclusion: CoSSeg-TTA框架能有效解决单模态MRI肝脏分割中的域泛化问题，在有限标注数据下实现鲁棒分割性能。

Abstract: Accurate liver segmentation from contrast-enhanced MRI is essential for diagnosis, treatment planning, and disease monitoring. However, it remains challenging due to limited annotated data, heterogeneous enhancement protocols, and significant domain shifts across scanners and institutions. Traditional image-to-image translation frameworks have made great progress in domain generalization, but their application is not straightforward. For example, Pix2Pix requires image registration, and cycle-GAN cannot be integrated seamlessly into segmentation pipelines. Meanwhile, these methods are originally used to deal with cross-modality scenarios, and often introduce structural distortions and suffer from unstable training, which may pose drawbacks in our single-modality scenario. To address these challenges, we propose CoSSeg-TTA, a compact segmentation framework for the GED4 (Gd-EOB-DTPA enhanced hepatobiliary phase MRI) modality built upon nnU-Netv2 and enhanced with a semi-supervised mean teacher scheme to exploit large amounts of unlabeled volumes. A domain adaptation module, incorporating a randomized histogram-based style appearance transfer function and a trainable contrast-aware network, enriches domain diversity and mitigates cross-center variability. Furthermore, a continual test-time adaptation strategy is employed to improve robustness during inference. Extensive experiments demonstrate that our framework consistently outperforms the nnU-Netv2 baseline, achieving superior Dice score and Hausdorff Distance while exhibiting strong generalization to unseen domains under low-annotation conditions.

</details>


### [37] [ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation](https://arxiv.org/abs/2510.04290)
*Jay Zhangjie Wu,Xuanchi Ren,Tianchang Shen,Tianshi Cao,Kai He,Yifan Lu,Ruiyuan Gao,Enze Xie,Shiyi Lan,Jose M. Alvarez,Jun Gao,Sanja Fidler,Zian Wang,Huan Ling*

Main category: cs.CV

TL;DR: ChronoEdit将图像编辑重构为视频生成问题，利用预训练视频生成模型来确保物理一致性，通过时间推理阶段生成物理可行的编辑轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有大型生成模型在图像编辑和上下文图像生成方面取得进展，但在确保物理一致性方面存在关键差距，这对于世界模拟相关任务尤为重要。

Method: 1) 将输入和编辑图像视为视频的首尾帧，利用预训练视频生成模型；2) 引入时间推理阶段，在推理时联合去噪目标帧和推理令牌，想象物理可行的编辑轨迹；3) 推理令牌在几步后被丢弃以避免完整视频渲染的高计算成本。

Result: 在PBench-Edit新基准测试中，ChronoEdit在视觉保真度和物理合理性方面均优于最先进的基线方法。

Conclusion: ChronoEdit通过将图像编辑重构为视频生成问题，有效解决了物理一致性问题，为需要物理一致性的图像编辑任务提供了新思路。

Abstract: Recent advances in large generative models have significantly advanced image editing and in-context image generation, yet a critical gap remains in ensuring physical consistency, where edited objects must remain coherent. This capability is especially vital for world simulation related tasks. In this paper, we present ChronoEdit, a framework that reframes image editing as a video generation problem. First, ChronoEdit treats the input and edited images as the first and last frames of a video, allowing it to leverage large pretrained video generative models that capture not only object appearance but also the implicit physics of motion and interaction through learned temporal consistency. Second, ChronoEdit introduces a temporal reasoning stage that explicitly performs editing at inference time. Under this setting, the target frame is jointly denoised with reasoning tokens to imagine a plausible editing trajectory that constrains the solution space to physically viable transformations. The reasoning tokens are then dropped after a few steps to avoid the high computational cost of rendering a full video. To validate ChronoEdit, we introduce PBench-Edit, a new benchmark of image-prompt pairs for contexts that require physical consistency, and demonstrate that ChronoEdit surpasses state-of-the-art baselines in both visual fidelity and physical plausibility. Code and models for both the 14B and 2B variants of ChronoEdit will be released on the project page: https://research.nvidia.com/labs/toronto-ai/chronoedit

</details>


### [38] [Diffusion^2: Dual Diffusion Model with Uncertainty-Aware Adaptive Noise for Momentary Trajectory Prediction](https://arxiv.org/abs/2510.04365)
*Yuhao Luo,Yuang Zhang,Kehua Chen,Xinyu Zheng,Shucheng Zhang,Sikai Chen,Yinhai Wang*

Main category: cs.CV

TL;DR: 提出了Diffusion^2框架，通过两个连接的扩散模型解决瞬时轨迹预测问题：一个用于生成未观测的历史轨迹，另一个用于预测未来轨迹，并在ETH/UCY和Stanford Drone数据集上达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶和人机交互场景中，当行人突然从盲区出现时，往往缺乏足够的观测数据（瞬时轨迹），这使得准确预测变得困难，增加了交通事故风险。

Method: 使用两个顺序连接的扩散模型：反向预测模型生成未观测的历史轨迹，前向预测模型预测未来轨迹。提出双头参数化机制估计生成轨迹的不确定性，设计时间自适应噪声模块动态调节前向扩散过程的噪声尺度。

Result: 在ETH/UCY和Stanford Drone数据集上实现了最先进的瞬时轨迹预测性能。

Conclusion: Diffusion^2框架通过生成未观测历史轨迹和不确定性建模，有效解决了瞬时轨迹预测的挑战，为提升交通安全提供了重要技术支撑。

Abstract: Accurate pedestrian trajectory prediction is crucial for ensuring safety and efficiency in autonomous driving and human-robot interaction scenarios. Earlier studies primarily utilized sufficient observational data to predict future trajectories. However, in real-world scenarios, such as pedestrians suddenly emerging from blind spots, sufficient observational data is often unavailable (i.e. momentary trajectory), making accurate prediction challenging and increasing the risk of traffic accidents. Therefore, advancing research on pedestrian trajectory prediction under extreme scenarios is critical for enhancing traffic safety. In this work, we propose a novel framework termed Diffusion^2, tailored for momentary trajectory prediction. Diffusion^2 consists of two sequentially connected diffusion models: one for backward prediction, which generates unobserved historical trajectories, and the other for forward prediction, which forecasts future trajectories. Given that the generated unobserved historical trajectories may introduce additional noise, we propose a dual-head parameterization mechanism to estimate their aleatoric uncertainty and design a temporally adaptive noise module that dynamically modulates the noise scale in the forward diffusion process. Empirically, Diffusion^2 sets a new state-of-the-art in momentary trajectory prediction on ETH/UCY and Stanford Drone datasets.

</details>


### [39] [TBStar-Edit: From Image Editing Pattern Shifting to Consistency Enhancement](https://arxiv.org/abs/2510.04483)
*Hao Fang,Zechao Zhan,Weixin Feng,Ziwei Huang,XuBin Li,Tiezheng Ge*

Main category: cs.CV

TL;DR: TBStar-Edit是一个专门为电商领域设计的图像编辑模型，通过数据工程、模型架构设计和两阶段训练策略，实现了精确高保真的图像编辑，同时保持产品外观和布局的完整性。


<details>
  <summary>Details</summary>
Motivation: 现有的通用图像生成和编辑模型在电商场景中经常遇到一致性问题，无法很好地保持产品外观和布局的完整性，因此需要专门针对电商领域开发定制化的编辑模型。

Method: 1) 建立完整的数据构建流程（收集、构建、过滤、增强）；2) 设计分层模型框架（基础模型、模式转换模块、一致性增强模块）；3) 采用两阶段训练策略（第一阶段模式转换训练，第二阶段一致性增强训练）。

Result: 在自建的电商基准测试中，TBStar-Edit在客观指标（VIE Score）和主观用户偏好方面均优于现有的通用领域编辑模型。

Conclusion: TBStar-Edit通过专门针对电商场景的定制化设计，成功解决了通用模型在电商图像编辑中的一致性问题，实现了高保真的编辑效果。

Abstract: Recent advances in image generation and editing technologies have enabled state-of-the-art models to achieve impressive results in general domains. However, when applied to e-commerce scenarios, these general models often encounter consistency limitations. To address this challenge, we introduce TBStar-Edit, an new image editing model tailored for the e-commerce domain. Through rigorous data engineering, model architecture design and training strategy, TBStar-Edit achieves precise and high-fidelity image editing while maintaining the integrity of product appearance and layout. Specifically, for data engineering, we establish a comprehensive data construction pipeline, encompassing data collection, construction, filtering, and augmentation, to acquire high-quality, instruction-following, and strongly consistent editing data to support model training. For model architecture design, we design a hierarchical model framework consisting of a base model, pattern shifting modules, and consistency enhancement modules. For model training, we adopt a two-stage training strategy to enhance the consistency preservation: first stage for editing pattern shifting, and second stage for consistency enhancement. Each stage involves training different modules with separate datasets. Finally, we conduct extensive evaluations of TBStar-Edit on a self-proposed e-commerce benchmark, and the results demonstrate that TBStar-Edit outperforms existing general-domain editing models in both objective metrics (VIE Score) and subjective user preference.

</details>


### [40] [Asynchronous Denoising Diffusion Models for Aligning Text-to-Image Generation](https://arxiv.org/abs/2510.04504)
*Zijing Hu,Yunze Tong,Fengda Zhang,Junkun Yuan,Jun Xiao,Kun Kuang*

Main category: cs.CV

TL;DR: 提出异步扩散模型框架，通过为不同像素分配不同时间步，让提示相关区域比无关区域更缓慢去噪，从而利用更清晰的像素间上下文，显著提升文本到图像的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型采用同步去噪，所有像素同时从噪声演化到清晰图像，导致提示相关区域只能参考相同噪声水平的无关区域，无法获得清晰上下文，最终影响文本到图像对齐效果。

Method: 提出异步扩散模型，为不同像素分配不同时间步，动态调节各像素的时间步调度，使提示相关区域比无关区域更缓慢去噪，从而利用更清晰的像素间上下文。

Result: 大量实验表明，异步扩散模型能显著提升各种提示下的文本到图像对齐效果。

Conclusion: 异步扩散模型通过异步去噪机制有效解决了传统扩散模型在文本到图像对齐方面的局限性，为生成更忠实于输入提示的图像提供了新思路。

Abstract: Diffusion models have achieved impressive results in generating high-quality images. Yet, they often struggle to faithfully align the generated images with the input prompts. This limitation arises from synchronous denoising, where all pixels simultaneously evolve from random noise to clear images. As a result, during generation, the prompt-related regions can only reference the unrelated regions at the same noise level, failing to obtain clear context and ultimately impairing text-to-image alignment. To address this issue, we propose asynchronous diffusion models -- a novel framework that allocates distinct timesteps to different pixels and reformulates the pixel-wise denoising process. By dynamically modulating the timestep schedules of individual pixels, prompt-related regions are denoised more gradually than unrelated regions, thereby allowing them to leverage clearer inter-pixel context. Consequently, these prompt-related regions achieve better alignment in the final images. Extensive experiments demonstrate that our asynchronous diffusion models can significantly improve text-to-image alignment across diverse prompts. The code repository for this work is available at https://github.com/hu-zijing/AsynDM.

</details>


### [41] [TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion Sampling](https://arxiv.org/abs/2510.04533)
*Hyunmin Cho,Donghoon Ahn,Susung Hong,Jee Eun Kim,Seungryong Kim,Kyong Hwan Jin*

Main category: cs.CV

TL;DR: 提出TAG方法，通过放大估计分数的切向分量来校正采样轨迹，提高扩散模型生成质量，无需修改底层模型架构。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在图像生成中存在语义不一致或幻觉问题，现有推理时引导方法通常依赖外部信号或架构修改，引入额外计算开销。

Method: 利用中间样本作为投影基，放大估计分数相对于该基的切向分量来校正采样轨迹，基于一阶泰勒展开形式化引导过程。

Result: TAG是一种即插即用、架构无关的模块，能以最小计算开销提高扩散采样保真度。

Conclusion: TAG为扩散引导提供了新视角，通过直接操作轨迹信号实现高效引导，减少不一致性并提升样本质量。

Abstract: Recent diffusion models achieve the state-of-the-art performance in image generation, but often suffer from semantic inconsistencies or hallucinations. While various inference-time guidance methods can enhance generation, they often operate indirectly by relying on external signals or architectural modifications, which introduces additional computational overhead. In this paper, we propose Tangential Amplifying Guidance (TAG), a more efficient and direct guidance method that operates solely on trajectory signals without modifying the underlying diffusion model. TAG leverages an intermediate sample as a projection basis and amplifies the tangential components of the estimated scores with respect to this basis to correct the sampling trajectory. We formalize this guidance process by leveraging a first-order Taylor expansion, which demonstrates that amplifying the tangential component steers the state toward higher-probability regions, thereby reducing inconsistencies and enhancing sample quality. TAG is a plug-and-play, architecture-agnostic module that improves diffusion sampling fidelity with minimal computational addition, offering a new perspective on diffusion guidance.

</details>


### [42] [ReactDiff: Fundamental Multiple Appropriate Facial Reaction Diffusion Model](https://arxiv.org/abs/2510.04712)
*Luo Cheng,Song Siyang,Yan Siyuan,Yu Zhen,Ge Zongyuan*

Main category: cs.CV

TL;DR: ReactDiff是一个用于生成多样化面部反应的时间扩散框架，通过整合时空面部运动学和面部动作单元依赖关系来提升反应的真实性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法模拟真实人类反应的随机性和动态特性，导致生成的面部反应缺乏多样性和真实感。

Method: 提出ReactDiff时间扩散框架，整合两个关键先验：时间面部行为运动学和面部动作单元依赖关系，引导模型生成符合人类面部解剖约束的平滑连贯反应。

Result: 在REACT2024数据集上的实验表明，该方法在反应质量、多样性和反应适切性方面均达到最先进水平。

Conclusion: ReactDiff通过整合面部运动学和动作单元约束，成功生成了多样化且符合人类面部解剖的面部反应，解决了现有方法的局限性。

Abstract: The automatic generation of diverse and human-like facial reactions in dyadic dialogue remains a critical challenge for human-computer interaction systems. Existing methods fail to model the stochasticity and dynamics inherent in real human reactions. To address this, we propose ReactDiff, a novel temporal diffusion framework for generating diverse facial reactions that are appropriate for responding to any given dialogue context. Our key insight is that plausible human reactions demonstrate smoothness, and coherence over time, and conform to constraints imposed by human facial anatomy. To achieve this, ReactDiff incorporates two vital priors (spatio-temporal facial kinematics) into the diffusion process: i) temporal facial behavioral kinematics and ii) facial action unit dependencies. These two constraints guide the model toward realistic human reaction manifolds, avoiding visually unrealistic jitters, unstable transitions, unnatural expressions, and other artifacts. Extensive experiments on the REACT2024 dataset demonstrate that our approach not only achieves state-of-the-art reaction quality but also excels in diversity and reaction appropriateness.

</details>


### [43] [DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing](https://arxiv.org/abs/2510.04797)
*Qi Li,Shuwen Qiu,Julien Han,Xingzi Xu,Mehmet Saygin Seyfioglu,Kee Kiat Koo,Karim Bouyarmane*

Main category: cs.CV

TL;DR: DiT-VTON是一个基于扩散变换器的新型虚拟试穿框架，通过多种配置探索最佳图像条件设置，在扩展数据集上训练以增强鲁棒性，支持多种产品类别和图像编辑功能，在VITON-HD上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿模型在细节保留、鲁棒性、采样效率、图像编辑能力和跨类别泛化方面存在挑战，需要更强大的解决方案。

Method: 利用扩散变换器适应图像条件虚拟试穿任务，探索上下文标记连接、通道连接和ControlNet集成等多种配置，在包含多样化背景和非服装类别的扩展数据集上训练。

Result: 在VITON-HD上超越最先进方法，实现更好的细节保留和鲁棒性，无需额外条件编码器；在涵盖数千产品类别的多样化数据集上优于具有VTA和图像编辑能力的模型。

Conclusion: DiT-VTON重新定义了虚拟试穿任务，提供通用的虚拟试穿解决方案，能够处理广泛产品类别并支持高级图像编辑功能，展示了数据扩展对VTO适应性的益处。

Abstract: The rapid growth of e-commerce has intensified the demand for Virtual Try-On (VTO) technologies, enabling customers to realistically visualize products overlaid on their own images. Despite recent advances, existing VTO models face challenges with fine-grained detail preservation, robustness to real-world imagery, efficient sampling, image editing capabilities, and generalization across diverse product categories. In this paper, we present DiT-VTON, a novel VTO framework that leverages a Diffusion Transformer (DiT), renowned for its performance on text-conditioned image generation, adapted here for the image-conditioned VTO task. We systematically explore multiple DiT configurations, including in-context token concatenation, channel concatenation, and ControlNet integration, to determine the best setup for VTO image conditioning.   To enhance robustness, we train the model on an expanded dataset encompassing varied backgrounds, unstructured references, and non-garment categories, demonstrating the benefits of data scaling for VTO adaptability. DiT-VTON also redefines the VTO task beyond garment try-on, offering a versatile Virtual Try-All (VTA) solution capable of handling a wide range of product categories and supporting advanced image editing functionalities such as pose preservation, localized editing, texture transfer, and object-level customization. Experimental results show that our model surpasses state-of-the-art methods on VITON-HD, achieving superior detail preservation and robustness without reliance on additional condition encoders. It also outperforms models with VTA and image editing capabilities on a diverse dataset spanning thousands of product categories.

</details>


### [44] [Flow Matching for Conditional MRI-CT and CBCT-CT Image Synthesis](https://arxiv.org/abs/2510.04823)
*Arnela Hadzic,Simon Johannes Joham,Martin Urschler*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D流匹配(Flow Matching)的合成CT生成方法，可以从MRI或CBCT生成sCT，用于MRI-only和CBCT自适应放疗，在SynthRAD2025挑战赛基准上进行了评估。


<details>
  <summary>Details</summary>
Motivation: 生成合成CT(sCT)对于实现MRI-only和CBCT自适应放疗至关重要，可以提高治疗精度同时减少患者辐射暴露。流匹配方法在生成高质量图像方面显示出高效性。

Method: 采用完全3D流匹配框架，将高斯噪声体积通过学习到的流匹配速度场转换为sCT图像，使用轻量级3D编码器从输入MRI或CBCT中提取特征作为条件。

Result: 该方法在SynthRAD2025挑战赛基准上进行了验证，能够准确重建全局解剖结构，但由于内存和运行时间限制导致的训练分辨率较低，保留精细细节的能力有限。

Conclusion: 该方法在全局结构重建方面表现良好，但需要改进以保留更多细节。未来工作将探索基于补丁的训练和潜在空间流模型来提高分辨率和局部结构保真度。

Abstract: Generating synthetic CT (sCT) from MRI or CBCT plays a crucial role in enabling MRI-only and CBCT-based adaptive radiotherapy, improving treatment precision while reducing patient radiation exposure. To address this task, we adopt a fully 3D Flow Matching (FM) framework, motivated by recent work demonstrating FM's efficiency in producing high-quality images. In our approach, a Gaussian noise volume is transformed into an sCT image by integrating a learned FM velocity field, conditioned on features extracted from the input MRI or CBCT using a lightweight 3D encoder. We evaluated the method on the SynthRAD2025 Challenge benchmark, training separate models for MRI $\rightarrow$ sCT and CBCT $\rightarrow$ sCT across three anatomical regions: abdomen, head and neck, and thorax. Validation and testing were performed through the challenge submission system. The results indicate that the method accurately reconstructs global anatomical structures; however, preservation of fine details was limited, primarily due to the relatively low training resolution imposed by memory and runtime constraints. Future work will explore patch-based training and latent-space flow models to improve resolution and local structural fidelity.

</details>


### [45] [Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion](https://arxiv.org/abs/2510.04947)
*Xin Li,Kaixiang Yang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: 提出CA3D-Diff框架，基于条件扩散模型解决乳腺X光片双视图转换问题，通过列感知交叉注意力和隐式3D重建模块提升跨视图生成质量。


<details>
  <summary>Details</summary>
Motivation: 临床实践中乳腺X光片的一个视图可能缺失或损坏，影响诊断效果。由于X射线投影中的大变形和组织重叠，视图间转换具有挑战性。

Method: 使用条件扩散模型，设计列感知交叉注意力机制利用解剖对应区域的位置相似性，并引入隐式3D结构重建模块通过投影几何生成3D特征指导2D生成。

Result: 在双向视图转换任务中表现优异，超越现有方法，生成视图能有效提升单视图恶性分类性能。

Conclusion: CA3D-Diff框架在乳腺X光片视图转换中实现了高质量的生成效果，具有实际诊断价值。

Abstract: Dual-view mammography, including craniocaudal (CC) and mediolateral oblique (MLO) projections, offers complementary anatomical views crucial for breast cancer diagnosis. However, in real-world clinical workflows, one view may be missing, corrupted, or degraded due to acquisition errors or compression artifacts, limiting the effectiveness of downstream analysis. View-to-view translation can help recover missing views and improve lesion alignment. Unlike natural images, this task in mammography is highly challenging due to large non-rigid deformations and severe tissue overlap in X-ray projections, which obscure pixel-level correspondences. In this paper, we propose Column-Aware and Implicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view translation framework based on conditional diffusion model. To address cross-view structural misalignment, we first design a column-aware cross-attention mechanism that leverages the geometric property that anatomically corresponding regions tend to lie in similar column positions across views. A Gaussian-decayed bias is applied to emphasize local column-wise correlations while suppressing distant mismatches. Furthermore, we introduce an implicit 3D structure reconstruction module that back-projects noisy 2D latents into a coarse 3D feature volume based on breast-view projection geometry. The reconstructed 3D structure is refined and injected into the denoising UNet to guide cross-view generation with enhanced anatomical awareness. Extensive experiments demonstrate that CA3D-Diff achieves superior performance in bidirectional tasks, outperforming state-of-the-art methods in visual fidelity and structural consistency. Furthermore, the synthesized views effectively improve single-view malignancy classification in screening settings, demonstrating the practical value of our method in real-world diagnostics.

</details>


### [46] [SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization](https://arxiv.org/abs/2510.04961)
*Théophane Vallaeys,Jakob Verbeek,Matthieu Cord*

Main category: cs.CV

TL;DR: SSDD是一种新型像素扩散解码器架构，通过蒸馏技术实现单步重建，在无对抗损失的情况下达到比KL-VAE更高的重建质量和更快采样速度。


<details>
  <summary>Details</summary>
Motivation: 解决当前KL-VAE tokenizer需要对抗损失和扩散解码器采样时间较长的问题，开发一种更高效、稳定的解码器架构。

Method: 提出新的像素扩散解码器架构，利用transformer组件和无GAN训练，通过蒸馏技术将扩散解码器性能复制到高效的单步解码器中。

Result: SSDD将重建FID从0.87提升到0.50，吞吐量提高1.4倍，DiT生成质量保持不变的同时采样速度加快3.8倍。

Conclusion: SSDD可作为KL-VAE的直接替代品，用于构建更高质量和更快的生成模型。

Abstract: Tokenizers are a key component of state-of-the-art generative image models, extracting the most important features from the signal while reducing data dimension and redundancy. Most current tokenizers are based on KL-regularized variational autoencoders (KL-VAE), trained with reconstruction, perceptual and adversarial losses. Diffusion decoders have been proposed as a more principled alternative to model the distribution over images conditioned on the latent. However, matching the performance of KL-VAE still requires adversarial losses, as well as a higher decoding time due to iterative sampling. To address these limitations, we introduce a new pixel diffusion decoder architecture for improved scaling and training stability, benefiting from transformer components and GAN-free training. We use distillation to replicate the performance of the diffusion decoder in an efficient single-step decoder. This makes SSDD the first diffusion decoder optimized for single-step reconstruction trained without adversarial losses, reaching higher reconstruction quality and faster sampling than KL-VAE. In particular, SSDD improves reconstruction FID from $0.87$ to $0.50$ with $1.4\times$ higher throughput and preserve generation quality of DiTs with $3.8\times$ faster sampling. As such, SSDD can be used as a drop-in replacement for KL-VAE, and for building higher-quality and faster generative models.

</details>


### [47] [No-reference Quality Assessment of Contrast-distorted Images using Contrast-enhanced Pseudo Reference](https://arxiv.org/abs/2510.05053)
*Mohammad-Ali Mahmoudpour,Saeed Mahmoudpour*

Main category: cs.CV

TL;DR: 提出了一种针对对比度失真图像的无参考图像质量评估方法，通过生成伪参考图像将NR-IQA问题转换为FR-IQA问题


<details>
  <summary>Details</summary>
Motivation: 对比度变化是影响图像质量的重要因素，但现有的图像质量评估方法主要关注模糊和噪声等失真，对比度失真被很大程度上忽视了

Method: 使用一组对比度增强算法生成视觉上接近实际参考图像的伪参考图像，训练分类网络选择最合适的对比度增强算法，然后在FR模式下评估对比度增强图像与退化图像之间的质量差异

Result: 在三个包含对比度失真的数据库（CCID2014、TID2013和CSIQ）上的性能评估表明该方法具有良好性能

Conclusion: 该方法通过生成伪参考图像成功地将无参考图像质量评估问题转换为全参考评估问题，提高了对比度失真图像质量评估的准确性

Abstract: Contrast change is an important factor that affects the quality of images. During image capturing, unfavorable lighting conditions can cause contrast change and visual quality loss. While various methods have been proposed to assess the quality of images under different distortions such as blur and noise, contrast distortion has been largely overlooked as its visual impact and properties are different from other conventional types of distortions. In this paper, we propose a no-reference image quality assessment (NR-IQA) metric for contrast-distorted images. Using a set of contrast enhancement algorithms, we aim to generate pseudo-reference images that are visually close to the actual reference image, such that the NR problem is transformed to a Full-reference (FR) assessment with higher accuracy. To this end, a large dataset of contrast-enhanced images is produced to train a classification network that can select the most suitable contrast enhancement algorithm based on image content and distortion for pseudo-reference image generation. Finally, the evaluation is performed in the FR manner to assess the quality difference between the contrast-enhanced (pseudoreference) and degraded images. Performance evaluation of the proposed method on three databases containing contrast distortions (CCID2014, TID2013, and CSIQ), indicates the promising performance of the proposed method.

</details>


### [48] [Factuality Matters: When Image Generation and Editing Meet Structured Visuals](https://arxiv.org/abs/2510.05091)
*Le Zhuo,Songhao Han,Yuandong Pu,Boxiang Qiu,Sayak Paul,Yue Liao,Yihao Liu,Jie Shao,Xi Chen,Si Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: 该论文提出了一个针对结构化视觉内容（如图表、图表、数学图形）生成和编辑的综合解决方案，包括大规模数据集构建、统一模型训练和评估基准。


<details>
  <summary>Details</summary>
Motivation: 现代视觉生成模型在生成美观的自然图像方面表现出色，但在处理需要组合规划、文本渲染和多模态推理的结构化视觉内容时表现不佳。

Method: 构建130万高质量结构化图像对数据集，训练集成VLM与FLUX.1 Kontext的统一模型，采用三阶段训练课程，并在推理时使用外部推理器增强。

Result: 评估15个模型显示，即使是领先的闭源系统也远未达到满意水平。作者模型在编辑任务上表现强劲，推理时推理在不同架构中带来一致增益。

Conclusion: 通过发布数据集、模型和基准，旨在推进结构化视觉的统一多模态基础。

Abstract: While modern visual generation models excel at creating aesthetically pleasing natural images, they struggle with producing or editing structured visuals like charts, diagrams, and mathematical figures, which demand composition planning, text rendering, and multimodal reasoning for factual fidelity. To address this, we present the first comprehensive, systematic investigation of this domain, encompassing data construction, model training, and an evaluation benchmark. First, we construct a large-scale dataset of 1.3 million high-quality structured image pairs derived from executable drawing programs and augmented with chain-of-thought reasoning annotations. Building on it, we train a unified model that integrates a VLM with FLUX.1 Kontext via a lightweight connector for enhanced multimodal understanding. A three-stage training curriculum enables progressive feature alignment, knowledge infusion, and reasoning-augmented generation, further boosted by an external reasoner at inference time. Finally, we introduce StructBench, a novel benchmark for generation and editing with over 1,700 challenging instances, and an accompanying evaluation metric, StructScore, which employs a multi-round Q\&A protocol to assess fine-grained factual accuracy. Evaluations of 15 models reveal that even leading closed-source systems remain far from satisfactory. Our model attains strong editing performance, and inference-time reasoning yields consistent gains across diverse architectures. By releasing the dataset, model, and benchmark, we aim to advance unified multimodal foundations for structured visuals.

</details>


### [49] [Character Mixing for Video Generation](https://arxiv.org/abs/2510.05093)
*Tingting Liao,Chongjian Ge,Guangyi Liu,Hao Li,Yi Zhou*

Main category: cs.CV

TL;DR: 提出了一个跨角色交互的视频生成框架，通过交叉角色嵌入和交叉角色增强技术，解决不同世界角色自然交互时的身份保持和风格失真问题。


<details>
  <summary>Details</summary>
Motivation: 研究如何让来自不同世界（如卡通和真人剧）的角色在视频中自然交互，同时保持各自的身份特征和行为逻辑，避免风格混淆。

Method: 使用交叉角色嵌入（CCE）学习角色的身份和行为逻辑，以及交叉角色增强（CCA）通过合成共存和混合风格数据来丰富训练。

Result: 在包含10个角色的卡通和真人剧基准测试中，在身份保持、交互质量和风格失真鲁棒性方面都显示出明显改进。

Conclusion: 该框架能够实现之前从未共存角色之间的自然交互，同时保持风格保真度，为生成式故事讲述开辟了新形式。

Abstract: Imagine Mr. Bean stepping into Tom and Jerry--can we generate videos where characters interact naturally across different worlds? We study inter-character interaction in text-to-video generation, where the key challenge is to preserve each character's identity and behaviors while enabling coherent cross-context interaction. This is difficult because characters may never have coexisted and because mixing styles often causes style delusion, where realistic characters appear cartoonish or vice versa. We introduce a framework that tackles these issues with Cross-Character Embedding (CCE), which learns identity and behavioral logic across multimodal sources, and Cross-Character Augmentation (CCA), which enriches training with synthetic co-existence and mixed-style data. Together, these techniques allow natural interactions between previously uncoexistent characters without losing stylistic fidelity. Experiments on a curated benchmark of cartoons and live-action series with 10 characters show clear improvements in identity preservation, interaction quality, and robustness to style delusion, enabling new forms of generative storytelling.Additional results and videos are available on our project page: https://tingtingliao.github.io/mimix/.

</details>


### [50] [VChain: Chain-of-Visual-Thought for Reasoning in Video Generation](https://arxiv.org/abs/2510.05094)
*Ziqi Huang,Ning Yu,Gordon Chen,Haonan Qiu,Paul Debevec,Ziwei Liu*

Main category: cs.CV

TL;DR: VChain是一个推理时视觉思维链框架，通过多模态模型生成关键帧来指导视频生成，提升复杂动态场景的视频质量。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在合成复杂动态和连贯因果链方面存在困难，而多模态模型在视觉状态推理和未来预测方面表现出色，需要结合两者优势。

Method: 利用大型多模态模型生成稀疏关键帧作为快照，然后在这些关键时刻对预训练视频生成器进行稀疏推理时调优。

Result: 在复杂多步骤场景的广泛实验中，VChain显著提升了生成视频的质量。

Conclusion: VChain通过注入多模态模型的视觉推理信号，实现了调优高效、开销最小的视频生成质量提升。

Abstract: Recent video generation models can produce smooth and visually appealing clips, but they often struggle to synthesize complex dynamics with a coherent chain of consequences. Accurately modeling visual outcomes and state transitions over time remains a core challenge. In contrast, large language and multimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and future prediction capabilities. To bridge these strengths, we introduce VChain, a novel inference-time chain-of-visual-thought framework that injects visual reasoning signals from multimodal models into video generation. Specifically, VChain contains a dedicated pipeline that leverages large multimodal models to generate a sparse set of critical keyframes as snapshots, which are then used to guide the sparse inference-time tuning of a pre-trained video generator only at these key moments. Our approach is tuning-efficient, introduces minimal overhead and avoids dense supervision. Extensive experiments on complex, multi-step scenarios show that VChain significantly enhances the quality of generated videos.

</details>


### [51] [Paper2Video: Automatic Video Generation from Scientific Papers](https://arxiv.org/abs/2510.05096)
*Zeyu Zhu,Kevin Qinghong Lin,Mike Zheng Shou*

Main category: cs.CV

TL;DR: PaperTalker是首个学术演示视频生成基准和框架，包含101篇研究论文及其演示视频，通过多智能体系统自动生成包含幻灯片、字幕、语音和说话人画面的学术演示视频。


<details>
  <summary>Details</summary>
Motivation: 学术演示视频制作耗时耗力，传统方法需要数小时制作2-10分钟视频，且面临研究论文输入、多模态信息协调等独特挑战。

Method: 提出多智能体框架，集成幻灯片生成与布局优化、光标定位、字幕生成、语音合成和说话人渲染，采用并行化幻灯片生成提高效率。

Result: 在Paper2Video数据集上的实验表明，该方法生成的演示视频比现有基线更加忠实和内容丰富。

Conclusion: 为自动化、即用型学术视频生成迈出了实用的一步，提供了数据集、智能体和代码资源。

Abstract: Academic presentation videos have become an essential medium for research communication, yet producing them remains highly labor-intensive, often requiring hours of slide design, recording, and editing for a short 2 to 10 minutes video. Unlike natural video, presentation video generation involves distinctive challenges: inputs from research papers, dense multi-modal information (text, figures, tables), and the need to coordinate multiple aligned channels such as slides, subtitles, speech, and human talker. To address these challenges, we introduce PaperTalker, the first benchmark of 101 research papers paired with author-created presentation videos, slides, and speaker metadata. We further design four tailored evaluation metrics--Meta Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos convey the paper's information to the audience. Building on this foundation, we propose PaperTalker, the first multi-agent framework for academic presentation video generation. It integrates slide generation with effective layout refinement by a novel effective tree search visual choice, cursor grounding, subtitling, speech synthesis, and talking-head rendering, while parallelizing slide-wise generation for efficiency. Experiments on Paper2Video demonstrate that the presentation videos produced by our approach are more faithful and informative than existing baselines, establishing a practical step toward automated and ready-to-use academic video generation. Our dataset, agent, and code are available at https://github.com/showlab/Paper2Video.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [52] [Rethinking Inter-LoRA Orthogonality in Adapter Merging: Insights from Orthogonal Monte Carlo Dropout](https://arxiv.org/abs/2510.03262)
*Andi Zhang,Xuan Ding,Haofan Wang,Steven McDonagh,Samuel Kaski*

Main category: cs.LG

TL;DR: 提出正交蒙特卡洛dropout方法，在合并LoRA模块时强制正交性以避免语义向量干扰，但实证发现正交性并不能实现语义解耦或组合性。


<details>
  <summary>Details</summary>
Motivation: LoRA微调方法在合并多个模块时，语义向量会相互干扰，需要解决这种干扰问题。

Method: 提出正交蒙特卡洛dropout机制，在运行时保证合并的LoRA模块保持正交性，且不增加额外时间复杂度。

Result: 方法在理论和运行时层面都保证了正交性，但实证分析显示这种正交性并不能实现语义解耦或组合性。

Conclusion: 仅靠LoRA间的正交性可能不足以实现真正的语义组合性，需要重新审视其在适配器合并中的作用。

Abstract: We propose Orthogonal Monte Carlo Dropout, a mechanism that enforces strict orthogonality when combining sparse semantic vectors without extra time complexity. LoRA, a popular fine-tuning method for large models, typically trains a module to represent a specific concept such as an object or a style. When multiple LoRAs are merged, for example to generate an object in a particular style, their semantic vectors may interfere with each other. Our method guarantees, at the theoretical and runtime levels, that merged LoRAs remain orthogonal and thus free from direct interference. However, empirical analysis reveals that such orthogonality does not lead to the semantic disentanglement or compositionality highlighted in prior work on compositional adaptation. This finding suggests that inter-LoRA orthogonality alone may be insufficient for achieving true semantic compositionality, prompting a re-examination of its role in adapter merging.

</details>


### [53] [SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size](https://arxiv.org/abs/2510.03275)
*Junhao Xia,Ming Zhao,Limin Xiao,Xiujun Zhang*

Main category: cs.LG

TL;DR: SDQ-LLM是一种用于1位LLM的Sigma-Delta量化框架，通过过采样比率(OSR)连续可调、Hadamard权重平滑和细粒度OSR分配策略，在极低位量化下保持语言推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在计算和内存方面的显著挑战，实现高效的极低位量化部署。

Method: 使用上采样结合Sigma-Delta量化器将权重二值化或三值化，采用Hadamard权重平滑减少量化精度损失，并提出基于权重方差的细粒度OSR分配策略MultiOSR。

Result: 在OPT和LLaMA模型系列上的广泛实验表明，SDQ-LLM在高度激进的低OSR设置下实现了更高效和高精度的性能。

Conclusion: SDQ-LLM框架能够动态适应内存或VRAM约束，在极低位量化下显著提升推理效率，同时保持模型精度。

Abstract: Large language models (LLMs) face significant computational and memory challenges, making extremely low-bit quantization crucial for their efficient deployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size, a novel framework that enables extremely low-bit quantization of LLMs while preserving their linguistic reasoning capabilities. A distinctive feature of SDQ-LLM is the continuous adjustability of the Over-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM constraints by selecting fractional OSR (e.g. 2.5 times) for an optimal trade-off between model size and accuracy. SDQ-LLM uses upsampling combined with Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding high-precision parameters into 1-bit or 1.58-bit representations, replacing the multiplication operations within linear layers with addition. This approach significantly enhances inference efficiency under extremely low-bit quantization. To further reduce the loss of quantization precision, we incorporate Hadamard-based weight smoothing prior to quantization, improving the stability and robustness of the weight representations. Furthermore, to fully leverage the continuity of the OSR and reduce precision loss, recognizing the correlation between quantization sensitivity and weight variance, we propose a fine-grained, layer- and linear-wise OSR allocation strategy, MultiOSR. This strategy distributes OSR both across layers and within each layer, based on weight variance and parameter scale. Finally, extensive experiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a more efficient and high-precision performance even under highly aggressive low-OSR settings. Our code is available at https://github.com/Dreamlittlecat/LLM-Quant-Factory.

</details>


### [54] [Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models](https://arxiv.org/abs/2510.03302)
*Daiheng Gao,Nanxiang Jiang,Andi Zhang,Shilin Lu,Yufei Tang,Wenbo Zhou,Weiming Zhang,Zhaoxin Fan*

Main category: cs.LG

TL;DR: 论文揭示了当前概念擦除方法在扩散模型中只是制造了"遗忘"的假象，实际上通过偏置采样轨迹来避开目标概念，这种擦除是可逆的。作者提出了RevAm框架，利用RL轨迹优化技术复活被擦除的概念，暴露了当前安全机制的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着模型架构演进到Flux等新一代架构，现有的概念擦除方法（如ESD、UCE、AC）效果下降，需要探究其真正机制。研究发现这些方法并非真正删除概念，而是通过偏置采样轨迹制造遗忘假象，这种擦除是可逆的。

Method: 提出RevAm框架，基于RL的轨迹优化方法，通过动态引导去噪过程来复活被擦除的概念，无需修改模型权重。采用Group Relative Policy Optimization (GRPO)适应扩散模型，通过轨迹级奖励探索多样恢复轨迹。

Result: RevAm在概念复活保真度上表现优越，同时将计算时间减少10倍，成功暴露了当前安全机制的关键漏洞。

Conclusion: 当前基于轨迹操纵的安全机制存在根本性脆弱性，需要开发更鲁棒的概念擦除技术，而不仅仅是轨迹层面的操控。

Abstract: Concept erasure techniques have been widely deployed in T2I diffusion models to prevent inappropriate content generation for safety and copyright considerations. However, as models evolve to next-generation architectures like Flux, established erasure methods (\textit{e.g.}, ESD, UCE, AC) exhibit degraded effectiveness, raising questions about their true mechanisms. Through systematic analysis, we reveal that concept erasure creates only an illusion of ``amnesia": rather than genuine forgetting, these methods bias sampling trajectories away from target concepts, making the erasure fundamentally reversible. This insight motivates the need to distinguish superficial safety from genuine concept removal. In this work, we propose \textbf{RevAm} (\underline{Rev}oking \underline{Am}nesia), an RL-based trajectory optimization framework that resurrects erased concepts by dynamically steering the denoising process without modifying model weights. By adapting Group Relative Policy Optimization (GRPO) to diffusion models, RevAm explores diverse recovery trajectories through trajectory-level rewards, overcoming local optima that limit existing methods. Extensive experiments demonstrate that RevAm achieves superior concept resurrection fidelity while reducing computational time by 10$\times$, exposing critical vulnerabilities in current safety mechanisms and underscoring the need for more robust erasure techniques beyond trajectory manipulation.

</details>


### [55] [DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks](https://arxiv.org/abs/2510.04331)
*Nghiem T. Diep,Hien Dang,Tuan Truong,Tan Dinh,Huy Nguyen,Nhat Ho*

Main category: cs.LG

TL;DR: 提出了DoRAN方法，通过噪声注入和动态参数生成来改进DoRA，提升训练稳定性和样本效率


<details>
  <summary>Details</summary>
Motivation: DoRA方法虽然改进了LoRA，但在训练稳定性和样本效率方面仍有提升空间，需要更稳定的训练过程和更高效的参数利用

Method: 1. 在DoRA权重分解的分母中注入噪声作为自适应正则化器；2. 用辅助网络替换静态低秩矩阵，实现跨层参数耦合的动态生成

Result: 在视觉和语言基准测试中，DoRAN持续优于LoRA、DoRA和其他PEFT基线方法

Conclusion: 结合噪声正则化和网络参数生成的方法为基础模型的稳健高效微调提供了有前景的方向

Abstract: Parameter-efficient fine-tuning (PEFT) methods have become the standard paradigm for adapting large-scale models. Among these techniques, Weight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the learning capacity and training stability of the vanilla Low-Rank Adaptation (LoRA) method by explicitly decomposing pre-trained weights into magnitude and directional components. In this work, we propose DoRAN, a new variant of DoRA designed to further stabilize training and boost the sample efficiency of DoRA. Our approach includes two key stages: (i) injecting noise into the denominator of DoRA's weight decomposition, which serves as an adaptive regularizer to mitigate instabilities; and (ii) replacing static low-rank matrices with auxiliary networks that generate them dynamically, enabling parameter coupling across layers and yielding better sample efficiency in both theory and practice. Comprehensive experiments on vision and language benchmarks show that DoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. These results underscore the effectiveness of combining stabilization through noise-based regularization with network-based parameter generation, offering a promising direction for robust and efficient fine-tuning of foundation models.

</details>


### [56] [Partial Information Decomposition via Normalizing Flows in Latent Gaussian Distributions](https://arxiv.org/abs/2510.04417)
*Wenyuan Zhao,Adithya Balachandran,Chao Tian,Paul Pu Liang*

Main category: cs.LG

TL;DR: 提出了高斯部分信息分解(GPID)方法，通过梯度优化算法和编码器转换，高效解决多模态数据中的信息分解问题。


<details>
  <summary>Details</summary>
Motivation: 现有部分信息分解方法在处理连续高维模态时计算成本高且不准确，需要更高效的解决方案。

Method: 基于高斯分布假设，开发梯度优化算法，并使用信息保持编码器将任意分布转换为高斯分布。

Result: 在合成和真实多模态数据集上验证，比现有基线方法更准确高效。

Conclusion: GPID方法为多模态数据分析提供了实用的信息分解工具，有助于模型选择和性能评估。

Abstract: The study of multimodality has garnered significant interest in fields where the analysis of interactions among multiple information sources can enhance predictive modeling, data fusion, and interpretability. Partial information decomposition (PID) has emerged as a useful information-theoretic framework to quantify the degree to which individual modalities independently, redundantly, or synergistically convey information about a target variable. However, existing PID methods depend on optimizing over a joint distribution constrained by estimated pairwise probability distributions, which are costly and inaccurate for continuous and high-dimensional modalities. Our first key insight is that the problem can be solved efficiently when the pairwise distributions are multivariate Gaussians, and we refer to this problem as Gaussian PID (GPID). We propose a new gradient-based algorithm that substantially improves the computational efficiency of GPID based on an alternative formulation of the underlying optimization problem. To generalize the applicability to non-Gaussian data, we learn information-preserving encoders to transform random variables of arbitrary input distributions into pairwise Gaussian random variables. Along the way, we resolved an open problem regarding the optimality of joint Gaussian solutions for GPID. Empirical validation in diverse synthetic examples demonstrates that our proposed method provides more accurate and efficient PID estimates than existing baselines. We further evaluate a series of large-scale multimodal benchmarks to show its utility in real-world applications of quantifying PID in multimodal datasets and selecting high-performing models.

</details>


### [57] [Real-time Prediction of Urban Sound Propagation with Conditioned Normalizing Flows](https://arxiv.org/abs/2510.04510)
*Achim Eckerle,Martin Spitznagel,Janis Keuper*

Main category: cs.LG

TL;DR: 使用条件归一化流(Full-Glow)实时生成符合标准的城市声压地图，相比传统物理求解器加速2000倍以上，在非视距场景下精度提升24%，支持城市规划和合规映射的交互式探索。


<details>
  <summary>Details</summary>
Motivation: 城市噪声预测对公共健康和监管工作流程至关重要，但基于物理的求解器速度太慢，无法满足时间关键的迭代性"假设分析"研究需求。

Method: 采用条件归一化流模型，从2D城市布局实时生成256x256声压地图，在单个RTX 4090上实现交互式探索。

Result: 在覆盖基线、衍射和反射机制的数据集上，模型比参考求解器加速>2000倍，非视距精度比先前深度模型提升24%，基线非视距场景下达到0.65 dB MAE。

Conclusion: 该模型能够重现衍射和干涉模式，支持在声源或几何变化下的即时重新计算，成为城市规划、合规映射和运营的实用引擎。

Abstract: Accurate and fast urban noise prediction is pivotal for public health and for regulatory workflows in cities, where the Environmental Noise Directive mandates regular strategic noise maps and action plans, often needed in permission workflows, right-of-way allocation, and construction scheduling. Physics-based solvers are too slow for such time-critical, iterative "what-if" studies. We evaluate conditional Normalizing Flows (Full-Glow) for generating for generating standards-compliant urban sound-pressure maps from 2D urban layouts in real time per 256x256 map on a single RTX 4090), enabling interactive exploration directly on commodity hardware. On datasets covering Baseline, Diffraction, and Reflection regimes, our model accelerates map generation by >2000 times over a reference solver while improving NLoS accuracy by up to 24% versus prior deep models; in Baseline NLoS we reach 0.65 dB MAE with high structural fidelity. The model reproduces diffraction and interference patterns and supports instant recomputation under source or geometry changes, making it a practical engine for urban planning, compliance mapping, and operations (e.g., temporary road closures, night-work variance assessments).

</details>


### [58] [SONA: Learning Conditional, Unconditional, and Mismatching-Aware Discriminator](https://arxiv.org/abs/2510.04576)
*Yuhta Takida,Satoshi Hayakawa,Takashi Shibuya,Masaaki Imaizumi,Naoki Murata,Bac Nguyen,Toshimitsu Uesaka,Chieh-Hsin Lai,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 提出SONA方法，通过分离自然性和对齐性评估，结合自适应加权机制，在条件生成任务中实现更好的样本质量和条件对齐。


<details>
  <summary>Details</summary>
Motivation: 现有条件生成对抗网络在条件鉴别器中难以平衡真实性和条件对齐的双重目标，需要更有效的鉴别器设计。

Method: 提出SONA方法，在最终层使用分离投影分别评估自然性和对齐性，引入匹配感知监督增强对齐敏感性，采用自适应加权机制动态平衡各目标。

Result: 在类条件生成任务中，SONA在样本质量和条件对齐方面优于现有最优方法，在文本到图像生成中也表现出有效性。

Conclusion: SONA方法具有多功能性和鲁棒性，通过改进的鉴别器设计有效解决了条件生成中的对齐挑战。

Abstract: Deep generative models have made significant advances in generating complex content, yet conditional generation remains a fundamental challenge. Existing conditional generative adversarial networks often struggle to balance the dual objectives of assessing authenticity and conditional alignment of input samples within their conditional discriminators. To address this, we propose a novel discriminator design that integrates three key capabilities: unconditional discrimination, matching-aware supervision to enhance alignment sensitivity, and adaptive weighting to dynamically balance all objectives. Specifically, we introduce Sum of Naturalness and Alignment (SONA), which employs separate projections for naturalness (authenticity) and alignment in the final layer with an inductive bias, supported by dedicated objective functions and an adaptive weighting mechanism. Extensive experiments on class-conditional generation tasks show that \ours achieves superior sample quality and conditional alignment compared to state-of-the-art methods. Furthermore, we demonstrate its effectiveness in text-to-image generation, confirming the versatility and robustness of our approach.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [59] [Super-resolution image projection over an extended depth of field using a diffractive decoder](https://arxiv.org/abs/2510.03938)
*Hanlong Chen,Cagatay Isil,Tianyi Gan,Mona Jarrahi,Aydogan Ozcan*

Main category: physics.optics

TL;DR: 提出了一种混合图像投影系统，结合CNN数字编码器和全光学衍射解码器，实现扩展景深和超分辨率图像投影，显著提升空间带宽积。


<details>
  <summary>Details</summary>
Motivation: 图像投影系统需要在保持大空间带宽积的同时，实现数据存储、计算和传输的高效性。传统系统在景深和分辨率方面存在限制。

Method: 使用CNN编码器将输入图像压缩为紧凑相位表示，通过低分辨率投影仪显示，再由被动式衍射解码器进行全光学图像重建，实现像素超分辨率和扩展景深。

Result: 系统在THz频谱实验中验证，实现~267倍波长的扩展景深，每个横向平面提供~16倍空间带宽积提升，且光学解码器无需额外功耗。

Conclusion: 该架构可显著降低显示系统的数据存储和传输需求，其原理可扩展到光学计量和显微镜等应用领域。

Abstract: Image projection systems must be efficient in data storage, computation and transmission while maintaining a large space-bandwidth-product (SBP) at their output. Here, we introduce a hybrid image projection system that achieves extended depth-of-field (DOF) with improved resolution, combining a convolutional neural network (CNN)-based digital encoder with an all-optical diffractive decoder. A CNN-based encoder compresses input images into compact phase representations, which are subsequently displayed by a low-resolution (LR) projector and processed by an analog diffractive decoder for all-optical image reconstruction. This optical decoder is completely passive, designed to synthesize pixel super-resolved image projections that feature an extended DOF while eliminating the need for additional power consumption for super-resolved image reconstruction. Our pixel super-resolution (PSR) image projection system demonstrates high-fidelity image synthesis over an extended DOF of ~267xW, where W is the illumination wavelength, concurrently offering up to ~16-fold SBP improvement at each lateral plane. The proof of concept of this approach is validated through an experiment conducted in the THz spectrum, and the system is scalable across different parts of the electromagnetic spectrum. This image projection architecture can reduce data storage and transmission requirements for display systems without imposing additional power constraints on the optical decoder. Beyond extended DOF PSR image projection, the underlying principles of this approach can be extended to various applications, including optical metrology and microscopy.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [60] [Towards Robust and Generalizable Continuous Space-Time Video Super-Resolution with Events](https://arxiv.org/abs/2510.03833)
*Shuoyan Wei,Feng Li,Shengeng Tang,Runmin Cong,Yao Zhao,Meng Wang,Huihui Bai*

Main category: eess.IV

TL;DR: EvEnhancer是一个结合事件流特性的连续时空视频超分辨率方法，通过事件自适应合成和局部隐式视频变换器实现任意尺度的鲁棒重建，EvEnhancerPlus进一步增加了可控切换机制来降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有连续时空视频超分辨率方法在分布外尺度上泛化能力差，无法产生令人满意的结果。

Method: 使用事件流的高时间分辨率和高动态范围特性，结合事件自适应合成捕获长期运动轨迹，以及局部隐式视频变换器学习连续视频表示。EvEnhancerPlus增加了基于局部事件统计的可控切换机制。

Result: 在合成和真实数据集上达到最先进性能，在分布外尺度上保持优越的泛化能力。

Conclusion: 该方法通过结合事件流特性和自适应机制，实现了高效且泛化能力强的连续时空视频超分辨率。

Abstract: Continuous space-time video super-resolution (C-STVSR) has garnered increasing interest for its capability to reconstruct high-resolution and high-frame-rate videos at arbitrary spatial and temporal scales. However, prevailing methods often generalize poorly, producing unsatisfactory results when applied to out-of-distribution (OOD) scales. To overcome this limitation, we present EvEnhancer, a novel approach that marries the unique properties of high temporal resolution and high dynamic range encapsulated in event streams to achieve robust and generalizable C-STVSR. Our approach incorporates event-adapted synthesis that capitalizes on the spatiotemporal correlations between frames and events to capture long-term motion trajectories, enabling adaptive interpolation and fusion across space and time. This is then coupled with a local implicit video transformer that integrates local implicit video neural function with cross-scale spatiotemporal attention to learn continuous video representations and generate plausible videos at arbitrary resolutions and frame rates. We further develop EvEnhancerPlus, which builds a controllable switching mechanism that dynamically determines the reconstruction difficulty for each spatiotemporal pixel based on local event statistics. This allows the model to adaptively route reconstruction along the most suitable pathways at a fine-grained pixel level, substantially reducing computational overhead while maintaining excellent performance. Furthermore, we devise a cross-derivative training strategy that stabilizes the convergence of such a multi-pathway framework through staged cross-optimization. Extensive experiments demonstrate that our method achieves state-of-the-art performance on both synthetic and real-world datasets, while maintaining superior generalizability at OOD scales. The code is available at https://github.com/W-Shuoyan/EvEnhancerPlus.

</details>


### [61] [The method of the approximate inverse for limited-angle CT](https://arxiv.org/abs/2510.04369)
*Bernadette Hahn,Gael Rigaud,Richard Schmähl*

Main category: eess.IV

TL;DR: 提出了一种基于近似逆方法的新型模型驱动方法，用于解决有限角度计算机断层扫描的重建问题，避免了传统方法产生的条纹伪影。


<details>
  <summary>Details</summary>
Motivation: 有限角度CT在工业和医学中具有重要应用价值，但传统方法如滤波反投影和总变分方法会产生各种伪影，而深度学习方法需要大量数据集。需要开发不依赖大数据集的有效重建方法。

Method: 使用近似逆方法，通过预计算重建核作为辅助问题的解，结合谱滤波、近似逆方法和自定义边缘保持去噪来稳定重建过程。

Result: 提出的LARK方法能够完全重建物体而不产生条纹伪影，即使在大有限角度下也有效。CLARK方法进一步稳定了重建过程，并在合成和真实数据上得到验证。

Conclusion: 该方法为未来的学习策略提供了新的起点，能够有效解决有限角度CT重建中的伪影问题，特别是在处理半离散测量数据时表现出色。

Abstract: Limited-angle computerized tomography stands for one of the most difficult challenges in imaging. Although it opens the way to faster data acquisition in industry and less dangerous scans in medicine, standard approaches, such as the filtered backprojection (FBP) algorithm or the widely used total-variation functional, often produce various artefacts that hinder the diagnosis. With the rise of deep learning, many modern techniques have proven themselves successful in removing such artefacts but at the cost of large datasets. In this paper, we propose a new model-driven approach based on the method of the approximate inverse, which could serve as new starting point for learning strategies in the future. In contrast to FBP-type approaches, our reconstruction step consists in evaluating linear functionals on the measured data using reconstruction kernels that are precomputed as solution of an auxiliary problem. With this problem being uniquely solvable, the derived limited-angle reconstruction kernel (LARK) is able to fully reconstruct the object without the well-known streak artefacts, even for large limited angles. However, it inherits severe ill-conditioning which leads to a different kind of artefacts arising from the singular functions of the limited-angle Radon transform. The problem becomes particularly challenging when working on semi-discrete (real or analytical) measurements. We develop a general regularization strategy, named constrained limited-angle reconstruction kernel (CLARK), by combining spectral filter, the method of the approximate inverse and custom edge-preserving denoising in order to stabilize the whole process. We further derive and interpret error estimates for the application on real, i.e. semi-discrete, data and we validate our approach on synthetic and real data.

</details>


### [62] [Adaptive double-phase Rudin--Osher--Fatemi denoising model](https://arxiv.org/abs/2510.04382)
*Wojciech Górny,Michał Łasica,Alexandros Matsoukas*

Main category: eess.IV

TL;DR: 提出了一种基于可变增长双相型总变差正则化的图像去噪模型，通过自适应权重减少阶梯效应，同时保持边缘保护效果。


<details>
  <summary>Details</summary>
Motivation: 解决经典Rudin-Osher-Fatemi模型中的阶梯效应问题，同时保持其对图像边缘的良好保护能力。

Method: 采用可变增长双相型总变差正则化方法，结合自适应权重机制来平衡去噪和边缘保护。

Result: 在1D和2D合成图像及自然图像上测试，模型在不同噪声水平下均表现良好。

Conclusion: 该模型能有效减少阶梯效应，同时保持与经典模型相当的边缘保护能力，适用于多种噪声场景。

Abstract: We propose a new image denoising model based on a variable-growth total variation regularization of double-phase type with adaptive weight. It is designed to reduce staircasing with respect to the classical Rudin--Osher--Fatemi model, while preserving the edges of the image in a similar fashion. We implement the model and test its performance on synthetic and natural images in 1D and 2D over a range of noise levels.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [63] [Model-Guided Microstimulation Steers Primate Visual Behavior](https://arxiv.org/abs/2510.03684)
*Johannes Mehrer,Ben Lonnqvist,Anna Mitola,Abdulkadir Gokce,Paolo Papale,Martin Schrimpf*

Main category: q-bio.NC

TL;DR: 该论文提出了一个计算框架来建模和指导高级视觉皮层的微刺激，通过结合扰动模块、地形模型和映射程序，成功在猕猴实验中验证了模型预测的刺激能显著改变感知选择。


<details>
  <summary>Details</summary>
Motivation: 现有视觉假体主要刺激早期视觉皮层，只能诱发简单符号感知，受硬件限制和皮层低层表征特性的制约。高级视觉区域编码更复杂的物体表征，是更有前景的刺激目标，但确定能可靠诱发物体级感知的表征目标是一个重大挑战。

Method: 开发了一个包含三个关键组件的计算框架：(1)扰动模块将微刺激参数转换为神经活动的空间变化；(2)地形模型捕捉皮层神经元的空间组织，支持刺激实验原型设计；(3)映射程序将模型优化的刺激位点映射回灵长类皮层。在猕猴视觉识别任务中应用该框架进行验证。

Result: 在两只猕猴的视觉识别任务中，模型预测的刺激实验产生了显著的体内感知选择变化。每个位点的模型预测与猴子行为高度相关。图像生成显示，对面部选择性位点的硅内刺激与患者报告的面部幻视现象在质量上相似。

Conclusion: 这项原理验证为模型引导的微刺激奠定了基础，并指向能够诱发更复杂视觉体验的下一代视觉假体。

Abstract: Brain stimulation is a powerful tool for understanding cortical function and holds promise for therapeutic interventions in neuropsychiatric disorders. Initial visual prosthetics apply electric microstimulation to early visual cortex which can evoke percepts of simple symbols such as letters. However, these approaches are fundamentally limited by hardware constraints and the low-level representational properties of this cortical region. In contrast, higher-level visual areas encode more complex object representations and therefore constitute a promising target for stimulation - but determining representational targets that reliably evoke object-level percepts constitutes a major challenge. We here introduce a computational framework to causally model and guide stimulation of high-level cortex, comprising three key components: (1) a perturbation module that translates microstimulation parameters into spatial changes to neural activity, (2) topographic models that capture the spatial organization of cortical neurons and thus enable prototyping of stimulation experiments, and (3) a mapping procedure that links model-optimized stimulation sites back to primate cortex. Applying this framework in two macaque monkeys performing a visual recognition task, model-predicted stimulation experiments produced significant in-vivo changes in perceptual choices. Per-site model predictions and monkey behavior were strongly correlated, underscoring the promise of model-guided stimulation. Image generation further revealed a qualitative similarity between in-silico stimulation of face-selective sites and a patient's report of facephenes. This proof-of-principle establishes a foundation for model-guided microstimulation and points toward next-generation visual prosthetics capable of inducing more complex visual experiences.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [64] [MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition](https://arxiv.org/abs/2510.04136)
*Umberto Cappellazzo,Minsu Kim,Pingchuan Ma,Honglie Chen,Xubo Liu,Stavros Petridis,Maja Pantic*

Main category: eess.AS

TL;DR: MoME框架将稀疏混合专家(MoE)集成到基于Matryoshka表示学习(MRL)的大语言模型中，用于音视频语音识别，实现动态容量分配和跨粒度一致性，在保持高性能的同时显著减少参数需求。


<details>
  <summary>Details</summary>
Motivation: 解决现有MRL方法在训练时独立处理不同尺度，导致跨尺度泛化能力有限、高压缩率下鲁棒性差和可解释性不足的问题。

Method: 在冻结的LLM基础上添加top-k路由和共享专家，通过共享路由器促进跨粒度的一致专家激活，使压缩序列能从低压缩率表示中受益。

Result: 在LRS2和LRS3数据集上，MoME在AVSR、ASR和VSR任务中达到最先进性能，同时需要显著更少的参数并在噪声下保持鲁棒性。

Conclusion: MoME统一了MRL的适应性和MoE的效率，为资源感知的语音识别提供了可扩展且可解释的解决方案。

Abstract: Large language models (LLMs) have recently shown strong potential in audio-visual speech recognition (AVSR), but their high computational demands and sensitivity to token granularity limit their practicality in resource-constrained settings. Token compression methods can reduce inference cost, but they require fixing a compression rate in advance and produce a single fixed-length output, offering no flexibility to balance information density and efficiency at inference time. Matryoshka representation learning (MRL) addresses this by enabling a single model to operate across multiple token granularities, allowing compression rates to be adjusted dynamically. However, current MRL-based methods treat each scale independently during training, limiting cross-scale generalization, robustness at high compression, and interpretability. To overcome these limitations, we propose MoME (Mixture of Matryoshka Experts), a novel framework that integrates sparse Mixture-of-Experts (MoE) into MRL-based LLMs for AVSR. MoME augments a frozen LLM with top-k routed and shared experts, allowing dynamic capacity allocation across scales and modalities. A shared router promotes consistent expert activation across granularities, enabling compressed sequences to benefit from representations learned at lower compression. Experiments on LRS2 and LRS3 demonstrate that MoME achieves state-of-the-art performance across AVSR, ASR, and VSR tasks, while requiring significantly fewer parameters and maintaining robustness under noise. MoME unifies the adaptability of MRL with the efficiency of MoE, offering a scalable and interpretable solution for resource-aware speech recognition.

</details>
