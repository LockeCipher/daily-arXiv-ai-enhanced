{"id": "2506.15742", "pdf": "https://arxiv.org/pdf/2506.15742", "abs": "https://arxiv.org/abs/2506.15742", "authors": ["Black Forest Labs", "Stephen Batifol", "Andreas Blattmann", "Frederic Boesel", "Saksham Consul", "Cyril Diagne", "Tim Dockhorn", "Jack English", "Zion English", "Patrick Esser", "Sumith Kulal", "Kyle Lacey", "Yam Levi", "Cheng Li", "Dominik Lorenz", "Jonas M\u00fcller", "Dustin Podell", "Robin Rombach", "Harry Saini", "Axel Sauer", "Luke Smith"], "title": "FLUX.1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space", "categories": ["cs.GR"], "comment": null, "summary": "We present evaluation results for FLUX.1 Kontext, a generative flow matching model that unifies image generation and editing. The model generates novel output views by incorporating semantic context from text and image inputs. Using a simple sequence concatenation approach, FLUX.1 Kontext handles both local editing and generative in-context tasks within a single unified architecture. Compared to current editing models that exhibit degradation in character consistency and stability across multiple turns, we observe that FLUX.1 Kontext improved preservation of objects and characters, leading to greater robustness in iterative workflows.The model achieves competitive performance with current state-of-the-art systems while delivering significantly faster generation times, enabling interactive applications and rapid prototyping workflows. To validate these improvements, we introduce KontextBench, a comprehensive benchmark with 1026 image-prompt pairs covering five task categories: local editing, global editing, character reference, style reference and text editing. Detailed evaluations show the superior performance of FLUX.1 Kontext in terms of both single-turn quality and multi-turn consistency, setting new standards for unified image processing models.", "AI": {"tldr": "FLUX.1 Kontext\u662f\u4e00\u79cd\u751f\u6210\u6d41\u5339\u914d\u6a21\u578b\uff0c\u7edf\u4e00\u4e86\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u548c\u56fe\u50cf\u7684\u8bed\u4e49\u4e0a\u4e0b\u6587\u751f\u6210\u65b0\u89c6\u56fe\u3002\u5b83\u5728\u5355\u6b21\u548c\u591a\u8f6e\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u901f\u5ea6\u5feb\u4e14\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u5f53\u524d\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u5728\u591a\u8f6e\u4efb\u52a1\u4e2d\u5b58\u5728\u89d2\u8272\u4e00\u81f4\u6027\u548c\u7a33\u5b9a\u6027\u4e0b\u964d\u7684\u95ee\u9898\uff0cFLUX.1 Kontext\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u751f\u6210\u901f\u5ea6\u548c\u4efb\u52a1\u7edf\u4e00\u6027\u3002", "method": "\u91c7\u7528\u7b80\u5355\u7684\u5e8f\u5217\u62fc\u63a5\u65b9\u6cd5\uff0c\u7edf\u4e00\u5904\u7406\u5c40\u90e8\u7f16\u8f91\u548c\u751f\u6210\u4efb\u52a1\uff0c\u7ed3\u5408\u6587\u672c\u548c\u56fe\u50cf\u8f93\u5165\u7684\u8bed\u4e49\u4e0a\u4e0b\u6587\u3002", "result": "\u6a21\u578b\u5728\u5355\u6b21\u548c\u591a\u8f6e\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5bf9\u8c61\u548c\u89d2\u8272\u4fdd\u7559\u80fd\u529b\uff0c\u751f\u6210\u901f\u5ea6\u5feb\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "FLUX.1 Kontext\u4e3a\u7edf\u4e00\u56fe\u50cf\u5904\u7406\u6a21\u578b\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u9002\u7528\u4e8e\u4ea4\u4e92\u5f0f\u5e94\u7528\u548c\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u3002"}}
{"id": "2506.15821", "pdf": "https://arxiv.org/pdf/2506.15821", "abs": "https://arxiv.org/abs/2506.15821", "authors": ["Pham Khai Nguyen Do", "Bao Nguyen Tran", "Nam Nguyen", "Duc Dung Nguyen"], "title": "VEIGAR: View-consistent Explicit Inpainting and Geometry Alignment for 3D object Removal", "categories": ["cs.GR", "cs.AI", "cs.CV", "eess.IV"], "comment": null, "summary": "Recent advances in Novel View Synthesis (NVS) and 3D generation have significantly improved editing tasks, with a primary emphasis on maintaining cross-view consistency throughout the generative process. Contemporary methods typically address this challenge using a dual-strategy framework: performing consistent 2D inpainting across all views guided by embedded priors either explicitly in pixel space or implicitly in latent space; and conducting 3D reconstruction with additional consistency guidance. Previous strategies, in particular, often require an initial 3D reconstruction phase to establish geometric structure, introducing considerable computational overhead. Even with the added cost, the resulting reconstruction quality often remains suboptimal. In this paper, we present VEIGAR, a computationally efficient framework that outperforms existing methods without relying on an initial reconstruction phase. VEIGAR leverages a lightweight foundation model to reliably align priors explicitly in the pixel space. In addition, we introduce a novel supervision strategy based on scale-invariant depth loss, which removes the need for traditional scale-and-shift operations in monocular depth regularization. Through extensive experimentation, VEIGAR establishes a new state-of-the-art benchmark in reconstruction quality and cross-view consistency, while achieving a threefold reduction in training time compared to the fastest existing method, highlighting its superior balance of efficiency and effectiveness.", "AI": {"tldr": "VEIGAR\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u65b0\u89c6\u89d2\u5408\u6210\u6846\u67b6\uff0c\u65e0\u9700\u521d\u59cb3D\u91cd\u5efa\u9636\u6bb5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u57fa\u7840\u6a21\u578b\u548c\u5c3a\u5ea6\u4e0d\u53d8\u6df1\u5ea6\u635f\u5931\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa\u548c\u8de8\u89c6\u89d2\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u521d\u59cb3D\u91cd\u5efa\u9636\u6bb5\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u91cd\u5efa\u8d28\u91cf\u4e0d\u4f73\uff0cVEIGAR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "VEIGAR\u4f7f\u7528\u8f7b\u91cf\u7ea7\u57fa\u7840\u6a21\u578b\u5728\u50cf\u7d20\u7a7a\u95f4\u663e\u5f0f\u5bf9\u9f50\u5148\u9a8c\uff0c\u5e76\u5f15\u5165\u5c3a\u5ea6\u4e0d\u53d8\u6df1\u5ea6\u635f\u5931\u76d1\u7763\u7b56\u7565\u3002", "result": "VEIGAR\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u8de8\u89c6\u89d2\u4e00\u81f4\u6027\u4e0a\u8fbe\u5230\u65b0SOTA\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e09\u500d\u3002", "conclusion": "VEIGAR\u5728\u6548\u7387\u548c\u6548\u679c\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3aNVS\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.16827", "pdf": "https://arxiv.org/pdf/2506.16827", "abs": "https://arxiv.org/abs/2506.16827", "authors": ["Grzegorz Gruszczynski", "Michal Jan Wlodarczyk", "Jakub J Meixner", "Przemyslaw Musialski"], "title": "Beyond Blur: A Fluid Perspective on Generative Diffusion Models", "categories": ["cs.GR", "cs.CV", "cs.LG", "I.2.6; I.4.10; I.4.8"], "comment": "11 pages, 8 figures, pre-print, supplementary pseudocode in appendix", "summary": "We propose a novel PDE-driven corruption process for generative image synthesis based on advection-diffusion processes which generalizes existing PDE-based approaches. Our forward pass formulates image corruption via a physically motivated PDE that couples directional advection with isotropic diffusion and Gaussian noise, controlled by dimensionless numbers (Peclet, Fourier). We implement this PDE numerically through a GPU-accelerated custom Lattice Boltzmann solver for fast evaluation. To induce realistic turbulence, we generate stochastic velocity fields that introduce coherent motion and capture multi-scale mixing. In the generative process, a neural network learns to reverse the advection-diffusion operator thus constituting a novel generative model. We discuss how previous methods emerge as specific cases of our operator, demonstrating that our framework generalizes prior PDE-based corruption techniques. We illustrate how advection improves the diversity and quality of the generated images while keeping the overall color palette unaffected. This work bridges fluid dynamics, dimensionless PDE theory, and deep generative modeling, offering a fresh perspective on physically informed image corruption processes for diffusion-based synthesis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePDE\u7684\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u7269\u7406\u9a71\u52a8\u7684\u8150\u8680\u8fc7\u7a0b\u7ed3\u5408\u5bf9\u6d41-\u6269\u6563\u548c\u566a\u58f0\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u7269\u7406\u542f\u53d1\u7684PDE\u65b9\u6cd5\uff0c\u7ed3\u5408\u6d41\u4f53\u52a8\u529b\u5b66\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u6539\u8fdb\u73b0\u6709\u57fa\u4e8ePDE\u7684\u56fe\u50cf\u751f\u6210\u6280\u672f\u3002", "method": "\u4f7f\u7528GPU\u52a0\u901f\u7684Lattice Boltzmann\u6c42\u89e3\u5668\u5b9e\u73b0PDE\uff0c\u7ed3\u5408\u968f\u673a\u901f\u5ea6\u573a\u6a21\u62df\u6e4d\u6d41\uff0c\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u53cd\u8f6c\u8150\u8680\u8fc7\u7a0b\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u591a\u6837\u4e14\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\uff0c\u540c\u65f6\u4fdd\u6301\u8272\u5f69\u4e00\u81f4\u6027\uff0c\u5e76\u63a8\u5e7f\u4e86\u73b0\u6709\u7684PDE\u8150\u8680\u6280\u672f\u3002", "conclusion": "\u672c\u6587\u4e3a\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u5408\u6210\u63d0\u4f9b\u4e86\u7269\u7406\u542f\u53d1\u7684\u521b\u65b0\u89c6\u89d2\uff0c\u7ed3\u5408\u4e86\u6d41\u4f53\u52a8\u529b\u5b66\u548c\u6df1\u5ea6\u5b66\u4e60\u3002"}}
{"id": "2506.17206", "pdf": "https://arxiv.org/pdf/2506.17206", "abs": "https://arxiv.org/abs/2506.17206", "authors": ["Yukun Huang", "Yanning Zhou", "Jianan Wang", "Kaiyi Huang", "Xihui Liu"], "title": "DreamCube: 3D Panorama Generation via Multi-plane Synchronization", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Project page: https://yukun-huang.github.io/DreamCube/", "summary": "3D panorama synthesis is a promising yet challenging task that demands high-quality and diverse visual appearance and geometry of the generated omnidirectional content. Existing methods leverage rich image priors from pre-trained 2D foundation models to circumvent the scarcity of 3D panoramic data, but the incompatibility between 3D panoramas and 2D single views limits their effectiveness. In this work, we demonstrate that by applying multi-plane synchronization to the operators from 2D foundation models, their capabilities can be seamlessly extended to the omnidirectional domain. Based on this design, we further introduce DreamCube, a multi-plane RGB-D diffusion model for 3D panorama generation, which maximizes the reuse of 2D foundation model priors to achieve diverse appearances and accurate geometry while maintaining multi-view consistency. Extensive experiments demonstrate the effectiveness of our approach in panoramic image generation, panoramic depth estimation, and 3D scene generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDreamCube\uff0c\u901a\u8fc7\u591a\u5e73\u9762\u540c\u6b65\u6280\u672f\u5c062D\u57fa\u7840\u6a21\u578b\u6269\u5c55\u5230\u5168\u666f\u9886\u57df\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf3D\u5168\u666f\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u56e03D\u5168\u666f\u6570\u636e\u7a00\u7f3a\u800c\u4f9d\u8d562D\u57fa\u7840\u6a21\u578b\uff0c\u4f462D\u5355\u89c6\u56fe\u4e0e3D\u5168\u666f\u4e0d\u517c\u5bb9\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u5e73\u9762\u540c\u6b65\u6280\u672f\uff0c\u8bbe\u8ba1DreamCube\uff08\u591a\u5e73\u9762RGB-D\u6269\u6563\u6a21\u578b\uff09\uff0c\u6700\u5927\u5316\u5229\u75282D\u57fa\u7840\u6a21\u578b\u5148\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u65b9\u6cd5\u5728\u5168\u666f\u56fe\u50cf\u751f\u6210\u3001\u6df1\u5ea6\u4f30\u8ba1\u548c3D\u573a\u666f\u751f\u6210\u4e2d\u6709\u6548\u3002", "conclusion": "DreamCube\u901a\u8fc7\u591a\u5e73\u9762\u540c\u6b65\u6280\u672f\u6210\u529f\u6269\u5c552D\u57fa\u7840\u6a21\u578b\u80fd\u529b\uff0c\u5b9e\u73b0\u591a\u6837\u5316\u5916\u89c2\u548c\u7cbe\u786e\u51e0\u4f55\u76843D\u5168\u666f\u751f\u6210\u3002"}}
{"id": "2506.15837", "pdf": "https://arxiv.org/pdf/2506.15837", "abs": "https://arxiv.org/abs/2506.15837", "authors": ["Fatmah AlHindaassi", "Mohammed Talha Alam", "Fakhri Karray"], "title": "ADAM-Dehaze: Adaptive Density-Aware Multi-Stage Dehazing for Improved Object Detection in Foggy Conditions", "categories": ["cs.CV"], "comment": "Under-review at IEEE SMC 2025", "summary": "Adverse weather conditions, particularly fog, pose a significant challenge to autonomous vehicles, surveillance systems, and other safety-critical applications by severely degrading visual information. We introduce ADAM-Dehaze, an adaptive, density-aware dehazing framework that jointly optimizes image restoration and object detection under varying fog intensities. A lightweight Haze Density Estimation Network (HDEN) classifies each input as light, medium, or heavy fog. Based on this score, the system dynamically routes the image through one of three CORUN branches: Light, Medium, or Complex, each tailored to its haze regime. A novel adaptive loss balances physical-model coherence and perceptual fidelity, ensuring both accurate defogging and preservation of fine details. On Cityscapes and the real-world RTTS benchmark, ADAM-Dehaze improves PSNR by up to 2.1 dB, reduces FADE by 30 percent, and increases object detection mAP by up to 13 points, while cutting inference time by 20 percent. These results highlight the importance of intensity-specific processing and seamless integration with downstream vision tasks. Code available at: https://github.com/talha-alam/ADAM-Dehaze.", "AI": {"tldr": "ADAM-Dehaze\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u3001\u5bc6\u5ea6\u611f\u77e5\u7684\u53bb\u96fe\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u7531\u548c\u81ea\u9002\u5e94\u635f\u5931\u4f18\u5316\u56fe\u50cf\u6062\u590d\u548c\u76ee\u6807\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u96fe\u5929\u6c14\u4e25\u91cd\u5f71\u54cd\u89c6\u89c9\u4fe1\u606f\uff0c\u5bf9\u81ea\u52a8\u9a7e\u9a76\u548c\u76d1\u63a7\u7cfb\u7edf\u7b49\u5b89\u5168\u5173\u952e\u5e94\u7528\u6784\u6210\u6311\u6218\u3002", "method": "\u4f7f\u7528HDEN\u7f51\u7edc\u5206\u7c7b\u96fe\u5bc6\u5ea6\uff0c\u52a8\u6001\u8def\u7531\u5230\u4e09\u4e2aCORUN\u5206\u652f\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u635f\u5931\u5e73\u8861\u7269\u7406\u6a21\u578b\u548c\u611f\u77e5\u4fdd\u771f\u5ea6\u3002", "result": "\u5728Cityscapes\u548cRTTS\u57fa\u51c6\u4e0a\uff0cPSNR\u63d0\u53472.1 dB\uff0cFADE\u51cf\u5c1130%\uff0c\u76ee\u6807\u68c0\u6d4bmAP\u63d0\u9ad813\u70b9\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c1120%\u3002", "conclusion": "ADAM-Dehaze\u5c55\u793a\u4e86\u5bc6\u5ea6\u7279\u5b9a\u5904\u7406\u548c\u4e0e\u4e0b\u6e38\u89c6\u89c9\u4efb\u52a1\u65e0\u7f1d\u96c6\u6210\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.15838", "pdf": "https://arxiv.org/pdf/2506.15838", "abs": "https://arxiv.org/abs/2506.15838", "authors": ["Jiahao Wang", "Hualian Sheng", "Sijia Cai", "Weizhan Zhang", "Caixia Yan", "Yachuang Feng", "Bing Deng", "Jieping Ye"], "title": "EchoShot: Multi-Shot Portrait Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Video diffusion models substantially boost the productivity of artistic workflows with high-quality portrait video generative capacity. However, prevailing pipelines are primarily constrained to single-shot creation, while real-world applications urge for multiple shots with identity consistency and flexible content controllability. In this work, we propose EchoShot, a native and scalable multi-shot framework for portrait customization built upon a foundation video diffusion model. To start with, we propose shot-aware position embedding mechanisms within video diffusion transformer architecture to model inter-shot variations and establish intricate correspondence between multi-shot visual content and their textual descriptions. This simple yet effective design enables direct training on multi-shot video data without introducing additional computational overhead. To facilitate model training within multi-shot scenario, we construct PortraitGala, a large-scale and high-fidelity human-centric video dataset featuring cross-shot identity consistency and fine-grained captions such as facial attributes, outfits, and dynamic motions. To further enhance applicability, we extend EchoShot to perform reference image-based personalized multi-shot generation and long video synthesis with infinite shot counts. Extensive evaluations demonstrate that EchoShot achieves superior identity consistency as well as attribute-level controllability in multi-shot portrait video generation. Notably, the proposed framework demonstrates potential as a foundational paradigm for general multi-shot video modeling.", "AI": {"tldr": "EchoShot\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u591a\u955c\u5934\u8096\u50cf\u5b9a\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u4f4d\u7f6e\u5d4c\u5165\u673a\u5236\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6PortraitGala\uff0c\u5b9e\u73b0\u4e86\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u5185\u5bb9\u53ef\u63a7\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6269\u6563\u6a21\u578b\u4ec5\u9650\u4e8e\u5355\u955c\u5934\u751f\u6210\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u9700\u8981\u591a\u955c\u5934\u4e14\u8eab\u4efd\u4e00\u81f4\u7684\u89c6\u9891\u751f\u6210\u3002", "method": "\u63d0\u51faEchoShot\u6846\u67b6\uff0c\u91c7\u7528\u955c\u5934\u611f\u77e5\u4f4d\u7f6e\u5d4c\u5165\u673a\u5236\uff0c\u7ed3\u5408PortraitGala\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u652f\u6301\u57fa\u4e8e\u53c2\u8003\u56fe\u50cf\u7684\u4e2a\u6027\u5316\u751f\u6210\u548c\u957f\u89c6\u9891\u5408\u6210\u3002", "result": "EchoShot\u5728\u591a\u955c\u5934\u8096\u50cf\u89c6\u9891\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u5c5e\u6027\u7ea7\u53ef\u63a7\u6027\u3002", "conclusion": "EchoShot\u4e3a\u901a\u7528\u591a\u955c\u5934\u89c6\u9891\u5efa\u6a21\u63d0\u4f9b\u4e86\u6f5c\u5728\u7684\u57fa\u7840\u8303\u5f0f\u3002"}}
{"id": "2506.15929", "pdf": "https://arxiv.org/pdf/2506.15929", "abs": "https://arxiv.org/abs/2506.15929", "authors": ["Liangyan Li", "Yimo Ning", "Kevin Le", "Wei Dong", "Yunzhe Li", "Jun Chen", "Xiaohong Liu"], "title": "Moir\u00e9XNet: Adaptive Multi-Scale Demoir\u00e9ing with Linear Attention Test-Time Training and Truncated Flow Matching Prior", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "This paper introduces a novel framework for image and video demoir\\'eing by integrating Maximum A Posteriori (MAP) estimation with advanced deep learning techniques. Demoir\\'eing addresses inherently nonlinear degradation processes, which pose significant challenges for existing methods.   Traditional supervised learning approaches either fail to remove moir\\'e patterns completely or produce overly smooth results. This stems from constrained model capacity and scarce training data, which inadequately represent the clean image distribution and hinder accurate reconstruction of ground-truth images. While generative models excel in image restoration for linear degradations, they struggle with nonlinear cases such as demoir\\'eing and often introduce artifacts.   To address these limitations, we propose a hybrid MAP-based framework that integrates two complementary components. The first is a supervised learning model enhanced with efficient linear attention Test-Time Training (TTT) modules, which directly learn nonlinear mappings for RAW-to-sRGB demoir\\'eing. The second is a Truncated Flow Matching Prior (TFMP) that further refines the outputs by aligning them with the clean image distribution, effectively restoring high-frequency details and suppressing artifacts. These two components combine the computational efficiency of linear attention with the refinement abilities of generative models, resulting in improved restoration performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408MAP\u4f30\u8ba1\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7684\u56fe\u50cf\u548c\u89c6\u9891\u53bb\u6469\u5c14\u7eb9\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u975e\u7ebf\u6027\u9000\u5316\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u5b8c\u5168\u53bb\u9664\u6469\u5c14\u7eb9\u6216\u5bfc\u81f4\u8fc7\u5ea6\u5e73\u6ed1\uff0c\u751f\u6210\u6a21\u578b\u5728\u975e\u7ebf\u6027\u9000\u5316\u4e2d\u8868\u73b0\u4e0d\u4f73\u4e14\u6613\u5f15\u5165\u4f2a\u5f71\u3002", "method": "\u63d0\u51fa\u6df7\u5408MAP\u6846\u67b6\uff0c\u7ed3\u5408\u76d1\u7763\u5b66\u4e60\u6a21\u578b\uff08\u5e26\u7ebf\u6027\u6ce8\u610f\u529bTTT\u6a21\u5757\uff09\u548cTFMP\u5148\u9a8c\uff0c\u9ad8\u6548\u5b66\u4e60\u975e\u7ebf\u6027\u6620\u5c04\u5e76\u7ec6\u5316\u8f93\u51fa\u3002", "result": "\u6846\u67b6\u7ed3\u5408\u4e86\u7ebf\u6027\u6ce8\u610f\u529b\u7684\u8ba1\u7b97\u6548\u7387\u548c\u751f\u6210\u6a21\u578b\u7684\u7ec6\u5316\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6062\u590d\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u53bb\u6469\u5c14\u7eb9\u95ee\u9898\uff0c\u6062\u590d\u4e86\u9ad8\u9891\u7ec6\u8282\u5e76\u6291\u5236\u4e86\u4f2a\u5f71\u3002"}}
{"id": "2506.15980", "pdf": "https://arxiv.org/pdf/2506.15980", "abs": "https://arxiv.org/abs/2506.15980", "authors": ["Cong Wang", "Zexuan Deng", "Zhiwei Jiang", "Fei Shen", "Yafeng Yin", "Shiwei Gan", "Zifeng Cheng", "Shiping Ge", "Qing Gu"], "title": "Advanced Sign Language Video Generation with Compressed and Quantized Multi-Condition Tokenization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Sign Language Video Generation (SLVG) seeks to generate identity-preserving sign language videos from spoken language texts. Existing methods primarily rely on the single coarse condition (\\eg, skeleton sequences) as the intermediary to bridge the translation model and the video generation model, which limits both the naturalness and expressiveness of the generated videos. To overcome these limitations, we propose SignViP, a novel SLVG framework that incorporates multiple fine-grained conditions for improved generation fidelity. Rather than directly translating error-prone high-dimensional conditions, SignViP adopts a discrete tokenization paradigm to integrate and represent fine-grained conditions (\\ie, fine-grained poses and 3D hands). SignViP contains three core components. (1) Sign Video Diffusion Model is jointly trained with a multi-condition encoder to learn continuous embeddings that encapsulate fine-grained motion and appearance. (2) Finite Scalar Quantization (FSQ) Autoencoder is further trained to compress and quantize these embeddings into discrete tokens for compact representation of the conditions. (3) Multi-Condition Token Translator is trained to translate spoken language text to discrete multi-condition tokens. During inference, Multi-Condition Token Translator first translates the spoken language text into discrete multi-condition tokens. These tokens are then decoded to continuous embeddings by FSQ Autoencoder, which are subsequently injected into Sign Video Diffusion Model to guide video generation. Experimental results show that SignViP achieves state-of-the-art performance across metrics, including video quality, temporal coherence, and semantic fidelity. The code is available at https://github.com/umnooob/signvip/.", "AI": {"tldr": "SignViP\u662f\u4e00\u4e2a\u65b0\u7684\u624b\u8bed\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ec6\u7c92\u5ea6\u6761\u4ef6\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u7c97\u7cd9\u6761\u4ef6\uff08\u5982\u9aa8\u67b6\u5e8f\u5217\uff09\uff0c\u9650\u5236\u4e86\u751f\u6210\u89c6\u9891\u7684\u81ea\u7136\u6027\u548c\u8868\u73b0\u529b\u3002", "method": "SignViP\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1aSign Video Diffusion Model\u3001FSQ Autoencoder\u548cMulti-Condition Token Translator\uff0c\u901a\u8fc7\u79bb\u6563\u5316\u8303\u5f0f\u6574\u5408\u7ec6\u7c92\u5ea6\u6761\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSignViP\u5728\u89c6\u9891\u8d28\u91cf\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "SignViP\u901a\u8fc7\u591a\u7ec6\u7c92\u5ea6\u6761\u4ef6\u663e\u8457\u63d0\u5347\u4e86\u624b\u8bed\u89c6\u9891\u751f\u6210\u7684\u8d28\u91cf\u548c\u8868\u73b0\u529b\u3002"}}
{"id": "2506.16054", "pdf": "https://arxiv.org/pdf/2506.16054", "abs": "https://arxiv.org/abs/2506.16054", "authors": ["Tianchen Zhao", "Ke Hong", "Xinhao Yang", "Xuefeng Xiao", "Huixia Li", "Feng Ling", "Ruiqi Xie", "Siqi Chen", "Hongyu Zhu", "Yichong Zhang", "Yu Wang"], "title": "PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models", "categories": ["cs.CV", "cs.GR"], "comment": "project page: https://a-suozhang.xyz/paroattn.github.io", "summary": "In visual generation, the quadratic complexity of attention mechanisms results in high memory and computational costs, especially for longer token sequences required in high-resolution image or multi-frame video generation. To address this, prior research has explored techniques such as sparsification and quantization. However, these techniques face significant challenges under low density and reduced bitwidths. Through systematic analysis, we identify that the core difficulty stems from the dispersed and irregular characteristics of visual attention patterns. Therefore, instead of introducing specialized sparsification and quantization design to accommodate such patterns, we propose an alternative strategy: *reorganizing* the attention pattern to alleviate the challenges. Inspired by the local aggregation nature of visual feature extraction, we design a novel **Pattern-Aware token ReOrdering (PARO)** technique, which unifies the diverse attention patterns into a hardware-friendly block-wise pattern. This unification substantially simplifies and enhances both sparsification and quantization. We evaluate the performance-efficiency trade-offs of various design choices and finalize a methodology tailored for the unified pattern. Our approach, **PAROAttention**, achieves video and image generation with lossless metrics, and nearly identical results from full-precision (FP) baselines, while operating at notably lower density (~20%-30%) and bitwidth (**INT8/INT4**), achieving a **1.9x** to **2.7x** end-to-end latency speedup.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPARO\u7684\u6280\u672f\uff0c\u901a\u8fc7\u91cd\u65b0\u7ec4\u7ec7\u6ce8\u610f\u529b\u6a21\u5f0f\u6765\u964d\u4f4e\u89c6\u89c9\u751f\u6210\u4e2d\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u751f\u6210\u4e2d\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u6216\u591a\u5e27\u89c6\u9891\u751f\u6210\u4e2d\u3002\u73b0\u6709\u7a00\u758f\u5316\u548c\u91cf\u5316\u6280\u672f\u9762\u4e34\u4f4e\u5bc6\u5ea6\u548c\u4f4e\u6bd4\u7279\u5bbd\u5ea6\u7684\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u540d\u4e3aPARO\u7684\u6280\u672f\uff0c\u901a\u8fc7\u91cd\u65b0\u7ec4\u7ec7\u6ce8\u610f\u529b\u6a21\u5f0f\u4e3a\u786c\u4ef6\u53cb\u597d\u7684\u5757\u72b6\u6a21\u5f0f\uff0c\u7b80\u5316\u5e76\u589e\u5f3a\u7a00\u758f\u5316\u548c\u91cf\u5316\u3002", "result": "PAROAttention\u5728\u89c6\u9891\u548c\u56fe\u50cf\u751f\u6210\u4e2d\u5b9e\u73b0\u4e86\u65e0\u635f\u6307\u6807\uff0c\u6027\u80fd\u63a5\u8fd1\u5168\u7cbe\u5ea6\u57fa\u7ebf\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5bc6\u5ea6\uff0820%-30%\uff09\u548c\u6bd4\u7279\u5bbd\u5ea6\uff08INT8/INT4\uff09\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u52a0\u901f1.9x\u81f32.7x\u3002", "conclusion": "PARO\u6280\u672f\u901a\u8fc7\u91cd\u65b0\u7ec4\u7ec7\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u751f\u6210\u4e2d\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002"}}
{"id": "2506.16061", "pdf": "https://arxiv.org/pdf/2506.16061", "abs": "https://arxiv.org/abs/2506.16061", "authors": ["Yucheng Jin", "Jinyan Chen", "Ziyue He", "Baojun Han", "Furan An"], "title": "STAR-Pose: Efficient Low-Resolution Video Human Pose Estimation via Spatial-Temporal Adaptive Super-Resolution", "categories": ["cs.CV"], "comment": "14pages 3figures, alredy submiss to PRCV 2025", "summary": "Human pose estimation in low-resolution videos presents a fundamental challenge in computer vision. Conventional methods either assume high-quality inputs or employ computationally expensive cascaded processing, which limits their deployment in resource-constrained environments. We propose STAR-Pose, a spatial-temporal adaptive super-resolution framework specifically designed for video-based human pose estimation. Our method features a novel spatial-temporal Transformer with LeakyReLU-modified linear attention, which efficiently captures long-range temporal dependencies. Moreover, it is complemented by an adaptive fusion module that integrates parallel CNN branch for local texture enhancement. We also design a pose-aware compound loss to achieve task-oriented super-resolution. This loss guides the network to reconstruct structural features that are most beneficial for keypoint localization, rather than optimizing purely for visual quality. Extensive experiments on several mainstream video HPE datasets demonstrate that STAR-Pose outperforms existing approaches. It achieves up to 5.2% mAP improvement under extremely low-resolution (64x48) conditions while delivering 2.8x to 4.4x faster inference than cascaded approaches.", "AI": {"tldr": "STAR-Pose\u662f\u4e00\u79cd\u9488\u5bf9\u4f4e\u5206\u8fa8\u7387\u89c6\u9891\u4e2d\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u7a7a\u95f4-\u65f6\u95f4\u81ea\u9002\u5e94\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u7684Transformer\u548c\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f4e\u5206\u8fa8\u7387\u89c6\u9891\u4e2d\u7684\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u5b58\u5728\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u8d28\u91cf\u8f93\u5165\u6216\u8ba1\u7b97\u5bc6\u96c6\u578b\u5904\u7406\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u3002", "method": "\u63d0\u51faSTAR-Pose\u6846\u67b6\uff0c\u7ed3\u5408\u7a7a\u95f4-\u65f6\u95f4Transformer\uff08\u5e26LeakyReLU\u7684\u7ebf\u6027\u6ce8\u610f\u529b\uff09\u548c\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757\uff0c\u8bbe\u8ba1\u59ff\u6001\u611f\u77e5\u590d\u5408\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u591a\u4e2a\u4e3b\u6d41\u89c6\u9891HPE\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c64x48\u5206\u8fa8\u7387\u4e0bmAP\u63d0\u53475.2%\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u7ea7\u8054\u65b9\u6cd5\u5feb2.8x\u81f34.4x\u3002", "conclusion": "STAR-Pose\u5728\u4f4e\u5206\u8fa8\u7387\u89c6\u9891\u4e2d\u9ad8\u6548\u4e14\u51c6\u786e\u5730\u5b9e\u73b0\u4e86\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.16119", "pdf": "https://arxiv.org/pdf/2506.16119", "abs": "https://arxiv.org/abs/2506.16119", "authors": ["Chengyu Bai", "Yuming Li", "Zhongyu Zhao", "Jintao Chen", "Peidong Jia", "Qi She", "Ming Lu", "Shanghang Zhang"], "title": "FastInit: Fast Noise Initialization for Temporally Consistent Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Video generation has made significant strides with the development of diffusion models; however, achieving high temporal consistency remains a challenging task. Recently, FreeInit identified a training-inference gap and introduced a method to iteratively refine the initial noise during inference. However, iterative refinement significantly increases the computational cost associated with video generation. In this paper, we introduce FastInit, a fast noise initialization method that eliminates the need for iterative refinement. FastInit learns a Video Noise Prediction Network (VNPNet) that takes random noise and a text prompt as input, generating refined noise in a single forward pass. Therefore, FastInit greatly enhances the efficiency of video generation while achieving high temporal consistency across frames. To train the VNPNet, we create a large-scale dataset consisting of pairs of text prompts, random noise, and refined noise. Extensive experiments with various text-to-video models show that our method consistently improves the quality and temporal consistency of the generated videos. FastInit not only provides a substantial improvement in video generation but also offers a practical solution that can be applied directly during inference. The code and dataset will be released.", "AI": {"tldr": "FastInit\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u566a\u58f0\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u566a\u58f0\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u751f\u6210\u6548\u7387\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982FreeInit\uff09\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u566a\u58f0\u63d0\u9ad8\u65f6\u5e8f\u4e00\u81f4\u6027\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0cFastInit\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "FastInit\u8bad\u7ec3\u4e86\u4e00\u4e2a\u89c6\u9891\u566a\u58f0\u9884\u6d4b\u7f51\u7edc\uff08VNPNet\uff09\uff0c\u8f93\u5165\u968f\u673a\u566a\u58f0\u548c\u6587\u672c\u63d0\u793a\uff0c\u5355\u6b21\u751f\u6210\u4f18\u5316\u540e\u7684\u566a\u58f0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFastInit\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u7684\u8d28\u91cf\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "FastInit\u4e3a\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u63a8\u7406\u9636\u6bb5\u3002"}}
{"id": "2506.16186", "pdf": "https://arxiv.org/pdf/2506.16186", "abs": "https://arxiv.org/abs/2506.16186", "authors": ["Zhenghao Xi", "Xiang Liu", "Yaqi Liu", "Yitong Cai", "Yangyu Zheng"], "title": "Integrating Generative Adversarial Networks and Convolutional Neural Networks for Enhanced Traffic Accidents Detection and Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Accident detection using Closed Circuit Television (CCTV) footage is one of the most imperative features for enhancing transport safety and efficient traffic control. To this end, this research addresses the issues of supervised monitoring and data deficiency in accident detection systems by adapting excellent deep learning technologies. The motivation arises from rising statistics in the number of car accidents worldwide; this calls for innovation and the establishment of a smart, efficient and automated way of identifying accidents and calling for help to save lives. Addressing the problem of the scarcity of data, the presented framework joins Generative Adversarial Networks (GANs) for synthesizing data and Convolutional Neural Networks (CNN) for model training. Video frames for accidents and non-accidents are collected from YouTube videos, and we perform resizing, image enhancement and image normalisation pixel range adjustments. Three models are used: CNN, Fine-tuned Convolutional Neural Network (FTCNN) and Vision Transformer (VIT) worked best for detecting accidents from CCTV, obtaining an accuracy rate of 94% and 95%, while the CNN model obtained 88%. Such results show that the proposed framework suits traffic safety applications due to its high real-time accident detection capabilities and broad-scale applicability. This work lays the foundation for intelligent surveillance systems in the future for real-time traffic monitoring, smart city framework, and integration of intelligent surveillance systems into emergency management systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4ea4\u901a\u4e8b\u6545\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408GANs\u751f\u6210\u6570\u636e\u548cCNNs\u8bad\u7ec3\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5b9e\u65f6\u4e8b\u6545\u68c0\u6d4b\u3002", "motivation": "\u5168\u7403\u4ea4\u901a\u4e8b\u6545\u6570\u91cf\u4e0a\u5347\uff0c\u9700\u8981\u667a\u80fd\u3001\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u4e8b\u6545\u68c0\u6d4b\u7cfb\u7edf\u4ee5\u63d0\u9ad8\u4ea4\u901a\u5b89\u5168\u3002", "method": "\u4f7f\u7528GANs\u5408\u6210\u6570\u636e\uff0cCNNs\u8bad\u7ec3\u6a21\u578b\uff0c\u7ed3\u5408FTCNN\u548cVIT\u6a21\u578b\uff0c\u4eceYouTube\u89c6\u9891\u4e2d\u6536\u96c6\u4e8b\u6545\u548c\u975e\u4e8b\u6545\u5e27\u5e76\u8fdb\u884c\u9884\u5904\u7406\u3002", "result": "FTCNN\u548cVIT\u6a21\u578b\u5206\u522b\u8fbe\u523094%\u548c95%\u7684\u51c6\u786e\u7387\uff0cCNN\u6a21\u578b\u4e3a88%\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u9ad8\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u9002\u7528\u4e8e\u5b9e\u65f6\u4ea4\u901a\u76d1\u63a7\u548c\u667a\u80fd\u57ce\u5e02\u7cfb\u7edf\uff0c\u4e3a\u672a\u6765\u667a\u80fd\u76d1\u63a7\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.16209", "pdf": "https://arxiv.org/pdf/2506.16209", "abs": "https://arxiv.org/abs/2506.16209", "authors": ["Annajoyce Mariani", "Kira Maag", "Hanno Gottschalk"], "title": "VideoGAN-based Trajectory Proposal for Automated Vehicles", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Being able to generate realistic trajectory options is at the core of increasing the degree of automation of road vehicles. While model-driven, rule-based, and classical learning-based methods are widely used to tackle these tasks at present, they can struggle to effectively capture the complex, multimodal distributions of future trajectories. In this paper we investigate whether a generative adversarial network (GAN) trained on videos of bird's-eye view (BEV) traffic scenarios can generate statistically accurate trajectories that correctly capture spatial relationships between the agents. To this end, we propose a pipeline that uses low-resolution BEV occupancy grid videos as training data for a video generative model. From the generated videos of traffic scenarios we extract abstract trajectory data using single-frame object detection and frame-to-frame object matching. We particularly choose a GAN architecture for the fast training and inference times with respect to diffusion models. We obtain our best results within 100 GPU hours of training, with inference times under 20\\,ms. We demonstrate the physical realism of the proposed trajectories in terms of distribution alignment of spatial and dynamic parameters with respect to the ground truth videos from the Waymo Open Motion Dataset.", "AI": {"tldr": "\u4f7f\u7528GAN\u751f\u6210\u9e1f\u77b0\u89c6\u89d2\u4ea4\u901a\u573a\u666f\u89c6\u9891\uff0c\u4ece\u4e2d\u63d0\u53d6\u8f68\u8ff9\u6570\u636e\uff0c\u9a8c\u8bc1\u5176\u7edf\u8ba1\u51c6\u786e\u6027\u548c\u7269\u7406\u771f\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u672a\u6765\u8f68\u8ff9\u7684\u590d\u6742\u591a\u6a21\u6001\u5206\u5e03\uff0c\u56e0\u6b64\u63a2\u7d22GAN\u662f\u5426\u80fd\u751f\u6210\u51c6\u786e\u4e14\u7a7a\u95f4\u5173\u7cfb\u6b63\u786e\u7684\u8f68\u8ff9\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7ba1\u9053\uff0c\u5229\u7528\u4f4e\u5206\u8fa8\u7387\u9e1f\u77b0\u5360\u7528\u7f51\u683c\u89c6\u9891\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5355\u5e27\u76ee\u6807\u68c0\u6d4b\u548c\u5e27\u95f4\u76ee\u6807\u5339\u914d\u63d0\u53d6\u8f68\u8ff9\u6570\u636e\u3002", "result": "\u5728100 GPU\u5c0f\u65f6\u5185\u5b8c\u6210\u8bad\u7ec3\uff0c\u63a8\u7406\u65f6\u95f4\u4f4e\u4e8e20\u6beb\u79d2\uff0c\u751f\u6210\u7684\u8f68\u8ff9\u5728\u7a7a\u95f4\u548c\u52a8\u6001\u53c2\u6570\u4e0a\u4e0e\u771f\u5b9e\u6570\u636e\u5206\u5e03\u4e00\u81f4\u3002", "conclusion": "GAN\u80fd\u9ad8\u6548\u751f\u6210\u7269\u7406\u771f\u5b9e\u7684\u8f68\u8ff9\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u3002"}}
{"id": "2506.16262", "pdf": "https://arxiv.org/pdf/2506.16262", "abs": "https://arxiv.org/abs/2506.16262", "authors": ["Weeyoung Kwon", "Jeahun Sung", "Minkyu Jeon", "Chanho Eom", "Jihyong Oh"], "title": "R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement for 3D Low-Level Vision", "categories": ["cs.CV"], "comment": "Please visit our project page at   https://github.com/CMLab-Korea/Awesome-3D-Low-Level-Vision", "summary": "Neural rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved significant progress in photorealistic 3D scene reconstruction and novel view synthesis. However, most existing models assume clean and high-resolution (HR) multi-view inputs, which limits their robustness under real-world degradations such as noise, blur, low-resolution (LR), and weather-induced artifacts. To address these limitations, the emerging field of 3D Low-Level Vision (3D LLV) extends classical 2D Low-Level Vision tasks including super-resolution (SR), deblurring, weather degradation removal, restoration, and enhancement into the 3D spatial domain. This survey, referred to as R\\textsuperscript{3}eVision, provides a comprehensive overview of robust rendering, restoration, and enhancement for 3D LLV by formalizing the degradation-aware rendering problem and identifying key challenges related to spatio-temporal consistency and ill-posed optimization. Recent methods that integrate LLV into neural rendering frameworks are categorized to illustrate how they enable high-fidelity 3D reconstruction under adverse conditions. Application domains such as autonomous driving, AR/VR, and robotics are also discussed, where reliable 3D perception from degraded inputs is critical. By reviewing representative methods, datasets, and evaluation protocols, this work positions 3D LLV as a fundamental direction for robust 3D content generation and scene-level reconstruction in real-world environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e863D\u4f4e\u5c42\u89c6\u89c9\uff083D LLV\uff09\u9886\u57df\uff0c\u63a2\u8ba8\u4e86\u5982\u4f55\u5728\u9000\u5316\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u4fdd\u771f\u76843D\u91cd\u5efa\u548c\u89c6\u56fe\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\uff08\u5982NeRF\u548c3DGS\uff09\u901a\u5e38\u5047\u8bbe\u8f93\u5165\u4e3a\u9ad8\u8d28\u91cf\u591a\u89c6\u56fe\u56fe\u50cf\uff0c\u4f46\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u5e38\u9762\u4e34\u566a\u58f0\u3001\u6a21\u7cca\u3001\u4f4e\u5206\u8fa8\u7387\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u9c81\u68d2\u6027\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5f62\u5f0f\u5316\u9000\u5316\u611f\u77e5\u6e32\u67d3\u95ee\u9898\uff0c\u5206\u7c7b\u603b\u7ed3\u4e86\u5c06\u4f4e\u5c42\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u8d85\u5206\u8fa8\u7387\u3001\u53bb\u6a21\u7cca\u7b49\uff09\u878d\u5165\u795e\u7ecf\u6e32\u67d3\u6846\u67b6\u7684\u73b0\u6709\u65b9\u6cd5\u3002", "result": "\u7efc\u8ff0\u5c55\u793a\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5982\u4f55\u63d0\u5347\u5728\u9000\u5316\u6761\u4ef6\u4e0b\u76843D\u91cd\u5efa\u8d28\u91cf\uff0c\u5e76\u8ba8\u8bba\u4e86\u5728\u81ea\u52a8\u9a7e\u9a76\u3001AR/VR\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002", "conclusion": "3D LLV\u88ab\u89c6\u4e3a\u5b9e\u73b0\u771f\u5b9e\u73af\u5883\u4e2d\u9c81\u68d23D\u5185\u5bb9\u751f\u6210\u548c\u573a\u666f\u91cd\u5efa\u7684\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2506.16307", "pdf": "https://arxiv.org/pdf/2506.16307", "abs": "https://arxiv.org/abs/2506.16307", "authors": ["Xu Zhao", "Chen Zhao", "Xiantao Hu", "Hongliang Zhang", "Ying Tai", "Jian Yang"], "title": "Learning Multi-scale Spatial-frequency Features for Image Denoising", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Recent advancements in multi-scale architectures have demonstrated exceptional performance in image denoising tasks. However, existing architectures mainly depends on a fixed single-input single-output Unet architecture, ignoring the multi-scale representations of pixel level. In addition, previous methods treat the frequency domain uniformly, ignoring the different characteristics of high-frequency and low-frequency noise. In this paper, we propose a novel multi-scale adaptive dual-domain network (MADNet) for image denoising. We use image pyramid inputs to restore noise-free results from low-resolution images. In order to realize the interaction of high-frequency and low-frequency information, we design an adaptive spatial-frequency learning unit (ASFU), where a learnable mask is used to separate the information into high-frequency and low-frequency components. In the skip connections, we design a global feature fusion block to enhance the features at different scales. Extensive experiments on both synthetic and real noisy image datasets verify the effectiveness of MADNet compared with current state-of-the-art denoising approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u53cc\u57df\u7f51\u7edc\uff08MADNet\uff09\uff0c\u7528\u4e8e\u56fe\u50cf\u53bb\u566a\uff0c\u901a\u8fc7\u56fe\u50cf\u91d1\u5b57\u5854\u8f93\u5165\u548c\u81ea\u9002\u5e94\u7a7a\u95f4\u9891\u7387\u5b66\u4e60\u5355\u5143\uff08ASFU\uff09\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u5355\u8f93\u5165\u5355\u8f93\u51fa\u7684Unet\u67b6\u6784\uff0c\u5ffd\u7565\u4e86\u50cf\u7d20\u7ea7\u591a\u5c3a\u5ea6\u8868\u793a\uff0c\u4e14\u672a\u533a\u5206\u9ad8\u9891\u548c\u4f4e\u9891\u566a\u58f0\u7279\u6027\u3002", "method": "\u4f7f\u7528\u56fe\u50cf\u91d1\u5b57\u5854\u8f93\u5165\uff0c\u8bbe\u8ba1ASFU\u5355\u5143\u5206\u79bb\u9ad8\u4f4e\u9891\u4fe1\u606f\uff0c\u5e76\u5728\u8df3\u8dc3\u8fde\u63a5\u4e2d\u5f15\u5165\u5168\u5c40\u7279\u5f81\u878d\u5408\u5757\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u566a\u58f0\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MADNet\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u53bb\u566a\u65b9\u6cd5\u3002", "conclusion": "MADNet\u901a\u8fc7\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u53cc\u57df\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u53bb\u566a\u6027\u80fd\u3002"}}
{"id": "2506.16504", "pdf": "https://arxiv.org/pdf/2506.16504", "abs": "https://arxiv.org/abs/2506.16504", "authors": ["Zeqiang Lai", "Yunfei Zhao", "Haolin Liu", "Zibo Zhao", "Qingxiang Lin", "Huiwen Shi", "Xianghui Yang", "Mingxin Yang", "Shuhui Yang", "Yifei Feng", "Sheng Zhang", "Xin Huang", "Di Luo", "Fan Yang", "Fang Yang", "Lifu Wang", "Sicong Liu", "Yixuan Tang", "Yulin Cai", "Zebin He", "Tian Liu", "Yuhong Liu", "Jie Jiang", "Linus", "Jingwei Huang", "Chunchao Guo"], "title": "Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate Details", "categories": ["cs.CV", "cs.AI"], "comment": "Technical report", "summary": "In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion models aimed at generating high-fidelity and detailed textured 3D assets. Hunyuan3D 2.5 follows two-stages pipeline of its previous version Hunyuan3D 2.0, while demonstrating substantial advancements in both shape and texture generation. In terms of shape generation, we introduce a new shape foundation model -- LATTICE, which is trained with scaled high-quality datasets, model-size, and compute. Our largest model reaches 10B parameters and generates sharp and detailed 3D shape with precise image-3D following while keeping mesh surface clean and smooth, significantly closing the gap between generated and handcrafted 3D shapes. In terms of texture generation, it is upgraded with phyiscal-based rendering (PBR) via a novel multi-view architecture extended from Hunyuan3D 2.0 Paint model. Our extensive evaluation shows that Hunyuan3D 2.5 significantly outperforms previous methods in both shape and end-to-end texture generation.", "AI": {"tldr": "Hunyuan3D 2.5\u662f\u4e00\u4e2a\u5f3a\u5927\u76843D\u6269\u6563\u6a21\u578b\u5957\u4ef6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u548c\u7ec6\u8282\u4e30\u5bcc\u76843D\u8d44\u4ea7\u3002\u5b83\u5728\u5f62\u72b6\u548c\u7eb9\u7406\u751f\u6210\u65b9\u9762\u5747\u6709\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u89e3\u51b3\u73b0\u67093D\u751f\u6210\u6a21\u578b\u5728\u5f62\u72b6\u548c\u7eb9\u7406\u7ec6\u8282\u4e0a\u7684\u4e0d\u8db3\uff0c\u7f29\u5c0f\u751f\u6210\u4e0e\u624b\u5de5\u5236\u4f5c3D\u5f62\u72b6\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff0c\u5f15\u5165\u65b0\u7684\u5f62\u72b6\u57fa\u7840\u6a21\u578bLATTICE\uff0810B\u53c2\u6570\uff09\uff0c\u5e76\u5347\u7ea7\u7eb9\u7406\u751f\u6210\u6a21\u578b\uff0c\u652f\u6301\u57fa\u4e8e\u7269\u7406\u7684\u6e32\u67d3\uff08PBR\uff09\u3002", "result": "\u5728\u5f62\u72b6\u548c\u7aef\u5230\u7aef\u7eb9\u7406\u751f\u6210\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "Hunyuan3D 2.5\u57283D\u8d44\u4ea7\u751f\u6210\u9886\u57df\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2506.16679", "pdf": "https://arxiv.org/pdf/2506.16679", "abs": "https://arxiv.org/abs/2506.16679", "authors": ["Manuel Brack", "Sudeep Katakol", "Felix Friedrich", "Patrick Schramowski", "Hareesh Ravi", "Kristian Kersting", "Ajinkya Kale"], "title": "How to Train your Text-to-Image Model: Evaluating Design Choices for Synthetic Training Captions", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Training data is at the core of any successful text-to-image models. The quality and descriptiveness of image text are crucial to a model's performance. Given the noisiness and inconsistency in web-scraped datasets, recent works shifted towards synthetic training captions. While this setup is generally believed to produce more capable models, current literature does not provide any insights into its design choices. This study closes this gap by systematically investigating how different synthetic captioning strategies impact the downstream performance of text-to-image models. Our experiments demonstrate that dense, high-quality captions enhance text alignment but may introduce trade-offs in output aesthetics and diversity. Conversely, captions of randomized lengths yield balanced improvements across aesthetics and alignment without compromising sample diversity. We also demonstrate that varying caption distributions introduce significant shifts in the output bias of a trained model. Our findings underscore the importance of caption design in achieving optimal model performance and provide practical insights for more effective training data strategies in text-to-image generation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5408\u6210\u5b57\u5e55\u7b56\u7565\u5bf9\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9ad8\u8d28\u91cf\u5b57\u5e55\u80fd\u63d0\u5347\u6587\u672c\u5bf9\u9f50\u4f46\u53ef\u80fd\u727a\u7272\u7f8e\u5b66\u548c\u591a\u6837\u6027\uff0c\u800c\u968f\u673a\u957f\u5ea6\u5b57\u5e55\u5219\u80fd\u5e73\u8861\u7f8e\u5b66\u548c\u5bf9\u9f50\u3002", "motivation": "\u7531\u4e8e\u7f51\u7edc\u6293\u53d6\u6570\u636e\u96c6\u7684\u566a\u58f0\u548c\u4e0d\u4e00\u81f4\u6027\uff0c\u5408\u6210\u5b57\u5e55\u6210\u4e3a\u8bad\u7ec3\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u65b0\u8d8b\u52bf\uff0c\u4f46\u5176\u8bbe\u8ba1\u9009\u62e9\u7f3a\u4e4f\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u7814\u7a76\u4e0d\u540c\u5408\u6210\u5b57\u5e55\u7b56\u7565\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u9ad8\u8d28\u91cf\u5b57\u5e55\u63d0\u5347\u6587\u672c\u5bf9\u9f50\u4f46\u53ef\u80fd\u964d\u4f4e\u7f8e\u5b66\u548c\u591a\u6837\u6027\uff1b\u968f\u673a\u957f\u5ea6\u5b57\u5e55\u5e73\u8861\u7f8e\u5b66\u548c\u5bf9\u9f50\uff1b\u4e0d\u540c\u5b57\u5e55\u5206\u5e03\u4f1a\u663e\u8457\u6539\u53d8\u6a21\u578b\u8f93\u51fa\u504f\u5dee\u3002", "conclusion": "\u5b57\u5e55\u8bbe\u8ba1\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u6570\u636e\u7b56\u7565\u3002"}}
{"id": "2506.16730", "pdf": "https://arxiv.org/pdf/2506.16730", "abs": "https://arxiv.org/abs/2506.16730", "authors": ["Mingrui Zhu", "Xiru Chen", "Xin Wei", "Nannan Wang", "Xinbo Gao"], "title": "TeSG: Textual Semantic Guidance for Infrared and Visible Image Fusion", "categories": ["cs.CV"], "comment": "11 pages, 6 figures", "summary": "Infrared and visible image fusion (IVF) aims to combine complementary information from both image modalities, producing more informative and comprehensive outputs. Recently, text-guided IVF has shown great potential due to its flexibility and versatility. However, the effective integration and utilization of textual semantic information remains insufficiently studied. To tackle these challenges, we introduce textual semantics at two levels: the mask semantic level and the text semantic level, both derived from textual descriptions extracted by large Vision-Language Models (VLMs). Building on this, we propose Textual Semantic Guidance for infrared and visible image fusion, termed TeSG, which guides the image synthesis process in a way that is optimized for downstream tasks such as detection and segmentation. Specifically, TeSG consists of three core components: a Semantic Information Generator (SIG), a Mask-Guided Cross-Attention (MGCA) module, and a Text-Driven Attentional Fusion (TDAF) module. The SIG generates mask and text semantics based on textual descriptions. The MGCA module performs initial attention-based fusion of visual features from both infrared and visible images, guided by mask semantics. Finally, the TDAF module refines the fusion process with gated attention driven by text semantics. Extensive experiments demonstrate the competitiveness of our approach, particularly in terms of performance on downstream tasks, compared to existing state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u8bed\u4e49\u6307\u5bfc\u7684\u7ea2\u5916\u4e0e\u53ef\u89c1\u5149\u56fe\u50cf\u878d\u5408\u65b9\u6cd5TeSG\uff0c\u901a\u8fc7\u63a9\u7801\u548c\u6587\u672c\u8bed\u4e49\u4fe1\u606f\u4f18\u5316\u878d\u5408\u8fc7\u7a0b\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5f15\u5bfc\u7684\u7ea2\u5916\u4e0e\u53ef\u89c1\u5149\u56fe\u50cf\u878d\u5408\u65b9\u6cd5\u5bf9\u6587\u672c\u8bed\u4e49\u4fe1\u606f\u7684\u5229\u7528\u4e0d\u8db3\uff0c\u5f71\u54cd\u4e86\u878d\u5408\u6548\u679c\u548c\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002", "method": "TeSG\u5305\u542b\u8bed\u4e49\u4fe1\u606f\u751f\u6210\u5668\uff08SIG\uff09\u3001\u63a9\u7801\u5f15\u5bfc\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff08MGCA\uff09\u548c\u6587\u672c\u9a71\u52a8\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\uff08TDAF\uff09\uff0c\u5206\u522b\u751f\u6210\u8bed\u4e49\u4fe1\u606f\u3001\u521d\u6b65\u878d\u5408\u89c6\u89c9\u7279\u5f81\u548c\u4f18\u5316\u878d\u5408\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTeSG\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TeSG\u901a\u8fc7\u591a\u5c42\u6b21\u6587\u672c\u8bed\u4e49\u6307\u5bfc\uff0c\u6709\u6548\u63d0\u5347\u4e86\u56fe\u50cf\u878d\u5408\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.16735", "pdf": "https://arxiv.org/pdf/2506.16735", "abs": "https://arxiv.org/abs/2506.16735", "authors": ["Yunshan Li", "Wenwu Gong", "Qianqian Wang", "Chao Wang", "Lili Yang"], "title": "3DeepRep: 3D Deep Low-rank Tensor Representation for Hyperspectral Image Inpainting", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Recent approaches based on transform-based tensor nuclear norm (TNN) have demonstrated notable effectiveness in hyperspectral image (HSI) inpainting by leveraging low-rank structures in latent representations. Recent developments incorporate deep transforms to improve low-rank tensor representation; however, existing approaches typically restrict the transform to the spectral mode, neglecting low-rank properties along other tensor modes. In this paper, we propose a novel 3-directional deep low-rank tensor representation (3DeepRep) model, which performs deep nonlinear transforms along all three modes of the HSI tensor. To enforce low-rankness, the model minimizes the nuclear norms of mode-i frontal slices in the corresponding latent space for each direction (i=1,2,3), forming a 3-directional TNN regularization. The outputs from the three directional branches are subsequently fused via a learnable aggregation module to produce the final result. An efficient gradient-based optimization algorithm is developed to solve the model in a self-supervised manner. Extensive experiments on real-world HSI datasets demonstrate that the proposed method achieves superior inpainting performance compared to existing state-of-the-art techniques, both qualitatively and quantitatively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b3\u65b9\u5411\u6df1\u5ea6\u4f4e\u79e9\u5f20\u91cf\u8868\u793a\u6a21\u578b\uff083DeepRep\uff09\uff0c\u901a\u8fc7\u591a\u65b9\u5411\u975e\u7ebf\u6027\u53d8\u6362\u63d0\u5347HSI\u4fee\u590d\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u5149\u8c31\u6a21\u5f0f\u7684\u4f4e\u79e9\u7279\u6027\uff0c\u5ffd\u7565\u4e86\u5176\u4ed6\u5f20\u91cf\u6a21\u5f0f\u7684\u4f4e\u79e9\u7ed3\u6784\u3002", "method": "3DeepRep\u6a21\u578b\u5728HSI\u5f20\u91cf\u7684\u4e09\u4e2a\u65b9\u5411\u4e0a\u6267\u884c\u6df1\u5ea6\u975e\u7ebf\u6027\u53d8\u6362\uff0c\u5e76\u901a\u8fc7\u6838\u8303\u6570\u6700\u5c0f\u5316\u5b9e\u73b0\u4f4e\u79e9\u8868\u793a\uff0c\u6700\u7ec8\u901a\u8fc7\u53ef\u5b66\u4e60\u805a\u5408\u6a21\u5757\u878d\u5408\u7ed3\u679c\u3002", "result": "\u5728\u771f\u5b9eHSI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "3DeepRep\u901a\u8fc7\u591a\u65b9\u5411\u4f4e\u79e9\u8868\u793a\u663e\u8457\u63d0\u5347\u4e86HSI\u4fee\u590d\u6027\u80fd\u3002"}}
{"id": "2506.16743", "pdf": "https://arxiv.org/pdf/2506.16743", "abs": "https://arxiv.org/abs/2506.16743", "authors": ["Weinan Guan", "Wei Wang", "Bo Peng", "Ziwen He", "Jing Dong", "Haonan Cheng"], "title": "Noise-Informed Diffusion-Generated Image Detection with Anomaly Attention", "categories": ["cs.CV"], "comment": "Accepted by TIFS 2025. Our code is availabel at   https://github.com/WeinanGuan/NASA-Swin", "summary": "With the rapid development of image generation technologies, especially the advancement of Diffusion Models, the quality of synthesized images has significantly improved, raising concerns among researchers about information security. To mitigate the malicious abuse of diffusion models, diffusion-generated image detection has proven to be an effective countermeasure.However, a key challenge for forgery detection is generalising to diffusion models not seen during training. In this paper, we address this problem by focusing on image noise. We observe that images from different diffusion models share similar noise patterns, distinct from genuine images. Building upon this insight, we introduce a novel Noise-Aware Self-Attention (NASA) module that focuses on noise regions to capture anomalous patterns. To implement a SOTA detection model, we incorporate NASA into Swin Transformer, forming an novel detection architecture NASA-Swin. Additionally, we employ a cross-modality fusion embedding to combine RGB and noise images, along with a channel mask strategy to enhance feature learning from both modalities. Extensive experiments demonstrate the effectiveness of our approach in enhancing detection capabilities for diffusion-generated images. When encountering unseen generation methods, our approach achieves the state-of-the-art performance.Our code is available at https://github.com/WeinanGuan/NASA-Swin.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u566a\u58f0\u611f\u77e5\u81ea\u6ce8\u610f\u529b\u6a21\u5757\uff08NASA\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u8bad\u7ec3\u4e2d\u672a\u89c1\u6269\u6563\u6a21\u578b\u7684\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u6269\u6563\u6a21\u578b\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u7684\u63d0\u5347\uff0c\u4fe1\u606f\u5b89\u5168\u9690\u60a3\u589e\u52a0\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u6269\u6563\u6a21\u578b\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5206\u6790\u56fe\u50cf\u566a\u58f0\u6a21\u5f0f\uff0c\u63d0\u51faNASA\u6a21\u5757\uff0c\u7ed3\u5408Swin Transformer\u6784\u5efaNASA-Swin\u67b6\u6784\uff0c\u5e76\u91c7\u7528\u8de8\u6a21\u6001\u878d\u5408\u5d4c\u5165\u548c\u901a\u9053\u63a9\u7801\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u6269\u6563\u751f\u6210\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5bf9\u672a\u89c1\u751f\u6210\u65b9\u6cd5\u5177\u6709SOTA\u6027\u80fd\u3002", "conclusion": "NASA-Swin\u67b6\u6784\u901a\u8fc7\u566a\u58f0\u611f\u77e5\u548c\u8de8\u6a21\u6001\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u751f\u6210\u56fe\u50cf\u7684\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2506.16773", "pdf": "https://arxiv.org/pdf/2506.16773", "abs": "https://arxiv.org/abs/2506.16773", "authors": ["Shuchen Sun", "Ligen Shi", "Chang Liu", "Lina Wu", "Jun Qiu"], "title": "Infrared and Visible Image Fusion Based on Implicit Neural Representations", "categories": ["cs.CV"], "comment": null, "summary": "Infrared and visible light image fusion aims to combine the strengths of both modalities to generate images that are rich in information and fulfill visual or computational requirements. This paper proposes an image fusion method based on Implicit Neural Representations (INR), referred to as INRFuse. This method parameterizes a continuous function through a neural network to implicitly represent the multimodal information of the image, breaking through the traditional reliance on discrete pixels or explicit features. The normalized spatial coordinates of the infrared and visible light images serve as inputs, and multi-layer perceptrons is utilized to adaptively fuse the features of both modalities, resulting in the output of the fused image. By designing multiple loss functions, the method jointly optimizes the similarity between the fused image and the original images, effectively preserving the thermal radiation information of the infrared image while maintaining the texture details of the visible light image. Furthermore, the resolution-independent characteristic of INR allows for the direct fusion of images with varying resolutions and achieves super-resolution reconstruction through high-density coordinate queries. Experimental results indicate that INRFuse outperforms existing methods in both subjective visual quality and objective evaluation metrics, producing fused images with clear structures, natural details, and rich information without the necessity for a training dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u7684\u56fe\u50cf\u878d\u5408\u65b9\u6cd5INRFuse\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316\u8fde\u7eed\u51fd\u6570\uff0c\u7a81\u7834\u4f20\u7edf\u4f9d\u8d56\u79bb\u6563\u50cf\u7d20\u6216\u663e\u5f0f\u7279\u5f81\u7684\u5c40\u9650\uff0c\u5b9e\u73b0\u7ea2\u5916\u4e0e\u53ef\u89c1\u5149\u56fe\u50cf\u7684\u9ad8\u8d28\u91cf\u878d\u5408\u3002", "motivation": "\u7ed3\u5408\u7ea2\u5916\u4e0e\u53ef\u89c1\u5149\u56fe\u50cf\u7684\u4f18\u52bf\uff0c\u751f\u6210\u4fe1\u606f\u4e30\u5bcc\u4e14\u6ee1\u8db3\u89c6\u89c9\u6216\u8ba1\u7b97\u9700\u6c42\u7684\u56fe\u50cf\u3002", "method": "\u5229\u7528\u5f52\u4e00\u5316\u7a7a\u95f4\u5750\u6807\u4f5c\u4e3a\u8f93\u5165\uff0c\u901a\u8fc7\u591a\u5c42\u611f\u77e5\u5668\u81ea\u9002\u5e94\u878d\u5408\u4e24\u79cd\u6a21\u6001\u7684\u7279\u5f81\uff0c\u8bbe\u8ba1\u591a\u635f\u5931\u51fd\u6570\u8054\u5408\u4f18\u5316\u76f8\u4f3c\u6027\u3002", "result": "INRFuse\u5728\u4e3b\u89c2\u89c6\u89c9\u8d28\u91cf\u548c\u5ba2\u89c2\u8bc4\u4ef7\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u878d\u5408\u56fe\u50cf\u7ed3\u6784\u6e05\u6670\u3001\u7ec6\u8282\u81ea\u7136\u3001\u4fe1\u606f\u4e30\u5bcc\u3002", "conclusion": "INRFuse\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u652f\u6301\u4e0d\u540c\u5206\u8fa8\u7387\u56fe\u50cf\u76f4\u63a5\u878d\u5408\uff0c\u5e76\u80fd\u901a\u8fc7\u9ad8\u5bc6\u5ea6\u5750\u6807\u67e5\u8be2\u5b9e\u73b0\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u3002"}}
{"id": "2506.16776", "pdf": "https://arxiv.org/pdf/2506.16776", "abs": "https://arxiv.org/abs/2506.16776", "authors": ["Beomseok Ko", "Hyeryung Jang"], "title": "PQCAD-DM: Progressive Quantization and Calibration-Assisted Distillation for Extremely Efficient Diffusion Model", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 6 figures", "summary": "Diffusion models excel in image generation but are computational and resource-intensive due to their reliance on iterative Markov chain processes, leading to error accumulation and limiting the effectiveness of naive compression techniques. In this paper, we propose PQCAD-DM, a novel hybrid compression framework combining Progressive Quantization (PQ) and Calibration-Assisted Distillation (CAD) to address these challenges. PQ employs a two-stage quantization with adaptive bit-width transitions guided by a momentum-based mechanism, reducing excessive weight perturbations in low-precision. CAD leverages full-precision calibration datasets during distillation, enabling the student to match full-precision performance even with a quantized teacher. As a result, PQCAD-DM achieves a balance between computational efficiency and generative quality, halving inference time while maintaining competitive performance. Extensive experiments validate PQCAD-DM's superior generative capabilities and efficiency across diverse datasets, outperforming fixed-bit quantization methods.", "AI": {"tldr": "PQCAD-DM\u662f\u4e00\u79cd\u7ed3\u5408\u6e10\u8fdb\u91cf\u5316\uff08PQ\uff09\u548c\u6821\u51c6\u8f85\u52a9\u84b8\u998f\uff08CAD\uff09\u7684\u6df7\u5408\u538b\u7f29\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u6269\u6563\u6a21\u578b\u7684\u8ba1\u7b97\u548c\u8d44\u6e90\u5bc6\u96c6\u578b\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u4f9d\u8d56\u8fed\u4ee3\u9a6c\u5c14\u53ef\u592b\u94fe\u8fc7\u7a0b\u5bfc\u81f4\u8ba1\u7b97\u548c\u8d44\u6e90\u5bc6\u96c6\uff0c\u4e14\u8bef\u5dee\u7d2f\u79ef\u9650\u5236\u4e86\u7b80\u5355\u538b\u7f29\u6280\u672f\u7684\u6709\u6548\u6027\u3002", "method": "PQ\u91c7\u7528\u4e24\u9636\u6bb5\u91cf\u5316\uff0c\u901a\u8fc7\u52a8\u91cf\u673a\u5236\u81ea\u9002\u5e94\u8c03\u6574\u4f4d\u5bbd\uff0c\u51cf\u5c11\u4f4e\u7cbe\u5ea6\u4e0b\u7684\u6743\u91cd\u6270\u52a8\uff1bCAD\u5229\u7528\u5168\u7cbe\u5ea6\u6821\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u84b8\u998f\uff0c\u4f7f\u91cf\u5316\u5b66\u751f\u6a21\u578b\u80fd\u5339\u914d\u5168\u7cbe\u5ea6\u6027\u80fd\u3002", "result": "PQCAD-DM\u5728\u8ba1\u7b97\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u534a\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u591a\u6837\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u56fa\u5b9a\u4f4d\u91cf\u5316\u65b9\u6cd5\u3002", "conclusion": "PQCAD-DM\u901a\u8fc7PQ\u548cCAD\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e3a\u538b\u7f29\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.16796", "pdf": "https://arxiv.org/pdf/2506.16796", "abs": "https://arxiv.org/abs/2506.16796", "authors": ["Junbo Qiao", "Miaomiao Cai", "Wei Li", "Yutong Liu", "Xudong Huang", "Gaoqi He", "Jiao Xie", "Jie Hu", "Xinghao Chen", "Shaohui Lin"], "title": "RealSR-R1: Reinforcement Learning for Real-World Image Super-Resolution with Vision-Language Chain-of-Thought", "categories": ["cs.CV"], "comment": null, "summary": "Real-World Image Super-Resolution is one of the most challenging task in image restoration. However, existing methods struggle with an accurate understanding of degraded image content, leading to reconstructed results that are both low-fidelity and unnatural. We present RealSR-R1 in this work, which empowers the RealSR models with understanding and reasoning capabilities. Inspired by the success of Chain of Thought (CoT) in large language models (LLMs), we simulate the human process of handling degraded images and propose the VLCoT framework, which integrates vision and language reasoning. The framework aims to precisely restore image details by progressively generating more comprehensive text and higher-resolution images. To overcome the challenge of traditional supervised learning CoT failing to generalize to real-world scenarios, we introduce, for the first time, Group Relative Policy Optimization (GRPO) into the Real-World Image Super-Resolution task. We propose VLCoT-GRPO as a solution, which designs four reward functions: (1) Format reward, used to standardize the CoT process; (2) Degradation reward, to incentivize accurate degradation estimation; (3) Understanding reward, to ensure the accuracy of the generated content; and (4) Generation reward, where we propose using a visual expert model to evaluate the quality of generated images, encouraging the model to generate more realistic images. Extensive experiments demonstrate that our proposed RealSR-R1 can generate realistic details and accurately understand image content, particularly in semantically rich scenes or images with severe degradation.", "AI": {"tldr": "RealSR-R1\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u4e0e\u8bed\u8a00\u63a8\u7406\u7684VLCoT\u6846\u67b6\uff0c\u5e76\u5f15\u5165GRPO\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7406\u89e3\u9000\u5316\u56fe\u50cf\u5185\u5bb9\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u91cd\u5efa\u7ed3\u679c\u4f4e\u4fdd\u771f\u4e14\u4e0d\u81ea\u7136\u3002", "method": "\u63d0\u51faVLCoT\u6846\u67b6\u6a21\u62df\u4eba\u7c7b\u5904\u7406\u9000\u5316\u56fe\u50cf\u7684\u8fc7\u7a0b\uff0c\u5e76\u9996\u6b21\u5f15\u5165GRPO\u4f18\u5316\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e86\u56db\u79cd\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRealSR-R1\u80fd\u751f\u6210\u66f4\u771f\u5b9e\u7684\u7ec6\u8282\u5e76\u51c6\u786e\u7406\u89e3\u56fe\u50cf\u5185\u5bb9\uff0c\u5c24\u5176\u5728\u8bed\u4e49\u4e30\u5bcc\u6216\u4e25\u91cd\u9000\u5316\u7684\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "VLCoT-GRPO\u6846\u67b6\u4e3a\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.16802", "pdf": "https://arxiv.org/pdf/2506.16802", "abs": "https://arxiv.org/abs/2506.16802", "authors": ["Riccardo Corvi", "Davide Cozzolino", "Ekta Prashnani", "Shalini De Mello", "Koki Nagano", "Luisa Verdoliva"], "title": "Seeing What Matters: Generalizable AI-generated Video Detection with Forensic-Oriented Augmentation", "categories": ["cs.CV"], "comment": null, "summary": "Synthetic video generation is progressing very rapidly. The latest models can produce very realistic high-resolution videos that are virtually indistinguishable from real ones. Although several video forensic detectors have been recently proposed, they often exhibit poor generalization, which limits their applicability in a real-world scenario. Our key insight to overcome this issue is to guide the detector towards seeing what really matters. In fact, a well-designed forensic classifier should focus on identifying intrinsic low-level artifacts introduced by a generative architecture rather than relying on high-level semantic flaws that characterize a specific model. In this work, first, we study different generative architectures, searching and identifying discriminative features that are unbiased, robust to impairments, and shared across models. Then, we introduce a novel forensic-oriented data augmentation strategy based on the wavelet decomposition and replace specific frequency-related bands to drive the model to exploit more relevant forensic cues. Our novel training paradigm improves the generalizability of AI-generated video detectors, without the need for complex algorithms and large datasets that include multiple synthetic generators. To evaluate our approach, we train the detector using data from a single generative model and test it against videos produced by a wide range of other models. Despite its simplicity, our method achieves a significant accuracy improvement over state-of-the-art detectors and obtains excellent results even on very recent generative models, such as NOVA and FLUX. Code and data will be made publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5bfc\u5206\u7c7b\u5668\u5173\u6ce8\u751f\u6210\u6a21\u578b\u5f15\u5165\u7684\u4f4e\u7ea7\u4f2a\u5f71\u800c\u975e\u9ad8\u7ea7\u8bed\u4e49\u7f3a\u9677\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u5668\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u4f2a\u9020\u68c0\u6d4b\u5668\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u7814\u7a76\u4e0d\u540c\u751f\u6210\u67b6\u6784\uff0c\u8bc6\u522b\u5171\u4eab\u7684\u5224\u522b\u7279\u5f81\uff1b\u63d0\u51fa\u57fa\u4e8e\u5c0f\u6ce2\u5206\u89e3\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u5f15\u5bfc\u6a21\u578b\u5229\u7528\u66f4\u76f8\u5173\u7684\u4f2a\u9020\u7ebf\u7d22\u3002", "result": "\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u5668\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u591a\u79cd\u751f\u6210\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u65e0\u9700\u590d\u6742\u7b97\u6cd5\u6216\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u3002"}}
{"id": "2506.16806", "pdf": "https://arxiv.org/pdf/2506.16806", "abs": "https://arxiv.org/abs/2506.16806", "authors": ["Fan Yang", "Yousong Zhu", "Xin Li", "Yufei Zhan", "Hongyin Zhao", "Shurong Zheng", "Yaowei Wang", "Ming Tang", "Jinqiao Wang"], "title": "FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Recent Large Vision Language Models (LVLMs) demonstrate promising capabilities in unifying visual understanding and generative modeling, enabling both accurate content understanding and flexible editing. However, current approaches treat \"what to see\" and \"how to edit\" separately: they either perform isolated object segmentation or utilize segmentation masks merely as conditional prompts for local edit generation tasks, often relying on multiple disjointed models. To bridge these gaps, we introduce FOCUS, a unified LVLM that integrates segmentation-aware perception and controllable object-centric generation within an end-to-end framework. FOCUS employs a dual-branch visual encoder to simultaneously capture global semantic context and fine-grained spatial details. In addition, we leverage a MoVQGAN-based visual tokenizer to produce discrete visual tokens that enhance generation quality. To enable accurate and controllable image editing, we propose a progressive multi-stage training pipeline, where segmentation masks are jointly optimized and used as spatial condition prompts to guide the diffusion decoder. This strategy aligns visual encoding, segmentation, and generation modules, effectively bridging segmentation-aware perception with fine-grained visual synthesis. Extensive experiments across three core tasks, including multimodal understanding, referring segmentation accuracy, and controllable image generation, demonstrate that FOCUS achieves strong performance by jointly optimizing visual perception and generative capabilities.", "AI": {"tldr": "FOCUS\u662f\u4e00\u4e2a\u7edf\u4e00\u7684LVLM\u6a21\u578b\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u6846\u67b6\u6574\u5408\u4e86\u5206\u5272\u611f\u77e5\u548c\u53ef\u63a7\u5bf9\u8c61\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5c06\u89c6\u89c9\u7406\u89e3\u548c\u751f\u6210\u5206\u79bb\uff0c\u4f9d\u8d56\u591a\u4e2a\u72ec\u7acb\u6a21\u578b\uff0cFOCUS\u65e8\u5728\u7edf\u4e00\u8fd9\u4e24\u8005\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u89c6\u89c9\u7f16\u7801\u5668\u3001MoVQGAN\u89c6\u89c9\u5206\u8bcd\u5668\u548c\u591a\u9636\u6bb5\u8bad\u7ec3\u7ba1\u9053\uff0c\u7ed3\u5408\u5206\u5272\u63a9\u7801\u6307\u5bfc\u6269\u6563\u89e3\u7801\u5668\u3002", "result": "\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u89c6\u89c9\u611f\u77e5\u4e0e\u751f\u6210\u80fd\u529b\u7684\u8054\u5408\u4f18\u5316\u6548\u679c\u3002", "conclusion": "FOCUS\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u6709\u6548\u6574\u5408\u4e86\u5206\u5272\u611f\u77e5\u4e0e\u751f\u6210\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.16960", "pdf": "https://arxiv.org/pdf/2506.16960", "abs": "https://arxiv.org/abs/2506.16960", "authors": ["Wenyang Luo", "Haina Qin", "Zewen Chen", "Libin Wang", "Dandan Zheng", "Yuming Li", "Yufan Liu", "Bing Li", "Weiming Hu"], "title": "Visual-Instructed Degradation Diffusion for All-in-One Image Restoration", "categories": ["cs.CV", "68U10", "I.4.4"], "comment": "CVPR2025 Final Version; Corresponding Author: Bing Li", "summary": "Image restoration tasks like deblurring, denoising, and dehazing usually need distinct models for each degradation type, restricting their generalization in real-world scenarios with mixed or unknown degradations. In this work, we propose \\textbf{Defusion}, a novel all-in-one image restoration framework that utilizes visual instruction-guided degradation diffusion. Unlike existing methods that rely on task-specific models or ambiguous text-based priors, Defusion constructs explicit \\textbf{visual instructions} that align with the visual degradation patterns. These instructions are grounded by applying degradations to standardized visual elements, capturing intrinsic degradation features while agnostic to image semantics. Defusion then uses these visual instructions to guide a diffusion-based model that operates directly in the degradation space, where it reconstructs high-quality images by denoising the degradation effects with enhanced stability and generalizability. Comprehensive experiments demonstrate that Defusion outperforms state-of-the-art methods across diverse image restoration tasks, including complex and real-world degradations.", "AI": {"tldr": "Defusion\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u6307\u4ee4\u5f15\u5bfc\u9000\u5316\u6269\u6563\u7684\u5168\u80fd\u56fe\u50cf\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u660e\u786e\u7684\u89c6\u89c9\u6307\u4ee4\u6765\u6307\u5bfc\u6269\u6563\u6a21\u578b\uff0c\u5728\u9000\u5316\u7a7a\u95f4\u4e2d\u76f4\u63a5\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u9488\u5bf9\u6bcf\u79cd\u9000\u5316\u7c7b\u578b\u5355\u72ec\u8bbe\u8ba1\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5176\u5728\u6df7\u5408\u6216\u672a\u77e5\u9000\u5316\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "Defusion\u901a\u8fc7\u6784\u5efa\u89c6\u89c9\u6307\u4ee4\uff08\u57fa\u4e8e\u6807\u51c6\u5316\u89c6\u89c9\u5143\u7d20\u7684\u9000\u5316\uff09\u6765\u6355\u6349\u9000\u5316\u7279\u5f81\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u6307\u4ee4\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u5728\u9000\u5316\u7a7a\u95f4\u4e2d\u76f4\u63a5\u64cd\u4f5c\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u56fe\u50cf\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDefusion\u5728\u591a\u79cd\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u4e2d\uff08\u5305\u62ec\u590d\u6742\u548c\u771f\u5b9e\u4e16\u754c\u7684\u9000\u5316\uff09\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "Defusion\u901a\u8fc7\u89c6\u89c9\u6307\u4ee4\u5f15\u5bfc\u7684\u9000\u5316\u6269\u6563\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684\u56fe\u50cf\u4fee\u590d\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7684\u9000\u5316\u573a\u666f\u3002"}}
{"id": "2506.16961", "pdf": "https://arxiv.org/pdf/2506.16961", "abs": "https://arxiv.org/abs/2506.16961", "authors": ["Haina Qin", "Wenyang Luo", "Libin Wang", "Dandan Zheng", "Jingdong Chen", "Ming Yang", "Bing Li", "Weiming Hu"], "title": "Reversing Flow for Image Restoration", "categories": ["cs.CV", "eess.IV", "68U10", "I.4.4"], "comment": "CVPR2025 Final Version; Corresponding Author: Bing Li", "summary": "Image restoration aims to recover high-quality (HQ) images from degraded low-quality (LQ) ones by reversing the effects of degradation. Existing generative models for image restoration, including diffusion and score-based models, often treat the degradation process as a stochastic transformation, which introduces inefficiency and complexity. In this work, we propose ResFlow, a novel image restoration framework that models the degradation process as a deterministic path using continuous normalizing flows. ResFlow augments the degradation process with an auxiliary process that disambiguates the uncertainty in HQ prediction to enable reversible modeling of the degradation process. ResFlow adopts entropy-preserving flow paths and learns the augmented degradation flow by matching the velocity field. ResFlow significantly improves the performance and speed of image restoration, completing the task in fewer than four sampling steps. Extensive experiments demonstrate that ResFlow achieves state-of-the-art results across various image restoration benchmarks, offering a practical and efficient solution for real-world applications.", "AI": {"tldr": "ResFlow\u662f\u4e00\u79cd\u65b0\u578b\u56fe\u50cf\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u786e\u5b9a\u6027\u8def\u5f84\u5efa\u6a21\u9000\u5316\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u548c\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u5c06\u9000\u5316\u8fc7\u7a0b\u89c6\u4e3a\u968f\u673a\u53d8\u6362\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u590d\u6742\u6027\u589e\u52a0\u3002", "method": "\u4f7f\u7528\u8fde\u7eed\u5f52\u4e00\u5316\u6d41\u5efa\u6a21\u9000\u5316\u8fc7\u7a0b\uff0c\u5f15\u5165\u8f85\u52a9\u8fc7\u7a0b\u6d88\u9664HQ\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u91c7\u7528\u71b5\u4fdd\u6301\u6d41\u8def\u5f84\u5e76\u5339\u914d\u901f\u5ea6\u573a\u3002", "result": "\u5728\u5c11\u4e8e\u56db\u4e2a\u91c7\u6837\u6b65\u9aa4\u5185\u5b8c\u6210\u4efb\u52a1\uff0c\u5e76\u5728\u591a\u4e2a\u56fe\u50cf\u4fee\u590d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "ResFlow\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u5b9e\u7528\u7684\u56fe\u50cf\u4fee\u590d\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.17027", "pdf": "https://arxiv.org/pdf/2506.17027", "abs": "https://arxiv.org/abs/2506.17027", "authors": ["Yiyang Tie", "Hong Zhu", "Yunyun Luo", "Jing Shi"], "title": "Unsupervised Image Super-Resolution Reconstruction Based on Real-World Degradation Patterns", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "The training of real-world super-resolution reconstruction models heavily relies on datasets that reflect real-world degradation patterns. Extracting and modeling degradation patterns for super-resolution reconstruction using only real-world low-resolution (LR) images remains a challenging task. When synthesizing datasets to simulate real-world degradation, relying solely on degradation extraction methods fails to capture both blur and diverse noise characteristics across varying LR distributions, as well as more implicit degradations such as color gamut shifts. Conversely, domain translation alone cannot accurately approximate real-world blur characteristics due to the significant degradation domain gap between synthetic and real data. To address these challenges, we propose a novel TripleGAN framework comprising two strategically designed components: The FirstGAN primarily focuses on narrowing the domain gap in blur characteristics, while the SecondGAN performs domain-specific translation to approximate target-domain blur properties and learn additional degradation patterns. The ThirdGAN is trained on pseudo-real data generated by the FirstGAN and SecondGAN to reconstruct real-world LR images. Extensive experiments on the RealSR and DRealSR datasets demonstrate that our method exhibits clear advantages in quantitative metrics while maintaining sharp reconstructions without over-smoothing artifacts. The proposed framework effectively learns real-world degradation patterns from LR observations and synthesizes aligned datasets with corresponding degradation characteristics, thereby enabling the trained network to achieve superior performance in reconstructing high-quality SR images from real-world LR inputs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdTripleGAN\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2aGAN\u7ec4\u4ef6\u5206\u522b\u5904\u7406\u6a21\u7cca\u7279\u6027\u548c\u5176\u4ed6\u9000\u5316\u6a21\u5f0f\uff0c\u7b2c\u4e09\u4e2aGAN\u7528\u4e8e\u91cd\u5efa\u771f\u5b9e\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4ec5\u4f9d\u8d56\u771f\u5b9e\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u63d0\u53d6\u9000\u5316\u6a21\u5f0f\u65f6\u7684\u6311\u6218\uff0c\u5982\u6a21\u7cca\u548c\u566a\u58f0\u591a\u6837\u6027\uff0c\u4ee5\u53ca\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u95f4\u7684\u9000\u5316\u57df\u5dee\u8ddd\u3002", "method": "TripleGAN\u6846\u67b6\uff1aFirstGAN\u7f29\u5c0f\u6a21\u7cca\u57df\u5dee\u8ddd\uff0cSecondGAN\u5b66\u4e60\u76ee\u6807\u57df\u6a21\u7cca\u7279\u6027\u548c\u5176\u4ed6\u9000\u5316\u6a21\u5f0f\uff0cThirdGAN\u57fa\u4e8e\u4f2a\u771f\u5b9e\u6570\u636e\u91cd\u5efa\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u3002", "result": "\u5728RealSR\u548cDRealSR\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u91cf\u5316\u6307\u6807\u660e\u663e\u4f18\u52bf\uff0c\u91cd\u5efa\u56fe\u50cf\u6e05\u6670\u65e0\u8fc7\u5e73\u6ed1\u4f2a\u5f71\u3002", "conclusion": "TripleGAN\u80fd\u6709\u6548\u5b66\u4e60\u771f\u5b9e\u9000\u5316\u6a21\u5f0f\u5e76\u5408\u6210\u5bf9\u9f50\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2506.17201", "pdf": "https://arxiv.org/pdf/2506.17201", "abs": "https://arxiv.org/abs/2506.17201", "authors": ["Jiaqi Li", "Junshu Tang", "Zhiyong Xu", "Longhuang Wu", "Yuan Zhou", "Shuai Shao", "Tianbao Yu", "Zhiguo Cao", "Qinglin Lu"], "title": "Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition", "categories": ["cs.CV"], "comment": "Project page: https://hunyuan-gamecraft.github.io/", "summary": "Recent advances in diffusion-based and controllable video generation have enabled high-quality and temporally coherent video synthesis, laying the groundwork for immersive interactive gaming experiences. However, current methods face limitations in dynamics, generality, long-term consistency, and efficiency, which limit the ability to create various gameplay videos. To address these gaps, we introduce Hunyuan-GameCraft, a novel framework for high-dynamic interactive video generation in game environments. To achieve fine-grained action control, we unify standard keyboard and mouse inputs into a shared camera representation space, facilitating smooth interpolation between various camera and movement operations. Then we propose a hybrid history-conditioned training strategy that extends video sequences autoregressively while preserving game scene information. Additionally, to enhance inference efficiency and playability, we achieve model distillation to reduce computational overhead while maintaining consistency across long temporal sequences, making it suitable for real-time deployment in complex interactive environments. The model is trained on a large-scale dataset comprising over one million gameplay recordings across over 100 AAA games, ensuring broad coverage and diversity, then fine-tuned on a carefully annotated synthetic dataset to enhance precision and control. The curated game scene data significantly improves the visual fidelity, realism and action controllability. Extensive experiments demonstrate that Hunyuan-GameCraft significantly outperforms existing models, advancing the realism and playability of interactive game video generation.", "AI": {"tldr": "Hunyuan-GameCraft\u662f\u4e00\u4e2a\u7528\u4e8e\u6e38\u620f\u73af\u5883\u4e2d\u9ad8\u52a8\u6001\u4ea4\u4e92\u89c6\u9891\u751f\u6210\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u8f93\u5165\u3001\u6df7\u5408\u5386\u53f2\u6761\u4ef6\u8bad\u7ec3\u548c\u6a21\u578b\u84b8\u998f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u6027\u3001\u901a\u7528\u6027\u3001\u957f\u671f\u4e00\u81f4\u6027\u548c\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u591a\u6837\u5316\u6e38\u620f\u89c6\u9891\u7684\u751f\u6210\u80fd\u529b\u3002", "method": "\u7edf\u4e00\u952e\u76d8\u548c\u9f20\u6807\u8f93\u5165\u5230\u5171\u4eab\u76f8\u673a\u8868\u793a\u7a7a\u95f4\uff0c\u63d0\u51fa\u6df7\u5408\u5386\u53f2\u6761\u4ef6\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u8fdb\u884c\u6a21\u578b\u84b8\u998f\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u5728\u5927\u578b\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u548c\u5fae\u8c03\u540e\uff0cHunyuan-GameCraft\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u3001\u771f\u5b9e\u611f\u548c\u52a8\u4f5c\u53ef\u63a7\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "Hunyuan-GameCraft\u63d0\u5347\u4e86\u4ea4\u4e92\u6e38\u620f\u89c6\u9891\u7684\u771f\u5b9e\u611f\u548c\u53ef\u73a9\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5b9e\u65f6\u73af\u5883\u3002"}}
{"id": "2506.17212", "pdf": "https://arxiv.org/pdf/2506.17212", "abs": "https://arxiv.org/abs/2506.17212", "authors": ["Tianjiao Yu", "Vedant Shah", "Muntasir Wahed", "Ying Shen", "Kiet A. Nguyen", "Ismini Lourentzou"], "title": "Part$^{2}$GS: Part-aware Modeling of Articulated Objects using 3D Gaussian Splatting", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Articulated objects are common in the real world, yet modeling their structure and motion remains a challenging task for 3D reconstruction methods. In this work, we introduce Part$^{2}$GS, a novel framework for modeling articulated digital twins of multi-part objects with high-fidelity geometry and physically consistent articulation. Part$^{2}$GS leverages a part-aware 3D Gaussian representation that encodes articulated components with learnable attributes, enabling structured, disentangled transformations that preserve high-fidelity geometry. To ensure physically consistent motion, we propose a motion-aware canonical representation guided by physics-based constraints, including contact enforcement, velocity consistency, and vector-field alignment. Furthermore, we introduce a field of repel points to prevent part collisions and maintain stable articulation paths, significantly improving motion coherence over baselines. Extensive evaluations on both synthetic and real-world datasets show that Part$^{2}$GS consistently outperforms state-of-the-art methods by up to 10$\\times$ in Chamfer Distance for movable parts.", "AI": {"tldr": "Part$^{2}$GS\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u591a\u90e8\u5206\u7269\u4f53\u7684\u9ad8\u4fdd\u771f\u51e0\u4f55\u548c\u7269\u7406\u4e00\u81f4\u6027\u7684\u6570\u5b57\u5b6a\u751f\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5173\u8282\u7269\u4f53\u5efa\u6a21\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4fdd\u6301\u9ad8\u4fdd\u771f\u51e0\u4f55\u548c\u7269\u7406\u4e00\u81f4\u6027\u3002", "method": "\u91c7\u7528\u90e8\u5206\u611f\u77e5\u76843D\u9ad8\u65af\u8868\u793a\uff0c\u7ed3\u5408\u7269\u7406\u7ea6\u675f\uff08\u5982\u63a5\u89e6\u5f3a\u5236\u3001\u901f\u5ea6\u4e00\u81f4\u6027\u548c\u77e2\u91cf\u573a\u5bf9\u9f50\uff09\u548c\u6392\u65a5\u70b9\u573a\u6765\u9632\u6b62\u78b0\u649e\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cPart$^{2}$GS\u5728\u53ef\u79fb\u52a8\u90e8\u5206\u7684Chamfer\u8ddd\u79bb\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u9ad8\u8fbe10\u500d\u3002", "conclusion": "Part$^{2}$GS\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u548c\u7269\u7406\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5173\u8282\u7269\u4f53\u5efa\u6a21\u7684\u6027\u80fd\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2506.17218", "pdf": "https://arxiv.org/pdf/2506.17218", "abs": "https://arxiv.org/abs/2506.17218", "authors": ["Zeyuan Yang", "Xueyang Yu", "Delin Chen", "Maohao Shen", "Chuang Gan"], "title": "Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://vlm-mirage.github.io/", "summary": "Vision-language models (VLMs) excel at multimodal understanding, yet their text-only decoding forces them to verbalize visual reasoning, limiting performance on tasks that demand visual imagination. Recent attempts train VLMs to render explicit images, but the heavy image-generation pre-training often hinders the reasoning ability. Inspired by the way humans reason with mental imagery-the internal construction and manipulation of visual cues-we investigate whether VLMs can reason through interleaved multimodal trajectories without producing explicit images. To this end, we present a Machine Mental Imagery framework, dubbed as Mirage, which augments VLM decoding with latent visual tokens alongside ordinary text. Concretely, whenever the model chooses to ``think visually'', it recasts its hidden states as next tokens, thereby continuing a multimodal trajectory without generating pixel-level images. Begin by supervising the latent tokens through distillation from ground-truth image embeddings, we then switch to text-only supervision to make the latent trajectory align tightly with the task objective. A subsequent reinforcement learning stage further enhances the multimodal reasoning capability. Experiments on diverse benchmarks demonstrate that Mirage unlocks stronger multimodal reasoning without explicit image generation.", "AI": {"tldr": "Mirage\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u6f5c\u5728\u89c6\u89c9\u6807\u8bb0\uff0c\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u751f\u6210\u663e\u5f0f\u56fe\u50cf\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u89e3\u7801\u65f6\u9700\u5c06\u89c6\u89c9\u63a8\u7406\u8f6c\u5316\u4e3a\u8bed\u8a00\u8868\u8fbe\uff0c\u9650\u5236\u4e86\u9700\u8981\u89c6\u89c9\u60f3\u8c61\u7684\u4efb\u52a1\u6027\u80fd\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u663e\u5f0f\u56fe\u50cf\u89e3\u51b3\uff0c\u4f46\u9884\u8bad\u7ec3\u8d1f\u62c5\u91cd\u4e14\u5f71\u54cd\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faMirage\u6846\u67b6\uff0c\u5728\u89e3\u7801\u65f6\u5f15\u5165\u6f5c\u5728\u89c6\u89c9\u6807\u8bb0\uff0c\u901a\u8fc7\u84b8\u998f\u76d1\u7763\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u591a\u6a21\u6001\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMirage\u5728\u4e0d\u751f\u6210\u663e\u5f0f\u56fe\u50cf\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u4e86\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "Mirage\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u6a21\u6001\u63a8\u7406\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u663e\u5f0f\u56fe\u50cf\u751f\u6210\u7684\u8d1f\u62c5\u3002"}}
{"id": "2506.17220", "pdf": "https://arxiv.org/pdf/2506.17220", "abs": "https://arxiv.org/abs/2506.17220", "authors": ["Jisu Nam", "Soowon Son", "Dahyun Chung", "Jiyoung Kim", "Siyoon Jin", "Junhwa Hur", "Seungryong Kim"], "title": "Emergent Temporal Correspondences from Video Diffusion Transformers", "categories": ["cs.CV"], "comment": "Project page is available at https:/cvlab-kaist.github.io/DiffTrack", "summary": "Recent advancements in video diffusion models based on Diffusion Transformers (DiTs) have achieved remarkable success in generating temporally coherent videos. Yet, a fundamental question persists: how do these models internally establish and represent temporal correspondences across frames? We introduce DiffTrack, the first quantitative analysis framework designed to answer this question. DiffTrack constructs a dataset of prompt-generated video with pseudo ground-truth tracking annotations and proposes novel evaluation metrics to systematically analyze how each component within the full 3D attention mechanism of DiTs (e.g., representations, layers, and timesteps) contributes to establishing temporal correspondences. Our analysis reveals that query-key similarities in specific, but not all, layers play a critical role in temporal matching, and that this matching becomes increasingly prominent during the denoising process. We demonstrate practical applications of DiffTrack in zero-shot point tracking, where it achieves state-of-the-art performance compared to existing vision foundation and self-supervised video models. Further, we extend our findings to motion-enhanced video generation with a novel guidance method that improves temporal consistency of generated videos without additional training. We believe our work offers crucial insights into the inner workings of video DiTs and establishes a foundation for further research and applications leveraging their temporal understanding.", "AI": {"tldr": "DiffTrack\u662f\u4e00\u4e2a\u5b9a\u91cf\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u7814\u7a76\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08DiTs\uff09\u5982\u4f55\u5efa\u7acb\u65f6\u95f4\u5bf9\u5e94\u5173\u7cfb\uff0c\u63ed\u793a\u4e86\u7279\u5b9a\u5c42\u5728\u65f6\u95f4\u5339\u914d\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u5728\u96f6\u6837\u672c\u70b9\u8ddf\u8e2a\u548c\u89c6\u9891\u751f\u6210\u4e2d\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u7814\u7a76\u89c6\u9891\u6269\u6563\u6a21\u578b\u5185\u90e8\u5982\u4f55\u5efa\u7acb\u548c\u8868\u793a\u65f6\u95f4\u5bf9\u5e94\u5173\u7cfb\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u5e26\u6709\u4f2a\u771f\u5b9e\u8ddf\u8e2a\u6ce8\u91ca\u7684\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u65b0\u8bc4\u4f30\u6307\u6807\uff0c\u5206\u67903D\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u5404\u7ec4\u4ef6\u7684\u4f5c\u7528\u3002", "result": "\u7279\u5b9a\u5c42\u7684\u67e5\u8be2-\u952e\u76f8\u4f3c\u6027\u5728\u65f6\u95f4\u5339\u914d\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u4e14\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u9010\u6e10\u663e\u8457\uff1bDiffTrack\u5728\u96f6\u6837\u672c\u70b9\u8ddf\u8e2a\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DiffTrack\u4e3a\u7406\u89e3\u89c6\u9891DiTs\u7684\u5185\u90e8\u673a\u5236\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u548c\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.15748", "pdf": "https://arxiv.org/pdf/2506.15748", "abs": "https://arxiv.org/abs/2506.15748", "authors": ["Zhe Wang", "Yuhua Ru", "Aladine Chetouani", "Tina Shiang", "Fang Chen", "Fabian Bauer", "Liping Zhang", "Didier Hans", "Rachid Jennane", "William Ewing Palmer", "Mohamed Jarraya", "Yung Hsin Chen"], "title": "Diffusion-based Counterfactual Augmentation: Towards Robust and Interpretable Knee Osteoarthritis Grading", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Automated grading of Knee Osteoarthritis (KOA) from radiographs is challenged by significant inter-observer variability and the limited robustness of deep learning models, particularly near critical decision boundaries. To address these limitations, this paper proposes a novel framework, Diffusion-based Counterfactual Augmentation (DCA), which enhances model robustness and interpretability by generating targeted counterfactual examples. The method navigates the latent space of a diffusion model using a Stochastic Differential Equation (SDE), governed by balancing a classifier-informed boundary drive with a manifold constraint. The resulting counterfactuals are then used within a self-corrective learning strategy to improve the classifier by focusing on its specific areas of uncertainty. Extensive experiments on the public Osteoarthritis Initiative (OAI) and Multicenter Osteoarthritis Study (MOST) datasets demonstrate that this approach significantly improves classification accuracy across multiple model architectures. Furthermore, the method provides interpretability by visualizing minimal pathological changes and revealing that the learned latent space topology aligns with clinical knowledge of KOA progression. The DCA framework effectively converts model uncertainty into a robust training signal, offering a promising pathway to developing more accurate and trustworthy automated diagnostic systems. Our code is available at https://github.com/ZWang78/DCA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u53cd\u4e8b\u5b9e\u589e\u5f3a\u6846\u67b6\uff08DCA\uff09\uff0c\u901a\u8fc7\u751f\u6210\u9488\u5bf9\u6027\u53cd\u4e8b\u5b9e\u6837\u672c\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u819d\u9aa8\u5173\u8282\u708e\uff08KOA\uff09\u5206\u7ea7\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u819d\u9aa8\u5173\u8282\u708e\u5206\u7ea7\u4e2d\u89c2\u5bdf\u8005\u95f4\u5dee\u5f02\u5927\u53ca\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5173\u952e\u51b3\u7b56\u8fb9\u754c\u9644\u8fd1\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u901a\u8fc7\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08SDE\uff09\u751f\u6210\u53cd\u4e8b\u5b9e\u6837\u672c\uff0c\u5e76\u7ed3\u5408\u81ea\u6821\u6b63\u5b66\u4e60\u7b56\u7565\u4f18\u5316\u5206\u7c7b\u5668\u3002", "result": "\u5728OAI\u548cMOST\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e0e\u4e34\u5e8a\u77e5\u8bc6\u4e00\u81f4\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "DCA\u6846\u67b6\u901a\u8fc7\u5c06\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u8f6c\u5316\u4e3a\u8bad\u7ec3\u4fe1\u53f7\uff0c\u4e3a\u5f00\u53d1\u66f4\u51c6\u786e\u3001\u53ef\u4fe1\u7684\u81ea\u52a8\u5316\u8bca\u65ad\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2506.16201", "pdf": "https://arxiv.org/pdf/2506.16201", "abs": "https://arxiv.org/abs/2506.16201", "authors": ["Sen Wang", "Le Wang", "Sanping Zhou", "Jingyi Tian", "Jiayi Li", "Haowen Sun", "Wei Tang"], "title": "FlowRAM: Grounding Flow Matching Policy with Region-Aware Mamba Framework for Robotic Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Robotic manipulation in high-precision tasks is essential for numerous industrial and real-world applications where accuracy and speed are required. Yet current diffusion-based policy learning methods generally suffer from low computational efficiency due to the iterative denoising process during inference. Moreover, these methods do not fully explore the potential of generative models for enhancing information exploration in 3D environments. In response, we propose FlowRAM, a novel framework that leverages generative models to achieve region-aware perception, enabling efficient multimodal information processing. Specifically, we devise a Dynamic Radius Schedule, which allows adaptive perception, facilitating transitions from global scene comprehension to fine-grained geometric details. Furthermore, we integrate state space models to integrate multimodal information, while preserving linear computational complexity. In addition, we employ conditional flow matching to learn action poses by regressing deterministic vector fields, simplifying the learning process while maintaining performance. We verify the effectiveness of the FlowRAM in the RLBench, an established manipulation benchmark, and achieve state-of-the-art performance. The results demonstrate that FlowRAM achieves a remarkable improvement, particularly in high-precision tasks, where it outperforms previous methods by 12.0% in average success rate. Additionally, FlowRAM is able to generate physically plausible actions for a variety of real-world tasks in less than 4 time steps, significantly increasing inference speed.", "AI": {"tldr": "FlowRAM\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u9ad8\u6548\u533a\u57df\u611f\u77e5\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u673a\u5668\u4eba\u9ad8\u7cbe\u5ea6\u64cd\u4f5c\u7684\u6027\u80fd\u4e0e\u901f\u5ea6\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u7684\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u8ba1\u7b97\u6548\u7387\u4f4e\uff0c\u4e14\u672a\u5145\u5206\u5229\u7528\u751f\u6210\u6a21\u578b\u57283D\u73af\u5883\u4e2d\u7684\u4fe1\u606f\u63a2\u7d22\u6f5c\u529b\u3002", "method": "FlowRAM\u91c7\u7528\u52a8\u6001\u534a\u5f84\u8c03\u5ea6\u5b9e\u73b0\u81ea\u9002\u5e94\u611f\u77e5\uff0c\u7ed3\u5408\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5904\u7406\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u5e76\u5229\u7528\u6761\u4ef6\u6d41\u5339\u914d\u5b66\u4e60\u52a8\u4f5c\u4f4d\u59ff\u3002", "result": "\u5728RLBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFlowRAM\u7684\u5e73\u5747\u6210\u529f\u7387\u63d0\u534712.0%\uff0c\u4e14\u63a8\u7406\u901f\u5ea6\u663e\u8457\u63d0\u9ad8\uff08\u5c11\u4e8e4\u6b65\uff09\u3002", "conclusion": "FlowRAM\u5728\u9ad8\u7cbe\u5ea6\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.16349", "pdf": "https://arxiv.org/pdf/2506.16349", "abs": "https://arxiv.org/abs/2506.16349", "authors": ["Nikola Jovanovi\u0107", "Ismail Labiad", "Tom\u00e1\u0161 Sou\u010dek", "Martin Vechev", "Pierre Fernandez"], "title": "Watermarking Autoregressive Image Generation", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV"], "comment": "Code: https://github.com/facebookresearch/wmar", "summary": "Watermarking the outputs of generative models has emerged as a promising approach for tracking their provenance. Despite significant interest in autoregressive image generation models and their potential for misuse, no prior work has attempted to watermark their outputs at the token level. In this work, we present the first such approach by adapting language model watermarking techniques to this setting. We identify a key challenge: the lack of reverse cycle-consistency (RCC), wherein re-tokenizing generated image tokens significantly alters the token sequence, effectively erasing the watermark. To address this and to make our method robust to common image transformations, neural compression, and removal attacks, we introduce (i) a custom tokenizer-detokenizer finetuning procedure that improves RCC, and (ii) a complementary watermark synchronization layer. As our experiments demonstrate, our approach enables reliable and robust watermark detection with theoretically grounded p-values.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6a21\u578b\u8f93\u51fa\u7684\u6c34\u5370\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u91cd\u65b0\u6807\u8bb0\u56fe\u50cf\u4ee4\u724c\u65f6\u6c34\u5370\u4e22\u5931\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u8f93\u51fa\u53ef\u80fd\u88ab\u6ee5\u7528\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5728\u4ee4\u724c\u7ea7\u522b\u4e0a\u6c34\u5370\u7684\u65b9\u6cd5\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u53ef\u9760\u7684\u6c34\u5370\u6280\u672f\u6765\u8ffd\u8e2a\u5176\u6765\u6e90\u3002", "method": "\u901a\u8fc7\u8c03\u6574\u8bed\u8a00\u6a21\u578b\u6c34\u5370\u6280\u672f\uff0c\u5f15\u5165\u81ea\u5b9a\u4e49\u7684\u6807\u8bb0\u5668-\u89e3\u6807\u8bb0\u5668\u5fae\u8c03\u7a0b\u5e8f\u4ee5\u63d0\u9ad8\u53cd\u5411\u5faa\u73af\u4e00\u81f4\u6027\uff08RCC\uff09\uff0c\u5e76\u589e\u52a0\u6c34\u5370\u540c\u6b65\u5c42\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u53ef\u9760\u4e14\u9c81\u68d2\u5730\u68c0\u6d4b\u6c34\u5370\uff0c\u5e76\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u7684p\u503c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u8f93\u51fa\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6c34\u5370\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u91cd\u65b0\u6807\u8bb0\u5bfc\u81f4\u7684\u6c34\u5370\u4e22\u5931\u95ee\u9898\u3002"}}
{"id": "2506.16495", "pdf": "https://arxiv.org/pdf/2506.16495", "abs": "https://arxiv.org/abs/2506.16495", "authors": ["Changsheng Gao", "Zijie Liu", "Li Li", "Dong Liu", "Xiaoyan Sun", "Weisi Lin"], "title": "DT-UFC: Universal Large Model Feature Coding via Peaky-to-Balanced Distribution Transformation", "categories": ["cs.MM", "cs.CV"], "comment": null, "summary": "Like image coding in visual data transmission, feature coding is essential for the distributed deployment of large models by significantly reducing transmission and storage overhead. However, prior studies have mostly targeted task- or model-specific scenarios, leaving the challenge of universal feature coding across diverse large models largely unaddressed. In this paper, we present the first systematic study on universal feature coding for large models. The key challenge lies in the inherently diverse and distributionally incompatible nature of features extracted from different models. For example, features from DINOv2 exhibit highly peaky, concentrated distributions, while those from Stable Diffusion 3 (SD3) are more dispersed and uniform. This distributional heterogeneity severely hampers both compression efficiency and cross-model generalization. To address this, we propose a learned peaky-to-balanced distribution transformation, which reshapes highly skewed feature distributions into a common, balanced target space. This transformation is non-uniform, data-driven, and plug-and-play, enabling effective alignment of heterogeneous distributions without modifying downstream codecs. With this alignment, a universal codec trained on the balanced target distribution can effectively generalize to features from different models and tasks. We validate our approach on three representative large models-LLaMA3, DINOv2, and SD3-across multiple tasks and modalities. Extensive experiments show that our method achieves notable improvements in both compression efficiency and cross-model generalization over task-specific baselines. All source code will be released for future research.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u5927\u578b\u6a21\u578b\u7684\u901a\u7528\u7279\u5f81\u7f16\u7801\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e60\u5cf0\u503c\u5230\u5e73\u8861\u5206\u5e03\u53d8\u6362\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u7279\u5f81\u5206\u5e03\u5f02\u6784\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u6548\u7387\u548c\u8de8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u6216\u6a21\u578b\uff0c\u800c\u8de8\u5927\u578b\u6a21\u578b\u7684\u901a\u7528\u7279\u5f81\u7f16\u7801\u95ee\u9898\u5c1a\u672a\u89e3\u51b3\uff0c\u7279\u5f81\u5206\u5e03\u5f02\u6784\u6027\u4e25\u91cd\u5f71\u54cd\u4e86\u538b\u7f29\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u5747\u5300\u3001\u6570\u636e\u9a71\u52a8\u7684\u5cf0\u503c\u5230\u5e73\u8861\u5206\u5e03\u53d8\u6362\u65b9\u6cd5\uff0c\u5c06\u5f02\u6784\u7279\u5f81\u5206\u5e03\u5bf9\u9f50\u5230\u5171\u540c\u7684\u76ee\u6807\u7a7a\u95f4\uff0c\u65e0\u9700\u4fee\u6539\u4e0b\u6e38\u7f16\u89e3\u7801\u5668\u3002", "result": "\u5728LLaMA3\u3001DINOv2\u548cSD3\u7b49\u591a\u4e2a\u6a21\u578b\u548c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u538b\u7f29\u6548\u7387\u548c\u8de8\u6a21\u578b\u6cdb\u5316\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4efb\u52a1\u7279\u5b9a\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u578b\u6a21\u578b\u7684\u901a\u7528\u7279\u5f81\u7f16\u7801\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u5c06\u5f00\u6e90\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2506.16572", "pdf": "https://arxiv.org/pdf/2506.16572", "abs": "https://arxiv.org/abs/2506.16572", "authors": ["Chanung Park", "Joo Chan Lee", "Jong Hwan Ko"], "title": "DiffO: Single-step Diffusion for Image Compression at Ultra-Low Bitrates", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Although image compression is fundamental to visual data processing and has inspired numerous standard and learned codecs, these methods still suffer severe quality degradation at extremely low bits per pixel. While recent diffusion based models provided enhanced generative performance at low bitrates, they still yields limited perceptual quality and prohibitive decoding latency due to multiple denoising steps. In this paper, we propose the first single step diffusion model for image compression (DiffO) that delivers high perceptual quality and fast decoding at ultra low bitrates. DiffO achieves these goals by coupling two key innovations: (i) VQ Residual training, which factorizes a structural base code and a learned residual in latent space, capturing both global geometry and high frequency details; and (ii) rate adaptive noise modulation, which tunes denoising strength on the fly to match the desired bitrate. Extensive experiments show that DiffO surpasses state of the art compression performance while improving decoding speed by about 50x compared to prior diffusion-based methods, greatly improving the practicality of generative codecs. The code will be available at https://github.com/Freemasti/DiffO.", "AI": {"tldr": "DiffO\u662f\u4e00\u79cd\u5355\u6b65\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u56fe\u50cf\u538b\u7f29\uff0c\u5728\u6781\u4f4e\u6bd4\u7279\u7387\u4e0b\u63d0\u4f9b\u9ad8\u611f\u77e5\u8d28\u91cf\u548c\u5feb\u901f\u89e3\u7801\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\u5728\u6781\u4f4e\u6bd4\u7279\u7387\u4e0b\u8d28\u91cf\u4e0b\u964d\u4e25\u91cd\uff0c\u800c\u57fa\u4e8e\u6269\u6563\u7684\u6a21\u578b\u89e3\u7801\u5ef6\u8fdf\u9ad8\u4e14\u611f\u77e5\u8d28\u91cf\u6709\u9650\u3002", "method": "DiffO\u7ed3\u5408\u4e86VQ\u6b8b\u5dee\u8bad\u7ec3\u548c\u901f\u7387\u81ea\u9002\u5e94\u566a\u58f0\u8c03\u5236\uff0c\u5206\u522b\u6355\u6349\u5168\u5c40\u51e0\u4f55\u4e0e\u9ad8\u9891\u7ec6\u8282\uff0c\u5e76\u52a8\u6001\u8c03\u6574\u53bb\u566a\u5f3a\u5ea6\u4ee5\u5339\u914d\u76ee\u6807\u6bd4\u7279\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDiffO\u5728\u538b\u7f29\u6027\u80fd\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u89e3\u7801\u901f\u5ea6\u63d0\u5347\u7ea650\u500d\u3002", "conclusion": "DiffO\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u5f0f\u7f16\u89e3\u7801\u5668\u7684\u5b9e\u7528\u6027\uff0c\u9002\u7528\u4e8e\u6781\u4f4e\u6bd4\u7279\u7387\u573a\u666f\u3002"}}
{"id": "2506.16733", "pdf": "https://arxiv.org/pdf/2506.16733", "abs": "https://arxiv.org/abs/2506.16733", "authors": ["Fang Chen", "Weifeng Zhang", "Xingyu Ai", "BingXuan Li", "An Li", "Qiegen Liu"], "title": "A Prior-Guided Joint Diffusion Model in Projection Domain for PET Tracer Conversion", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Positron emission tomography (PET) is widely used to assess metabolic activity, but its application is limited by the availability of radiotracers. 18F-labeled fluorodeoxyglucose (18F-FDG) is the most commonly used tracer but shows limited effectiveness for certain tumors. In contrast, 6-18F-fluoro-3,4-dihydroxy-L-phenylalanine (18F-DOPA) offers higher specificity for neuroendocrine tumors and neurological disorders. However, its complex synthesis and limitations in transportation and clinical use hinder widespread adoption. During PET imaging, the sinogram represents a form of raw data acquired by the scanner. Therefore, modeling in projection domain enables more direct utilization of the original information, potentially reducing the accumulation of errors introduced during the image reconstruction process. Inspired by these factors, this study proposes a prior-guided joint diffusion model (PJDM) for transforming 18F-FDG PET images into 18F-DOPA PET images in projection domain. Specifically, a coarse estimation model and a prior refinement model are trained independently. During inference, an initial synthetic 18F-DOPA PET sinogram is generated using a higher-order hybrid sampler. This sinogram is then degraded and serves as an additional condition to guide the iterative refinement process using learned prior. Experimental results demonstrated that PJDM effectively improved both sinogram quality and synthetic outcomes. The code is available at: https://github.com/yqx7150/PJDM.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5148\u9a8c\u5f15\u5bfc\u7684\u8054\u5408\u6269\u6563\u6a21\u578b\uff08PJDM\uff09\uff0c\u7528\u4e8e\u5728\u6295\u5f71\u57df\u4e2d\u5c0618F-FDG PET\u56fe\u50cf\u8f6c\u6362\u4e3a18F-DOPA PET\u56fe\u50cf\uff0c\u4ee5\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\u548c\u5408\u6210\u6548\u679c\u3002", "motivation": "18F-FDG PET\u5728\u7279\u5b9a\u80bf\u7624\u4e2d\u7684\u6548\u679c\u6709\u9650\uff0c\u800c18F-DOPA\u867d\u7136\u7279\u5f02\u6027\u66f4\u9ad8\uff0c\u4f46\u5408\u6210\u590d\u6742\u4e14\u4e34\u5e8a\u5e94\u7528\u53d7\u9650\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6295\u5f71\u57df\u5efa\u6a21\u76f4\u63a5\u5229\u7528\u539f\u59cb\u4fe1\u606f\uff0c\u51cf\u5c11\u91cd\u5efa\u8fc7\u7a0b\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u3002", "method": "\u7814\u7a76\u91c7\u7528\u5148\u9a8c\u5f15\u5bfc\u7684\u8054\u5408\u6269\u6563\u6a21\u578b\uff08PJDM\uff09\uff0c\u5305\u62ec\u7c97\u4f30\u8ba1\u6a21\u578b\u548c\u5148\u9a8c\u7ec6\u5316\u6a21\u578b\u3002\u901a\u8fc7\u9ad8\u9636\u6df7\u5408\u91c7\u6837\u5668\u751f\u6210\u521d\u59cb\u5408\u6210\u56fe\u50cf\uff0c\u5e76\u5229\u7528\u5b66\u4e60\u5230\u7684\u5148\u9a8c\u8fdb\u884c\u8fed\u4ee3\u7ec6\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPJDM\u663e\u8457\u63d0\u9ad8\u4e86\u6295\u5f71\u57df\u56fe\u50cf\u8d28\u91cf\u548c\u5408\u6210\u6548\u679c\u3002", "conclusion": "PJDM\u4e3a18F-DOPA PET\u56fe\u50cf\u7684\u5408\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2506.16803", "pdf": "https://arxiv.org/pdf/2506.16803", "abs": "https://arxiv.org/abs/2506.16803", "authors": ["Ning Chu", "Siya Zheng", "Shanqing Zhang", "Li Li", "Caifang Cai", "Ali Mohammad-Djafari", "Feng Zhao", "Yuanbo Song"], "title": "Temperature calibration of surface emissivities with an improved thermal image enhancement network", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Infrared thermography faces persistent challenges in temperature accuracy due to material emissivity variations, where existing methods often neglect the joint optimization of radiometric calibration and image degradation. This study introduces a physically guided neural framework that unifies temperature correction and image enhancement through a symmetric skip-CNN architecture and an emissivity-aware attention module. The pre-processing stage segments the ROIs of the image and and initially corrected the firing rate. A novel dual-constrained loss function strengthens the statistical consistency between the target and reference regions through mean-variance alignment and histogram matching based on Kullback-Leibler dispersion. The method works by dynamically fusing thermal radiation features and spatial context, and the model suppresses emissivity artifacts while recovering structural details. After validating the industrial blower system under different conditions, the improved network realizes the dynamic fusion of thermal radiation characteristics and spatial background, with accurate calibration results in various industrial conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u5f15\u5bfc\u7684\u795e\u7ecf\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u79f0\u8df3\u8dc3-CNN\u67b6\u6784\u548c\u53d1\u5c04\u7387\u611f\u77e5\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u7edf\u4e00\u6e29\u5ea6\u6821\u6b63\u548c\u56fe\u50cf\u589e\u5f3a\uff0c\u89e3\u51b3\u4e86\u7ea2\u5916\u70ed\u6210\u50cf\u4e2d\u56e0\u6750\u6599\u53d1\u5c04\u7387\u53d8\u5316\u5bfc\u81f4\u7684\u6e29\u5ea6\u7cbe\u5ea6\u95ee\u9898\u3002", "motivation": "\u7ea2\u5916\u70ed\u6210\u50cf\u56e0\u6750\u6599\u53d1\u5c04\u7387\u53d8\u5316\u5bfc\u81f4\u6e29\u5ea6\u7cbe\u5ea6\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5e38\u5ffd\u7565\u8f90\u5c04\u6821\u51c6\u548c\u56fe\u50cf\u9000\u5316\u7684\u8054\u5408\u4f18\u5316\u3002", "method": "\u91c7\u7528\u5bf9\u79f0\u8df3\u8dc3-CNN\u67b6\u6784\u548c\u53d1\u5c04\u7387\u611f\u77e5\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u901a\u8fc7\u53cc\u7ea6\u675f\u635f\u5931\u51fd\u6570\uff08\u5747\u503c-\u65b9\u5dee\u5bf9\u9f50\u548c\u57fa\u4e8eKL\u6563\u5ea6\u7684\u76f4\u65b9\u56fe\u5339\u914d\uff09\u4f18\u5316\u6e29\u5ea6\u6821\u6b63\u548c\u56fe\u50cf\u589e\u5f3a\u3002", "result": "\u5728\u5de5\u4e1a\u9f13\u98ce\u673a\u7cfb\u7edf\u9a8c\u8bc1\u4e2d\uff0c\u6a21\u578b\u5b9e\u73b0\u4e86\u70ed\u8f90\u5c04\u7279\u5f81\u4e0e\u7a7a\u95f4\u80cc\u666f\u7684\u52a8\u6001\u878d\u5408\uff0c\u5e76\u5728\u591a\u79cd\u5de5\u4e1a\u6761\u4ef6\u4e0b\u83b7\u5f97\u51c6\u786e\u6821\u51c6\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u6291\u5236\u53d1\u5c04\u7387\u4f2a\u5f71\u5e76\u6062\u590d\u7ed3\u6784\u7ec6\u8282\uff0c\u4e3a\u7ea2\u5916\u70ed\u6210\u50cf\u7684\u6e29\u5ea6\u7cbe\u5ea6\u95ee\u9898\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.16898", "pdf": "https://arxiv.org/pdf/2506.16898", "abs": "https://arxiv.org/abs/2506.16898", "authors": ["Ciro Beneduce", "Massimiliano Luca", "Bruno Lepri"], "title": "AI's Blind Spots: Geographic Knowledge and Diversity Deficit in Generated Urban Scenario", "categories": ["cs.AI", "cs.CV", "cs.CY"], "comment": null, "summary": "Image generation models are revolutionizing many domains, and urban analysis and design is no exception. While such models are widely adopted, there is a limited literature exploring their geographic knowledge, along with the biases they embed. In this work, we generated 150 synthetic images for each state in the USA and related capitals using FLUX 1 and Stable Diffusion 3.5, two state-of-the-art models for image generation. We embed each image using DINO-v2 ViT-S/14 and the Fr\\'echet Inception Distances to measure the similarity between the generated images. We found that while these models have implicitly learned aspects of USA geography, if we prompt the models to generate an image for \"United States\" instead of specific cities or states, the models exhibit a strong representative bias toward metropolis-like areas, excluding rural states and smaller cities. {\\color{black} In addition, we found that models systematically exhibit some entity-disambiguation issues with European-sounding names like Frankfort or Devon.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.17140", "pdf": "https://arxiv.org/pdf/2506.17140", "abs": "https://arxiv.org/abs/2506.17140", "authors": ["David Jacob Drexlin", "Jonas Dippel", "Julius Hense", "Niklas Preni\u00dfl", "Gr\u00e9goire Montavon", "Frederick Klauschen", "Klaus-Robert M\u00fcller"], "title": "MeDi: Metadata-Guided Diffusion Models for Mitigating Biases in Tumor Classification", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Deep learning models have made significant advances in histological prediction tasks in recent years. However, for adaptation in clinical practice, their lack of robustness to varying conditions such as staining, scanner, hospital, and demographics is still a limiting factor: if trained on overrepresented subpopulations, models regularly struggle with less frequent patterns, leading to shortcut learning and biased predictions. Large-scale foundation models have not fully eliminated this issue. Therefore, we propose a novel approach explicitly modeling such metadata into a Metadata-guided generative Diffusion model framework (MeDi). MeDi allows for a targeted augmentation of underrepresented subpopulations with synthetic data, which balances limited training data and mitigates biases in downstream models. We experimentally show that MeDi generates high-quality histopathology images for unseen subpopulations in TCGA, boosts the overall fidelity of the generated images, and enables improvements in performance for downstream classifiers on datasets with subpopulation shifts. Our work is a proof-of-concept towards better mitigating data biases with generative models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMeDi\u7684\u751f\u6210\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5143\u6570\u636e\u6765\u589e\u5f3a\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u5b50\u7fa4\u4f53\u6570\u636e\uff0c\u4ee5\u51cf\u5c11\u4e0b\u6e38\u6a21\u578b\u7684\u504f\u89c1\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u7ec4\u7ec7\u5b66\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u56e0\u5bf9\u67d3\u8272\u3001\u626b\u63cf\u4eea\u3001\u533b\u9662\u548c\u4eba\u53e3\u7edf\u8ba1\u7b49\u6761\u4ef6\u7f3a\u4e4f\u9c81\u68d2\u6027\u800c\u53d7\u9650\uff0c\u5bfc\u81f4\u5b66\u4e60\u6377\u5f84\u548c\u504f\u89c1\u9884\u6d4b\u3002", "method": "\u63d0\u51faMetadata-guided generative Diffusion model\uff08MeDi\uff09\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u6570\u636e\u5e73\u8861\u8bad\u7ec3\u6570\u636e\uff0c\u51cf\u5c11\u504f\u89c1\u3002", "result": "MeDi\u5728TCGA\u4e2d\u4e3a\u672a\u89c1\u5b50\u7fa4\u4f53\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u63d0\u5347\u751f\u6210\u56fe\u50cf\u4fdd\u771f\u5ea6\uff0c\u5e76\u6539\u5584\u4e0b\u6e38\u5206\u7c7b\u5668\u5728\u5b50\u7fa4\u4f53\u504f\u79fb\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "MeDi\u662f\u51cf\u5c11\u6570\u636e\u504f\u89c1\u7684\u751f\u6210\u6a21\u578b\u6982\u5ff5\u9a8c\u8bc1\uff0c\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.17165", "pdf": "https://arxiv.org/pdf/2506.17165", "abs": "https://arxiv.org/abs/2506.17165", "authors": ["Mahin Montasir Afif", "Abdullah Al Noman", "K. M. Tahsin Kabir", "Md. Mortuza Ahmmed", "Md. Mostafizur Rahman", "Mufti Mahmud", "Md. Ashraful Babu"], "title": "Proportional Sensitivity in Generative Adversarial Network (GAN)-Augmented Brain Tumor Classification Using Convolutional Neural Network", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "This papaer has been submitted to The 18th International Conference   on Brain Informatics (BI'25), Italy", "summary": "Generative Adversarial Networks (GAN) have shown potential in expanding limited medical imaging datasets. This study explores how different ratios of GAN-generated and real brain tumor MRI images impact the performance of a CNN in classifying healthy vs. tumorous scans. A DCGAN was used to create synthetic images which were mixed with real ones at various ratios to train a custom CNN. The CNN was then evaluated on a separate real-world test set. Our results indicate that the model maintains high sensitivity and precision in tumor classification, even when trained predominantly on synthetic data. When only a small portion of GAN data was added, such as 900 real images and 100 GAN images, the model achieved excellent performance, with test accuracy reaching 95.2%, and precision, recall, and F1-score all exceeding 95%. However, as the proportion of GAN images increased further, performance gradually declined. This study suggests that while GANs are useful for augmenting limited datasets especially when real data is scarce, too much synthetic data can introduce artifacts that affect the model's ability to generalize to real world cases.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86GAN\u751f\u6210\u7684\u8111\u80bf\u7624MRI\u56fe\u50cf\u4e0e\u771f\u5b9e\u56fe\u50cf\u4e0d\u540c\u6bd4\u4f8b\u5bf9CNN\u5206\u7c7b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5c11\u91cfGAN\u6570\u636e\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u8fc7\u591a\u4f1a\u964d\u4f4e\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u5f71\u50cf\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63a2\u7d22GAN\u751f\u6210\u6570\u636e\u5728\u589e\u5f3a\u6570\u636e\u96c6\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u4f7f\u7528DCGAN\u751f\u6210\u5408\u6210\u56fe\u50cf\uff0c\u4e0e\u771f\u5b9e\u56fe\u50cf\u6309\u4e0d\u540c\u6bd4\u4f8b\u6df7\u5408\u8bad\u7ec3CNN\uff0c\u5e76\u5728\u771f\u5b9e\u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u5c11\u91cfGAN\u6570\u636e\uff08\u5982100\u5f20\uff09\u4e0e900\u5f20\u771f\u5b9e\u6570\u636e\u6df7\u5408\u65f6\uff0c\u6a21\u578b\u51c6\u786e\u7387\u8fbe95.2%\uff0c\u4f46GAN\u6bd4\u4f8b\u8fc7\u9ad8\u65f6\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "GAN\u751f\u6210\u6570\u636e\u5728\u6709\u9650\u771f\u5b9e\u6570\u636e\u65f6\u6709\u7528\uff0c\u4f46\u8fc7\u591a\u4f1a\u5f15\u5165\u4f2a\u5f71\u5f71\u54cd\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002"}}
