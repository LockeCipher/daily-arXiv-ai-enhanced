<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 25]
- [cs.CL](#cs.CL) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Hyperparameters are all you need: Using five-step inference for an original diffusion model to generate images comparable to the latest distillation model](https://arxiv.org/abs/2510.02390)
*Zilai Li*

Main category: cs.GR

TL;DR: 提出了一种无需额外训练的高效扩散模型采样算法，能在8步内生成高质量512×512和1024×1024图像，性能优于现有ODE求解器。


<details>
  <summary>Details</summary>
Motivation: 基于扩散ODE和SDE的截断误差分析，旨在开发无需额外训练的高效采样算法，实现快速高质量图像生成。

Method: 通过分析扩散ODE和SDE的截断误差，设计训练自由的采样算法，支持灵活引导尺度。

Result: 在COCO和LAION数据集上验证，8步生成512×512图像的FID性能（15.7, 22.35, 17.52）优于DPM++ 2m（20步）和AMED-plugin；1024×1024图像生成在8步内达到与最新蒸馏模型相当的FID性能。

Conclusion: 该算法在无需额外训练的情况下，实现了高效的少步数图像生成，性能优于现有ODE求解器，且与蒸馏模型相当。

Abstract: The diffusion model is a state-of-the-art generative model that generates an image by applying a neural network iteratively. Moreover, this generation process is regarded as an algorithm solving an ordinary differential equation or a stochastic differential equation. Based on the analysis of the truncation error of the diffusion ODE and SDE, our study proposes a training-free algorithm that generates high-quality 512 x 512 and 1024 x 1024 images in eight steps, with flexible guidance scales. To the best of my knowledge, our algorithm is the first one that samples a 1024 x 1024 resolution image in 8 steps with an FID performance comparable to that of the latest distillation model, but without additional training. Meanwhile, our algorithm can also generate a 512 x 512 image in 8 steps, and its FID performance is better than the inference result using state-of-the-art ODE solver DPM++ 2m in 20 steps. We validate our eight-step image generation algorithm using the COCO 2014, COCO 2017, and LAION datasets. And our best FID performance is 15.7, 22.35, and 17.52. While the FID performance of DPM++2m is 17.3, 23.75, and 17.33. Further, it also outperforms the state-of-the-art AMED-plugin solver, whose FID performance is 19.07, 25.50, and 18.06. We also apply the algorithm in five-step inference without additional training, for which the best FID performance in the datasets mentioned above is 19.18, 23.24, and 19.61, respectively, and is comparable to the performance of the state-of-the-art AMED Pulgin solver in eight steps, SDXL-turbo in four steps, and the state-of-the-art diffusion distillation model Flash Diffusion in five steps. We also validate our algorithm in synthesizing 1024 * 1024 images within 6 steps, whose FID performance only has a limited distance to the latest distillation algorithm. The code is in repo: https://github.com/TheLovesOfLadyPurple/Hyperparameters-are-all-you-need

</details>


### [2] [GS-Share: Enabling High-fidelity Map Sharing with Incremental Gaussian Splatting](https://arxiv.org/abs/2510.02884)
*Xinran Zhang,Hanqi Zhu,Yifan Duan,Yanyong Zhang*

Main category: cs.GR

TL;DR: GS-Share是一个基于3D高斯泼溅的高保真地图共享系统，通过紧凑表示实现连续更新和网络效率，在保真度和传输效率方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D地图共享系统难以同时实现高保真度、连续更新和网络效率，特别是在自动驾驶和增强现实等应用中需要高效共享和更新3D地图。

Method: 采用锚点基础的全局地图构建、基于虚拟图像的地图增强和增量地图更新等核心技术，构建紧凑的3D高斯泼溅表示。

Result: 在PSNR、LPIPS和Depth L1指标上分别提升11%、22%和74%，地图传输开销减少36%，特别在外推视图方面表现优异。

Conclusion: GS-Share成功解决了3D地图共享中的关键挑战，实现了高保真、紧凑且支持连续更新的实用地图共享系统。

Abstract: Constructing and sharing 3D maps is essential for many applications, including autonomous driving and augmented reality. Recently, 3D Gaussian splatting has emerged as a promising approach for accurate 3D reconstruction. However, a practical map-sharing system that features high-fidelity, continuous updates, and network efficiency remains elusive. To address these challenges, we introduce GS-Share, a photorealistic map-sharing system with a compact representation. The core of GS-Share includes anchor-based global map construction, virtual-image-based map enhancement, and incremental map update. We evaluate GS-Share against state-of-the-art methods, demonstrating that our system achieves higher fidelity, particularly for extrapolated views, with improvements of 11%, 22%, and 74% in PSNR, LPIPS, and Depth L1, respectively. Furthermore, GS-Share is significantly more compact, reducing map transmission overhead by 36%.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [3] [FSFSplatter: Build Surface and Novel Views with Sparse-Views within 3min](https://arxiv.org/abs/2510.02691)
*Yibin Zhao,Yihan Pan,Jun Nan,Jianjun Yi*

Main category: cs.CV

TL;DR: FSFSplatter是一种从自由稀疏图像进行快速表面重建的新方法，通过端到端密集高斯初始化、相机参数估计和几何增强场景优化来解决传统高斯溅射方法需要密集校准视图的问题。


<details>
  <summary>Details</summary>
Motivation: 现有高斯溅射方法大多需要密集校准视图，从自由稀疏图像重建时由于重叠有限和过拟合会导致表面质量差。

Method: 使用大型Transformer编码多视图图像，通过自分裂高斯头生成密集几何一致的高斯场景初始化，采用贡献度剪枝消除局部漂浮物，利用深度和多视图特征监督结合可微分相机参数在快速优化中缓解过拟合。

Result: FSFSplatter在广泛使用的DTU和Replica数据集上优于当前最先进方法。

Conclusion: 该方法为从自由稀疏图像进行高质量表面重建提供了有效的解决方案。

Abstract: Gaussian Splatting has become a leading reconstruction technique, known for its high-quality novel view synthesis and detailed reconstruction. However, most existing methods require dense, calibrated views. Reconstructing from free sparse images often leads to poor surface due to limited overlap and overfitting. We introduce FSFSplatter, a new approach for fast surface reconstruction from free sparse images. Our method integrates end-to-end dense Gaussian initialization, camera parameter estimation, and geometry-enhanced scene optimization. Specifically, FSFSplatter employs a large Transformer to encode multi-view images and generates a dense and geometrically consistent Gaussian scene initialization via a self-splitting Gaussian head. It eliminates local floaters through contribution-based pruning and mitigates overfitting to limited views by leveraging depth and multi-view feature supervision with differentiable camera parameters during rapid optimization. FSFSplatter outperforms current state-of-the-art methods on widely used DTU and Replica.

</details>


### [4] [ROGR: Relightable 3D Objects using Generative Relighting](https://arxiv.org/abs/2510.03163)
*Jiapeng Tang,Matthew Lavine,Dor Verbin,Stephan J. Garbin,Matthias Nießner,Ricardo Martin Brualla,Pratul P. Srinivasan,Philipp Henzler*

Main category: cs.CV

TL;DR: ROGR是一种新颖的可重光照3D重建方法，使用生成式重光照模型从多视角图像重建物体模型，能够在任意环境光照下进行高效前向重光照。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在任意环境光照下进行重光照时需要每次优化或光线传输模拟的问题，实现高效的前向重光照。

Method: 使用生成式重光照模型采样物体在多种光照环境下的外观，训练光照条件神经辐射场（NeRF），采用双分支架构分别编码一般光照效果和镜面反射。

Result: 在TensoIR和Stanford-ORB数据集上评估，在大多数指标上优于现有最先进方法，并在真实物体捕捉上展示了良好效果。

Conclusion: ROGR方法能够高效重建可重光照的3D模型，在任意环境光照下实现前向重光照，无需每次优化或光线传输模拟。

Abstract: We introduce ROGR, a novel approach that reconstructs a relightable 3D model of an object captured from multiple views, driven by a generative relighting model that simulates the effects of placing the object under novel environment illuminations. Our method samples the appearance of the object under multiple lighting environments, creating a dataset that is used to train a lighting-conditioned Neural Radiance Field (NeRF) that outputs the object's appearance under any input environmental lighting. The lighting-conditioned NeRF uses a novel dual-branch architecture to encode the general lighting effects and specularities separately. The optimized lighting-conditioned NeRF enables efficient feed-forward relighting under arbitrary environment maps without requiring per-illumination optimization or light transport simulation. We evaluate our approach on the established TensoIR and Stanford-ORB datasets, where it improves upon the state-of-the-art on most metrics, and showcase our approach on real-world object captures.

</details>


### [5] [How Confident are Video Models? Empowering Video Models to Express their Uncertainty](https://arxiv.org/abs/2510.02571)
*Zhiting Mei,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.CV

TL;DR: 提出了首个用于生成视频模型不确定性量化的框架S-QUBED，包含评估指标、黑盒UQ方法和基准数据集，能分解预测不确定性为偶然性和认知性成分。


<details>
  <summary>Details</summary>
Motivation: 生成视频模型存在幻觉问题，会产生看似合理但事实错误的视频，而现有研究缺乏视频模型的不确定性量化方法，存在安全隐患。

Method: 通过潜在空间建模，将预测不确定性分解为偶然性（任务模糊性）和认知性（知识缺乏）成分，提出基于鲁棒秩相关估计的校准评估指标和黑盒UQ方法S-QUBED。

Result: 在基准视频数据集上的实验表明，S-QUBED能计算与任务准确度负相关的校准总不确定性估计，并有效分解不确定性的两个成分。

Conclusion: 这是首个针对生成视频模型的不确定性量化工作，提出的框架能可靠地评估和分解视频生成过程中的不确定性，为视频模型的安全应用提供重要保障。

Abstract: Generative video models demonstrate impressive text-to-video capabilities, spurring widespread adoption in many real-world applications. However, like large language models (LLMs), video generation models tend to hallucinate, producing plausible videos even when they are factually wrong. Although uncertainty quantification (UQ) of LLMs has been extensively studied in prior work, no UQ method for video models exists, raising critical safety concerns. To our knowledge, this paper represents the first work towards quantifying the uncertainty of video models. We present a framework for uncertainty quantification of generative video models, consisting of: (i) a metric for evaluating the calibration of video models based on robust rank correlation estimation with no stringent modeling assumptions; (ii) a black-box UQ method for video models (termed S-QUBED), which leverages latent modeling to rigorously decompose predictive uncertainty into its aleatoric and epistemic components; and (iii) a UQ dataset to facilitate benchmarking calibration in video models. By conditioning the generation task in the latent space, we disentangle uncertainty arising due to vague task specifications from that arising from lack of knowledge. Through extensive experiments on benchmark video datasets, we demonstrate that S-QUBED computes calibrated total uncertainty estimates that are negatively correlated with the task accuracy and effectively computes the aleatoric and epistemic constituents.

</details>


### [6] [PEO: Training-Free Aesthetic Quality Enhancement in Pre-Trained Text-to-Image Diffusion Models with Prompt Embedding Optimization](https://arxiv.org/abs/2510.02599)
*Hovhannes Margaryan,Bo Wan,Tinne Tuytelaars*

Main category: cs.CV

TL;DR: 提出Prompt Embedding Optimization (PEO)方法，通过优化文本嵌入来提升预训练文本到图像扩散模型的生成质量，无需训练且与主干模型无关。


<details>
  <summary>Details</summary>
Motivation: 解决预训练文本到图像扩散模型在简单提示词下生成图像美学质量不足的问题。

Method: 使用三重目标函数优化文本嵌入：提升生成图像的美学保真度、确保与优化文本嵌入的一致性、通过提示词保留项最小化与初始提示词的差异。

Result: 定量和定性评估表明该方法有效，性能达到或超过最先进的文本到图像和提示词适应方法。

Conclusion: PEO是一种无需训练、主干无关的方法，能有效提升简单提示词下生成图像的美学质量。

Abstract: This paper introduces a novel approach to aesthetic quality improvement in pre-trained text-to-image diffusion models when given a simple prompt. Our method, dubbed Prompt Embedding Optimization (PEO), leverages a pre-trained text-to-image diffusion model as a backbone and optimizes the text embedding of a given simple and uncurated prompt to enhance the visual quality of the generated image. We achieve this by a tripartite objective function that improves the aesthetic fidelity of the generated image, ensures adherence to the optimized text embedding, and minimal divergence from the initial prompt. The latter is accomplished through a prompt preservation term. Additionally, PEO is training-free and backbone-independent. Quantitative and qualitative evaluations confirm the effectiveness of the proposed method, exceeding or equating the performance of state-of-the-art text-to-image and prompt adaptation methods.

</details>


### [7] [Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation](https://arxiv.org/abs/2510.02617)
*Beijia Lu,Ziyi Chen,Jing Xiao,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: 提出了一种基于输入感知稀疏注意力和蒸馏损失的实时语音驱动视频生成方法，通过将多步扩散模型蒸馏为少步学生模型，实现了实时性能并提升了视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的语音驱动视频生成方法由于需要大量去噪步骤和昂贵的注意力机制，速度缓慢，无法实现实时部署。

Method: 1) 使用输入人体姿态关键点的准确对应关系来引导注意力到相关区域；2) 提出输入感知稀疏注意力减少冗余计算；3) 引入输入感知蒸馏损失提升唇部同步和手部运动真实性。

Result: 方法实现了实时性能，在视觉质量上优于最近的音频驱动和输入驱动方法，并通过大量实验验证了算法设计的有效性。

Conclusion: 通过结合输入感知稀疏注意力和蒸馏损失，成功将多步扩散模型蒸馏为少步模型，在保持高质量的同时实现了实时语音驱动视频生成。

Abstract: Diffusion models can synthesize realistic co-speech video from audio for various applications, such as video creation and virtual agents. However, existing diffusion-based methods are slow due to numerous denoising steps and costly attention mechanisms, preventing real-time deployment. In this work, we distill a many-step diffusion video model into a few-step student model. Unfortunately, directly applying recent diffusion distillation methods degrades video quality and falls short of real-time performance. To address these issues, our new video distillation method leverages input human pose conditioning for both attention and loss functions. We first propose using accurate correspondence between input human pose keypoints to guide attention to relevant regions, such as the speaker's face, hands, and upper body. This input-aware sparse attention reduces redundant computations and strengthens temporal correspondences of body parts, improving inference efficiency and motion coherence. To further enhance visual quality, we introduce an input-aware distillation loss that improves lip synchronization and hand motion realism. By integrating our input-aware sparse attention and distillation loss, our method achieves real-time performance with improved visual quality compared to recent audio-driven and input-driven methods. We also conduct extensive experiments showing the effectiveness of our algorithmic design choices.

</details>


### [8] [Deep Generative Continual Learning using Functional LoRA: FunLoRA](https://arxiv.org/abs/2510.02631)
*Victor Enescu,Hichem Sahbi*

Main category: cs.CV

TL;DR: 提出FunLoRA方法，通过低秩适应和动态条件机制解决生成模型持续学习中的灾难性遗忘问题，仅需在当前任务数据上训练，显著减少内存成本和采样时间。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型在持续适应中面临灾难性遗忘问题，传统基于合成数据的再训练方法存在训练时间不可持续和性能长期退化的问题。

Method: 设计基于低秩适应(LoRA)的功能性条件机制FunLoRA，使用秩1矩阵并通过精心选择的函数增加重参数化矩阵秩，实现动态条件。

Result: 在基于流匹配的模型上实验表明，FunLoRA超越了基于扩散模型的先前最优结果，达到更高分类准确率，同时仅需少量内存成本和采样时间。

Conclusion: FunLoRA通过参数高效微调方法有效解决了生成模型持续学习中的灾难性遗忘问题，在性能和效率方面均表现出色。

Abstract: Continual adaptation of deep generative models holds tremendous potential and critical importance, given their rapid and expanding usage in text and vision based applications. Incremental training, however, remains highly challenging due to catastrophic forgetting phenomenon, which makes it difficult for neural networks to effectively incorporate new knowledge. A common strategy consists in retraining the generative model on its own synthetic data in order to mitigate forgetting. Yet, such an approach faces two major limitations: (i) the continually increasing training time eventually becomes intractable, and (ii) reliance on synthetic data inevitably leads to long-term performance degradation, since synthetic samples lack the richness of real training data. In this paper, we attenuate these issues by designing a novel and more expressive conditioning mechanism for generative models based on low rank adaptation (LoRA), that exclusively employs rank 1 matrices, whose reparametrized matrix rank is functionally increased using carefully selected functions -- and dubbed functional LoRA: FunLoRA. Using this dynamic conditioning, the generative model is guaranteed to avoid catastrophic forgetting and needs only to be trained on data from the current task. Extensive experiments using flow-matching based models trained from scratch, showcase that our proposed parameter-efficient fine-tuning (PEFT) method surpasses prior state-of-the-art results based on diffusion models, reaching higher classification accuracy scores, while only requiring a fraction of the memory cost and sampling time.

</details>


### [9] [Sequence-Preserving Dual-FoV Defense for Traffic Sign and Light Recognition in Autonomous Vehicles](https://arxiv.org/abs/2510.02642)
*Abhishek Joshi,Jahnavi Krishna Koda,Abhishek Phadke*

Main category: cs.CV

TL;DR: 提出了一个双视场、序列保持的鲁棒性框架，用于美国交通灯和标志识别，通过统一的三层防御堆栈（特征压缩、防御蒸馏、基于熵的异常检测）结合时序投票，显著提升了对抗数字和自然扰动的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆中交通灯和标志识别错误直接影响导航和安全，现有方法缺乏对时间连续性、多静态视场感知以及数字和自然退化鲁棒性的考虑。

Method: 基于多源数据集构建双视场序列保持框架，采用统一三层防御堆栈（特征压缩、防御蒸馏、基于熵的异常检测）和序列时序投票机制，在四种操作设计域下进行实验评估。

Result: 统一防御堆栈达到79.8mAP，将攻击成功率降至18.2%，优于YOLOv8、YOLOv9和BEVFormer，同时将高风险误分类降至32%。

Conclusion: 该框架在真实场景异常检测中表现出色，通过物理可转移性验证，为自动驾驶感知系统提供了有效的鲁棒性解决方案。

Abstract: Traffic light and sign recognition are key for Autonomous Vehicles (AVs) because perception mistakes directly influence navigation and safety. In addition to digital adversarial attacks, models are vulnerable to existing perturbations (glare, rain, dirt, or graffiti), which could lead to dangerous misclassifications. The current work lacks consideration of temporal continuity, multistatic field-of-view (FoV) sensing, and robustness to both digital and natural degradation. This study proposes a dual FoV, sequence-preserving robustness framework for traffic lights and signs in the USA based on a multi-source dataset built on aiMotive, Udacity, Waymo, and self-recorded videos from the region of Texas. Mid and long-term sequences of RGB images are temporally aligned for four operational design domains (ODDs): highway, night, rainy, and urban. Over a series of experiments on a real-life application of anomaly detection, this study outlines a unified three-layer defense stack framework that incorporates feature squeezing, defensive distillation, and entropy-based anomaly detection, as well as sequence-wise temporal voting for further enhancement. The evaluation measures included accuracy, attack success rate (ASR), risk-weighted misclassification severity, and confidence stability. Physical transferability was confirmed using probes for recapture. The results showed that the Unified Defense Stack achieved 79.8mAP and reduced the ASR to 18.2%, which is superior to YOLOv8, YOLOv9, and BEVFormer, while reducing the high-risk misclassification to 32%.

</details>


### [10] [Smart-GRPO: Smartly Sampling Noise for Efficient RL of Flow-Matching Models](https://arxiv.org/abs/2510.02654)
*Benjamin Yu,Jackie Liu,Justin Cui*

Main category: cs.CV

TL;DR: Smart-GRPO是首个优化流匹配模型中强化学习噪声扰动的方法，通过迭代搜索策略改进噪声分布，提高奖励优化和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 流匹配模型虽然能生成高质量文本到图像，但其确定性特性不适合强化学习。现有方法通过随机噪声扰动引入随机性，但效率低且不稳定。

Method: 采用迭代搜索策略：解码候选扰动，用奖励函数评估，然后向高奖励区域优化噪声分布。

Result: 实验表明Smart-GRPO在奖励优化和视觉质量上均优于基线方法。

Conclusion: 为流匹配框架中的强化学习提供了实用路径，弥合了高效训练与人类对齐生成之间的差距。

Abstract: Recent advancements in flow-matching have enabled high-quality text-to-image generation. However, the deterministic nature of flow-matching models makes them poorly suited for reinforcement learning, a key tool for improving image quality and human alignment. Prior work has introduced stochasticity by perturbing latents with random noise, but such perturbations are inefficient and unstable. We propose Smart-GRPO, the first method to optimize noise perturbations for reinforcement learning in flow-matching models. Smart-GRPO employs an iterative search strategy that decodes candidate perturbations, evaluates them with a reward function, and refines the noise distribution toward higher-reward regions. Experiments demonstrate that Smart-GRPO improves both reward optimization and visual quality compared to baseline methods. Our results suggest a practical path toward reinforcement learning in flow-matching frameworks, bridging the gap between efficient training and human-aligned generation.

</details>


### [11] [From Tokens to Nodes: Semantic-Guided Motion Control for Dynamic 3D Gaussian Splatting](https://arxiv.org/abs/2510.02732)
*Jianing Chen,Zehao Li,Yujun Cai,Hao Jiang,Shuqin Gao,Honglong Zhao,Tianlu Mao,Yucheng Zhang*

Main category: cs.CV

TL;DR: 提出了一种运动自适应的3D重建框架，通过语义和运动先验将控制点密度与运动复杂度对齐，在动态区域集中控制点，抑制静态背景冗余，并使用样条轨迹参数化实现更平滑的运动表示。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏控制方法仅基于几何分配控制点，导致静态区域冗余和动态区域不足，无法有效处理3D运动重建中的计算需求和视图限制问题。

Method: 利用视觉基础模型的语义和运动先验建立补丁-标记-节点对应关系，通过运动自适应压缩和迭代体素化实现控制点密度适应，采用基于2D轨迹初始化的样条轨迹参数化替代MLP变形场。

Result: 在重建质量和效率方面相比现有最先进方法有显著提升，实现了更平滑的运动表示和更稳定的优化。

Conclusion: 提出的运动自适应框架通过将控制点分配与运动复杂度对齐，有效解决了动态3D重建中的关键限制，在计算效率和重建质量上都取得了突破。

Abstract: Dynamic 3D reconstruction from monocular videos remains difficult due to the ambiguity inferring 3D motion from limited views and computational demands of modeling temporally varying scenes. While recent sparse control methods alleviate computation by reducing millions of Gaussians to thousands of control points, they suffer from a critical limitation: they allocate points purely by geometry, leading to static redundancy and dynamic insufficiency. We propose a motion-adaptive framework that aligns control density with motion complexity. Leveraging semantic and motion priors from vision foundation models, we establish patch-token-node correspondences and apply motion-adaptive compression to concentrate control points in dynamic regions while suppressing redundancy in static backgrounds. Our approach achieves flexible representational density adaptation through iterative voxelization and motion tendency scoring, directly addressing the fundamental mismatch between control point allocation and motion complexity. To capture temporal evolution, we introduce spline-based trajectory parameterization initialized by 2D tracklets, replacing MLP-based deformation fields to achieve smoother motion representation and more stable optimization. Extensive experiments demonstrate significant improvements in reconstruction quality and efficiency over existing state-of-the-art methods.

</details>


### [12] [Net2Net: When Un-trained Meets Pre-trained Networks for Robust Real-World Denoising](https://arxiv.org/abs/2510.02733)
*Weimin Yuan,Cai Meng*

Main category: cs.CV

TL;DR: Net2Net是一种结合无监督DIP和预训练DRUNet网络的混合去噪方法，通过RED正则化实现，无需标记数据即可适应各种真实噪声模式。


<details>
  <summary>Details</summary>
Motivation: 传统去噪方法依赖手工先验，在复杂真实噪声中表现不佳；深度学习需要大量标记数据且泛化能力有限。需要一种能结合两者优势的方法来处理真实世界噪声。

Method: 结合无监督DIP网络和预训练DRUNet网络，通过RED正则化构建混合框架。无监督网络适应输入图像的独特噪声特征，预训练网络利用大规模数据集学习到的表示。

Result: 在基准数据集上的广泛实验表明，该方法在真实世界噪声去除方面具有优越性，特别是在训练数据有限的情况下表现更好。

Conclusion: Net2Net通过结合无监督和预训练网络的混合框架，有效解决了真实世界噪声去除的挑战，提高了跨不同噪声模式的泛化能力。

Abstract: Traditional denoising methods for noise removal have largely relied on handcrafted priors, often perform well in controlled environments but struggle to address the complexity and variability of real noise. In contrast, deep learning-based approaches have gained prominence for learning noise characteristics from large datasets, but these methods frequently require extensive labeled data and may not generalize effectively across diverse noise types and imaging conditions. In this paper, we present an innovative method, termed as Net2Net, that combines the strengths of untrained and pre-trained networks to tackle the challenges of real-world noise removal. The innovation of Net2Net lies in its combination of unsupervised DIP and supervised pre-trained model DRUNet by regularization by denoising (RED). The untrained network adapts to the unique noise characteristics of each input image without requiring labeled data, while the pre-trained network leverages learned representations from large-scale datasets to deliver robust denoising performance. This hybrid framework enhances generalization across varying noise patterns and improves performance, particularly in scenarios with limited training data. Extensive experiments on benchmark datasets demonstrate the superiority of our method for real-world noise removal.

</details>


### [13] [Med-K2N: Flexible K-to-N Modality Translation for Medical Image Synthesis](https://arxiv.org/abs/2510.02815)
*Feng Yuan,Yifan Gao,Yuehua Ye,Haoyue Li,Xin Gao*

Main category: cs.CV

TL;DR: 提出Med-K2N方法解决K到N医学图像合成中的三个关键挑战：异质模态贡献建模、融合质量控制、多输出模态一致性保持，通过三个协作模块和因果模态身份模块实现自适应权重学习和渐进增强。


<details>
  <summary>Details</summary>
Motivation: 临床诊断需要灵活重建缺失的成像模态，但面临异质模态贡献建模、融合质量控制、多输出模态一致性保持三大挑战。

Method: 将多模态医学数据视为具有质量驱动选择机制的序列帧，设计PreWeightNet、ThresholdNet、EffiWeightNet三个协作模块进行自适应权重学习和渐进增强，并提出因果模态身份模块(CMIM)保持模态一致性。

Result: 在多个基准测试中显著优于现有最先进方法。

Conclusion: Med-K2N通过自适应权重学习和因果约束有效解决了K到N医学图像合成的关键挑战，为临床诊断提供了灵活可靠的模态重建能力。

Abstract: Cross-modal medical image synthesis research focuses on reconstructing missing imaging modalities from available ones to support clinical diagnosis. Driven by clinical necessities for flexible modality reconstruction, we explore K to N medical generation, where three critical challenges emerge: How can we model the heterogeneous contributions of different modalities to various target tasks? How can we ensure fusion quality control to prevent degradation from noisy information? How can we maintain modality identity consistency in multi-output generation? Driven by these clinical necessities, and drawing inspiration from SAM2's sequential frame paradigm and clinicians' progressive workflow of incrementally adding and selectively integrating multi-modal information, we treat multi-modal medical data as sequential frames with quality-driven selection mechanisms. Our key idea is to "learn" adaptive weights for each modality-task pair and "memorize" beneficial fusion patterns through progressive enhancement. To achieve this, we design three collaborative modules: PreWeightNet for global contribution assessment, ThresholdNet for adaptive filtering, and EffiWeightNet for effective weight computation. Meanwhile, to maintain modality identity consistency, we propose the Causal Modality Identity Module (CMIM) that establishes causal constraints between generated images and target modality descriptions using vision-language modeling. Extensive experimental results demonstrate that our proposed Med-K2N outperforms state-of-the-art methods by significant margins on multiple benchmarks. Source code is available.

</details>


### [14] [Multimodal Carotid Risk Stratification with Large Vision-Language Models: Benchmarking, Fine-Tuning, and Clinical Insights](https://arxiv.org/abs/2510.02922)
*Daphne Tsolissou,Theofanis Ganitidis,Konstantinos Mitsis,Stergios CHristodoulidis,Maria Vakalopoulou,Konstantina Nikita*

Main category: cs.CV

TL;DR: 该研究评估了大型视觉语言模型在整合超声成像和临床数据用于颈动脉斑块风险评估的能力，发现现有模型在风险分类方面表现不佳，但通过领域适应和多模态数据整合可以显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 颈动脉粥样硬化疾病的风险评估需要整合多样化的临床和影像信息，但现有方法缺乏透明度和可解释性。本研究旨在探索大型视觉语言模型在多模态颈动脉斑块评估中的潜力。

Method: 提出了一个通过访谈式问题序列模拟真实诊断场景的框架，比较了多种开源LVLMs。使用低秩适应( LoRA )将LLaVa-NeXT-Vicuna模型适配到超声领域，并整合多模态表格数据。

Result: 零样本实验显示，并非所有LVLMs都能准确识别成像模态和解剖结构，所有模型在风险分类方面表现均不佳。领域适应后卒中风险分层显著改善，多模态数据整合进一步提高了特异性和平衡准确率。

Conclusion: LVLMs在超声心血管风险预测中既有前景也有局限性，强调了多模态整合、模型校准和领域适应对于临床转化的重要性。

Abstract: Reliable risk assessment for carotid atheromatous disease remains a major clinical challenge, as it requires integrating diverse clinical and imaging information in a manner that is transparent and interpretable to clinicians. This study investigates the potential of state-of-the-art and recent large vision-language models (LVLMs) for multimodal carotid plaque assessment by integrating ultrasound imaging (USI) with structured clinical, demographic, laboratory, and protein biomarker data. A framework that simulates realistic diagnostic scenarios through interview-style question sequences is proposed, comparing a range of open-source LVLMs, including both general-purpose and medically tuned models. Zero-shot experiments reveal that even if they are very powerful, not all LVLMs can accurately identify imaging modality and anatomy, while all of them perform poorly in accurate risk classification. To address this limitation, LLaVa-NeXT-Vicuna is adapted to the ultrasound domain using low-rank adaptation (LoRA), resulting in substantial improvements in stroke risk stratification. The integration of multimodal tabular data in the form of text further enhances specificity and balanced accuracy, yielding competitive performance compared to prior convolutional neural network (CNN) baselines trained on the same dataset. Our findings highlight both the promise and limitations of LVLMs in ultrasound-based cardiovascular risk prediction, underscoring the importance of multimodal integration, model calibration, and domain adaptation for clinical translation.

</details>


### [15] [Flip Distribution Alignment VAE for Multi-Phase MRI Synthesis](https://arxiv.org/abs/2510.02970)
*Xiaoyan Kui,Qianmu Xiao,Qqinsong Li,Zexin Ji,JIelin Zhang,Beiji Zou*

Main category: cs.CV

TL;DR: 提出FDA-VAE，一种轻量级特征解耦VAE模型，用于多期相增强MRI合成，通过对称潜在分布编码有效分离共享和独立特征。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用深度自编码器生成器参数效率低，缺乏可解释的训练策略，需要改进多期相增强MRI合成中的特征分离。

Method: 使用Flip Distribution Alignment VAE，将输入和目标图像编码为关于标准正态分布对称的两个潜在分布，采用Y形双向训练策略增强特征分离的可解释性。

Result: 相比现有深度自编码器端到端合成方法，FDA-VAE显著减少模型参数和推理时间，同时有效提高合成质量。

Conclusion: FDA-VAE是一种参数效率高、推理速度快且合成质量好的多期相增强MRI合成方法。

Abstract: Separating shared and independent features is crucial for multi-phase contrast-enhanced (CE) MRI synthesis. However, existing methods use deep autoencoder generators with low parameter efficiency and lack interpretable training strategies. In this paper, we propose Flip Distribution Alignment Variational Autoencoder (FDA-VAE), a lightweight feature-decoupled VAE model for multi-phase CE MRI synthesis. Our method encodes input and target images into two latent distributions that are symmetric concerning a standard normal distribution, effectively separating shared and independent features. The Y-shaped bidirectional training strategy further enhances the interpretability of feature separation. Experimental results show that compared to existing deep autoencoder-based end-to-end synthesis methods, FDA-VAE significantly reduces model parameters and inference time while effectively improving synthesis quality. The source code is publicly available at https://github.com/QianMuXiao/FDA-VAE.

</details>


### [16] [TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency](https://arxiv.org/abs/2510.02987)
*Juntong Wang,Huiyu Duan,Jiarui Wang,Ziheng Jia,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 提出了LPG-Bench基准测试和TIT评估方法，用于评估文本到图像模型在长提示生成中的表现，解决了现有模型对长详细提示理解不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型多模态模型在短提示下能生成高质量图像，但在长详细提示下表现不一致，缺乏有效的评估基准和指标。

Method: 构建包含200个平均超过250词的长提示基准LPG-Bench，生成2600张图像并进行人工标注；提出基于文本-图像-文本一致性的TIT评估方法，包括TIT-Score和TIT-Score-LLM两种实现。

Result: 现有T2I评估指标在长提示生成中与人类偏好一致性差；TIT方法显著优于基线，TIT-Score-LLM在成对准确率上比最强基线提升7.31%。

Conclusion: LPG-Bench和TIT方法为评估和促进T2I模型发展提供了新视角，所有资源将公开。

Abstract: With the rapid advancement of large multimodal models (LMMs), recent text-to-image (T2I) models can generate high-quality images and demonstrate great alignment to short prompts. However, they still struggle to effectively understand and follow long and detailed prompts, displaying inconsistent generation. To address this challenge, we introduce LPG-Bench, a comprehensive benchmark for evaluating long-prompt-based text-to-image generation. LPG-Bench features 200 meticulously crafted prompts with an average length of over 250 words, approaching the input capacity of several leading commercial models. Using these prompts, we generate 2,600 images from 13 state-of-the-art models and further perform comprehensive human-ranked annotations. Based on LPG-Bench, we observe that state-of-the-art T2I alignment evaluation metrics exhibit poor consistency with human preferences on long-prompt-based image generation. To address the gap, we introduce a novel zero-shot metric based on text-to-image-to-text consistency, termed TIT, for evaluating long-prompt-generated images. The core concept of TIT is to quantify T2I alignment by directly comparing the consistency between the raw prompt and the LMM-produced description on the generated image, which includes an efficient score-based instantiation TIT-Score and a large-language-model (LLM) based instantiation TIT-Score-LLM. Extensive experiments demonstrate that our framework achieves superior alignment with human judgment compared to CLIP-score, LMM-score, etc., with TIT-Score-LLM attaining a 7.31% absolute improvement in pairwise accuracy over the strongest baseline. LPG-Bench and TIT methods together offer a deeper perspective to benchmark and foster the development of T2I models. All resources will be made publicly available.

</details>


### [17] [PocketSR: The Super-Resolution Expert in Your Pocket Mobiles](https://arxiv.org/abs/2510.03012)
*Haoze Sun,Linfeng Jiang,Fan Li,Renjing Pei,Zhixin Wang,Yong Guo,Jiaqi Xu,Haoyu Chen,Jin Han,Fenglong Song,Yujiu Yang,Wenbo Li*

Main category: cs.CV

TL;DR: PocketSR是一个超轻量级的单步图像超分辨率模型，通过高效的LiteED编码器和在线退火剪枝技术，在保持生成质量的同时大幅减少计算成本，适合边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 现有基于大生成模型的RealSR方法虽然效果好，但计算成本高、延迟大，难以在边缘设备上部署。需要开发既高效又能保持高质量的超分辨率解决方案。

Method: 设计了LiteED作为SD中VAE的高效替代品，参数减少97.5%；提出在线退火剪枝技术，逐步将生成先验从重模块转移到轻模块；使用多层特征蒸馏损失来缓解剪枝过程中的知识损失。

Result: PocketSR模型仅146M参数，处理4K图像仅需0.8秒，速度显著提升。性能与最先进的单步和多步RealSR模型相当。

Conclusion: PocketSR在保持高质量的同时实现了高效率，为边缘设备上的图像超分辨率应用提供了实用解决方案，并为未来研究提供了有价值的见解。

Abstract: Real-world image super-resolution (RealSR) aims to enhance the visual quality of in-the-wild images, such as those captured by mobile phones. While existing methods leveraging large generative models demonstrate impressive results, the high computational cost and latency make them impractical for edge deployment. In this paper, we introduce PocketSR, an ultra-lightweight, single-step model that brings generative modeling capabilities to RealSR while maintaining high fidelity. To achieve this, we design LiteED, a highly efficient alternative to the original computationally intensive VAE in SD, reducing parameters by 97.5% while preserving high-quality encoding and decoding. Additionally, we propose online annealing pruning for the U-Net, which progressively shifts generative priors from heavy modules to lightweight counterparts, ensuring effective knowledge transfer and further optimizing efficiency. To mitigate the loss of prior knowledge during pruning, we incorporate a multi-layer feature distillation loss. Through an in-depth analysis of each design component, we provide valuable insights for future research. PocketSR, with a model size of 146M parameters, processes 4K images in just 0.8 seconds, achieving a remarkable speedup over previous methods. Notably, it delivers performance on par with state-of-the-art single-step and even multi-step RealSR models, making it a highly practical solution for edge-device applications.

</details>


### [18] [When and Where do Events Switch in Multi-Event Video Generation?](https://arxiv.org/abs/2510.03049)
*Ruotong Liao,Guowen Huang,Qing Cheng,Thomas Seidl,Daniel Cremers,Volker Tresp*

Main category: cs.CV

TL;DR: 本文提出了MEve评估套件，系统研究了多事件文本到视频生成中的事件转换控制问题，发现早期干预去噪步骤和块级模型层对多事件视频生成至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有方法在扩展到多事件生成时忽略了事件转换的内在因素，本文旨在回答多事件提示何时何地控制事件转换这一核心问题。

Method: 引入MEve自建提示套件，系统研究OpenSora和CogVideoX两个代表性模型家族，通过大量实验分析去噪步骤和模型层的影响。

Result: 实验表明早期干预去噪步骤和块级模型层对多事件视频生成至关重要，揭示了多事件生成的关键因素。

Conclusion: 研究强调了多事件调节在未来模型中的可能性，为多事件文本到视频生成提供了重要见解。

Abstract: Text-to-video (T2V) generation has surged in response to challenging questions, especially when a long video must depict multiple sequential events with temporal coherence and controllable content. Existing methods that extend to multi-event generation omit an inspection of the intrinsic factor in event shifting. The paper aims to answer the central question: When and where multi-event prompts control event transition during T2V generation. This work introduces MEve, a self-curated prompt suite for evaluating multi-event text-to-video (T2V) generation, and conducts a systematic study of two representative model families, i.e., OpenSora and CogVideoX. Extensive experiments demonstrate the importance of early intervention in denoising steps and block-wise model layers, revealing the essential factor for multi-event video generation and highlighting the possibilities for multi-event conditioning in future models.

</details>


### [19] [What Drives Compositional Generalization in Visual Generative Models?](https://arxiv.org/abs/2510.03075)
*Karim Farid,Rajat Sahay,Yumna Ali Alnaggar,Simon Schrodi,Volker Fischer,Cordelia Schmid,Thomas Brox*

Main category: cs.CV

TL;DR: 该论文系统研究了视觉生成模型中影响组合泛化的设计选择，发现离散/连续训练目标和条件信息是两大关键因素，并提出通过添加连续JEPA目标来改进MaskGIT的组合性能。


<details>
  <summary>Details</summary>
Motivation: 组合泛化是视觉生成模型的关键能力，但目前对其促进和抑制机制的理解还不够充分，需要进行系统性研究。

Method: 通过受控实验研究不同设计选择对组合泛化的影响，重点关注训练目标的离散/连续性质以及条件信息的作用，并提出在MaskGIT中引入连续JEPA目标的方法。

Result: 识别出两个关键因素：(i)训练目标的离散或连续分布；(ii)训练期间条件信息对组成概念的提供程度。基于这些发现，证明了在MaskGIT中添加连续JEPA目标可以改善组合性能。

Conclusion: 训练目标的离散/连续性质和条件信息是影响组合泛化的关键因素，通过适当的设计选择可以显著提升视觉生成模型的组合泛化能力。

Abstract: Compositional generalization, the ability to generate novel combinations of known concepts, is a key ingredient for visual generative models. Yet, not all mechanisms that enable or inhibit it are fully understood. In this work, we conduct a systematic study of how various design choices influence compositional generalization in image and video generation in a positive or negative way. Through controlled experiments, we identify two key factors: (i) whether the training objective operates on a discrete or continuous distribution, and (ii) to what extent conditioning provides information about the constituent concepts during training. Building on these insights, we show that relaxing the MaskGIT discrete loss with an auxiliary continuous JEPA-based objective can improve compositional performance in discrete models like MaskGIT.

</details>


### [20] [Latent Diffusion Unlearning: Protecting Against Unauthorized Personalization Through Trajectory Shifted Perturbations](https://arxiv.org/abs/2510.03089)
*Naresh Kumar Devulapally,Shruti Agarwal,Tejas Gokhale,Vishnu Suresh Lokhande*

Main category: cs.CV

TL;DR: 提出一种基于潜在扩散模型的不可学习样本生成方法，通过在潜在空间中交替进行去噪和反转操作，修改去噪轨迹的起点，实现高视觉保真度且能抵抗下游生成模型反演和个性化的图像扰动。


<details>
  <summary>Details</summary>
Motivation: 针对文本到图像扩散模型在个性化应用中可能引发的数据隐私、知识产权保护和未经授权使用等问题，现有基于像素空间的图像毒化方法存在视觉质量差的问题，需要在保持图像质量的同时实现有效的防御。

Method: 在潜在扩散模型的潜在空间中，采用交替去噪和反转的策略，修改去噪轨迹的起点，通过轨迹偏移采样生成不可学习的训练样本。

Result: 在四个基准数据集上的实验表明，该方法在感知指标（PSNR、SSIM、FID）上提升了约8%-10%，在五种对抗设置下平均鲁棒性提升了约10%，能有效抵抗最先进的反演攻击。

Conclusion: 该方法成功将不可学习特性集成到潜在扩散模型框架中，提供了一种实用且难以察觉的防御机制，有效保护敏感数据免受未经授权的模型适应。

Abstract: Text-to-image diffusion models have demonstrated remarkable effectiveness in rapid and high-fidelity personalization, even when provided with only a few user images. However, the effectiveness of personalization techniques has lead to concerns regarding data privacy, intellectual property protection, and unauthorized usage. To mitigate such unauthorized usage and model replication, the idea of generating ``unlearnable'' training samples utilizing image poisoning techniques has emerged. Existing methods for this have limited imperceptibility as they operate in the pixel space which results in images with noise and artifacts. In this work, we propose a novel model-based perturbation strategy that operates within the latent space of diffusion models. Our method alternates between denoising and inversion while modifying the starting point of the denoising trajectory: of diffusion models. This trajectory-shifted sampling ensures that the perturbed images maintain high visual fidelity to the original inputs while being resistant to inversion and personalization by downstream generative models. This approach integrates unlearnability into the framework of Latent Diffusion Models (LDMs), enabling a practical and imperceptible defense against unauthorized model adaptation. We validate our approach on four benchmark datasets to demonstrate robustness against state-of-the-art inversion attacks. Results demonstrate that our method achieves significant improvements in imperceptibility ($\sim 8 \% -10\%$ on perceptual metrics including PSNR, SSIM, and FID) and robustness ( $\sim 10\%$ on average across five adversarial settings), highlighting its effectiveness in safeguarding sensitive data.

</details>


### [21] [Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled Fields](https://arxiv.org/abs/2510.03104)
*Zhiting Mei,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.CV

TL;DR: 该论文研究了在辐射场语义蒸馏中几何接地特征的作用，发现视觉特征比几何接地特征在姿态估计等任务中表现更好，尽管后者包含更多几何细节。


<details>
  <summary>Details</summary>
Motivation: 探索几何接地语义特征在蒸馏辐射场中的潜在优势，特别是在空间任务如姿态估计中的表现。

Method: 提出了SPINE框架，包含基于蒸馏语义的粗反演和基于光度优化的精细反演两个核心组件。

Result: 几何接地特征包含更精细的结构细节，但在语义目标定位任务中无显著差异，且在姿态估计中精度反而下降。

Conclusion: 视觉特征在更广泛的下游任务中具有更好的通用性，未来需要研究更有效的几何接地策略来增强预训练语义特征的性能。

Abstract: Semantic distillation in radiance fields has spurred significant advances in open-vocabulary robot policies, e.g., in manipulation and navigation, founded on pretrained semantics from large vision models. While prior work has demonstrated the effectiveness of visual-only semantic features (e.g., DINO and CLIP) in Gaussian Splatting and neural radiance fields, the potential benefit of geometry-grounding in distilled fields remains an open question. In principle, visual-geometry features seem very promising for spatial tasks such as pose estimation, prompting the question: Do geometry-grounded semantic features offer an edge in distilled fields? Specifically, we ask three critical questions: First, does spatial-grounding produce higher-fidelity geometry-aware semantic features? We find that image features from geometry-grounded backbones contain finer structural details compared to their counterparts. Secondly, does geometry-grounding improve semantic object localization? We observe no significant difference in this task. Thirdly, does geometry-grounding enable higher-accuracy radiance field inversion? Given the limitations of prior work and their lack of semantics integration, we propose a novel framework SPINE for inverting radiance fields without an initial guess, consisting of two core components: coarse inversion using distilled semantics, and fine inversion using photometric-based optimization. Surprisingly, we find that the pose estimation accuracy decreases with geometry-grounded features. Our results suggest that visual-only features offer greater versatility for a broader range of downstream tasks, although geometry-grounded features contain more geometric detail. Notably, our findings underscore the necessity of future research on effective strategies for geometry-grounding that augment the versatility and performance of pretrained semantic features.

</details>


### [22] [GeoComplete: Geometry-Aware Diffusion for Reference-Driven Image Completion](https://arxiv.org/abs/2510.03110)
*Beibei Lin,Tingting Chen,Robby T. Tan*

Main category: cs.CV

TL;DR: GeoComplete是一个用于参考驱动图像补全的新框架，通过结合显式3D结构指导来增强几何一致性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的生成方法仅依赖扩散先验，缺乏几何线索（如相机姿态或深度），在目标视图与参考图像差异较大时经常产生错位或不合理的内容。

Method: 提出双分支扩散架构：一个分支从掩码目标合成缺失区域，另一个分支从投影点云提取几何特征；引入目标感知掩码策略，将目标视图投影到每个参考中以检测遮挡区域并在训练时掩码。

Result: 实验显示GeoComplete在PSNR指标上比最先进方法提升17.1，显著提高了几何准确性同时保持高视觉质量。

Conclusion: 通过整合几何感知的双分支扩散架构和目标感知掩码策略，GeoComplete为几何条件图像补全提供了统一且鲁棒的解决方案。

Abstract: Reference-driven image completion, which restores missing regions in a target view using additional images, is particularly challenging when the target view differs significantly from the references. Existing generative methods rely solely on diffusion priors and, without geometric cues such as camera pose or depth, often produce misaligned or implausible content. We propose GeoComplete, a novel framework that incorporates explicit 3D structural guidance to enforce geometric consistency in the completed regions, setting it apart from prior image-only approaches. GeoComplete introduces two key ideas: conditioning the diffusion process on projected point clouds to infuse geometric information, and applying target-aware masking to guide the model toward relevant reference cues. The framework features a dual-branch diffusion architecture. One branch synthesizes the missing regions from the masked target, while the other extracts geometric features from the projected point cloud. Joint self-attention across branches ensures coherent and accurate completion. To address regions visible in references but absent in the target, we project the target view into each reference to detect occluded areas, which are then masked during training. This target-aware masking directs the model to focus on useful cues, enhancing performance in difficult scenarios. By integrating a geometry-aware dual-branch diffusion architecture with a target-aware masking strategy, GeoComplete offers a unified and robust solution for geometry-conditioned image completion. Experiments show that GeoComplete achieves a 17.1 PSNR improvement over state-of-the-art methods, significantly boosting geometric accuracy while maintaining high visual quality.

</details>


### [23] [Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction](https://arxiv.org/abs/2510.03117)
*Kaisi Guan,Xihua Wang,Zhengfeng Lai,Xin Cheng,Peng Zhang,XiaoJiang Liu,Ruihua Song,Meng Cao*

Main category: cs.CV

TL;DR: 本文提出了一种解决文本到声音视频生成任务的新方法，通过分层视觉基础字幕框架生成解耦的视频和音频字幕，并引入桥接扩散变换器实现跨模态特征交互，在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决文本到声音视频生成中的两个关键挑战：单一共享文本字幕导致的模态干扰，以及跨模态特征交互机制不明确的问题。

Method: 提出分层视觉基础字幕框架生成解耦的视频和音频字幕，并设计桥接扩散变换器，采用双交叉注意力机制实现双向信息交换。

Result: 在三个基准数据集上的广泛实验和人工评估表明，该方法在大多数指标上达到了最先进的结果。

Conclusion: 该方法有效解决了模态干扰和跨模态交互问题，为未来文本到声音视频生成任务提供了重要见解。

Abstract: This study focuses on a challenging yet promising task, Text-to-Sounding-Video (T2SV) generation, which aims to generate a video with synchronized audio from text conditions, meanwhile ensuring both modalities are aligned with text. Despite progress in joint audio-video training, two critical challenges still remain unaddressed: (1) a single, shared text caption where the text for video is equal to the text for audio often creates modal interference, confusing the pretrained backbones, and (2) the optimal mechanism for cross-modal feature interaction remains unclear. To address these challenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC) framework that generates pairs of disentangled captions, a video caption, and an audio caption, eliminating interference at the conditioning stage. Based on HVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer, which employs a Dual CrossAttention (DCA) mechanism that acts as a robust ``bridge" to enable a symmetric, bidirectional exchange of information, achieving both semantic and temporal synchronization. Extensive experiments on three benchmark datasets, supported by human evaluations, demonstrate that our method achieves state-of-the-art results on most metrics. Comprehensive ablation studies further validate the effectiveness of our contributions, offering key insights for the future T2SV task. All the codes and checkpoints will be publicly released.

</details>


### [24] [HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion](https://arxiv.org/abs/2510.03122)
*Shiyi Zhang,Dong Liang,Hairong Zheng,Yihang Zhou*

Main category: cs.CV

TL;DR: HAVIR模型通过分离视觉皮层的两个层次区域来提取不同特征，结合结构生成器和语义提取器，使用Versatile Diffusion模型合成最终图像，在复杂场景中提升了重建的结构和语义质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在准确恢复高度复杂视觉刺激方面面临挑战，主要因为自然场景中低层特征异质性高，高层特征因上下文重叠而语义纠缠。受视觉皮层层次表示理论启发，需要分离不同层次的特征提取。

Method: 将视觉皮层分为两个层次区域：结构生成器从空间处理体素提取结构信息并转换为潜在扩散先验，语义提取器将语义处理体素转换为CLIP嵌入，通过Versatile Diffusion模型整合这些组件合成最终图像。

Result: 实验结果表明HAVIR在复杂场景中提升了重建图像的结构和语义质量，性能优于现有模型。

Conclusion: HAVIR模型通过层次化分离视觉皮层特征提取，有效解决了复杂视觉刺激重建中的挑战，在结构和语义质量方面均取得显著提升。

Abstract: The reconstruction of visual information from brain activity fosters interdisciplinary integration between neuroscience and computer vision. However, existing methods still face challenges in accurately recovering highly complex visual stimuli. This difficulty stems from the characteristics of natural scenes: low-level features exhibit heterogeneity, while high-level features show semantic entanglement due to contextual overlaps. Inspired by the hierarchical representation theory of the visual cortex, we propose the HAVIR model, which separates the visual cortex into two hierarchical regions and extracts distinct features from each. Specifically, the Structural Generator extracts structural information from spatial processing voxels and converts it into latent diffusion priors, while the Semantic Extractor converts semantic processing voxels into CLIP embeddings. These components are integrated via the Versatile Diffusion model to synthesize the final image. Experimental results demonstrate that HAVIR enhances both the structural and semantic quality of reconstructions, even in complex scenes, and outperforms existing models.

</details>


### [25] [Mask2IV: Interaction-Centric Video Generation via Mask Trajectories](https://arxiv.org/abs/2510.03135)
*Gen Li,Bo Zhao,Jianfei Yang,Laura Sevilla-Lara*

Main category: cs.CV

TL;DR: Mask2IV是一个用于交互中心视频生成的两阶段框架，无需密集掩码输入，通过预测动作轨迹来生成高质量的人类/机器人交互视频


<details>
  <summary>Details</summary>
Motivation: 现有方法难以建模复杂的动态交互，而获取精确的掩码标注在现实应用中具有挑战性

Method: 采用解耦的两阶段流程：首先预测动作和物体的合理运动轨迹，然后基于这些轨迹生成视频

Result: 在多样化的动作和物体类别上进行了广泛实验，证明方法在视觉真实性和可控性方面优于现有基线

Conclusion: Mask2IV框架成功解决了交互中心视频生成的挑战，提供了灵活的控制方式，无需密集掩码输入

Abstract: Generating interaction-centric videos, such as those depicting humans or robots interacting with objects, is crucial for embodied intelligence, as they provide rich and diverse visual priors for robot learning, manipulation policy training, and affordance reasoning. However, existing methods often struggle to model such complex and dynamic interactions. While recent studies show that masks can serve as effective control signals and enhance generation quality, obtaining dense and precise mask annotations remains a major challenge for real-world use. To overcome this limitation, we introduce Mask2IV, a novel framework specifically designed for interaction-centric video generation. It adopts a decoupled two-stage pipeline that first predicts plausible motion trajectories for both actor and object, then generates a video conditioned on these trajectories. This design eliminates the need for dense mask inputs from users while preserving the flexibility to manipulate the interaction process. Furthermore, Mask2IV supports versatile and intuitive control, allowing users to specify the target object of interaction and guide the motion trajectory through action descriptions or spatial position cues. To support systematic training and evaluation, we curate two benchmarks covering diverse action and object categories across both human-object interaction and robotic manipulation scenarios. Extensive experiments demonstrate that our method achieves superior visual realism and controllability compared to existing baselines.

</details>


### [26] [UniShield: An Adaptive Multi-Agent Framework for Unified Forgery Image Detection and Localization](https://arxiv.org/abs/2510.03161)
*Qing Huang,Zhipei Xu,Xuanyu Zhang,Jian Zhang*

Main category: cs.CV

TL;DR: 提出了UniShield多智能体统一系统，用于跨域检测和定位图像伪造，包括图像篡改、文档篡改、DeepFake和AI生成图像，通过感知智能体和检测智能体的协同工作实现自适应检测。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成技术的快速发展，合成图像越来越逼真，带来了错误信息和欺诈等社会风险。现有领域特定检测方法存在专业化过窄、跨域泛化能力差、缺乏集成自适应框架等局限性。

Method: UniShield系统创新性地集成了感知智能体和检测智能体。感知智能体智能分析图像特征以动态选择合适的检测模型，检测智能体将各种专家检测器整合到统一框架中并生成可解释报告。

Result: 大量实验表明，UniShield取得了最先进的结果，超越了现有的统一方法和领域特定检测器，突出了其卓越的实用性、自适应性和可扩展性。

Conclusion: UniShield为解决图像伪造检测的实际应用限制提供了有效的统一解决方案，具有重要的社会安全意义。

Abstract: With the rapid advancements in image generation, synthetic images have become increasingly realistic, posing significant societal risks, such as misinformation and fraud. Forgery Image Detection and Localization (FIDL) thus emerges as essential for maintaining information integrity and societal security. Despite impressive performances by existing domain-specific detection methods, their practical applicability remains limited, primarily due to their narrow specialization, poor cross-domain generalization, and the absence of an integrated adaptive framework. To address these issues, we propose UniShield, the novel multi-agent-based unified system capable of detecting and localizing image forgeries across diverse domains, including image manipulation, document manipulation, DeepFake, and AI-generated images. UniShield innovatively integrates a perception agent with a detection agent. The perception agent intelligently analyzes image features to dynamically select suitable detection models, while the detection agent consolidates various expert detectors into a unified framework and generates interpretable reports. Extensive experiments show that UniShield achieves state-of-the-art results, surpassing both existing unified approaches and domain-specific detectors, highlighting its superior practicality, adaptiveness, and scalability.

</details>


### [27] [Product-Quantised Image Representation for High-Quality Image Synthesis](https://arxiv.org/abs/2510.03191)
*Denis Zavadski,Nikita Philip Tatsch,Carsten Rother*

Main category: cs.CV

TL;DR: PQGAN将产品量化(PQ)集成到VQGAN框架中，在图像重建性能上显著优于现有方法，PSNR达到37dB，并能无缝集成到预训练扩散模型中实现更高效的图像生成。


<details>
  <summary>Details</summary>
Motivation: 产品量化(PQ)在可扩展向量编码中应用广泛，但在高保真图像生成的潜在表示中应用有限，作者希望将PQ集成到VQGAN框架中提升性能。

Method: 提出PQGAN，在VQGAN的向量量化框架中集成产品量化，深入分析码本大小、嵌入维度和子空间分解之间的相互作用。

Result: PSNR达到37dB（先前工作为27dB），FID、LPIPS和CMMD分数降低高达96%，并能实现更快的生成速度或双倍输出分辨率。

Conclusion: PQGAN展示了产品量化在图像合成中作为离散潜在表示的强大扩展能力，为超参数选择提供了指导性趋势分析。

Abstract: Product quantisation (PQ) is a classical method for scalable vector encoding, yet it has seen limited usage for latent representations in high-fidelity image generation. In this work, we introduce PQGAN, a quantised image autoencoder that integrates PQ into the well-known vector quantisation (VQ) framework of VQGAN. PQGAN achieves a noticeable improvement over state-of-the-art methods in terms of reconstruction performance, including both quantisation methods and their continuous counterparts. We achieve a PSNR score of 37dB, where prior work achieves 27dB, and are able to reduce the FID, LPIPS, and CMMD score by up to 96%. Our key to success is a thorough analysis of the interaction between codebook size, embedding dimensionality, and subspace factorisation, with vector and scalar quantisation as special cases. We obtain novel findings, such that the performance of VQ and PQ behaves in opposite ways when scaling the embedding dimension. Furthermore, our analysis shows performance trends for PQ that help guide optimal hyperparameter selection. Finally, we demonstrate that PQGAN can be seamlessly integrated into pre-trained diffusion models. This enables either a significantly faster and more compute-efficient generation, or a doubling of the output resolution at no additional cost, positioning PQ as a strong extension for discrete latent representation in image synthesis.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [28] [Words That Make Language Models Perceive](https://arxiv.org/abs/2510.02425)
*Sophie L. Wang,Phillip Isola,Brian Cheung*

Main category: cs.CL

TL;DR: 通过感官提示（如'看'或'听'）可以激活纯文本训练的大语言模型中的多模态表征能力，使其与专业视觉和音频编码器的表征对齐。


<details>
  <summary>Details</summary>
Motivation: 探索纯文本训练的LLMs是否包含隐式的多模态结构，以及是否可以通过感官提示来激活这些潜在表征。

Method: 使用感官提示（如'看'或'听'）来引导模型，让模型在生成下一个token时表现得好像基于潜在的视觉或听觉证据。

Result: 轻量级的提示工程可以可靠地激活纯文本训练LLMs中与模态相适应的表征。

Conclusion: 纯文本训练的LLMs确实包含隐式的多模态结构，通过适当的感官提示可以激活这些表征能力。

Abstract: Large language models (LLMs) trained purely on text ostensibly lack any direct perceptual experience, yet their internal representations are implicitly shaped by multimodal regularities encoded in language. We test the hypothesis that explicit sensory prompting can surface this latent structure, bringing a text-only LLM into closer representational alignment with specialist vision and audio encoders. When a sensory prompt tells the model to 'see' or 'hear', it cues the model to resolve its next-token predictions as if they were conditioned on latent visual or auditory evidence that is never actually supplied. Our findings reveal that lightweight prompt engineering can reliably activate modality-appropriate representations in purely text-trained LLMs.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [29] [Learning a distance measure from the information-estimation geometry of data](https://arxiv.org/abs/2510.02514)
*Guy Ohayon,Pierre-Etienne H. Fiquet,Florentin Guth,Jona Ballé,Eero P. Simoncelli*

Main category: eess.IV

TL;DR: 提出了一种基于信息论和估计理论的新型距离度量IEM，通过比较信号在不同噪声水平下的去噪误差向量来定义距离，能够适应复杂分布的几何结构。


<details>
  <summary>Details</summary>
Motivation: 传统距离度量无法很好地适应复杂概率分布的几何结构，需要一种能够从数据分布中学习并适应其几何特性的距离度量方法。

Method: 基于信息论与估计理论的基本关系，通过比较信号在不同噪声振幅下的最优去噪器误差向量来定义IEM，使用学习到的去噪器计算该度量。

Result: 在ImageNet数据库上学习的IEM在预测人类感知判断方面，与最先进的监督图像质量度量方法竞争或表现更优。

Conclusion: IEM是一种有效的全局度量，能够适应复杂分布的几何结构，在图像质量评估等应用中展现出优越性能。

Abstract: We introduce the Information-Estimation Metric (IEM), a novel form of distance function derived from an underlying continuous probability density over a domain of signals. The IEM is rooted in a fundamental relationship between information theory and estimation theory, which links the log-probability of a signal with the errors of an optimal denoiser, applied to noisy observations of the signal. In particular, the IEM between a pair of signals is obtained by comparing their denoising error vectors over a range of noise amplitudes. Geometrically, this amounts to comparing the score vector fields of the blurred density around the signals over a range of blur levels. We prove that the IEM is a valid global metric and derive a closed-form expression for its local second-order approximation, which yields a Riemannian metric. For Gaussian-distributed signals, the IEM coincides with the Mahalanobis distance. But for more complex distributions, it adapts, both locally and globally, to the geometry of the distribution. In practice, the IEM can be computed using a learned denoiser (analogous to generative diffusion models) and solving a one-dimensional integral. To demonstrate the value of our framework, we learn an IEM on the ImageNet database. Experiments show that this IEM is competitive with or outperforms state-of-the-art supervised image quality metrics in predicting human perceptual judgments.

</details>


### [30] [Image Enhancement Based on Pigment Representation](https://arxiv.org/abs/2510.02713)
*Se-Ho Lee,Keunsoo Ko,Seung-Wook Kim*

Main category: eess.IV

TL;DR: 提出了一种基于颜料表示的新型高效图像增强方法，通过将RGB颜色转换为高维颜料特征空间，动态适应输入内容，实现卓越的图像增强性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法局限于预定义色彩空间（如RGB），无法动态适应输入内容。本文旨在开发一种更灵活、表达能力更强的图像增强方法。

Method: 将输入RGB颜色转换为高维颜料表示，在颜料空间中进行单独重投影和混合，然后转换回RGB生成增强图像。转换和重投影参数由视觉编码器根据输入图像内容自适应估计。

Result: 在图像修饰和色调映射等图像增强任务中，该方法优于现有最先进方法，同时保持较低的计算复杂度和较小的模型尺寸。

Conclusion: 颜料表示方法为图像增强提供了适应性和表达能力，在保持效率的同时实现了卓越的性能。

Abstract: This paper presents a novel and efficient image enhancement method based on pigment representation. Unlike conventional methods where the color transformation is restricted to pre-defined color spaces like RGB, our method dynamically adapts to input content by transforming RGB colors into a high-dimensional feature space referred to as \textit{pigments}. The proposed pigment representation offers adaptability and expressiveness, achieving superior image enhancement performance. The proposed method involves transforming input RGB colors into high-dimensional pigments, which are then reprojected individually and blended to refine and aggregate the information of the colors in pigment spaces. Those pigments are then transformed back into RGB colors to generate an enhanced output image. The transformation and reprojection parameters are derived from the visual encoder which adaptively estimates such parameters based on the content in the input image. Extensive experimental results demonstrate the superior performance of the proposed method over state-of-the-art methods in image enhancement tasks, including image retouching and tone mapping, while maintaining relatively low computational complexity and small model size.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [31] [Dale meets Langevin: A Multiplicative Denoising Diffusion Model](https://arxiv.org/abs/2510.02730)
*Nishanth Shetty,Madhava Prasath,Chandra Sekhar Seelamantula*

Main category: cs.LG

TL;DR: 该论文提出了一种基于几何布朗运动的生物启发式生成模型，通过乘性更新方案实现图像生成，将指数梯度下降与分数匹配相结合。


<details>
  <summary>Details</summary>
Motivation: 标准梯度下降与生物学习机制不一致，受Dale定律启发，研究生物启发的学习技术，特别是基于几何布朗运动的乘性更新方案。

Method: 从几何布朗运动的随机微分方程出发，离散化反向时间SDE得到乘性更新规则，提出乘性去噪分数匹配形式化方法。

Result: 在MNIST、Fashion MNIST和Kuzushiji数据集上展示了新方案的生成能力，权重呈对数正态分布。

Conclusion: 这是首个基于几何布朗运动、采用乘性更新的生物启发式生成模型，为生物启发的机器学习提供了新方向。

Abstract: Gradient descent has proven to be a powerful and effective technique for optimization in numerous machine learning applications. Recent advances in computational neuroscience have shown that learning in standard gradient descent optimization formulation is not consistent with learning in biological systems. This has opened up interesting avenues for building biologically inspired learning techniques. One such approach is inspired by Dale's law, which states that inhibitory and excitatory synapses do not swap roles during the course of learning. The resulting exponential gradient descent optimization scheme leads to log-normally distributed synaptic weights. Interestingly, the density that satisfies the Fokker-Planck equation corresponding to the stochastic differential equation (SDE) with geometric Brownian motion (GBM) is the log-normal density. Leveraging this connection, we start with the SDE governing geometric Brownian motion, and show that discretizing the corresponding reverse-time SDE yields a multiplicative update rule, which surprisingly, coincides with the sampling equivalent of the exponential gradient descent update founded on Dale's law. Furthermore, we propose a new formalism for multiplicative denoising score-matching, subsuming the loss function proposed by Hyvaerinen for non-negative data. Indeed, log-normally distributed data is positive and the proposed score-matching formalism turns out to be a natural fit. This allows for training of score-based models for image data and results in a novel multiplicative update scheme for sample generation starting from a log-normal density. Experimental results on MNIST, Fashion MNIST, and Kuzushiji datasets demonstrate generative capability of the new scheme. To the best of our knowledge, this is the first instance of a biologically inspired generative model employing multiplicative updates, founded on geometric Brownian motion.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [32] [SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting](https://arxiv.org/abs/2510.02469)
*Sung-Yeon Park,Adam Lee,Juanwu Lu,Can Cui,Luyang Jiang,Rohit Gupta,Kyungtae Han,Ahmadreza Moradipari,Ziran Wang*

Main category: cs.RO

TL;DR: SIMSplat是一个基于语言对齐高斯泼溅的预测性驾驶场景编辑器，支持通过自然语言提示直观地操作驾驶场景，能够添加新对象、修改车辆和行人轨迹，并集成多智能体运动预测来生成逼真的交互。


<details>
  <summary>Details</summary>
Motivation: 现有的驾驶场景编辑框架由于编辑能力有限，难以高效生成逼真场景。需要一种能够直观操作且支持精确编辑的方法。

Method: 使用语言对齐的高斯泼溅技术，将自然语言与高斯重建场景对齐，支持直接查询道路对象。结合多智能体运动预测进行预测性路径优化，实现逼真的交互生成。

Result: 在Waymo数据集上的实验表明，SIMSplat具有广泛的编辑能力和跨场景适应性。

Conclusion: SIMSplat提供了一个直观且功能强大的驾驶场景编辑解决方案，能够生成逼真的交互场景，为自动驾驶模拟提供了有效工具。

Abstract: Driving scene manipulation with sensor data is emerging as a promising alternative to traditional virtual driving simulators. However, existing frameworks struggle to generate realistic scenarios efficiently due to limited editing capabilities. To address these challenges, we present SIMSplat, a predictive driving scene editor with language-aligned Gaussian splatting. As a language-controlled editor, SIMSplat enables intuitive manipulation using natural language prompts. By aligning language with Gaussian-reconstructed scenes, it further supports direct querying of road objects, allowing precise and flexible editing. Our method provides detailed object-level editing, including adding new objects and modifying the trajectories of both vehicles and pedestrians, while also incorporating predictive path refinement through multi-agent motion prediction to generate realistic interactions among all agents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's extensive editing capabilities and adaptability across a wide range of scenarios. Project page: https://sungyeonparkk.github.io/simsplat/

</details>
