<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 5]
- [cs.CV](#cs.CV) [Total: 19]
- [eess.IV](#eess.IV) [Total: 4]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [PreSem-Surf: RGB-D Surface Reconstruction with Progressive Semantic Modeling and SG-MLP Pre-Rendering Mechanism](https://arxiv.org/abs/2508.13228)
*Yuyan Ye,Hang Xu,Yanghang Huang,Jiali Huang,Qian Weng*

Main category: cs.GR

TL;DR: PreSem-Surf是一种基于NeRF的优化方法，通过整合RGB、深度和语义信息，在短时间内从RGB-D序列重建高质量场景表面。


<details>
  <summary>Details</summary>
Motivation: 现有的NeRF方法在场景表面重建方面存在效率和质量问题，需要一种能够快速且高质量重建的方法，同时充分利用RGB-D序列中的多模态信息。

Method: 提出SG-MLP采样结构与PR-MLP结合的体素预渲染方法，采用渐进式语义建模提取多精度语义信息，早期捕获场景相关信息并更好地区分噪声与局部细节。

Result: 在7个合成场景的6个评估指标上，PreSem-Surf在C-L1、F-score和IoU方面表现最佳，在NC、准确性和完整性方面保持竞争力。

Conclusion: PreSem-Surf方法在场景表面重建方面表现出高效性和实用性，通过多模态信息融合和渐进式语义建模显著提升了重建质量和效率。

Abstract: This paper proposes PreSem-Surf, an optimized method based on the Neural Radiance Field (NeRF) framework, capable of reconstructing high-quality scene surfaces from RGB-D sequences in a short time. The method integrates RGB, depth, and semantic information to improve reconstruction performance. Specifically, a novel SG-MLP sampling structure combined with PR-MLP (Preconditioning Multilayer Perceptron) is introduced for voxel pre-rendering, allowing the model to capture scene-related information earlier and better distinguish noise from local details. Furthermore, progressive semantic modeling is adopted to extract semantic information at increasing levels of precision, reducing training time while enhancing scene understanding. Experiments on seven synthetic scenes with six evaluation metrics show that PreSem-Surf achieves the best performance in C-L1, F-score, and IoU, while maintaining competitive results in NC, Accuracy, and Completeness, demonstrating its effectiveness and practical applicability.

</details>


### [2] [Sparse, Geometry- and Material-Aware Bases for Multilevel Elastodynamic Simulation](https://arxiv.org/abs/2508.13386)
*Ty Trusty,David I. W. Levin,Danny M. Kaufman*

Main category: cs.GR

TL;DR: 多级弹性动力学时间步汇母器，通过基于几何和材料的稀疏基构建方法，在保持IPC方法稳健性的同时实现较快的模拟速度


<details>
  <summary>Details</summary>
Motivation: 解决IPC模拟中直接汇母器计算费用高的问题，在保留复杂几何、异质材料分布和高分辨率输入的同时提高计算效率

Method: 提出新题的稀疏几何-材料感知基础构建方法，允许使用快速预处理共轭梯度汇母器替代稀疏直接汇母器

Result: 在相同硬件上达到最多13倍速度提升，每时间步相对位移错误仅约1%，视觉效果与标准IPC方法无法区分

Conclusion: 该方法在保持高治真度和稳健性的前提下，显著提高了IPC模拟的计算效率，为复杂异质材料的快速模拟提供了有效解决方案

Abstract: We present a multi-level elastodynamics timestep solver for accelerating incremental potential contact (IPC) simulations. Our method retains the robustness of gold standard IPC in the face of intricate geometry, complex heterogeneous material distributions and high resolution input data without sacrificing visual fidelity (per-timestep relative displacement error of $\approx1\%$). The success of our method is enabled by a novel, sparse, geometry- and material-aware basis construction method which allows for the use of fast preconditioned conjugate gradient solvers (in place of a sparse direct solver), but without suffering convergence issues due to stiff or heterogeneous materials. The end result is a solver that produces results visually indistinguishable and quantitatively very close to gold-standard IPC methods but up to $13\times$ faster on identical hardware.

</details>


### [3] [Eliminating Rasterization: Direct Vector Floor Plan Generation with DiffPlanner](https://arxiv.org/abs/2508.13738)
*Shidong Wang,Renato Pajarola*

Main category: cs.GR

TL;DR: DiffPlanner是一个直接在向量空间操作的Transformer条件扩散模型，用于边界约束的平面图生成，避免了传统基于图像方法的转换损失问题


<details>
  <summary>Details</summary>
Motivation: 解决现有基于学习的平面图生成方法需要将向量数据转换为栅格图像再转换回来的复杂流程，这种流程会导致信息损失和精度问题

Method: 提出基于Transformer的条件扩散模型，集成对齐机制来匹配设计师的迭代设计过程，直接在向量空间处理复杂数据

Result: 在定量比较、定性评估、消融实验和感知研究中都优于现有最先进方法，生成质量更高且更接近真实结果

Conclusion: DiffPlanner在创意阶段生成平面图和气泡图方面表现出色，为用户提供更多可控性，并能产生更高质量的结果

Abstract: The boundary-constrained floor plan generation problem aims to generate the topological and geometric properties of a set of rooms within a given boundary. Recently, learning-based methods have made significant progress in generating realistic floor plans. However, these methods involve a workflow of converting vector data into raster images, using image-based generative models, and then converting the results back into vector data. This process is complex and redundant, often resulting in information loss. Raster images, unlike vector data, cannot scale without losing detail and precision. To address these issues, we propose a novel deep learning framework called DiffPlanner for boundary-constrained floor plan generation, which operates entirely in vector space. Our framework is a Transformer-based conditional diffusion model that integrates an alignment mechanism in training, aligning the optimization trajectory of the model with the iterative design processes of designers. This enables our model to handle complex vector data, better fit the distribution of the predicted targets, accomplish the challenging task of floor plan layout design, and achieve user-controllable generation. We conduct quantitative comparisons, qualitative evaluations, ablation experiments, and perceptual studies to evaluate our method. Extensive experiments demonstrate that DiffPlanner surpasses existing state-of-the-art methods in generating floor plans and bubble diagrams in the creative stages, offering more controllability to users and producing higher-quality results that closely match the ground truths.

</details>


### [4] [Sketch3DVE: Sketch-based 3D-Aware Scene Video Editing](https://arxiv.org/abs/2508.13797)
*Feng-Lin Liu,Shi-Yang Li,Yan-Pei Cao,Hongbo Fu,Lin Gao*

Main category: cs.GR

TL;DR: Sketch3DVE是一个基于草图的3D感知视频编辑方法，能够处理大视角变化的视频，通过3D点云编辑和深度图表示实现精确的几何控制，生成与原始视频一致的编辑结果。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法在风格迁移和外观修改方面表现良好，但在处理大视角变化（如大角度相机旋转或缩放）时的3D场景结构内容编辑仍然具有挑战性，需要解决新视角内容生成、未编辑区域保持和稀疏2D输入到3D视频输出的转换问题。

Method: 使用图像编辑方法生成首帧编辑结果并传播到后续帧；利用草图进行精确几何控制；通过密集立体方法估计点云和相机参数；提出基于深度图的点云编辑方法；引入3D感知掩码传播策略和使用视频扩散模型生成真实编辑视频。

Result: 大量实验证明Sketch3DVE在视频编辑方面的优越性，能够有效处理大视角变化的视频编辑任务。

Conclusion: Sketch3DVE成功解决了大视角变化视频的3D结构编辑挑战，通过3D感知方法和点云编辑技术实现了精确的视频内容操控，为视频编辑领域提供了新的解决方案。

Abstract: Recent video editing methods achieve attractive results in style transfer or appearance modification. However, editing the structural content of 3D scenes in videos remains challenging, particularly when dealing with significant viewpoint changes, such as large camera rotations or zooms. Key challenges include generating novel view content that remains consistent with the original video, preserving unedited regions, and translating sparse 2D inputs into realistic 3D video outputs. To address these issues, we propose Sketch3DVE, a sketch-based 3D-aware video editing method to enable detailed local manipulation of videos with significant viewpoint changes. To solve the challenge posed by sparse inputs, we employ image editing methods to generate edited results for the first frame, which are then propagated to the remaining frames of the video. We utilize sketching as an interaction tool for precise geometry control, while other mask-based image editing methods are also supported. To handle viewpoint changes, we perform a detailed analysis and manipulation of the 3D information in the video. Specifically, we utilize a dense stereo method to estimate a point cloud and the camera parameters of the input video. We then propose a point cloud editing approach that uses depth maps to represent the 3D geometry of newly edited components, aligning them effectively with the original 3D scene. To seamlessly merge the newly edited content with the original video while preserving the features of unedited regions, we introduce a 3D-aware mask propagation strategy and employ a video diffusion model to produce realistic edited videos. Extensive experiments demonstrate the superiority of Sketch3DVE in video editing. Homepage and code: http://http://geometrylearning.com/Sketch3DVE/

</details>


### [5] [Is-NeRF: In-scattering Neural Radiance Field for Blurred Images](https://arxiv.org/abs/2508.13808)
*Nan Luo,Chenglin Ye,Jiaxu Li,Gang Liu,Bo Wan,Di Wang,Lupeng Liu,Jun Xiao*

Main category: cs.GR

TL;DR: 通过显式光路径建模和散射感知渲染流水线，Is-NeRF能够从动态模糊图片中恢复高保真场景表示，解决传统NeRF在处理运动模糊时的几何歧义问题


<details>
  <summary>Details</summary>
Motivation: 传统NeRF采用直线体积渲染，无法处理复杂光路径场景，在处理运动模糊图像时导致几何歧义

Method: 通过内散射表示统一六种常见光传播现象，建立散射感知体积渲染流水线；采用自适应学习策略自主确定散射方向和采样间隔；联合优化NeRF参数、散射参数和相机运动

Result: 在复杂实际场景中表现优异，超越现有最佳方法，能够生成具有准确几何细节的高保真图像

Conclusion: Is-NeRF通过显式光路径建模有效解决了动态模糊图像的场景重建问题，为复杂光学环境下的高保真新视角合成提供了有效解决方案

Abstract: Neural Radiance Fields (NeRF) has gained significant attention for its prominent implicit 3D representation and realistic novel view synthesis capabilities. Available works unexceptionally employ straight-line volume rendering, which struggles to handle sophisticated lightpath scenarios and introduces geometric ambiguities during training, particularly evident when processing motion-blurred images. To address these challenges, this work proposes a novel deblur neural radiance field, Is-NeRF, featuring explicit lightpath modeling in real-world environments. By unifying six common light propagation phenomena through an in-scattering representation, we establish a new scattering-aware volume rendering pipeline adaptable to complex lightpaths. Additionally, we introduce an adaptive learning strategy that enables autonomous determining of scattering directions and sampling intervals to capture finer object details. The proposed network jointly optimizes NeRF parameters, scattering parameters, and camera motions to recover fine-grained scene representations from blurry images. Comprehensive evaluations demonstrate that it effectively handles complex real-world scenarios, outperforming state-of-the-art approaches in generating high-fidelity images with accurate geometric details.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [6] [GaitCrafter: Diffusion Model for Biometric Preserving Gait Synthesis](https://arxiv.org/abs/2508.13300)
*Sirshapan Mitra,Yogesh S. Rawat*

Main category: cs.CV

TL;DR: GaitCrafter是一个基于扩散模型的步态序列生成框架，能够在轮廓域合成逼真的步态数据，支持多种条件控制，并能生成新身份数据以保护隐私。


<details>
  <summary>Details</summary>
Motivation: 步态识别面临缺乏大规模标注数据集和收集多样化步态样本的困难，同时需要保护个人隐私。

Method: 训练专门的视频扩散模型，使用步态轮廓数据生成时序一致且保持身份特征的步态序列，支持服装、携带物品、视角等多种条件控制，并能通过身份嵌入插值生成新身份。

Result: 将GaitCrafter生成的合成样本加入步态识别流程能提升性能，特别是在挑战性条件下；生成的新身份数据具有独特且一致的步态模式。

Conclusion: 这项工作在利用扩散模型进行高质量、可控且隐私保护的步态数据生成方面迈出了重要一步。

Abstract: Gait recognition is a valuable biometric task that enables the identification of individuals from a distance based on their walking patterns. However, it remains limited by the lack of large-scale labeled datasets and the difficulty of collecting diverse gait samples for each individual while preserving privacy. To address these challenges, we propose GaitCrafter, a diffusion-based framework for synthesizing realistic gait sequences in the silhouette domain. Unlike prior works that rely on simulated environments or alternative generative models, GaitCrafter trains a video diffusion model from scratch, exclusively on gait silhouette data. Our approach enables the generation of temporally consistent and identity-preserving gait sequences. Moreover, the generation process is controllable-allowing conditioning on various covariates such as clothing, carried objects, and view angle. We show that incorporating synthetic samples generated by GaitCrafter into the gait recognition pipeline leads to improved performance, especially under challenging conditions. Additionally, we introduce a mechanism to generate novel identities-synthetic individuals not present in the original dataset-by interpolating identity embeddings. These novel identities exhibit unique, consistent gait patterns and are useful for training models while maintaining privacy of real subjects. Overall, our work takes an important step toward leveraging diffusion models for high-quality, controllable, and privacy-aware gait data generation.

</details>


### [7] [Applications of Small Language Models in Medical Imaging Classification with a Focus on Prompt Strategies](https://arxiv.org/abs/2508.13378)
*Yiting Wang,Ziwei Wang,Jiachen Zhong,Di Zhu,Weiyi Li*

Main category: cs.CV

TL;DR: 小型语言模型经过精心设计的提示策略可以在医疗影像分类任务中达到竞争性的准确度，而无需深度AI知识


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗环境中遇到计算成本高、访问性限制和数据隐私问题，需要探索更简单易用的替代方案

Method: 使用NIH胸部X光数据集，测试多个小型语言模型在胸部X光位置分类任务上的表现，比较三种提示策略：基准指令、增量总结提示和纠正反思提示

Result: 某些小型语言模型经过良好设计的提示策略，能够达到与大型模型竞争的准确度

Conclusion: 提示工程可以在不需要深度AI专业知识的情况下，实现小型语言模型在医疗应用中性能的显著提升

Abstract: Large language models (LLMs) have shown remarkable capabilities in natural language processing and multi-modal understanding. However, their high computational cost, limited accessibility, and data privacy concerns hinder their adoption in resource-constrained healthcare environments. This study investigates the performance of small language models (SLMs) in a medical imaging classification task, comparing different models and prompt designs to identify the optimal combination for accuracy and usability. Using the NIH Chest X-ray dataset, we evaluate multiple SLMs on the task of classifying chest X-ray positions (anteroposterior [AP] vs. posteroanterior [PA]) under three prompt strategies: baseline instruction, incremental summary prompts, and correction-based reflective prompts. Our results show that certain SLMs achieve competitive accuracy with well-crafted prompts, suggesting that prompt engineering can substantially enhance SLM performance in healthcare applications without requiring deep AI expertise from end users.

</details>


### [8] [MINR: Efficient Implicit Neural Representations for Multi-Image Encoding](https://arxiv.org/abs/2508.13471)
*Wenyong Zhou,Taiqiang Wu,Zhengwu Liu,Yuxin Cheng,Chen Zhang,Ngai Wong*

Main category: cs.CV

TL;DR: MINR通过共享中间层来高效编码多图像，在保持性能的同时减少60%参数，可扩展到处理100张图像。


<details>
  <summary>Details</summary>
Motivation: 传统的隐式神经表示(INR)为每张图像使用单独的神经网络，导致编码多图像时计算和存储效率低下。研究发现不同INR的中间层权重分布高度相似，这启发了共享中间层的思路。

Method: 提出MINR框架：1）共享多个图像的中间层；2）保持输入和输出层为图像特定；3）为每张图像设计额外的投影层来捕获独特特征。

Result: 在图像重建和超分辨率任务中，MINR节省了60%的参数同时保持可比性能。可扩展到处理100张图像，平均PSNR达到34dB。

Conclusion: MINR通过层共享机制有效解决了多图像INR编码的效率问题，具有很好的扩展性和鲁棒性。

Abstract: Implicit Neural Representations (INRs) aim to parameterize discrete signals through implicit continuous functions. However, formulating each image with a separate neural network~(typically, a Multi-Layer Perceptron (MLP)) leads to computational and storage inefficiencies when encoding multi-images. To address this issue, we propose MINR, sharing specific layers to encode multi-image efficiently. We first compare the layer-wise weight distributions for several trained INRs and find that corresponding intermediate layers follow highly similar distribution patterns. Motivated by this, we share these intermediate layers across multiple images while preserving the input and output layers as input-specific. In addition, we design an extra novel projection layer for each image to capture its unique features. Experimental results on image reconstruction and super-resolution tasks demonstrate that MINR can save up to 60\% parameters while maintaining comparable performance. Particularly, MINR scales effectively to handle 100 images, maintaining an average peak signal-to-noise ratio (PSNR) of 34 dB. Further analysis of various backbones proves the robustness of the proposed MINR.

</details>


### [9] [AIM 2025 challenge on Inverse Tone Mapping Report: Methods and Results](https://arxiv.org/abs/2508.13479)
*Chao Wang,Francesco Banterle,Bin Ren,Radu Timofte,Xin Lu,Yufeng Peng,Chengjie Ge,Zhijing Sun,Ziang Zhou,Zihao Li,Zishun Liao,Qiyu Kang,Xueyang Fu,Zheng-Jun Zha,Zhijing Sun,Xingbo Wang,Kean Liu,Senyan Xu,Yang Qiu,Yifan Ding,Gabriel Eilertsen,Jonas Unger,Zihao Wang,Ke Wu,Jinshan Pan,Zhen Liu,Zhongyang Li,Shuaicheng Liu,S. M Nadim Uddin*

Main category: cs.CV

TL;DR: AIM 2025逆色调映射挑战赛综述，67个团队提交319个结果，最佳PU21-PSNR达到29.22dB，为HDR重建建立新基准


<details>
  <summary>Details</summary>
Motivation: 推动单张LDR图像到HDR图像重建的逆色调映射算法发展，关注感知保真度和数值一致性

Method: 组织大规模挑战赛，收集67个参与者的319个有效提交，对前五名团队的方法进行详细分析

Result: 最佳性能达到PU21-PSNR 29.22dB，识别出提升HDR重建质量的创新策略

Conclusion: 建立了强有力的基准来指导未来逆色调映射研究，推动了HDR重建技术的发展

Abstract: This paper presents a comprehensive review of the AIM 2025 Challenge on Inverse Tone Mapping (ITM). The challenge aimed to push forward the development of effective ITM algorithms for HDR image reconstruction from single LDR inputs, focusing on perceptual fidelity and numerical consistency. A total of \textbf{67} participants submitted \textbf{319} valid results, from which the best five teams were selected for detailed analysis. This report consolidates their methodologies and performance, with the lowest PU21-PSNR among the top entries reaching 29.22 dB. The analysis highlights innovative strategies for enhancing HDR reconstruction quality and establishes strong benchmarks to guide future research in inverse tone mapping.

</details>


### [10] [CORENet: Cross-Modal 4D Radar Denoising Network with LiDAR Supervision for Autonomous Driving](https://arxiv.org/abs/2508.13485)
*Fuyang Liu,Jilin Mei,Fangyuan Mao,Chen Min,Yan Xing,Yu Hu*

Main category: cs.CV

TL;DR: CORENet是一个基于4D雷达的跨模态去噪框架，利用LiDAR监督从原始雷达数据中识别噪声模式并提取特征，提升目标检测性能，在训练时使用LiDAR监督但推理时完全基于雷达。


<details>
  <summary>Details</summary>
Motivation: 4D雷达在恶劣天气条件下具有鲁棒性并能提供丰富的空间信息，但其点云数据稀疏且噪声严重，给有效感知带来巨大挑战。

Method: 提出CORENet跨模态去噪框架，利用LiDAR监督识别噪声模式并提取判别性特征，采用即插即用架构可无缝集成到基于体素的检测框架中。

Result: 在具有高噪声水平的Dual-Radar数据集上的广泛评估表明，该框架有效提升了检测鲁棒性，相比现有主流方法实现了更优越的性能。

Conclusion: CORENet通过跨模态监督有效解决了4D雷达点云稀疏噪声问题，在保持推理时雷达独立运行的同时显著提升了目标检测性能。

Abstract: 4D radar-based object detection has garnered great attention for its robustness in adverse weather conditions and capacity to deliver rich spatial information across diverse driving scenarios. Nevertheless, the sparse and noisy nature of 4D radar point clouds poses substantial challenges for effective perception. To address the limitation, we present CORENet, a novel cross-modal denoising framework that leverages LiDAR supervision to identify noise patterns and extract discriminative features from raw 4D radar data. Designed as a plug-and-play architecture, our solution enables seamless integration into voxel-based detection frameworks without modifying existing pipelines. Notably, the proposed method only utilizes LiDAR data for cross-modal supervision during training while maintaining full radar-only operation during inference. Extensive evaluation on the challenging Dual-Radar dataset, which is characterized by elevated noise level, demonstrates the effectiveness of our framework in enhancing detection robustness. Comprehensive experiments validate that CORENet achieves superior performance compared to existing mainstream approaches.

</details>


### [11] [AdaptiveAE: An Adaptive Exposure Strategy for HDR Capturing in Dynamic Scenes](https://arxiv.org/abs/2508.13503)
*Tianyi Xu,Fan Zhang,Boxin Shi,Tianfan Xue,Yujin Wang*

Main category: cs.CV

TL;DR: 基于强化学习的自适应曝光参数优化方法，通过模拟运动模糊和噪声来最大化HDR重建质量


<details>
  <summary>Details</summary>
Motivation: 现有HDR技术忽视了快门速度与ISO的复杂交互作用，以及动态场景中运动模糊的影响，导致高ISO产生噪声、长快门导致模糊

Method: 使用强化学习优化快门速度和ISO组合选择，集成包含运动模糊和噪声模拟的图像合成流程，利用语义信息和曝光直方图

Result: 在多个数据集上达到最先进性能，能够根据用户定义的曝光时间预算选择最优曝光策略

Conclusion: AdaptiveAE能够有效处理动态场景中的HDR拍摄问题，提供比传统方法更好的曝光调度方案

Abstract: Mainstream high dynamic range imaging techniques typically rely on fusing multiple images captured with different exposure setups (shutter speed and ISO). A good balance between shutter speed and ISO is crucial for achieving high-quality HDR, as high ISO values introduce significant noise, while long shutter speeds can lead to noticeable motion blur. However, existing methods often overlook the complex interaction between shutter speed and ISO and fail to account for motion blur effects in dynamic scenes.   In this work, we propose AdaptiveAE, a reinforcement learning-based method that optimizes the selection of shutter speed and ISO combinations to maximize HDR reconstruction quality in dynamic environments. AdaptiveAE integrates an image synthesis pipeline that incorporates motion blur and noise simulation into our training procedure, leveraging semantic information and exposure histograms. It can adaptively select optimal ISO and shutter speed sequences based on a user-defined exposure time budget, and find a better exposure schedule than traditional solutions. Experimental results across multiple datasets demonstrate that it achieves the state-of-the-art performance.

</details>


### [12] [2D Gaussians Meet Visual Tokenizer](https://arxiv.org/abs/2508.13515)
*Yiang Shi,Xiaoyang Guo,Wei Yin,Mingkai Jia,Qian Zhang,Xiaolin Hu,Wenyu Liu,Xinggang Wan*

Main category: cs.CV

TL;DR: VGQ是一种新型图像标记化框架，通过将2D高斯分布集成到视觉码本量化中，显式增强结构建模能力，显著提升重建质量


<details>
  <summary>Details</summary>
Motivation: 现有基于量化的标记器（如VQ-GAN）主要关注纹理和颜色等外观特征，往往忽略几何结构，因为其基于补丁的设计难以有效建模结构化视觉信息

Method: 提出视觉高斯量化（VGQ）框架，将图像潜在编码为2D高斯分布，直接建模位置、旋转和尺度等结构相关参数，通过增加2D高斯密度来平衡标记效率和视觉丰富度

Result: 在ImageNet 256x256基准测试中，VGQ达到rFID 1.00的重建质量，通过增加高斯密度进一步获得rFID 0.556和PSNR 24.93的SOTA性能，大幅超越现有方法

Conclusion: VGQ通过显式建模几何结构参数，有效解决了传统量化方法的局限性，为图像生成提供了更丰富的结构化表示，代码即将发布

Abstract: The image tokenizer is a critical component in AR image generation, as it determines how rich and structured visual content is encoded into compact representations. Existing quantization-based tokenizers such as VQ-GAN primarily focus on appearance features like texture and color, often neglecting geometric structures due to their patch-based design. In this work, we explored how to incorporate more visual information into the tokenizer and proposed a new framework named Visual Gaussian Quantization (VGQ), a novel tokenizer paradigm that explicitly enhances structural modeling by integrating 2D Gaussians into traditional visual codebook quantization frameworks. Our approach addresses the inherent limitations of naive quantization methods such as VQ-GAN, which struggle to model structured visual information due to their patch-based design and emphasis on texture and color. In contrast, VGQ encodes image latents as 2D Gaussian distributions, effectively capturing geometric and spatial structures by directly modeling structure-related parameters such as position, rotation and scale. We further demonstrate that increasing the density of 2D Gaussians within the tokens leads to significant gains in reconstruction fidelity, providing a flexible trade-off between token efficiency and visual richness. On the ImageNet 256x256 benchmark, VGQ achieves strong reconstruction quality with an rFID score of 1.00. Furthermore, by increasing the density of 2D Gaussians within the tokens, VGQ gains a significant boost in reconstruction capability and achieves a state-of-the-art reconstruction rFID score of 0.556 and a PSNR of 24.93, substantially outperforming existing methods. Codes will be released soon.

</details>


### [13] [Evaluating Open-Source Vision Language Models for Facial Emotion Recognition against Traditional Deep Learning Models](https://arxiv.org/abs/2508.13524)
*Vamsi Krishna Mulukutla,Sai Supriya Pavarala,Srinivasa Raju Rudraraju,Sridevi Bonthu*

Main category: cs.CV

TL;DR: 这篇论文首次对比了开源视觉-语言模型和传统深度学习模型在面部情感识别任务上的表现，发现传统模型在低质量数据集上显著更优。


<details>
  <summary>Details</summary>
Motivation: 面部情感识别在人机交互和心理健康诊断中具有重要应用价值，需要评估最新的视觉-语言模型在这些具有挑战性的任务上的表现。

Method: 使用FER-2013数据集（包含35,887张低分辨率灰度图片），对比Phi-3.5 Vision、CLIP等VLMs与VGG19、ResNet-50、EfficientNet-B0等传统模型。提出了整合GFPGAN图像恢复的新流程来处理数据噪声问题。

Result: 传统模型显著更优：EfficientNet-B0准确率达86.44%，ResNet-50为85.72%，而CLIP和Phi-3.5 Vision分别只有64.07%和51.66%。还提供了详细的计算成本分析。

Conclusion: 视觉-语言模型在低质量视觉任务中存在显著局限性，需要适应噪声环境。该研究为情感识别领域提供了可复现的基准测试。

Abstract: Facial Emotion Recognition (FER) is crucial for applications such as human-computer interaction and mental health diagnostics. This study presents the first empirical comparison of open-source Vision-Language Models (VLMs), including Phi-3.5 Vision and CLIP, against traditional deep learning models VGG19, ResNet-50, and EfficientNet-B0 on the challenging FER-2013 dataset, which contains 35,887 low-resolution grayscale images across seven emotion classes. To address the mismatch between VLM training assumptions and the noisy nature of FER data, we introduce a novel pipeline that integrates GFPGAN-based image restoration with FER evaluation. Results show that traditional models, particularly EfficientNet-B0 (86.44%) and ResNet-50 (85.72%), significantly outperform VLMs like CLIP (64.07%) and Phi-3.5 Vision (51.66%), highlighting the limitations of VLMs in low-quality visual tasks. In addition to performance evaluation using precision, recall, F1-score, and accuracy, we provide a detailed computational cost analysis covering preprocessing, training, inference, and evaluation phases, offering practical insights for deployment. This work underscores the need for adapting VLMs to noisy environments and provides a reproducible benchmark for future research in emotion recognition.

</details>


### [14] [EAvatar: Expression-Aware Head Avatar Reconstruction with Generative Geometry Priors](https://arxiv.org/abs/2508.13537)
*Shikun Zhang,Cunjian Chen,Yiqun Wang,Qiuhong Ke,Yong Li*

Main category: cs.CV

TL;DR: 提出EAvatar框架，基于3D高斯泼溅技术，通过稀疏表情控制机制和高质量3D先验，实现高保真头部重建，解决了现有方法在细粒度表情捕捉和局部纹理连续性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法在捕捉细粒度面部表情和保持局部纹理连续性方面存在显著挑战，特别是在高度可变形区域。需要一种既能准确建模局部变形又能保持纹理连续性的头部重建方法。

Method: 提出EAvatar框架：1）稀疏表情控制机制，使用少量关键高斯影响邻近高斯变形；2）利用预训练生成模型的高质量3D先验提供可靠面部几何结构指导；3）结合3D高斯泼溅技术实现实时渲染。

Result: 实验结果表明，该方法能够产生更准确和视觉连贯的头部重建结果，具有改进的表情可控性和细节保真度，在收敛稳定性和形状准确性方面都有提升。

Conclusion: EAvatar框架通过创新的稀疏控制机制和3D先验利用，成功解决了3DGS基头部重建中的关键挑战，为AR/VR、游戏和多媒体内容创作提供了更高质量的头部avatar重建方案。

Abstract: High-fidelity head avatar reconstruction plays a crucial role in AR/VR, gaming, and multimedia content creation. Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated effectiveness in modeling complex geometry with real-time rendering capability and are now widely used in high-fidelity head avatar reconstruction tasks. However, existing 3DGS-based methods still face significant challenges in capturing fine-grained facial expressions and preserving local texture continuity, especially in highly deformable regions. To mitigate these limitations, we propose a novel 3DGS-based framework termed EAvatar for head reconstruction that is both expression-aware and deformation-aware. Our method introduces a sparse expression control mechanism, where a small number of key Gaussians are used to influence the deformation of their neighboring Gaussians, enabling accurate modeling of local deformations and fine-scale texture transitions. Furthermore, we leverage high-quality 3D priors from pretrained generative models to provide a more reliable facial geometry, offering structural guidance that improves convergence stability and shape accuracy during training. Experimental results demonstrate that our method produces more accurate and visually coherent head reconstructions with improved expression controllability and detail fidelity.

</details>


### [15] [Temporal-Conditional Referring Video Object Segmentation with Noise-Free Text-to-Video Diffusion Model](https://arxiv.org/abs/2508.13584)
*Ruixin Zhang,Jiaqing Fan,Yifan Liao,Qian Qiao,Fanzhang Li*

Main category: cs.CV

TL;DR: 本文提出了一种时序条件引用视频对象分割模型，通过改进分割头设计、利用文本到视频扩散模型提取特征，以及新的时序上下文掩码精炼模块，在四个公开数据集上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 目前的引用视频对象分割方法往往过于重视特征提取和时序建模，而对分割头设计的关注不够。实际上，分割头设计仍有很大的改进空间。

Method: 提出时序条件引用视频对象分割模型：1）创新整合现有分割方法提升边界分割能力；2）利用文本到视频扩散模型进行特征提取，去除传统噪声预测模块以避免噪声随机性对分割准确性的影响；3）设计时序上下文掩码精炼模块（TCMR）来克服VAE特征提取能力的局限性。

Result: 在四个公开的RVOS测试集上，该方法一致地达到了最先进的性能水平。

Conclusion: 通过重新关注分割头设计并提出创新的模型结构，本研究显著提升了引用视频对象分割的边界分割质量和整体性能，为该领域提供了有效的解决方案。

Abstract: Referring Video Object Segmentation (RVOS) aims to segment specific objects in a video according to textual descriptions. We observe that recent RVOS approaches often place excessive emphasis on feature extraction and temporal modeling, while relatively neglecting the design of the segmentation head. In fact, there remains considerable room for improvement in segmentation head design. To address this, we propose a Temporal-Conditional Referring Video Object Segmentation model, which innovatively integrates existing segmentation methods to effectively enhance boundary segmentation capability. Furthermore, our model leverages a text-to-video diffusion model for feature extraction. On top of this, we remove the traditional noise prediction module to avoid the randomness of noise from degrading segmentation accuracy, thereby simplifying the model while improving performance. Finally, to overcome the limited feature extraction capability of the VAE, we design a Temporal Context Mask Refinement (TCMR) module, which significantly improves segmentation quality without introducing complex designs. We evaluate our method on four public RVOS benchmarks, where it consistently achieves state-of-the-art performance.

</details>


### [16] [Bridging Clear and Adverse Driving Conditions](https://arxiv.org/abs/2508.13592)
*Yoel Shapiro,Yahia Showgan,Koustav Mullick*

Main category: cs.CV

TL;DR: 通过基于GAN和汽温模型的混合域适应方法，将晴天图像转换为雾雨雪夜晚图像，提升自动驾驶系统在恶劣天气下的识别性能


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在恶劣天气条件下性能显著下降，而数据集中恶劣条件数据缺乏，收集和标注成本过高

Method: 提出了模拟、GAN基础和汽温-GAN混合方法的数据生成流水线，利用现有域适应GAN并支持辅助输入，通过模拟图像提供准确监督，实际图像缩小sim2real差距

Result: 在ACDC数据集上评测，语义分割整体提升1.85%，夜间场景下提升4.62%

Conclusion: 混合方法能够有效提升自动驾驶系统在挑战性条件下的感知稳健性

Abstract: Autonomous Driving (AD) systems exhibit markedly degraded performance under adverse environmental conditions, such as low illumination and precipitation. The underrepresentation of adverse conditions in AD datasets makes it challenging to address this deficiency. To circumvent the prohibitive cost of acquiring and annotating adverse weather data, we propose a novel Domain Adaptation (DA) pipeline that transforms clear-weather images into fog, rain, snow, and nighttime images. Here, we systematically develop and evaluate several novel data-generation pipelines, including simulation-only, GAN-based, and hybrid diffusion-GAN approaches, to synthesize photorealistic adverse images from labelled clear images. We leverage an existing DA GAN, extend it to support auxiliary inputs, and develop a novel training recipe that leverages both simulated and real images. The simulated images facilitate exact supervision by providing perfectly matched image pairs, while the real images help bridge the simulation-to-real (sim2real) gap. We further introduce a method to mitigate hallucinations and artifacts in Stable-Diffusion Image-to-Image (img2img) outputs by blending them adaptively with their progenitor images. We finetune downstream models on our synthetic data and evaluate them on the Adverse Conditions Dataset with Correspondences (ACDC). We achieve 1.85 percent overall improvement in semantic segmentation, and 4.62 percent on nighttime, demonstrating the efficacy of our hybrid method for robust AD perception under challenging conditions.

</details>


### [17] [DiffIER: Optimizing Diffusion Models with Iterative Error Reduction](https://arxiv.org/abs/2508.13628)
*Ao Chen,Lihe Ding,Tianfan Xue*

Main category: cs.CV

TL;DR: 本文提出了DiffIER方法来解决扩散模型中分类器无关引导(CFG)存在的训练-推理差距问题，通过迭代误差最小化优化生成质量，在多个条件生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在条件生成中对引导权重选择高度敏感，存在训练-推理差距问题，这影响了生成质量并导致输出对引导权重敏感。

Method: 提出DiffIER优化方法，在推理阶段每一步进行迭代误差最小化来减少累积误差，这是一个即插即用的优化框架。

Result: 实验表明该方法在条件生成任务中优于基线方法，在文本到图像生成、图像超分辨率和文本到语音生成等多个领域都取得了成功。

Conclusion: DiffIER方法有效缓解了训练-推理差距问题，提高了生成质量，具有广泛的适用性和应用潜力。

Abstract: Diffusion models have demonstrated remarkable capabilities in generating high-quality samples and enhancing performance across diverse domains through Classifier-Free Guidance (CFG). However, the quality of generated samples is highly sensitive to the selection of the guidance weight. In this work, we identify a critical ``training-inference gap'' and we argue that it is the presence of this gap that undermines the performance of conditional generation and renders outputs highly sensitive to the guidance weight. We quantify this gap by measuring the accumulated error during the inference stage and establish a correlation between the selection of guidance weight and minimizing this gap. Furthermore, to mitigate this gap, we propose DiffIER, an optimization-based method for high-quality generation. We demonstrate that the accumulated error can be effectively reduced by an iterative error minimization at each step during inference. By introducing this novel plug-and-play optimization framework, we enable the optimization of errors at every single inference step and enhance generation quality. Empirical results demonstrate that our proposed method outperforms baseline approaches in conditional generation tasks. Furthermore, the method achieves consistent success in text-to-image generation, image super-resolution, and text-to-speech generation, underscoring its versatility and potential for broad applications in future research.

</details>


### [18] [OmniTry: Virtual Try-On Anything without Masks](https://arxiv.org/abs/2508.13632)
*Yutong Feng,Linlin Zhang,Hengyuan Cao,Yiming Chen,Xiaoduan Feng,Jian Cao,Yuxiong Wu,Bin Wang*

Main category: cs.CV

TL;DR: OmniTry是一个统一的虚拟试穿框架，可处理任何可穿戴物品（珠宝、配饰等），采用无掩码设置，通过两阶段训练实现更好的定位和外观保持效果。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿方法主要关注服装，缺乏对其他可穿戴物品的支持，且数据收集困难。本文旨在扩展VTON到任意可穿戴物品，解决配对数据稀缺问题。

Method: 提出两阶段训练流程：第一阶段利用大规模无配对图像训练无掩码定位模型，通过重用途修复模型在合适位置自动绘制物品；第二阶段用少量配对图像微调以保持物品外观一致性。

Result: 在包含12类常见可穿戴物品的综合基准测试中，OmniTry在物品定位和身份保持方面均优于现有方法，且第一阶段后模型即使使用少量配对样本也能快速收敛。

Conclusion: OmniTry成功扩展了虚拟试穿的应用范围，通过创新的两阶段训练方法有效解决了数据稀缺问题，为任意可穿戴物品的虚拟试穿提供了实用解决方案。

Abstract: Virtual Try-ON (VTON) is a practical and widely-applied task, for which most of existing works focus on clothes. This paper presents OmniTry, a unified framework that extends VTON beyond garment to encompass any wearable objects, e.g., jewelries and accessories, with mask-free setting for more practical application. When extending to various types of objects, data curation is challenging for obtaining paired images, i.e., the object image and the corresponding try-on result. To tackle this problem, we propose a two-staged pipeline: For the first stage, we leverage large-scale unpaired images, i.e., portraits with any wearable items, to train the model for mask-free localization. Specifically, we repurpose the inpainting model to automatically draw objects in suitable positions given an empty mask. For the second stage, the model is further fine-tuned with paired images to transfer the consistency of object appearance. We observed that the model after the first stage shows quick convergence even with few paired samples. OmniTry is evaluated on a comprehensive benchmark consisting of 12 common classes of wearable objects, with both in-shop and in-the-wild images. Experimental results suggest that OmniTry shows better performance on both object localization and ID-preservation compared with existing methods. The code, model weights, and evaluation benchmark of OmniTry will be made publicly available at https://omnitry.github.io/.

</details>


### [19] [SAGA: Learning Signal-Aligned Distributions for Improved Text-to-Image Generation](https://arxiv.org/abs/2508.13866)
*Paul Grimal,Michaël Soumm,Hervé Le Borgne,Olivier Ferret,Akihiro Sugimoto*

Main category: cs.CV

TL;DR: 通过学习高成功率分布来提升文本到图像生成的准确性，避免缺失关键元素或概念混淆


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型虽然视觉效果优秀，但经常无法准确对齐提示，导致缺失关键元素或不应该混合的概念

Method: 新题方法学习高成功率分布来条件化目标提示，在去噪过程中显式建模信号组件，提供细粒度控制以减轻过度优化和分布外偏差

Result: 大量实验证明该方法在性能上超过了当前最先进的方法

Conclusion: 该新题方法不需训练、无缝集成现有扩散和流匹配架构，支持附加条件如边框来提升空间对齐能力

Abstract: State-of-the-art text-to-image models produce visually impressive results but often struggle with precise alignment to text prompts, leading to missing critical elements or unintended blending of distinct concepts. We propose a novel approach that learns a high-success-rate distribution conditioned on a target prompt, ensuring that generated images faithfully reflect the corresponding prompts. Our method explicitly models the signal component during the denoising process, offering fine-grained control that mitigates over-optimization and out-of-distribution artifacts. Moreover, our framework is training-free and seamlessly integrates with both existing diffusion and flow matching architectures. It also supports additional conditioning modalities -- such as bounding boxes -- for enhanced spatial alignment. Extensive experiments demonstrate that our approach outperforms current state-of-the-art methods. The code is available at https://github.com/grimalPaul/gsn-factory.

</details>


### [20] [PhysGM: Large Physical Gaussian Model for Feed-Forward 4D Synthesis](https://arxiv.org/abs/2508.13911)
*Chunji Lv,Zequn Chen,Donglin Di,Weinan Zhang,Hao Li,Wei Chen,Changsheng Li*

Main category: cs.CV

TL;DR: PhysGM是一个前馈框架，从单张图像联合预测3D高斯表示和物理属性，实现即时物理模拟和高保真4D渲染，相比现有方法显著加速且效果更优


<details>
  <summary>Details</summary>
Motivation: 现有物理驱动的3D运动合成方法依赖预重建的3D高斯表示，物理集成要么使用不灵活的手工定义物理属性，要么依赖不稳定、优化密集的视频模型指导

Method: 建立基础模型联合优化高斯重建和概率物理预测，使用物理合理参考视频进行精炼，采用直接偏好优化(DPO)对齐模拟与参考视频，避免通过复杂可微分模拟和光栅化的梯度反向传播

Result: 方法在1分钟内从单张图像有效生成高保真4D模拟，相比先前工作实现显著加速，同时提供逼真的渲染结果

Conclusion: PhysGM框架成功克服了现有方法的局限性，实现了高效且高质量的物理驱动4D内容生成

Abstract: While physics-grounded 3D motion synthesis has seen significant progress, current methods face critical limitations. They typically rely on pre-reconstructed 3D Gaussian Splatting (3DGS) representations, while physics integration depends on either inflexible, manually defined physical attributes or unstable, optimization-heavy guidance from video models. To overcome these challenges, we introduce PhysGM, a feed-forward framework that jointly predicts a 3D Gaussian representation and its physical properties from a single image, enabling immediate, physical simulation and high-fidelity 4D rendering. We first establish a base model by jointly optimizing for Gaussian reconstruction and probabilistic physics prediction. The model is then refined with physically plausible reference videos to enhance both rendering fidelity and physics prediction accuracy. We adopt the Direct Preference Optimization (DPO) to align its simulations with reference videos, circumventing Score Distillation Sampling (SDS) optimization which needs back-propagating gradients through the complex differentiable simulation and rasterization. To facilitate the training, we introduce a new dataset PhysAssets of over 24,000 3D assets, annotated with physical properties and corresponding guiding videos. Experimental results demonstrate that our method effectively generates high-fidelity 4D simulations from a single image in one minute. This represents a significant speedup over prior works while delivering realistic rendering results. Our project page is at:https://hihixiaolv.github.io/PhysGM.github.io/

</details>


### [21] [Online 3D Gaussian Splatting Modeling with Novel View Selection](https://arxiv.org/abs/2508.14014)
*Byeonggwon Lee,Junkyu Park,Khang Truong Giang,Soohwan Song*

Main category: cs.CV

TL;DR: 通过适应性视图选择技术，在线从RGB帧生成更完整的3D高斯拖尾模型，解决了仅依赖关键帧导致的场景重建不完整问题


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖关键帧进行3D场景估计，无法捐捐整个场景，导致重建不完整。在线处理限制了使用大量帧或长时间迭代训练的可能性

Method: 提出适应性视图选择算法，在线分析重建质量，选择最优的非关键帧进行额外训练。结合关键帧和选择的非关键帧，从多角度精炼不完整区域。集成在线多视觉立体方法确保3D信息一致性

Result: 在复杂户外场景中表现超越现有最先进方法，实现了突出的性能，显著提升了模型的完整性

Conclusion: 该方法通过智能选择非关键帧来补充场景信息，有效解决了在线3D高斯拖尾模型构建中的完整性问题，为实时三维重建提供了更加高质量的解决方案

Abstract: This study addresses the challenge of generating online 3D Gaussian Splatting (3DGS) models from RGB-only frames. Previous studies have employed dense SLAM techniques to estimate 3D scenes from keyframes for 3DGS model construction. However, these methods are limited by their reliance solely on keyframes, which are insufficient to capture an entire scene, resulting in incomplete reconstructions. Moreover, building a generalizable model requires incorporating frames from diverse viewpoints to achieve broader scene coverage. However, online processing restricts the use of many frames or extensive training iterations. Therefore, we propose a novel method for high-quality 3DGS modeling that improves model completeness through adaptive view selection. By analyzing reconstruction quality online, our approach selects optimal non-keyframes for additional training. By integrating both keyframes and selected non-keyframes, the method refines incomplete regions from diverse viewpoints, significantly enhancing completeness. We also present a framework that incorporates an online multi-view stereo approach, ensuring consistency in 3D information throughout the 3DGS modeling process. Experimental results demonstrate that our method outperforms state-of-the-art methods, delivering exceptional performance in complex outdoor scenes.

</details>


### [22] [InfiniteTalk: Audio-driven Video Generation for Sparse-Frame Video Dubbing](https://arxiv.org/abs/2508.14033)
*Shaoshu Yang,Zhe Kong,Feng Gao,Meng Cheng,Xiangyu Liu,Yong Zhang,Zhuoliang Kang,Wenhan Luo,Xunliang Cai,Ran He,Xiaoming Wei*

Main category: cs.CV

TL;DR: 这篇论文提出了InfiniteTalk方案，通过稀疏帧视频音频同步技术解决了传统音频驱动动画只能编辑嘴部的限制，实现了全身动作的整体同步编辑。


<details>
  <summary>Details</summary>
Motivation: 传统音频驱动动画技术仅能编辑嘴部区域，导致面部表情和身体手势不协调，影响观看体验。需要解决全身动作同步的技术挑战。

Method: 提出稀疏帧视频音频同步新范式，通过保留关键帧来维持身份识别、标志性手势和摄像机轨迹。InfiniteTalk流式生成器利用时间上下文帧实现平滑转换，采用精细参考帧位置优化控制强度的采样策略。

Result: 在HDTF、CelebV-HQ和EMTD数据集上进行了全面评估，展现出领先的性能。数据指标证实了更高的视觉真实性、情感一致性和全身动作同步性。

Conclusion: InfiniteTalk成功解决了长序列音频驱动动画的技术挑战，实现了整体性的全身动作同步，为视频AIGC领域带来了重要进展。

Abstract: Recent breakthroughs in video AIGC have ushered in a transformative era for audio-driven human animation. However, conventional video dubbing techniques remain constrained to mouth region editing, resulting in discordant facial expressions and body gestures that compromise viewer immersion. To overcome this limitation, we introduce sparse-frame video dubbing, a novel paradigm that strategically preserves reference keyframes to maintain identity, iconic gestures, and camera trajectories while enabling holistic, audio-synchronized full-body motion editing. Through critical analysis, we identify why naive image-to-video models fail in this task, particularly their inability to achieve adaptive conditioning. Addressing this, we propose InfiniteTalk, a streaming audio-driven generator designed for infinite-length long sequence dubbing. This architecture leverages temporal context frames for seamless inter-chunk transitions and incorporates a simple yet effective sampling strategy that optimizes control strength via fine-grained reference frame positioning. Comprehensive evaluations on HDTF, CelebV-HQ, and EMTD datasets demonstrate state-of-the-art performance. Quantitative metrics confirm superior visual realism, emotional coherence, and full-body motion synchronization.

</details>


### [23] [Distilled-3DGS:Distilled 3D Gaussian Splatting](https://arxiv.org/abs/2508.14037)
*Lintao Xiang,Xinkai Chen,Jianhuang Lai,Guangcong Wang*

Main category: cs.CV

TL;DR: 首个3D高斯拓扑矩阵的知识精缩框架，通过多老师模型和结构相似性损失，在保持渲染质量的同时大幅减少存储占用


<details>
  <summary>Details</summary>
Motivation: 3D高斯拓扑矩阵虽然在新视角合成上表现优异，但需要大量高斯分布导致内存和存储成本过高

Method: 提出知识精缩框架，使用普通3DGS、噪声增强版和dropout正则化版本的多老师模型，通过结构相似性损失促进学生模型学习隐藏几何结构

Result: 在多个数据集上进行完整评估，方法在渲染质量和存储效率方面都达到了最先进水平

Conclusion: Distilled-3DGS是一个简单但有效的框架，不需要复杂的设计就能在保持高保真渲染的同时显著减少存储需求

Abstract: 3D Gaussian Splatting (3DGS) has exhibited remarkable efficacy in novel view synthesis (NVS). However, it suffers from a significant drawback: achieving high-fidelity rendering typically necessitates a large number of 3D Gaussians, resulting in substantial memory consumption and storage requirements. To address this challenge, we propose the first knowledge distillation framework for 3DGS, featuring various teacher models, including vanilla 3DGS, noise-augmented variants, and dropout-regularized versions. The outputs of these teachers are aggregated to guide the optimization of a lightweight student model. To distill the hidden geometric structure, we propose a structural similarity loss to boost the consistency of spatial geometric distributions between the student and teacher model. Through comprehensive quantitative and qualitative evaluations across diverse datasets, the proposed Distilled-3DGS, a simple yet effective framework without bells and whistles, achieves promising rendering results in both rendering quality and storage efficiency compared to state-of-the-art methods. Project page: https://distilled3dgs.github.io . Code: https://github.com/lt-xiang/Distilled-3DGS .

</details>


### [24] [LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos](https://arxiv.org/abs/2508.14041)
*Chin-Yang Lin,Cheng Sun,Fu-En Yang,Min-Hung Chen,Yen-Yu Lin,Yu-Lun Liu*

Main category: cs.CV

TL;DR: LongSplat是一个针对长视频无标定相机的新视角合成框架，通过联合优化相机位姿和3D高斯、利用学习到的3D先验进行位姿估计，以及基于空间密度的八叉树锚点生成机制，解决了位姿漂移、几何初始化不准确和内存限制等关键问题。


<details>
  <summary>Details</summary>
Motivation: 解决从随意拍摄的长视频中进行新视角合成时面临的挑战，包括不规则的相机运动、未知的相机位姿和大规模场景，现有方法存在位姿漂移、几何初始化不准确和严重的内存限制问题。

Method: 提出LongSplat框架，包含三个核心组件：(1)增量联合优化同时优化相机位姿和3D高斯以避免局部极小值并确保全局一致性；(2)利用学习到的3D先验的鲁棒位姿估计模块；(3)基于空间密度的八叉树锚点形成机制，将密集点云转换为锚点。

Result: 在具有挑战性的基准测试上进行广泛实验，证明LongSplat达到了最先进的性能，在渲染质量、位姿精度和计算效率方面相比先前方法有显著提升。

Conclusion: LongSplat为长视频无标定相机的新视角合成提供了一个鲁棒且高效的解决方案，通过创新的联合优化策略和内存管理机制，成功解决了该领域的关键技术挑战。

Abstract: LongSplat addresses critical challenges in novel view synthesis (NVS) from casually captured long videos characterized by irregular camera motion, unknown camera poses, and expansive scenes. Current methods often suffer from pose drift, inaccurate geometry initialization, and severe memory limitations. To address these issues, we introduce LongSplat, a robust unposed 3D Gaussian Splatting framework featuring: (1) Incremental Joint Optimization that concurrently optimizes camera poses and 3D Gaussians to avoid local minima and ensure global consistency; (2) a robust Pose Estimation Module leveraging learned 3D priors; and (3) an efficient Octree Anchor Formation mechanism that converts dense point clouds into anchors based on spatial density. Extensive experiments on challenging benchmarks demonstrate that LongSplat achieves state-of-the-art results, substantially improving rendering quality, pose accuracy, and computational efficiency compared to prior approaches. Project page: https://linjohnss.github.io/longsplat/

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [25] [InnerGS: Internal Scenes Rendering via Factorized 3D Gaussian Splatting](https://arxiv.org/abs/2508.13287)
*Shuxin Liang,Yihan Xiao,Wenlu Tang*

Main category: eess.IV

TL;DR: 提出了一种基于3D高斯泼溅的内部场景重建方法，直接从稀疏切片数据重建平滑详细的内部结构，无需相机位姿且兼容多种数据模态


<details>
  <summary>Details</summary>
Motivation: 现有3DGS工作主要关注外部表面建模，而内部场景重建对于理解物体内部结构至关重要

Method: 通过内部3D高斯分布直接建模连续体积密度，从稀疏切片数据重建内部结构，无需相机位姿，即插即用

Result: 有效重建平滑详细的内部结构，与任何数据模态兼容

Conclusion: 该方法为内部场景重建提供了一种高效解决方案，在理解和分析物体内部结构方面具有重要应用价值

Abstract: 3D Gaussian Splatting (3DGS) has recently gained popularity for efficient scene rendering by representing scenes as explicit sets of anisotropic 3D Gaussians. However, most existing work focuses primarily on modeling external surfaces. In this work, we target the reconstruction of internal scenes, which is crucial for applications that require a deep understanding of an object's interior. By directly modeling a continuous volumetric density through the inner 3D Gaussian distribution, our model effectively reconstructs smooth and detailed internal structures from sparse sliced data. Our approach eliminates the need for camera poses, is plug-and-play, and is inherently compatible with any data modalities. We provide cuda implementation at: https://github.com/Shuxin-Liang/InnerGS.

</details>


### [26] [Comparing Conditional Diffusion Models for Synthesizing Contrast-Enhanced Breast MRI from Pre-Contrast Images](https://arxiv.org/abs/2508.13776)
*Sebastian Ibarra,Javier del Riego,Alessandro Catanese,Julian Cuba,Julian Cardona,Nataly Leon,Jonathan Infante,Karim Lekadir,Oliver Diaz,Richard Osuala*

Main category: eess.IV

TL;DR: 使用前对比条件化去噪模型生成乳腺DCE-MRI图像，通过肿瘤感知损失函数和分割条件提高病变保真度，评估显示了临床潜力


<details>
  <summary>Details</summary>
Motivation: 解决DCE-MRI对对比剂的依赖带来的安全风险、禁忌症、成本增加和流程复杂性问题

Method: 提出22个生成模型变体，包括单乳腺和全乳腺设置，使用前对比条件化去噪模型，引入肿瘤感知损失函数和明确的肿瘤分割条件

Result: 减影图像基础模型在5个评估指标上均超过后对比模型，肿瘤感知损失和分割条件提高了关键区域评估指标，读者研究确认了合成图像的高实体感

Conclusion: 生成式对比增强技术显示出出色的临床潜力，可能减少对对比剂的依赖，但肿瘤定位输入在筛查环境中可能无法保证获得

Abstract: Dynamic contrast-enhanced (DCE) MRI is essential for breast cancer diagnosis and treatment. However, its reliance on contrast agents introduces safety concerns, contraindications, increased cost, and workflow complexity. To this end, we present pre-contrast conditioned denoising diffusion probabilistic models to synthesize DCE-MRI, introducing, evaluating, and comparing a total of 22 generative model variants in both single-breast and full breast settings. Towards enhancing lesion fidelity, we introduce both tumor-aware loss functions and explicit tumor segmentation mask conditioning. Using a public multicenter dataset and comparing to respective pre-contrast baselines, we observe that subtraction image-based models consistently outperform post-contrast-based models across five complementary evaluation metrics. Apart from assessing the entire image, we also separately evaluate the region of interest, where both tumor-aware losses and segmentation mask inputs improve evaluation metrics. The latter notably enhance qualitative results capturing contrast uptake, albeit assuming access to tumor localization inputs that are not guaranteed to be available in screening settings. A reader study involving 2 radiologists and 4 MRI technologists confirms the high realism of the synthetic images, indicating an emerging clinical potential of generative contrast-enhancement. We share our codebase at https://github.com/sebastibar/conditional-diffusion-breast-MRI.

</details>


### [27] [Latent Interpolation Learning Using Diffusion Models for Cardiac Volume Reconstruction](https://arxiv.org/abs/2508.13826)
*Niklas Bubeck,Suprosanna Shit,Chen Chen,Can Zhao,Pengfei Guo,Dong Yang,Georg Zitzlsberger,Daguang Xu,Bernhard Kainz,Daniel Rueckert,Jiazhen Pan*

Main category: eess.IV

TL;DR: 基于激活模型的心脏磁共振3D重建方法CaLID，通过潜空间扩散模型实现高效的数据驱动插值，提升3倍速度并去除对辅助输入的依赖


<details>
  <summary>Details</summary>
Motivation: 解决传统2D短轴CMR成像在3D重建中存在的问题：预定义插值方案的局限性、计算效率低下、依赖分割标签等辅助输入

Method: 提出CaLID框架：1）基于扩散模型的数据驱动插值方案 2）潜空间运行提高24倍速度 3）仅需稀疏2D图像输入，无需辅助信息

Result: 在体积评估和下游分割任务中达到SOTA性能，重建质量和效率都有显著提升，同时支持2D+T时空动态建模

Conclusion: CaLID框架充分解决了现有方法的核心问题，为心血管成像提供了稳健且临床实用的解决方案，推动了心脏时空重建技术的发展

Abstract: Cardiac Magnetic Resonance (CMR) imaging is a critical tool for diagnosing and managing cardiovascular disease, yet its utility is often limited by the sparse acquisition of 2D short-axis slices, resulting in incomplete volumetric information. Accurate 3D reconstruction from these sparse slices is essential for comprehensive cardiac assessment, but existing methods face challenges, including reliance on predefined interpolation schemes (e.g., linear or spherical), computational inefficiency, and dependence on additional semantic inputs such as segmentation labels or motion data. To address these limitations, we propose a novel \textbf{Ca}rdiac \textbf{L}atent \textbf{I}nterpolation \textbf{D}iffusion (CaLID) framework that introduces three key innovations. First, we present a data-driven interpolation scheme based on diffusion models, which can capture complex, non-linear relationships between sparse slices and improves reconstruction accuracy. Second, we design a computationally efficient method that operates in the latent space and speeds up 3D whole-heart upsampling time by a factor of 24, reducing computational overhead compared to previous methods. Third, with only sparse 2D CMR images as input, our method achieves SOTA performance against baseline methods, eliminating the need for auxiliary input such as morphological guidance, thus simplifying workflows. We further extend our method to 2D+T data, enabling the effective modeling of spatiotemporal dynamics and ensuring temporal coherence. Extensive volumetric evaluations and downstream segmentation tasks demonstrate that CaLID achieves superior reconstruction quality and efficiency. By addressing the fundamental limitations of existing approaches, our framework advances the state of the art for spatio and spatiotemporal whole-heart reconstruction, offering a robust and clinically practical solution for cardiovascular imaging.

</details>


### [28] [Learning to See Through Flare](https://arxiv.org/abs/2508.13907)
*Xiaopeng Peng,Heath Gemar,Erin Fleet,Kyle Novak,Abbie Watnik,Grover Swartzlander*

Main category: eess.IV

TL;DR: NeuSee是首个计算成像框架，通过联合学习衍射光学元件和频率空间Mamba-GAN网络，实现全可见光谱范围内的高保真传感器保护和图像恢复，能够抑制高达传感器饱和阈值10^6倍的激光辐照度。


<details>
  <summary>Details</summary>
Motivation: 机器视觉系统容易受到激光耀斑的影响，强烈的激光照射会通过过饱和或永久损坏传感器像素来致盲和扭曲其对环境的感知，因此需要开发有效的传感器保护方法。

Method: 联合学习衍射光学元件(DOE)的神经表示和频率空间Mamba-GAN网络进行图像恢复，采用对抗性端到端训练，利用异构数据和模型并行化进行分布式计算，整合高光谱信息和多个神经网络进行真实模拟。

Result: NeuSee系统能够抑制高达传感器饱和阈值10^6倍的激光辐照度，在恢复图像质量方面相比其他学习方法有10.1%的提升，首次实现了全光谱成像和激光抑制。

Conclusion: NeuSee框架成功解决了机器视觉系统中的激光干扰问题，通过创新的计算成像方法实现了高效的传感器保护和高质量的图像恢复，为实际应用提供了可靠的解决方案。

Abstract: Machine vision systems are susceptible to laser flare, where unwanted intense laser illumination blinds and distorts its perception of the environment through oversaturation or permanent damage to sensor pixels. We introduce NeuSee, the first computational imaging framework for high-fidelity sensor protection across the full visible spectrum. It jointly learns a neural representation of a diffractive optical element (DOE) and a frequency-space Mamba-GAN network for image restoration. NeuSee system is adversarially trained end-to-end on 100K unique images to suppress the peak laser irradiance as high as $10^6$ times the sensor saturation threshold $I_{\textrm{sat}}$, the point at which camera sensors may experience damage without the DOE. Our system leverages heterogeneous data and model parallelism for distributed computing, integrating hyperspectral information and multiple neural networks for realistic simulation and image restoration. NeuSee takes into account open-world scenes with dynamically varying laser wavelengths, intensities, and positions, as well as lens flare effects, unknown ambient lighting conditions, and sensor noises. It outperforms other learned DOEs, achieving full-spectrum imaging and laser suppression for the first time, with a 10.1\% improvement in restored image quality.

</details>
