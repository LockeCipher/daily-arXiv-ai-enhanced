<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 5]
- [cs.CV](#cs.CV) [Total: 8]
- [quant-ph](#quant-ph) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [LuxDiT: Lighting Estimation with Video Diffusion Transformer](https://arxiv.org/abs/2509.03680)
*Ruofan Liang,Kai He,Zan Gojcic,Igor Gilitschenski,Sanja Fidler,Nandita Vijaykumar,Zian Wang*

Main category: cs.GR

TL;DR: LuxDiT是一种基于视频扩散变换器的数据驱动方法，通过微调生成HDR环境光照图，能够从单张图像或视频中准确估计场景光照，在定量和定性评估中均优于现有最先进技术。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的光照估计方法受限于真实HDR环境图的稀缺性，这些数据采集成本高且多样性有限。生成模型虽然提供了强大的图像合成先验，但光照估计仍然困难，因为它依赖于间接视觉线索、需要推断全局上下文，并且要恢复高动态范围输出。

Method: 提出LuxDiT方法，微调视频扩散变换器来生成基于视觉输入条件的HDR环境图。使用大型合成数据集训练，学习从间接视觉线索推断光照。引入低秩适应微调策略，使用收集的HDR全景数据集来提高输入与预测环境图之间的语义对齐。

Result: 该方法能够生成具有逼真角度高频细节的准确光照预测，在定量和定性评估中都优于现有的最先进技术，并且能够有效泛化到真实世界场景。

Conclusion: LuxDiT通过结合视频扩散变换器和精心设计的训练策略，成功解决了从单张图像或视频估计HDR环境光照的挑战，为计算机视觉和图形学中的光照估计问题提供了有效的解决方案。

Abstract: Estimating scene lighting from a single image or video remains a longstanding challenge in computer vision and graphics. Learning-based approaches are constrained by the scarcity of ground-truth HDR environment maps, which are expensive to capture and limited in diversity. While recent generative models offer strong priors for image synthesis, lighting estimation remains difficult due to its reliance on indirect visual cues, the need to infer global (non-local) context, and the recovery of high-dynamic-range outputs. We propose LuxDiT, a novel data-driven approach that fine-tunes a video diffusion transformer to generate HDR environment maps conditioned on visual input. Trained on a large synthetic dataset with diverse lighting conditions, our model learns to infer illumination from indirect visual cues and generalizes effectively to real-world scenes. To improve semantic alignment between the input and the predicted environment map, we introduce a low-rank adaptation finetuning strategy using a collected dataset of HDR panoramas. Our method produces accurate lighting predictions with realistic angular high-frequency details, outperforming existing state-of-the-art techniques in both quantitative and qualitative evaluations.

</details>


### [2] [ContraGS: Codebook-Condensed and Trainable Gaussian Splatting for Fast, Memory-Efficient Reconstruction](https://arxiv.org/abs/2509.03775)
*Sankeerth Durvasula,Sharanshangar Muhunthan,Zain Moustafa,Richard Chen,Ruofan Liang,Yushi Guan,Nilesh Ahuja,Nilesh Jain,Selvakumar Panneer,Nandita Vijaykumar*

Main category: cs.GR

TL;DR: ContraGS是一种直接在压缩的3D高斯泼溅表示上训练的方法，通过使用码本存储高斯参数向量，显著减少内存消耗并加速训练和渲染，同时保持接近最先进的质量。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅技术需要大量3D高斯来获得高质量表示，但这会显著增加GPU内存使用并降低训练/渲染效率。现有码本压缩方法只能用于已训练模型，无法直接用于训练过程。

Method: 使用码本紧凑存储高斯参数向量，将参数估计构建为贝叶斯推断问题，采用MCMC采样在后验分布上进行采样，解决码本压缩表示中不可微分参数的学习问题。

Result: 平均减少3.49倍训练峰值内存，加速训练1.36倍和渲染1.88倍，同时保持接近最先进的质量。

Conclusion: ContraGS成功解决了直接在码本压缩表示上训练的挑战，实现了内存效率、训练速度和模型质量的良好平衡。

Abstract: 3D Gaussian Splatting (3DGS) is a state-of-art technique to model real-world scenes with high quality and real-time rendering. Typically, a higher quality representation can be achieved by using a large number of 3D Gaussians. However, using large 3D Gaussian counts significantly increases the GPU device memory for storing model parameters. A large model thus requires powerful GPUs with high memory capacities for training and has slower training/rendering latencies due to the inefficiencies of memory access and data movement. In this work, we introduce ContraGS, a method to enable training directly on compressed 3DGS representations without reducing the Gaussian Counts, and thus with a little loss in model quality. ContraGS leverages codebooks to compactly store a set of Gaussian parameter vectors throughout the training process, thereby significantly reducing memory consumption. While codebooks have been demonstrated to be highly effective at compressing fully trained 3DGS models, directly training using codebook representations is an unsolved challenge. ContraGS solves the problem of learning non-differentiable parameters in codebook-compressed representations by posing parameter estimation as a Bayesian inference problem. To this end, ContraGS provides a framework that effectively uses MCMC sampling to sample over a posterior distribution of these compressed representations. With ContraGS, we demonstrate that ContraGS significantly reduces the peak memory during training (on average 3.49X) and accelerated training and rendering (1.36X and 1.88X on average, respectively), while retraining close to state-of-art quality.

</details>


### [3] [TensoIS: A Step Towards Feed-Forward Tensorial Inverse Subsurface Scattering for Perlin Distributed Heterogeneous Media](https://arxiv.org/abs/2509.04047)
*Ashish Tiwari,Satyam Bhardwaj,Yash Bachwana,Parag Sarvoday Sahu,T. M. Feroz Ali,Bhargava Chintalapati,Shanmuganathan Raman*

Main category: cs.GR

TL;DR: 这篇论文提出了一种基于学习的前向框架TensoIS，通过Perlin噪声模型和低秩张量表示来估计异质介质的散射参数，解决了从多视角图像中逆向散射的挑战性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在估计异质介质散射参数时遇到了严重的约束问题，并没有明确的分布模型来形容真实世界中的异质性。论文尝试利用Perlin噪声模型来模拟自然界的复杂异质性。

Method: 首先创建了HeteroSynth合成数据集，使用Fractal Perlin噪声模型化异质散射参数。然后提出TensoIS框架，通过学习低秩张量组件来表示3D散射参数体积，从稀疏多视角图像观测中估计参数。

Result: TensoIS在HeteroSynth测试集、有开源实际体积模拟的烟雾和云层几何结构以及部分真实样本上都表现出良好的效果，证明了其在逆向散射问题中的有效性。

Conclusion: 这项研究首次探索了Perlin噪声分布在形容真实世界异质散射中的应用潜力，为通过前向方式模型异质散射提供了新的解决方案。

Abstract: Estimating scattering parameters of heterogeneous media from images is a severely under-constrained and challenging problem. Most of the existing approaches model BSSRDF either through an analysis-by-synthesis approach, approximating complex path integrals, or using differentiable volume rendering techniques to account for heterogeneity. However, only a few studies have applied learning-based methods to estimate subsurface scattering parameters, but they assume homogeneous media. Interestingly, no specific distribution is known to us that can explicitly model the heterogeneous scattering parameters in the real world. Notably, procedural noise models such as Perlin and Fractal Perlin noise have been effective in representing intricate heterogeneities of natural, organic, and inorganic surfaces. Leveraging this, we first create HeteroSynth, a synthetic dataset comprising photorealistic images of heterogeneous media whose scattering parameters are modeled using Fractal Perlin noise. Furthermore, we propose Tensorial Inverse Scattering (TensoIS), a learning-based feed-forward framework to estimate these Perlin-distributed heterogeneous scattering parameters from sparse multi-view image observations. Instead of directly predicting the 3D scattering parameter volume, TensoIS uses learnable low-rank tensor components to represent the scattering volume. We evaluate TensoIS on unseen heterogeneous variations over shapes from the HeteroSynth test set, smoke and cloud geometries obtained from open-source realistic volumetric simulations, and some real-world samples to establish its effectiveness for inverse scattering. Overall, this study is an attempt to explore Perlin noise distribution, given the lack of any such well-defined distribution in literature, to potentially model real-world heterogeneous scattering in a feed-forward manner.

</details>


### [4] [SMooGPT: Stylized Motion Generation using Large Language Models](https://arxiv.org/abs/2509.04058)
*Lei Zhong,Yi Yang,Changjian Li*

Main category: cs.GR

TL;DR: 通过将人体运动分解为部位文本表示，利用细调的LLM作为推理器、组合器和生成器，实现了高解释性、细粒度控制和良好扩展性的风格化运动生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在解释性低、控制限制、对新风格扩展性差以及仅能生成"行走"动作的问题，需要从新角度解决风格化运动生成问题。

Method: 提出了推理-组合-生成的新视角：1)将人体运动分解为部位文本表示；2)使用细调的LLM（SMooGPT）作为推理器、组合器和生成器；3)在部位文本空间执行，利用LLM的开攻词能力实现良好的扩展性。

Result: 综合实验、评估和用户感知研究证明了方法的有效性，在纯文本驱动的风格化运动生成下表现特别突出。

Conclusion: 通过部位文本空间中间表示和LLM的强大推理能力，实现了高解释性、细粒度控制、冲突解决和良好扩展性的风格化运动生成方案。

Abstract: Stylized motion generation is actively studied in computer graphics, especially benefiting from the rapid advances in diffusion models. The goal of this task is to produce a novel motion respecting both the motion content and the desired motion style, e.g., ``walking in a loop like a Monkey''. Existing research attempts to address this problem via motion style transfer or conditional motion generation. They typically embed the motion style into a latent space and guide the motion implicitly in a latent space as well. Despite the progress, their methods suffer from low interpretability and control, limited generalization to new styles, and fail to produce motions other than ``walking'' due to the strong bias in the public stylization dataset. In this paper, we propose to solve the stylized motion generation problem from a new perspective of reasoning-composition-generation, based on our observations: i) human motion can often be effectively described using natural language in a body-part centric manner, ii) LLMs exhibit a strong ability to understand and reason about human motion, and iii) human motion has an inherently compositional nature, facilitating the new motion content or style generation via effective recomposing. We thus propose utilizing body-part text space as an intermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a reasoner, composer, and generator when generating the desired stylized motion. Our method executes in the body-part text space with much higher interpretability, enabling fine-grained motion control, effectively resolving potential conflicts between motion content and style, and generalizes well to new styles thanks to the open-vocabulary ability of LLMs. Comprehensive experiments and evaluations, and a user perceptual study, demonstrate the effectiveness of our approach, especially under the pure text-driven stylized motion generation.

</details>


### [5] [Hyper Diffusion Avatars: Dynamic Human Avatar Generation using Network Weight Space Diffusion](https://arxiv.org/abs/2509.04145)
*Dongliang Cao,Guoxing Sun,Marc Habermann,Florian Bernard*

Main category: cs.GR

TL;DR: 通过两阶段深度学习方法，结合个人特定渲染和生成式模型优势，实现高保真度动态人物演员生成，支持姿势依赖性变形如衣服皱红


<details>
  <summary>Details</summary>
Motivation: 解决现有人物演员方法的局限性：个人特定渲染法无法跨身份通用，生成式方法渲染质量低且缺乏姿势依赖性变形

Method: 两阶段管线：首先优化一组个人特定UNet网络捕捉细致姿势变形，然后训练超应散模型学习网络权重分布，推理时生成网络权重实现实时可控渲染

Result: 在大规模跨身份多视角视频数据集上验证，方法表现超越现有最佳人物演员生成方法

Conclusion: 新方法成功结合了个人特定渲染的高保真度和生成式模型的跨身份通用性，实现了具有现实姿势依赖变形的动态人物演员生成

Abstract: Creating human avatars is a highly desirable yet challenging task. Recent advancements in radiance field rendering have achieved unprecedented photorealism and real-time performance for personalized dynamic human avatars. However, these approaches are typically limited to person-specific rendering models trained on multi-view video data for a single individual, limiting their ability to generalize across different identities. On the other hand, generative approaches leveraging prior knowledge from pre-trained 2D diffusion models can produce cartoonish, static human avatars, which are animated through simple skeleton-based articulation. Therefore, the avatars generated by these methods suffer from lower rendering quality compared to person-specific rendering methods and fail to capture pose-dependent deformations such as cloth wrinkles. In this paper, we propose a novel approach that unites the strengths of person-specific rendering and diffusion-based generative modeling to enable dynamic human avatar generation with both high photorealism and realistic pose-dependent deformations. Our method follows a two-stage pipeline: first, we optimize a set of person-specific UNets, with each network representing a dynamic human avatar that captures intricate pose-dependent deformations. In the second stage, we train a hyper diffusion model over the optimized network weights. During inference, our method generates network weights for real-time, controllable rendering of dynamic human avatars. Using a large-scale, cross-identity, multi-view video dataset, we demonstrate that our approach outperforms state-of-the-art human avatar generation methods.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [6] [Singular Value Few-shot Adaptation of Vision-Language Models](https://arxiv.org/abs/2509.03740)
*Taha Koleilat,Hassan Rivaz,Yiming Xiao*

Main category: cs.CV

TL;DR: CLIP-SVD是一种基于奇异值分解的多模态参数高效适应方法，仅需微调CLIP参数矩阵的奇异值，用0.04%的参数实现领域适应，在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型适应方法依赖提示工程和完整微调，成本高且可能破坏预训练知识。需要一种更高效、稳定的适应技术。

Method: 利用奇异值分解(SVD)修改CLIP内部参数空间，仅微调参数矩阵的奇异值来重新缩放基向量，不注入额外模块。

Result: 在11个自然数据集和10个生物医学数据集上实现最先进的分类结果，在少样本设置下准确率和泛化能力均优于先前方法。

Conclusion: CLIP-SVD提供了一种参数高效、性能优越的CLIP适应方法，能更好地保持模型的泛化能力，同时支持可解释性分析。

Abstract: Vision-language models (VLMs) like CLIP have shown impressive zero-shot and few-shot learning capabilities across diverse applications. However, adapting these models to new fine-grained domains remains difficult due to reliance on prompt engineering and the high cost of full model fine-tuning. Existing adaptation approaches rely on augmented components, such as prompt tokens and adapter modules, which could limit adaptation quality, destabilize the model, and compromise the rich knowledge learned during pretraining. In this work, we present \textbf{CLIP-SVD}, a novel \textit{multi-modal} and \textit{parameter-efficient} adaptation technique that leverages Singular Value Decomposition (SVD) to modify the internal parameter space of CLIP without injecting additional modules. Specifically, we fine-tune only the singular values of the CLIP parameter matrices to rescale the basis vectors for domain adaptation while retaining the pretrained model. This design enables enhanced adaptation performance using only \textbf{0.04\%} of the model's total parameters and better preservation of its generalization ability. CLIP-SVD achieves state-of-the-art classification results on 11 natural and 10 biomedical datasets, outperforming previous methods in both accuracy and generalization under few-shot settings. Additionally, we leverage a natural language-based approach to analyze the effectiveness and dynamics of the CLIP adaptation to allow interpretability of CLIP-SVD. The code is publicly available at https://github.com/HealthX-Lab/CLIP-SVD.

</details>


### [7] [Causality-guided Prompt Learning for Vision-language Models via Visual Granulation](https://arxiv.org/abs/2509.03803)
*Mengyu Gao,Qiulei Dong*

Main category: cs.CV

TL;DR: CaPL是一种基于因果推理的视觉粒度化提示学习方法，通过属性解耦和粒度学习模块，显著提升了CLIP模型在细粒度识别任务上的性能


<details>
  <summary>Details</summary>
Motivation: 现有的CLIP提示学习方法在处理细粒度数据集时表现有限，需要一种能够捕捉细粒度类别间细微差异的方法

Method: 提出CaPL方法，包含属性解耦模块（使用布朗桥扩散模型分解视觉特征）和粒度学习模块（通过因果推理策略构建视觉粒度）

Result: 在15个数据集上的实验结果表明，CaPL方法显著优于最先进的提示学习方法，特别是在细粒度数据集上

Conclusion: 通过视觉粒度化和因果推理，CaPL能够学习更具判别性的文本提示，有效提升CLIP在细粒度识别任务中的性能

Abstract: Prompt learning has recently attracted much attention for adapting pre-trained vision-language models (e.g., CLIP) to downstream recognition tasks. However, most of the existing CLIP-based prompt learning methods only show a limited ability for handling fine-grained datasets. To address this issue, we propose a causality-guided text prompt learning method via visual granulation for CLIP, called CaPL, where the explored visual granulation technique could construct sets of visual granules for the text prompt to capture subtle discrepancies among different fine-grained classes through casual inference. The CaPL method contains the following two modules: (1) An attribute disentanglement module is proposed to decompose visual features into non-individualized attributes (shared by some classes) and individualized attributes (specific to single classes) using a Brownian Bridge Diffusion Model; (2) A granule learning module is proposed to construct visual granules by integrating the aforementioned attributes for recognition under two causal inference strategies. Thanks to the learned visual granules, more discriminative text prompt is expected to be learned. Extensive experimental results on 15 datasets demonstrate that our CaPL method significantly outperforms the state-of-the-art prompt learning methods, especially on fine-grained datasets.

</details>


### [8] [Human Motion Video Generation: A Survey](https://arxiv.org/abs/2509.03883)
*Haiwei Xue,Xiangyang Luo,Zhanghao Hu,Xin Zhang,Xunzhi Xiang,Yuqin Dai,Jianzhuang Liu,Zhensong Zhang,Minglei Li,Jian Yang,Fei Ma,Zhiyong Wu,Changpeng Yang,Zonghong Dai,Fei Richard Yu*

Main category: cs.CV

TL;DR: 这是一份关于人体运动视频生成的综述性论文，系统分析了该领域的生成过程五阶段、三种主要模态，并首次讨论了大语言模型在该领域的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的人体运动视频生成研究综述太过分散，缺乏对整个生成过程的系统性概览。本文旨在填补这一空白，提供全面的领域概览。

Method: 本文将人体运动视频生成过程分为五个关键阶段：输入、运动规划、运动视频生成、精炼和输出，并针对视觉、文本和音频三种主要模态进行分析。研究涵盖了超过200篇论文。

Result: 本文提供了人体运动视频生成领域的完整技术图谱，标记了重要的里程碑式研究成果，并首次探讨了大语言模型在该领域的应用潜力。

Conclusion: 这份综述为人体运动视频生成领域提供了份价值的研究资源，展示了该技术在数字人类全面应用中的幻想前景，对推动领域发展具有重要意义。

Abstract: Human motion video generation has garnered significant research interest due to its broad applications, enabling innovations such as photorealistic singing heads or dynamic avatars that seamlessly dance to music. However, existing surveys in this field focus on individual methods, lacking a comprehensive overview of the entire generative process. This paper addresses this gap by providing an in-depth survey of human motion video generation, encompassing over ten sub-tasks, and detailing the five key phases of the generation process: input, motion planning, motion video generation, refinement, and output. Notably, this is the first survey that discusses the potential of large language models in enhancing human motion video generation. Our survey reviews the latest developments and technological trends in human motion video generation across three primary modalities: vision, text, and audio. By covering over two hundred papers, we offer a thorough overview of the field and highlight milestone works that have driven significant technological breakthroughs. Our goal for this survey is to unveil the prospects of human motion video generation and serve as a valuable resource for advancing the comprehensive applications of digital humans. A complete list of the models examined in this survey is available in Our Repository https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.

</details>


### [9] [MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image Generation](https://arxiv.org/abs/2509.04126)
*Yuan Zhao,Liu Lin*

Main category: cs.CV

TL;DR: 提出了MEPG框架，通过位置-风格感知LLM和多专家扩散模块，解决了文本到图像扩散模型在处理复杂多元素提示和风格多样性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型在处理复杂多元素提示时表现不佳，且风格多样性有限，需要一种能够精确控制空间布局和风格的专业化解决方案。

Method: 使用监督微调的LLM分解输入提示为精确空间坐标和风格编码语义指令，通过注意力门控机制在局部区域和全局区域动态路由专家模型（如写实专家、风格化专家）。

Result: 实验表明MEPG在图像质量和风格多样性方面显著优于相同骨干网络的基线模型。

Conclusion: MEPG框架通过多专家规划和生成方法，有效提升了复杂文本到图像生成的质量和多样性，并支持轻量级专家模型集成和实时空间布局编辑。

Abstract: Text-to-image diffusion models have achieved remarkable image quality, but they still struggle with complex, multiele ment prompts, and limited stylistic diversity. To address these limitations, we propose a Multi-Expert Planning and Gen eration Framework (MEPG) that synergistically integrates position- and style-aware large language models (LLMs) with spatial-semantic expert modules. The framework comprises two core components: (1) a Position-Style-Aware (PSA) module that utilizes a supervised fine-tuned LLM to decom pose input prompts into precise spatial coordinates and style encoded semantic instructions; and (2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera tion through dynamic expert routing across both local regions and global areas. During the generation process for each lo cal region, specialized models (e.g., realism experts, styliza tion specialists) are selectively activated for each spatial par tition via attention-based gating mechanisms. The architec ture supports lightweight integration and replacement of ex pert models, providing strong extensibility. Additionally, an interactive interface enables real-time spatial layout editing and per-region style selection from a portfolio of experts. Ex periments show that MEPG significantly outperforms base line models with the same backbone in both image quality   and style diversity.

</details>


### [10] [TauGenNet: Plasma-Driven Tau PET Image Synthesis via Text-Guided 3D Diffusion Models](https://arxiv.org/abs/2509.04269)
*Yuxin Gong,Se-in Jang,Wei Shao,Yi Su,Kuang Gong*

Main category: cs.CV

TL;DR: 提出基于文本引导的3D扩散模型，利用结构MRI和血浆p-tau217测量值合成3D tau PET图像，为阿尔茨海默病诊断提供低成本替代方案


<details>
  <summary>Details</summary>
Motivation: tau PET成像成本高且可用性有限，而结构MRI和血浆生物标志物具有非侵入性和广泛可用性，需要开发一种经济有效的tau病理可视化方法

Method: 使用文本引导的3D扩散模型，以血浆p-tau217测量值作为文本提示，结构MRI提供解剖结构约束，在ADNI数据库的AV1451 tau PET数据上进行训练和评估

Result: 能够生成跨不同疾病阶段的真实且具有临床意义的3D tau PET图像，实现了数据增强和病理可视化

Conclusion: 该框架可为不同设置下的tau PET数据增强提供支持，提供非侵入性、成本效益高的tau病理可视化替代方案，并支持在不同血浆生物标志物水平和认知条件下模拟疾病进展

Abstract: Accurate quantification of tau pathology via tau positron emission tomography (PET) scan is crucial for diagnosing and monitoring Alzheimer's disease (AD). However, the high cost and limited availability of tau PET restrict its widespread use. In contrast, structural magnetic resonance imaging (MRI) and plasma-based biomarkers provide non-invasive and widely available complementary information related to brain anatomy and disease progression. In this work, we propose a text-guided 3D diffusion model for 3D tau PET image synthesis, leveraging multimodal conditions from both structural MRI and plasma measurement. Specifically, the textual prompt is from the plasma p-tau217 measurement, which is a key indicator of AD progression, while MRI provides anatomical structure constraints. The proposed framework is trained and evaluated using clinical AV1451 tau PET data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. Experimental results demonstrate that our approach can generate realistic, clinically meaningful 3D tau PET across a range of disease stages. The proposed framework can help perform tau PET data augmentation under different settings, provide a non-invasive, cost-effective alternative for visualizing tau pathology, and support the simulation of disease progression under varying plasma biomarker levels and cognitive conditions.

</details>


### [11] [SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer](https://arxiv.org/abs/2509.04379)
*Jimin Xu,Bosheng Qin,Tao Jin,Zhou Zhao,Zhenhui Ye,Jun Yu,Fei Wu*

Main category: cs.CV

TL;DR: 一种新的3D风格转换方法，利用预训练2D滿散模型的知识，通过跨视图风格对齐和实例级风格转换，实现了更结构化、视觉一致的3D场景风格化效果。


<details>
  <summary>Details</summary>
Motivation: 现有的3D风格转换方法无法有效提取和转换高级风格语义，并且风格化结果缺乏结构清晰度和分离度，导致不同实例或物体难以区分。

Method: 两阶段流水线：1）利用滿散模型生成关键视点的风格化渲染；2）将风格化关键视图转换到3D表示。包含跨视图风格对齐和实例级风格转换两个创新设计。

Result: 实验结果显示，该方法在广泛的场景中（从前向视角到挑战性的360度环境）都显著超过了现有最先进方法。

Conclusion: 该研究提出的新方3D风格转换流水线能够生成结构更清晰、视觉更一致、艺术效果更丰富的风格化结果，为3D场景风格化领域提供了有效解决方案。

Abstract: Recent advancements in neural representations, such as Neural Radiance Fields and 3D Gaussian Splatting, have increased interest in applying style transfer to 3D scenes. While existing methods can transfer style patterns onto 3D-consistent neural representations, they struggle to effectively extract and transfer high-level style semantics from the reference style image. Additionally, the stylized results often lack structural clarity and separation, making it difficult to distinguish between different instances or objects within the 3D scene. To address these limitations, we propose a novel 3D style transfer pipeline that effectively integrates prior knowledge from pretrained 2D diffusion models. Our pipeline consists of two key stages: First, we leverage diffusion priors to generate stylized renderings of key viewpoints. Then, we transfer the stylized key views onto the 3D representation. This process incorporates two innovative designs. The first is cross-view style alignment, which inserts cross-view attention into the last upsampling block of the UNet, allowing feature interactions across multiple key views. This ensures that the diffusion model generates stylized key views that maintain both style fidelity and instance-level consistency. The second is instance-level style transfer, which effectively leverages instance-level consistency across stylized key views and transfers it onto the 3D representation. This results in a more structured, visually coherent, and artistically enriched stylization. Extensive qualitative and quantitative experiments demonstrate that our 3D style transfer pipeline significantly outperforms state-of-the-art methods across a wide range of scenes, from forward-facing to challenging 360-degree environments. Visit our project page https://jm-xu.github.io/SSGaussian for immersive visualization.

</details>


### [12] [Durian: Dual Reference-guided Portrait Animation with Attribute Transfer](https://arxiv.org/abs/2509.04434)
*Hyunsoo Cha,Byungjun Kim,Hanbyul Joo*

Main category: cs.CV

TL;DR: Durian是首个零样本肖像动画视频生成方法，能够从参考图像向目标肖像进行面部属性迁移，通过双参考网络和扩散模型实现高保真度、空间一致的跨帧属性迁移。


<details>
  <summary>Details</summary>
Motivation: 解决肖像动画中面部属性迁移的挑战，需要实现高保真度、空间一致的跨帧属性迁移，同时支持零样本泛化和多属性组合。

Method: 采用双参考网络将肖像和属性图像的空间特征注入扩散去噪过程；使用自重建训练框架；提出基于关键点的掩码扩展策略；应用空间和外观级变换增强鲁棒性。

Result: 在肖像动画属性迁移任务上达到最先进性能，双参考设计支持单次生成过程中的多属性组合，无需额外训练。

Conclusion: Durian通过创新的双参考网络设计和训练策略，成功实现了零样本肖像动画属性迁移，为多属性组合生成提供了有效解决方案。

Abstract: We present Durian, the first method for generating portrait animation videos with facial attribute transfer from a given reference image to a target portrait in a zero-shot manner. To enable high-fidelity and spatially consistent attribute transfer across frames, we introduce dual reference networks that inject spatial features from both the portrait and attribute images into the denoising process of a diffusion model. We train the model using a self-reconstruction formulation, where two frames are sampled from the same portrait video: one is treated as the attribute reference and the other as the target portrait, and the remaining frames are reconstructed conditioned on these inputs and their corresponding masks. To support the transfer of attributes with varying spatial extent, we propose a mask expansion strategy using keypoint-conditioned image generation for training. In addition, we further augment the attribute and portrait images with spatial and appearance-level transformations to improve robustness to positional misalignment between them. These strategies allow the model to effectively generalize across diverse attributes and in-the-wild reference combinations, despite being trained without explicit triplet supervision. Durian achieves state-of-the-art performance on portrait animation with attribute transfer, and notably, its dual reference design enables multi-attribute composition in a single generation pass without additional training.

</details>


### [13] [Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual Try-On from a Single Image -- Technical Preview](https://arxiv.org/abs/2509.04450)
*Jun-Kun Chen,Aayush Bansal,Minh Phuoc Vo,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: VFR是一个创新的视频生成模型，能够生成任意长度的虚拟试穿视频，通过分段自回归生成方式解决长视频生成问题，确保局部平滑性和全局时间一致性。


<details>
  <summary>Details</summary>
Motivation: 传统虚拟试穿视频生成需要大量计算资源和长视频数据，难以生成任意长度的视频。VFR旨在解决长视频生成中的局部平滑性和全局一致性挑战。

Method: 采用分段自回归生成方法，使用前缀视频条件确保局部平滑性，通过360度锚点视频保持全局时间一致性，无需大量计算资源。

Result: 成功生成了分钟级别的虚拟试穿视频，在各种动作下都能保持局部平滑和全局一致性，成为长虚拟试穿视频生成的先驱工作。

Conclusion: VFR框架为长视频生成提供了有效的解决方案，在虚拟试穿领域具有重要应用价值，能够生成高质量、任意长度的试穿视频。

Abstract: We introduce the Virtual Fitting Room (VFR), a novel video generative model that produces arbitrarily long virtual try-on videos. Our VFR models long video generation tasks as an auto-regressive, segment-by-segment generation process, eliminating the need for resource-intensive generation and lengthy video data, while providing the flexibility to generate videos of arbitrary length. The key challenges of this task are twofold: ensuring local smoothness between adjacent segments and maintaining global temporal consistency across different segments. To address these challenges, we propose our VFR framework, which ensures smoothness through a prefix video condition and enforces consistency with the anchor video -- a 360-degree video that comprehensively captures the human's wholebody appearance. Our VFR generates minute-scale virtual try-on videos with both local smoothness and global temporal consistency under various motions, making it a pioneering work in long virtual try-on video generation.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [14] [The Chaotic Art: Quantum Representation and Manipulation of Color](https://arxiv.org/abs/2509.03542)
*Guosheng Hu*

Main category: quant-ph

TL;DR: 量子计算技术通过颜色量子位表示、颜色通道处理和颜色图像生成的方式，为颜色计算提供了新技术路径，建立了经典色彩学与量子图形学之间的桥梁。


<details>
  <summary>Details</summary>
Motivation: 利用量子计算的独特计算原理，深刻改变颜色艺术的景观，开拓颜色计算的新技术路径。

Method: 通过量子位表示数字颜色，在量子计算环境中进行操作和测量，然后恢复为经典计算机的计算结果。使用Qiskit和IBM Q进行编程实验。

Result: 证明了颜色量子位表示和量子计算作为艺术技术的可行性，建立了经典色彩学与量子图形学之间的联系。

Conclusion: 量子计算可用于信息可视化、图像处理等颜色计算任务，并有望推动新的颜色理论和艺术概念的发展。

Abstract: Due to its unique computing principles, quantum computing technology will profoundly change the spectacle of color art. Focusing on experimental exploration of color qubit representation, color channel processing, and color image generation via quantum computing, this article proposes a new technical path for color computing in quantum computing environment, by which digital color is represented, operated, and measured in quantum bits, and then restored for classical computers as computing results. This method has been proved practicable as an artistic technique of color qubit representation and quantum computing via programming experiments in Qiskit and IBM Q. By building a bridge between classical chromatics and quantum graphics, quantum computers can be used for information visualization, image processing, and more color computing tasks. Furthermore, quantum computing can be expected to facilitate new color theories and artistic concepts.

</details>
