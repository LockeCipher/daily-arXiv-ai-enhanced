{"id": "2506.20875", "pdf": "https://arxiv.org/pdf/2506.20875", "abs": "https://arxiv.org/abs/2506.20875", "authors": ["Chengan He", "Junxuan Li", "Tobias Kirschstein", "Artem Sevastopolsky", "Shunsuke Saito", "Qingyang Tan", "Javier Romero", "Chen Cao", "Holly Rushmeier", "Giljoo Nam"], "title": "3DGH: 3D Head Generation with Composable Hair and Face", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted to SIGGRAPH 2025. Project page:   https://c-he.github.io/projects/3dgh/", "summary": "We present 3DGH, an unconditional generative model for 3D human heads with composable hair and face components. Unlike previous work that entangles the modeling of hair and face, we propose to separate them using a novel data representation with template-based 3D Gaussian Splatting, in which deformable hair geometry is introduced to capture the geometric variations across different hairstyles. Based on this data representation, we design a 3D GAN-based architecture with dual generators and employ a cross-attention mechanism to model the inherent correlation between hair and face. The model is trained on synthetic renderings using carefully designed objectives to stabilize training and facilitate hair-face separation. We conduct extensive experiments to validate the design choice of 3DGH, and evaluate it both qualitatively and quantitatively by comparing with several state-of-the-art 3D GAN methods, demonstrating its effectiveness in unconditional full-head image synthesis and composable 3D hairstyle editing. More details will be available on our project page: https://c-he.github.io/projects/3dgh/.", "AI": {"tldr": "3DGH\u662f\u4e00\u79cd\u65e0\u6761\u4ef6\u751f\u62103D\u4eba\u5934\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u652f\u6301\u53ef\u7ec4\u5408\u7684\u5934\u53d1\u548c\u9762\u90e8\u7ec4\u4ef6\uff0c\u901a\u8fc7\u5206\u79bb\u5efa\u6a21\u548c\u53cc\u751f\u6210\u5668\u67b6\u6784\u5b9e\u73b0\u9ad8\u6548\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5934\u53d1\u548c\u9762\u90e8\u7684\u5efa\u6a21\u4e0a\u5b58\u5728\u8026\u5408\u95ee\u9898\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u7f16\u8f91\u80fd\u529b\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6a21\u677f\u76843D\u9ad8\u65af\u6563\u5c04\u8868\u793a\u548c\u53cc\u751f\u6210\u5668\u67b6\u6784\uff0c\u7ed3\u5408\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5efa\u6a21\u5934\u53d1\u4e0e\u9762\u90e8\u7684\u5173\u8054\u3002", "result": "\u5b9e\u9a8c\u8868\u660e3DGH\u5728\u65e0\u6761\u4ef6\u5168\u5934\u56fe\u50cf\u5408\u6210\u548c\u53ef\u7ec4\u54083D\u53d1\u578b\u7f16\u8f91\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "3DGH\u901a\u8fc7\u5206\u79bb\u5efa\u6a21\u548c\u9ad8\u6548\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u76843D\u4eba\u5934\u751f\u6210\u548c\u7f16\u8f91\u3002"}}
{"id": "2506.20946", "pdf": "https://arxiv.org/pdf/2506.20946", "abs": "https://arxiv.org/abs/2506.20946", "authors": ["Donggoo Kang", "Jangyeong Kim", "Dasol Jeong", "Junyoung Choi", "Jeonga Wi", "Hyunmin Lee", "Joonho Gwon", "Joonki Paik"], "title": "Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware Diffusion and Temporal Video Models", "categories": ["cs.GR", "cs.AI", "cs.CV", "68T45, 68U05", "I.3.7; I.4.10; I.2.10"], "comment": null, "summary": "Current texture synthesis methods, which generate textures from fixed viewpoints, suffer from inconsistencies due to the lack of global context and geometric understanding. Meanwhile, recent advancements in video generation models have demonstrated remarkable success in achieving temporally consistent videos. In this paper, we introduce VideoTex, a novel framework for seamless texture synthesis that leverages video generation models to address both spatial and temporal inconsistencies in 3D textures. Our approach incorporates geometry-aware conditions, enabling precise utilization of 3D mesh structures. Additionally, we propose a structure-wise UV diffusion strategy, which enhances the generation of occluded areas by preserving semantic information, resulting in smoother and more coherent textures. VideoTex not only achieves smoother transitions across UV boundaries but also ensures high-quality, temporally stable textures across video frames. Extensive experiments demonstrate that VideoTex outperforms existing methods in texture fidelity, seam blending, and stability, paving the way for dynamic real-time applications that demand both visual quality and temporal coherence.", "AI": {"tldr": "VideoTex\u5229\u7528\u89c6\u9891\u751f\u6210\u6a21\u578b\u89e3\u51b33D\u7eb9\u7406\u5408\u6210\u4e2d\u7684\u65f6\u7a7a\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u6761\u4ef6\u548cUV\u6269\u6563\u7b56\u7565\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u7a33\u5b9a\u7684\u7eb9\u7406\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7eb9\u7406\u5408\u6210\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u51e0\u4f55\u7406\u89e3\uff0c\u5bfc\u81f4\u4e0d\u4e00\u81f4\u6027\u3002\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u6210\u529f\u542f\u53d1\u5229\u7528\u5176\u89e3\u51b3\u7eb9\u7406\u5408\u6210\u7684\u65f6\u7a7a\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u51e0\u4f55\u611f\u77e5\u6761\u4ef6\uff0c\u5229\u75283D\u7f51\u683c\u7ed3\u6784\uff1b\u63d0\u51fa\u7ed3\u6784\u5316\u7684UV\u6269\u6563\u7b56\u7565\uff0c\u589e\u5f3a\u906e\u6321\u533a\u57df\u7684\u751f\u6210\u3002", "result": "VideoTex\u5728\u7eb9\u7406\u4fdd\u771f\u5ea6\u3001\u63a5\u7f1d\u878d\u5408\u548c\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u65f6\u95f4\u7a33\u5b9a\u7684\u7eb9\u7406\u3002", "conclusion": "VideoTex\u4e3a\u52a8\u6001\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u89c6\u89c9\u8d28\u91cf\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21272", "pdf": "https://arxiv.org/pdf/2506.21272", "abs": "https://arxiv.org/abs/2506.21272", "authors": ["Jiayi Zheng", "Xiaodong Cun"], "title": "FairyGen: Storied Cartoon Video from a Single Child-Drawn Character", "categories": ["cs.GR", "cs.CV", "cs.MM"], "comment": "Project Page: https://jayleejia.github.io/FairyGen/ ; Code:   https://github.com/GVCLab/FairyGen", "summary": "We propose FairyGen, an automatic system for generating story-driven cartoon videos from a single child's drawing, while faithfully preserving its unique artistic style. Unlike previous storytelling methods that primarily focus on character consistency and basic motion, FairyGen explicitly disentangles character modeling from stylized background generation and incorporates cinematic shot design to support expressive and coherent storytelling. Given a single character sketch, we first employ an MLLM to generate a structured storyboard with shot-level descriptions that specify environment settings, character actions, and camera perspectives. To ensure visual consistency, we introduce a style propagation adapter that captures the character's visual style and applies it to the background, faithfully retaining the character's full visual identity while synthesizing style-consistent scenes. A shot design module further enhances visual diversity and cinematic quality through frame cropping and multi-view synthesis based on the storyboard. To animate the story, we reconstruct a 3D proxy of the character to derive physically plausible motion sequences, which are then used to fine-tune an MMDiT-based image-to-video diffusion model. We further propose a two-stage motion customization adapter: the first stage learns appearance features from temporally unordered frames, disentangling identity from motion; the second stage models temporal dynamics using a timestep-shift strategy with frozen identity weights. Once trained, FairyGen directly renders diverse and coherent video scenes aligned with the storyboard. Extensive experiments demonstrate that our system produces animations that are stylistically faithful, narratively structured natural motion, highlighting its potential for personalized and engaging story animation. The code will be available at https://github.com/GVCLab/FairyGen", "AI": {"tldr": "FairyGen\u662f\u4e00\u4e2a\u81ea\u52a8\u7cfb\u7edf\uff0c\u80fd\u4ece\u5355\u5f20\u513f\u7ae5\u7ed8\u753b\u751f\u6210\u6545\u4e8b\u9a71\u52a8\u7684\u5361\u901a\u89c6\u9891\uff0c\u5e76\u4fdd\u7559\u5176\u72ec\u7279\u7684\u827a\u672f\u98ce\u683c\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u89d2\u8272\u4e00\u81f4\u6027\u548c\u57fa\u672c\u52a8\u4f5c\uff0c\u800cFairyGen\u65e8\u5728\u901a\u8fc7\u5206\u79bb\u89d2\u8272\u5efa\u6a21\u4e0e\u98ce\u683c\u5316\u80cc\u666f\u751f\u6210\uff0c\u5e76\u7ed3\u5408\u7535\u5f71\u955c\u5934\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u66f4\u5177\u8868\u73b0\u529b\u548c\u8fde\u8d2f\u6027\u7684\u6545\u4e8b\u8bb2\u8ff0\u3002", "method": "\u7cfb\u7edf\u9996\u5148\u4f7f\u7528MLLM\u751f\u6210\u7ed3\u6784\u5316\u6545\u4e8b\u677f\uff0c\u7136\u540e\u901a\u8fc7\u98ce\u683c\u4f20\u64ad\u9002\u914d\u5668\u786e\u4fdd\u89c6\u89c9\u4e00\u81f4\u6027\uff0c\u518d\u901a\u8fc7\u955c\u5934\u8bbe\u8ba1\u6a21\u5757\u589e\u5f3a\u89c6\u89c9\u591a\u6837\u6027\u548c\u7535\u5f71\u8d28\u91cf\u3002\u89d2\u8272\u52a8\u753b\u901a\u8fc73D\u4ee3\u7406\u91cd\u5efa\u548cMMDiT\u6a21\u578b\u5b9e\u73b0\uff0c\u5e76\u7ed3\u5408\u4e24\u9636\u6bb5\u8fd0\u52a8\u5b9a\u5236\u9002\u914d\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFairyGen\u751f\u6210\u7684\u52a8\u753b\u98ce\u683c\u5fe0\u5b9e\u3001\u53d9\u4e8b\u7ed3\u6784\u81ea\u7136\uff0c\u5177\u6709\u4e2a\u6027\u5316\u548c\u5438\u5f15\u529b\u7684\u6545\u4e8b\u52a8\u753b\u6f5c\u529b\u3002", "conclusion": "FairyGen\u4e3a\u4e2a\u6027\u5316\u6545\u4e8b\u52a8\u753b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u98ce\u683c\u4e00\u81f4\u7684\u65b9\u6cd5\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2506.20756", "pdf": "https://arxiv.org/pdf/2506.20756", "abs": "https://arxiv.org/abs/2506.20756", "authors": ["Haodong Li", "Chen Wang", "Jiahui Lei", "Kostas Daniilidis", "Lingjie Liu"], "title": "StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation", "categories": ["cs.CV"], "comment": "Work done in Nov. 2024. Project page: https://stereodiff.github.io/", "summary": "Recent video depth estimation methods achieve great performance by following the paradigm of image depth estimation, i.e., typically fine-tuning pre-trained video diffusion models with massive data. However, we argue that video depth estimation is not a naive extension of image depth estimation. The temporal consistency requirements for dynamic and static regions in videos are fundamentally different. Consistent video depth in static regions, typically backgrounds, can be more effectively achieved via stereo matching across all frames, which provides much stronger global 3D cues. While the consistency for dynamic regions still should be learned from large-scale video depth data to ensure smooth transitions, due to the violation of triangulation constraints. Based on these insights, we introduce StereoDiff, a two-stage video depth estimator that synergizes stereo matching for mainly the static areas with video depth diffusion for maintaining consistent depth transitions in dynamic areas. We mathematically demonstrate how stereo matching and video depth diffusion offer complementary strengths through frequency domain analysis, highlighting the effectiveness of their synergy in capturing the advantages of both. Experimental results on zero-shot, real-world, dynamic video depth benchmarks, both indoor and outdoor, demonstrate StereoDiff's SoTA performance, showcasing its superior consistency and accuracy in video depth estimation.", "AI": {"tldr": "StereoDiff\u7ed3\u5408\u7acb\u4f53\u5339\u914d\u548c\u89c6\u9891\u6df1\u5ea6\u6269\u6563\uff0c\u5206\u522b\u5904\u7406\u9759\u6001\u548c\u52a8\u6001\u533a\u57df\uff0c\u5b9e\u73b0\u4e86\u89c6\u9891\u6df1\u5ea6\u4f30\u8ba1\u7684SoTA\u6027\u80fd\u3002", "motivation": "\u89c6\u9891\u6df1\u5ea6\u4f30\u8ba1\u4e0d\u4ec5\u4ec5\u662f\u56fe\u50cf\u6df1\u5ea6\u4f30\u8ba1\u7684\u7b80\u5355\u6269\u5c55\uff0c\u9759\u6001\u548c\u52a8\u6001\u533a\u57df\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u9700\u6c42\u4e0d\u540c\u3002\u9759\u6001\u533a\u57df\u53ef\u901a\u8fc7\u7acb\u4f53\u5339\u914d\u5b9e\u73b0\u5168\u5c403D\u7ebf\u7d22\uff0c\u800c\u52a8\u6001\u533a\u57df\u9700\u4f9d\u8d56\u5927\u89c4\u6a21\u89c6\u9891\u6570\u636e\u5b66\u4e60\u5e73\u6ed1\u8fc7\u6e21\u3002", "method": "\u63d0\u51faStereoDiff\uff0c\u5206\u4e24\u9636\u6bb5\uff1a\u7acb\u4f53\u5339\u914d\u5904\u7406\u9759\u6001\u533a\u57df\uff0c\u89c6\u9891\u6df1\u5ea6\u6269\u6563\u5904\u7406\u52a8\u6001\u533a\u57df\uff0c\u901a\u8fc7\u9891\u57df\u5206\u6790\u5c55\u793a\u4e8c\u8005\u4e92\u8865\u6027\u3002", "result": "\u5728\u96f6\u6837\u672c\u3001\u771f\u5b9e\u4e16\u754c\u52a8\u6001\u89c6\u9891\u6df1\u5ea6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cStereoDiff\u8868\u73b0\u6700\u4f18\uff0c\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u663e\u8457\u63d0\u5347\u3002", "conclusion": "StereoDiff\u901a\u8fc7\u7ed3\u5408\u7acb\u4f53\u5339\u914d\u548c\u89c6\u9891\u6df1\u5ea6\u6269\u6563\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u9759\u6001\u548c\u52a8\u6001\u533a\u57df\u7684\u4e0d\u540c\u9700\u6c42\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u3002"}}
{"id": "2506.20832", "pdf": "https://arxiv.org/pdf/2506.20832", "abs": "https://arxiv.org/abs/2506.20832", "authors": ["Cansu Korkmaz", "Ahmet Murat Tekalp", "Zafer Dogan"], "title": "Leveraging Vision-Language Models to Select Trustworthy Super-Resolution Samples Generated by Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages, 9 figures, 5 tables, accepted to IEEE Transactions on   Circuits and Systems for Video Technology", "summary": "Super-resolution (SR) is an ill-posed inverse problem with many feasible solutions consistent with a given low-resolution image. On one hand, regressive SR models aim to balance fidelity and perceptual quality to yield a single solution, but this trade-off often introduces artifacts that create ambiguity in information-critical applications such as recognizing digits or letters. On the other hand, diffusion models generate a diverse set of SR images, but selecting the most trustworthy solution from this set remains a challenge. This paper introduces a robust, automated framework for identifying the most trustworthy SR sample from a diffusion-generated set by leveraging the semantic reasoning capabilities of vision-language models (VLMs). Specifically, VLMs such as BLIP-2, GPT-4o, and their variants are prompted with structured queries to assess semantic correctness, visual quality, and artifact presence. The top-ranked SR candidates are then ensembled to yield a single trustworthy output in a cost-effective manner. To rigorously assess the validity of VLM-selected samples, we propose a novel Trustworthiness Score (TWS) a hybrid metric that quantifies SR reliability based on three complementary components: semantic similarity via CLIP embeddings, structural integrity using SSIM on edge maps, and artifact sensitivity through multi-level wavelet decomposition. We empirically show that TWS correlates strongly with human preference in both ambiguous and natural images, and that VLM-guided selections consistently yield high TWS values. Compared to conventional metrics like PSNR, LPIPS, which fail to reflect information fidelity, our approach offers a principled, scalable, and generalizable solution for navigating the uncertainty of the diffusion SR space. By aligning outputs with human expectations and semantic correctness, this work sets a new benchmark for trustworthiness in generative SR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4ece\u6269\u6563\u751f\u6210\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u9009\u62e9\u6700\u53ef\u4fe1\u6837\u672c\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u6df7\u5408\u6307\u6807Trustworthiness Score\uff08TWS\uff09\u8bc4\u4f30\u5176\u53ef\u9760\u6027\u3002", "motivation": "\u89e3\u51b3\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u95ee\u9898\u4e2d\u4f20\u7edf\u65b9\u6cd5\u5728\u4fe1\u606f\u4fdd\u771f\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u4e0d\u8db3\uff0c\u4ee5\u53ca\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u6837\u6027\u6837\u672c\u540e\u9009\u62e9\u53ef\u4fe1\u6837\u672c\u7684\u6311\u6218\u3002", "method": "\u5229\u7528VLM\uff08\u5982BLIP-2\u3001GPT-4o\uff09\u901a\u8fc7\u7ed3\u6784\u5316\u67e5\u8be2\u8bc4\u4f30\u8bed\u4e49\u6b63\u786e\u6027\u3001\u89c6\u89c9\u8d28\u91cf\u548c\u4f2a\u5f71\u5b58\u5728\uff0c\u5e76\u8bbe\u8ba1TWS\u6307\u6807\uff08\u7ed3\u5408CLIP\u5d4c\u5165\u3001SSIM\u548c\u5c0f\u6ce2\u5206\u89e3\uff09\u91cf\u5316\u53ef\u9760\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTWS\u4e0e\u4eba\u7c7b\u504f\u597d\u9ad8\u5ea6\u76f8\u5173\uff0c\u4e14VLM\u5f15\u5bfc\u7684\u9009\u62e9\u80fd\u6301\u7eed\u83b7\u5f97\u9ad8TWS\u503c\uff0c\u4f18\u4e8e\u4f20\u7edf\u6307\u6807\u5982PSNR\u548cLPIPS\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6269\u6563SR\u7a7a\u95f4\u7684\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u751f\u6210SR\u7684\u53ef\u4fe1\u6027\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2506.20879", "pdf": "https://arxiv.org/pdf/2506.20879", "abs": "https://arxiv.org/abs/2506.20879", "authors": ["Shubhankar Borse", "Seokeon Choi", "Sunghyun Park", "Jeongho Kim", "Shreya Kadambi", "Risheek Garrepalli", "Sungrack Yun", "Munawar Hayat", "Fatih Porikli"], "title": "MultiHuman-Testbench: Benchmarking Image Generation for Multiple Humans", "categories": ["cs.CV"], "comment": null, "summary": "Generation of images containing multiple humans, performing complex actions, while preserving their facial identities, is a significant challenge. A major factor contributing to this is the lack of a a dedicated benchmark. To address this, we introduce MultiHuman-Testbench, a novel benchmark for rigorously evaluating generative models for multi-human generation. The benchmark comprises 1800 samples, including carefully curated text prompts, describing a range of simple to complex human actions. These prompts are matched with a total of 5,550 unique human face images, sampled uniformly to ensure diversity across age, ethnic background, and gender. Alongside captions, we provide human-selected pose conditioning images which accurately match the prompt. We propose a multi-faceted evaluation suite employing four key metrics to quantify face count, ID similarity, prompt alignment, and action detection. We conduct a thorough evaluation of a diverse set of models, including zero-shot approaches and training-based methods, with and without regional priors. We also propose novel techniques to incorporate image and region isolation using human segmentation and Hungarian matching, significantly improving ID similarity. Our proposed benchmark and key findings provide valuable insights and a standardized tool for advancing research in multi-human image generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MultiHuman-Testbench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u4eba\u751f\u6210\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1800\u4e2a\u6837\u672c\u548c5550\u5f20\u4eba\u8138\u56fe\u50cf\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u591a\u4eba\u751f\u6210\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u9650\u5236\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u6784\u5efa\u5305\u542b\u591a\u6837\u6587\u672c\u63d0\u793a\u548c\u4eba\u8138\u56fe\u50cf\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63d0\u51fa\u57fa\u4e8e\u5206\u5272\u548c\u5308\u7259\u5229\u5339\u914d\u7684\u65b0\u6280\u672f\uff0c\u5e76\u8bbe\u8ba1\u56db\u9879\u5173\u952e\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u548c\u65b0\u6280\u672f\u663e\u8457\u63d0\u9ad8\u4e86\u8eab\u4efd\u76f8\u4f3c\u6027\uff0c\u4e3a\u591a\u4eba\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u5de5\u5177\u3002", "conclusion": "MultiHuman-Testbench\u4e3a\u591a\u4eba\u751f\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u548c\u6807\u51c6\u5316\u5de5\u5177\u3002"}}
{"id": "2506.20922", "pdf": "https://arxiv.org/pdf/2506.20922", "abs": "https://arxiv.org/abs/2506.20922", "authors": ["Ju-Hyeon Nam", "Dong-Hyun Moon", "Sang-Chul Lee"], "title": "M2SFormer: Multi-Spectral and Multi-Scale Attention with Edge-Aware Difficulty Guidance for Image Forgery Localization", "categories": ["cs.CV"], "comment": "Accepted in International Conference on Computer Vision (ICCV) 2025", "summary": "Image editing techniques have rapidly advanced, facilitating both innovative use cases and malicious manipulation of digital images. Deep learning-based methods have recently achieved high accuracy in pixel-level forgery localization, yet they frequently struggle with computational overhead and limited representation power, particularly for subtle or complex tampering. In this paper, we propose M2SFormer, a novel Transformer encoder-based framework designed to overcome these challenges. Unlike approaches that process spatial and frequency cues separately, M2SFormer unifies multi-frequency and multi-scale attentions in the skip connection, harnessing global context to better capture diverse forgery artifacts. Additionally, our framework addresses the loss of fine detail during upsampling by utilizing a global prior map, a curvature metric indicating the difficulty of forgery localization, which then guides a difficulty-guided attention module to preserve subtle manipulations more effectively. Extensive experiments on multiple benchmark datasets demonstrate that M2SFormer outperforms existing state-of-the-art models, offering superior generalization in detecting and localizing forgeries across unseen domains.", "AI": {"tldr": "M2SFormer\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7f16\u7801\u5668\u7684\u65b0\u578b\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u56fe\u50cf\u4f2a\u9020\u5b9a\u4f4d\u4e2d\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u8868\u793a\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u50cf\u7d20\u7ea7\u4f2a\u9020\u5b9a\u4f4d\u4e2d\u867d\u51c6\u786e\u4f46\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u4e14\u5bf9\u590d\u6742\u6216\u7ec6\u5fae\u7be1\u6539\u7684\u8868\u793a\u80fd\u529b\u6709\u9650\u3002", "method": "M2SFormer\u901a\u8fc7\u7edf\u4e00\u591a\u9891\u7387\u548c\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408\u5168\u5c40\u5148\u9a8c\u56fe\u548c\u96be\u5ea6\u5f15\u5bfc\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u63d0\u5347\u4f2a\u9020\u5b9a\u4f4d\u6548\u679c\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cM2SFormer\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u6a21\u578b\uff0c\u5c24\u5176\u5728\u8de8\u57df\u4f2a\u9020\u68c0\u6d4b\u548c\u5b9a\u4f4d\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "M2SFormer\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u4f2a\u9020\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.20967", "pdf": "https://arxiv.org/pdf/2506.20967", "abs": "https://arxiv.org/abs/2506.20967", "authors": ["Lingling Cai", "Kang Zhao", "Hangjie Yuan", "Xiang Wang", "Yingya Zhang", "Kejie Huang"], "title": "DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing", "categories": ["cs.CV", "cs.AI"], "comment": "Zero-shot video editing", "summary": "The advent of Video Diffusion Transformers (Video DiTs) marks a milestone in video generation. However, directly applying existing video editing methods to Video DiTs often incurs substantial computational overhead, due to resource-intensive attention modification or finetuning. To alleviate this problem, we present DFVEdit, an efficient zero-shot video editing method tailored for Video DiTs. DFVEdit eliminates the need for both attention modification and fine-tuning by directly operating on clean latents via flow transformation. To be more specific, we observe that editing and sampling can be unified under the continuous flow perspective. Building upon this foundation, we propose the Conditional Delta Flow Vector (CDFV) -- a theoretically unbiased estimation of DFV -- and integrate Implicit Cross Attention (ICA) guidance as well as Embedding Reinforcement (ER) to further enhance editing quality. DFVEdit excels in practical efficiency, offering at least 20x inference speed-up and 85\\% memory reduction on Video DiTs compared to attention-engineering-based editing methods. Extensive quantitative and qualitative experiments demonstrate that DFVEdit can be seamlessly applied to popular Video DiTs (e.g., CogVideoX and Wan2.1), attaining state-of-the-art performance on structural fidelity, spatial-temporal consistency, and editing quality.", "AI": {"tldr": "DFVEdit\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u96f6\u6837\u672c\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\uff0c\u4e13\u4e3aVideo DiTs\u8bbe\u8ba1\uff0c\u65e0\u9700\u6ce8\u610f\u529b\u4fee\u6539\u6216\u5fae\u8c03\uff0c\u901a\u8fc7\u6d41\u8f6c\u6362\u76f4\u63a5\u64cd\u4f5c\u6f5c\u5728\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u7f16\u8f91\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u5e94\u7528\u4e8eVideo DiTs\u65f6\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faDFVEdit\uff0c\u57fa\u4e8e\u6d41\u53d8\u6362\u7edf\u4e00\u7f16\u8f91\u4e0e\u91c7\u6837\uff0c\u5f15\u5165CDFV\u3001ICA\u548cER\u6280\u672f\u3002", "result": "DFVEdit\u5728Video DiTs\u4e0a\u5b9e\u73b020\u500d\u63a8\u7406\u52a0\u901f\u548c85%\u5185\u5b58\u8282\u7701\uff0c\u7f16\u8f91\u8d28\u91cf\u8fbe\u5230SOTA\u3002", "conclusion": "DFVEdit\u9ad8\u6548\u4e14\u901a\u7528\uff0c\u9002\u7528\u4e8e\u4e3b\u6d41Video DiTs\uff0c\u6027\u80fd\u4f18\u8d8a\u3002"}}
{"id": "2506.20977", "pdf": "https://arxiv.org/pdf/2506.20977", "abs": "https://arxiv.org/abs/2506.20977", "authors": ["Tao Liu", "Dafeng Zhang", "Gengchen Li", "Shizhuo Liu", "Yongqi Song", "Senmao Li", "Shiqi Yang", "Boqian Li", "Kai Wang", "Yaxing Wang"], "title": "From Cradle to Cane: A Two-Pass Framework for High-Fidelity Lifespan Face Aging", "categories": ["cs.CV", "cs.AI"], "comment": "30 pages, 12 figures", "summary": "Face aging has become a crucial task in computer vision, with applications ranging from entertainment to healthcare. However, existing methods struggle with achieving a realistic and seamless transformation across the entire lifespan, especially when handling large age gaps or extreme head poses. The core challenge lies in balancing age accuracy and identity preservation--what we refer to as the Age-ID trade-off. Most prior methods either prioritize age transformation at the expense of identity consistency or vice versa. In this work, we address this issue by proposing a two-pass face aging framework, named Cradle2Cane, based on few-step text-to-image (T2I) diffusion models. The first pass focuses on solving age accuracy by introducing an adaptive noise injection (AdaNI) mechanism. This mechanism is guided by including prompt descriptions of age and gender for the given person as the textual condition. Also, by adjusting the noise level, we can control the strength of aging while allowing more flexibility in transforming the face. However, identity preservation is weakly ensured here to facilitate stronger age transformations. In the second pass, we enhance identity preservation while maintaining age-specific features by conditioning the model on two identity-aware embeddings (IDEmb): SVR-ArcFace and Rotate-CLIP. This pass allows for denoising the transformed image from the first pass, ensuring stronger identity preservation without compromising the aging accuracy. Both passes are jointly trained in an end-to-end way. Extensive experiments on the CelebA-HQ test dataset, evaluated through Face++ and Qwen-VL protocols, show that our Cradle2Cane outperforms existing face aging methods in age accuracy and identity consistency.", "AI": {"tldr": "Cradle2Cane\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4e24\u9636\u6bb5\u4eba\u8138\u8001\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u566a\u58f0\u6ce8\u5165\u548c\u8eab\u4efd\u611f\u77e5\u5d4c\u5165\u89e3\u51b3\u5e74\u9f84-\u8eab\u4efd\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u5e74\u9f84\u51c6\u786e\u6027\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5c24\u5176\u662f\u5728\u5927\u5e74\u9f84\u8de8\u5ea6\u6216\u6781\u7aef\u5934\u90e8\u59ff\u6001\u4e0b\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u81ea\u9002\u5e94\u566a\u58f0\u6ce8\u5165\uff08AdaNI\uff09\u5b9e\u73b0\u5e74\u9f84\u51c6\u786e\u6027\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u8eab\u4efd\u611f\u77e5\u5d4c\u5165\uff08IDEmb\uff09\u589e\u5f3a\u8eab\u4efd\u4e00\u81f4\u6027\u3002", "result": "\u5728CelebA-HQ\u6d4b\u8bd5\u96c6\u4e0a\uff0cCradle2Cane\u5728\u5e74\u9f84\u51c6\u786e\u6027\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Cradle2Cane\u6709\u6548\u89e3\u51b3\u4e86\u4eba\u8138\u8001\u5316\u4e2d\u7684Age-ID\u5e73\u8861\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.20983", "pdf": "https://arxiv.org/pdf/2506.20983", "abs": "https://arxiv.org/abs/2506.20983", "authors": ["Wenjie Xuan", "Jing Zhang", "Juhua Liu", "Bo Du", "Dacheng Tao"], "title": "Rethink Sparse Signals for Pose-guided Text-to-image Generation", "categories": ["cs.CV"], "comment": "accepted by ICCV 2025", "summary": "Recent works favored dense signals (e.g., depth, DensePose), as an alternative to sparse signals (e.g., OpenPose), to provide detailed spatial guidance for pose-guided text-to-image generation. However, dense representations raised new challenges, including editing difficulties and potential inconsistencies with textual prompts. This fact motivates us to revisit sparse signals for pose guidance, owing to their simplicity and shape-agnostic nature, which remains underexplored. This paper proposes a novel Spatial-Pose ControlNet(SP-Ctrl), equipping sparse signals with robust controllability for pose-guided image generation. Specifically, we extend OpenPose to a learnable spatial representation, making keypoint embeddings discriminative and expressive. Additionally, we introduce keypoint concept learning, which encourages keypoint tokens to attend to the spatial positions of each keypoint, thus improving pose alignment. Experiments on animal- and human-centric image generation tasks demonstrate that our method outperforms recent spatially controllable T2I generation approaches under sparse-pose guidance and even matches the performance of dense signal-based methods. Moreover, SP-Ctrl shows promising capabilities in diverse and cross-species generation through sparse signals. Codes will be available at https://github.com/DREAMXFAR/SP-Ctrl.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u4fe1\u53f7\uff08\u5982OpenPose\uff09\u7684\u65b0\u578b\u7a7a\u95f4\u59ff\u6001\u63a7\u5236\u7f51\u7edc\uff08SP-Ctrl\uff09\uff0c\u7528\u4e8e\u59ff\u6001\u5f15\u5bfc\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u5bc6\u96c6\u4fe1\u53f7\uff08\u5982\u6df1\u5ea6\u56fe\uff09\u5e26\u6765\u7684\u7f16\u8f91\u56f0\u96be\u548c\u6587\u672c\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u5bc6\u96c6\u4fe1\u53f7\uff08\u5982\u6df1\u5ea6\u56fe\uff09\u5728\u59ff\u6001\u5f15\u5bfc\u7684\u56fe\u50cf\u751f\u6210\u4e2d\u5b58\u5728\u7f16\u8f91\u56f0\u96be\u548c\u4e0e\u6587\u672c\u63d0\u793a\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u800c\u7a00\u758f\u4fe1\u53f7\uff08\u5982OpenPose\uff09\u56e0\u5176\u7b80\u5355\u6027\u548c\u5f62\u72b6\u65e0\u5173\u6027\u88ab\u91cd\u65b0\u63a2\u7d22\u3002", "method": "\u6269\u5c55OpenPose\u4e3a\u53ef\u5b66\u4e60\u7684\u7a7a\u95f4\u8868\u793a\uff0c\u5f15\u5165\u5173\u952e\u70b9\u6982\u5ff5\u5b66\u4e60\uff0c\u4f7f\u5173\u952e\u70b9\u5d4c\u5165\u66f4\u5177\u533a\u5206\u6027\u548c\u8868\u8fbe\u529b\uff0c\u5e76\u6539\u8fdb\u59ff\u6001\u5bf9\u9f50\u3002", "result": "\u5728\u4eba\u7c7b\u548c\u52a8\u7269\u7684\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0cSP-Ctrl\u4f18\u4e8e\u5176\u4ed6\u7a00\u758f\u59ff\u6001\u5f15\u5bfc\u65b9\u6cd5\uff0c\u751a\u81f3\u4e0e\u5bc6\u96c6\u4fe1\u53f7\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u7269\u79cd\u751f\u6210\u7684\u6f5c\u529b\u3002", "conclusion": "\u7a00\u758f\u4fe1\u53f7\u5728\u59ff\u6001\u5f15\u5bfc\u7684\u56fe\u50cf\u751f\u6210\u4e2d\u5177\u6709\u6f5c\u529b\uff0cSP-Ctrl\u4e3a\u7a00\u758f\u4fe1\u53f7\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u53ef\u63a7\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7b80\u5355\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2506.20998", "pdf": "https://arxiv.org/pdf/2506.20998", "abs": "https://arxiv.org/abs/2506.20998", "authors": ["Yeon-Ji Song", "Jaein Kim", "Byung-Ju Kim", "Byoung-Tak Zhang"], "title": "DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting", "categories": ["cs.CV"], "comment": "CVPRW 2025, Neural Fields Beyond Conventional Cameras", "summary": "Novel view synthesis is a task of generating scenes from unseen perspectives; however, synthesizing dynamic scenes from blurry monocular videos remains an unresolved challenge that has yet to be effectively addressed. Existing novel view synthesis methods are often constrained by their reliance on high-resolution images or strong assumptions about static geometry and rigid scene priors. Consequently, their approaches lack robustness in real-world environments with dynamic object and camera motion, leading to instability and degraded visual fidelity. To address this, we propose Motion-aware Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting (DBMovi-GS), a method designed for dynamic view synthesis from blurry monocular videos. Our model generates dense 3D Gaussians, restoring sharpness from blurry videos and reconstructing detailed 3D geometry of the scene affected by dynamic motion variations. Our model achieves robust performance in novel view synthesis under dynamic blurry scenes and sets a new benchmark in realistic novel view synthesis for blurry monocular video inputs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5DBMovi-GS\uff0c\u7528\u4e8e\u4ece\u6a21\u7cca\u5355\u76ee\u89c6\u9891\u4e2d\u5408\u6210\u52a8\u6001\u573a\u666f\u7684\u65b0\u89c6\u89d2\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u548c\u6a21\u7cca\u573a\u666f\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u6216\u9759\u6001\u51e0\u4f55\u5047\u8bbe\uff0c\u96be\u4ee5\u5904\u7406\u52a8\u6001\u7269\u4f53\u548c\u76f8\u673a\u8fd0\u52a8\u5bfc\u81f4\u7684\u6a21\u7cca\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u63a7\u5236\u7684\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u4ece\u6a21\u7cca\u89c6\u9891\u4e2d\u751f\u6210\u5bc6\u96c63D\u9ad8\u65af\uff0c\u6062\u590d\u6e05\u6670\u5ea6\u5e76\u91cd\u5efa\u52a8\u6001\u573a\u666f\u76843D\u51e0\u4f55\u3002", "result": "\u6a21\u578b\u5728\u52a8\u6001\u6a21\u7cca\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u6a21\u7cca\u5355\u76ee\u89c6\u9891\u8f93\u5165\u8bbe\u5b9a\u4e86\u65b0\u89c6\u89d2\u5408\u6210\u7684\u57fa\u51c6\u3002", "conclusion": "DBMovi-GS\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u6a21\u7cca\u573a\u666f\u7684\u65b0\u89c6\u89d2\u5408\u6210\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2506.21006", "pdf": "https://arxiv.org/pdf/2506.21006", "abs": "https://arxiv.org/abs/2506.21006", "authors": ["Tyler Ward", "Xiaoqin Wang", "Braxton McFarland", "Md Atik Ahamed", "Sahar Nozad", "Talal Arshad", "Hafsa Nebbache", "Jin Chen", "Abdullah Imran"], "title": "Detection of Breast Cancer Lumpectomy Margin with SAM-incorporated Forward-Forward Contrastive Learning", "categories": ["cs.CV"], "comment": "19 pages, 7 figures, 3 tables", "summary": "Complete removal of cancer tumors with a negative specimen margin during lumpectomy is essential in reducing breast cancer recurrence. However, 2D specimen radiography (SR), the current method used to assess intraoperative specimen margin status, has limited accuracy, resulting in nearly a quarter of patients requiring additional surgery. To address this, we propose a novel deep learning framework combining the Segment Anything Model (SAM) with Forward-Forward Contrastive Learning (FFCL), a pre-training strategy leveraging both local and global contrastive learning for patch-level classification of SR images. After annotating SR images with regions of known maligancy, non-malignant tissue, and pathology-confirmed margins, we pre-train a ResNet-18 backbone with FFCL to classify margin status, then reconstruct coarse binary masks to prompt SAM for refined tumor margin segmentation. Our approach achieved an AUC of 0.8455 for margin classification and segmented margins with a 27.4% improvement in Dice similarity over baseline models, while reducing inference time to 47 milliseconds per image. These results demonstrate that FFCL-SAM significantly enhances both the speed and accuracy of intraoperative margin assessment, with strong potential to reduce re-excision rates and improve surgical outcomes in breast cancer treatment. Our code is available at https://github.com/tbwa233/FFCL-SAM/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408SAM\u548cFFCL\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u4e73\u817a\u764c\u672f\u4e2d\u6807\u672c\u8fb9\u7f18\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u901f\u5ea6\u3002", "motivation": "\u5f53\u524d2D\u6807\u672c\u653e\u5c04\u6210\u50cf\uff08SR\uff09\u8bc4\u4f30\u8fb9\u7f18\u72b6\u6001\u7684\u51c6\u786e\u6027\u6709\u9650\uff0c\u5bfc\u81f4\u8fd1\u56db\u5206\u4e4b\u4e00\u7684\u60a3\u8005\u9700\u8981\u989d\u5916\u624b\u672f\u3002", "method": "\u7ed3\u5408FFCL\u9884\u8bad\u7ec3\u7b56\u7565\u548cSAM\u6a21\u578b\uff0c\u901a\u8fc7\u6807\u6ce8SR\u56fe\u50cf\u5e76\u9884\u8bad\u7ec3ResNet-18\uff0c\u751f\u6210\u7c97\u4e8c\u8fdb\u5236\u63a9\u7801\u4ee5\u4f18\u5316\u8fb9\u7f18\u5206\u5272\u3002", "result": "AUC\u4e3a0.8455\uff0cDice\u76f8\u4f3c\u6027\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u9ad827.4%\uff0c\u63a8\u7406\u65f6\u95f4\u7f29\u77ed\u81f347\u6beb\u79d2/\u56fe\u50cf\u3002", "conclusion": "FFCL-SAM\u663e\u8457\u63d0\u5347\u4e86\u672f\u4e2d\u8fb9\u7f18\u8bc4\u4f30\u7684\u901f\u5ea6\u548c\u51c6\u786e\u6027\uff0c\u6709\u671b\u964d\u4f4e\u518d\u5207\u9664\u7387\u5e76\u6539\u5584\u4e73\u817a\u764c\u6cbb\u7597\u6548\u679c\u3002"}}
{"id": "2506.21009", "pdf": "https://arxiv.org/pdf/2506.21009", "abs": "https://arxiv.org/abs/2506.21009", "authors": ["Ayaka Yasunaga", "Hideo Saito", "Shohei Mori"], "title": "User-in-the-Loop View Sampling with Error Peaking Visualization", "categories": ["cs.CV"], "comment": "Accepted at IEEE ICIP 2025, Project Page:   https://mediated-reality.github.io/projects/yasunaga_icip25/", "summary": "Augmented reality (AR) provides ways to visualize missing view samples for novel view synthesis. Existing approaches present 3D annotations for new view samples and task users with taking images by aligning the AR display. This data collection task is known to be mentally demanding and limits capture areas to pre-defined small areas due to the ideal but restrictive underlying sampling theory. To free users from 3D annotations and limited scene exploration, we propose using locally reconstructed light fields and visualizing errors to be removed by inserting new views. Our results show that the error-peaking visualization is less invasive, reduces disappointment in final results, and is satisfactory with fewer view samples in our mobile view synthesis system. We also show that our approach can contribute to recent radiance field reconstruction for larger scenes, such as 3D Gaussian splatting.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c40\u90e8\u91cd\u5efa\u5149\u573a\u548c\u53ef\u89c6\u5316\u8bef\u5dee\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u51cf\u5c11AR\u4e2d3D\u6807\u6ce8\u7684\u9700\u6c42\uff0c\u63d0\u5347\u65b0\u89c6\u89d2\u5408\u6210\u7684\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u73b0\u6709AR\u65b9\u6cd5\u4f9d\u8d563D\u6807\u6ce8\u4e14\u9650\u5236\u573a\u666f\u63a2\u7d22\u8303\u56f4\uff0c\u5bfc\u81f4\u6570\u636e\u6536\u96c6\u4efb\u52a1\u7e41\u91cd\u4e14\u573a\u666f\u53d7\u9650\u3002", "method": "\u4f7f\u7528\u5c40\u90e8\u91cd\u5efa\u5149\u573a\u548c\u53ef\u89c6\u5316\u8bef\u5dee\uff0c\u6307\u5bfc\u7528\u6237\u63d2\u5165\u65b0\u89c6\u89d2\u4ee5\u51cf\u5c11\u8bef\u5dee\u3002", "result": "\u8bef\u5dee\u5cf0\u503c\u53ef\u89c6\u5316\u65b9\u6cd5\u4fb5\u5165\u6027\u66f4\u4f4e\uff0c\u51cf\u5c11\u7528\u6237\u5931\u671b\uff0c\u4e14\u9700\u8981\u66f4\u5c11\u7684\u89c6\u89d2\u6837\u672c\u3002\u8be5\u65b9\u6cd5\u8fd8\u53ef\u7528\u4e8e\u5927\u573a\u666f\u7684\u8f90\u5c04\u573a\u91cd\u5efa\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u89e3\u653e\u4e86\u7528\u6237\u5bf93D\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u6269\u5c55\u4e86\u573a\u666f\u63a2\u7d22\u8303\u56f4\uff0c\u63d0\u5347\u4e86\u65b0\u89c6\u89d2\u5408\u6210\u7684\u6548\u679c\u548c\u6548\u7387\u3002"}}
{"id": "2506.21015", "pdf": "https://arxiv.org/pdf/2506.21015", "abs": "https://arxiv.org/abs/2506.21015", "authors": ["Qingyue Jiao", "Kangyu Zheng", "Yiyu Shi", "Zhiding Liang"], "title": "HybridQ: Hybrid Classical-Quantum Generative Adversarial Network for Skin Disease Image Generation", "categories": ["cs.CV", "cs.LG", "quant-ph"], "comment": null, "summary": "Machine learning-assisted diagnosis is gaining traction in skin disease detection, but training effective models requires large amounts of high-quality data. Skin disease datasets often suffer from class imbalance, privacy concerns, and object bias, making data augmentation essential. While classical generative models are widely used, they demand extensive computational resources and lengthy training time. Quantum computing offers a promising alternative, but existing quantum-based image generation methods can only yield grayscale low-quality images. Through a novel classical-quantum latent space fusion technique, our work overcomes this limitation and introduces the first classical-quantum generative adversarial network (GAN) capable of generating color medical images. Our model outperforms classical deep convolutional GANs and existing hybrid classical-quantum GANs in both image generation quality and classification performance boost when used as data augmentation. Moreover, the performance boost is comparable with that achieved using state-of-the-art classical generative models, yet with over 25 times fewer parameters and 10 times fewer training epochs. Such results suggest a promising future for quantum image generation as quantum hardware advances. Finally, we demonstrate the robust performance of our model on real IBM quantum machine with hardware noise.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ecf\u5178-\u91cf\u5b50\u6df7\u5408\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\uff0c\u80fd\u591f\u751f\u6210\u5f69\u8272\u533b\u5b66\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u91cf\u5b50\u56fe\u50cf\u751f\u6210\u7684\u4f4e\u8d28\u91cf\u548c\u7070\u5ea6\u9650\u5236\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u76ae\u80a4\u75be\u75c5\u6570\u636e\u96c6\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u9690\u79c1\u95ee\u9898\u548c\u5bf9\u8c61\u504f\u5dee\u95ee\u9898\u4f7f\u5f97\u6570\u636e\u589e\u5f3a\u81f3\u5173\u91cd\u8981\uff0c\u800c\u73b0\u6709\u91cf\u5b50\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u53ea\u80fd\u751f\u6210\u4f4e\u8d28\u91cf\u7070\u5ea6\u56fe\u50cf\u3002", "method": "\u901a\u8fc7\u4e00\u79cd\u65b0\u9896\u7684\u7ecf\u5178-\u91cf\u5b50\u6f5c\u5728\u7a7a\u95f4\u878d\u5408\u6280\u672f\uff0c\u5f00\u53d1\u4e86\u9996\u4e2a\u80fd\u591f\u751f\u6210\u5f69\u8272\u533b\u5b66\u56fe\u50cf\u7684\u7ecf\u5178-\u91cf\u5b50GAN\u3002", "result": "\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u548c\u5206\u7c7b\u6027\u80fd\u63d0\u5347\u4e0a\u4f18\u4e8e\u7ecf\u5178\u6df1\u5ea6\u5377\u79efGAN\u548c\u73b0\u6709\u6df7\u5408\u7ecf\u5178-\u91cf\u5b50GAN\uff0c\u4e14\u53c2\u6570\u548c\u8bad\u7ec3\u65f6\u95f4\u5927\u5e45\u51cf\u5c11\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u91cf\u5b50\u56fe\u50cf\u751f\u6210\u5728\u533b\u5b66\u9886\u57df\u7684\u5e94\u7528\u5c55\u793a\u4e86\u5e7f\u9614\u524d\u666f\uff0c\u5c24\u5176\u662f\u5728\u91cf\u5b50\u786c\u4ef6\u8fdb\u4e00\u6b65\u53d1\u5c55\u7684\u80cc\u666f\u4e0b\u3002"}}
{"id": "2506.21022", "pdf": "https://arxiv.org/pdf/2506.21022", "abs": "https://arxiv.org/abs/2506.21022", "authors": ["Ze Wang", "Hao Chen", "Benran Hu", "Jiang Liu", "Ximeng Sun", "Jialian Wu", "Yusheng Su", "Xiaodong Yu", "Emad Barsoum", "Zicheng Liu"], "title": "Instella-T2I: Pushing the Limits of 1D Discrete Latent Space Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Image tokenization plays a critical role in reducing the computational demands of modeling high-resolution images, significantly improving the efficiency of image and multimodal understanding and generation. Recent advances in 1D latent spaces have reduced the number of tokens required by eliminating the need for a 2D grid structure. In this paper, we further advance compact discrete image representation by introducing 1D binary image latents. By representing each image as a sequence of binary vectors, rather than using traditional one-hot codebook tokens, our approach preserves high-resolution details while maintaining the compactness of 1D latents. To the best of our knowledge, our text-to-image models are the first to achieve competitive performance in both diffusion and auto-regressive generation using just 128 discrete tokens for images up to 1024x1024, demonstrating up to a 32-fold reduction in token numbers compared to standard VQ-VAEs. The proposed 1D binary latent space, coupled with simple model architectures, achieves marked improvements in speed training and inference speed. Our text-to-image models allow for a global batch size of 4096 on a single GPU node with 8 AMD MI300X GPUs, and the training can be completed within 200 GPU days. Our models achieve competitive performance compared to modern image generation models without any in-house private training data or post-training refinements, offering a scalable and efficient alternative to conventional tokenization methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd1D\u4e8c\u8fdb\u5236\u56fe\u50cf\u6f5c\u5728\u8868\u793a\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u56fe\u50cf\u5efa\u6a21\u6240\u9700\u7684token\u6570\u91cf\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u4f20\u7edf2D\u7f51\u683c\u7ed3\u6784\u7684\u56fe\u50cftoken\u5316\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\uff0c1D\u6f5c\u5728\u7a7a\u95f4\u867d\u51cf\u5c11\u4e86token\u6570\u91cf\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "method": "\u5f15\u51651D\u4e8c\u8fdb\u5236\u56fe\u50cf\u6f5c\u5728\u8868\u793a\uff0c\u5c06\u56fe\u50cf\u8868\u793a\u4e3a\u4e8c\u8fdb\u5236\u5411\u91cf\u5e8f\u5217\uff0c\u66ff\u4ee3\u4f20\u7edfone-hot\u7f16\u7801\u3002", "result": "\u57281024x1024\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u4ec5\u9700128\u4e2atoken\uff0c\u6bd4\u6807\u51c6VQ-VAE\u51cf\u5c1132\u500d\uff0c\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u56fe\u50cftoken\u5316\u66ff\u4ee3\u65b9\u6848\uff0c\u65e0\u9700\u79c1\u6709\u6570\u636e\u6216\u540e\u8bad\u7ec3\u4f18\u5316\u5373\u53ef\u8fbe\u5230\u73b0\u4ee3\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.21045", "pdf": "https://arxiv.org/pdf/2506.21045", "abs": "https://arxiv.org/abs/2506.21045", "authors": ["Hansam Cho", "Seoung Bum Kim"], "title": "Improving Diffusion-Based Image Editing Faithfulness via Guidance and Scheduling", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "preprint", "summary": "Text-guided diffusion models have become essential for high-quality image synthesis, enabling dynamic image editing. In image editing, two crucial aspects are editability, which determines the extent of modification, and faithfulness, which reflects how well unaltered elements are preserved. However, achieving optimal results is challenging because of the inherent trade-off between editability and faithfulness. To address this, we propose Faithfulness Guidance and Scheduling (FGS), which enhances faithfulness with minimal impact on editability. FGS incorporates faithfulness guidance to strengthen the preservation of input image information and introduces a scheduling strategy to resolve misalignment between editability and faithfulness. Experimental results demonstrate that FGS achieves superior faithfulness while maintaining editability. Moreover, its compatibility with various editing methods enables precise, high-quality image edits across diverse tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFGS\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5fe0\u5b9e\u6027\u6307\u5bfc\u548c\u8c03\u5ea6\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u56fe\u50cf\u7f16\u8f91\u4e2d\u7f16\u8f91\u6027\u4e0e\u5fe0\u5b9e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u5728\u6587\u672c\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u56fe\u50cf\u7f16\u8f91\u7684\u4e24\u4e2a\u5173\u952e\u65b9\u9762\u662f\u7f16\u8f91\u6027\u548c\u5fe0\u5b9e\u6027\uff0c\u4f46\u4e8c\u8005\u4e4b\u95f4\u5b58\u5728\u56fa\u6709\u6743\u8861\uff0c\u96be\u4ee5\u540c\u65f6\u8fbe\u5230\u6700\u4f18\u6548\u679c\u3002", "method": "FGS\u7ed3\u5408\u4e86\u5fe0\u5b9e\u6027\u6307\u5bfc\u4ee5\u589e\u5f3a\u8f93\u5165\u56fe\u50cf\u4fe1\u606f\u7684\u4fdd\u7559\uff0c\u5e76\u5f15\u5165\u8c03\u5ea6\u7b56\u7565\u89e3\u51b3\u7f16\u8f91\u6027\u4e0e\u5fe0\u5b9e\u6027\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFGS\u5728\u4fdd\u6301\u7f16\u8f91\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u5fe0\u5b9e\u6027\uff0c\u4e14\u517c\u5bb9\u591a\u79cd\u7f16\u8f91\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u4efb\u52a1\u3002", "conclusion": "FGS\u901a\u8fc7\u4f18\u5316\u5fe0\u5b9e\u6027\u4e0e\u7f16\u8f91\u6027\u7684\u5e73\u8861\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u7cbe\u786e\u7684\u56fe\u50cf\u7f16\u8f91\u3002"}}
{"id": "2506.21091", "pdf": "https://arxiv.org/pdf/2506.21091", "abs": "https://arxiv.org/abs/2506.21091", "authors": ["Mahmoud Tahmasebi", "Saif Huq", "Kevin Meehan", "Marion McAfee"], "title": "ESMStereo: Enhanced ShuffleMixer Disparity Upsampling for Real-Time and Accurate Stereo Matching", "categories": ["cs.CV"], "comment": "Under peer review", "summary": "Stereo matching has become an increasingly important component of modern autonomous systems. Developing deep learning-based stereo matching models that deliver high accuracy while operating in real-time continues to be a major challenge in computer vision. In the domain of cost-volume-based stereo matching, accurate disparity estimation depends heavily on large-scale cost volumes. However, such large volumes store substantial redundant information and also require computationally intensive aggregation units for processing and regression, making real-time performance unattainable. Conversely, small-scale cost volumes followed by lightweight aggregation units provide a promising route for real-time performance, but lack sufficient information to ensure highly accurate disparity estimation. To address this challenge, we propose the Enhanced Shuffle Mixer (ESM) to mitigate information loss associated with small-scale cost volumes. ESM restores critical details by integrating primary features into the disparity upsampling unit. It quickly extracts features from the initial disparity estimation and fuses them with image features. These features are mixed by shuffling and layer splitting then refined through a compact feature-guided hourglass network to recover more detailed scene geometry. The ESM focuses on local contextual connectivity with a large receptive field and low computational cost, leading to the reconstruction of a highly accurate disparity map at real-time. The compact version of ESMStereo achieves an inference speed of 116 FPS on high-end GPUs and 91 FPS on the AGX Orin.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEnhanced Shuffle Mixer (ESM)\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u57fa\u4e8e\u5c0f\u89c4\u6a21\u6210\u672c\u4f53\u79ef\u7684\u7acb\u4f53\u5339\u914d\u4e2d\u4fe1\u606f\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u7acb\u4f53\u5339\u914d\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u80fd\u3002\u5927\u89c4\u6a21\u6210\u672c\u4f53\u79ef\u5197\u4f59\u4e14\u8ba1\u7b97\u5bc6\u96c6\uff0c\u800c\u5c0f\u89c4\u6a21\u6210\u672c\u4f53\u79ef\u7f3a\u4e4f\u8db3\u591f\u4fe1\u606f\u3002", "method": "ESM\u901a\u8fc7\u5c06\u4e3b\u8981\u7279\u5f81\u6574\u5408\u5230\u89c6\u5dee\u4e0a\u91c7\u6837\u5355\u5143\u4e2d\uff0c\u6062\u590d\u5173\u952e\u7ec6\u8282\u3002\u5b83\u5feb\u901f\u63d0\u53d6\u521d\u59cb\u89c6\u5dee\u4f30\u8ba1\u7684\u7279\u5f81\u5e76\u4e0e\u56fe\u50cf\u7279\u5f81\u878d\u5408\uff0c\u901a\u8fc7\u6df7\u6d17\u548c\u5c42\u5206\u5272\u6df7\u5408\uff0c\u518d\u901a\u8fc7\u7d27\u51d1\u7684\u7279\u5f81\u5f15\u5bfc\u6c99\u6f0f\u7f51\u7edc\u7ec6\u5316\u3002", "result": "ESM\u5728\u4fdd\u6301\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u5927\u611f\u53d7\u91ce\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u89c6\u5dee\u56fe\u7684\u5b9e\u65f6\u91cd\u5efa\u3002ESMStereo\u7684\u7d27\u51d1\u7248\u672c\u5728\u9ad8\u6027\u80fdGPU\u4e0a\u8fbe\u5230116 FPS\uff0c\u5728AGX Orin\u4e0a\u8fbe\u523091 FPS\u3002", "conclusion": "ESM\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u89c4\u6a21\u6210\u672c\u4f53\u79ef\u4fe1\u606f\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6027\u80fd\uff0c\u4e3a\u7acb\u4f53\u5339\u914d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21117", "pdf": "https://arxiv.org/pdf/2506.21117", "abs": "https://arxiv.org/abs/2506.21117", "authors": ["Jan Ackermann", "Jonas Kulhanek", "Shengqu Cai", "Haofei Xu", "Marc Pollefeys", "Gordon Wetzstein", "Leonidas Guibas", "Songyou Peng"], "title": "CL-Splats: Continual Learning of Gaussian Splatting with Local Optimization", "categories": ["cs.CV"], "comment": "ICCV 2025, Project Page: https://cl-splats.github.io", "summary": "In dynamic 3D environments, accurately updating scene representations over time is crucial for applications in robotics, mixed reality, and embodied AI. As scenes evolve, efficient methods to incorporate changes are needed to maintain up-to-date, high-quality reconstructions without the computational overhead of re-optimizing the entire scene. This paper introduces CL-Splats, which incrementally updates Gaussian splatting-based 3D representations from sparse scene captures. CL-Splats integrates a robust change-detection module that segments updated and static components within the scene, enabling focused, local optimization that avoids unnecessary re-computation. Moreover, CL-Splats supports storing and recovering previous scene states, facilitating temporal segmentation and new scene-analysis applications. Our extensive experiments demonstrate that CL-Splats achieves efficient updates with improved reconstruction quality over the state-of-the-art. This establishes a robust foundation for future real-time adaptation in 3D scene reconstruction tasks.", "AI": {"tldr": "CL-Splats\u662f\u4e00\u79cd\u52a8\u60013D\u573a\u666f\u8868\u793a\u66f4\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u91cf\u66f4\u65b0\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\uff0c\u7ed3\u5408\u53d8\u5316\u68c0\u6d4b\u6a21\u5757\uff0c\u5b9e\u73b0\u5c40\u90e8\u4f18\u5316\u548c\u9ad8\u6548\u8ba1\u7b97\u3002", "motivation": "\u5728\u52a8\u60013D\u73af\u5883\u4e2d\uff0c\u5b9e\u65f6\u66f4\u65b0\u573a\u666f\u8868\u793a\u5bf9\u673a\u5668\u4eba\u3001\u6df7\u5408\u73b0\u5b9e\u548c\u5177\u8eabAI\u81f3\u5173\u91cd\u8981\uff0c\u9700\u907f\u514d\u91cd\u65b0\u4f18\u5316\u6574\u4e2a\u573a\u666f\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "method": "CL-Splats\u901a\u8fc7\u53d8\u5316\u68c0\u6d4b\u6a21\u5757\u5206\u5272\u573a\u666f\u4e2d\u7684\u52a8\u6001\u548c\u9759\u6001\u90e8\u5206\uff0c\u8fdb\u884c\u5c40\u90e8\u4f18\u5316\uff0c\u5e76\u652f\u6301\u5b58\u50a8\u548c\u6062\u590d\u5386\u53f2\u573a\u666f\u72b6\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCL-Splats\u5728\u66f4\u65b0\u6548\u7387\u548c\u91cd\u5efa\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CL-Splats\u4e3a\u672a\u6765\u5b9e\u65f63D\u573a\u666f\u91cd\u5efa\u4efb\u52a1\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2506.21132", "pdf": "https://arxiv.org/pdf/2506.21132", "abs": "https://arxiv.org/abs/2506.21132", "authors": ["Hai Jiang", "Binhao Guan", "Zhen Liu", "Xiaohong Liu", "Jian Yu", "Zheng Liu", "Songchen Han", "Shuaicheng Liu"], "title": "Learning to See in the Extremely Dark", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Learning-based methods have made promising advances in low-light RAW image enhancement, while their capability to extremely dark scenes where the environmental illuminance drops as low as 0.0001 lux remains to be explored due to the lack of corresponding datasets. To this end, we propose a paired-to-paired data synthesis pipeline capable of generating well-calibrated extremely low-light RAW images at three precise illuminance ranges of 0.01-0.1 lux, 0.001-0.01 lux, and 0.0001-0.001 lux, together with high-quality sRGB references to comprise a large-scale paired dataset named See-in-the-Extremely-Dark (SIED) to benchmark low-light RAW image enhancement approaches. Furthermore, we propose a diffusion-based framework that leverages the generative ability and intrinsic denoising property of diffusion models to restore visually pleasing results from extremely low-SNR RAW inputs, in which an Adaptive Illumination Correction Module (AICM) and a color consistency loss are introduced to ensure accurate exposure correction and color restoration. Extensive experiments on the proposed SIED and publicly available benchmarks demonstrate the effectiveness of our method. The code and dataset are available at https://github.com/JianghaiSCU/SIED.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6781\u4f4e\u5149RAW\u56fe\u50cf\u589e\u5f3a\u7684\u6570\u636e\u5408\u6210\u65b9\u6cd5\u548c\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u5e76\u521b\u5efa\u4e86SIED\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u7684\u6781\u4f4e\u5149\uff080.0001 lux\uff09RAW\u56fe\u50cf\u589e\u5f3a\u80fd\u529b\u5c1a\u672a\u63a2\u7d22\uff0c\u7f3a\u4e4f\u5bf9\u5e94\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u6570\u636e\u5408\u6210\u7ba1\u9053\u751f\u6210\u6781\u4f4e\u5149RAW\u56fe\u50cf\uff0c\u5e76\u8bbe\u8ba1\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u5305\u542b\u81ea\u9002\u5e94\u5149\u7167\u6821\u6b63\u6a21\u5757\u548c\u989c\u8272\u4e00\u81f4\u6027\u635f\u5931\u3002", "result": "\u5728SIED\u548c\u516c\u5f00\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u65b9\u6cd5\u5728\u6781\u4f4e\u5149RAW\u56fe\u50cf\u589e\u5f3a\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.21152", "pdf": "https://arxiv.org/pdf/2506.21152", "abs": "https://arxiv.org/abs/2506.21152", "authors": ["Pufan Li", "Bi'an Du", "Wei Hu"], "title": "Geometry and Perception Guided Gaussians for Multiview-consistent 3D Generation from a Single Image", "categories": ["cs.CV", "68", "I.4.0"], "comment": "10 pages, 5 figures", "summary": "Generating realistic 3D objects from single-view images requires natural appearance, 3D consistency, and the ability to capture multiple plausible interpretations of unseen regions. Existing approaches often rely on fine-tuning pretrained 2D diffusion models or directly generating 3D information through fast network inference or 3D Gaussian Splatting, but their results generally suffer from poor multiview consistency and lack geometric detail. To takle these issues, we present a novel method that seamlessly integrates geometry and perception priors without requiring additional model training to reconstruct detailed 3D objects from a single image. Specifically, we train three different Gaussian branches initialized from the geometry prior, perception prior and Gaussian noise, respectively. The geometry prior captures the rough 3D shapes, while the perception prior utilizes the 2D pretrained diffusion model to enhance multiview information. Subsequently, we refine 3D Gaussian branches through mutual interaction between geometry and perception priors, further enhanced by a reprojection-based strategy that enforces depth consistency. Experiments demonstrate the higher-fidelity reconstruction results of our method, outperforming existing methods on novel view synthesis and 3D reconstruction, demonstrating robust and consistent 3D object generation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u51e0\u4f55\u548c\u611f\u77e5\u5148\u9a8c\uff0c\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf3D\u7269\u4f53\uff0c\u89e3\u51b3\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u51e0\u4f55\u7ec6\u8282\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4ece\u5355\u89c6\u56fe\u56fe\u50cf\u751f\u62103D\u7269\u4f53\u65f6\uff0c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u51e0\u4f55\u7ec6\u8282\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u8bad\u7ec3\u4e09\u4e2a\u4e0d\u540c\u7684\u9ad8\u65af\u5206\u652f\uff08\u51e0\u4f55\u5148\u9a8c\u3001\u611f\u77e5\u5148\u9a8c\u548c\u9ad8\u65af\u566a\u58f0\uff09\uff0c\u901a\u8fc7\u4ea4\u4e92\u4f18\u5316\u548c\u91cd\u6295\u5f71\u7b56\u7565\u589e\u5f3a\u6df1\u5ea6\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u57283D\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u5408\u6210\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u7ed3\u679c\u66f4\u9ad8\u8d28\u91cf\u4e14\u4e00\u81f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u6709\u6548\u6574\u5408\u5148\u9a8c\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u56fe\u50cf3D\u91cd\u5efa\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2506.21209", "pdf": "https://arxiv.org/pdf/2506.21209", "abs": "https://arxiv.org/abs/2506.21209", "authors": ["Louis Kerner", "Michel Meintz", "Bihe Zhao", "Franziska Boenisch", "Adam Dziedzic"], "title": "BitMark for Infinity: Watermarking Bitwise Autoregressive Image Generative Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "State-of-the-art text-to-image models like Infinity generate photorealistic images at an unprecedented speed. These models operate in a bitwise autoregressive manner over a discrete set of tokens that is practically infinite in size. However, their impressive generative power comes with a growing risk: as their outputs increasingly populate the Internet, they are likely to be scraped and reused as training data-potentially by the very same models. This phenomenon has been shown to lead to model collapse, where repeated training on generated content, especially from the models' own previous versions, causes a gradual degradation in performance. A promising mitigation strategy is watermarking, which embeds human-imperceptible yet detectable signals into generated images-enabling the identification of generated content. In this work, we introduce BitMark, a robust bitwise watermarking framework for Infinity. Our method embeds a watermark directly at the bit level of the token stream across multiple scales (also referred to as resolutions) during Infinity's image generation process. Our bitwise watermark subtly influences the bits to preserve visual fidelity and generation speed while remaining robust against a spectrum of removal techniques. Furthermore, it exhibits high radioactivity, i.e., when watermarked generated images are used to train another image generative model, this second model's outputs will also carry the watermark. The radioactive traces remain detectable even when only fine-tuning diffusion or image autoregressive models on images watermarked with our BitMark. Overall, our approach provides a principled step toward preventing model collapse in image generative models by enabling reliable detection of generated outputs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faBitMark\uff0c\u4e00\u79cd\u9488\u5bf9Infinity\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u9c81\u68d2\u4f4d\u7ea7\u6c34\u5370\u6846\u67b6\uff0c\u65e8\u5728\u9632\u6b62\u6a21\u578b\u5d29\u6e83\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6a21\u578b\u8f93\u51fa\u5728\u4e92\u8054\u7f51\u4e0a\u7684\u6cdb\u6ee5\uff0c\u8fd9\u4e9b\u5185\u5bb9\u53ef\u80fd\u88ab\u91cd\u65b0\u7528\u4f5c\u8bad\u7ec3\u6570\u636e\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u9010\u6e10\u9000\u5316\uff08\u6a21\u578b\u5d29\u6e83\uff09\u3002\u6c34\u5370\u6280\u672f\u53ef\u4ee5\u8bc6\u522b\u751f\u6210\u5185\u5bb9\uff0c\u4ece\u800c\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "BitMark\u5728Infinity\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u76f4\u63a5\u5728\u4f4d\u7ea7\u522b\u5d4c\u5165\u6c34\u5370\uff0c\u8de8\u591a\u5c3a\u5ea6\u4fdd\u7559\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u751f\u6210\u901f\u5ea6\uff0c\u540c\u65f6\u62b5\u6297\u591a\u79cd\u53bb\u9664\u6280\u672f\u3002", "result": "BitMark\u5177\u6709\u9ad8\u653e\u5c04\u6027\uff0c\u5373\u4f7f\u7528\u6c34\u5370\u56fe\u50cf\u8bad\u7ec3\u7684\u65b0\u6a21\u578b\u8f93\u51fa\u4e5f\u4f1a\u643a\u5e26\u6c34\u5370\uff0c\u4e14\u6c34\u5370\u5728\u5fae\u8c03\u540e\u4ecd\u53ef\u68c0\u6d4b\u3002", "conclusion": "BitMark\u4e3a\u56fe\u50cf\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9632\u6b62\u6a21\u578b\u5d29\u6e83\u7684\u53ef\u9760\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u751f\u6210\u5185\u5bb9\u5b9e\u73b0\u3002"}}
{"id": "2506.21270", "pdf": "https://arxiv.org/pdf/2506.21270", "abs": "https://arxiv.org/abs/2506.21270", "authors": ["Cheng Zou", "Senlin Cheng", "Bolei Xu", "Dandan Zheng", "Xiaobo Li", "Jingdong Chen", "Ming Yang"], "title": "Video Virtual Try-on with Conditional Diffusion Transformer Inpainter", "categories": ["cs.CV"], "comment": "10 pages, 6 figures", "summary": "Video virtual try-on aims to naturally fit a garment to a target person in consecutive video frames. It is a challenging task, on the one hand, the output video should be in good spatial-temporal consistency, on the other hand, the details of the given garment need to be preserved well in all the frames. Naively using image-based try-on methods frame by frame can get poor results due to severe inconsistency. Recent diffusion-based video try-on methods, though very few, happen to coincide with a similar solution: inserting temporal attention into image-based try-on model to adapt it for video try-on task, which have shown improvements but there still exist inconsistency problems. In this paper, we propose ViTI (Video Try-on Inpainter), formulate and implement video virtual try-on as a conditional video inpainting task, which is different from previous methods. In this way, we start with a video generation problem instead of an image-based try-on problem, which from the beginning has a better spatial-temporal consistency. Specifically, at first we build a video inpainting framework based on Diffusion Transformer with full 3D spatial-temporal attention, and then we progressively adapt it for video garment inpainting, with a collection of masking strategies and multi-stage training. After these steps, the model can inpaint the masked garment area with appropriate garment pixels according to the prompt with good spatial-temporal consistency. Finally, as other try-on methods, garment condition is added to the model to make sure the inpainted garment appearance and details are as expected. Both quantitative and qualitative experimental results show that ViTI is superior to previous works.", "AI": {"tldr": "ViTI\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u89c6\u9891\u4fee\u590d\u7684\u89c6\u9891\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\uff0c\u901a\u8fc73D\u65f6\u7a7a\u6ce8\u610f\u529b\u6269\u6563\u53d8\u6362\u5668\u5b9e\u73b0\u66f4\u597d\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\u5b58\u5728\u65f6\u7a7a\u4e0d\u4e00\u81f4\u95ee\u9898\uff0cViTI\u65e8\u5728\u901a\u8fc7\u89c6\u9891\u751f\u6210\u800c\u975e\u9010\u5e27\u56fe\u50cf\u8bd5\u7a7f\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6269\u6563\u53d8\u6362\u5668\u6784\u5efa\u89c6\u9891\u4fee\u590d\u6846\u67b6\uff0c\u7ed3\u54083D\u65f6\u7a7a\u6ce8\u610f\u529b\u548c\u591a\u9636\u6bb5\u8bad\u7ec3\uff0c\u9010\u6b65\u9002\u5e94\u89c6\u9891\u670d\u88c5\u4fee\u590d\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cViTI\u5728\u65f6\u7a7a\u4e00\u81f4\u6027\u548c\u670d\u88c5\u7ec6\u8282\u4fdd\u7559\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ViTI\u901a\u8fc7\u89c6\u9891\u4fee\u590d\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u89c6\u9891\u865a\u62df\u8bd5\u7a7f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u679c\u3002"}}
{"id": "2506.21287", "pdf": "https://arxiv.org/pdf/2506.21287", "abs": "https://arxiv.org/abs/2506.21287", "authors": ["Diego Biagini", "Nassir Navab", "Azade Farshad"], "title": "HieraSurg: Hierarchy-Aware Diffusion Model for Surgical Video Generation", "categories": ["cs.CV"], "comment": "Accepted at MICCAI 2025", "summary": "Surgical Video Synthesis has emerged as a promising research direction following the success of diffusion models in general-domain video generation. Although existing approaches achieve high-quality video generation, most are unconditional and fail to maintain consistency with surgical actions and phases, lacking the surgical understanding and fine-grained guidance necessary for factual simulation. We address these challenges by proposing HieraSurg, a hierarchy-aware surgical video generation framework consisting of two specialized diffusion models. Given a surgical phase and an initial frame, HieraSurg first predicts future coarse-grained semantic changes through a segmentation prediction model. The final video is then generated by a second-stage model that augments these temporal segmentation maps with fine-grained visual features, leading to effective texture rendering and integration of semantic information in the video space. Our approach leverages surgical information at multiple levels of abstraction, including surgical phase, action triplets, and panoptic segmentation maps. The experimental results on Cholecystectomy Surgical Video Generation demonstrate that the model significantly outperforms prior work both quantitatively and qualitatively, showing strong generalization capabilities and the ability to generate higher frame-rate videos. The model exhibits particularly fine-grained adherence when provided with existing segmentation maps, suggesting its potential for practical surgical applications.", "AI": {"tldr": "HieraSurg\u662f\u4e00\u4e2a\u5c42\u6b21\u611f\u77e5\u7684\u624b\u672f\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u89c6\u9891\u5408\u6210\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u624b\u672f\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u624b\u672f\u52a8\u4f5c\u548c\u9636\u6bb5\u7684\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u624b\u672f\u6a21\u62df\u7684\u9700\u6c42\u3002", "method": "HieraSurg\u91c7\u7528\u4e24\u9636\u6bb5\u6269\u6563\u6a21\u578b\uff1a\u7b2c\u4e00\u9636\u6bb5\u9884\u6d4b\u7c97\u7c92\u5ea6\u8bed\u4e49\u53d8\u5316\uff0c\u7b2c\u4e8c\u9636\u6bb5\u7ed3\u5408\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7279\u5f81\u751f\u6210\u89c6\u9891\u3002", "result": "\u5728\u80c6\u56ca\u5207\u9664\u672f\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\uff0cHieraSurg\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u751f\u6210\u66f4\u9ad8\u5e27\u7387\u7684\u89c6\u9891\u3002", "conclusion": "HieraSurg\u5c55\u793a\u4e86\u5728\u591a\u5c42\u6b21\u62bd\u8c61\u4fe1\u606f\u4e0b\u7684\u5f3a\u5927\u751f\u6210\u80fd\u529b\uff0c\u5177\u6709\u5b9e\u9645\u624b\u672f\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.21348", "pdf": "https://arxiv.org/pdf/2506.21348", "abs": "https://arxiv.org/abs/2506.21348", "authors": ["Lojze Zust", "Yohann Cabon", "Juliette Marrie", "Leonid Antsfeld", "Boris Chidlovskii", "Jerome Revaud", "Gabriela Csurka"], "title": "PanSt3R: Multi-view Consistent Panoptic Segmentation", "categories": ["cs.CV"], "comment": "Accepted at ICCV 2025", "summary": "Panoptic segmentation of 3D scenes, involving the segmentation and classification of object instances in a dense 3D reconstruction of a scene, is a challenging problem, especially when relying solely on unposed 2D images. Existing approaches typically leverage off-the-shelf models to extract per-frame 2D panoptic segmentations, before optimizing an implicit geometric representation (often based on NeRF) to integrate and fuse the 2D predictions. We argue that relying on 2D panoptic segmentation for a problem inherently 3D and multi-view is likely suboptimal as it fails to leverage the full potential of spatial relationships across views. In addition to requiring camera parameters, these approaches also necessitate computationally expensive test-time optimization for each scene. Instead, in this work, we propose a unified and integrated approach PanSt3R, which eliminates the need for test-time optimization by jointly predicting 3D geometry and multi-view panoptic segmentation in a single forward pass. Our approach builds upon recent advances in 3D reconstruction, specifically upon MUSt3R, a scalable multi-view version of DUSt3R, and enhances it with semantic awareness and multi-view panoptic segmentation capabilities. We additionally revisit the standard post-processing mask merging procedure and introduce a more principled approach for multi-view segmentation. We also introduce a simple method for generating novel-view predictions based on the predictions of PanSt3R and vanilla 3DGS. Overall, the proposed PanSt3R is conceptually simple, yet fast and scalable, and achieves state-of-the-art performance on several benchmarks, while being orders of magnitude faster than existing methods.", "AI": {"tldr": "PanSt3R\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6d4b\u8bd5\u65f6\u4f18\u5316\u7684\u7edf\u4e00\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u9884\u6d4b3D\u51e0\u4f55\u548c\u591a\u89c6\u89d2\u5168\u666f\u5206\u5272\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u901f\u5ea6\u66f4\u5feb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d562D\u5168\u666f\u5206\u5272\u548c\u6d4b\u8bd5\u65f6\u4f18\u5316\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u591a\u89c6\u89d2\u7a7a\u95f4\u5173\u7cfb\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u57fa\u4e8eMUSt3R\u6539\u8fdb\uff0c\u8054\u5408\u9884\u6d4b3D\u51e0\u4f55\u548c\u591a\u89c6\u89d2\u5168\u666f\u5206\u5272\uff0c\u5e76\u4f18\u5316\u63a9\u7801\u5408\u5e76\u6d41\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u901f\u5ea6\u663e\u8457\u5feb\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PanSt3R\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u76843D\u573a\u666f\u5168\u666f\u5206\u5272\u65b9\u6cd5\u3002"}}
{"id": "2506.21356", "pdf": "https://arxiv.org/pdf/2506.21356", "abs": "https://arxiv.org/abs/2506.21356", "authors": ["Hongbo Liu", "Jingwen He", "Yi Jin", "Dian Zheng", "Yuhao Dong", "Fan Zhang", "Ziqi Huang", "Yinan He", "Yangguang Li", "Weichao Chen", "Yu Qiao", "Wanli Ouyang", "Shengjie Zhao", "Ziwei Liu"], "title": "ShotBench: Expert-Level Cinematic Understanding in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Cinematography, the fundamental visual language of film, is essential for conveying narrative, emotion, and aesthetic quality. While recent Vision-Language Models (VLMs) demonstrate strong general visual understanding, their proficiency in comprehending the nuanced cinematic grammar embedded within individual shots remains largely unexplored and lacks robust evaluation. This critical gap limits both fine-grained visual comprehension and the precision of AI-assisted video generation. To address this, we introduce \\textbf{ShotBench}, a comprehensive benchmark specifically designed for cinematic language understanding. It features over 3.5k expert-annotated QA pairs from images and video clips, meticulously curated from over 200 acclaimed (predominantly Oscar-nominated) films and spanning eight key cinematography dimensions. Our evaluation of 24 leading VLMs on ShotBench reveals their substantial limitations: even the top-performing model achieves less than 60\\% average accuracy, particularly struggling with fine-grained visual cues and complex spatial reasoning. To catalyze advancement in this domain, we construct \\textbf{ShotQA}, a large-scale multimodal dataset comprising approximately 70k cinematic QA pairs. Leveraging ShotQA, we develop \\textbf{ShotVL} through supervised fine-tuning and Group Relative Policy Optimization. ShotVL significantly outperforms all existing open-source and proprietary models on ShotBench, establishing new \\textbf{state-of-the-art} performance. We open-source our models, data, and code to foster rapid progress in this crucial area of AI-driven cinematic understanding and generation.", "AI": {"tldr": "ShotBench\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u7535\u5f71\u8bed\u8a00\u7406\u89e3\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u7535\u5f71\u8bed\u6cd5\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86ShotQA\u6570\u636e\u96c6\u548cShotVL\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u7535\u5f71\u8bed\u6cd5\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86AI\u5728\u89c6\u9891\u751f\u6210\u4e2d\u7684\u7cbe\u786e\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u5de5\u5177\u548c\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6784\u5efaShotBench\u57fa\u51c6\u6d4b\u8bd5\u548cShotQA\u6570\u636e\u96c6\uff0c\u5229\u7528\u76d1\u7763\u5fae\u8c03\u548cGroup Relative Policy Optimization\u5f00\u53d1\u4e86ShotVL\u6a21\u578b\u3002", "result": "\u73b0\u6709\u6a21\u578b\u5728ShotBench\u4e0a\u8868\u73b0\u4e0d\u4f73\uff08\u5e73\u5747\u51c6\u786e\u7387\u4f4e\u4e8e60%\uff09\uff0c\u800cShotVL\u663e\u8457\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u6a21\u578b\uff0c\u8fbe\u5230\u65b0\u6c34\u5e73\u3002", "conclusion": "ShotBench\u548cShotQA\u4e3a\u7535\u5f71\u8bed\u8a00\u7406\u89e3\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0cShotVL\u7684\u63a8\u51fa\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u6b65\uff0c\u76f8\u5173\u8d44\u6e90\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.21369", "pdf": "https://arxiv.org/pdf/2506.21369", "abs": "https://arxiv.org/abs/2506.21369", "authors": ["Duc-Hung Nguyen", "Huu-Phuc Huynh", "Minh-Triet Tran", "Trung-Nghia Le"], "title": "GenFlow: Interactive Modular System for Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Generative art unlocks boundless creative possibilities, yet its full potential remains untapped due to the technical expertise required for advanced architectural concepts and computational workflows. To bridge this gap, we present GenFlow, a novel modular framework that empowers users of all skill levels to generate images with precision and ease. Featuring a node-based editor for seamless customization and an intelligent assistant powered by natural language processing, GenFlow transforms the complexity of workflow creation into an intuitive and accessible experience. By automating deployment processes and minimizing technical barriers, our framework makes cutting-edge generative art tools available to everyone. A user study demonstrated GenFlow's ability to optimize workflows, reduce task completion times, and enhance user understanding through its intuitive interface and adaptive features. These results position GenFlow as a groundbreaking solution that redefines accessibility and efficiency in the realm of generative art.", "AI": {"tldr": "GenFlow\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u65e8\u5728\u964d\u4f4e\u751f\u6210\u827a\u672f\u7684\u5165\u95e8\u95e8\u69db\uff0c\u901a\u8fc7\u8282\u70b9\u7f16\u8f91\u5668\u548c\u667a\u80fd\u52a9\u624b\u7b80\u5316\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u751f\u6210\u827a\u672f\u9700\u8981\u9ad8\u7ea7\u6280\u672f\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002GenFlow\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f7f\u6240\u6709\u6280\u80fd\u6c34\u5e73\u7684\u7528\u6237\u90fd\u80fd\u8f7b\u677e\u4f7f\u7528\u3002", "method": "GenFlow\u91c7\u7528\u57fa\u4e8e\u8282\u70b9\u7684\u7f16\u8f91\u5668\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u667a\u80fd\u52a9\u624b\uff0c\u81ea\u52a8\u5316\u90e8\u7f72\u6d41\u7a0b\uff0c\u964d\u4f4e\u6280\u672f\u969c\u788d\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cGenFlow\u80fd\u4f18\u5316\u5de5\u4f5c\u6d41\u7a0b\uff0c\u51cf\u5c11\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\uff0c\u5e76\u901a\u8fc7\u76f4\u89c2\u754c\u9762\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "GenFlow\u901a\u8fc7\u63d0\u5347\u53ef\u8bbf\u95ee\u6027\u548c\u6548\u7387\uff0c\u91cd\u65b0\u5b9a\u4e49\u4e86\u751f\u6210\u827a\u672f\u7684\u5de5\u5177\u3002"}}
{"id": "2506.21401", "pdf": "https://arxiv.org/pdf/2506.21401", "abs": "https://arxiv.org/abs/2506.21401", "authors": ["Zhirui Gao. Renjiao Yi", "Yaqiao Dai", "Xuening Zhu", "Wei Chen", "Chenyang Zhu", "Kai Xu"], "title": "Curve-Aware Gaussian Splatting for 3D Parametric Curve Reconstruction", "categories": ["cs.CV"], "comment": "Code: https://github.com/zhirui-gao/Curve-Gaussian Accepted by ICCV   2025", "summary": "This paper presents an end-to-end framework for reconstructing 3D parametric curves directly from multi-view edge maps. Contrasting with existing two-stage methods that follow a sequential ``edge point cloud reconstruction and parametric curve fitting'' pipeline, our one-stage approach optimizes 3D parametric curves directly from 2D edge maps, eliminating error accumulation caused by the inherent optimization gap between disconnected stages. However, parametric curves inherently lack suitability for rendering-based multi-view optimization, necessitating a complementary representation that preserves their geometric properties while enabling differentiable rendering. We propose a novel bi-directional coupling mechanism between parametric curves and edge-oriented Gaussian components. This tight correspondence formulates a curve-aware Gaussian representation, \\textbf{CurveGaussian}, that enables differentiable rendering of 3D curves, allowing direct optimization guided by multi-view evidence. Furthermore, we introduce a dynamically adaptive topology optimization framework during training to refine curve structures through linearization, merging, splitting, and pruning operations. Comprehensive evaluations on the ABC dataset and real-world benchmarks demonstrate our one-stage method's superiority over two-stage alternatives, particularly in producing cleaner and more robust reconstructions. Additionally, by directly optimizing parametric curves, our method significantly reduces the parameter count during training, achieving both higher efficiency and superior performance compared to existing approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u591a\u89c6\u89d2\u8fb9\u7f18\u56fe\u91cd\u5efa3D\u53c2\u6570\u66f2\u7ebf\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u4e24\u9636\u6bb5\u65b9\u6cd5\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4e24\u9636\u6bb5\u65b9\u6cd5\u5b58\u5728\u4f18\u5316\u95f4\u9699\u5bfc\u81f4\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u4e14\u53c2\u6570\u66f2\u7ebf\u4e0d\u9002\u5408\u57fa\u4e8e\u6e32\u67d3\u7684\u591a\u89c6\u89d2\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5411\u8026\u5408\u673a\u5236\uff0c\u5c06\u53c2\u6570\u66f2\u7ebf\u4e0e\u8fb9\u7f18\u5bfc\u5411\u7684\u9ad8\u65af\u5206\u91cf\u7ed3\u5408\uff0c\u5f62\u6210\u53ef\u5fae\u6e32\u67d3\u7684\u66f2\u7ebf\u611f\u77e5\u8868\u793a\uff08CurveGaussian\uff09\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u81ea\u9002\u5e94\u62d3\u6251\u4f18\u5316\u6846\u67b6\u3002", "result": "\u5728ABC\u6570\u636e\u96c6\u548c\u771f\u5b9e\u573a\u666f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u53c2\u6570\u66f4\u5c11\u4e14\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u76f4\u63a5\u4f18\u5316\u53c2\u6570\u66f2\u7ebf\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u548c\u66f4\u9c81\u68d2\u7684\u91cd\u5efa\u3002"}}
{"id": "2506.21416", "pdf": "https://arxiv.org/pdf/2506.21416", "abs": "https://arxiv.org/abs/2506.21416", "authors": ["Bowen Chen", "Mengyi Zhao", "Haomiao Sun", "Li Chen", "Xu Wang", "Kang Du", "Xinglong Wu"], "title": "XVerse: Consistent Multi-Subject Control of Identity and Semantic Attributes via DiT Modulation", "categories": ["cs.CV"], "comment": "Project Page: https://bytedance.github.io/XVerse Github Link:   https://github.com/bytedance/XVerse", "summary": "Achieving fine-grained control over subject identity and semantic attributes (pose, style, lighting) in text-to-image generation, particularly for multiple subjects, often undermines the editability and coherence of Diffusion Transformers (DiTs). Many approaches introduce artifacts or suffer from attribute entanglement. To overcome these challenges, we propose a novel multi-subject controlled generation model XVerse. By transforming reference images into offsets for token-specific text-stream modulation, XVerse allows for precise and independent control for specific subject without disrupting image latents or features. Consequently, XVerse offers high-fidelity, editable multi-subject image synthesis with robust control over individual subject characteristics and semantic attributes. This advancement significantly improves personalized and complex scene generation capabilities.", "AI": {"tldr": "XVerse\u6a21\u578b\u901a\u8fc7\u5c06\u53c2\u8003\u56fe\u50cf\u8f6c\u6362\u4e3a\u7279\u5b9a\u6587\u672c\u6d41\u7684\u504f\u79fb\u91cf\uff0c\u5b9e\u73b0\u5bf9\u591a\u4e3b\u9898\u56fe\u50cf\u751f\u6210\u7684\u7cbe\u786e\u72ec\u7acb\u63a7\u5236\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u7f16\u8f91\u6027\u548c\u4e00\u81f4\u6027\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u591a\u4e3b\u9898\u63a7\u5236\u65f6\u7f16\u8f91\u6027\u548c\u4e00\u81f4\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u5c06\u53c2\u8003\u56fe\u50cf\u8f6c\u6362\u4e3a\u504f\u79fb\u91cf\uff0c\u7528\u4e8e\u7279\u5b9a\u6587\u672c\u6d41\u7684\u8c03\u5236\uff0c\u4ece\u800c\u72ec\u7acb\u63a7\u5236\u6bcf\u4e2a\u4e3b\u9898\u3002", "result": "XVerse\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u3001\u53ef\u7f16\u8f91\u7684\u591a\u4e3b\u9898\u56fe\u50cf\u5408\u6210\uff0c\u5e76\u80fd\u7cbe\u786e\u63a7\u5236\u5355\u4e2a\u4e3b\u9898\u7684\u7279\u5f81\u548c\u8bed\u4e49\u5c5e\u6027\u3002", "conclusion": "XVerse\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u751f\u6210\u7684\u80fd\u529b\uff0c\u4e3a\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2506.21420", "pdf": "https://arxiv.org/pdf/2506.21420", "abs": "https://arxiv.org/abs/2506.21420", "authors": ["Taoyu Wu", "Yiyi Miao", "Zhuoxiao Li", "Haocheng Zhao", "Kang Dang", "Jionglong Su", "Limin Yu", "Haoang Li"], "title": "EndoFlow-SLAM: Real-Time Endoscopic SLAM with Flow-Constrained Gaussian Splatting", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Efficient three-dimensional reconstruction and real-time visualization are critical in surgical scenarios such as endoscopy. In recent years, 3D Gaussian Splatting (3DGS) has demonstrated remarkable performance in efficient 3D reconstruction and rendering. Most 3DGS-based Simultaneous Localization and Mapping (SLAM) methods only rely on the appearance constraints for optimizing both 3DGS and camera poses. However, in endoscopic scenarios, the challenges include photometric inconsistencies caused by non-Lambertian surfaces and dynamic motion from breathing affects the performance of SLAM systems. To address these issues, we additionally introduce optical flow loss as a geometric constraint, which effectively constrains both the 3D structure of the scene and the camera motion. Furthermore, we propose a depth regularisation strategy to mitigate the problem of photometric inconsistencies and ensure the validity of 3DGS depth rendering in endoscopic scenes. In addition, to improve scene representation in the SLAM system, we improve the 3DGS refinement strategy by focusing on viewpoints corresponding to Keyframes with suboptimal rendering quality frames, achieving better rendering results. Extensive experiments on the C3VD static dataset and the StereoMIS dynamic dataset demonstrate that our method outperforms existing state-of-the-art methods in novel view synthesis and pose estimation, exhibiting high performance in both static and dynamic surgical scenes. The source code will be publicly available upon paper acceptance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5149\u6d41\u635f\u5931\u548c\u6df1\u5ea6\u6b63\u5219\u5316\u76843D\u9ad8\u65af\u6cfc\u6e85SLAM\u65b9\u6cd5\uff0c\u7528\u4e8e\u5185\u7aa5\u955c\u573a\u666f\u4e2d\u7684\u9ad8\u65483D\u91cd\u5efa\u548c\u5b9e\u65f6\u53ef\u89c6\u5316\u3002", "motivation": "\u5185\u7aa5\u955c\u573a\u666f\u4e2d\u5b58\u5728\u975e\u6717\u4f2f\u8868\u9762\u548c\u547c\u5438\u5f15\u8d77\u7684\u52a8\u6001\u8fd0\u52a8\uff0c\u5bfc\u81f4\u4f20\u7edfSLAM\u7cfb\u7edf\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u65b0\u7684\u7ea6\u675f\u548c\u4f18\u5316\u7b56\u7565\u3002", "method": "\u5f15\u5165\u5149\u6d41\u635f\u5931\u4f5c\u4e3a\u51e0\u4f55\u7ea6\u675f\uff0c\u63d0\u51fa\u6df1\u5ea6\u6b63\u5219\u5316\u7b56\u7565\uff0c\u5e76\u6539\u8fdb3DGS\u7ec6\u5316\u7b56\u7565\u4ee5\u4f18\u5316\u5173\u952e\u5e27\u7684\u6e32\u67d3\u8d28\u91cf\u3002", "result": "\u5728C3VD\u9759\u6001\u6570\u636e\u96c6\u548cStereoMIS\u52a8\u6001\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u5728\u65b0\u89c6\u89d2\u5408\u6210\u548c\u59ff\u6001\u4f30\u8ba1\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5185\u7aa5\u955c\u9759\u6001\u548c\u52a8\u6001\u573a\u666f\u4e2d\u5747\u8868\u73b0\u51fa\u9ad8\u6027\u80fd\uff0c\u4ee3\u7801\u5c06\u5728\u8bba\u6587\u63a5\u53d7\u540e\u516c\u5f00\u3002"}}
{"id": "2506.21444", "pdf": "https://arxiv.org/pdf/2506.21444", "abs": "https://arxiv.org/abs/2506.21444", "authors": ["Sweta Banerjee", "Viktoria Weiss", "Taryn A. Donovan", "Rutger A. Fick", "Thomas Conrad", "Jonas Ammeling", "Nils Porsche", "Robert Klopfleisch", "Christopher Kaltenecker", "Katharina Breininger", "Marc Aubreville", "Christof A. Bertram"], "title": "Benchmarking Deep Learning and Vision Foundation Models for Atypical vs. Normal Mitosis Classification with Cross-Dataset Evaluation", "categories": ["cs.CV"], "comment": null, "summary": "Atypical mitoses mark a deviation in the cell division process that can be an independent prognostically relevant marker for tumor malignancy. However, their identification remains challenging due to low prevalence, at times subtle morphological differences from normal mitoses, low inter-rater agreement among pathologists, and class imbalance in datasets. Building on the Atypical Mitosis dataset for Breast Cancer (AMi-Br), this study presents a comprehensive benchmark comparing deep learning approaches for automated atypical mitotic figure (AMF) classification, including baseline models, foundation models with linear probing, and foundation models fine-tuned with low-rank adaptation (LoRA). For rigorous evaluation, we further introduce two new hold-out AMF datasets - AtNorM-Br, a dataset of mitoses from the The TCGA breast cancer cohort, and AtNorM-MD, a multi-domain dataset of mitoses from the MIDOG++ training set. We found average balanced accuracy values of up to 0.8135, 0.7696, and 0.7705 on the in-domain AMi-Br and the out-of-domain AtNorm-Br and AtNorM-MD datasets, respectively, with the results being particularly good for LoRA-based adaptation of the Virchow-line of foundation models. Our work shows that atypical mitosis classification, while being a challenging problem, can be effectively addressed through the use of recent advances in transfer learning and model fine-tuning techniques. We make available all code and data used in this paper in this github repository: https://github.com/DeepMicroscopy/AMi-Br_Benchmark.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5305\u62ec\u57fa\u7ebf\u6a21\u578b\u3001\u57fa\u7840\u6a21\u578b\u7ebf\u6027\u63a2\u6d4b\u548cLoRA\u5fae\u8c03\uff09\u5bf9\u4e73\u817a\u764c\u4e2d\u7684\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\uff08AMF\uff09\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u662f\u80bf\u7624\u6076\u6027\u7a0b\u5ea6\u7684\u72ec\u7acb\u9884\u540e\u6807\u5fd7\u7269\uff0c\u4f46\u5176\u8bc6\u522b\u5b58\u5728\u6311\u6218\uff0c\u5982\u4f4e\u53d1\u751f\u7387\u3001\u5f62\u6001\u5dee\u5f02\u5c0f\u3001\u75c5\u7406\u5b66\u5bb6\u95f4\u4e00\u81f4\u6027\u4f4e\u4ee5\u53ca\u6570\u636e\u96c6\u7c7b\u522b\u4e0d\u5e73\u8861\u3002", "method": "\u7814\u7a76\u4f7f\u7528AMi-Br\u6570\u636e\u96c6\uff0c\u5e76\u5f15\u5165\u4e24\u4e2a\u65b0\u6570\u636e\u96c6AtNorM-Br\u548cAtNorM-MD\uff0c\u6bd4\u8f83\u4e86\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5305\u62ecLoRA\u5fae\u8c03\u7684\u57fa\u7840\u6a21\u578b\u3002", "result": "\u5728AMi-Br\u3001AtNorM-Br\u548cAtNorM-MD\u6570\u636e\u96c6\u4e0a\uff0c\u5e73\u5747\u5e73\u8861\u51c6\u786e\u7387\u5206\u522b\u8fbe\u52300.8135\u30010.7696\u548c0.7705\uff0cLoRA\u5fae\u8c03\u7684Virchow\u57fa\u7840\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5c3d\u7ba1\u975e\u5178\u578b\u6709\u4e1d\u5206\u88c2\u5206\u7c7b\u5177\u6709\u6311\u6218\u6027\uff0c\u4f46\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u548c\u6a21\u578b\u5fae\u8c03\u6280\u672f\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u3002\u6240\u6709\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.21446", "pdf": "https://arxiv.org/pdf/2506.21446", "abs": "https://arxiv.org/abs/2506.21446", "authors": ["Mohamed Omran", "Dimitris Kalatzis", "Jens Petersen", "Amirhossein Habibian", "Auke Wiggers"], "title": "Controllable 3D Placement of Objects with Scene-Aware Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Image editing approaches have become more powerful and flexible with the advent of powerful text-conditioned generative models. However, placing objects in an environment with a precise location and orientation still remains a challenge, as this typically requires carefully crafted inpainting masks or prompts. In this work, we show that a carefully designed visual map, combined with coarse object masks, is sufficient for high quality object placement. We design a conditioning signal that resolves ambiguities, while being flexible enough to allow for changing of shapes or object orientations. By building on an inpainting model, we leave the background intact by design, in contrast to methods that model objects and background jointly. We demonstrate the effectiveness of our method in the automotive setting, where we compare different conditioning signals in novel object placement tasks. These tasks are designed to measure edit quality not only in terms of appearance, but also in terms of pose and location accuracy, including cases that require non-trivial shape changes. Lastly, we show that fine location control can be combined with appearance control to place existing objects in precise locations in a scene.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u5730\u56fe\u548c\u7c97\u7565\u5bf9\u8c61\u63a9\u7801\u7684\u9ad8\u8d28\u91cf\u5bf9\u8c61\u653e\u7f6e\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u7cbe\u786e\u4f4d\u7f6e\u548c\u65b9\u5411\u63a7\u5236\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5bf9\u8c61\u653e\u7f6e\u65f6\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63a9\u7801\u6216\u63d0\u793a\uff0c\u96be\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u4f4d\u7f6e\u548c\u65b9\u5411\u63a7\u5236\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6761\u4ef6\u4fe1\u53f7\uff0c\u7ed3\u5408\u89c6\u89c9\u5730\u56fe\u548c\u7c97\u7565\u63a9\u7801\uff0c\u652f\u6301\u5f62\u72b6\u548c\u65b9\u5411\u53d8\u5316\uff0c\u5e76\u57fa\u4e8e\u4fee\u590d\u6a21\u578b\u4fdd\u6301\u80cc\u666f\u4e0d\u53d8\u3002", "result": "\u5728\u6c7d\u8f66\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u5bf9\u8c61\u653e\u7f6e\uff0c\u5305\u62ec\u975e\u5e73\u51e1\u5f62\u72b6\u53d8\u5316\u548c\u7cbe\u786e\u4f4d\u7f6e\u63a7\u5236\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5bf9\u8c61\u653e\u7f6e\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7ed3\u5408\u4e86\u4f4d\u7f6e\u548c\u5916\u89c2\u63a7\u5236\uff0c\u4e3a\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21452", "pdf": "https://arxiv.org/pdf/2506.21452", "abs": "https://arxiv.org/abs/2506.21452", "authors": ["Kaiyu Song", "Hanjiang Lai"], "title": "Rethinking Oversaturation in Classifier-Free Guidance via Low Frequency", "categories": ["cs.CV"], "comment": null, "summary": "Classifier-free guidance (CFG) succeeds in condition diffusion models that use a guidance scale to balance the influence of conditional and unconditional terms. A high guidance scale is used to enhance the performance of the conditional term. However, the high guidance scale often results in oversaturation and unrealistic artifacts. In this paper, we introduce a new perspective based on low-frequency signals, identifying the accumulation of redundant information in these signals as the key factor behind oversaturation and unrealistic artifacts. Building on this insight, we propose low-frequency improved classifier-free guidance (LF-CFG) to mitigate these issues. Specifically, we introduce an adaptive threshold-based measurement to pinpoint the locations of redundant information. We determine a reasonable threshold by analyzing the change rate of low-frequency information between prior and current steps. We then apply a down-weight strategy to reduce the impact of redundant information in the low-frequency signals. Experimental results demonstrate that LF-CFG effectively alleviates oversaturation and unrealistic artifacts across various diffusion models, including Stable Diffusion-XL, Stable Diffusion 2.1, 3.0, 3.5, and SiT-XL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f4e\u9891\u4fe1\u53f7\u7684\u6539\u8fdb\u65b9\u6cd5\uff08LF-CFG\uff09\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9608\u503c\u68c0\u6d4b\u5197\u4f59\u4fe1\u606f\u5e76\u964d\u4f4e\u5176\u5f71\u54cd\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\uff08CFG\uff09\u4e2d\u7684\u8fc7\u9971\u548c\u548c\u4e0d\u771f\u5b9e\u4f2a\u5f71\u95ee\u9898\u3002", "motivation": "\u9ad8\u5f15\u5bfc\u5c3a\u5ea6\u5728\u589e\u5f3a\u6761\u4ef6\u6269\u6563\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5e38\u5bfc\u81f4\u8fc7\u9971\u548c\u548c\u4e0d\u771f\u5b9e\u4f2a\u5f71\u3002\u7814\u7a76\u53d1\u73b0\u4f4e\u9891\u4fe1\u53f7\u4e2d\u5197\u4f59\u4fe1\u606f\u7684\u79ef\u7d2f\u662f\u4e3b\u8981\u539f\u56e0\u3002", "method": "\u63d0\u51faLF-CFG\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9608\u503c\u5b9a\u4f4d\u5197\u4f59\u4fe1\u606f\uff0c\u5e76\u6839\u636e\u4f4e\u9891\u4fe1\u606f\u7684\u53d8\u5316\u7387\u786e\u5b9a\u5408\u7406\u9608\u503c\uff0c\u91c7\u7528\u964d\u6743\u7b56\u7565\u51cf\u5c11\u5197\u4f59\u4fe1\u606f\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLF-CFG\u5728\u591a\u79cd\u6269\u6563\u6a21\u578b\uff08\u5982Stable Diffusion-XL\u7b49\uff09\u4e2d\u6709\u6548\u7f13\u89e3\u4e86\u8fc7\u9971\u548c\u548c\u4e0d\u771f\u5b9e\u4f2a\u5f71\u3002", "conclusion": "LF-CFG\u4e3a\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u63d0\u4f9b\u4e86\u4e00\u79cd\u6539\u8fdb\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u3002"}}
{"id": "2506.21520", "pdf": "https://arxiv.org/pdf/2506.21520", "abs": "https://arxiv.org/abs/2506.21520", "authors": ["Polina Karpikova", "Daniil Selikhanovych", "Kirill Struminsky", "Ruslan Musaev", "Maria Golitsyna", "Dmitry Baranchuk"], "title": "MADrive: Memory-Augmented Driving Scene Modeling", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in scene reconstruction have pushed toward highly realistic modeling of autonomous driving (AD) environments using 3D Gaussian splatting. However, the resulting reconstructions remain closely tied to the original observations and struggle to support photorealistic synthesis of significantly altered or novel driving scenarios. This work introduces MADrive, a memory-augmented reconstruction framework designed to extend the capabilities of existing scene reconstruction methods by replacing observed vehicles with visually similar 3D assets retrieved from a large-scale external memory bank. Specifically, we release MAD-Cars, a curated dataset of ${\\sim}70$K 360{\\deg} car videos captured in the wild and present a retrieval module that finds the most similar car instances in the memory bank, reconstructs the corresponding 3D assets from video, and integrates them into the target scene through orientation alignment and relighting. The resulting replacements provide complete multi-view representations of vehicles in the scene, enabling photorealistic synthesis of substantially altered configurations, as demonstrated in our experiments. Project page: https://yandex-research.github.io/madrive/", "AI": {"tldr": "MADrive\u6846\u67b6\u901a\u8fc7\u5916\u90e8\u8bb0\u5fc6\u5e93\u66ff\u6362\u573a\u666f\u4e2d\u7684\u8f66\u8f86\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u73af\u5883\u91cd\u5efa\u7684\u771f\u5b9e\u611f\u548c\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u67093D\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u65b9\u6cd5\u96be\u4ee5\u652f\u6301\u663e\u8457\u6539\u53d8\u6216\u65b0\u9896\u9a7e\u9a76\u573a\u666f\u7684\u903c\u771f\u5408\u6210\u3002", "method": "\u5f15\u5165MADrive\u6846\u67b6\uff0c\u5229\u7528MAD-Cars\u6570\u636e\u96c6\u68c0\u7d22\u76f8\u4f3c\u8f66\u8f86\u8d44\u4ea7\uff0c\u901a\u8fc7\u65b9\u5411\u5bf9\u9f50\u548c\u91cd\u65b0\u5149\u7167\u96c6\u6210\u5230\u76ee\u6807\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u66ff\u6362\u540e\u7684\u8f66\u8f86\u63d0\u4f9b\u5b8c\u6574\u591a\u89c6\u89d2\u8868\u793a\uff0c\u652f\u6301\u663e\u8457\u6539\u53d8\u914d\u7f6e\u7684\u903c\u771f\u5408\u6210\u3002", "conclusion": "MADrive\u6269\u5c55\u4e86\u73b0\u6709\u573a\u666f\u91cd\u5efa\u80fd\u529b\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u73af\u5883\u63d0\u4f9b\u66f4\u7075\u6d3b\u548c\u903c\u771f\u7684\u5408\u6210\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21544", "pdf": "https://arxiv.org/pdf/2506.21544", "abs": "https://arxiv.org/abs/2506.21544", "authors": ["Yansong Qu", "Shaohui Dai", "Xinyang Li", "Yuze Wang", "You Shen", "Liujuan Cao", "Rongrong Ji"], "title": "DeOcc-1-to-3: 3D De-Occlusion from a Single Image via Self-Supervised Multi-View Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing 3D objects from a single image is a long-standing challenge, especially under real-world occlusions. While recent diffusion-based view synthesis models can generate consistent novel views from a single RGB image, they generally assume fully visible inputs and fail when parts of the object are occluded. This leads to inconsistent views and degraded 3D reconstruction quality. To overcome this limitation, we propose an end-to-end framework for occlusion-aware multi-view generation. Our method directly synthesizes six structurally consistent novel views from a single partially occluded image, enabling downstream 3D reconstruction without requiring prior inpainting or manual annotations. We construct a self-supervised training pipeline using the Pix2Gestalt dataset, leveraging occluded-unoccluded image pairs and pseudo-ground-truth views to teach the model structure-aware completion and view consistency. Without modifying the original architecture, we fully fine-tune the view synthesis model to jointly learn completion and multi-view generation. Additionally, we introduce the first benchmark for occlusion-aware reconstruction, encompassing diverse occlusion levels, object categories, and mask patterns. This benchmark provides a standardized protocol for evaluating future methods under partial occlusions. Our code is available at https://github.com/Quyans/DeOcc123.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u906e\u6321\u611f\u77e5\u591a\u89c6\u89d2\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5355\u5f20\u90e8\u5206\u906e\u6321\u56fe\u50cf\u751f\u6210\u516d\u79cd\u7ed3\u6784\u4e00\u81f4\u7684\u65b0\u89c6\u89d2\uff0c\u65e0\u9700\u5148\u9a8c\u4fee\u590d\u6216\u624b\u52a8\u6807\u6ce8\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u89d2\u5408\u6210\u6a21\u578b\u5047\u8bbe\u8f93\u5165\u5b8c\u5168\u53ef\u89c1\uff0c\u65e0\u6cd5\u5904\u7406\u906e\u6321\u60c5\u51b5\uff0c\u5bfc\u81f4\u89c6\u89d2\u4e0d\u4e00\u81f4\u548c3D\u91cd\u5efa\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u6784\u5efa\u81ea\u76d1\u7763\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5229\u7528\u906e\u6321-\u672a\u906e\u6321\u56fe\u50cf\u5bf9\u548c\u4f2a\u771f\u5b9e\u89c6\u89d2\uff0c\u8054\u5408\u5b66\u4e60\u4fee\u590d\u548c\u591a\u89c6\u89d2\u751f\u6210\u3002", "result": "\u63d0\u51fa\u4e86\u9996\u4e2a\u906e\u6321\u611f\u77e5\u91cd\u5efa\u57fa\u51c6\uff0c\u6db5\u76d6\u591a\u79cd\u906e\u6321\u7a0b\u5ea6\u3001\u7269\u4f53\u7c7b\u522b\u548c\u63a9\u7801\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u906e\u6321\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u7ed3\u6784\u4e00\u81f4\u7684\u591a\u89c6\u89d2\u751f\u6210\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\u3002"}}
{"id": "2506.21547", "pdf": "https://arxiv.org/pdf/2506.21547", "abs": "https://arxiv.org/abs/2506.21547", "authors": ["Jianyun Xu", "Song Wang", "Ziqian Ni", "Chunyong Hu", "Sheng Yang", "Jianke Zhu", "Qiang Li"], "title": "SAM4D: Segment Anything in Camera and LiDAR Streams", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted by ICCV2025, Project Page: https://SAM4D-Project.github.io", "summary": "We present SAM4D, a multi-modal and temporal foundation model designed for promptable segmentation across camera and LiDAR streams. Unified Multi-modal Positional Encoding (UMPE) is introduced to align camera and LiDAR features in a shared 3D space, enabling seamless cross-modal prompting and interaction. Additionally, we propose Motion-aware Cross-modal Memory Attention (MCMA), which leverages ego-motion compensation to enhance temporal consistency and long-horizon feature retrieval, ensuring robust segmentation across dynamically changing autonomous driving scenes. To avoid annotation bottlenecks, we develop a multi-modal automated data engine that synergizes VFM-driven video masklets, spatiotemporal 4D reconstruction, and cross-modal masklet fusion. This framework generates camera-LiDAR aligned pseudo-labels at a speed orders of magnitude faster than human annotation while preserving VFM-derived semantic fidelity in point cloud representations. We conduct extensive experiments on the constructed Waymo-4DSeg, which demonstrate the powerful cross-modal segmentation ability and great potential in data annotation of proposed SAM4D.", "AI": {"tldr": "SAM4D\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u65f6\u5e8f\u57fa\u7840\u6a21\u578b\uff0c\u652f\u6301\u76f8\u673a\u548cLiDAR\u6d41\u7684\u53ef\u63d0\u793a\u5206\u5272\uff0c\u901a\u8fc7UMPE\u548cMCMA\u6280\u672f\u5b9e\u73b0\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\uff0c\u5e76\u5229\u7528\u81ea\u52a8\u5316\u6570\u636e\u5f15\u64ce\u9ad8\u6548\u751f\u6210\u4f2a\u6807\u7b7e\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u76f8\u673a\u548cLiDAR\u6570\u636e\u8de8\u6a21\u6001\u5206\u5272\u7684\u6311\u6218\uff0c\u540c\u65f6\u907f\u514d\u4eba\u5de5\u6807\u6ce8\u7684\u74f6\u9888\u3002", "method": "\u63d0\u51faUMPE\u5bf9\u9f50\u591a\u6a21\u6001\u7279\u5f81\uff0cMCMA\u589e\u5f3a\u65f6\u5e8f\u4e00\u81f4\u6027\uff0c\u5e76\u5f00\u53d1\u591a\u6a21\u6001\u81ea\u52a8\u5316\u6570\u636e\u5f15\u64ce\u751f\u6210\u4f2a\u6807\u7b7e\u3002", "result": "\u5728Waymo-4DSeg\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86SAM4D\u7684\u5f3a\u5927\u8de8\u6a21\u6001\u5206\u5272\u80fd\u529b\u548c\u9ad8\u6548\u6570\u636e\u6807\u6ce8\u6f5c\u529b\u3002", "conclusion": "SAM4D\u4e3a\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u8de8\u6a21\u6001\u5206\u5272\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21552", "pdf": "https://arxiv.org/pdf/2506.21552", "abs": "https://arxiv.org/abs/2506.21552", "authors": ["Yutong Bai", "Danny Tran", "Amir Bar", "Yann LeCun", "Trevor Darrell", "Jitendra Malik"], "title": "Whole-Body Conditioned Egocentric Video Prediction", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.RO"], "comment": "Project Page: https://dannytran123.github.io/PEVA", "summary": "We train models to Predict Ego-centric Video from human Actions (PEVA), given the past video and an action represented by the relative 3D body pose. By conditioning on kinematic pose trajectories, structured by the joint hierarchy of the body, our model learns to simulate how physical human actions shape the environment from a first-person point of view. We train an auto-regressive conditional diffusion transformer on Nymeria, a large-scale dataset of real-world egocentric video and body pose capture. We further design a hierarchical evaluation protocol with increasingly challenging tasks, enabling a comprehensive analysis of the model's embodied prediction and control abilities. Our work represents an initial attempt to tackle the challenges of modeling complex real-world environments and embodied agent behaviors with video prediction from the perspective of a human.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4eba\u4f53\u52a8\u4f5c\u9884\u6d4b\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c6\u9891\u7684\u65b9\u6cd5\uff08PEVA\uff09\uff0c\u5229\u7528\u8fc7\u53bb\u89c6\u9891\u548c\u76f8\u5bf93D\u8eab\u4f53\u59ff\u6001\u4f5c\u4e3a\u8f93\u5165\uff0c\u8bad\u7ec3\u81ea\u56de\u5f52\u6761\u4ef6\u6269\u6563\u53d8\u6362\u5668\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u4eba\u4f53\u52a8\u4f5c\u6a21\u62df\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u4e0b\u73af\u5883\u7684\u53d8\u5316\uff0c\u4ee5\u89e3\u51b3\u590d\u6742\u73b0\u5b9e\u73af\u5883\u548c\u5177\u8eab\u884c\u4e3a\u7684\u5efa\u6a21\u6311\u6218\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u57fa\u4e8eNymeria\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8bad\u7ec3\u81ea\u56de\u5f52\u6761\u4ef6\u6269\u6563\u53d8\u6362\u5668\uff0c\u5e76\u7ed3\u5408\u4eba\u4f53\u5173\u8282\u5c42\u6b21\u7ed3\u6784\u5bf9\u8fd0\u52a8\u8f68\u8ff9\u8fdb\u884c\u5efa\u6a21\u3002", "result": "\u8bbe\u8ba1\u4e86\u5206\u5c42\u8bc4\u4f30\u534f\u8bae\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u5728\u5177\u8eab\u9884\u6d4b\u548c\u63a7\u5236\u80fd\u529b\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\u8fd9\u662f\u4ece\u4eba\u7c7b\u89c6\u89d2\u5efa\u6a21\u590d\u6742\u73af\u5883\u548c\u5177\u8eab\u884c\u4e3a\u7684\u521d\u6b65\u5c1d\u8bd5\u3002"}}
{"id": "2506.20969", "pdf": "https://arxiv.org/pdf/2506.20969", "abs": "https://arxiv.org/abs/2506.20969", "authors": ["Shruti Bansal", "Wenshan Wang", "Yifei Liu", "Parv Maheshwari"], "title": "ThermalDiffusion: Visual-to-Thermal Image-to-Image Translation for Autonomous Navigation", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted at Thermal Infrared in Robotics (TIRO) Workshop, ICRA 2025", "summary": "Autonomous systems rely on sensors to estimate the environment around them. However, cameras, LiDARs, and RADARs have their own limitations. In nighttime or degraded environments such as fog, mist, or dust, thermal cameras can provide valuable information regarding the presence of objects of interest due to their heat signature. They make it easy to identify humans and vehicles that are usually at higher temperatures compared to their surroundings. In this paper, we focus on the adaptation of thermal cameras for robotics and automation, where the biggest hurdle is the lack of data. Several multi-modal datasets are available for driving robotics research in tasks such as scene segmentation, object detection, and depth estimation, which are the cornerstone of autonomous systems. However, they are found to be lacking in thermal imagery. Our paper proposes a solution to augment these datasets with synthetic thermal data to enable widespread and rapid adaptation of thermal cameras. We explore the use of conditional diffusion models to convert existing RGB images to thermal images using self-attention to learn the thermal properties of real-world objects.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5229\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5c06RGB\u56fe\u50cf\u8f6c\u6362\u4e3a\u70ed\u6210\u50cf\u56fe\u50cf\uff0c\u4ee5\u89e3\u51b3\u70ed\u6210\u50cf\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4fc3\u8fdb\u70ed\u6210\u50cf\u76f8\u673a\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5e94\u7528\u3002", "motivation": "\u70ed\u6210\u50cf\u76f8\u673a\u5728\u591c\u95f4\u6216\u6076\u52a3\u73af\u5883\u4e2d\u80fd\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u4fe1\u606f\uff0c\u4f46\u7f3a\u4e4f\u8db3\u591f\u7684\u6570\u636e\u9650\u5236\u4e86\u5176\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5c06\u73b0\u6709RGB\u56fe\u50cf\u8f6c\u6362\u4e3a\u70ed\u6210\u50cf\u56fe\u50cf\u3002", "result": "\u901a\u8fc7\u5408\u6210\u70ed\u6210\u50cf\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u70ed\u6210\u50cf\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u63a8\u52a8\u70ed\u6210\u50cf\u76f8\u673a\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2506.21499", "pdf": "https://arxiv.org/pdf/2506.21499", "abs": "https://arxiv.org/abs/2506.21499", "authors": ["Hojat Asgariandehkordi", "Mostafa Sharifzadeh", "Hassan Rivaz"], "title": "Lightweight Physics-Informed Zero-Shot Ultrasound Plane Wave Denoising", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Ultrasound Coherent Plane Wave Compounding (CPWC) enhances image contrast by combining echoes from multiple steered transmissions. While increasing the number of angles generally improves image quality, it drastically reduces the frame rate and can introduce blurring artifacts in fast-moving targets. Moreover, compounded images remain susceptible to noise, particularly when acquired with a limited number of transmissions. We propose a zero-shot denoising framework tailored for low-angle CPWC acquisitions, which enhances contrast without relying on a separate training dataset. The method divides the available transmission angles into two disjoint subsets, each used to form compound images that include higher noise levels. The new compounded images are then used to train a deep model via a self-supervised residual learning scheme, enabling it to suppress incoherent noise while preserving anatomical structures. Because angle-dependent artifacts vary between the subsets while the underlying tissue response is similar, this physics-informed pairing allows the network to learn to disentangle the inconsistent artifacts from the consistent tissue signal. Unlike supervised methods, our model requires no domain-specific fine-tuning or paired data, making it adaptable across anatomical regions and acquisition setups. The entire pipeline supports efficient training with low computational cost due to the use of a lightweight architecture, which comprises only two convolutional layers. Evaluations on simulation, phantom, and in vivo data demonstrate superior contrast enhancement and structure preservation compared to both classical and deep learning-based denoising methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u96f6\u6837\u672c\u53bb\u566a\u6846\u67b6\uff0c\u7528\u4e8e\u4f4e\u89d2\u5ea6CPWC\u6210\u50cf\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u589e\u5f3a\u5bf9\u6bd4\u5ea6\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u89e3\u51b3CPWC\u6210\u50cf\u4e2d\u56e0\u89d2\u5ea6\u589e\u52a0\u5bfc\u81f4\u7684\u5e27\u7387\u4e0b\u964d\u548c\u6a21\u7cca\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u4f4e\u89d2\u5ea6\u91c7\u96c6\u65f6\u7684\u53bb\u566a\u80fd\u529b\u3002", "method": "\u5c06\u4f20\u8f93\u89d2\u5ea6\u5206\u4e3a\u4e24\u4e2a\u5b50\u96c6\uff0c\u5206\u522b\u751f\u6210\u542b\u566a\u58f0\u7684\u590d\u5408\u56fe\u50cf\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u6b8b\u5dee\u5b66\u4e60\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u5206\u79bb\u566a\u58f0\u4e0e\u7ec4\u7ec7\u4fe1\u53f7\u3002", "result": "\u5728\u4eff\u771f\u3001\u4f53\u6a21\u548c\u6d3b\u4f53\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u548c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u53bb\u566a\u65b9\u6cd5\uff0c\u63d0\u5347\u5bf9\u6bd4\u5ea6\u5e76\u4fdd\u7559\u7ed3\u6784\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u6216\u914d\u5bf9\u6570\u636e\uff0c\u9002\u5e94\u6027\u5f3a\uff0c\u8ba1\u7b97\u6210\u672c\u4f4e\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u89e3\u5256\u533a\u57df\u548c\u91c7\u96c6\u8bbe\u7f6e\u3002"}}
