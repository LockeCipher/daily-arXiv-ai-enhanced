{"id": "2507.21493", "pdf": "https://arxiv.org/pdf/2507.21493", "abs": "https://arxiv.org/abs/2507.21493", "authors": ["Longwen Zhang", "Qixuan Zhang", "Haoran Jiang", "Yinuo Bai", "Wei Yang", "Lan Xu", "Jingyi Yu"], "title": "BANG: Dividing 3D Assets via Generative Exploded Dynamics", "categories": ["cs.GR"], "comment": "Homepage: https://sites.google.com/view/bang7355608", "summary": "3D creation has always been a unique human strength, driven by our ability to deconstruct and reassemble objects using our eyes, mind and hand. However, current 3D design tools struggle to replicate this natural process, requiring considerable artistic expertise and manual labor. This paper introduces BANG, a novel generative approach that bridges 3D generation and reasoning, allowing for intuitive and flexible part-level decomposition of 3D objects. At the heart of BANG is \"Generative Exploded Dynamics\", which creates a smooth sequence of exploded states for an input geometry, progressively separating parts while preserving their geometric and semantic coherence.   BANG utilizes a pre-trained large-scale latent diffusion model, fine-tuned for exploded dynamics with a lightweight exploded view adapter, allowing precise control over the decomposition process. It also incorporates a temporal attention module to ensure smooth transitions and consistency across time. BANG enhances control with spatial prompts, such as bounding boxes and surface regions, enabling users to specify which parts to decompose and how. This interaction can be extended with multimodal models like GPT-4, enabling 2D-to-3D manipulations for more intuitive and creative workflows.   The capabilities of BANG extend to generating detailed part-level geometry, associating parts with functional descriptions, and facilitating component-aware 3D creation and manufacturing workflows. Additionally, BANG offers applications in 3D printing, where separable parts are generated for easy printing and reassembly. In essence, BANG enables seamless transformation from imaginative concepts to detailed 3D assets, offering a new perspective on creation that resonates with human intuition."}
{"id": "2507.21200", "pdf": "https://arxiv.org/pdf/2507.21200", "abs": "https://arxiv.org/abs/2507.21200", "authors": ["Soren Pedersen", "Sanyam Jain", "Mikkel Chavez", "Viktor Ladehoff", "Bruna Neves de Freitas", "Ruben Pauwels"], "title": "PanoGAN A Deep Generative Model for Panoramic Dental Radiographs", "categories": ["cs.CV", "cs.ET", "cs.LG", "eess.IV"], "comment": null, "summary": "This paper presents the development of a generative adversarial network (GAN) for synthesizing dental panoramic radiographs. Although exploratory in nature, the study aims to address the scarcity of data in dental research and education. We trained a deep convolutional GAN (DCGAN) using a Wasserstein loss with gradient penalty (WGANGP) on a dataset of 2322 radiographs of varying quality. The focus was on the dentoalveolar regions, other anatomical structures were cropped out. Extensive preprocessing and data cleaning were performed to standardize the inputs while preserving anatomical variability. We explored four candidate models by varying critic iterations, feature depth, and the use of denoising prior to training. A clinical expert evaluated the generated radiographs based on anatomical visibility and realism, using a 5-point scale (1 very poor 5 excellent). Most images showed moderate anatomical depiction, although some were degraded by artifacts. A trade-off was observed the model trained on non-denoised data yielded finer details especially in structures like the mandibular canal and trabecular bone, while a model trained on denoised data offered superior overall image clarity and sharpness. These findings provide a foundation for future work on GAN-based methods in dental imaging."}
{"id": "2507.21261", "pdf": "https://arxiv.org/pdf/2507.21261", "abs": "https://arxiv.org/abs/2507.21261", "authors": ["Jack Hilliard", "Adrian Hilton", "Jean-Yves Guillemaut"], "title": "HDR Environment Map Estimation with Latent Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "We advance the field of HDR environment map estimation from a single-view image by establishing a novel approach leveraging the Latent Diffusion Model (LDM) to produce high-quality environment maps that can plausibly light mirror-reflective surfaces. A common issue when using the ERP representation, the format used by the vast majority of approaches, is distortions at the poles and a seam at the sides of the environment map. We remove the border seam artefact by proposing an ERP convolutional padding in the latent autoencoder. Additionally, we investigate whether adapting the diffusion network architecture to the ERP format can improve the quality and accuracy of the estimated environment map by proposing a panoramically-adapted Diffusion Transformer architecture. Our proposed PanoDiT network reduces ERP distortions and artefacts, but at the cost of image quality and plausibility. We evaluate with standard benchmarks to demonstrate that our models estimate high-quality environment maps that perform competitively with state-of-the-art approaches in both image quality and lighting accuracy."}
{"id": "2507.21353", "pdf": "https://arxiv.org/pdf/2507.21353", "abs": "https://arxiv.org/abs/2507.21353", "authors": ["Deep Anil Patel", "Iain Melvin", "Zachary Izzo", "Martin Renqiang Min"], "title": "Group Relative Augmentation for Data Efficient Action Detection", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Adapting large Video-Language Models (VLMs) for action detection using only a few examples poses challenges like overfitting and the granularity mismatch between scene-level pre-training and required person-centric understanding. We propose an efficient adaptation strategy combining parameter-efficient tuning (LoRA) with a novel learnable internal feature augmentation. Applied within the frozen VLM backbone using FiLM, these augmentations generate diverse feature variations directly relevant to the task. Additionally, we introduce a group-weighted loss function that dynamically modulates the training contribution of each augmented sample based on its prediction divergence relative to the group average. This promotes robust learning by prioritizing informative yet reasonable augmentations. We demonstrate our method's effectiveness on complex multi-label, multi-person action detection datasets (AVA, MOMA), achieving strong mAP performance and showcasing significant data efficiency for adapting VLMs from limited examples."}
{"id": "2507.21371", "pdf": "https://arxiv.org/pdf/2507.21371", "abs": "https://arxiv.org/abs/2507.21371", "authors": ["Zitong Zhang", "Suranjan Gautam", "Rui Yu"], "title": "Top2Pano: Learning to Generate Indoor Panoramas from Top-Down View", "categories": ["cs.CV"], "comment": "ICCV 2025. Project page: https://top2pano.github.io/", "summary": "Generating immersive 360{\\deg} indoor panoramas from 2D top-down views has applications in virtual reality, interior design, real estate, and robotics. This task is challenging due to the lack of explicit 3D structure and the need for geometric consistency and photorealism. We propose Top2Pano, an end-to-end model for synthesizing realistic indoor panoramas from top-down views. Our method estimates volumetric occupancy to infer 3D structures, then uses volumetric rendering to generate coarse color and depth panoramas. These guide a diffusion-based refinement stage using ControlNet, enhancing realism and structural fidelity. Evaluations on two datasets show Top2Pano outperforms baselines, effectively reconstructing geometry, occlusions, and spatial arrangements. It also generalizes well, producing high-quality panoramas from schematic floorplans. Our results highlight Top2Pano's potential in bridging top-down views with immersive indoor synthesis."}
{"id": "2507.21391", "pdf": "https://arxiv.org/pdf/2507.21391", "abs": "https://arxiv.org/abs/2507.21391", "authors": ["Shijie Zhou", "Ruiyi Zhang", "Huaisheng Zhu", "Branislav Kveton", "Yufan Zhou", "Jiuxiang Gu", "Jian Chen", "Changyou Chen"], "title": "Multimodal LLMs as Customized Reward Models for Text-to-Image Generation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted at ICCV 2025. Code available at   https://github.com/sjz5202/LLaVA-Reward", "summary": "We introduce LLaVA-Reward, an efficient reward model designed to automatically evaluate text-to-image (T2I) generations across multiple perspectives, leveraging pretrained multimodal large language models (MLLMs). Existing MLLM-based approaches require instruction-following data for supervised fine-tuning and evaluate generation quality on analyzing text response, which is time-consuming and difficult to train. To address this problem, we propose LLaVA-Reward, which directly utilizes the hidden states of MLLMs given text-image pairs. To enhance the bidirectional interaction between visual and textual representations in decoder-only MLLMs, we further propose adding a Skip-connection Cross Attention (SkipCA) module. This design enhances text-image correlation reasoning by connecting early-layer visual features with later-layer hidden representations.In addition, LLaVA-Reward supports different types of preference data for efficient fine-tuning, including paired preference data and unpaired data. We train LLaVA-Reward on four evaluation perspectives: text-image alignment, fidelity/artifact, safety, and overall ranking. Empirical results demonstrate that LLaVA-Reward outperforms conventional and MLLM-based methods in generating human-aligned scores for automatic evaluations and inference-time scaling in text-to-image generations."}
{"id": "2507.21489", "pdf": "https://arxiv.org/pdf/2507.21489", "abs": "https://arxiv.org/abs/2507.21489", "authors": ["Zhichuan Wang", "Yang Zhou", "Zhe Liu", "Rui Yu", "Song Bai", "Yulong Wang", "Xinwei He", "Xiang Bai"], "title": "Describe, Adapt and Combine: Empowering CLIP Encoders for Open-set 3D Object Retrieval", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "Open-set 3D object retrieval (3DOR) is an emerging task aiming to retrieve 3D objects of unseen categories beyond the training set. Existing methods typically utilize all modalities (i.e., voxels, point clouds, multi-view images) and train specific backbones before fusion. However, they still struggle to produce generalized representations due to insufficient 3D training data. Being contrastively pre-trained on web-scale image-text pairs, CLIP inherently produces generalized representations for a wide range of downstream tasks. Building upon it, we present a simple yet effective framework named Describe, Adapt and Combine (DAC) by taking only multi-view images for open-set 3DOR. DAC innovatively synergizes a CLIP model with a multi-modal large language model (MLLM) to learn generalized 3D representations, where the MLLM is used for dual purposes. First, it describes the seen category information to align with CLIP's training objective for adaptation during training. Second, it provides external hints about unknown objects complementary to visual cues during inference. To improve the synergy, we introduce an Additive-Bias Low-Rank adaptation (AB-LoRA), which alleviates overfitting and further enhances the generalization to unseen categories. With only multi-view images, DAC significantly surpasses prior arts by an average of +10.01\\% mAP on four open-set 3DOR datasets. Moreover, its generalization is also validated on image-based and cross-dataset setups. Code is available at https://github.com/wangzhichuan123/DAC."}
{"id": "2507.21521", "pdf": "https://arxiv.org/pdf/2507.21521", "abs": "https://arxiv.org/abs/2507.21521", "authors": ["Athmanarayanan Lakshmi Narayanan", "Amrutha Machireddy", "Ranganath Krishnan"], "title": "Optimizing Active Learning in Vision-Language Models via Parameter-Efficient Uncertainty Calibration", "categories": ["cs.CV"], "comment": "International Joint Conference on Neural Networks 2025 (Accepted)", "summary": "Active Learning (AL) has emerged as a powerful approach for minimizing labeling costs by selectively sampling the most informative data for neural network model development. Effective AL for large-scale vision-language models necessitates addressing challenges in uncertainty estimation and efficient sampling given the vast number of parameters involved. In this work, we introduce a novel parameter-efficient learning methodology that incorporates uncertainty calibration loss within the AL framework. We propose a differentiable loss function that promotes uncertainty calibration for effectively selecting fewer and most informative data samples for fine-tuning. Through extensive experiments across several datasets and vision backbones, we demonstrate that our solution can match and exceed the performance of complex feature-based sampling techniques while being computationally very efficient. Additionally, we investigate the efficacy of Prompt learning versus Low-rank adaptation (LoRA) in sample selection, providing a detailed comparative analysis of these methods in the context of efficient AL."}
{"id": "2507.21529", "pdf": "https://arxiv.org/pdf/2507.21529", "abs": "https://arxiv.org/abs/2507.21529", "authors": ["Mengling Xu", "Ming Tao", "Bing-Kun Bao"], "title": "Chain-of-Cooking:Cooking Process Visualization via Bidirectional Chain-of-Thought Guidance", "categories": ["cs.CV"], "comment": "Accepted by ACM MM 2025", "summary": "Cooking process visualization is a promising task in the intersection of image generation and food analysis, which aims to generate an image for each cooking step of a recipe. However, most existing works focus on generating images of finished foods based on the given recipes, and face two challenges to visualize the cooking process. First, the appearance of ingredients changes variously across cooking steps, it is difficult to generate the correct appearances of foods that match the textual description, leading to semantic inconsistency. Second, the current step might depend on the operations of previous step, it is crucial to maintain the contextual coherence of images in sequential order. In this work, we present a cooking process visualization model, called Chain-of-Cooking. Specifically, to generate correct appearances of ingredients, we present a Dynamic Patch Selection Module to retrieve previously generated image patches as references, which are most related to current textual contents. Furthermore, to enhance the coherence and keep the rational order of generated images, we propose a Semantic Evolution Module and a Bidirectional Chain-of-Thought (CoT) Guidance. To better utilize the semantics of previous texts, the Semantic Evolution Module establishes the semantical association between latent prompts and current cooking step, and merges it with the latent features. Then the CoT Guidance updates the merged features to guide the current cooking step remain coherent with the previous step. Moreover, we construct a dataset named CookViz, consisting of intermediate image-text pairs for the cooking process. Quantitative and qualitative experiments show that our method outperforms existing methods in generating coherent and semantic consistent cooking process."}
{"id": "2507.21606", "pdf": "https://arxiv.org/pdf/2507.21606", "abs": "https://arxiv.org/abs/2507.21606", "authors": ["Yaozong Zheng", "Bineng Zhong", "Qihua Liang", "Ning Li", "Shuxiang Song"], "title": "Decoupled Spatio-Temporal Consistency Learning for Self-Supervised Tracking", "categories": ["cs.CV"], "comment": "Accepted by AAAI2025", "summary": "The success of visual tracking has been largely driven by datasets with manual box annotations. However, these box annotations require tremendous human effort, limiting the scale and diversity of existing tracking datasets. In this work, we present a novel Self-Supervised Tracking framework named \\textbf{{\\tracker}}, designed to eliminate the need of box annotations. Specifically, a decoupled spatio-temporal consistency training framework is proposed to learn rich target information across timestamps through global spatial localization and local temporal association. This allows for the simulation of appearance and motion variations of instances in real-world scenarios. Furthermore, an instance contrastive loss is designed to learn instance-level correspondences from a multi-view perspective, offering robust instance supervision without additional labels. This new design paradigm enables {\\tracker} to effectively learn generic tracking representations in a self-supervised manner, while reducing reliance on extensive box annotations. Extensive experiments on nine benchmark datasets demonstrate that {\\tracker} surpasses \\textit{SOTA} self-supervised tracking methods, achieving an improvement of more than 25.3\\%, 20.4\\%, and 14.8\\% in AUC (AO) score on the GOT10K, LaSOT, TrackingNet datasets, respectively. Code: https://github.com/GXNU-ZhongLab/SSTrack."}
{"id": "2507.21627", "pdf": "https://arxiv.org/pdf/2507.21627", "abs": "https://arxiv.org/abs/2507.21627", "authors": ["Qimin Wang", "Xinda Liu", "Guohua Geng"], "title": "GuidPaint: Class-Guided Image Inpainting with Diffusion Models", "categories": ["cs.CV", "I.4.4"], "comment": null, "summary": "In recent years, diffusion models have been widely adopted for image inpainting tasks due to their powerful generative capabilities, achieving impressive results. Existing multimodal inpainting methods based on diffusion models often require architectural modifications and retraining, resulting in high computational cost. In contrast, context-aware diffusion inpainting methods leverage the model's inherent priors to adjust intermediate denoising steps, enabling high-quality inpainting without additional training and significantly reducing computation. However, these methods lack fine-grained control over the masked regions, often leading to semantically inconsistent or visually implausible content. To address this issue, we propose GuidPaint, a training-free, class-guided image inpainting framework. By incorporating classifier guidance into the denoising process, GuidPaint enables precise control over intermediate generations within the masked areas, ensuring both semantic consistency and visual realism. Furthermore, it integrates stochastic and deterministic sampling, allowing users to select preferred intermediate results and deterministically refine them. Experimental results demonstrate that GuidPaint achieves clear improvements over existing context-aware inpainting methods in both qualitative and quantitative evaluations."}
{"id": "2507.21690", "pdf": "https://arxiv.org/pdf/2507.21690", "abs": "https://arxiv.org/abs/2507.21690", "authors": ["Sangmin Han", "Jinho Jeong", "Jinwoo Kim", "Seon Joo Kim"], "title": "APT: Improving Diffusion Models for High Resolution Image Generation with Adaptive Path Tracing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Latent Diffusion Models (LDMs) are generally trained at fixed resolutions, limiting their capability when scaling up to high-resolution images. While training-based approaches address this limitation by training on high-resolution datasets, they require large amounts of data and considerable computational resources, making them less practical. Consequently, training-free methods, particularly patch-based approaches, have become a popular alternative. These methods divide an image into patches and fuse the denoising paths of each patch, showing strong performance on high-resolution generation. However, we observe two critical issues for patch-based approaches, which we call ``patch-level distribution shift\" and ``increased patch monotonicity.\" To address these issues, we propose Adaptive Path Tracing (APT), a framework that combines Statistical Matching to ensure patch distributions remain consistent in upsampled latents and Scale-aware Scheduling to deal with the patch monotonicity. As a result, APT produces clearer and more refined details in high-resolution images. In addition, APT enables a shortcut denoising process, resulting in faster sampling with minimal quality degradation. Our experimental results confirm that APT produces more detailed outputs with improved inference speed, providing a practical approach to high-resolution image generation."}
{"id": "2507.21715", "pdf": "https://arxiv.org/pdf/2507.21715", "abs": "https://arxiv.org/abs/2507.21715", "authors": ["Jason M. Summers", "Mark W. Jones"], "title": "Impact of Underwater Image Enhancement on Feature Matching", "categories": ["cs.CV"], "comment": null, "summary": "We introduce local matching stability and furthest matchable frame as quantitative measures for evaluating the success of underwater image enhancement. This enhancement process addresses visual degradation caused by light absorption, scattering, marine growth, and debris. Enhanced imagery plays a critical role in downstream tasks such as path detection and autonomous navigation for underwater vehicles, relying on robust feature extraction and frame matching. To assess the impact of enhancement techniques on frame-matching performance, we propose a novel evaluation framework tailored to underwater environments. Through metric-based analysis, we identify strengths and limitations of existing approaches and pinpoint gaps in their assessment of real-world applicability. By incorporating a practical matching strategy, our framework offers a robust, context-aware benchmark for comparing enhancement methods. Finally, we demonstrate how visual improvements affect the performance of a complete real-world algorithm -- Simultaneous Localization and Mapping (SLAM) -- reinforcing the framework's relevance to operational underwater scenarios."}
{"id": "2507.21723", "pdf": "https://arxiv.org/pdf/2507.21723", "abs": "https://arxiv.org/abs/2507.21723", "authors": ["Nils H\u00fctten", "Florian H\u00f6lken", "Hasan Tercan", "Tobias Meisen"], "title": "Detection Transformers Under the Knife: A Neuroscience-Inspired Approach to Ablations", "categories": ["cs.CV", "cs.AI", "I.2; I.4"], "comment": null, "summary": "In recent years, Explainable AI has gained traction as an approach to enhancing model interpretability and transparency, particularly in complex models such as detection transformers. Despite rapid advancements, a substantial research gap remains in understanding the distinct roles of internal components - knowledge that is essential for improving transparency and efficiency. Inspired by neuroscientific ablation studies, which investigate the functions of brain regions through selective impairment, we systematically analyze the impact of ablating key components in three state-of-the-art detection transformer models: Detection transformer (DETR), deformable detection transformer (DDETR), and DETR with improved denoising anchor boxes (DINO). The ablations target query embeddings, encoder and decoder multi-head self-attentions (MHSA) as well as decoder multi-head cross-attention (MHCA) layers. We evaluate the effects of these ablations on the performance metrics gIoU and F1-score, quantifying effects on both the classification and regression sub-tasks on the COCO dataset. To facilitate reproducibility and future research, we publicly release the DeepDissect library. Our findings reveal model-specific resilience patterns: while DETR is particularly sensitive to ablations in encoder MHSA and decoder MHCA, DDETR's multi-scale deformable attention enhances robustness, and DINO exhibits the greatest resilience due to its look-forward twice update rule, which helps distributing knowledge across blocks. These insights also expose structural redundancies, particularly in DDETR's and DINO's decoder MHCA layers, highlighting opportunities for model simplification without sacrificing performance. This study advances XAI for DETRs by clarifying the contributions of internal components to model performance, offering insights to optimize and improve transparency and efficiency in critical applications."}
{"id": "2507.21858", "pdf": "https://arxiv.org/pdf/2507.21858", "abs": "https://arxiv.org/abs/2507.21858", "authors": ["Jianhui Wang", "Yinda Chen", "Yangfan He", "Xinyuan Song", "Yi Xin", "Dapeng Zhang", "Zhongwei Wan", "Bin Li", "Rongchao Zhang"], "title": "Low-Cost Test-Time Adaptation for Robust Video Editing", "categories": ["cs.CV"], "comment": null, "summary": "Video editing is a critical component of content creation that transforms raw footage into coherent works aligned with specific visual and narrative objectives. Existing approaches face two major challenges: temporal inconsistencies due to failure in capturing complex motion patterns, and overfitting to simple prompts arising from limitations in UNet backbone architectures. While learning-based methods can enhance editing quality, they typically demand substantial computational resources and are constrained by the scarcity of high-quality annotated data. In this paper, we present Vid-TTA, a lightweight test-time adaptation framework that personalizes optimization for each test video during inference through self-supervised auxiliary tasks. Our approach incorporates a motion-aware frame reconstruction mechanism that identifies and preserves crucial movement regions, alongside a prompt perturbation and reconstruction strategy that strengthens model robustness to diverse textual descriptions. These innovations are orchestrated by a meta-learning driven dynamic loss balancing mechanism that adaptively adjusts the optimization process based on video characteristics. Extensive experiments demonstrate that Vid-TTA significantly improves video temporal consistency and mitigates prompt overfitting while maintaining low computational overhead, offering a plug-and-play performance boost for existing video editing models."}
{"id": "2507.21905", "pdf": "https://arxiv.org/pdf/2507.21905", "abs": "https://arxiv.org/abs/2507.21905", "authors": ["Viacheslav Pirogov", "Maksim Artemev"], "title": "Evaluating Deepfake Detectors in the Wild", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted to the ICML 2025 Workshop 'DataWorld: Unifying Data Curation   Frameworks Across Domains'", "summary": "Deepfakes powered by advanced machine learning models present a significant and evolving threat to identity verification and the authenticity of digital media. Although numerous detectors have been developed to address this problem, their effectiveness has yet to be tested when applied to real-world data. In this work we evaluate modern deepfake detectors, introducing a novel testing procedure designed to mimic real-world scenarios for deepfake detection. Using state-of-the-art deepfake generation methods, we create a comprehensive dataset containing more than 500,000 high-quality deepfake images. Our analysis shows that detecting deepfakes still remains a challenging task. The evaluation shows that in fewer than half of the deepfake detectors tested achieved an AUC score greater than 60%, with the lowest being 50%. We demonstrate that basic image manipulations, such as JPEG compression or image enhancement, can significantly reduce model performance. All code and data are publicly available at https://github.com/messlav/Deepfake-Detectors-in-the-Wild."}
{"id": "2507.21947", "pdf": "https://arxiv.org/pdf/2507.21947", "abs": "https://arxiv.org/abs/2507.21947", "authors": ["Jiwoong Park", "Chaeun Lee", "Yongseok Choi", "Sein Park", "Deokki Hong", "Jungwook Choi"], "title": "Enhancing Generalization in Data-free Quantization via Mixup-class Prompting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Post-training quantization (PTQ) improves efficiency but struggles with limited calibration data, especially under privacy constraints. Data-free quantization (DFQ) mitigates this by generating synthetic images using generative models such as generative adversarial networks (GANs) and text-conditioned latent diffusion models (LDMs), while applying existing PTQ algorithms. However, the relationship between generated synthetic images and the generalizability of the quantized model during PTQ remains underexplored. Without investigating this relationship, synthetic images generated by previous prompt engineering methods based on single-class prompts suffer from issues such as polysemy, leading to performance degradation. We propose \\textbf{mixup-class prompt}, a mixup-based text prompting strategy that fuses multiple class labels at the text prompt level to generate diverse, robust synthetic data. This approach enhances generalization, and improves optimization stability in PTQ. We provide quantitative insights through gradient norm and generalization error analysis. Experiments on convolutional neural networks (CNNs) and vision transformers (ViTs) show that our method consistently outperforms state-of-the-art DFQ methods like GenQ. Furthermore, it pushes the performance boundary in extremely low-bit scenarios, achieving new state-of-the-art accuracy in challenging 2-bit weight, 4-bit activation (W2A4) quantization."}
{"id": "2507.22003", "pdf": "https://arxiv.org/pdf/2507.22003", "abs": "https://arxiv.org/abs/2507.22003", "authors": ["Ziyun Dai", "Xiaoqiang Li", "Shaohua Zhang", "Yuanchen Wu", "Jide Li"], "title": "See Different, Think Better: Visual Variations Mitigating Hallucinations in LVLMs", "categories": ["cs.CV"], "comment": "Accepted by ACM MM25", "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in visual understanding and multimodal reasoning. However, LVLMs frequently exhibit hallucination phenomena, manifesting as the generated textual responses that demonstrate inconsistencies with the provided visual content. Existing hallucination mitigation methods are predominantly text-centric, the challenges of visual-semantic alignment significantly limit their effectiveness, especially when confronted with fine-grained visual understanding scenarios. To this end, this paper presents ViHallu, a Vision-Centric Hallucination mitigation framework that enhances visual-semantic alignment through Visual Variation Image Generation and Visual Instruction Construction. ViHallu introduces \\textbf{\\textit{visual variation images}} with controllable visual alterations while maintaining the overall image structure. These images, combined with carefully constructed visual instructions, enable LVLMs to better understand fine-grained visual content through fine-tuning, allowing models to more precisely capture the correspondence between visual content and text, thereby enhancing visual-semantic alignment. Extensive experiments on multiple benchmarks show that ViHallu effectively enhances models' fine-grained visual understanding while significantly reducing hallucination tendencies. Furthermore, we release ViHallu-Instruction, a visual instruction dataset specifically designed for hallucination mitigation and visual-semantic alignment. Code is available at https://github.com/oliviadzy/ViHallu."}
{"id": "2507.22008", "pdf": "https://arxiv.org/pdf/2507.22008", "abs": "https://arxiv.org/abs/2507.22008", "authors": ["Sajay Raj"], "title": "VeS: Teaching Pixels to Listen Without Supervision", "categories": ["cs.CV", "I.2.10"], "comment": "6 pages, 1 figure, 1 table. Code and models are released", "summary": "Recent dense audio-visual (AV) models achieve impressive retrieval and emergent localization, but almost all evidence comes from English-centric, caption-rich web video. It is unclear whether these objectives survive in low-resource, code-switched, and noisy multilingual settings that typify developing regions. We show they do**-**and that the choice of aggregation function becomes even more critical. Using a multilingual subset of Project Vaani spanning dozens of Indian languages and dialectal variants, we compare three contrastive objectives: (i) a global mean-pooled loss (CLIP-style), (ii) a dense max-mean token matcher (DenseAV-style), and (iii) a simple hybrid (motivated by frozen-vision alignment strategies). The dense objective delivers a +59% relative R@1 (Audio Visual) improvement over global pooling and substantially lower mean/median ranks, while consistently producing sharp zero-shot localization heatmaps of spoken objects-despite keeping the vision backbone entirely frozen (no LoRA / partial fine-tuning). Our results demonstrate that dense token routing is not a luxury of high-resource English corpora; it is more decisive when annotations and acoustic cleanliness are scarce. We release the codebase and trained models."}
{"id": "2507.22058", "pdf": "https://arxiv.org/pdf/2507.22058", "abs": "https://arxiv.org/abs/2507.22058", "authors": ["Zigang Geng", "Yibing Wang", "Yeyao Ma", "Chen Li", "Yongming Rao", "Shuyang Gu", "Zhao Zhong", "Qinglin Lu", "Han Hu", "Xiaosong Zhang", "Linus", "Di Wang", "Jie Jiang"], "title": "X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again", "categories": ["cs.CV"], "comment": null, "summary": "Numerous efforts have been made to extend the ``next token prediction'' paradigm to visual contents, aiming to create a unified approach for both image generation and understanding. Nevertheless, attempts to generate images through autoregressive modeling with discrete tokens have been plagued by issues such as low visual fidelity, distorted outputs, and failure to adhere to complex instructions when rendering intricate details. These shortcomings are likely attributed to cumulative errors during autoregressive inference or information loss incurred during the discretization process. Probably due to this challenge, recent research has increasingly shifted toward jointly training image generation with diffusion objectives and language generation with autoregressive objectives, moving away from unified modeling approaches. In this work, we demonstrate that reinforcement learning can effectively mitigate artifacts and largely enhance the generation quality of a discrete autoregressive modeling method, thereby enabling seamless integration of image and language generation. Our framework comprises a semantic image tokenizer, a unified autoregressive model for both language and images, and an offline diffusion decoder for image generation, termed X-Omni. X-Omni achieves state-of-the-art performance in image generation tasks using a 7B language model, producing images with high aesthetic quality while exhibiting strong capabilities in following instructions and rendering long texts."}
{"id": "2507.20650", "pdf": "https://arxiv.org/pdf/2507.20650", "abs": "https://arxiv.org/abs/2507.20650", "authors": ["Zhicheng Zhang", "Peizhuo Lv", "Mengke Wan", "Jiang Fang", "Diandian Guo", "Yezeng Chen", "Yinlong Liu", "Wei Ma", "Jiyan Sun", "Liru Geng"], "title": "Hot-Swap MarkBoard: An Efficient Black-box Watermarking Approach for Large-scale Model Distribution", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": null, "summary": "Recently, Deep Learning (DL) models have been increasingly deployed on end-user devices as On-Device AI, offering improved efficiency and privacy. However, this deployment trend poses more serious Intellectual Property (IP) risks, as models are distributed on numerous local devices, making them vulnerable to theft and redistribution. Most existing ownership protection solutions (e.g., backdoor-based watermarking) are designed for cloud-based AI-as-a-Service (AIaaS) and are not directly applicable to large-scale distribution scenarios, where each user-specific model instance must carry a unique watermark. These methods typically embed a fixed watermark, and modifying the embedded watermark requires retraining the model. To address these challenges, we propose Hot-Swap MarkBoard, an efficient watermarking method. It encodes user-specific $n$-bit binary signatures by independently embedding multiple watermarks into a multi-branch Low-Rank Adaptation (LoRA) module, enabling efficient watermark customization without retraining through branch swapping. A parameter obfuscation mechanism further entangles the watermark weights with those of the base model, preventing removal without degrading model performance. The method supports black-box verification and is compatible with various model architectures and DL tasks, including classification, image generation, and text generation. Extensive experiments across three types of tasks and six backbone models demonstrate our method's superior efficiency and adaptability compared to existing approaches, achieving 100\\% verification accuracy."}
{"id": "2507.21100", "pdf": "https://arxiv.org/pdf/2507.21100", "abs": "https://arxiv.org/abs/2507.21100", "authors": ["Wei Meng"], "title": "A Tactical Behaviour Recognition Framework Based on Causal Multimodal Reasoning: A Study on Covert Audio-Video Analysis Combining GAN Structure Enhancement and Phonetic Accent Modelling", "categories": ["cs.CY", "cs.AI", "cs.CV", "05C82, 68T07, 68T05, 62H30", "I.2.10; I.4.8; H.5.1; H.2.8"], "comment": "This paper introduces a structurally innovative and mathematically   rigorous framework for multimodal tactical reasoning, offering a significant   advance in causal inference and graph-based threat recognition under noisy   conditions", "summary": "This paper introduces TACTIC-GRAPHS, a system that combines spectral graph theory and multimodal graph neural reasoning for semantic understanding and threat detection in tactical video under high noise and weak structure. The framework incorporates spectral embedding, temporal causal edge modeling, and discriminative path inference across heterogeneous modalities. A semantic-aware keyframe extraction method fuses visual, acoustic, and action cues to construct temporal graphs. Using graph attention and Laplacian spectral mapping, the model performs cross-modal weighting and causal signal analysis. Experiments on TACTIC-AVS and TACTIC-Voice datasets show 89.3 percent accuracy in temporal alignment and over 85 percent recognition of complete threat chains, with node latency within plus-minus 150 milliseconds. The approach enhances structural interpretability and supports applications in surveillance, defense, and intelligent security systems."}
{"id": "2507.21540", "pdf": "https://arxiv.org/pdf/2507.21540", "abs": "https://arxiv.org/abs/2507.21540", "authors": ["Quanchen Zou", "Zonghao Ying", "Moyang Chen", "Wenzhuo Xu", "Yisong Xiao", "Yakai Li", "Deyue Zhang", "Dongdong Yang", "Zhao Liu", "Xiangzheng Zhang"], "title": "PRISM: Programmatic Reasoning with Image Sequence Manipulation for LVLM Jailbreaking", "categories": ["cs.CR", "cs.CV"], "comment": null, "summary": "The increasing sophistication of large vision-language models (LVLMs) has been accompanied by advances in safety alignment mechanisms designed to prevent harmful content generation. However, these defenses remain vulnerable to sophisticated adversarial attacks. Existing jailbreak methods typically rely on direct and semantically explicit prompts, overlooking subtle vulnerabilities in how LVLMs compose information over multiple reasoning steps. In this paper, we propose a novel and effective jailbreak framework inspired by Return-Oriented Programming (ROP) techniques from software security. Our approach decomposes a harmful instruction into a sequence of individually benign visual gadgets. A carefully engineered textual prompt directs the sequence of inputs, prompting the model to integrate the benign visual gadgets through its reasoning process to produce a coherent and harmful output. This makes the malicious intent emergent and difficult to detect from any single component. We validate our method through extensive experiments on established benchmarks including SafeBench and MM-SafetyBench, targeting popular LVLMs. Results show that our approach consistently and substantially outperforms existing baselines on state-of-the-art models, achieving near-perfect attack success rates (over 0.90 on SafeBench) and improving ASR by up to 0.39. Our findings reveal a critical and underexplored vulnerability that exploits the compositional reasoning abilities of LVLMs, highlighting the urgent need for defenses that secure the entire reasoning process."}
{"id": "2507.21802", "pdf": "https://arxiv.org/pdf/2507.21802", "abs": "https://arxiv.org/abs/2507.21802", "authors": ["Junzhe Li", "Yutao Cui", "Tao Huang", "Yinping Ma", "Chun Fan", "Miles Yang", "Zhao Zhong"], "title": "MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Although GRPO substantially enhances flow matching models in human preference alignment of image generation, methods such as FlowGRPO still exhibit inefficiency due to the necessity of sampling and optimizing over all denoising steps specified by the Markov Decision Process (MDP). In this paper, we propose $\\textbf{MixGRPO}$, a novel framework that leverages the flexibility of mixed sampling strategies through the integration of stochastic differential equations (SDE) and ordinary differential equations (ODE). This streamlines the optimization process within the MDP to improve efficiency and boost performance. Specifically, MixGRPO introduces a sliding window mechanism, using SDE sampling and GRPO-guided optimization only within the window, while applying ODE sampling outside. This design confines sampling randomness to the time-steps within the window, thereby reducing the optimization overhead, and allowing for more focused gradient updates to accelerate convergence. Additionally, as time-steps beyond the sliding window are not involved in optimization, higher-order solvers are supported for sampling. So we present a faster variant, termed $\\textbf{MixGRPO-Flash}$, which further improves training efficiency while achieving comparable performance. MixGRPO exhibits substantial gains across multiple dimensions of human preference alignment, outperforming DanceGRPO in both effectiveness and efficiency, with nearly 50% lower training time. Notably, MixGRPO-Flash further reduces training time by 71%. Codes and models are available at $\\href{https://github.com/Tencent-Hunyuan/MixGRPO}{MixGRPO}$."}
