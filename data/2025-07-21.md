<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 25]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [DLSF: Dual-Layer Synergistic Fusion for High-Fidelity Image Syn-thesis](https://arxiv.org/abs/2507.13388)
*Zhen-Qi Chen,Yuan-Fu Yang*

Main category: cs.GR

TL;DR: 提出了一种双潜在集成框架，通过特征拼接和自适应融合模块改进Stable Diffusion模型的特征聚合问题。


<details>
  <summary>Details</summary>
Motivation: 现有SD模型在特征聚合上表现不佳，导致语义对齐不完整和细节丢失，特别是在复杂场景中。

Method: 采用双潜在集成框架，结合特征拼接和自适应融合模块（AGF或DSF），以增强潜在表示间的特征交互。

Result: 新框架提升了全局一致性和局部纹理保真度，改进了复杂场景下的生成效果。

Conclusion: 提出的方法有效解决了SD模型的特征聚合问题，为高保真图像合成提供了新思路。

Abstract: With the rapid advancement of diffusion-based generative models, Stable Diffusion (SD) has emerged as a state-of-the-art framework for high-fidelity im-age synthesis. However, existing SD models suffer from suboptimal feature aggregation, leading to in-complete semantic alignment and loss of fine-grained details, especially in highly textured and complex scenes. To address these limitations, we propose a novel dual-latent integration framework that en-hances feature interactions between the base latent and refined latent representations. Our approach em-ploys a feature concatenation strategy followed by an adaptive fusion module, which can be instantiated as either (i) an Adaptive Global Fusion (AGF) for hier-archical feature harmonization, or (ii) a Dynamic Spatial Fusion (DSF) for spatially-aware refinement. This design enables more effective cross-latent com-munication, preserving both global coherence and local texture fidelity. Our GitHub page: https://anonymous.4open.science/r/MVA2025-22 .

</details>


### [2] [TexGS-VolVis: Expressive Scene Editing for Volume Visualization via Textured Gaussian Splatting](https://arxiv.org/abs/2507.13586)
*Kaiyuan Tang,Kuangshi Ai,Jun Han,Chaoli Wang*

Main category: cs.GR

TL;DR: TexGS-VolVis提出了一种基于纹理高斯溅射的框架，通过结合2D高斯基元和预训练大模型，实现了高质量、几何一致的风格化和实时渲染。


<details>
  <summary>Details</summary>
Motivation: 现有体积可视化方法依赖复杂预定义规则且风格单一，限制了灵活性和表现力。

Method: 采用纹理高斯溅射框架（TexGS-VolVis），扩展高斯基元以包含纹理和光照属性，并结合图像和文本驱动编辑。

Result: 在多种体积渲染场景中，TexGS-VolVis在效率、视觉质量和编辑灵活性上优于现有方法。

Conclusion: TexGS-VolVis为体积可视化提供了更灵活、高质量的解决方案，支持实时渲染和精细编辑。

Abstract: Advancements in volume visualization (VolVis) focus on extracting insights from 3D volumetric data by generating visually compelling renderings that reveal complex internal structures. Existing VolVis approaches have explored non-photorealistic rendering techniques to enhance the clarity, expressiveness, and informativeness of visual communication. While effective, these methods often rely on complex predefined rules and are limited to transferring a single style, restricting their flexibility. To overcome these limitations, we advocate the representation of VolVis scenes using differentiable Gaussian primitives combined with pretrained large models to enable arbitrary style transfer and real-time rendering. However, conventional 3D Gaussian primitives tightly couple geometry and appearance, leading to suboptimal stylization results. To address this, we introduce TexGS-VolVis, a textured Gaussian splatting framework for VolVis. TexGS-VolVis employs 2D Gaussian primitives, extending each Gaussian with additional texture and shading attributes, resulting in higher-quality, geometry-consistent stylization and enhanced lighting control during inference. Despite these improvements, achieving flexible and controllable scene editing remains challenging. To further enhance stylization, we develop image- and text-driven non-photorealistic scene editing tailored for TexGS-VolVis and 2D-lift-3D segmentation to enable partial editing with fine-grained control. We evaluate TexGS-VolVis both qualitatively and quantitatively across various volume rendering scenes, demonstrating its superiority over existing methods in terms of efficiency, visual quality, and editing flexibility.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [3] [Low-Light Enhancement via Encoder-Decoder Network with Illumination Guidance](https://arxiv.org/abs/2507.13360)
*Le-Anh Tran,Chung Nguyen Tran,Ngoc-Luu Nguyen,Nhan Cach Dang,Jordi Carrabina,David Castells-Rufas,Minh Son Nguyen*

Main category: cs.CV

TL;DR: EDNIG是一种基于U-Net架构的新型深度学习框架，通过亮度图引导和SPP模块提升低光图像增强效果，结合GAN框架优化，性能优越且模型复杂度低。


<details>
  <summary>Details</summary>
Motivation: 解决低光图像增强问题，提出一种更高效的方法，结合亮度引导和多尺度特征提取。

Method: 基于U-Net架构，引入亮度图（BCP）和SPP模块，使用Swish激活函数，并在GAN框架下优化。

Result: 在定量指标和视觉质量上优于现有方法，模型复杂度较低。

Conclusion: EDNIG适合实际应用，性能优越且高效。

Abstract: This paper introduces a novel deep learning framework for low-light image enhancement, named the Encoder-Decoder Network with Illumination Guidance (EDNIG). Building upon the U-Net architecture, EDNIG integrates an illumination map, derived from Bright Channel Prior (BCP), as a guidance input. This illumination guidance helps the network focus on underexposed regions, effectively steering the enhancement process. To further improve the model's representational power, a Spatial Pyramid Pooling (SPP) module is incorporated to extract multi-scale contextual features, enabling better handling of diverse lighting conditions. Additionally, the Swish activation function is employed to ensure smoother gradient propagation during training. EDNIG is optimized within a Generative Adversarial Network (GAN) framework using a composite loss function that combines adversarial loss, pixel-wise mean squared error (MSE), and perceptual loss. Experimental results show that EDNIG achieves competitive performance compared to state-of-the-art methods in quantitative metrics and visual quality, while maintaining lower model complexity, demonstrating its suitability for real-world applications. The source code for this work is available at https://github.com/tranleanh/ednig.

</details>


### [4] [Transformer-Based Framework for Motion Capture Denoising and Anomaly Detection in Medical Rehabilitation](https://arxiv.org/abs/2507.13371)
*Yeming Cai,Yang Wang,Zhenglin Li*

Main category: cs.CV

TL;DR: 提出了一种端到端深度学习框架，结合光学动作捕捉与Transformer模型，用于医疗康复，解决数据噪声和缺失问题，并实时检测异常动作。


<details>
  <summary>Details</summary>
Motivation: 医疗康复中，光学动作捕捉常因遮挡和环境因素导致数据噪声和缺失，影响康复效果和患者安全。

Method: 采用Transformer模型进行时间序列建模，对动作捕捉数据去噪和补全，增强鲁棒性。

Result: 在卒中和骨科康复数据集上表现优异，数据重建和异常检测效果显著。

Conclusion: 该框架为远程康复提供了可扩展、经济高效的解决方案，减少现场监督需求。

Abstract: This paper proposes an end-to-end deep learning framework integrating optical motion capture with a Transformer-based model to enhance medical rehabilitation. It tackles data noise and missing data caused by occlusion and environmental factors, while detecting abnormal movements in real time to ensure patient safety. Utilizing temporal sequence modeling, our framework denoises and completes motion capture data, improving robustness. Evaluations on stroke and orthopedic rehabilitation datasets show superior performance in data reconstruction and anomaly detection, providing a scalable, cost-effective solution for remote rehabilitation with reduced on-site supervision.

</details>


### [5] [Smart Routing for Multimodal Video Retrieval: When to Search What](https://arxiv.org/abs/2507.13374)
*Kevin Dela Rosa*

Main category: cs.CV

TL;DR: ModaRoute是一个基于LLM的多模态视频检索系统，通过动态选择最优模态减少计算开销，同时保持较高的检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有密集文本描述方法计算成本高且可能遗漏关键视觉信息，因此需要一种更高效的解决方案。

Method: 利用GPT-4.1分析查询意图并预测信息需求，动态选择ASR、OCR和视觉索引中的最优模态。

Result: 在180万视频片段上测试，计算开销减少41%，Recall@5达到60.9%，平均每查询使用1.78种模态。

Conclusion: 智能路由为多模态检索系统提供了实用解决方案，降低了成本并保持了竞争力。

Abstract: We introduce ModaRoute, an LLM-based intelligent routing system that dynamically selects optimal modalities for multimodal video retrieval. While dense text captions can achieve 75.9% Recall@5, they require expensive offline processing and miss critical visual information present in 34% of clips with scene text not captured by ASR. By analyzing query intent and predicting information needs, ModaRoute reduces computational overhead by 41% while achieving 60.9% Recall@5. Our approach uses GPT-4.1 to route queries across ASR (speech), OCR (text), and visual indices, averaging 1.78 modalities per query versus exhaustive 3.0 modality search. Evaluation on 1.8M video clips demonstrates that intelligent routing provides a practical solution for scaling multimodal retrieval systems, reducing infrastructure costs while maintaining competitive effectiveness for real-world deployment.

</details>


### [6] [MADI: Masking-Augmented Diffusion with Inference-Time Scaling for Visual Editing](https://arxiv.org/abs/2507.13401)
*Shreya Kadambi,Risheek Garrepalli,Shubhankar Borse,Munawar Hyatt,Fatih Porikli*

Main category: cs.CV

TL;DR: MADI框架通过Masking-Augmented gaussian Diffusion和Pause Tokens机制，显著提升了扩散模型的可编辑性和可控性。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在文本到图像生成中表现出色，但在结构化编辑和组合控制方面仍面临挑战。

Method: 提出MADI框架，包括MAgD训练策略（结合去噪和掩码重建）和Pause Tokens机制（推理时增加计算能力）。

Result: MADI显著提升了扩散模型的编辑性和组合性，支持局部化和结构感知的编辑。

Conclusion: MADI为扩散模型在通用上下文生成架构中的应用铺平了道路。

Abstract: Despite the remarkable success of diffusion models in text-to-image generation, their effectiveness in grounded visual editing and compositional control remains challenging. Motivated by advances in self-supervised learning and in-context generative modeling, we propose a series of simple yet powerful design choices that significantly enhance diffusion model capacity for structured, controllable generation and editing. We introduce Masking-Augmented Diffusion with Inference-Time Scaling (MADI), a framework that improves the editability, compositionality and controllability of diffusion models through two core innovations. First, we introduce Masking-Augmented gaussian Diffusion (MAgD), a novel training strategy with dual corruption process which combines standard denoising score matching and masked reconstruction by masking noisy input from forward process. MAgD encourages the model to learn discriminative and compositional visual representations, thus enabling localized and structure-aware editing. Second, we introduce an inference-time capacity scaling mechanism based on Pause Tokens, which act as special placeholders inserted into the prompt for increasing computational capacity at inference time. Our findings show that adopting expressive and dense prompts during training further enhances performance, particularly for MAgD. Together, these contributions in MADI substantially enhance the editability of diffusion models, paving the way toward their integration into more general-purpose, in-context generative diffusion architectures.

</details>


### [7] [AortaDiff: Volume-Guided Conditional Diffusion Models for Multi-Branch Aortic Surface Generation](https://arxiv.org/abs/2507.13404)
*Delin An,Pan Du,Jian-Xun Wang,Chaoli Wang*

Main category: cs.CV

TL;DR: AortaDiff是一种基于扩散的框架，直接从CT/MRI体积生成平滑的主动脉表面，适用于CFD分析，减少对大型标注数据集的依赖。


<details>
  <summary>Details</summary>
Motivation: 准确的3D主动脉构建对临床诊断和CFD模拟至关重要，但现有方法依赖大量标注数据和手动干预，难以生成几何一致的表面。

Method: AortaDiff通过体积引导的条件扩散模型生成主动脉中心线，并自动提取血管轮廓，最终拟合为平滑的3D表面。

Result: 实验表明，AortaDiff在有限训练数据下仍能有效构建正常和病理主动脉网格，包括动脉瘤或狭窄病例。

Conclusion: AortaDiff提供端到端工作流程，生成高几何保真度的CFD兼容网格，是心血管研究的实用解决方案。

Abstract: Accurate 3D aortic construction is crucial for clinical diagnosis, preoperative planning, and computational fluid dynamics (CFD) simulations, as it enables the estimation of critical hemodynamic parameters such as blood flow velocity, pressure distribution, and wall shear stress. Existing construction methods often rely on large annotated training datasets and extensive manual intervention. While the resulting meshes can serve for visualization purposes, they struggle to produce geometrically consistent, well-constructed surfaces suitable for downstream CFD analysis. To address these challenges, we introduce AortaDiff, a diffusion-based framework that generates smooth aortic surfaces directly from CT/MRI volumes. AortaDiff first employs a volume-guided conditional diffusion model (CDM) to iteratively generate aortic centerlines conditioned on volumetric medical images. Each centerline point is then automatically used as a prompt to extract the corresponding vessel contour, ensuring accurate boundary delineation. Finally, the extracted contours are fitted into a smooth 3D surface, yielding a continuous, CFD-compatible mesh representation. AortaDiff offers distinct advantages over existing methods, including an end-to-end workflow, minimal dependency on large labeled datasets, and the ability to generate CFD-compatible aorta meshes with high geometric fidelity. Experimental results demonstrate that AortaDiff performs effectively even with limited training data, successfully constructing both normal and pathologically altered aorta meshes, including cases with aneurysms or coarctation. This capability enables the generation of high-quality visualizations and positions AortaDiff as a practical solution for cardiovascular research.

</details>


### [8] ["PhyWorldBench": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models](https://arxiv.org/abs/2507.13428)
*Jing Gu,Xian Liu,Yu Zeng,Ashwin Nagarajan,Fangrui Zhu,Daniel Hong,Yue Fan,Qianqi Yan,Kaiwen Zhou,Ming-Yu Liu,Xin Eric Wang*

Main category: cs.CV

TL;DR: PhyWorldBench是一个评估视频生成模型物理模拟能力的基准，涵盖从基础物理现象到复杂场景，并引入“反物理”类别。通过人类评估和MLLM方法测试了12个模型，发现其在物理一致性上的挑战。


<details>
  <summary>Details</summary>
Motivation: 视频生成模型在物理现象模拟方面存在不足，需系统评估其物理一致性。

Method: 设计PhyWorldBench基准，包含多级物理现象和“反物理”类别，结合人类评估和MLLM方法测试模型。

Result: 测试12个模型，发现其在物理一致性上的显著挑战，并提出优化提示的建议。

Conclusion: PhyWorldBench揭示了视频生成模型在物理模拟上的不足，为未来改进提供了方向。

Abstract: Video generation models have achieved remarkable progress in creating high-quality, photorealistic content. However, their ability to accurately simulate physical phenomena remains a critical and unresolved challenge. This paper presents PhyWorldBench, a comprehensive benchmark designed to evaluate video generation models based on their adherence to the laws of physics. The benchmark covers multiple levels of physical phenomena, ranging from fundamental principles like object motion and energy conservation to more complex scenarios involving rigid body interactions and human or animal motion. Additionally, we introduce a novel ""Anti-Physics"" category, where prompts intentionally violate real-world physics, enabling the assessment of whether models can follow such instructions while maintaining logical consistency. Besides large-scale human evaluation, we also design a simple yet effective method that could utilize current MLLM to evaluate the physics realism in a zero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation models, including five open-source and five proprietary models, with a detailed comparison and analysis. we identify pivotal challenges models face in adhering to real-world physics. Through systematic testing of their outputs across 1,050 curated prompts-spanning fundamental, composite, and anti-physics scenarios-we identify pivotal challenges these models face in adhering to real-world physics. We then rigorously examine their performance on diverse physical phenomena with varying prompt types, deriving targeted recommendations for crafting prompts that enhance fidelity to physical principles.

</details>


### [9] [Total Generalized Variation of the Normal Vector Field and Applications to Mesh Denoising](https://arxiv.org/abs/2507.13530)
*Lukas Baumgärtner,Ronny Bergmann,Roland Herzog,Stephan Schmidt,Manuel Weiß*

Main category: cs.CV

TL;DR: 提出了一种新的二阶总广义变分（TGV）公式，用于处理嵌入在三维空间中的三角网格上的法向量，并将其与现有方法在网格去噪实验中进行了比较。


<details>
  <summary>Details</summary>
Motivation: 扩展离散TGV模型以处理流形值函数（如单位球面上的法向量），并构建适合的有限元空间。

Method: 构建了一个定制的切向Raviart-Thomas型有限元空间，用于流形设置下的TGV公式。

Result: 新正则化器在网格去噪实验中表现优于现有方法。

Conclusion: 提出的方法在法向量处理上具有潜力，尤其在网格去噪应用中表现优异。

Abstract: We propose a novel formulation for the second-order total generalized variation (TGV) of the normal vector on an oriented, triangular mesh embedded in $\mathbb{R}^3$. The normal vector is considered as a manifold-valued function, taking values on the unit sphere. Our formulation extends previous discrete TGV models for piecewise constant scalar data that utilize a Raviart-Thomas function space. To exctend this formulation to the manifold setting, a tailor-made tangential Raviart-Thomas type finite element space is constructed in this work. The new regularizer is compared to existing methods in mesh denoising experiments.

</details>


### [10] [$\nabla$NABLA: Neighborhood Adaptive Block-Level Attention](https://arxiv.org/abs/2507.13546)
*Dmitrii Mikhailov,Aleksey Letunovskiy,Maria Kovaleva,Vladimir Arkhipkin,Vladimir Korviakov,Vladimir Polovnikov,Viacheslav Vasilev,Evelina Sidorova,Denis Dimitrov*

Main category: cs.CV

TL;DR: NABLA提出了一种新型的邻域自适应块级注意力机制，显著降低了视频生成任务中的计算开销，同时保持了生成质量。


<details>
  <summary>Details</summary>
Motivation: 全注意力机制的二次复杂度是视频生成任务中的瓶颈，尤其是高分辨率和长视频序列。

Method: 采用块级注意力机制，结合自适应稀疏阈值，动态适应视频扩散变换器中的稀疏模式。

Result: NABLA在训练和推理速度上比基线快2.7倍，且几乎不影响生成质量和量化指标。

Conclusion: NABLA是一种高效且无需定制低层操作设计的注意力机制，适用于视频生成任务。

Abstract: Recent progress in transformer-based architectures has demonstrated remarkable success in video generation tasks. However, the quadratic complexity of full attention mechanisms remains a critical bottleneck, particularly for high-resolution and long-duration video sequences. In this paper, we propose NABLA, a novel Neighborhood Adaptive Block-Level Attention mechanism that dynamically adapts to sparsity patterns in video diffusion transformers (DiTs). By leveraging block-wise attention with adaptive sparsity-driven threshold, NABLA reduces computational overhead while preserving generative quality. Our method does not require custom low-level operator design and can be seamlessly integrated with PyTorch's Flex Attention operator. Experiments demonstrate that NABLA achieves up to 2.7x faster training and inference compared to baseline almost without compromising quantitative metrics (CLIP score, VBench score, human evaluation score) and visual quality drop. The code and model weights are available here: https://github.com/gen-ai-team/Wan2.1-NABLA

</details>


### [11] [LoRA-Loop: Closing the Synthetic Replay Cycle for Continual VLM Learning](https://arxiv.org/abs/2507.13568)
*Kaihong Wang,Donghyun Kim,Margrit Betke*

Main category: cs.CV

TL;DR: 提出了一种基于LoRA增强的合成重放框架，通过任务特定的低秩适配器改进Stable Diffusion模型，提升持续学习中视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有合成重放方法生成的样本可能因未捕捉领域特定细节而误导微调，导致知识遗忘。

Method: 采用LoRA增强的合成重放框架，结合两阶段置信度样本选择，优化生成样本的代表性。

Result: 在MTIL基准测试中表现优于现有方法，平衡了可塑性、稳定性和零样本能力。

Conclusion: LoRA适配生成器能有效提升持续学习中视觉语言模型的鲁棒性。

Abstract: Continual learning for vision-language models has achieved remarkable performance through synthetic replay, where samples are generated using Stable Diffusion to regularize during finetuning and retain knowledge. However, real-world downstream applications often exhibit domain-specific nuances and fine-grained semantics not captured by generators, causing synthetic-replay methods to produce misaligned samples that misguide finetuning and undermine retention of prior knowledge. In this work, we propose a LoRA-enhanced synthetic-replay framework that injects task-specific low-rank adapters into a frozen Stable Diffusion model, efficiently capturing each new task's unique visual and semantic patterns. Specifically, we introduce a two-stage, confidence-based sample selection: we first rank real task data by post-finetuning VLM confidence to focus LoRA finetuning on the most representative examples, then generate synthetic samples and again select them by confidence for distillation. Our approach integrates seamlessly with existing replay pipelines-simply swap in the adapted generator to boost replay fidelity. Extensive experiments on the Multi-domain Task Incremental Learning (MTIL) benchmark show that our method outperforms previous synthetic-replay techniques, achieving an optimal balance among plasticity, stability, and zero-shot capability. These results demonstrate the effectiveness of generator adaptation via LoRA for robust continual learning in VLMs.

</details>


### [12] [Learning Deblurring Texture Prior from Unpaired Data with Diffusion Model](https://arxiv.org/abs/2507.13599)
*Chengxu Liu,Lu Qi,Jinshan Pan,Xueming Qian,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的框架（\ours），通过从非配对数据中学习空间变化的纹理先验，实现图像去模糊。该方法利用纹理先验编码器（TPE）和纹理转移变换层（TTformer）高效去除空间变化的模糊，并通过小波对抗损失保留高频纹理细节。


<details>
  <summary>Details</summary>
Motivation: 由于获取大量真实的模糊-清晰图像对困难且昂贵，从非配对数据中学习盲图像去模糊更具实用性和前景。现有方法主要依赖对抗学习，忽略了真实世界模糊模式的复杂性和不可预测性。

Method: 提出扩散模型框架，包括纹理先验编码器（TPE）和纹理转移变换层（TTformer），利用FM-MSA自适应滤波去除空间变化模糊，并采用小波对抗损失保留高频细节。

Result: 在广泛使用的基准测试中表现优异，优于现有最先进方法。

Conclusion: \ours 提供了一种有前景的无监督去模糊解决方案，能够有效处理真实世界的复杂模糊模式。

Abstract: Since acquiring large amounts of realistic blurry-sharp image pairs is difficult and expensive, learning blind image deblurring from unpaired data is a more practical and promising solution. Unfortunately, dominant approaches rely heavily on adversarial learning to bridge the gap from blurry domains to sharp domains, ignoring the complex and unpredictable nature of real-world blur patterns. In this paper, we propose a novel diffusion model (DM)-based framework, dubbed \ours, for image deblurring by learning spatially varying texture prior from unpaired data. In particular, \ours performs DM to generate the prior knowledge that aids in recovering the textures of blurry images. To implement this, we propose a Texture Prior Encoder (TPE) that introduces a memory mechanism to represent the image textures and provides supervision for DM training. To fully exploit the generated texture priors, we present the Texture Transfer Transformer layer (TTformer), in which a novel Filter-Modulated Multi-head Self-Attention (FM-MSA) efficiently removes spatially varying blurring through adaptive filtering. Furthermore, we implement a wavelet-based adversarial loss to preserve high-frequency texture details. Extensive evaluations show that \ours provides a promising unsupervised deblurring solution and outperforms SOTA methods in widely-used benchmarks.

</details>


### [13] [Efficient Burst Super-Resolution with One-step Diffusion](https://arxiv.org/abs/2507.13607)
*Kento Kawai,Takeru Oba,Kyotaro Tokoro,Kazutoshi Akita,Norimichi Ukita*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的爆发低分辨率图像超分辨率方法，通过随机采样器和知识蒸馏提高效率，显著减少运行时间并保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有爆发超分辨率方法生成的图像模糊且感知质量差，本文旨在通过扩散模型重建清晰且高保真的超分辨率图像。

Method: 使用随机采样器结合高阶ODE和知识蒸馏的一步扩散方法，提高扩散模型的效率。

Result: 实验结果表明，该方法将运行时间减少至基线的1.6%，同时保持基于图像失真和感知质量的超分辨率效果。

Conclusion: 该方法在保持图像质量的同时显著提高了效率，为爆发超分辨率提供了一种高效解决方案。

Abstract: While burst Low-Resolution (LR) images are useful for improving their Super Resolution (SR) image compared to a single LR image, prior burst SR methods are trained in a deterministic manner, which produces a blurry SR image. Since such blurry images are perceptually degraded, we aim to reconstruct sharp and high-fidelity SR images by a diffusion model. Our method improves the efficiency of the diffusion model with a stochastic sampler with a high-order ODE as well as one-step diffusion using knowledge distillation. Our experimental results demonstrate that our method can reduce the runtime to 1.6 % of its baseline while maintaining the SR quality measured based on image distortion and perceptual quality.

</details>


### [14] [EPSilon: Efficient Point Sampling for Lightening of Hybrid-based 3D Avatar Generation](https://arxiv.org/abs/2507.13648)
*Seungjun Moon,Sangjoon Yu,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: EPSilon提出了一种高效的混合3D头像生成方法，通过空点采样策略显著提升了训练和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于NeRF和SMPL的混合方法因变形计算成本高导致推理速度慢，EPSilon旨在通过优化采样策略解决这一问题。

Method: 提出两种空点采样策略：空射线忽略（ERO）和空区间忽略（EIO），以减少无效计算。

Result: EPSilon仅需3.9%的采样点，推理速度提升约20倍，训练收敛速度提升4倍，同时保持生成质量。

Conclusion: EPSilon通过高效采样策略显著提升了混合3D头像生成的效率，为实时应用提供了可能。

Abstract: The rapid advancement of neural radiance fields (NeRF) has paved the way to generate animatable human avatars from a monocular video. However, the sole usage of NeRF suffers from a lack of details, which results in the emergence of hybrid representation that utilizes SMPL-based mesh together with NeRF representation. While hybrid-based models show photo-realistic human avatar generation qualities, they suffer from extremely slow inference due to their deformation scheme: to be aligned with the mesh, hybrid-based models use the deformation based on SMPL skinning weights, which needs high computational costs on each sampled point. We observe that since most of the sampled points are located in empty space, they do not affect the generation quality but result in inference latency with deformation. In light of this observation, we propose EPSilon, a hybrid-based 3D avatar generation scheme with novel efficient point sampling strategies that boost both training and inference. In EPSilon, we propose two methods to omit empty points at rendering; empty ray omission (ERO) and empty interval omission (EIO). In ERO, we wipe out rays that progress through the empty space. Then, EIO narrows down the sampling interval on the ray, which wipes out the region not occupied by either clothes or mesh. The delicate sampling scheme of EPSilon enables not only great computational cost reduction during deformation but also the designation of the important regions to be sampled, which enables a single-stage NeRF structure without hierarchical sampling. Compared to existing methods, EPSilon maintains the generation quality while using only 3.9% of sampled points and achieves around 20 times faster inference, together with 4 times faster training convergence. We provide video results on https://github.com/seungjun-moon/epsilon.

</details>


### [15] [Global Modeling Matters: A Fast, Lightweight and Effective Baseline for Efficient Image Restoration](https://arxiv.org/abs/2507.13663)
*Xingyu Jiang,Ning Gao,Hongkun Dou,Xiuhui Zhang,Xiaoqing Zhong,Yue Deng,Hongjue Li*

Main category: cs.CV

TL;DR: 提出了一种基于金字塔小波-傅里叶迭代管道的高效图像恢复基线PW-FNet，结合多尺度多频带分解和傅里叶变换替代自注意力机制，显著提升恢复质量和效率。


<details>
  <summary>Details</summary>
Motivation: 自然图像质量常因恶劣天气条件下降，影响下游任务性能。现有基于Transformer的方法虽有效但复杂度高，难以实时处理。

Method: PW-FNet采用金字塔小波多输入多输出结构实现多尺度分解，并在块内用傅里叶变换替代自注意力机制，降低计算复杂度。

Result: 在多种图像恢复任务中，PW-FNet在恢复质量和效率上均超越现有方法，参数规模和计算成本显著降低。

Conclusion: PW-FNet展示了小波-傅里叶处理在图像恢复中的潜力，为高效恢复提供了新基线。

Abstract: Natural image quality is often degraded by adverse weather conditions, significantly impairing the performance of downstream tasks. Image restoration has emerged as a core solution to this challenge and has been widely discussed in the literature. Although recent transformer-based approaches have made remarkable progress in image restoration, their increasing system complexity poses significant challenges for real-time processing, particularly in real-world deployment scenarios. To this end, most existing methods attempt to simplify the self-attention mechanism, such as by channel self-attention or state space model. However, these methods primarily focus on network architecture while neglecting the inherent characteristics of image restoration itself. In this context, we explore a pyramid Wavelet-Fourier iterative pipeline to demonstrate the potential of Wavelet-Fourier processing for image restoration. Inspired by the above findings, we propose a novel and efficient restoration baseline, named Pyramid Wavelet-Fourier Network (PW-FNet). Specifically, PW-FNet features two key design principles: 1) at the inter-block level, integrates a pyramid wavelet-based multi-input multi-output structure to achieve multi-scale and multi-frequency bands decomposition; and 2) at the intra-block level, incorporates Fourier transforms as an efficient alternative to self-attention mechanisms, effectively reducing computational complexity while preserving global modeling capability. Extensive experiments on tasks such as image deraining, raindrop removal, image super-resolution, motion deblurring, image dehazing, image desnowing and underwater/low-light enhancement demonstrate that PW-FNet not only surpasses state-of-the-art methods in restoration quality but also achieves superior efficiency, with significantly reduced parameter size, computational cost and inference time.

</details>


### [16] [Gaussian kernel-based motion measurement](https://arxiv.org/abs/2507.13693)
*Hongyi Liu,Haifeng Wang*

Main category: cs.CV

TL;DR: 提出了一种基于高斯核的运动测量方法，用于高精度结构健康监测，无需手动调参即可实现亚像素级精度。


<details>
  <summary>Details</summary>
Motivation: 结构健康监测需求增长，现有视觉方法在亚像素级运动测量中精度不足或需大量手动调参。

Method: 开发了基于高斯核的运动测量方法，引入运动一致性和超分辨率约束以提高精度和鲁棒性。

Result: 数值和实验验证表明，该方法无需定制参数即可实现高精度。

Conclusion: 该方法为结构健康监测提供了一种高效、高精度的运动测量解决方案。

Abstract: The growing demand for structural health monitoring has driven increasing interest in high-precision motion measurement, as structural information derived from extracted motions can effectively reflect the current condition of the structure. Among various motion measurement techniques, vision-based methods stand out due to their low cost, easy installation, and large-scale measurement. However, when it comes to sub-pixel-level motion measurement, current vision-based methods either lack sufficient accuracy or require extensive manual parameter tuning (e.g., pyramid layers, target pixels, and filter parameters) to reach good precision. To address this issue, we developed a novel Gaussian kernel-based motion measurement method, which can extract the motion between different frames via tracking the location of Gaussian kernels. The motion consistency, which fits practical structural conditions, and a super-resolution constraint, are introduced to increase accuracy and robustness of our method. Numerical and experimental validations show that it can consistently reach high accuracy without customized parameter setup for different test samples.

</details>


### [17] [PoemTale Diffusion: Minimising Information Loss in Poem to Image Generation with Multi-Stage Prompt Refinement](https://arxiv.org/abs/2507.13708)
*Sofia Jamil,Bollampalli Areen Reddy,Raghvendra Kumar,Sriparna Saha,Koustava Goswami,K. J. Joseph*

Main category: cs.CV

TL;DR: 提出了一种无训练方法PoemTale Diffusion，通过多阶段提示优化和自注意力机制改进，提升诗歌文本到图像的生成效果，并发布了P4I数据集。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在处理复杂、抽象的诗歌语言时效果不佳，信息丢失严重。

Method: 采用多阶段提示优化循环和自注意力机制改进，生成一致的多幅图像以传达诗歌含义。

Result: 通过人类和定量评估验证了方法的有效性，生成图像能更好地捕捉诗歌信息。

Conclusion: PoemTale Diffusion为诗歌到图像生成提供了新视角，并推动了相关研究。

Abstract: Recent advancements in text-to-image diffusion models have achieved remarkable success in generating realistic and diverse visual content. A critical factor in this process is the model's ability to accurately interpret textual prompts. However, these models often struggle with creative expressions, particularly those involving complex, abstract, or highly descriptive language. In this work, we introduce a novel training-free approach tailored to improve image generation for a unique form of creative language: poetic verse, which frequently features layered, abstract, and dual meanings. Our proposed PoemTale Diffusion approach aims to minimise the information that is lost during poetic text-to-image conversion by integrating a multi stage prompt refinement loop into Language Models to enhance the interpretability of poetic texts. To support this, we adapt existing state-of-the-art diffusion models by modifying their self-attention mechanisms with a consistent self-attention technique to generate multiple consistent images, which are then collectively used to convey the poem's meaning. Moreover, to encourage research in the field of poetry, we introduce the P4I (PoemForImage) dataset, consisting of 1111 poems sourced from multiple online and offline resources. We engaged a panel of poetry experts for qualitative assessments. The results from both human and quantitative evaluations validate the efficacy of our method and contribute a novel perspective to poem-to-image generation with enhanced information capture in the generated images.

</details>


### [18] [Can Synthetic Images Conquer Forgetting? Beyond Unexplored Doubts in Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2507.13739)
*Junsu Kim,Yunhoe Ku,Seungryul Baek*

Main category: cs.CV

TL;DR: Diffusion-FSCIL利用预训练的扩散模型解决少样本类增量学习问题，通过多尺度特征提取和轻量级蒸馏提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决少样本类增量学习中数据不足和灾难性遗忘的挑战。

Method: 使用冻结的文本到图像扩散模型作为主干，提取多尺度特征并辅以轻量级蒸馏。

Result: 在CUB-200、miniImageNet和CIFAR-100上超越现有方法，保持旧类性能并适应新类。

Conclusion: Diffusion-FSCIL通过生成模型的能力和高效设计，有效解决了少样本类增量学习问题。

Abstract: Few-shot class-incremental learning (FSCIL) is challenging due to extremely limited training data; while aiming to reduce catastrophic forgetting and learn new information. We propose Diffusion-FSCIL, a novel approach that employs a text-to-image diffusion model as a frozen backbone. Our conjecture is that FSCIL can be tackled using a large generative model's capabilities benefiting from 1) generation ability via large-scale pre-training; 2) multi-scale representation; 3) representational flexibility through the text encoder. To maximize the representation capability, we propose to extract multiple complementary diffusion features to play roles as latent replay with slight support from feature distillation for preventing generative biases. Our framework realizes efficiency through 1) using a frozen backbone; 2) minimal trainable components; 3) batch processing of multiple feature extractions. Extensive experiments on CUB-200, \emph{mini}ImageNet, and CIFAR-100 show that Diffusion-FSCIL surpasses state-of-the-art methods, preserving performance on previously learned classes and adapting effectively to new ones.

</details>


### [19] [Encapsulated Composition of Text-to-Image and Text-to-Video Models for High-Quality Video Synthesis](https://arxiv.org/abs/2507.13753)
*Tongtong Su,Chengyu Wang,Bingyan Liu,Jun Huang,Dongming Lu*

Main category: cs.CV

TL;DR: EVS是一种无需训练的封装视频合成器，结合T2I和T2V模型，提升生成视频的视觉保真度和运动平滑性。


<details>
  <summary>Details</summary>
Motivation: 现有T2V模型在生成高质量视频时存在画面闪烁和伪影问题，EVS旨在解决这些问题。

Method: 利用预训练的T2I模型优化低质量视频帧，同时结合T2V模型保证运动一致性。

Result: 实验表明EVS在视频质量和推理速度上均有显著提升。

Conclusion: EVS通过结合T2I和T2V模型的优势，实现了高质量视频生成和高效推理。

Abstract: In recent years, large text-to-video (T2V) synthesis models have garnered considerable attention for their abilities to generate videos from textual descriptions. However, achieving both high imaging quality and effective motion representation remains a significant challenge for these T2V models. Existing approaches often adapt pre-trained text-to-image (T2I) models to refine video frames, leading to issues such as flickering and artifacts due to inconsistencies across frames. In this paper, we introduce EVS, a training-free Encapsulated Video Synthesizer that composes T2I and T2V models to enhance both visual fidelity and motion smoothness of generated videos. Our approach utilizes a well-trained diffusion-based T2I model to refine low-quality video frames by treating them as out-of-distribution samples, effectively optimizing them with noising and denoising steps. Meanwhile, we employ T2V backbones to ensure consistent motion dynamics. By encapsulating the T2V temporal-only prior into the T2I generation process, EVS successfully leverages the strengths of both types of models, resulting in videos of improved imaging and motion quality. Experimental results validate the effectiveness of our approach compared to previous approaches. Our composition process also leads to a significant improvement of 1.6x-4.5x speedup in inference time. Source codes: https://github.com/Tonniia/EVS.

</details>


### [20] [Learning Spectral Diffusion Prior for Hyperspectral Image Reconstruction](https://arxiv.org/abs/2507.13769)
*Mingyang Yu,Zhijian Wu,Dingjiang Huang*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的光谱扩散先验（SDP）和光谱先验注入模块（SPIM），显著提升了高光谱图像（HSI）重建的性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法难以准确捕捉HSI的高频细节，需要一种更有效的先验知识来提升重建质量。

Method: 通过扩散模型隐式学习HSI的光谱扩散先验（SDP），并结合动态指导的光谱先验注入模块（SPIM）来恢复细节。

Result: 在MST和BISRNet两种代表性HSI方法上，性能提升约0.5 dB。

Conclusion: SDP和SPIM的结合有效提升了HSI重建的细节恢复能力。

Abstract: Hyperspectral image (HSI) reconstruction aims to recover 3D HSI from its degraded 2D measurements. Recently great progress has been made in deep learning-based methods, however, these methods often struggle to accurately capture high-frequency details of the HSI. To address this issue, this paper proposes a Spectral Diffusion Prior (SDP) that is implicitly learned from hyperspectral images using a diffusion model. Leveraging the powerful ability of the diffusion model to reconstruct details, this learned prior can significantly improve the performance when injected into the HSI model. To further improve the effectiveness of the learned prior, we also propose the Spectral Prior Injector Module (SPIM) to dynamically guide the model to recover the HSI details. We evaluate our method on two representative HSI methods: MST and BISRNet. Experimental results show that our method outperforms existing networks by about 0.5 dB, effectively improving the performance of HSI reconstruction.

</details>


### [21] [Localized FNO for Spatiotemporal Hemodynamic Upsampling in Aneurysm MRI](https://arxiv.org/abs/2507.13789)
*Kyriakos Flouris,Moritz Halter,Yolanne Y. R. Lee,Samuel Castonguay,Luuk Jacobs,Pietro Dirix,Jonathan Nestmann,Sebastian Kozerke,Ender Konukoglu*

Main category: cs.CV

TL;DR: LoFNO是一种新型3D架构，通过结合几何先验和神经算子框架，提升血流数据的时空分辨率，直接预测壁面剪切应力，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 磁共振血流成像的低时空分辨率和信噪比限制了其诊断效果，需要一种新方法提升血流动力学分析的精度。

Method: 提出LoFNO架构，整合拉普拉斯特征向量作为几何先验，结合EDSR层进行鲁棒上采样，实现去噪和时空分辨率提升。

Result: LoFNO在速度和壁面剪切应力预测上优于插值和其他深度学习方法。

Conclusion: LoFNO为脑血管诊断提供了更精确的工具，提升了血流动力学分析的实用性。

Abstract: Hemodynamic analysis is essential for predicting aneurysm rupture and guiding treatment. While magnetic resonance flow imaging enables time-resolved volumetric blood velocity measurements, its low spatiotemporal resolution and signal-to-noise ratio limit its diagnostic utility. To address this, we propose the Localized Fourier Neural Operator (LoFNO), a novel 3D architecture that enhances both spatial and temporal resolution with the ability to predict wall shear stress (WSS) directly from clinical imaging data. LoFNO integrates Laplacian eigenvectors as geometric priors for improved structural awareness on irregular, unseen geometries and employs an Enhanced Deep Super-Resolution Network (EDSR) layer for robust upsampling. By combining geometric priors with neural operator frameworks, LoFNO de-noises and spatiotemporally upsamples flow data, achieving superior velocity and WSS predictions compared to interpolation and alternative deep learning methods, enabling more precise cerebrovascular diagnostics.

</details>


### [22] [DynFaceRestore: Balancing Fidelity and Quality in Diffusion-Guided Blind Face Restoration with Dynamic Blur-Level Mapping and Guidance](https://arxiv.org/abs/2507.13797)
*Huu-Phu Do,Yu-Wei Chen,Yi-Cheng Liao,Chi-Wei Hsiao,Han-Yang Wang,Wei-Chen Chiu,Ching-Chun Huang*

Main category: cs.CV

TL;DR: DynFaceRestore是一种新颖的盲脸恢复方法，通过动态选择扩散采样起始时间步和局部调整引导强度，平衡了保真度和细节生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在盲脸恢复中因固定扩散采样时间步和全局引导尺度导致保真度与质量不平衡。

Method: 利用模糊图像和高斯核动态选择起始时间步，引入动态引导缩放调节器局部调整引导强度。

Result: 在定量和定性评估中均达到最先进性能，展示了鲁棒性和有效性。

Conclusion: DynFaceRestore通过动态策略有效解决了盲脸恢复中的保真度与质量平衡问题。

Abstract: Blind Face Restoration aims to recover high-fidelity, detail-rich facial images from unknown degraded inputs, presenting significant challenges in preserving both identity and detail. Pre-trained diffusion models have been increasingly used as image priors to generate fine details. Still, existing methods often use fixed diffusion sampling timesteps and a global guidance scale, assuming uniform degradation. This limitation and potentially imperfect degradation kernel estimation frequently lead to under- or over-diffusion, resulting in an imbalance between fidelity and quality. We propose DynFaceRestore, a novel blind face restoration approach that learns to map any blindly degraded input to Gaussian blurry images. By leveraging these blurry images and their respective Gaussian kernels, we dynamically select the starting timesteps for each blurry image and apply closed-form guidance during the diffusion sampling process to maintain fidelity. Additionally, we introduce a dynamic guidance scaling adjuster that modulates the guidance strength across local regions, enhancing detail generation in complex areas while preserving structural fidelity in contours. This strategy effectively balances the trade-off between fidelity and quality. DynFaceRestore achieves state-of-the-art performance in both quantitative and qualitative evaluations, demonstrating robustness and effectiveness in blind face restoration.

</details>


### [23] [GRAM-MAMBA: Holistic Feature Alignment for Wireless Perception with Adaptive Low-Rank Compensation](https://arxiv.org/abs/2507.13803)
*Weiqi Yang,Xu Zhou,Jingfu Guan,Hao Du,Tianyu Bai*

Main category: cs.CV

TL;DR: GRAM-MAMBA框架通过线性复杂度的Mamba模型和优化的GRAM矩阵策略，解决了多模态融合中的效率、对齐和鲁棒性问题，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合系统在资源受限环境中部署困难，模态对齐不充分，且对缺失数据鲁棒性差，需要高效且鲁棒的解决方案。

Method: 结合Mamba模型处理时间序列数据，使用GRAM矩阵优化模态对齐，并引入低秩自适应层补偿缺失模态。

Result: 在SPAWC2021和USC-HAD数据集上表现优异，定位误差更低，活动识别准确率更高，且参数训练量极少。

Conclusion: GRAM-MAMBA在资源受限环境中实现了高效且鲁棒的多模态感知，具有广泛应用潜力。

Abstract: Multi-modal fusion is crucial for Internet of Things (IoT) perception, widely deployed in smart homes, intelligent transport, industrial automation, and healthcare. However, existing systems often face challenges: high model complexity hinders deployment in resource-constrained environments, unidirectional modal alignment neglects inter-modal relationships, and robustness suffers when sensor data is missing. These issues impede efficient and robust multimodal perception in real-world IoT settings. To overcome these limitations, we propose GRAM-MAMBA. This framework utilizes the linear-complexity Mamba model for efficient sensor time-series processing, combined with an optimized GRAM matrix strategy for pairwise alignment among modalities, addressing the shortcomings of traditional single-modality alignment. Inspired by Low-Rank Adaptation (LoRA), we introduce an adaptive low-rank layer compensation strategy to handle missing modalities post-training. This strategy freezes the pre-trained model core and irrelevant adaptive layers, fine-tuning only those related to available modalities and the fusion process. Extensive experiments validate GRAM-MAMBA's effectiveness. On the SPAWC2021 indoor positioning dataset, the pre-trained model shows lower error than baselines; adapting to missing modalities yields a 24.5% performance boost by training less than 0.2% of parameters. On the USC-HAD human activity recognition dataset, it achieves 93.55% F1 and 93.81% Overall Accuracy (OA), outperforming prior work; the update strategy increases F1 by 23% while training less than 0.3% of parameters. These results highlight GRAM-MAMBA's potential for achieving efficient and robust multimodal perception in resource-constrained environments.

</details>


### [24] [A Quantum-assisted Attention U-Net for Building Segmentation over Tunis using Sentinel-1 Data](https://arxiv.org/abs/2507.13852)
*Luigi Russo,Francesco Mauro,Babak Memar,Alessandro Sebastianelli,Silvia Liberata Ullo,Paolo Gamba*

Main category: cs.CV

TL;DR: 该研究探讨了使用量子卷积预处理增强Attention U-Net模型在城市建筑分割中的能力，结果表明该方法在保持精度的同时提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 城市建筑分割在规划、灾害响应等领域至关重要，但高分辨率卫星图像的处理存在挑战。

Method: 采用量子卷积预处理提取SAR图像特征，结合Attention U-Net模型进行分割。

Result: 方法在测试精度上与标准模型相当，同时显著减少了网络参数。

Conclusion: 量子辅助深度学习框架在大规模城市建筑分割中具有潜力。

Abstract: Building segmentation in urban areas is essential in fields such as urban planning, disaster response, and population mapping. Yet accurately segmenting buildings in dense urban regions presents challenges due to the large size and high resolution of satellite images. This study investigates the use of a Quanvolutional pre-processing to enhance the capability of the Attention U-Net model in the building segmentation. Specifically, this paper focuses on the urban landscape of Tunis, utilizing Sentinel-1 Synthetic Aperture Radar (SAR) imagery. In this work, Quanvolution was used to extract more informative feature maps that capture essential structural details in radar imagery, proving beneficial for accurate building segmentation. Preliminary results indicate that proposed methodology achieves comparable test accuracy to the standard Attention U-Net model while significantly reducing network parameters. This result aligns with findings from previous works, confirming that Quanvolution not only maintains model accuracy but also increases computational efficiency. These promising outcomes highlight the potential of quantum-assisted Deep Learning frameworks for large-scale building segmentation in urban environments.

</details>


### [25] [PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations](https://arxiv.org/abs/2507.13891)
*Yu Wei,Jiahui Zhang,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: PCR-GS是一种无需COLMAP的3D高斯泼溅技术，通过相机姿态共正则化解决了复杂相机轨迹下的场景建模问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂相机轨迹时表现不佳，导致相机姿态估计和3D场景建模的联合优化陷入局部最小值。

Method: PCR-GS通过特征重投影正则化和基于小波的高频正则化，优化相机姿态估计。

Result: 实验表明，PCR-GS在剧烈变化的相机轨迹下仍能实现高质量的3D场景建模。

Conclusion: PCR-GS通过双重正则化机制显著提升了复杂场景下的3D建模性能。

Abstract: COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing attention due to its remarkable performance in reconstructing high-quality 3D scenes from unposed images or videos. However, it often struggles to handle scenes with complex camera trajectories as featured by drastic rotation and translation across adjacent camera views, leading to degraded estimation of camera poses and further local minima in joint optimization of camera poses and 3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that achieves superior 3D scene modeling and camera pose estimation via camera pose co-regularization. PCR-GS achieves regularization from two perspectives. The first is feature reprojection regularization which extracts view-robust DINO features from adjacent camera views and aligns their semantic information for camera pose regularization. The second is wavelet-based frequency regularization which exploits discrepancy in high-frequency details to further optimize the rotation matrix in camera poses. Extensive experiments over multiple real-world scenes show that the proposed PCR-GS achieves superior pose-free 3D-GS scene modeling under dramatic changes of camera trajectories.

</details>


### [26] [TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views](https://arxiv.org/abs/2507.13929)
*Hsiang-Hui Hung,Huu-Phu Do,Yung-Hui Li,Ching-Chun Huang*

Main category: cs.CV

TL;DR: TimeNeRF是一种通用的神经渲染方法，能够在任意视角和时间渲染新视图，即使输入视图较少。它结合多视角立体视觉、神经辐射场和解缠策略，实现少样本泛化能力，并能构建任意时间的神经辐射场。实验表明，TimeNeRF无需逐场景优化即可生成新视图，并能平滑过渡不同时间的场景变化。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，多视图采集成本高且逐场景优化效率低。随着元宇宙对沉浸式体验的需求增加，建模自然过渡昼夜的3D场景变得至关重要。现有NeRF技术在时间3D场景建模方面潜力未充分探索，缺乏专用数据集。

Method: 结合多视角立体视觉、神经辐射场和解缠策略，构建隐式内容辐射场表示场景，并支持任意时间的神经辐射场构建。通过体渲染合成新视图。

Result: TimeNeRF在少样本设置下无需逐场景优化即可渲染新视图，并能平滑过渡不同时间的场景变化，如从黎明到黄昏的自然场景变化。

Conclusion: TimeNeRF展示了在少样本设置下生成任意时间新视图的能力，为时间动态3D场景建模提供了有效解决方案。

Abstract: We present TimeNeRF, a generalizable neural rendering approach for rendering novel views at arbitrary viewpoints and at arbitrary times, even with few input views. For real-world applications, it is expensive to collect multiple views and inefficient to re-optimize for unseen scenes. Moreover, as the digital realm, particularly the metaverse, strives for increasingly immersive experiences, the ability to model 3D environments that naturally transition between day and night becomes paramount. While current techniques based on Neural Radiance Fields (NeRF) have shown remarkable proficiency in synthesizing novel views, the exploration of NeRF's potential for temporal 3D scene modeling remains limited, with no dedicated datasets available for this purpose. To this end, our approach harnesses the strengths of multi-view stereo, neural radiance fields, and disentanglement strategies across diverse datasets. This equips our model with the capability for generalizability in a few-shot setting, allows us to construct an implicit content radiance field for scene representation, and further enables the building of neural radiance fields at any arbitrary time. Finally, we synthesize novel views of that time via volume rendering. Experiments show that TimeNeRF can render novel views in a few-shot setting without per-scene optimization. Most notably, it excels in creating realistic novel views that transition smoothly across different times, adeptly capturing intricate natural scene changes from dawn to dusk.

</details>


### [27] [DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization](https://arxiv.org/abs/2507.13934)
*Marzieh Gheisari,Auguste Genovesio*

Main category: cs.CV

TL;DR: DiViD是一种端到端视频扩散框架，用于显式分离静态外观和动态运动，通过全局静态令牌和帧级动态令牌实现，并引入多种归纳偏置和正则化方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于VAE和GAN的方法在视频中分离静态和动态信息时存在信息泄漏和模糊重建问题，DiViD旨在解决这些问题。

Method: DiViD使用序列编码器提取全局静态令牌和帧级动态令牌，并采用条件DDPM解码器，结合共享噪声计划、时变KL瓶颈和交叉注意力等机制。

Result: DiViD在真实基准测试中表现优于现有方法，实现了最高的交换联合准确率，同时保持静态保真度和动态传递能力。

Conclusion: DiViD通过显式分离静态和动态信息，显著减少了信息泄漏，为视频分解任务提供了新思路。

Abstract: Unsupervised disentanglement of static appearance and dynamic motion in video remains a fundamental challenge, often hindered by information leakage and blurry reconstructions in existing VAE- and GAN-based approaches. We introduce DiViD, the first end-to-end video diffusion framework for explicit static-dynamic factorization. DiViD's sequence encoder extracts a global static token from the first frame and per-frame dynamic tokens, explicitly removing static content from the motion code. Its conditional DDPM decoder incorporates three key inductive biases: a shared-noise schedule for temporal consistency, a time-varying KL-based bottleneck that tightens at early timesteps (compressing static information) and relaxes later (enriching dynamics), and cross-attention that routes the global static token to all frames while keeping dynamic tokens frame-specific. An orthogonality regularizer further prevents residual static-dynamic leakage. We evaluate DiViD on real-world benchmarks using swap-based accuracy and cross-leakage metrics. DiViD outperforms state-of-the-art sequential disentanglement methods: it achieves the highest swap-based joint accuracy, preserves static fidelity while improving dynamic transfer, and reduces average cross-leakage.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [28] [Converting T1-weighted MRI from 3T to 7T quality using deep learning](https://arxiv.org/abs/2507.13782)
*Malo Gicquel,Ruoyi Zhao,Anika Wuestefeld,Nicola Spotorno,Olof Strandberg,Kalle Åström,Yu Xiao,Laura EM Wisse,Danielle van Westen,Rik Ossenkoppele,Niklas Mattsson-Carlgren,David Berron,Oskar Hansson,Gabrielle Flood,Jacob Vogel*

Main category: eess.IV

TL;DR: 提出了一种深度学习模型，用于从3T MRI合成7T MRI，模型表现优于现有方法，合成图像在细节和视觉质量上接近真实7T图像。


<details>
  <summary>Details</summary>
Motivation: 7T MRI提供更高的分辨率和组织对比度，但普及性较低，因此希望通过3T MRI合成7T图像以提升图像质量。

Method: 使用U-Net和GAN U-Net模型，基于172名参与者的配对3T和7T T1加权图像进行训练。

Result: 合成7T图像在细节和视觉质量上优于真实7T图像，且自动分割结果更接近手动分割。

Conclusion: 合成7T图像可提升图像质量和分割效果，且不影响下游任务性能，具有潜在临床应用价值。

Abstract: Ultra-high resolution 7 tesla (7T) magnetic resonance imaging (MRI) provides detailed anatomical views, offering better signal-to-noise ratio, resolution and tissue contrast than 3T MRI, though at the cost of accessibility. We present an advanced deep learning model for synthesizing 7T brain MRI from 3T brain MRI. Paired 7T and 3T T1-weighted images were acquired from 172 participants (124 cognitively unimpaired, 48 impaired) from the Swedish BioFINDER-2 study. To synthesize 7T MRI from 3T images, we trained two models: a specialized U-Net, and a U-Net integrated with a generative adversarial network (GAN U-Net). Our models outperformed two additional state-of-the-art 3T-to-7T models in image-based evaluation metrics. Four blinded MRI professionals judged our synthetic 7T images as comparable in detail to real 7T images, and superior in subjective visual quality to 7T images, apparently due to the reduction of artifacts. Importantly, automated segmentations of the amygdalae of synthetic GAN U-Net 7T images were more similar to manually segmented amygdalae (n=20), than automated segmentations from the 3T images that were used to synthesize the 7T images. Finally, synthetic 7T images showed similar performance to real 3T images in downstream prediction of cognitive status using MRI derivatives (n=3,168). In all, we show that synthetic T1-weighted brain images approaching 7T quality can be generated from 3T images, which may improve image quality and segmentation, without compromising performance in downstream tasks. Future directions, possible clinical use cases, and limitations are discussed.

</details>


### [29] [Blind Super Resolution with Reference Images and Implicit Degradation Representation](https://arxiv.org/abs/2507.13915)
*Huu-Phu Do,Po-Chih Hu,Hao-Chien Hsueh,Che-Kai Liu,Vu-Hoang Tran,Ching-Chun Huang*

Main category: eess.IV

TL;DR: 本文提出了一种基于参考图像的盲超分辨率方法，通过利用高分辨率参考图像生成尺度感知的退化核，显著提升了超分辨率性能。


<details>
  <summary>Details</summary>
Motivation: 传统的盲超分辨率方法直接估计退化核，但忽略了不同超分辨率尺度下退化核的差异。本文旨在通过引入高分辨率参考图像，解决这一问题。

Method: 利用内容无关的高分辨率参考图像与目标低分辨率图像，自适应地识别退化过程，并生成额外的低-高分辨率对以提升性能。

Result: 该方法在训练好的盲超分辨率模型和零样本盲超分辨率方法中均优于先前方法。

Conclusion: 结合模糊核和尺度因子，并利用参考图像，显著提升了盲超分辨率任务的效果。

Abstract: Previous studies in blind super-resolution (BSR) have primarily concentrated on estimating degradation kernels directly from low-resolution (LR) inputs to enhance super-resolution. However, these degradation kernels, which model the transition from a high-resolution (HR) image to its LR version, should account for not only the degradation process but also the downscaling factor. Applying the same degradation kernel across varying super-resolution scales may be impractical. Our research acknowledges degradation kernels and scaling factors as pivotal elements for the BSR task and introduces a novel strategy that utilizes HR images as references to establish scale-aware degradation kernels. By employing content-irrelevant HR reference images alongside the target LR image, our model adaptively discerns the degradation process. It is then applied to generate additional LR-HR pairs through down-sampling the HR reference images, which are keys to improving the SR performance. Our reference-based training procedure is applicable to proficiently trained blind SR models and zero-shot blind SR methods, consistently outperforming previous methods in both scenarios. This dual consideration of blur kernels and scaling factors, coupled with the use of a reference image, contributes to the effectiveness of our approach in blind super-resolution tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [30] [Generalist Bimanual Manipulation via Foundation Video Diffusion Models](https://arxiv.org/abs/2507.12898)
*Yao Feng,Hengkai Tan,Xinyi Mao,Guodong Liu,Shuhe Huang,Chendong Xiang,Hang Su,Jun Zhu*

Main category: cs.LG

TL;DR: VIDAR是一个两阶段框架，通过大规模视频预训练和掩码逆动力学模型解决双手机器人操作中的数据稀缺和异构性问题。


<details>
  <summary>Details</summary>
Motivation: 双手机器人操作中数据稀缺和异构性限制了其扩展能力，需要一种更高效的方法。

Method: 结合扩散模型视频预训练和掩码逆动力学模型，利用多视角视频数据和无像素标签的动作预测。

Result: 仅需20分钟人类演示，VIDAR在未见过的任务和背景中表现优异，超越现有方法。

Conclusion: 视频基础模型与掩码动作预测结合，有望实现可扩展和通用的机器人操作。

Abstract: Bimanual robotic manipulation, which involves the coordinated control of two robotic arms, is foundational for solving challenging tasks. Despite recent progress in general-purpose manipulation, data scarcity and embodiment heterogeneity remain serious obstacles to further scaling up in bimanual settings. In this paper, we introduce VIdeo Diffusion for Action Reasoning (VIDAR), a two-stage framework that leverages large-scale, diffusion-based video pre-training and a novel masked inverse dynamics model for action prediction. We pre-train the video diffusion model on 750K multi-view videos from three real-world bimanual robot platforms, utilizing a unified observation space that encodes robot, camera, task, and scene contexts. Our masked inverse dynamics model learns masks to extract action-relevant information from generated trajectories without requiring pixel-level labels, and the masks can effectively generalize to unseen backgrounds. Our experiments demonstrate that with only 20 minutes of human demonstrations on an unseen robot platform (only 1% of typical data requirements), VIDAR generalizes to unseen tasks and backgrounds with strong semantic understanding, surpassing state-of-the-art methods. Our findings highlight the potential of video foundation models, coupled with masked action prediction, to enable scalable and generalizable robotic manipulation in diverse real-world settings.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [31] [Leveraging the Spatial Hierarchy: Coarse-to-fine Trajectory Generation via Cascaded Hybrid Diffusion](https://arxiv.org/abs/2507.13366)
*Baoshen Guo,Zhiqing Hong,Junyi Li,Shenhao Wang,Jinhua Zhao*

Main category: cs.SI

TL;DR: 提出了一种名为Cardiff的分层扩散框架，用于生成细粒度且隐私保护的轨迹数据，解决了现有方法在高维复杂分布和隐私保护方面的不足。


<details>
  <summary>Details</summary>
Motivation: 由于隐私问题和数据收集成本，细粒度轨迹数据难以大规模公开。现有轨迹合成方法无法处理高维复杂分布，生成不真实。

Method: Cardiff采用分层生成方法：先通过扩散变换器合成离散路段级轨迹，再基于条件去噪网络生成细粒度GPS级轨迹。

Result: 在三个真实数据集上，Cardiff在多项指标上优于现有方法。

Conclusion: Cardiff通过分层生成和隐私保护机制，实现了高保真轨迹合成，平衡了隐私与实用性。

Abstract: Urban mobility data has significant connections with economic growth and plays an essential role in various smart-city applications. However, due to privacy concerns and substantial data collection costs, fine-grained human mobility trajectories are difficult to become publicly available on a large scale. A promising solution to address this issue is trajectory synthesizing. However, existing works often ignore the inherent structural complexity of trajectories, unable to handle complicated high-dimensional distributions and generate realistic fine-grained trajectories. In this paper, we propose Cardiff, a coarse-to-fine Cascaded hybrid diffusion-based trajectory synthesizing framework for fine-grained and privacy-preserving mobility generation. By leveraging the hierarchical nature of urban mobility, Cardiff decomposes the generation process into two distinct levels, i.e., discrete road segment-level and continuous fine-grained GPS-level: (i) In the segment-level, to reduce computational costs and redundancy in raw trajectories, we first encode the discrete road segments into low-dimensional latent embeddings and design a diffusion transformer-based latent denoising network for segment-level trajectory synthesis. (ii) Taking the first stage of generation as conditions, we then design a fine-grained GPS-level conditional denoising network with a noise augmentation mechanism to achieve robust and high-fidelity generation. Additionally, the Cardiff framework not only progressively generates high-fidelity trajectories through cascaded denoising but also flexibly enables a tunable balance between privacy preservation and utility. Experimental results on three large real-world trajectory datasets demonstrate that our method outperforms state-of-the-art baselines in various metrics.

</details>
