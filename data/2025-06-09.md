<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 6]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Can ChatGPT Perform Image Splicing Detection? A Preliminary Study](https://arxiv.org/abs/2506.05358)
*Souradip Nath*

Main category: cs.CV

TL;DR: GPT-4V在图像取证领域展示了强大的零样本能力，尤其在检测图像拼接方面表现优异，准确率超过85%。


<details>
  <summary>Details</summary>
Motivation: 研究GPT-4V在图像取证中的零样本能力，探索其在检测图像拼接方面的潜力。

Method: 使用三种提示策略（零样本、少样本和思维链）在CASIA v2.0数据集上评估GPT-4V。

Result: GPT-4V在零样本设置下表现优异，思维链提示在真实和拼接图像间取得最佳平衡。

Conclusion: GPT-4V虽不及专用模型，但其通用性和推理能力使其成为图像取证中的灵活工具。

Abstract: Multimodal Large Language Models (MLLMs) like GPT-4V are capable of reasoning
across text and image modalities, showing promise in a variety of complex
vision-language tasks. In this preliminary study, we investigate the
out-of-the-box capabilities of GPT-4V in the domain of image forensics,
specifically, in detecting image splicing manipulations. Without any
task-specific fine-tuning, we evaluate GPT-4V using three prompting strategies:
Zero-Shot (ZS), Few-Shot (FS), and Chain-of-Thought (CoT), applied over a
curated subset of the CASIA v2.0 splicing dataset.
  Our results show that GPT-4V achieves competitive detection performance in
zero-shot settings (more than 85% accuracy), with CoT prompting yielding the
most balanced trade-off across authentic and spliced images. Qualitative
analysis further reveals that the model not only detects low-level visual
artifacts but also draws upon real-world contextual knowledge such as object
scale, semantic consistency, and architectural facts, to identify implausible
composites. While GPT-4V lags behind specialized state-of-the-art splicing
detection models, its generalizability, interpretability, and encyclopedic
reasoning highlight its potential as a flexible tool in image forensics.

</details>


### [2] [CarboNeXT and CarboFormer: Dual Semantic Segmentation Architectures for Detecting and Quantifying Carbon Dioxide Emissions Using Optical Gas Imaging](https://arxiv.org/abs/2506.05360)
*Taminul Islam,Toqi Tahamid Sarker,Mohamed G Embaby,Khaled R Ahmed,Amer AbuGhazaleh*

Main category: cs.CV

TL;DR: CarboNeXT是一个用于光学气体成像的语义分割框架，用于检测和量化CO₂排放，在低流量场景下表现优异，并支持实时监测。


<details>
  <summary>Details</summary>
Motivation: CO₂排放是环境和工业过程的重要指标，特别是在畜牧业管理中，需要高效的工具进行监测和分析。

Method: 结合多尺度上下文聚合网络、UPerHead和辅助FCN组件，提出CarboNeXT框架，并贡献了两个新数据集（CCR和RTA）。

Result: CarboNeXT在CCR和RTA数据集上分别达到88.46%和92.95%的mIoU，实时性能为60.95 FPS；轻量版CarboFormer在资源受限平台上表现优异。

Conclusion: 该研究为CO₂排放分析提供了高效工具，特别适用于畜牧业和环境监测，推动了精准畜牧业的发展。

Abstract: Carbon dioxide (CO$_2$) emissions are critical indicators of both
environmental impact and various industrial processes, including livestock
management. We introduce CarboNeXT, a semantic segmentation framework for
Optical Gas Imaging (OGI), designed to detect and quantify CO$_2$ emissions
across diverse applications. Our approach integrates a multi-scale context
aggregation network with UPerHead and auxiliary FCN components to effectively
model both local details and global relationships in gas plume imagery. We
contribute two novel datasets: (1) the Controlled Carbon Dioxide Release (CCR)
dataset, which simulates gas leaks with systematically varied flow rates
(10-100 SCCM), and (2) the Real Time Ankom (RTA) dataset, focusing on emissions
from dairy cow rumen fluid in vitro experiments. Extensive evaluations
demonstrate that CarboNeXT outperforms state-of-the-art methods, achieving
88.46% mIoU on CCR and 92.95% mIoU on RTA, with particular effectiveness in
challenging low-flow scenarios. The model operates at 60.95 FPS, enabling
real-time monitoring applications. Additionally, we propose CarboFormer, a
lightweight variant with only 5.07M parameters that achieves 84.68 FPS, with
competitive performance of 84.88% mIoU on CCR and 92.98% on RTA, making it
suitable for resource-constrained platforms such as programmable drones. Our
work advances both environmental sensing and precision livestock management by
providing robust tools for CO$_2$ emission analysis, with a specific focus on
livestock applications.

</details>


### [3] [Scalable Generation of Spatial Transcriptomics from Histology Images via Whole-Slide Flow Matching](https://arxiv.org/abs/2506.05361)
*Tinglin Huang,Tianyu Liu,Mehrtash Babadi,Wengong Jin,Rex Ying*

Main category: cs.CV

TL;DR: STFlow提出了一种基于流匹配的生成模型，通过建模整个切片的基因表达联合分布来考虑细胞间相互作用，解决了现有方法在内存和独立性预测上的限制。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学（ST）技术虽强大，但受限于低通量和实验设施需求。现有预测方法因独立预测和内存问题表现不佳。

Method: STFlow采用流匹配生成模型，建模整个切片的基因表达联合分布，并引入高效的切片级编码器与局部空间注意力机制。

Result: 在HEST-1k和STImage-1K4M基准测试中，STFlow显著优于现有方法，相对病理基础模型提升超过18%。

Conclusion: STFlow通过建模细胞间相互作用和优化内存使用，显著提升了空间转录组学预测的性能。

Abstract: Spatial transcriptomics (ST) has emerged as a powerful technology for
bridging histology imaging with gene expression profiling. However, its
application has been limited by low throughput and the need for specialized
experimental facilities. Prior works sought to predict ST from whole-slide
histology images to accelerate this process, but they suffer from two major
limitations. First, they do not explicitly model cell-cell interaction as they
factorize the joint distribution of whole-slide ST data and predict the gene
expression of each spot independently. Second, their encoders struggle with
memory constraints due to the large number of spots (often exceeding 10,000) in
typical ST datasets. Herein, we propose STFlow, a flow matching generative
model that considers cell-cell interaction by modeling the joint distribution
of gene expression of an entire slide. It also employs an efficient slide-level
encoder with local spatial attention, enabling whole-slide processing without
excessive memory overhead. On the recently curated HEST-1k and STImage-1K4M
benchmarks, STFlow substantially outperforms state-of-the-art baselines and
achieves over 18% relative improvements over the pathology foundation models.

</details>


### [4] [Can ChatGPT Perform Image Splicing Detection? A Preliminary Study](https://arxiv.org/abs/2506.05358)
*Souradip Nath*

Main category: cs.CV

TL;DR: GPT-4V在图像取证领域（特别是图像拼接检测）中的零样本能力表现优异，准确率超过85%，且链式思维（CoT）提示策略效果最佳。


<details>
  <summary>Details</summary>
Motivation: 研究GPT-4V在图像取证任务中的零样本能力，探索其在无需微调的情况下检测图像拼接的潜力。

Method: 使用三种提示策略（零样本、少样本、链式思维）在CASIA v2.0数据集的子集上评估GPT-4V。

Result: GPT-4V在零样本设置下表现优异，CoT提示策略在真实与拼接图像间取得最佳平衡。

Conclusion: 尽管不及专用模型，GPT-4V的通用性、可解释性和百科全书式推理能力使其成为图像取证中的灵活工具。

Abstract: Multimodal Large Language Models (MLLMs) like GPT-4V are capable of reasoning
across text and image modalities, showing promise in a variety of complex
vision-language tasks. In this preliminary study, we investigate the
out-of-the-box capabilities of GPT-4V in the domain of image forensics,
specifically, in detecting image splicing manipulations. Without any
task-specific fine-tuning, we evaluate GPT-4V using three prompting strategies:
Zero-Shot (ZS), Few-Shot (FS), and Chain-of-Thought (CoT), applied over a
curated subset of the CASIA v2.0 splicing dataset.
  Our results show that GPT-4V achieves competitive detection performance in
zero-shot settings (more than 85% accuracy), with CoT prompting yielding the
most balanced trade-off across authentic and spliced images. Qualitative
analysis further reveals that the model not only detects low-level visual
artifacts but also draws upon real-world contextual knowledge such as object
scale, semantic consistency, and architectural facts, to identify implausible
composites. While GPT-4V lags behind specialized state-of-the-art splicing
detection models, its generalizability, interpretability, and encyclopedic
reasoning highlight its potential as a flexible tool in image forensics.

</details>


### [5] [CarboNeXT and CarboFormer: Dual Semantic Segmentation Architectures for Detecting and Quantifying Carbon Dioxide Emissions Using Optical Gas Imaging](https://arxiv.org/abs/2506.05360)
*Taminul Islam,Toqi Tahamid Sarker,Mohamed G Embaby,Khaled R Ahmed,Amer AbuGhazaleh*

Main category: cs.CV

TL;DR: CarboNeXT是一个用于光学气体成像（OGI）的语义分割框架，用于检测和量化CO$_2$排放，在低流量场景下表现优异，并支持实时监测。


<details>
  <summary>Details</summary>
Motivation: CO$_2$排放是环境和工业过程的重要指标，尤其是在畜牧业管理中。需要一种高效的方法来检测和量化这些排放。

Method: 结合多尺度上下文聚合网络、UPerHead和辅助FCN组件，构建CarboNeXT框架，并开发了两个新数据集（CCR和RTA）。

Result: CarboNeXT在CCR和RTA数据集上分别达到88.46%和92.95%的mIoU，支持60.95 FPS的实时监测。轻量级变体CarboFormer表现同样出色。

Conclusion: CarboNeXT和CarboFormer为CO$_2$排放分析提供了高效工具，特别适用于畜牧业和环境监测。

Abstract: Carbon dioxide (CO$_2$) emissions are critical indicators of both
environmental impact and various industrial processes, including livestock
management. We introduce CarboNeXT, a semantic segmentation framework for
Optical Gas Imaging (OGI), designed to detect and quantify CO$_2$ emissions
across diverse applications. Our approach integrates a multi-scale context
aggregation network with UPerHead and auxiliary FCN components to effectively
model both local details and global relationships in gas plume imagery. We
contribute two novel datasets: (1) the Controlled Carbon Dioxide Release (CCR)
dataset, which simulates gas leaks with systematically varied flow rates
(10-100 SCCM), and (2) the Real Time Ankom (RTA) dataset, focusing on emissions
from dairy cow rumen fluid in vitro experiments. Extensive evaluations
demonstrate that CarboNeXT outperforms state-of-the-art methods, achieving
88.46% mIoU on CCR and 92.95% mIoU on RTA, with particular effectiveness in
challenging low-flow scenarios. The model operates at 60.95 FPS, enabling
real-time monitoring applications. Additionally, we propose CarboFormer, a
lightweight variant with only 5.07M parameters that achieves 84.68 FPS, with
competitive performance of 84.88% mIoU on CCR and 92.98% on RTA, making it
suitable for resource-constrained platforms such as programmable drones. Our
work advances both environmental sensing and precision livestock management by
providing robust tools for CO$_2$ emission analysis, with a specific focus on
livestock applications.

</details>


### [6] [Scalable Generation of Spatial Transcriptomics from Histology Images via Whole-Slide Flow Matching](https://arxiv.org/abs/2506.05361)
*Tinglin Huang,Tianyu Liu,Mehrtash Babadi,Wengong Jin,Rex Ying*

Main category: cs.CV

TL;DR: STFlow是一种基于流匹配的生成模型，通过建模整个切片的基因表达联合分布来考虑细胞间相互作用，解决了现有方法在内存和独立性预测上的限制。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学（ST）技术结合组织学成像和基因表达分析，但受限于低通量和实验设施需求。现有预测方法未能有效建模细胞间相互作用且内存效率低。

Method: 提出STFlow，采用流匹配生成模型建模切片级基因表达联合分布，并结合局部空间注意力的高效编码器。

Result: 在HEST-1k和STImage-1K4M基准测试中，STFlow显著优于现有方法，相对病理学基础模型提升18%。

Conclusion: STFlow通过建模细胞间相互作用和高效编码，显著提升了空间转录组学预测的性能和实用性。

Abstract: Spatial transcriptomics (ST) has emerged as a powerful technology for
bridging histology imaging with gene expression profiling. However, its
application has been limited by low throughput and the need for specialized
experimental facilities. Prior works sought to predict ST from whole-slide
histology images to accelerate this process, but they suffer from two major
limitations. First, they do not explicitly model cell-cell interaction as they
factorize the joint distribution of whole-slide ST data and predict the gene
expression of each spot independently. Second, their encoders struggle with
memory constraints due to the large number of spots (often exceeding 10,000) in
typical ST datasets. Herein, we propose STFlow, a flow matching generative
model that considers cell-cell interaction by modeling the joint distribution
of gene expression of an entire slide. It also employs an efficient slide-level
encoder with local spatial attention, enabling whole-slide processing without
excessive memory overhead. On the recently curated HEST-1k and STImage-1K4M
benchmarks, STFlow substantially outperforms state-of-the-art baselines and
achieves over 18% relative improvements over the pathology foundation models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [Positional Encoding meets Persistent Homology on Graphs](https://arxiv.org/abs/2506.05814)
*Yogesh Verma,Amauri H. Souza,Vikas Garg*

Main category: cs.LG

TL;DR: 论文提出了一种结合位置编码（PE）和持久同调（PH）的新方法PiPE，解决了GNN在结构信息利用上的局限性，并在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 消息传递图神经网络（GNNs）的局部归纳偏差限制了其对关键结构信息（如连通性和循环）的利用，PE和PH是两种潜在的解决方案，但缺乏理论比较。

Method: 通过理论分析比较PE和PH的表达能力，提出了一种新的可学习方法PiPE，结合了两者的优势。

Result: PiPE在表达能力上优于PE和PH，并在分子属性预测、图分类和分布外泛化等任务中表现优异。

Conclusion: PiPE为图表示学习提供了更强大的工具，推动了该领域的发展。

Abstract: The local inductive bias of message-passing graph neural networks (GNNs)
hampers their ability to exploit key structural information (e.g., connectivity
and cycles). Positional encoding (PE) and Persistent Homology (PH) have emerged
as two promising approaches to mitigate this issue. PE schemes endow GNNs with
location-aware features, while PH methods enhance GNNs with multiresolution
topological features. However, a rigorous theoretical characterization of the
relative merits and shortcomings of PE and PH has remained elusive. We bridge
this gap by establishing that neither paradigm is more expressive than the
other, providing novel constructions where one approach fails but the other
succeeds. Our insights inform the design of a novel learnable method, PiPE
(Persistence-informed Positional Encoding), which is provably more expressive
than both PH and PE. PiPE demonstrates strong performance across a variety of
tasks (e.g., molecule property prediction, graph classification, and
out-of-distribution generalization), thereby advancing the frontiers of graph
representation learning. Code is available at
https://github.com/Aalto-QuML/PIPE.

</details>


### [8] [RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory](https://arxiv.org/abs/2506.05994)
*Yi-Chun Liao,Chieh-Lin Tsai,Yuan-Hao Chang,Camélia Slimani,Jalil Boukhobza,Tei-Wei Kuo*

Main category: cs.LG

TL;DR: RETENTION框架通过迭代剪枝和树映射方案，显著降低树模型推理中CAM的容量需求，同时保持精度。


<details>
  <summary>Details</summary>
Motivation: 树模型在结构化数据上表现优异，但传统加速器难以高效支持，现有CAM设计存在内存消耗高和利用率低的问题。

Method: 提出迭代剪枝算法和树映射方案，优化CAM使用并减少内存冗余。

Result: 实验显示，RETENTION框架将空间效率提升4.35×至207.12×，精度损失低于3%。

Conclusion: RETENTION为树模型加速提供了高效资源利用的解决方案。

Abstract: Although deep learning has demonstrated remarkable capabilities in learning
from unstructured data, modern tree-based ensemble models remain superior in
extracting relevant information and learning from structured datasets. While
several efforts have been made to accelerate tree-based models, the inherent
characteristics of the models pose significant challenges for conventional
accelerators. Recent research leveraging content-addressable memory (CAM)
offers a promising solution for accelerating tree-based models, yet existing
designs suffer from excessive memory consumption and low utilization. This work
addresses these challenges by introducing RETENTION, an end-to-end framework
that significantly reduces CAM capacity requirement for tree-based model
inference. We propose an iterative pruning algorithm with a novel pruning
criterion tailored for bagging-based models (e.g., Random Forest), which
minimizes model complexity while ensuring controlled accuracy degradation.
Additionally, we present a tree mapping scheme that incorporates two innovative
data placement strategies to alleviate the memory redundancy caused by the
widespread use of don't care states in CAM. Experimental results show that
implementing the tree mapping scheme alone achieves $1.46\times$ to $21.30
\times$ better space efficiency, while the full RETENTION framework yields
$4.35\times$ to $207.12\times$ improvement with less than 3% accuracy loss.
These results demonstrate that RETENTION is highly effective in reducing CAM
capacity requirement, providing a resource-efficient direction for tree-based
model acceleration.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [9] [Towards Autonomous In-situ Soil Sampling and Mapping in Large-Scale Agricultural Environments](https://arxiv.org/abs/2506.05653)
*Thien Hoang Nguyen,Erik Muller,Michael Rubin,Xiaofei Wang,Fiorella Sibona,Salah Sukkarieh*

Main category: cs.RO

TL;DR: 提出了一种机器人系统，用于实时土壤采样、分析和绘图，以解决传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统土壤采样方法耗时、费力且空间分辨率低，不适合大规模精准农业。

Method: 系统包括土壤采样子系统（SAS）和实时分析实验室（Lab），通过实地试验验证性能。

Result: SAS能稳定采集200mm深、50g重的土壤样本，Lab能在10分钟内分析pH和宏量营养素。

Conclusion: 该系统能为农民提供及时数据，支持高效、可持续的土壤管理和施肥。

Abstract: Traditional soil sampling and analysis methods are labor-intensive,
time-consuming, and limited in spatial resolution, making them unsuitable for
large-scale precision agriculture. To address these limitations, we present a
robotic solution for real-time sampling, analysis and mapping of key soil
properties. Our system consists of two main sub-systems: a Sample Acquisition
System (SAS) for precise, automated in-field soil sampling; and a Sample
Analysis Lab (Lab) for real-time soil property analysis. The system's
performance was validated through extensive field trials at a large-scale
Australian farm. Experimental results show that the SAS can consistently
acquire soil samples with a mass of 50g at a depth of 200mm, while the Lab can
process each sample within 10 minutes to accurately measure pH and
macronutrients. These results demonstrate the potential of the system to
provide farmers with timely, data-driven insights for more efficient and
sustainable soil management and fertilizer application.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [10] [Preprocessing Methods for Memristive Reservoir Computing for Image Recognition](https://arxiv.org/abs/2506.05588)
*Rishona Daniels,Duna Wattad,Ronny Ronen,David Saad,Shahar Kvatinsky*

Main category: cs.NE

TL;DR: 本文系统比较了忆阻器储层计算（RC）系统的预处理方法，提出了一种基于奇偶校验的预处理方法，提高了准确性，并强调了预处理策略对系统效率和可扩展性的重要性。


<details>
  <summary>Details</summary>
Motivation: 忆阻器储层计算系统因其简化训练和动态特性受到关注，但高性能的实现仍受限于预处理方法和储层规模，缺乏对这些因素影响的全面评估。

Method: 系统比较了多种预处理方法对忆阻RC系统的影响，并提出了一种基于奇偶校验的预处理方法。

Result: 提出的预处理方法将准确性提高了2-6%，同时设备数量仅适度增加。

Conclusion: 研究强调了预处理策略对提升忆阻RC系统效率和可扩展性的重要性。

Abstract: Reservoir computing (RC) has attracted attention as an efficient recurrent
neural network architecture due to its simplified training, requiring only its
last perceptron readout layer to be trained. When implemented with memristors,
RC systems benefit from their dynamic properties, which make them ideal for
reservoir construction. However, achieving high performance in memristor-based
RC remains challenging, as it critically depends on the input preprocessing
method and reservoir size. Despite growing interest, a comprehensive evaluation
that quantifies the impact of these factors is still lacking. This paper
systematically compares various preprocessing methods for memristive RC
systems, assessing their effects on accuracy and energy consumption. We also
propose a parity-based preprocessing method that improves accuracy by 2-6%
while requiring only a modest increase in device count compared to other
methods. Our findings highlight the importance of informed preprocessing
strategies to improve the efficiency and scalability of memristive RC systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [Contextual Memory Intelligence -- A Foundational Paradigm for Human-AI Collaboration and Reflective Generative AI Systems](https://arxiv.org/abs/2506.05370)
*Kristy Wedel*

Main category: cs.AI

TL;DR: 论文提出了一种名为上下文记忆智能（CMI）的新范式，旨在解决生成式AI系统中的记忆限制问题，通过结构化捕捉和推理上下文，提升系统的长期一致性、可解释性和决策能力。


<details>
  <summary>Details</summary>
Motivation: 生成式AI系统在组织中广泛应用，但其记忆组件（如RAG、向量存储等）仍存在局限性，导致重复错误和决策不透明。CMI旨在解决这一问题。

Method: CMI将记忆重新定位为自适应基础设施，结合认知科学、组织理论和AI治理，提出Insight Layer架构，通过人机协作、漂移检测和理由保存实现上下文记忆。

Result: CMI能够使系统基于数据、历史、判断和动态上下文进行推理，填补当前AI架构的盲点，提升系统效能和社会责任。

Conclusion: CMI为构建高效、可审计且社会责任的智能系统提供了框架，增强人机协作和生成式AI设计的韧性。

Abstract: A critical challenge remains unresolved as generative AI systems are quickly
implemented in various organizational settings. Despite significant advances in
memory components such as RAG, vector stores, and LLM agents, these systems
still have substantial memory limitations. Gen AI workflows rarely store or
reflect on the full context in which decisions are made. This leads to repeated
errors and a general lack of clarity. This paper introduces Contextual Memory
Intelligence (CMI) as a new foundational paradigm for building intelligent
systems. It repositions memory as an adaptive infrastructure necessary for
longitudinal coherence, explainability, and responsible decision-making rather
than passive data. Drawing on cognitive science, organizational theory,
human-computer interaction, and AI governance, CMI formalizes the structured
capture, inference, and regeneration of context as a fundamental system
capability. The Insight Layer is presented in this paper to operationalize this
vision. This modular architecture uses human-in-the-loop reflection, drift
detection, and rationale preservation to incorporate contextual memory into
systems. The paper argues that CMI allows systems to reason with data, history,
judgment, and changing context, thereby addressing a foundational blind spot in
current AI architectures and governance efforts. A framework for creating
intelligent systems that are effective, reflective, auditable, and socially
responsible is presented through CMI. This enhances human-AI collaboration,
generative AI design, and the resilience of the institutions.

</details>
