{"id": "2506.14835", "pdf": "https://arxiv.org/pdf/2506.14835", "abs": "https://arxiv.org/abs/2506.14835", "authors": ["Kiet Dang Vu", "Trung Thai Tran", "Duc Dung Nguyen"], "title": "MonoVQD: Monocular 3D Object Detection with Variational Query Denoising and Self-Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Precisely localizing 3D objects from a single image constitutes a central challenge in monocular 3D detection. While DETR-like architectures offer a powerful paradigm, their direct application in this domain encounters inherent limitations, preventing optimal performance. Our work addresses these challenges by introducing MonoVQD, a novel framework designed to fundamentally advance DETR-based monocular 3D detection. We propose three main contributions. First, we propose the Mask Separated Self-Attention mechanism that enables the integration of the denoising process into a DETR architecture. This improves the stability of Hungarian matching to achieve a consistent optimization objective. Second, we present the Variational Query Denoising technique to address the gradient vanishing problem of conventional denoising methods, which severely restricts the efficiency of the denoising process. This explicitly introduces stochastic properties to mitigate this fundamental limitation and unlock substantial performance gains. Finally, we introduce a sophisticated self-distillation strategy, leveraging insights from later decoder layers to synergistically improve query quality in earlier layers, thereby amplifying the iterative refinement process. Rigorous experimentation demonstrates that MonoVQD achieves superior performance on the challenging KITTI monocular benchmark. Highlighting its broad applicability, MonoVQD's core components seamlessly integrate into other architectures, delivering significant performance gains even in multi-view 3D detection scenarios on the nuScenes dataset and underscoring its robust generalization capabilities.", "AI": {"tldr": "MonoVQD\u662f\u4e00\u4e2a\u57fa\u4e8eDETR\u7684\u5355\u76ee3D\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165Mask Separated Self-Attention\u3001Variational Query Denoising\u548c\u81ea\u84b8\u998f\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3DETR\u67b6\u6784\u5728\u5355\u76ee3D\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u9650\u5236\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u521b\u65b0\uff1aMask Separated Self-Attention\u3001Variational Query Denoising\u548c\u81ea\u84b8\u998f\u7b56\u7565\u3002", "result": "\u5728KITTI\u548cnuScenes\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MonoVQD\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5355\u76ee3D\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.14856", "pdf": "https://arxiv.org/pdf/2506.14856", "abs": "https://arxiv.org/abs/2506.14856", "authors": ["Zhengquan Zhang", "Feng Xu", "Mengmi Zhang"], "title": "Peering into the Unknown: Active View Selection with Neural Uncertainty Maps for 3D Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 3 figures in the main text. Under review for NeurIPS 2025", "summary": "Some perspectives naturally provide more information than others. How can an AI system determine which viewpoint offers the most valuable insight for accurate and efficient 3D object reconstruction? Active view selection (AVS) for 3D reconstruction remains a fundamental challenge in computer vision. The aim is to identify the minimal set of views that yields the most accurate 3D reconstruction. Instead of learning radiance fields, like NeRF or 3D Gaussian Splatting, from a current observation and computing uncertainty for each candidate viewpoint, we introduce a novel AVS approach guided by neural uncertainty maps predicted by a lightweight feedforward deep neural network, named UPNet. UPNet takes a single input image of a 3D object and outputs a predicted uncertainty map, representing uncertainty values across all possible candidate viewpoints. By leveraging heuristics derived from observing many natural objects and their associated uncertainty patterns, we train UPNet to learn a direct mapping from viewpoint appearance to uncertainty in the underlying volumetric representations. Next, our approach aggregates all previously predicted neural uncertainty maps to suppress redundant candidate viewpoints and effectively select the most informative one. Using these selected viewpoints, we train 3D neural rendering models and evaluate the quality of novel view synthesis against other competitive AVS methods. Remarkably, despite using half of the viewpoints than the upper bound, our method achieves comparable reconstruction accuracy. In addition, it significantly reduces computational overhead during AVS, achieving up to a 400 times speedup along with over 50\\% reductions in CPU, RAM, and GPU usage compared to baseline methods. Notably, our approach generalizes effectively to AVS tasks involving novel object categories, without requiring any additional training.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u4e0d\u786e\u5b9a\u6027\u6620\u5c04\u7684\u4e3b\u52a8\u89c6\u89d2\u9009\u62e9\u65b9\u6cd5\uff08UPNet\uff09\uff0c\u7528\u4e8e\u9ad8\u65483D\u91cd\u5efa\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u5e76\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b33D\u91cd\u5efa\u4e2d\u5982\u4f55\u9009\u62e9\u6700\u5177\u4fe1\u606f\u91cf\u7684\u89c6\u89d2\u4ee5\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u5e76\u63d0\u9ad8\u6548\u7387\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u524d\u9988\u795e\u7ecf\u7f51\u7edcUPNet\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u6620\u5c04\uff0c\u901a\u8fc7\u542f\u53d1\u5f0f\u65b9\u6cd5\u9009\u62e9\u6700\u5177\u4fe1\u606f\u91cf\u7684\u89c6\u89d2\u3002", "result": "\u65b9\u6cd5\u4ec5\u9700\u4e00\u534a\u89c6\u89d2\u5373\u53ef\u8fbe\u5230\u4e0e\u4e0a\u9650\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u8ba1\u7b97\u901f\u5ea6\u63d0\u5347400\u500d\uff0c\u8d44\u6e90\u6d88\u8017\u51cf\u5c1150%\u4ee5\u4e0a\u3002", "conclusion": "UPNet\u5728\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u6cdb\u5316\u5230\u65b0\u7269\u4f53\u7c7b\u522b\u7684\u89c6\u89d2\u9009\u62e9\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2506.14903", "pdf": "https://arxiv.org/pdf/2506.14903", "abs": "https://arxiv.org/abs/2506.14903", "authors": ["Renjith Prasad", "Abhilekh Borah", "Hasnat Md Abdullah", "Chathurangi Shyalika", "Gurpreet Singh", "Ritvik Garimella", "Rajarshi Roy", "Harshul Surana", "Nasrin Imanpour", "Suranjana Trivedy", "Amit Sheth", "Amitava Das"], "title": "DETONATE: A Benchmark for Text-to-Image Alignment and Kernelized Direct Preference Optimization", "categories": ["cs.CV"], "comment": "59 pages, 10 figures", "summary": "Alignment is crucial for text-to-image (T2I) models to ensure that generated images faithfully capture user intent while maintaining safety and fairness. Direct Preference Optimization (DPO), prominent in large language models (LLMs), is extending its influence to T2I systems. This paper introduces DPO-Kernels for T2I models, a novel extension enhancing alignment across three dimensions: (i) Hybrid Loss, integrating embedding-based objectives with traditional probability-based loss for improved optimization; (ii) Kernelized Representations, employing Radial Basis Function (RBF), Polynomial, and Wavelet kernels for richer feature transformations and better separation between safe and unsafe inputs; and (iii) Divergence Selection, expanding beyond DPO's default Kullback-Leibler (KL) regularizer by incorporating Wasserstein and R'enyi divergences for enhanced stability and robustness. We introduce DETONATE, the first large-scale benchmark of its kind, comprising approximately 100K curated image pairs categorized as chosen and rejected. DETONATE encapsulates three axes of social bias and discrimination: Race, Gender, and Disability. Prompts are sourced from hate speech datasets, with images generated by leading T2I models including Stable Diffusion 3.5 Large, Stable Diffusion XL, and Midjourney. Additionally, we propose the Alignment Quality Index (AQI), a novel geometric measure quantifying latent-space separability of safe/unsafe image activations, revealing hidden vulnerabilities. Empirically, we demonstrate that DPO-Kernels maintain strong generalization bounds via Heavy-Tailed Self-Regularization (HT-SR). DETONATE and complete code are publicly released.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDPO-Kernels\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u635f\u5931\u3001\u6838\u5316\u8868\u793a\u548c\u6563\u5ea6\u9009\u62e9\u589e\u5f3a\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u6821\u51c6\uff0c\u5e76\u5f15\u5165DETONATE\u57fa\u51c6\u548cAQI\u6307\u6807\u3002", "motivation": "\u786e\u4fdd\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\u51c6\u786e\u53cd\u6620\u7528\u6237\u610f\u56fe\uff0c\u540c\u65f6\u4fdd\u6301\u5b89\u5168\u6027\u548c\u516c\u5e73\u6027\u3002", "method": "\u63d0\u51faDPO-Kernels\u65b9\u6cd5\uff0c\u5305\u62ec\u6df7\u5408\u635f\u5931\u3001\u6838\u5316\u8868\u793a\u548c\u6563\u5ea6\u9009\u62e9\uff0c\u5e76\u5f15\u5165DETONATE\u57fa\u51c6\u548cAQI\u6307\u6807\u3002", "result": "DPO-Kernels\u901a\u8fc7HT-SR\u4fdd\u6301\u5f3a\u6cdb\u5316\u80fd\u529b\uff0cDETONATE\u57fa\u51c6\u5305\u542b10\u4e07\u5bf9\u56fe\u50cf\u3002", "conclusion": "DPO-Kernels\u6709\u6548\u63d0\u5347\u6a21\u578b\u6821\u51c6\uff0cDETONATE\u548cAQI\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2506.14919", "pdf": "https://arxiv.org/pdf/2506.14919", "abs": "https://arxiv.org/abs/2506.14919", "authors": ["Xinkai Zhao", "Yuta Tokuoka", "Junichiro Iwasawa", "Keita Oda"], "title": "Frequency-Calibrated Membership Inference Attacks on Medical Image Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The increasing use of diffusion models for image generation, especially in sensitive areas like medical imaging, has raised significant privacy concerns. Membership Inference Attack (MIA) has emerged as a potential approach to determine if a specific image was used to train a diffusion model, thus quantifying privacy risks. Existing MIA methods often rely on diffusion reconstruction errors, where member images are expected to have lower reconstruction errors than non-member images. However, applying these methods directly to medical images faces challenges. Reconstruction error is influenced by inherent image difficulty, and diffusion models struggle with high-frequency detail reconstruction. To address these issues, we propose a Frequency-Calibrated Reconstruction Error (FCRE) method for MIAs on medical image diffusion models. By focusing on reconstruction errors within a specific mid-frequency range and excluding both high-frequency (difficult to reconstruct) and low-frequency (less informative) regions, our frequency-selective approach mitigates the confounding factor of inherent image difficulty. Specifically, we analyze the reverse diffusion process, obtain the mid-frequency reconstruction error, and compute the structural similarity index score between the reconstructed and original images. Membership is determined by comparing this score to a threshold. Experiments on several medical image datasets demonstrate that our FCRE method outperforms existing MIA methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u533b\u5b66\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u9891\u7387\u6821\u51c6\u91cd\u5efa\u8bef\u5dee\uff08FCRE\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u6210\u5458\u63a8\u65ad\u653b\u51fb\uff08MIA\uff09\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u533b\u5b66\u56fe\u50cf\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u9690\u79c1\u98ce\u9669\u65e5\u76ca\u7a81\u51fa\uff0c\u73b0\u6709MIA\u65b9\u6cd5\u4f9d\u8d56\u91cd\u5efa\u8bef\u5dee\uff0c\u4f46\u533b\u5b66\u56fe\u50cf\u7684\u9ad8\u9891\u7ec6\u8282\u91cd\u5efa\u56f0\u96be\u5bfc\u81f4\u65b9\u6cd5\u5931\u6548\u3002", "method": "\u901a\u8fc7\u5206\u6790\u53cd\u5411\u6269\u6563\u8fc7\u7a0b\uff0c\u805a\u7126\u4e2d\u9891\u8303\u56f4\u7684\u91cd\u5efa\u8bef\u5dee\uff0c\u6392\u9664\u9ad8\u9891\u548c\u4f4e\u9891\u533a\u57df\uff0c\u8ba1\u7b97\u7ed3\u6784\u76f8\u4f3c\u6027\u6307\u6570\u5f97\u5206\uff0c\u5e76\u4e0e\u9608\u503c\u6bd4\u8f83\u786e\u5b9a\u6210\u5458\u5173\u7cfb\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFCRE\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709MIA\u65b9\u6cd5\u3002", "conclusion": "FCRE\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u9690\u79c1\u98ce\u9669\u8bc4\u4f30\u95ee\u9898\uff0c\u4e3a\u654f\u611f\u9886\u57df\u7684\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.15033", "pdf": "https://arxiv.org/pdf/2506.15033", "abs": "https://arxiv.org/abs/2506.15033", "authors": ["Gary Song Yan", "Yusen Zhang", "Jinyu Zhao", "Hao Zhang", "Zhangping Yang", "Guanye Xiong", "Yanfei Liu", "Tao Zhang", "Yujie He", "Siyuan Tian", "Yao Gou", "Min Li"], "title": "Break Stylistic Sophon: Are We Really Meant to Confine the Imagination in Style Transfer?", "categories": ["cs.CV"], "comment": null, "summary": "In this pioneering study, we introduce StyleWallfacer, a groundbreaking unified training and inference framework, which not only addresses various issues encountered in the style transfer process of traditional methods but also unifies the framework for different tasks. This framework is designed to revolutionize the field by enabling artist level style transfer and text driven stylization. First, we propose a semantic-based style injection method that uses BLIP to generate text descriptions strictly aligned with the semantics of the style image in CLIP space. By leveraging a large language model to remove style-related descriptions from these descriptions, we create a semantic gap. This gap is then used to fine-tune the model, enabling efficient and drift-free injection of style knowledge. Second, we propose a data augmentation strategy based on human feedback, incorporating high-quality samples generated early in the fine-tuning process into the training set to facilitate progressive learning and significantly reduce its overfitting. Finally, we design a training-free triple diffusion process using the fine-tuned model, which manipulates the features of self-attention layers in a manner similar to the cross-attention mechanism. Specifically, in the generation process, the key and value of the content-related process are replaced with those of the style-related process to inject style while maintaining text control over the model. We also introduce query preservation to mitigate disruptions to the original content. Under such a design, we have achieved high-quality image-driven style transfer and text-driven stylization, delivering artist-level style transfer results while preserving the original image content. Moreover, we achieve image color editing during the style transfer process for the first time.", "AI": {"tldr": "StyleWallfacer\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u98ce\u683c\u6ce8\u5165\u3001\u6570\u636e\u589e\u5f3a\u7b56\u7565\u548c\u65e0\u8bad\u7ec3\u4e09\u91cd\u6269\u6563\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u98ce\u683c\u8fc1\u79fb\u548c\u6587\u672c\u9a71\u52a8\u98ce\u683c\u5316\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\u4e2d\u7684\u95ee\u9898\uff0c\u7edf\u4e00\u4e0d\u540c\u4efb\u52a1\u7684\u6846\u67b6\uff0c\u5b9e\u73b0\u827a\u672f\u5bb6\u7ea7\u522b\u7684\u98ce\u683c\u8fc1\u79fb\u548c\u6587\u672c\u9a71\u52a8\u98ce\u683c\u5316\u3002", "method": "1. \u57fa\u4e8eBLIP\u7684\u8bed\u4e49\u98ce\u683c\u6ce8\u5165\u65b9\u6cd5\uff1b2. \u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff1b3. \u65e0\u8bad\u7ec3\u4e09\u91cd\u6269\u6563\u8fc7\u7a0b\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u98ce\u683c\u8fc1\u79fb\u548c\u6587\u672c\u9a71\u52a8\u98ce\u683c\u5316\uff0c\u9996\u6b21\u5728\u98ce\u683c\u8fc1\u79fb\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u56fe\u50cf\u989c\u8272\u7f16\u8f91\u3002", "conclusion": "StyleWallfacer\u6846\u67b6\u5728\u98ce\u683c\u8fc1\u79fb\u9886\u57df\u5177\u6709\u9769\u547d\u6027\u610f\u4e49\uff0c\u80fd\u591f\u9ad8\u6548\u3001\u65e0\u6f02\u79fb\u5730\u6ce8\u5165\u98ce\u683c\u77e5\u8bc6\uff0c\u5e76\u4fdd\u6301\u5bf9\u6587\u672c\u7684\u63a7\u5236\u3002"}}
{"id": "2506.15166", "pdf": "https://arxiv.org/pdf/2506.15166", "abs": "https://arxiv.org/abs/2506.15166", "authors": ["Abdur Rahman", "Keerthiveena Balraj", "Manojkumar Ramteke", "Anurag Singh Rathore"], "title": "Echo-DND: A dual noise diffusion model for robust and precise left ventricle segmentation in echocardiography", "categories": ["cs.CV"], "comment": "Version of record published in Discover Applied Sciences (Springer   Nature). The definitive article is available at   https://doi.org/10.1007/s42452-025-07055-5", "summary": "Recent advancements in diffusion probabilistic models (DPMs) have revolutionized image processing, demonstrating significant potential in medical applications. Accurate segmentation of the left ventricle (LV) in echocardiograms is crucial for diagnostic procedures and necessary treatments. However, ultrasound images are notoriously noisy with low contrast and ambiguous LV boundaries, thereby complicating the segmentation process. To address these challenges, this paper introduces Echo-DND, a novel dual-noise diffusion model specifically designed for this task. Echo-DND leverages a unique combination of Gaussian and Bernoulli noises. It also incorporates a multi-scale fusion conditioning module to improve segmentation precision. Furthermore, it utilizes spatial coherence calibration to maintain spatial integrity in segmentation masks. The model's performance was rigorously validated on the CAMUS and EchoNet-Dynamic datasets. Extensive evaluations demonstrate that the proposed framework outperforms existing SOTA models. It achieves high Dice scores of 0.962 and 0.939 on these datasets, respectively. The proposed Echo-DND model establishes a new standard in echocardiogram segmentation, and its architecture holds promise for broader applicability in other medical imaging tasks, potentially improving diagnostic accuracy across various medical domains. Project page: https://abdur75648.github.io/Echo-DND", "AI": {"tldr": "Echo-DND\u662f\u4e00\u79cd\u65b0\u578b\u53cc\u566a\u58f0\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u8d85\u58f0\u5fc3\u52a8\u56fe\u4e2d\u5de6\u5fc3\u5ba4\u7684\u5206\u5272\uff0c\u7ed3\u5408\u9ad8\u65af\u548c\u4f2f\u52aa\u5229\u566a\u58f0\uff0c\u5e76\u5f15\u5165\u591a\u5c3a\u5ea6\u878d\u5408\u6761\u4ef6\u6a21\u5757\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u6821\u51c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u8d85\u58f0\u56fe\u50cf\u566a\u58f0\u591a\u3001\u5bf9\u6bd4\u5ea6\u4f4e\u4e14\u8fb9\u754c\u6a21\u7cca\uff0c\u5bfc\u81f4\u5de6\u5fc3\u5ba4\u5206\u5272\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faEcho-DND\u6a21\u578b\uff0c\u7ed3\u5408\u9ad8\u65af\u548c\u4f2f\u52aa\u5229\u566a\u58f0\uff0c\u91c7\u7528\u591a\u5c3a\u5ea6\u878d\u5408\u6761\u4ef6\u6a21\u5757\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u6821\u51c6\u6280\u672f\u3002", "result": "\u5728CAMUS\u548cEchoNet-Dynamic\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cDice\u5206\u6570\u5206\u522b\u8fbe\u52300.962\u548c0.939\uff0c\u8d85\u8d8a\u73b0\u6709SOTA\u6a21\u578b\u3002", "conclusion": "Echo-DND\u4e3a\u8d85\u58f0\u5fc3\u52a8\u56fe\u5206\u5272\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u5176\u67b6\u6784\u6709\u671b\u6269\u5c55\u5230\u5176\u4ed6\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\uff0c\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u3002"}}
{"id": "2506.15218", "pdf": "https://arxiv.org/pdf/2506.15218", "abs": "https://arxiv.org/abs/2506.15218", "authors": ["Dan He", "Weisheng Li", "Guofen Wang", "Yuping Huang", "Shiqiang Liu"], "title": "DM-FNet: Unified multimodal medical image fusion via diffusion process-trained encoder-decoder", "categories": ["cs.CV"], "comment": "This paper has been accepted by IEEE Transactions on Multimedia (TMM)   in March 2025", "summary": "Multimodal medical image fusion (MMIF) extracts the most meaningful information from multiple source images, enabling a more comprehensive and accurate diagnosis. Achieving high-quality fusion results requires a careful balance of brightness, color, contrast, and detail; this ensures that the fused images effectively display relevant anatomical structures and reflect the functional status of the tissues. However, existing MMIF methods have limited capacity to capture detailed features during conventional training and suffer from insufficient cross-modal feature interaction, leading to suboptimal fused image quality. To address these issues, this study proposes a two-stage diffusion model-based fusion network (DM-FNet) to achieve unified MMIF. In Stage I, a diffusion process trains UNet for image reconstruction. UNet captures detailed information through progressive denoising and represents multilevel data, providing a rich set of feature representations for the subsequent fusion network. In Stage II, noisy images at various steps are input into the fusion network to enhance the model's feature recognition capability. Three key fusion modules are also integrated to process medical images from different modalities adaptively. Ultimately, the robust network structure and a hybrid loss function are integrated to harmonize the fused image's brightness, color, contrast, and detail, enhancing its quality and information density. The experimental results across various medical image types demonstrate that the proposed method performs exceptionally well regarding objective evaluation metrics. The fused image preserves appropriate brightness, a comprehensive distribution of radioactive tracers, rich textures, and clear edges. The code is available at https://github.com/HeDan-11/DM-FNet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e24\u9636\u6bb5\u6269\u6563\u6a21\u578b\u7684\u878d\u5408\u7f51\u7edc\uff08DM-FNet\uff09\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u878d\u5408\uff0c\u901a\u8fc7\u589e\u5f3a\u7279\u5f81\u8bc6\u522b\u548c\u4ea4\u4e92\u80fd\u529b\u63d0\u5347\u878d\u5408\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u878d\u5408\u65b9\u6cd5\u5728\u7279\u5f81\u6355\u83b7\u548c\u8de8\u6a21\u6001\u4ea4\u4e92\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u5bfc\u81f4\u878d\u5408\u56fe\u50cf\u8d28\u91cf\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6269\u6563\u6a21\u578b\uff1a\u7b2c\u4e00\u9636\u6bb5\u8bad\u7ec3UNet\u8fdb\u884c\u56fe\u50cf\u91cd\u5efa\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5c06\u566a\u58f0\u56fe\u50cf\u8f93\u5165\u878d\u5408\u7f51\u7edc\uff0c\u7ed3\u5408\u4e09\u4e2a\u5173\u952e\u878d\u5408\u6a21\u5757\u81ea\u9002\u5e94\u5904\u7406\u4e0d\u540c\u6a21\u6001\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5ba2\u89c2\u8bc4\u4ef7\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u878d\u5408\u56fe\u50cf\u4fdd\u7559\u4e86\u4eae\u5ea6\u3001\u653e\u5c04\u6027\u793a\u8e2a\u5242\u5206\u5e03\u3001\u7eb9\u7406\u548c\u8fb9\u7f18\u6e05\u6670\u5ea6\u3002", "conclusion": "DM-FNet\u901a\u8fc7\u4e24\u9636\u6bb5\u8bbe\u8ba1\u548c\u6df7\u5408\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u878d\u5408\u7684\u8d28\u91cf\u548c\u4fe1\u606f\u5bc6\u5ea6\u3002"}}
{"id": "2506.15220", "pdf": "https://arxiv.org/pdf/2506.15220", "abs": "https://arxiv.org/abs/2506.15220", "authors": ["Changli Tang", "Yixuan Li", "Yudong Yang", "Jimin Zhuang", "Guangzhi Sun", "Wei Li", "Zejun Ma", "Chao Zhang"], "title": "video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models", "categories": ["cs.CV", "cs.CL", "cs.SD"], "comment": null, "summary": "Videos contain a wealth of information, and generating detailed and accurate descriptions in natural language is a key aspect of video understanding. In this paper, we present video-SALMONN 2, an advanced audio-visual large language model (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with paired audio) captioning through directed preference optimisation (DPO). We propose new metrics to evaluate the completeness and accuracy of video descriptions, which are optimised using DPO. To further improve training, we propose a novel multi-round DPO (MrDPO) approach, which involves periodically updating the DPO reference model, merging and re-initialising the LoRA module as a proxy for parameter updates after each training round (1,000 steps), and incorporating guidance from ground-truth video captions to stabilise the process. Experimental results show that MrDPO significantly enhances video-SALMONN 2's captioning accuracy, reducing the captioning error rates by 28\\%. The final video-SALMONN 2 model, with just 7 billion parameters, surpasses leading models such as GPT-4o and Gemini-1.5-Pro in video captioning tasks, while maintaining highly competitive performance to the state-of-the-art on widely used video question-answering benchmarks among models of similar size. Codes are available at \\href{https://github.com/bytedance/video-SALMONN-2}{https://github.com/bytedance/video-SALMONN-2}.", "AI": {"tldr": "\u89c6\u9891-SALMONN 2\u662f\u4e00\u79cd\u5148\u8fdb\u7684\u89c6\u542c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u548c\u5b9a\u5411\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u63d0\u5347\u89c6\u9891\uff08\u5e26\u97f3\u9891\uff09\u5b57\u5e55\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\u3002", "motivation": "\u89c6\u9891\u5305\u542b\u4e30\u5bcc\u4fe1\u606f\uff0c\u751f\u6210\u8be6\u7ec6\u51c6\u786e\u7684\u63cf\u8ff0\u662f\u89c6\u9891\u7406\u89e3\u7684\u5173\u952e\u3002", "method": "\u63d0\u51fa\u591a\u8f6eDPO\uff08MrDPO\uff09\u65b9\u6cd5\uff0c\u5468\u671f\u6027\u66f4\u65b0\u53c2\u8003\u6a21\u578b\u3001\u5408\u5e76\u5e76\u91cd\u65b0\u521d\u59cb\u5316LoRA\u6a21\u5757\uff0c\u5e76\u7ed3\u5408\u771f\u5b9e\u5b57\u5e55\u6307\u5bfc\u3002", "result": "MrDPO\u663e\u8457\u63d0\u5347\u5b57\u5e55\u51c6\u786e\u6027\uff0c\u9519\u8bef\u7387\u964d\u4f4e28%\uff0c7B\u53c2\u6570\u7684\u6a21\u578b\u8d85\u8d8aGPT-4o\u548cGemini-1.5-Pro\u3002", "conclusion": "\u89c6\u9891-SALMONN 2\u5728\u89c6\u9891\u5b57\u5e55\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.15242", "pdf": "https://arxiv.org/pdf/2506.15242", "abs": "https://arxiv.org/abs/2506.15242", "authors": ["Qingsong Yan", "Qiang Wang", "Kaiyong Zhao", "Jie Chen", "Bo Li", "Xiaowen Chu", "Fei Deng"], "title": "RA-NeRF: Robust Neural Radiance Field Reconstruction with Accurate Camera Pose Estimation under Complex Trajectories", "categories": ["cs.CV"], "comment": "IROS 2025", "summary": "Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have emerged as powerful tools for 3D reconstruction and SLAM tasks. However, their performance depends heavily on accurate camera pose priors. Existing approaches attempt to address this issue by introducing external constraints but fall short of achieving satisfactory accuracy, particularly when camera trajectories are complex. In this paper, we propose a novel method, RA-NeRF, capable of predicting highly accurate camera poses even with complex camera trajectories. Following the incremental pipeline, RA-NeRF reconstructs the scene using NeRF with photometric consistency and incorporates flow-driven pose regulation to enhance robustness during initialization and localization. Additionally, RA-NeRF employs an implicit pose filter to capture the camera movement pattern and eliminate the noise for pose estimation. To validate our method, we conduct extensive experiments on the Tanks\\&Temple dataset for standard evaluation, as well as the NeRFBuster dataset, which presents challenging camera pose trajectories. On both datasets, RA-NeRF achieves state-of-the-art results in both camera pose estimation and visual quality, demonstrating its effectiveness and robustness in scene reconstruction under complex pose trajectories.", "AI": {"tldr": "RA-NeRF\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u590d\u6742\u76f8\u673a\u8f68\u8ff9\u4e0b\u9884\u6d4b\u9ad8\u7cbe\u5ea6\u76f8\u673a\u4f4d\u59ff\uff0c\u7ed3\u5408NeRF\u548c\u5149\u6d41\u9a71\u52a8\u4f4d\u59ff\u8c03\u8282\uff0c\u663e\u8457\u63d0\u5347\u91cd\u5efa\u548c\u5b9a\u4f4d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7cbe\u786e\u7684\u76f8\u673a\u4f4d\u59ff\u5148\u9a8c\uff0c\u4f46\u5728\u590d\u6742\u8f68\u8ff9\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "RA-NeRF\u91c7\u7528\u589e\u91cf\u5f0f\u6d41\u7a0b\uff0c\u7ed3\u5408NeRF\u7684\u5149\u5ea6\u4e00\u81f4\u6027\u548c\u5149\u6d41\u9a71\u52a8\u4f4d\u59ff\u8c03\u8282\uff0c\u5e76\u4f7f\u7528\u9690\u5f0f\u4f4d\u59ff\u6ee4\u6ce2\u5668\u6d88\u9664\u566a\u58f0\u3002", "result": "\u5728Tanks&Temple\u548cNeRFBuster\u6570\u636e\u96c6\u4e0a\uff0cRA-NeRF\u5728\u4f4d\u59ff\u4f30\u8ba1\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u8fbe\u5230SOTA\u3002", "conclusion": "RA-NeRF\u5728\u590d\u6742\u76f8\u673a\u8f68\u8ff9\u4e0b\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.15276", "pdf": "https://arxiv.org/pdf/2506.15276", "abs": "https://arxiv.org/abs/2506.15276", "authors": ["Jun Zhu", "Xinfeng Zhang", "Lv Tang", "JunHao Jiang"], "title": "MSNeRV: Neural Video Representation with Multi-Scale Feature Fusion", "categories": ["cs.CV", "cs.MM", "eess.IV"], "comment": null, "summary": "Implicit Neural representations (INRs) have emerged as a promising approach for video compression, and have achieved comparable performance to the state-of-the-art codecs such as H.266/VVC. However, existing INR-based methods struggle to effectively represent detail-intensive and fast-changing video content. This limitation mainly stems from the underutilization of internal network features and the absence of video-specific considerations in network design. To address these challenges, we propose a multi-scale feature fusion framework, MSNeRV, for neural video representation. In the encoding stage, we enhance temporal consistency by employing temporal windows, and divide the video into multiple Groups of Pictures (GoPs), where a GoP-level grid is used for background representation. Additionally, we design a multi-scale spatial decoder with a scale-adaptive loss function to integrate multi-resolution and multi-frequency information. To further improve feature extraction, we introduce a multi-scale feature block that fully leverages hidden features. We evaluate MSNeRV on HEVC ClassB and UVG datasets for video representation and compression. Experimental results demonstrate that our model exhibits superior representation capability among INR-based approaches and surpasses VTM-23.7 (Random Access) in dynamic scenarios in terms of compression efficiency.", "AI": {"tldr": "MSNeRV\u662f\u4e00\u79cd\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u795e\u7ecf\u89c6\u9891\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709INR\u65b9\u6cd5\u5728\u7ec6\u8282\u5bc6\u96c6\u548c\u5feb\u901f\u53d8\u5316\u89c6\u9891\u5185\u5bb9\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709INR\u65b9\u6cd5\u5728\u7ec6\u8282\u5bc6\u96c6\u548c\u5feb\u901f\u53d8\u5316\u89c6\u9891\u5185\u5bb9\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f51\u7edc\u7279\u5f81\u5229\u7528\u4e0d\u8db3\u548c\u7f3a\u4e4f\u89c6\u9891\u7279\u5b9a\u8bbe\u8ba1\u3002", "method": "\u63d0\u51fa\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u6846\u67b6MSNeRV\uff0c\u91c7\u7528\u65f6\u95f4\u7a97\u53e3\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5206\u7ec4\u5904\u7406\u89c6\u9891\uff0c\u8bbe\u8ba1\u591a\u5c3a\u5ea6\u7a7a\u95f4\u89e3\u7801\u5668\u548c\u81ea\u9002\u5e94\u635f\u5931\u51fd\u6570\uff0c\u5e76\u5f15\u5165\u591a\u5c3a\u5ea6\u7279\u5f81\u5757\u3002", "result": "\u5728HEVC ClassB\u548cUVG\u6570\u636e\u96c6\u4e0a\uff0cMSNeRV\u5728INR\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u538b\u7f29\u6548\u7387\u8d85\u8fc7VTM-23.7\uff08\u968f\u673a\u8bbf\u95ee\uff09\u3002", "conclusion": "MSNeRV\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u548c\u89c6\u9891\u7279\u5b9a\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u795e\u7ecf\u89c6\u9891\u8868\u793a\u548c\u538b\u7f29\u6027\u80fd\u3002"}}
{"id": "2506.15381", "pdf": "https://arxiv.org/pdf/2506.15381", "abs": "https://arxiv.org/abs/2506.15381", "authors": ["Yujin Kim", "Hyunsoo Kim", "Hyunwoo J. Kim", "Suhyun Kim"], "title": "When Model Knowledge meets Diffusion Model: Diffusion-assisted Data-free Image Synthesis with Alignment of Domain and Class", "categories": ["cs.CV"], "comment": "Published at ICML 2025", "summary": "Open-source pre-trained models hold great potential for diverse applications, but their utility declines when their training data is unavailable. Data-Free Image Synthesis (DFIS) aims to generate images that approximate the learned data distribution of a pre-trained model without accessing the original data. However, existing DFIS meth ods produce samples that deviate from the training data distribution due to the lack of prior knowl edge about natural images. To overcome this limitation, we propose DDIS, the first Diffusion-assisted Data-free Image Synthesis method that leverages a text-to-image diffusion model as a powerful image prior, improving synthetic image quality. DDIS extracts knowledge about the learned distribution from the given model and uses it to guide the diffusion model, enabling the generation of images that accurately align with the training data distribution. To achieve this, we introduce Domain Alignment Guidance (DAG) that aligns the synthetic data domain with the training data domain during the diffusion sampling process. Furthermore, we optimize a single Class Alignment Token (CAT) embedding to effectively capture class-specific attributes in the training dataset. Experiments on PACS and Ima geNet demonstrate that DDIS outperforms prior DFIS methods by generating samples that better reflect the training data distribution, achieving SOTA performance in data-free applications.", "AI": {"tldr": "DDIS\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6570\u636e\u81ea\u7531\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u9886\u57df\u5bf9\u9f50\u6307\u5bfc\u548c\u7c7b\u5bf9\u9f50\u6807\u8bb0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u548c\u5bf9\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5f00\u6e90\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5e94\u7528\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u82e5\u65e0\u6cd5\u83b7\u53d6\u5176\u8bad\u7ec3\u6570\u636e\uff0c\u5176\u6548\u7528\u4f1a\u4e0b\u964d\u3002\u73b0\u6709\u6570\u636e\u81ea\u7531\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u81ea\u7136\u56fe\u50cf\u5148\u9a8c\u77e5\u8bc6\uff0c\u751f\u6210\u6837\u672c\u504f\u79bb\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u3002", "method": "\u63d0\u51faDDIS\u65b9\u6cd5\uff0c\u5229\u7528\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u56fe\u50cf\u5148\u9a8c\uff0c\u901a\u8fc7\u9886\u57df\u5bf9\u9f50\u6307\u5bfc\uff08DAG\uff09\u548c\u7c7b\u5bf9\u9f50\u6807\u8bb0\uff08CAT\uff09\u4f18\u5316\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5728PACS\u548cImageNet\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDDIS\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u6837\u672c\u66f4\u51c6\u786e\u5730\u53cd\u6620\u8bad\u7ec3\u6570\u636e\u5206\u5e03\uff0c\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "DDIS\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u9886\u57df\u5bf9\u9f50\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u81ea\u7531\u56fe\u50cf\u5408\u6210\u7684\u8d28\u91cf\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2506.15442", "pdf": "https://arxiv.org/pdf/2506.15442", "abs": "https://arxiv.org/abs/2506.15442", "authors": ["Team Hunyuan3D", "Shuhui Yang", "Mingxin Yang", "Yifei Feng", "Xin Huang", "Sheng Zhang", "Zebin He", "Di Luo", "Haolin Liu", "Yunfei Zhao", "Qingxiang Lin", "Zeqiang Lai", "Xianghui Yang", "Huiwen Shi", "Zibo Zhao", "Bowen Zhang", "Hongyu Yan", "Lifu Wang", "Sicong Liu", "Jihong Zhang", "Meng Chen", "Liang Dong", "Yiwen Jia", "Yulin Cai", "Jiaao Yu", "Yixuan Tang", "Dongyuan Guo", "Junlin Yu", "Hao Zhang", "Zheng Ye", "Peng He", "Runzhou Wu", "Shida Wei", "Chao Zhang", "Yonghao Tan", "Yifu Sun", "Lin Niu", "Shirui Huang", "Bojian Zheng", "Shu Liu", "Shilin Chen", "Xiang Yuan", "Xiaofeng Yang", "Kai Liu", "Jianchen Zhu", "Peng Chen", "Tian Liu", "Di Wang", "Yuhong Liu", "Linus", "Jie Jiang", "Jingwei Huang", "Chunchao Guo"], "title": "Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with Production-Ready PBR Material", "categories": ["cs.CV", "cs.AI"], "comment": "Github link: https://github.com/Tencent-Hunyuan/Hunyuan3D-2.1", "summary": "3D AI-generated content (AIGC) is a passionate field that has significantly accelerated the creation of 3D models in gaming, film, and design. Despite the development of several groundbreaking models that have revolutionized 3D generation, the field remains largely accessible only to researchers, developers, and designers due to the complexities involved in collecting, processing, and training 3D models. To address these challenges, we introduce Hunyuan3D 2.1 as a case study in this tutorial. This tutorial offers a comprehensive, step-by-step guide on processing 3D data, training a 3D generative model, and evaluating its performance using Hunyuan3D 2.1, an advanced system for producing high-resolution, textured 3D assets. The system comprises two core components: the Hunyuan3D-DiT for shape generation and the Hunyuan3D-Paint for texture synthesis. We will explore the entire workflow, including data preparation, model architecture, training strategies, evaluation metrics, and deployment. By the conclusion of this tutorial, you will have the knowledge to finetune or develop a robust 3D generative model suitable for applications in gaming, virtual reality, and industrial design.", "AI": {"tldr": "\u672c\u6559\u7a0b\u4ee5Hunyuan3D 2.1\u4e3a\u4f8b\uff0c\u63d0\u4f9b3D\u6570\u636e\u5904\u7406\u3001\u751f\u6210\u6a21\u578b\u8bad\u7ec3\u53ca\u6027\u80fd\u8bc4\u4f30\u7684\u5b8c\u6574\u6307\u5357\uff0c\u65e8\u5728\u964d\u4f4e3D AI\u751f\u6210\u5185\u5bb9\u7684\u95e8\u69db\u3002", "motivation": "3D AIGC\u9886\u57df\u867d\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u56e0\u6570\u636e\u6536\u96c6\u3001\u5904\u7406\u548c\u6a21\u578b\u8bad\u7ec3\u7684\u590d\u6742\u6027\uff0c\u4ecd\u4e3b\u8981\u5c40\u9650\u4e8e\u4e13\u4e1a\u4eba\u58eb\u3002\u672c\u6559\u7a0b\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u6559\u7a0b\u6db5\u76d6\u6570\u636e\u51c6\u5907\u3001\u6a21\u578b\u67b6\u6784\uff08Hunyuan3D-DiT\u548cHunyuan3D-Paint\uff09\u3001\u8bad\u7ec3\u7b56\u7565\u3001\u8bc4\u4f30\u6307\u6807\u53ca\u90e8\u7f72\u3002", "result": "\u901a\u8fc7Hunyuan3D 2.1\u7cfb\u7edf\uff0c\u7528\u6237\u53ef\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u3001\u5e26\u7eb9\u7406\u76843D\u8d44\u4ea7\u3002", "conclusion": "\u6559\u7a0b\u5b8c\u6210\u540e\uff0c\u7528\u6237\u5c06\u5177\u5907\u5fae\u8c03\u6216\u5f00\u53d1\u9002\u7528\u4e8e\u6e38\u620f\u3001\u865a\u62df\u73b0\u5b9e\u53ca\u5de5\u4e1a\u8bbe\u8ba1\u76843D\u751f\u6210\u6a21\u578b\u7684\u80fd\u529b\u3002"}}
{"id": "2506.15483", "pdf": "https://arxiv.org/pdf/2506.15483", "abs": "https://arxiv.org/abs/2506.15483", "authors": ["Shujia Li", "Haiyu Zhang", "Xinyuan Chen", "Yaohui Wang", "Yutong Ban"], "title": "GenHOI: Generalizing Text-driven 4D Human-Object Interaction Synthesis for Unseen Objects", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "While diffusion models and large-scale motion datasets have advanced text-driven human motion synthesis, extending these advances to 4D human-object interaction (HOI) remains challenging, mainly due to the limited availability of large-scale 4D HOI datasets. In our study, we introduce GenHOI, a novel two-stage framework aimed at achieving two key objectives: 1) generalization to unseen objects and 2) the synthesis of high-fidelity 4D HOI sequences. In the initial stage of our framework, we employ an Object-AnchorNet to reconstruct sparse 3D HOI keyframes for unseen objects, learning solely from 3D HOI datasets, thereby mitigating the dependence on large-scale 4D HOI datasets. Subsequently, we introduce a Contact-Aware Diffusion Model (ContactDM) in the second stage to seamlessly interpolate sparse 3D HOI keyframes into densely temporally coherent 4D HOI sequences. To enhance the quality of generated 4D HOI sequences, we propose a novel Contact-Aware Encoder within ContactDM to extract human-object contact patterns and a novel Contact-Aware HOI Attention to effectively integrate the contact signals into diffusion models. Experimental results show that we achieve state-of-the-art results on the publicly available OMOMO and 3D-FUTURE datasets, demonstrating strong generalization abilities to unseen objects, while enabling high-fidelity 4D HOI generation.", "AI": {"tldr": "GenHOI\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u76844D\u4eba-\u7269\u4ea4\u4e92\u5e8f\u5217\uff0c\u5e76\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a214D\u4eba-\u7269\u4ea4\u4e92\u6570\u636e\u96c6\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u76844D HOI\u5408\u6210\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528Object-AnchorNet\u4ece3D HOI\u6570\u636e\u4e2d\u91cd\u5efa\u7a00\u758f\u5173\u952e\u5e27\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7Contact-Aware Diffusion Model\uff08ContactDM\uff09\u63d2\u503c\u751f\u6210\u5bc6\u96c64D\u5e8f\u5217\u3002", "result": "\u5728OMOMO\u548c3D-FUTURE\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f18\u7ed3\u679c\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9ad8\u4fdd\u771f\u751f\u6210\u80fd\u529b\u3002", "conclusion": "GenHOI\u901a\u8fc7\u4e24\u9636\u6bb5\u8bbe\u8ba1\u548c\u63a5\u89e6\u611f\u77e5\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e864D HOI\u5408\u6210\u7684\u6311\u6218\u3002"}}
{"id": "2506.15563", "pdf": "https://arxiv.org/pdf/2506.15563", "abs": "https://arxiv.org/abs/2506.15563", "authors": ["Bonan Li", "Yinhan Hu", "Songhua Liu", "Xinchao Wang"], "title": "Control and Realism: Best of Both Worlds in Layout-to-Image without Training", "categories": ["cs.CV"], "comment": "Accepted by ICML2025", "summary": "Layout-to-Image generation aims to create complex scenes with precise control over the placement and arrangement of subjects. Existing works have demonstrated that pre-trained Text-to-Image diffusion models can achieve this goal without training on any specific data; however, they often face challenges with imprecise localization and unrealistic artifacts. Focusing on these drawbacks, we propose a novel training-free method, WinWinLay. At its core, WinWinLay presents two key strategies, Non-local Attention Energy Function and Adaptive Update, that collaboratively enhance control precision and realism. On one hand, we theoretically demonstrate that the commonly used attention energy function introduces inherent spatial distribution biases, hindering objects from being uniformly aligned with layout instructions. To overcome this issue, non-local attention prior is explored to redistribute attention scores, facilitating objects to better conform to the specified spatial conditions. On the other hand, we identify that the vanilla backpropagation update rule can cause deviations from the pre-trained domain, leading to out-of-distribution artifacts. We accordingly introduce a Langevin dynamics-based adaptive update scheme as a remedy that promotes in-domain updating while respecting layout constraints. Extensive experiments demonstrate that WinWinLay excels in controlling element placement and achieving photorealistic visual fidelity, outperforming the current state-of-the-art methods.", "AI": {"tldr": "WinWinLay\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5e03\u5c40\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u5c40\u90e8\u6ce8\u610f\u529b\u80fd\u91cf\u51fd\u6570\u548c\u81ea\u9002\u5e94\u66f4\u65b0\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u771f\u5b9e\u6027\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9884\u8bad\u7ec3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\u5728\u5e03\u5c40\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u5b58\u5728\u5b9a\u4f4d\u4e0d\u7cbe\u786e\u548c\u4f2a\u5f71\u95ee\u9898\uff0cWinWinLay\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "WinWinLay\u91c7\u7528\u975e\u5c40\u90e8\u6ce8\u610f\u529b\u80fd\u91cf\u51fd\u6570\u548c\u57fa\u4e8eLangevin\u52a8\u529b\u5b66\u7684\u81ea\u9002\u5e94\u66f4\u65b0\u7b56\u7565\uff0c\u5206\u522b\u4f18\u5316\u6ce8\u610f\u529b\u5206\u5e03\u548c\u66f4\u65b0\u89c4\u5219\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cWinWinLay\u5728\u5143\u7d20\u5e03\u5c40\u63a7\u5236\u548c\u89c6\u89c9\u771f\u5b9e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "WinWinLay\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u521b\u65b0\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e03\u5c40\u5230\u56fe\u50cf\u751f\u6210\u7684\u6027\u80fd\u3002"}}
{"id": "2506.15564", "pdf": "https://arxiv.org/pdf/2506.15564", "abs": "https://arxiv.org/abs/2506.15564", "authors": ["Jinheng Xie", "Zhenheng Yang", "Mike Zheng Shou"], "title": "Show-o2: Improved Native Unified Multimodal Models", "categories": ["cs.CV"], "comment": "Technical report", "summary": "This paper presents improved native unified multimodal models, \\emph{i.e.,} Show-o2, that leverage autoregressive modeling and flow matching. Built upon a 3D causal variational autoencoder space, unified visual representations are constructed through a dual-path of spatial (-temporal) fusion, enabling scalability across image and video modalities while ensuring effective multimodal understanding and generation. Based on a language model, autoregressive modeling and flow matching are natively applied to the language head and flow head, respectively, to facilitate text token prediction and image/video generation. A two-stage training recipe is designed to effectively learn and scale to larger models. The resulting Show-o2 models demonstrate versatility in handling a wide range of multimodal understanding and generation tasks across diverse modalities, including text, images, and videos. Code and models are released at https://github.com/showlab/Show-o.", "AI": {"tldr": "Show-o2\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u7ed3\u5408\u81ea\u56de\u5f52\u5efa\u6a21\u548c\u6d41\u5339\u914d\uff0c\u652f\u6301\u56fe\u50cf\u548c\u89c6\u9891\u7684\u591a\u6a21\u6001\u7406\u89e3\u4e0e\u751f\u6210\u3002", "motivation": "\u6784\u5efa\u4e00\u4e2a\u7edf\u4e00\u7684\u89c6\u89c9\u8868\u793a\u7a7a\u95f4\uff0c\u652f\u6301\u8de8\u6a21\u6001\uff08\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\uff09\u7684\u9ad8\u6548\u7406\u89e3\u548c\u751f\u6210\u3002", "method": "\u57fa\u4e8e3D\u56e0\u679c\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7a7a\u95f4\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u65f6\u7a7a\u878d\u5408\u6784\u5efa\u7edf\u4e00\u8868\u793a\uff0c\u5e76\u91c7\u7528\u81ea\u56de\u5f52\u5efa\u6a21\u548c\u6d41\u5339\u914d\u6280\u672f\u3002", "result": "Show-o2\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u652f\u6301\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u7684\u591a\u6837\u5316\u5904\u7406\u3002", "conclusion": "Show-o2\u5c55\u793a\u4e86\u8de8\u6a21\u6001\u7edf\u4e00\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.15591", "pdf": "https://arxiv.org/pdf/2506.15591", "abs": "https://arxiv.org/abs/2506.15591", "authors": ["Yujing Sun", "Lingchen Sun", "Shuaizheng Liu", "Rongyuan Wu", "Zhengqiang Zhang", "Lei Zhang"], "title": "One-Step Diffusion for Detail-Rich and Temporally Consistent Video Super-Resolution", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "It is a challenging problem to reproduce rich spatial details while maintaining temporal consistency in real-world video super-resolution (Real-VSR), especially when we leverage pre-trained generative models such as stable diffusion (SD) for realistic details synthesis. Existing SD-based Real-VSR methods often compromise spatial details for temporal coherence, resulting in suboptimal visual quality. We argue that the key lies in how to effectively extract the degradation-robust temporal consistency priors from the low-quality (LQ) input video and enhance the video details while maintaining the extracted consistency priors. To achieve this, we propose a Dual LoRA Learning (DLoRAL) paradigm to train an effective SD-based one-step diffusion model, achieving realistic frame details and temporal consistency simultaneously. Specifically, we introduce a Cross-Frame Retrieval (CFR) module to aggregate complementary information across frames, and train a Consistency-LoRA (C-LoRA) to learn robust temporal representations from degraded inputs. After consistency learning, we fix the CFR and C-LoRA modules and train a Detail-LoRA (D-LoRA) to enhance spatial details while aligning with the temporal space defined by C-LoRA to keep temporal coherence. The two phases alternate iteratively for optimization, collaboratively delivering consistent and detail-rich outputs. During inference, the two LoRA branches are merged into the SD model, allowing efficient and high-quality video restoration in a single diffusion step. Experiments show that DLoRAL achieves strong performance in both accuracy and speed. Code and models are available at https://github.com/yjsunnn/DLoRAL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDLoRAL\u7684\u53ccLoRA\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u7ed3\u5408Cross-Frame Retrieval\u6a21\u5757\u548cConsistency-LoRA\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4e2d\u7ec6\u8282\u4e30\u5bcc\u4e0e\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7a33\u5b9a\u6269\u6563\uff08SD\uff09\u7684Real-VSR\u65b9\u6cd5\u5728\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\u65f6\u727a\u7272\u4e86\u7a7a\u95f4\u7ec6\u8282\uff0c\u5bfc\u81f4\u89c6\u89c9\u8d28\u91cf\u4e0d\u4f73\u3002", "method": "\u8bbe\u8ba1\u4e86Dual LoRA Learning\uff08DLoRAL\uff09\u8303\u5f0f\uff0c\u5305\u62ecCross-Frame Retrieval\u6a21\u5757\u548cConsistency-LoRA\u6a21\u5757\uff0c\u5206\u9636\u6bb5\u8bad\u7ec3\u4ee5\u4f18\u5316\u7ec6\u8282\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "DLoRAL\u5728\u5355\u6b65\u6269\u6563\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u7ec6\u8282\u6062\u590d\u548c\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6027\u80fd\u548c\u901f\u5ea6\u7684\u4f18\u52bf\u3002", "conclusion": "DLoRAL\u901a\u8fc7\u53ccLoRA\u5b66\u4e60\u8303\u5f0f\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4e2d\u7ec6\u8282\u4e0e\u4e00\u81f4\u6027\u7684\u77db\u76fe\uff0c\u63d0\u5347\u4e86\u89c6\u89c9\u8d28\u91cf\u3002"}}
{"id": "2506.15625", "pdf": "https://arxiv.org/pdf/2506.15625", "abs": "https://arxiv.org/abs/2506.15625", "authors": ["Roey Ron", "Guy Tevet", "Haim Sawdayee", "Amit H. Bermano"], "title": "HOIDiNi: Human-Object Interaction through Diffusion Noise Optimization", "categories": ["cs.CV"], "comment": "Project page: https://hoidini.github.io", "summary": "We present HOIDiNi, a text-driven diffusion framework for synthesizing realistic and plausible human-object interaction (HOI). HOI generation is extremely challenging since it induces strict contact accuracies alongside a diverse motion manifold. While current literature trades off between realism and physical correctness, HOIDiNi optimizes directly in the noise space of a pretrained diffusion model using Diffusion Noise Optimization (DNO), achieving both. This is made feasible thanks to our observation that the problem can be separated into two phases: an object-centric phase, primarily making discrete choices of hand-object contact locations, and a human-centric phase that refines the full-body motion to realize this blueprint. This structured approach allows for precise hand-object contact without compromising motion naturalness. Quantitative, qualitative, and subjective evaluations on the GRAB dataset alone clearly indicate HOIDiNi outperforms prior works and baselines in contact accuracy, physical validity, and overall quality. Our results demonstrate the ability to generate complex, controllable interactions, including grasping, placing, and full-body coordination, driven solely by textual prompts. https://hoidini.github.io.", "AI": {"tldr": "HOIDiNi\u662f\u4e00\u4e2a\u57fa\u4e8e\u6587\u672c\u9a71\u52a8\u7684\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u771f\u5b9e\u4e14\u5408\u7406\u7684\u4eba-\u7269\u4ea4\u4e92\uff08HOI\uff09\u3002\u5b83\u901a\u8fc7\u4f18\u5316\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u566a\u58f0\u7a7a\u95f4\uff0c\u540c\u65f6\u5b9e\u73b0\u771f\u5b9e\u6027\u548c\u7269\u7406\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u4eba-\u7269\u4ea4\u4e92\u751f\u6210\u65b9\u6cd5\u5728\u771f\u5b9e\u6027\u548c\u7269\u7406\u6b63\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0cHOIDiNi\u65e8\u5728\u901a\u8fc7\u7ed3\u6784\u5316\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "HOIDiNi\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u5bf9\u8c61\u4e2d\u5fc3\u9636\u6bb5\u9009\u62e9\u624b-\u7269\u63a5\u89e6\u4f4d\u7f6e\uff0c\u4eba\u4f53\u4e2d\u5fc3\u9636\u6bb5\u4f18\u5316\u5168\u8eab\u52a8\u4f5c\u3002\u4f7f\u7528\u6269\u6563\u566a\u58f0\u4f18\u5316\uff08DNO\uff09\u76f4\u63a5\u4f18\u5316\u566a\u58f0\u7a7a\u95f4\u3002", "result": "\u5728GRAB\u6570\u636e\u96c6\u4e0a\u7684\u5b9a\u91cf\u3001\u5b9a\u6027\u548c\u4e3b\u89c2\u8bc4\u4f30\u8868\u660e\uff0cHOIDiNi\u5728\u63a5\u89e6\u7cbe\u5ea6\u3001\u7269\u7406\u6709\u6548\u6027\u548c\u6574\u4f53\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HOIDiNi\u80fd\u591f\u4ec5\u901a\u8fc7\u6587\u672c\u63d0\u793a\u751f\u6210\u590d\u6742\u4e14\u53ef\u63a7\u7684\u4ea4\u4e92\u52a8\u4f5c\uff0c\u5305\u62ec\u6293\u53d6\u3001\u653e\u7f6e\u548c\u5168\u8eab\u534f\u8c03\u3002"}}
{"id": "2506.15645", "pdf": "https://arxiv.org/pdf/2506.15645", "abs": "https://arxiv.org/abs/2506.15645", "authors": ["Shuo Xing", "Lanqing Guo", "Hongyuan Hua", "Seoyoung Lee", "Peiran Li", "Yufei Wang", "Zhangyang Wang", "Zhengzhong Tu"], "title": "Demystifying the Visual Quality Paradox in Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages", "summary": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark vision-language tasks, yet little is known about how input visual quality shapes their responses. Does higher perceptual quality of images already translate to better MLLM understanding? We conduct the first systematic study spanning leading MLLMs and a suite of vision-language benchmarks, applying controlled degradations and stylistic shifts to each image. Surprisingly, we uncover a visual-quality paradox: model, task, and even individual-instance performance can improve when images deviate from human-perceived fidelity. Off-the-shelf restoration pipelines fail to reconcile these idiosyncratic preferences. To close the gap, we introduce Visual-Quality Test-Time Tuning (VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable, low-rank kernel before the frozen vision encoder to modulate frequency content; and (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT dynamically adjusts each input image in a single forward pass, aligning it with task-specific model preferences. Across the evaluated MLLMs and all datasets, VQ-TTT lifts significant average accuracy, with no external models, cached features, or extra training data. These findings redefine ``better'' visual inputs for MLLMs and highlight the need for adaptive, rather than universally ``clean'', imagery, in the new era of AI being the main data customer.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u56fe\u50cf\u8d28\u91cf\u4e0e\u4efb\u52a1\u8868\u73b0\u4e4b\u95f4\u5b58\u5728\u89c6\u89c9\u8d28\u91cf\u6096\u8bba\uff0c\u5373\u504f\u79bb\u4eba\u7c7b\u611f\u77e5\u4fdd\u771f\u5ea6\u7684\u56fe\u50cf\u53ef\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u9002\u914d\u6a21\u5757VQ-TTT\uff0c\u52a8\u6001\u8c03\u6574\u8f93\u5165\u56fe\u50cf\u4ee5\u63d0\u5347\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u63a2\u8ba8\u8f93\u5165\u89c6\u89c9\u8d28\u91cf\u5982\u4f55\u5f71\u54cdMLLMs\u7684\u54cd\u5e94\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u4fee\u590d\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u6a21\u578b\u72ec\u7279\u504f\u597d\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faVQ-TTT\u6a21\u5757\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u4f4e\u79e9\u6838\u548cLoRA\u5fae\u8c03\u6d45\u5c42\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u52a8\u6001\u8c03\u6574\u8f93\u5165\u56fe\u50cf\u7684\u9891\u7387\u5185\u5bb9\u3002", "result": "VQ-TTT\u663e\u8457\u63d0\u5347\u4e86\u6240\u6709\u8bc4\u4f30MLLMs\u548c\u6570\u636e\u96c6\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u65e0\u9700\u5916\u90e8\u6a21\u578b\u6216\u989d\u5916\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "\u7814\u7a76\u91cd\u65b0\u5b9a\u4e49\u4e86MLLMs\u7684\u201c\u66f4\u597d\u201d\u89c6\u89c9\u8f93\u5165\uff0c\u5f3a\u8c03\u9700\u8981\u9002\u5e94\u6027\u800c\u975e\u666e\u904d\u201c\u5e72\u51c0\u201d\u7684\u56fe\u50cf\u3002"}}
{"id": "2506.15673", "pdf": "https://arxiv.org/pdf/2506.15673", "abs": "https://arxiv.org/abs/2506.15673", "authors": ["Kai He", "Ruofan Liang", "Jacob Munkberg", "Jon Hasselgren", "Nandita Vijaykumar", "Alexander Keller", "Sanja Fidler", "Igor Gilitschenski", "Zan Gojcic", "Zian Wang"], "title": "UniRelight: Learning Joint Decomposition and Synthesis for Video Relighting", "categories": ["cs.CV"], "comment": "Project page: https://research.nvidia.com/labs/toronto-ai/UniRelight/", "summary": "We address the challenge of relighting a single image or video, a task that demands precise scene intrinsic understanding and high-quality light transport synthesis. Existing end-to-end relighting models are often limited by the scarcity of paired multi-illumination data, restricting their ability to generalize across diverse scenes. Conversely, two-stage pipelines that combine inverse and forward rendering can mitigate data requirements but are susceptible to error accumulation and often fail to produce realistic outputs under complex lighting conditions or with sophisticated materials. In this work, we introduce a general-purpose approach that jointly estimates albedo and synthesizes relit outputs in a single pass, harnessing the generative capabilities of video diffusion models. This joint formulation enhances implicit scene comprehension and facilitates the creation of realistic lighting effects and intricate material interactions, such as shadows, reflections, and transparency. Trained on synthetic multi-illumination data and extensive automatically labeled real-world videos, our model demonstrates strong generalization across diverse domains and surpasses previous methods in both visual fidelity and temporal consistency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u4f30\u8ba1\u53cd\u7167\u7387\u5e76\u5408\u6210\u91cd\u5149\u7167\u8f93\u51fa\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u5229\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u7f3a\u548c\u590d\u6742\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u91cd\u5149\u7167\u6a21\u578b\u56e0\u914d\u5bf9\u591a\u5149\u7167\u6570\u636e\u7a00\u7f3a\u800c\u6cdb\u5316\u80fd\u529b\u53d7\u9650\uff0c\u800c\u4e24\u9636\u6bb5\u6d41\u6c34\u7ebf\u6613\u4ea7\u751f\u8bef\u5dee\u7d2f\u79ef\u4e14\u96be\u4ee5\u5904\u7406\u590d\u6742\u5149\u7167\u6216\u6750\u8d28\u3002", "method": "\u8054\u5408\u4f30\u8ba1\u53cd\u7167\u7387\u5e76\u5408\u6210\u91cd\u5149\u7167\u8f93\u51fa\u7684\u5355\u9636\u6bb5\u65b9\u6cd5\uff0c\u5229\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u589e\u5f3a\u573a\u666f\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u3002", "result": "\u6a21\u578b\u5728\u5408\u6210\u591a\u5149\u7167\u6570\u636e\u548c\u81ea\u52a8\u6807\u6ce8\u7684\u771f\u5b9e\u89c6\u9891\u4e0a\u8bad\u7ec3\uff0c\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u4f18\u5316\u548c\u751f\u6210\u6a21\u578b\u7684\u5e94\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5149\u7167\u4efb\u52a1\u7684\u6548\u679c\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.15675", "pdf": "https://arxiv.org/pdf/2506.15675", "abs": "https://arxiv.org/abs/2506.15675", "authors": ["Zhen Li", "Chuanhao Li", "Xiaofeng Mao", "Shaoheng Lin", "Ming Li", "Shitian Zhao", "Zhaopan Xu", "Xinyue Li", "Yukang Feng", "Jianwen Sun", "Zizhen Li", "Fanrui Zhang", "Jiaxin Ai", "Zhixiang Wang", "Yuwei Wu", "Tong He", "Jiangmiao Pang", "Yu Qiao", "Yunde Jia", "Kaipeng Zhang"], "title": "Sekai: A Video Dataset towards World Exploration", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 6 figures", "summary": "Video generation techniques have made remarkable progress, promising to be the foundation of interactive world exploration. However, existing video generation datasets are not well-suited for world exploration training as they suffer from some limitations: limited locations, short duration, static scenes, and a lack of annotations about exploration and the world. In this paper, we introduce Sekai (meaning ``world'' in Japanese), a high-quality first-person view worldwide video dataset with rich annotations for world exploration. It consists of over 5,000 hours of walking or drone view (FPV and UVA) videos from over 100 countries and regions across 750 cities. We develop an efficient and effective toolbox to collect, pre-process and annotate videos with location, scene, weather, crowd density, captions, and camera trajectories. Experiments demonstrate the quality of the dataset. And, we use a subset to train an interactive video world exploration model, named YUME (meaning ``dream'' in Japanese). We believe Sekai will benefit the area of video generation and world exploration, and motivate valuable applications.", "AI": {"tldr": "Sekai\u662f\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u5168\u7403\u89c6\u9891\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u4e16\u754c\u63a2\u7d22\u8bad\u7ec3\uff0c\u5305\u542b\u4e30\u5bcc\u6807\u6ce8\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6570\u636e\u96c6\u4e0d\u9002\u5408\u4e16\u754c\u63a2\u7d22\u8bad\u7ec3\uff0c\u5b58\u5728\u5730\u70b9\u6709\u9650\u3001\u65f6\u957f\u77ed\u3001\u573a\u666f\u9759\u6001\u548c\u7f3a\u4e4f\u63a2\u7d22\u6807\u6ce8\u7b49\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u9ad8\u6548\u5de5\u5177\u7bb1\uff0c\u6536\u96c6\u3001\u9884\u5904\u7406\u5e76\u6807\u6ce8\u89c6\u9891\uff08\u5305\u62ec\u4f4d\u7f6e\u3001\u573a\u666f\u3001\u5929\u6c14\u7b49\uff09\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b5000\u591a\u5c0f\u65f6\u89c6\u9891\uff0c\u8986\u76d6100\u591a\u4e2a\u56fd\u5bb6\u548c750\u4e2a\u57ce\u5e02\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u8d28\u91cf\u3002", "conclusion": "Sekai\u5c06\u63a8\u52a8\u89c6\u9891\u751f\u6210\u548c\u4e16\u754c\u63a2\u7d22\u9886\u57df\u53d1\u5c55\uff0c\u5e76\u6fc0\u53d1\u6709\u4ef7\u503c\u7684\u5e94\u7528\u3002"}}
{"id": "2506.15682", "pdf": "https://arxiv.org/pdf/2506.15682", "abs": "https://arxiv.org/abs/2506.15682", "authors": ["Anirud Aggarwal", "Abhinav Shrivastava", "Matthew Gwilliam"], "title": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model", "categories": ["cs.CV"], "comment": "29 pages, 22 figures, 9 tables", "summary": "Diffusion-based image generation models excel at producing high-quality synthetic content, but suffer from slow and computationally expensive inference. Prior work has attempted to mitigate this by caching and reusing features within diffusion transformers across inference steps. These methods, however, often rely on rigid heuristics that result in limited acceleration or poor generalization across architectures. We propose Evolutionary Caching to Accelerate Diffusion models (ECAD), a genetic algorithm that learns efficient, per-model, caching schedules forming a Pareto frontier, using only a small set of calibration prompts. ECAD requires no modifications to network parameters or reference images. It offers significant inference speedups, enables fine-grained control over the quality-latency trade-off, and adapts seamlessly to different diffusion models. Notably, ECAD's learned schedules can generalize effectively to resolutions and model variants not seen during calibration. We evaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple metrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k, PartiPrompts), demonstrating consistent improvements over previous approaches. On PixArt-alpha, ECAD identifies a schedule that outperforms the previous state-of-the-art method by 4.47 COCO FID while increasing inference speedup from 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable approach for accelerating diffusion inference. Our project website is available at https://aniaggarwal.github.io/ecad and our code is available at https://github.com/aniaggarwal/ecad.", "AI": {"tldr": "ECAD\u662f\u4e00\u79cd\u57fa\u4e8e\u9057\u4f20\u7b97\u6cd5\u7684\u7f13\u5b58\u8c03\u5ea6\u65b9\u6cd5\uff0c\u663e\u8457\u52a0\u901f\u6269\u6563\u6a21\u578b\u7684\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u6162\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u542f\u53d1\u5f0f\uff0c\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51faECAD\uff0c\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u5b66\u4e60\u9ad8\u6548\u7684\u7f13\u5b58\u8c03\u5ea6\uff0c\u65e0\u9700\u4fee\u6539\u7f51\u7edc\u53c2\u6570\u6216\u53c2\u8003\u56fe\u50cf\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cECAD\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\uff08\u5982PixArt-alpha\u52a0\u901f\u81f32.58x\uff09\u5e76\u6539\u5584\u751f\u6210\u8d28\u91cf\uff08COCO FID\u63d0\u53474.47\uff09\u3002", "conclusion": "ECAD\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u901a\u7528\u7684\u6269\u6563\u6a21\u578b\u52a0\u901f\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u6a21\u578b\u548c\u5206\u8fa8\u7387\u3002"}}
{"id": "2506.14803", "pdf": "https://arxiv.org/pdf/2506.14803", "abs": "https://arxiv.org/abs/2506.14803", "authors": ["Arbind Agrahari Baniya", "Tsz-Kwan Lee", "Peter W. Eklund", "Sunil Aryal"], "title": "Omnidirectional Video Super-Resolution using Deep Learning", "categories": ["cs.MM", "cs.CV", "cs.LG"], "comment": null, "summary": "Omnidirectional Videos (or 360{\\deg} videos) are widely used in Virtual Reality (VR) to facilitate immersive and interactive viewing experiences. However, the limited spatial resolution in 360{\\deg} videos does not allow for each degree of view to be represented with adequate pixels, limiting the visual quality offered in the immersive experience. Deep learning Video Super-Resolution (VSR) techniques used for conventional videos could provide a promising software-based solution; however, these techniques do not tackle the distortion present in equirectangular projections of 360{\\deg} video signals. An additional obstacle is the limited availability of 360{\\deg} video datasets for study. To address these issues, this paper creates a novel 360{\\deg} Video Dataset (360VDS) with a study of the extensibility of conventional VSR models to 360{\\deg} videos. This paper further proposes a novel deep learning model for 360{\\deg} Video Super-Resolution (360{\\deg} VSR), called Spherical Signal Super-resolution with a Proportioned Optimisation (S3PO). S3PO adopts recurrent modelling with an attention mechanism, unbound from conventional VSR techniques like alignment. With a purpose-built feature extractor and a novel loss function addressing spherical distortion, S3PO outperforms most state-of-the-art conventional VSR models and 360{\\deg}~specific super-resolution models on 360{\\deg} video datasets. A step-wise ablation study is presented to understand and demonstrate the impact of the chosen architectural sub-components, targeted training and optimisation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9360\u5ea6\u89c6\u9891\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5S3PO\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfVSR\u6280\u672f\u5728\u5904\u7406\u7403\u9762\u5931\u771f\u65f6\u7684\u4e0d\u8db3\uff0c\u5e76\u521b\u5efa\u4e86\u65b0\u7684360VDS\u6570\u636e\u96c6\u3002", "motivation": "360\u5ea6\u89c6\u9891\u5728VR\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5206\u8fa8\u7387\u4e0d\u8db3\u5bfc\u81f4\u89c6\u89c9\u8d28\u91cf\u53d7\u9650\uff0c\u4e14\u7f3a\u4e4f\u9002\u7528\u7684\u6570\u636e\u96c6\u548c\u9488\u5bf9\u7403\u9762\u5931\u771f\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u3002", "method": "\u63d0\u51faS3PO\u6a21\u578b\uff0c\u91c7\u7528\u5faa\u73af\u5efa\u6a21\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408\u4e13\u7528\u7279\u5f81\u63d0\u53d6\u5668\u548c\u9488\u5bf9\u7403\u9762\u5931\u771f\u7684\u635f\u5931\u51fd\u6570\u3002", "result": "S3PO\u5728360\u5ea6\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709VSR\u548c360\u5ea6\u4e13\u7528\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u3002", "conclusion": "S3PO\u901a\u8fc7\u9488\u5bf9\u6027\u8bbe\u8ba1\u548c\u4f18\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86360\u5ea6\u89c6\u9891\u7684\u8d85\u5206\u8fa8\u7387\u6548\u679c\u3002"}}
{"id": "2506.15312", "pdf": "https://arxiv.org/pdf/2506.15312", "abs": "https://arxiv.org/abs/2506.15312", "authors": ["Han Wu", "Junyao Li", "Kangbo Zhao", "Sen Zhang", "Yukai Shi", "Liang Lin"], "title": "One-shot Face Sketch Synthesis in the Wild via Generative Diffusion Prior and Instruction Tuning", "categories": ["cs.GR", "cs.CR", "cs.CV", "cs.CY"], "comment": "We propose a novel framework for face sketch synthesis, where merely   a single pair of samples suffices to enable in-the-wild face sketch synthesis", "summary": "Face sketch synthesis is a technique aimed at converting face photos into sketches. Existing face sketch synthesis research mainly relies on training with numerous photo-sketch sample pairs from existing datasets. However, these large-scale discriminative learning methods will have to face problems such as data scarcity and high human labor costs. Once the training data becomes scarce, their generative performance significantly degrades. In this paper, we propose a one-shot face sketch synthesis method based on diffusion models. We optimize text instructions on a diffusion model using face photo-sketch image pairs. Then, the instructions derived through gradient-based optimization are used for inference. To simulate real-world scenarios more accurately and evaluate method effectiveness more comprehensively, we introduce a new benchmark named One-shot Face Sketch Dataset (OS-Sketch). The benchmark consists of 400 pairs of face photo-sketch images, including sketches with different styles and photos with different backgrounds, ages, sexes, expressions, illumination, etc. For a solid out-of-distribution evaluation, we select only one pair of images for training at each time, with the rest used for inference. Extensive experiments demonstrate that the proposed method can convert various photos into realistic and highly consistent sketches in a one-shot context. Compared to other methods, our approach offers greater convenience and broader applicability. The dataset will be available at: https://github.com/HanWu3125/OS-Sketch", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5355\u6b21\u4eba\u8138\u7d20\u63cf\u5408\u6210\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u9ad8\u4eba\u529b\u6210\u672c\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6OS-Sketch\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u5224\u522b\u5b66\u4e60\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u7f3a\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e14\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u3002", "method": "\u901a\u8fc7\u4f18\u5316\u6269\u6563\u6a21\u578b\u7684\u6587\u672c\u6307\u4ee4\uff0c\u5229\u7528\u68af\u5ea6\u4f18\u5316\u540e\u7684\u6307\u4ee4\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5355\u6b21\u8bad\u7ec3\u4e0b\u80fd\u751f\u6210\u903c\u771f\u4e14\u4e00\u81f4\u7684\u7d20\u63cf\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u4fbf\u5229\u6027\u548c\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5355\u6b21\u5b66\u4e60\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2506.15395", "pdf": "https://arxiv.org/pdf/2506.15395", "abs": "https://arxiv.org/abs/2506.15395", "authors": ["Yu Xing", "Shishi Huang", "Meng Lv", "Guo Chen", "Huailiang Wang", "Lingzhi Sui"], "title": "A Real-time Endoscopic Image Denoising System", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Endoscopes featuring a miniaturized design have significantly enhanced operational flexibility, portability, and diagnostic capability while substantially reducing the invasiveness of medical procedures. Recently, single-use endoscopes equipped with an ultra-compact analogue image sensor measuring less than 1mm x 1mm bring revolutionary advancements to medical diagnosis. They reduce the structural redundancy and large capital expenditures associated with reusable devices, eliminate the risk of patient infections caused by inadequate disinfection, and alleviate patient suffering. However, the limited photosensitive area results in reduced photon capture per pixel, requiring higher photon sensitivity settings to maintain adequate brightness. In high-contrast medical imaging scenarios, the small-sized sensor exhibits a constrained dynamic range, making it difficult to simultaneously capture details in both highlights and shadows, and additional localized digital gain is required to compensate. Moreover, the simplified circuit design and analog signal transmission introduce additional noise sources. These factors collectively contribute to significant noise issues in processed endoscopic images. In this work, we developed a comprehensive noise model for analog image sensors in medical endoscopes, addressing three primary noise types: fixed-pattern noise, periodic banding noise, and mixed Poisson-Gaussian noise. Building on this analysis, we propose a hybrid denoising system that synergistically combines traditional image processing algorithms with advanced learning-based techniques for captured raw frames from sensors. Experiments demonstrate that our approach effectively reduces image noise without fine detail loss or color distortion, while achieving real-time performance on FPGA platforms and an average PSNR improvement from 21.16 to 33.05 on our test dataset.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u533b\u7597\u5185\u7aa5\u955c\u4e2d\u6a21\u62df\u56fe\u50cf\u4f20\u611f\u5668\u7684\u6df7\u5408\u53bb\u566a\u7cfb\u7edf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u566a\u58f0\u95ee\u9898\u5e76\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u5c0f\u578b\u5316\u5185\u7aa5\u955c\u4f20\u611f\u5668\u56e0\u611f\u5149\u9762\u79ef\u6709\u9650\u5bfc\u81f4\u566a\u58f0\u95ee\u9898\u4e25\u91cd\uff0c\u5f71\u54cd\u56fe\u50cf\u8d28\u91cf\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u53bb\u566a\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u566a\u58f0\u6a21\u578b\uff0c\u7ed3\u5408\u4f20\u7edf\u56fe\u50cf\u5904\u7406\u7b97\u6cd5\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u6280\u672f\uff0c\u63d0\u51fa\u6df7\u5408\u53bb\u566a\u7cfb\u7edf\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u7cfb\u7edf\u6709\u6548\u964d\u566a\uff0cPSNR\u4ece21.16\u63d0\u5347\u81f333.05\uff0c\u4e14\u80fd\u5728FPGA\u5e73\u53f0\u4e0a\u5b9e\u65f6\u8fd0\u884c\u3002", "conclusion": "\u6df7\u5408\u53bb\u566a\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u5185\u7aa5\u955c\u56fe\u50cf\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u533b\u7597\u5e94\u7528\u3002"}}
