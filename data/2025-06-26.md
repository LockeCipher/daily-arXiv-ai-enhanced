<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 3]
- [cs.CV](#cs.CV) [Total: 12]
- [cs.LG](#cs.LG) [Total: 2]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [RaRa Clipper: A Clipper for Gaussian Splatting Based on Ray Tracer and Rasterizer](https://arxiv.org/abs/2506.20202)
*Da Li,Donggang Jia,Yousef Rajeh,Dominik Engel,Ivan Viola*

Main category: cs.GR

TL;DR: 提出了一种结合光栅化和光线追踪的混合渲染框架，用于高效且高保真地裁剪高斯泼溅数据。


<details>
  <summary>Details</summary>
Motivation: 高斯泼溅技术的进步带来了大量基于此表示的数据集，但因其体积特性，精确裁剪仍是一个未解决的挑战。

Method: 采用RaRa策略，先用光栅化快速识别被裁剪平面相交的高斯，再用光线追踪计算部分遮挡的衰减权重，以准确估计贡献。

Result: 在多种数据集上验证，实验结果表明方法在视觉上表现优越，同时保持实时渲染性能和高保真度。

Conclusion: 该方法实现了平滑连续的裁剪效果，解决了高斯泼溅数据的精确裁剪问题。

Abstract: With the advancement of Gaussian Splatting techniques, a growing number of datasets based on this representation have been developed. However, performing accurate and efficient clipping for Gaussian Splatting remains a challenging and unresolved problem, primarily due to the volumetric nature of Gaussian primitives, which makes hard clipping incapable of precisely localizing their pixel-level contributions. In this paper, we propose a hybrid rendering framework that combines rasterization and ray tracing to achieve efficient and high-fidelity clipping of Gaussian Splatting data. At the core of our method is the RaRa strategy, which first leverages rasterization to quickly identify Gaussians intersected by the clipping plane, followed by ray tracing to compute attenuation weights based on their partial occlusion. These weights are then used to accurately estimate each Gaussian's contribution to the final image, enabling smooth and continuous clipping effects. We validate our approach on diverse datasets, including general Gaussians, hair strand Gaussians, and multi-layer Gaussians, and conduct user studies to evaluate both perceptual quality and quantitative performance. Experimental results demonstrate that our method delivers visually superior results while maintaining real-time rendering performance and preserving high fidelity in the unclipped regions.

</details>


### [2] [DreamAnywhere: Object-Centric Panoramic 3D Scene Generation](https://arxiv.org/abs/2506.20367)
*Edoardo Alberto Dominici,Jozef Hladky,Floor Verhoeven,Lukas Radl,Thomas Deixelberger,Stefan Ainetter,Philipp Drescher,Stefan Hauswiesner,Arno Coomans,Giacomo Nazzaro,Konstantinos Vardis,Markus Steinberger*

Main category: cs.GR

TL;DR: DreamAnywhere是一个模块化系统，用于快速生成和原型化3D场景，解决了现有方法在视觉保真度、场景理解和多环境适应性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有文本到3D场景生成方法存在场景单一、视觉保真度低、场景理解有限且仅适用于特定环境的问题，DreamAnywhere旨在解决这些问题。

Method: 系统通过生成360°全景图像，分解为背景和对象，通过混合修复构建完整3D表示，并将对象掩码提升为详细3D对象，支持沉浸式导航和对象级编辑。

Result: DreamAnywhere在新视角合成一致性和图像质量上显著优于现有方法，用户研究显示其技术稳健性和实用性。

Conclusion: DreamAnywhere为低成本电影制作等场景提供了快速迭代和高效原型化的解决方案，模块化设计使其具有高度可定制性。

Abstract: Recent advances in text-to-3D scene generation have demonstrated significant potential to transform content creation across multiple industries. Although the research community has made impressive progress in addressing the challenges of this complex task, existing methods often generate environments that are only front-facing, lack visual fidelity, exhibit limited scene understanding, and are typically fine-tuned for either indoor or outdoor settings. In this work, we address these issues and propose DreamAnywhere, a modular system for the fast generation and prototyping of 3D scenes. Our system synthesizes a 360{\deg} panoramic image from text, decomposes it into background and objects, constructs a complete 3D representation through hybrid inpainting, and lifts object masks to detailed 3D objects that are placed in the virtual environment. DreamAnywhere supports immersive navigation and intuitive object-level editing, making it ideal for scene exploration, visual mock-ups, and rapid prototyping -- all with minimal manual modeling. These features make our system particularly suitable for low-budget movie production, enabling quick iteration on scene layout and visual tone without the overhead of traditional 3D workflows. Our modular pipeline is highly customizable as it allows components to be replaced independently. Compared to current state-of-the-art text and image-based 3D scene generation approaches, DreamAnywhere shows significant improvements in coherence in novel view synthesis and achieves competitive image quality, demonstrating its effectiveness across diverse and challenging scenarios. A comprehensive user study demonstrates a clear preference for our method over existing approaches, validating both its technical robustness and practical usefulness.

</details>


### [3] [EditP23: 3D Editing via Propagation of Image Prompts to Multi-View](https://arxiv.org/abs/2506.20652)
*Roi Bar-On,Dana Cohen-Bar,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: EditP23是一种无需掩码的3D编辑方法，通过2D图像编辑传播到多视角表示，实现3D一致性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖文本提示或显式空间掩码，而EditP23通过一对图像（原始视图和用户编辑后的视图）实现直观编辑。

Method: 利用预训练的多视角扩散模型的潜在空间中的编辑感知流，以图像提示为指导，实现跨视图的一致性编辑。

Result: 方法无需优化，前馈操作，保持原始对象的结构和外观，适用于多种对象类别和编辑场景。

Conclusion: EditP23在保持高保真度的同时，无需手动掩码，展示了高效且直观的3D编辑能力。

Abstract: We present EditP23, a method for mask-free 3D editing that propagates 2D image edits to multi-view representations in a 3D-consistent manner. In contrast to traditional approaches that rely on text-based prompting or explicit spatial masks, EditP23 enables intuitive edits by conditioning on a pair of images: an original view and its user-edited counterpart. These image prompts are used to guide an edit-aware flow in the latent space of a pre-trained multi-view diffusion model, allowing the edit to be coherently propagated across views. Our method operates in a feed-forward manner, without optimization, and preserves the identity of the original object, in both structure and appearance. We demonstrate its effectiveness across a range of object categories and editing scenarios, achieving high fidelity to the source while requiring no manual masks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos](https://arxiv.org/abs/2506.20103)
*Jiahao Lin,Weixuan Peng,Bojia Zi,Yifeng Gao,Xianbiao Qi,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 论文介绍了BrokenVideos数据集，用于AI生成视频中的视觉伪影定位，填补了现有基准的不足，并展示了其在提升检测模型性能方面的有效性。


<details>
  <summary>Details</summary>
Motivation: AI生成视频中存在视觉伪影（如运动不一致、不自然的变形等），但缺乏专门用于伪影定位的基准数据集，限制了相关研究的进展。

Method: 提出BrokenVideos数据集，包含3,254个AI生成视频，带有像素级标注的伪影区域，并通过人工验证确保标注质量。

Result: 实验表明，使用BrokenVideos训练模型显著提升了伪影定位能力。

Conclusion: BrokenVideos为生成视频模型的伪影定位研究提供了重要基准，推动了该领域的进步。

Abstract: Recent advances in deep generative models have led to significant progress in video generation, yet the fidelity of AI-generated videos remains limited. Synthesized content often exhibits visual artifacts such as temporally inconsistent motion, physically implausible trajectories, unnatural object deformations, and local blurring that undermine realism and user trust. Accurate detection and spatial localization of these artifacts are crucial for both automated quality control and for guiding the development of improved generative models. However, the research community currently lacks a comprehensive benchmark specifically designed for artifact localization in AI generated videos. Existing datasets either restrict themselves to video or frame level detection or lack the fine-grained spatial annotations necessary for evaluating localization methods. To address this gap, we introduce BrokenVideos, a benchmark dataset of 3,254 AI-generated videos with meticulously annotated, pixel-level masks highlighting regions of visual corruption. Each annotation is validated through detailed human inspection to ensure high quality ground truth. Our experiments show that training state of the art artifact detection models and multi modal large language models (MLLMs) on BrokenVideos significantly improves their ability to localize corrupted regions. Through extensive evaluation, we demonstrate that BrokenVideos establishes a critical foundation for benchmarking and advancing research on artifact localization in generative video models. The dataset is available at: https://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/.

</details>


### [5] [EAR: Erasing Concepts from Unified Autoregressive Models](https://arxiv.org/abs/2506.20151)
*Haipeng Fan,Shiyuan Zhang,Baohunesitu,Zihang Guo,Huaiwen Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为EAR的微调方法，用于在自回归模型中有效且保留效用的概念擦除。通过WGA和TLM策略，以及新基准ECGVF，实验证明EAR在擦除效果和模型效用保持上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在视觉理解和图像生成任务中表现优异，但如何在保持生成质量的同时去除不想要的概念仍是一个挑战。

Method: 提出EAR方法，包括WGA策略和TLM策略，并引入ECGVF基准用于评估概念擦除效果。

Result: 在Janus-Pro模型上的实验表明，EAR在概念擦除和模型效用保持上均有显著改进。

Conclusion: EAR为自回归模型中的概念擦除提供了一种有效且实用的解决方案。

Abstract: Autoregressive (AR) models have achieved unified and strong performance across both visual understanding and image generation tasks. However, removing undesired concepts from AR models while maintaining overall generation quality remains an open challenge. In this paper, we propose Erasure Autoregressive Model (EAR), a fine-tuning method for effective and utility-preserving concept erasure in AR models. Specifically, we introduce Windowed Gradient Accumulation (WGA) strategy to align patch-level decoding with erasure objectives, and Thresholded Loss Masking (TLM) strategy to protect content unrelated to the target concept during fine-tuning. Furthermore, we propose a novel benchmark, Erase Concept Generator and Visual Filter (ECGVF), aim at provide a more rigorous and comprehensive foundation for evaluating concept erasure in AR models. Specifically, we first employ structured templates across diverse large language models (LLMs) to pre-generate a large-scale corpus of target-replacement concept prompt pairs. Subsequently, we generate images from these prompts and subject them to rigorous filtering via a visual classifier to ensure concept fidelity and alignment. Extensive experimental results conducted on the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR achieves marked improvements in both erasure effectiveness and model utility preservation. Code is available at: https://github.com/immc-lab/ear/

</details>


### [6] [Dynamic Bandwidth Allocation for Hybrid Event-RGB Transmission](https://arxiv.org/abs/2506.20222)
*Pujing Yang,Guangyi Zhang,Yunlong Cai,Lei Yu,Guanding Yu*

Main category: cs.CV

TL;DR: 提出了一种联合事件和图像传输框架，通过贝叶斯建模和信息瓶颈方法消除冗余，优化带宽利用，同时实现实时去模糊。


<details>
  <summary>Details</summary>
Motivation: 解决混合系统中事件和RGB图像传输的冗余问题，优化带宽利用并提升重建质量。

Method: 采用贝叶斯建模和信息瓶颈方法，分离共享和领域特定信息，动态分配传输带宽。

Result: 仿真结果表明，该方案在重建质量和去模糊性能上优于传统系统。

Conclusion: 提出的框架有效优化了带宽利用，同时提升了重建和去模糊性能。

Abstract: Event cameras asynchronously capture pixel-level intensity changes with extremely low latency. They are increasingly used in conjunction with RGB cameras for a wide range of vision-related applications. However, a major challenge in these hybrid systems lies in the transmission of the large volume of triggered events and RGB images. To address this, we propose a transmission scheme that retains efficient reconstruction performance of both sources while accomplishing real-time deblurring in parallel. Conventional RGB cameras and event cameras typically capture the same scene in different ways, often resulting in significant redundant information across their outputs. To address this, we develop a joint event and image (E-I) transmission framework to eliminate redundancy and thereby optimize channel bandwidth utilization. Our approach employs Bayesian modeling and the information bottleneck method to disentangle the shared and domain-specific information within the E-I inputs. This disentangled information bottleneck framework ensures both the compactness and informativeness of extracted shared and domain-specific information. Moreover, it adaptively allocates transmission bandwidth based on scene dynamics, i.e., more symbols are allocated to events for dynamic details or to images for static information. Simulation results demonstrate that the proposed scheme not only achieves superior reconstruction quality compared to conventional systems but also delivers enhanced deblurring performance.

</details>


### [7] [Recognizing Surgical Phases Anywhere: Few-Shot Test-time Adaptation and Task-graph Guided Refinement](https://arxiv.org/abs/2506.20254)
*Kun Yuan,Tingxuan Chen,Shi Li,Joel L. Lavanchy,Christian Heiliger,Ege Özsoy,Yiming Huang,Long Bai,Nassir Navab,Vinkle Srivastav,Hongliang Ren,Nicolas Padoy*

Main category: cs.CV

TL;DR: SPA是一个轻量级框架，通过少量标注和自然语言定义手术阶段，实现跨机构和跨手术的工作流理解，性能优于全标注模型。


<details>
  <summary>Details</summary>
Motivation: 解决手术工作流多样性带来的模型泛化性差问题，提升在未见手术环境中的适应性。

Method: SPA结合少样本空间适应、扩散建模和动态测试时适应，利用多模态嵌入和任务图先验。

Result: SPA在少样本手术阶段识别中表现最优，甚至优于全标注模型。

Conclusion: SPA为医院提供了一种快速定制手术阶段识别模型的轻量级解决方案。

Abstract: The complexity and diversity of surgical workflows, driven by heterogeneous operating room settings, institutional protocols, and anatomical variability, present a significant challenge in developing generalizable models for cross-institutional and cross-procedural surgical understanding. While recent surgical foundation models pretrained on large-scale vision-language data offer promising transferability, their zero-shot performance remains constrained by domain shifts, limiting their utility in unseen surgical environments. To address this, we introduce Surgical Phase Anywhere (SPA), a lightweight framework for versatile surgical workflow understanding that adapts foundation models to institutional settings with minimal annotation. SPA leverages few-shot spatial adaptation to align multi-modal embeddings with institution-specific surgical scenes and phases. It also ensures temporal consistency through diffusion modeling, which encodes task-graph priors derived from institutional procedure protocols. Finally, SPA employs dynamic test-time adaptation, exploiting the mutual agreement between multi-modal phase prediction streams to adapt the model to a given test video in a self-supervised manner, enhancing the reliability under test-time distribution shifts. SPA is a lightweight adaptation framework, allowing hospitals to rapidly customize phase recognition models by defining phases in natural language text, annotating a few images with the phase labels, and providing a task graph defining phase transitions. The experimental results show that the SPA framework achieves state-of-the-art performance in few-shot surgical phase recognition across multiple institutions and procedures, even outperforming full-shot models with 32-shot labeled data. Code is available at https://github.com/CAMMA-public/SPA

</details>


### [8] [Ctrl-Z Sampling: Diffusion Sampling with Controlled Random Zigzag Explorations](https://arxiv.org/abs/2506.20294)
*Shunqi Mao,Wei Guo,Chaoyi Zhang,Weidong Cai*

Main category: cs.CV

TL;DR: 提出了一种名为Ctrl-Z采样的新方法，用于在扩散模型中检测并逃离局部最优解，提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在条件生成中表现出色，但容易陷入局部最优解，导致全局不一致或条件不匹配。

Method: 通过奖励模型识别局部最优解，注入噪声并回退到更早状态，动态交替前向优化和后向探索。

Result: 实验表明，Ctrl-Z采样显著提升了生成质量，仅增加约7.6倍函数评估。

Conclusion: Ctrl-Z采样是一种模型无关的方法，适用于现有扩散框架，有效解决了局部最优问题。

Abstract: Diffusion models have shown strong performance in conditional generation by progressively denoising Gaussian noise toward a target data distribution. This denoising process can be interpreted as a form of hill climbing in a learned latent space, where the model iteratively refines the sample toward regions of higher probability. However, diffusion models often converge to local optima that are locally visually coherent yet globally inconsistent or conditionally misaligned, due to latent space complexity and suboptimal initialization. Prior efforts attempted to address this by strengthening guidance signals or manipulating the initial noise distribution. We introduce Controlled Random Zigzag Sampling (Ctrl-Z Sampling), a novel sampling strategy designed to detect and escape such local maxima during conditional generation. The method first identifies potential local maxima using a reward model. Upon detection, it injects noise and reverts to a previous, noisier state to escape the current optimization plateau. The reward model then evaluates candidate trajectories, accepting only those that offer improvement, while progressively deeper retreat enables stronger escapes when nearby alternatives fail. This controlled random zigzag process allows dynamic alternation between forward refinement and backward exploration, enhancing both alignment and visual quality in the generated outputs. The proposed Ctrl-Z Sampling is model-agnostic and compatible with existing diffusion frameworks. Experimental results show that Ctrl-Z Sampling substantially improves generation quality with only around 7.6X increase in function evaluations.

</details>


### [9] [TDiR: Transformer based Diffusion for Image Restoration Tasks](https://arxiv.org/abs/2506.20302)
*Abbas Anwar,Mohammad Shullar,Ali Arshad Nasir,Mudassir Masood,Saeed Anwar*

Main category: cs.CV

TL;DR: 论文提出了一种基于Transformer的扩散模型，用于图像恢复任务，在多个质量指标上优于现有深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 挑战性环境中捕获的图像常因噪声、色偏、模糊和光散射等问题质量下降，影响下游任务的应用。

Method: 开发了一种结合Transformer的扩散模型，用于图像恢复任务，并在公开数据集上评估其性能。

Result: 模型在图像增强、去噪和去雨等任务中表现优于现有方法。

Conclusion: 扩散模型与Transformer结合能有效提升退化图像质量，增强其在需要高保真视觉数据的下游任务中的实用性。

Abstract: Images captured in challenging environments often experience various forms of degradation, including noise, color cast, blur, and light scattering. These effects significantly reduce image quality, hindering their applicability in downstream tasks such as object detection, mapping, and classification. Our transformer-based diffusion model was developed to address image restoration tasks, aiming to improve the quality of degraded images. This model was evaluated against existing deep learning methodologies across multiple quality metrics for underwater image enhancement, denoising, and deraining on publicly available datasets. Our findings demonstrate that the diffusion model, combined with transformers, surpasses current methods in performance. The results of our model highlight the efficacy of diffusion models and transformers in improving the quality of degraded images, consequently expanding their utility in downstream tasks that require high-fidelity visual data.

</details>


### [10] [Med-Art: Diffusion Transformer for 2D Medical Text-to-Image Generation](https://arxiv.org/abs/2506.20449)
*Changlu Guo,Anders Nymark Christensen,Morten Rieger Hannemose*

Main category: cs.CV

TL;DR: Med-Art是一个针对小规模医学数据设计的文本到图像生成框架，通过结合视觉语言模型和预训练模型PixArt-α，解决了医学图像生成中的文本数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 医学图像生成面临数据集小和文本数据稀缺的挑战，需要一种高效的方法。

Method: Med-Art利用视觉语言模型生成医学图像描述，并基于PixArt-α模型进行微调，提出HLDF方法优化像素级损失。

Result: 在两个医学图像数据集上取得了FID、KID和分类性能的最优结果。

Conclusion: Med-Art在有限数据下实现了高性能医学图像生成，为医学领域提供了实用工具。

Abstract: Text-to-image generative models have achieved remarkable breakthroughs in recent years. However, their application in medical image generation still faces significant challenges, including small dataset sizes, and scarcity of medical textual data. To address these challenges, we propose Med-Art, a framework specifically designed for medical image generation with limited data. Med-Art leverages vision-language models to generate visual descriptions of medical images which overcomes the scarcity of applicable medical textual data. Med-Art adapts a large-scale pre-trained text-to-image model, PixArt-$\alpha$, based on the Diffusion Transformer (DiT), achieving high performance under limited data. Furthermore, we propose an innovative Hybrid-Level Diffusion Fine-tuning (HLDF) method, which enables pixel-level losses, effectively addressing issues such as overly saturated colors. We achieve state-of-the-art performance on two medical image datasets, measured by FID, KID, and downstream classification performance.

</details>


### [11] [HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based Diffusion Sampling](https://arxiv.org/abs/2506.20452)
*Tobias Vontobel,Seyedmorteza Sadat,Farnood Salehi,Romann M. Weber*

Main category: cs.CV

TL;DR: HiWave是一种无需训练、零样本的方法，通过两阶段流程（基础图像生成和小波增强）提升超高分辨率图像合成的视觉保真度和结构一致性。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型在高分辨率图像合成中的计算成本高和生成图像时出现的伪影问题。

Method: 采用两阶段流程：1) 生成基础图像；2) 通过DDIM反演和小波增强模块提升细节。

Result: HiWave显著减少了伪影，在用户研究中80%的情况下优于现有方法。

Conclusion: HiWave为无需重新训练或修改架构的高质量超高分辨率图像合成提供了有效解决方案。

Abstract: Diffusion models have emerged as the leading approach for image synthesis, demonstrating exceptional photorealism and diversity. However, training diffusion models at high resolutions remains computationally prohibitive, and existing zero-shot generation techniques for synthesizing images beyond training resolutions often produce artifacts, including object duplication and spatial incoherence. In this paper, we introduce HiWave, a training-free, zero-shot approach that substantially enhances visual fidelity and structural coherence in ultra-high-resolution image synthesis using pretrained diffusion models. Our method employs a two-stage pipeline: generating a base image from the pretrained model followed by a patch-wise DDIM inversion step and a novel wavelet-based detail enhancer module. Specifically, we first utilize inversion methods to derive initial noise vectors that preserve global coherence from the base image. Subsequently, during sampling, our wavelet-domain detail enhancer retains low-frequency components from the base image to ensure structural consistency, while selectively guiding high-frequency components to enrich fine details and textures. Extensive evaluations using Stable Diffusion XL demonstrate that HiWave effectively mitigates common visual artifacts seen in prior methods, achieving superior perceptual quality. A user study confirmed HiWave's performance, where it was preferred over the state-of-the-art alternative in more than 80% of comparisons, highlighting its effectiveness for high-quality, ultra-high-resolution image synthesis without requiring retraining or architectural modifications.

</details>


### [12] [WonderFree: Enhancing Novel View Quality and Cross-View Consistency for 3D Scene Exploration](https://arxiv.org/abs/2506.20590)
*Chaojun Ni,Jie Li,Haoyun Li,Hengyu Liu,Xiaofeng Wang,Zheng Zhu,Guosheng Zhao,Boyuan Wang,Chenxin Li,Guan Huang,Wenjun Mei*

Main category: cs.CV

TL;DR: WonderFree是一个交互式3D场景生成模型，解决了现有方法在探索性和视角一致性上的不足，通过WorldRestorer和ConsistView技术提升渲染质量和多视角一致性。


<details>
  <summary>Details</summary>
Motivation: 当前3D生成方法在探索性上受限，无法在大范围视角变化下保持高质量渲染，尤其是在未观察区域。

Method: 提出WonderFree模型，分解问题为视角质量和一致性两个子问题，分别通过WorldRestorer（视频修复模型）和ConsistView（多视角联合修复机制）解决。

Result: 实验表明WonderFree显著提升了渲染质量和全局一致性，用户偏好率达77.20%。

Conclusion: WonderFree为3D场景探索提供了高质量和一致性的解决方案，代码和模型将公开。

Abstract: Interactive 3D scene generation from a single image has gained significant attention due to its potential to create immersive virtual worlds. However, a key challenge in current 3D generation methods is the limited explorability, which cannot render high-quality images during larger maneuvers beyond the original viewpoint, particularly when attempting to move forward into unseen areas. To address this challenge, we propose WonderFree, the first model that enables users to interactively generate 3D worlds with the freedom to explore from arbitrary angles and directions. Specifically, we decouple this challenge into two key subproblems: novel view quality, which addresses visual artifacts and floating issues in novel views, and cross-view consistency, which ensures spatial consistency across different viewpoints. To enhance rendering quality in novel views, we introduce WorldRestorer, a data-driven video restoration model designed to eliminate floaters and artifacts. In addition, a data collection pipeline is presented to automatically gather training data for WorldRestorer, ensuring it can handle scenes with varying styles needed for 3D scene generation. Furthermore, to improve cross-view consistency, we propose ConsistView, a multi-view joint restoration mechanism that simultaneously restores multiple perspectives while maintaining spatiotemporal coherence. Experimental results demonstrate that WonderFree not only enhances rendering quality across diverse viewpoints but also significantly improves global coherence and consistency. These improvements are confirmed by CLIP-based metrics and a user study showing a 77.20% preference for WonderFree over WonderWorld enabling a seamless and immersive 3D exploration experience. The code, model, and data will be publicly available.

</details>


### [13] [Video Perception Models for 3D Scene Synthesis](https://arxiv.org/abs/2506.20601)
*Rui Huang,Guangyao Zhai,Zuria Bauer,Marc Pollefeys,Federico Tombari,Leonidas Guibas,Gao Huang,Francis Engelmann*

Main category: cs.CV

TL;DR: VIPScene利用视频生成模型的3D常识知识，结合文本和图像提示，实现高真实感和结构一致性的3D场景合成。


<details>
  <summary>Details</summary>
Motivation: 传统3D场景合成需要专家知识和大量手动工作，自动化此过程可广泛应用于建筑设计、机器人模拟等领域。现有方法（如LLM或图像生成模型）在3D空间推理或多视角一致性上存在局限。

Method: VIPScene整合视频生成、前馈3D重建和开放词汇感知模型，通过语义和几何分析实现场景合成，并引入FPVScore评估一致性和合理性。

Result: 实验表明VIPScene显著优于现有方法，且能泛化到多样场景。

Conclusion: VIPScene通过视频生成模型的3D常识知识，解决了现有方法的局限性，实现了高真实感和一致性的3D场景合成。

Abstract: Traditionally, 3D scene synthesis requires expert knowledge and significant manual effort. Automating this process could greatly benefit fields such as architectural design, robotics simulation, virtual reality, and gaming. Recent approaches to 3D scene synthesis often rely on the commonsense reasoning of large language models (LLMs) or strong visual priors of modern image generation models. However, current LLMs demonstrate limited 3D spatial reasoning ability, which restricts their ability to generate realistic and coherent 3D scenes. Meanwhile, image generation-based methods often suffer from constraints in viewpoint selection and multi-view inconsistencies. In this work, we present Video Perception models for 3D Scene synthesis (VIPScene), a novel framework that exploits the encoded commonsense knowledge of the 3D physical world in video generation models to ensure coherent scene layouts and consistent object placements across views. VIPScene accepts both text and image prompts and seamlessly integrates video generation, feedforward 3D reconstruction, and open-vocabulary perception models to semantically and geometrically analyze each object in a scene. This enables flexible scene synthesis with high realism and structural consistency. For more precise analysis, we further introduce First-Person View Score (FPVScore) for coherence and plausibility evaluation, utilizing continuous first-person perspective to capitalize on the reasoning ability of multimodal large language models. Extensive experiments show that VIPScene significantly outperforms existing methods and generalizes well across diverse scenarios. The code will be released.

</details>


### [14] [Shape2Animal: Creative Animal Generation from Natural Silhouettes](https://arxiv.org/abs/2506.20616)
*Quoc-Duy Tran,Anh-Tuan Vo,Dinh-Khoi Vo,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: Shape2Animal框架通过重新解释自然物体轮廓（如云、石头或火焰）为动物形态，模拟人类的pareidolia现象。


<details>
  <summary>Details</summary>
Motivation: 模仿人类在模糊刺激中感知有意义模式的能力，为视觉叙事、教育内容和数字艺术提供新机会。

Method: 使用开放词汇分割提取物体轮廓，结合视觉语言模型解释动物概念，利用文本到图像扩散模型合成动物图像并融入原始场景。

Result: 在多样化真实输入上验证了框架的鲁棒性和创造力。

Conclusion: Shape2Animal为视觉叙事、教育内容和交互媒体设计提供了新可能性。

Abstract: Humans possess a unique ability to perceive meaningful patterns in ambiguous stimuli, a cognitive phenomenon known as pareidolia. This paper introduces Shape2Animal framework to mimics this imaginative capacity by reinterpreting natural object silhouettes, such as clouds, stones, or flames, as plausible animal forms. Our automated framework first performs open-vocabulary segmentation to extract object silhouette and interprets semantically appropriate animal concepts using vision-language models. It then synthesizes an animal image that conforms to the input shape, leveraging text-to-image diffusion model and seamlessly blends it into the original scene to generate visually coherent and spatially consistent compositions. We evaluated Shape2Animal on a diverse set of real-world inputs, demonstrating its robustness and creative potential. Our Shape2Animal can offer new opportunities for visual storytelling, educational content, digital art, and interactive media design. Our project page is here: https://shape2image.github.io

</details>


### [15] [Joint attitude estimation and 3D neural reconstruction of non-cooperative space objects](https://arxiv.org/abs/2506.20638)
*Clément Forray,Pauline Delporte,Nicolas Delaygue,Florence Genin,Dawa Derksen*

Main category: cs.CV

TL;DR: 利用NeRF从模拟图像中重建非合作空间物体的3D模型，重点优化相机姿态，实验表明逐帧训练效果最佳。


<details>
  <summary>Details</summary>
Motivation: 提升空间态势感知能力，支持碎片清除、在轨维护等应用。

Method: 利用NeRF进行3D重建，联合优化相机姿态，采用逐帧训练和正则化方法。

Result: 逐帧训练和相机姿态优化实现了最准确的3D重建。

Conclusion: NeRF结合相机姿态优化在空间物体3D重建中表现优异。

Abstract: Obtaining a better knowledge of the current state and behavior of objects orbiting Earth has proven to be essential for a range of applications such as active debris removal, in-orbit maintenance, or anomaly detection. 3D models represent a valuable source of information in the field of Space Situational Awareness (SSA). In this work, we leveraged Neural Radiance Fields (NeRF) to perform 3D reconstruction of non-cooperative space objects from simulated images. This scenario is challenging for NeRF models due to unusual camera characteristics and environmental conditions : mono-chromatic images, unknown object orientation, limited viewing angles, absence of diffuse lighting etc. In this work we focus primarly on the joint optimization of camera poses alongside the NeRF. Our experimental results show that the most accurate 3D reconstruction is achieved when training with successive images one-by-one. We estimate camera poses by optimizing an uniform rotation and use regularization to prevent successive poses from being too far apart.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [16] [Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and Architecture](https://arxiv.org/abs/2506.19935)
*Shuchen Xue,Tianyu Xie,Tianyang Hu,Zijin Feng,Jiacheng Sun,Kenji Kawaguchi,Zhenguo Li,Zhi-Ming Ma*

Main category: cs.LG

TL;DR: 该研究通过将掩码扩散模型（MDM）与解码器架构结合，公平比较了自回归（AR）与MDM范式，并探讨了架构对MDM性能的影响。


<details>
  <summary>Details</summary>
Motivation: 比较AR和MDM范式时，传统方法同时改变架构和范式，导致不公平对比。本研究旨在分离范式与架构的影响。

Method: 在解码器架构中评估MDM（称为Any-Order AR，AO-AR），并分析其与标准AR的差异。同时探讨解码器与编码器架构对MDM的影响。

Result: 解码器MDM在生成速度上显著提升（约25倍），且困惑度与编码器MDM相当。AO-AR目标可能需要优化，因其部分排列信息较少。

Conclusion: 研究解耦了范式与架构的影响，为未来模型设计提供了见解。解码器MDM在速度和性能上表现优越。

Abstract: Large language models (LLMs) predominantly use autoregressive (AR) approaches, but masked diffusion models (MDMs) are emerging as viable alternatives. A key challenge in comparing AR and MDM paradigms is their typical architectural difference: AR models are often decoder-only, while MDMs have largely been encoder-only. This practice of changing both the modeling paradigm and architecture simultaneously makes direct comparisons unfair, as it's hard to distinguish whether observed differences stem from the paradigm itself or the architectural shift. This research evaluates MDMs within a decoder-only framework to: (1) equitably compare MDM (as Any-Order AR, or AO-AR) and standard AR paradigms. Our investigation suggests that the standard AO-AR objective, which averages over all token permutations, may benefit from refinement, as many permutations appear less informative compared to the language's inherent left-to-right structure. (2) Investigate architectural influences (decoder-only vs. encoder-only) within MDMs. We demonstrate that while encoder-only MDMs model a simpler conditional probability space, decoder-only MDMs can achieve dramatic generation speedups ($\sim25\times$) and comparable perplexity with temperature annealing despite modeling a vastly larger space, highlighting key trade-offs. This work thus decouples core paradigm differences from architectural influences, offering insights for future model design. Code is available at https://github.com/scxue/AO-GPT-MDM.

</details>


### [17] [FedBKD: Distilled Federated Learning to Embrace Gerneralization and Personalization on Non-IID Data](https://arxiv.org/abs/2506.20245)
*Yushan Zhao,Jinyuan He,Donglai Chen,Weijie Luo,Chong Xie,Ri Zhang,Yonghong Chen,Yan Xu*

Main category: cs.LG

TL;DR: 论文提出了一种名为FedBKD的无数据蒸馏框架，通过生成对抗网络（GAN）生成合成数据，实现全局和局部模型之间的双向知识蒸馏，解决了联邦学习中非独立同分布数据的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习（FL）中非独立同分布（non-IID）数据的问题，同时避免引入公共数据集带来的数据泄露风险。

Method: 使用GAN生成合成数据，局部模型作为判别器，参数冻结；通过合成数据实现全局与局部模型的双向知识蒸馏。

Result: 在4个基准测试中，FedBKD在不同非IID设置下均达到最优性能。

Conclusion: FedBKD框架在提升全局和局部模型性能的同时，避免了数据泄露风险，为非IID数据问题提供了有效解决方案。

Abstract: Federated learning (FL) is a decentralized collaborative machine learning (ML) technique. It provides a solution to the issues of isolated data islands and data privacy leakage in industrial ML practices. One major challenge in FL is handling the non-identical and independent distributed (non-IID) data. Current solutions either focus on constructing an all-powerful global model, or customizing personalized local models. Few of them can provide both a well-generalized global model and well-performed local models at the same time. Additionally, many FL solutions to the non-IID problem are benefited from introducing public datasets. However, this will also increase the risk of data leakage. To tackle the problems, we propose a novel data-free distillation framework, Federated Bidirectional Knowledge Distillation (FedBKD). Specifically, we train Generative Adversarial Networks (GAN) for synthetic data. During the GAN training, local models serve as discriminators and their parameters are frozen. The synthetic data is then used for bidirectional distillation between global and local models to achieve knowledge interactions so that performances for both sides are improved. We conduct extensive experiments on 4 benchmarks under different non-IID settings. The results show that FedBKD achieves SOTA performances in every case.

</details>
