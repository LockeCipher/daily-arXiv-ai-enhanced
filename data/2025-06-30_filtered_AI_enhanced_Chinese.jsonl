{"id": "2506.21629", "pdf": "https://arxiv.org/pdf/2506.21629", "abs": "https://arxiv.org/abs/2506.21629", "authors": ["Chenhao Zhang", "Yezhi Shen", "Fengqing Zhu"], "title": "ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded Scenes", "categories": ["cs.GR"], "comment": "6 pages, Source code is available at   https://github.com/Chenhao-Z/ICP-3DGS. To appear at ICIP 2025", "summary": "In recent years, neural rendering methods such as NeRFs and 3D Gaussian Splatting (3DGS) have made significant progress in scene reconstruction and novel view synthesis. However, they heavily rely on preprocessed camera poses and 3D structural priors from structure-from-motion (SfM), which are challenging to obtain in outdoor scenarios. To address this challenge, we propose to incorporate Iterative Closest Point (ICP) with optimization-based refinement to achieve accurate camera pose estimation under large camera movements. Additionally, we introduce a voxel-based scene densification approach to guide the reconstruction in large-scale scenes. Experiments demonstrate that our approach ICP-3DGS outperforms existing methods in both camera pose estimation and novel view synthesis across indoor and outdoor scenes of various scales. Source code is available at https://github.com/Chenhao-Z/ICP-3DGS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408ICP\u4e0e\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u8303\u56f4\u573a\u666f\u4e0b\u7684\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c\u573a\u666f\u91cd\u5efa\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\uff08\u5982NeRF\u548c3DGS\uff09\u4f9d\u8d56\u9884\u5904\u7406\u76f8\u673a\u59ff\u6001\u548c3D\u7ed3\u6784\u5148\u9a8c\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6237\u5916\u573a\u666f\u4e2d\u3002", "method": "\u7ed3\u5408\u8fed\u4ee3\u6700\u8fd1\u70b9\uff08ICP\uff09\u548c\u57fa\u4e8e\u4f18\u5316\u7684\u7ec6\u5316\u65b9\u6cd5\u8fdb\u884c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u4f53\u7d20\u7684\u573a\u666f\u5bc6\u96c6\u5316\u65b9\u6cd5\u6307\u5bfc\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cICP-3DGS\u5728\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u89c4\u6a21\u7684\u5ba4\u5185\u5916\u573a\u666f\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5927\u8303\u56f4\u573a\u666f\u4e0b\u7684\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c\u91cd\u5efa\u95ee\u9898\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.21632", "pdf": "https://arxiv.org/pdf/2506.21632", "abs": "https://arxiv.org/abs/2506.21632", "authors": ["Da Li", "Donggang Jia", "Markus Hadwiger", "Ivan Viola"], "title": "SkinningGS: Editable Dynamic Human Scene Reconstruction Using Gaussian Splatting Based on a Skinning Model", "categories": ["cs.GR"], "comment": null, "summary": "Reconstructing an interactive human avatar and the background from a monocular video of a dynamic human scene is highly challenging. In this work we adopt a strategy of point cloud decoupling and joint optimization to achieve the decoupled reconstruction of backgrounds and human bodies while preserving the interactivity of human motion. We introduce a position texture to subdivide the Skinned Multi-Person Linear (SMPL) body model's surface and grow the human point cloud. To capture fine details of human dynamics and deformations, we incorporate a convolutional neural network structure to predict human body point cloud features based on texture. This strategy makes our approach free of hyperparameter tuning for densification and efficiently represents human points with half the point cloud of HUGS. This approach ensures high-quality human reconstruction and reduces GPU resource consumption during training. As a result, our method surpasses the previous state-of-the-art HUGS in reconstruction metrics while maintaining the ability to generalize to novel poses and views. Furthermore, our technique achieves real-time rendering at over 100 FPS, $\\sim$6$\\times$ the HUGS speed using only Linear Blend Skinning (LBS) weights for human transformation. Additionally, this work demonstrates that this framework can be extended to animal scene reconstruction when an accurately-posed model of an animal is available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u70b9\u4e91\u89e3\u8026\u548c\u8054\u5408\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u91cd\u5efa\u4ea4\u4e92\u5f0f\u4eba\u4f53\u548c\u80cc\u666f\uff0c\u540c\u65f6\u4fdd\u7559\u4eba\u4f53\u8fd0\u52a8\u7684\u4ea4\u4e92\u6027\u3002", "motivation": "\u89e3\u51b3\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u91cd\u5efa\u52a8\u6001\u4eba\u4f53\u573a\u666f\u65f6\uff0c\u80cc\u666f\u548c\u4eba\u4f53\u89e3\u8026\u91cd\u5efa\u7684\u6311\u6218\uff0c\u540c\u65f6\u4fdd\u6301\u4eba\u4f53\u8fd0\u52a8\u7684\u4ea4\u4e92\u6027\u3002", "method": "\u91c7\u7528\u70b9\u4e91\u89e3\u8026\u548c\u8054\u5408\u4f18\u5316\u7b56\u7565\uff0c\u5f15\u5165\u4f4d\u7f6e\u7eb9\u7406\u7ec6\u5206SMPL\u6a21\u578b\u8868\u9762\uff0c\u5e76\u7ed3\u5408CNN\u9884\u6d4b\u4eba\u4f53\u70b9\u4e91\u7279\u5f81\u3002", "result": "\u5728\u91cd\u5efa\u8d28\u91cf\u4e0a\u8d85\u8d8aHUGS\uff0c\u51cf\u5c11GPU\u8d44\u6e90\u6d88\u8017\uff0c\u5b9e\u65f6\u6e32\u67d3\u901f\u5ea6\u8fbe100 FPS\uff0c\u5e76\u53ef\u6269\u5c55\u81f3\u52a8\u7269\u573a\u666f\u91cd\u5efa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u901a\u7528\uff0c\u9002\u7528\u4e8e\u4eba\u4f53\u548c\u52a8\u7269\u573a\u666f\u7684\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002"}}
{"id": "2506.21633", "pdf": "https://arxiv.org/pdf/2506.21633", "abs": "https://arxiv.org/abs/2506.21633", "authors": ["Aobo Li", "Zhengxin Lei", "Jiangtao Wei", "Feng Xu"], "title": "SAR-GS: 3D Gaussian Splatting for Synthetic Aperture Radar Target Reconstruction", "categories": ["cs.GR"], "comment": null, "summary": "Three-dimensional target reconstruction from synthetic aperture radar (SAR) imagery is crucial for interpreting complex scattering information in SAR data. However, the intricate electromagnetic scattering mechanisms inherent to SAR imaging pose significant reconstruction challenges. Inspired by the remarkable success of 3D Gaussian Splatting (3D-GS) in optical domain reconstruction, this paper presents a novel SAR Differentiable Gaussian Splatting Rasterizer (SDGR) specifically designed for SAR target reconstruction. Our approach combines Gaussian splatting with the Mapping and Projection Algorithm to compute scattering intensities of Gaussian primitives and generate simulated SAR images through SDGR. Subsequently, the loss function between the rendered image and the ground truth image is computed to optimize the Gaussian primitive parameters representing the scene, while a custom CUDA gradient flow is employed to replace automatic differentiation for accelerated gradient computation. Through experiments involving the rendering of simplified architectural targets and SAR images of multiple vehicle targets, we validate the imaging rationality of SDGR on simulated SAR imagery. Furthermore, the effectiveness of our method for target reconstruction is demonstrated on both simulated and real-world datasets containing multiple vehicle targets, with quantitative evaluations conducted to assess its reconstruction performance. Experimental results indicate that our approach can effectively reconstruct the geometric structures and scattering properties of targets, thereby providing a novel solution for 3D reconstruction in the field of SAR imaging.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSDGR\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u9ad8\u65af\u6cfc\u6e85\u548c\u6620\u5c04\u6295\u5f71\u7b97\u6cd5\uff0c\u7528\u4e8eSAR\u76ee\u6807\u7684\u4e09\u7ef4\u91cd\u5efa\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "SAR\u56fe\u50cf\u4e2d\u590d\u6742\u7684\u7535\u78c1\u6563\u5c04\u673a\u5236\u7ed9\u76ee\u6807\u91cd\u5efa\u5e26\u6765\u6311\u6218\uff0c\u800c3D\u9ad8\u65af\u6cfc\u6e85\u5728\u5149\u5b66\u9886\u57df\u7684\u6210\u529f\u542f\u53d1\u4e86\u8be5\u65b9\u6cd5\u7684\u8bbe\u8ba1\u3002", "method": "\u7ed3\u5408\u9ad8\u65af\u6cfc\u6e85\u4e0e\u6620\u5c04\u6295\u5f71\u7b97\u6cd5\uff0c\u901a\u8fc7SDGR\u751f\u6210\u6a21\u62dfSAR\u56fe\u50cf\uff0c\u5e76\u4f18\u5316\u9ad8\u65af\u57fa\u5143\u53c2\u6570\uff0c\u4f7f\u7528\u81ea\u5b9a\u4e49CUDA\u68af\u5ea6\u6d41\u52a0\u901f\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSDGR\u80fd\u6709\u6548\u91cd\u5efa\u76ee\u6807\u7684\u51e0\u4f55\u7ed3\u6784\u548c\u6563\u5c04\u7279\u6027\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u826f\u597d\u3002", "conclusion": "SDGR\u4e3aSAR\u6210\u50cf\u9886\u57df\u76843D\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21681", "pdf": "https://arxiv.org/pdf/2506.21681", "abs": "https://arxiv.org/abs/2506.21681", "authors": ["Hakan \u00c7apuk", "Andrew Bond", "Muhammed Burak K\u0131z\u0131l", "Emir G\u00f6\u00e7en", "Erkut Erdem", "Aykut Erdem"], "title": "TanDiT: Tangent-Plane Diffusion Transformer for High-Quality 360\u00b0 Panorama Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recent advances in image generation have led to remarkable improvements in synthesizing perspective images. However, these models still struggle with panoramic image generation due to unique challenges, including varying levels of geometric distortion and the requirement for seamless loop-consistency. To address these issues while leveraging the strengths of the existing models, we introduce TanDiT, a method that synthesizes panoramic scenes by generating grids of tangent-plane images covering the entire 360$^\\circ$ view. Unlike previous methods relying on multiple diffusion branches, TanDiT utilizes a unified diffusion model trained to produce these tangent-plane images simultaneously within a single denoising iteration. Furthermore, we propose a model-agnostic post-processing step specifically designed to enhance global coherence across the generated panoramas. To accurately assess panoramic image quality, we also present two specialized metrics, TangentIS and TangentFID, and provide a comprehensive benchmark comprising captioned panoramic datasets and standardized evaluation scripts. Extensive experiments demonstrate that our method generalizes effectively beyond its training data, robustly interprets detailed and complex text prompts, and seamlessly integrates with various generative models to yield high-quality, diverse panoramic images.", "AI": {"tldr": "TanDiT\u662f\u4e00\u79cd\u901a\u8fc7\u751f\u6210\u8986\u76d6360\u5ea6\u89c6\u56fe\u7684\u5207\u5e73\u9762\u56fe\u50cf\u7f51\u683c\u6765\u5408\u6210\u5168\u666f\u573a\u666f\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u5168\u666f\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u51e0\u4f55\u5931\u771f\u548c\u5faa\u73af\u4e00\u81f4\u6027\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u5168\u666f\u56fe\u50cf\u751f\u6210\u4e2d\u5b58\u5728\u51e0\u4f55\u5931\u771f\u548c\u5faa\u73af\u4e00\u81f4\u6027\u95ee\u9898\uff0cTanDiT\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5e76\u63d0\u5347\u5168\u666f\u56fe\u50cf\u8d28\u91cf\u3002", "method": "TanDiT\u4f7f\u7528\u7edf\u4e00\u7684\u6269\u6563\u6a21\u578b\u5728\u5355\u6b21\u53bb\u566a\u8fed\u4ee3\u4e2d\u540c\u65f6\u751f\u6210\u5207\u5e73\u9762\u56fe\u50cf\uff0c\u5e76\u63d0\u51fa\u6a21\u578b\u65e0\u5173\u7684\u540e\u5904\u7406\u6b65\u9aa4\u4ee5\u589e\u5f3a\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTanDiT\u80fd\u6709\u6548\u6cdb\u5316\u5230\u8bad\u7ec3\u6570\u636e\u4e4b\u5916\uff0c\u5904\u7406\u590d\u6742\u6587\u672c\u63d0\u793a\uff0c\u5e76\u4e0e\u591a\u79cd\u751f\u6210\u6a21\u578b\u7ed3\u5408\u751f\u6210\u9ad8\u8d28\u91cf\u5168\u666f\u56fe\u50cf\u3002", "conclusion": "TanDiT\u4e3a\u5168\u666f\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u65b0\u6307\u6807\u548c\u57fa\u51c6\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.21722", "pdf": "https://arxiv.org/pdf/2506.21722", "abs": "https://arxiv.org/abs/2506.21722", "authors": ["Xin Lu", "Xueyang Fu", "Jie Xiao", "Zihao Fan", "Yurui Zhu", "Zheng-Jun Zha"], "title": "Elucidating and Endowing the Diffusion Training Paradigm for General Image Restoration", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "While diffusion models demonstrate strong generative capabilities in image restoration (IR) tasks, their complex architectures and iterative processes limit their practical application compared to mainstream reconstruction-based general ordinary IR networks. Existing approaches primarily focus on optimizing network architecture and diffusion paths but overlook the integration of the diffusion training paradigm within general ordinary IR frameworks. To address these challenges, this paper elucidates key principles for adapting the diffusion training paradigm to general IR training through systematic analysis of time-step dependencies, network hierarchies, noise-level relationships, and multi-restoration task correlations, proposing a new IR framework supported by diffusion-based training. To enable IR networks to simultaneously restore images and model generative representations, we introduce a series of regularization strategies that align diffusion objectives with IR tasks, improving generalization in single-task scenarios. Furthermore, recognizing that diffusion-based generation exerts varying influences across different IR tasks, we develop an incremental training paradigm and task-specific adaptors, further enhancing performance in multi-task unified IR. Experiments demonstrate that our method significantly improves the generalization of IR networks in single-task IR and achieves superior performance in multi-task unified IR. Notably, the proposed framework can be seamlessly integrated into existing general IR architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6269\u6563\u8bad\u7ec3\u8303\u5f0f\u878d\u5165\u666e\u901a\u56fe\u50cf\u4fee\u590d\uff08IR\uff09\u6846\u67b6\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u65f6\u95f4\u6b65\u4f9d\u8d56\u3001\u7f51\u7edc\u5c42\u6b21\u3001\u566a\u58f0\u7ea7\u522b\u5173\u7cfb\u548c\u591a\u4efb\u52a1\u76f8\u5173\u6027\uff0c\u4f18\u5316\u4e86IR\u7f51\u7edc\u7684\u6cdb\u5316\u80fd\u529b\u548c\u591a\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u751f\u6210\u80fd\u529b\uff0c\u4f46\u5176\u590d\u6742\u67b6\u6784\u548c\u8fed\u4ee3\u8fc7\u7a0b\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u5c06\u6269\u6563\u8bad\u7ec3\u8303\u5f0f\u878d\u5165\u666e\u901aIR\u6846\u67b6\u7684\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u5206\u6790\u65f6\u95f4\u6b65\u4f9d\u8d56\u3001\u7f51\u7edc\u5c42\u6b21\u7b49\u5173\u952e\u56e0\u7d20\uff0c\u63d0\u51fa\u6269\u6563\u8bad\u7ec3\u652f\u6301\u7684IR\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u6b63\u5219\u5316\u7b56\u7565\u548c\u5bf9\u9f50\u6269\u6563\u76ee\u6807\u4e0eIR\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u5f00\u53d1\u589e\u91cf\u8bad\u7ec3\u8303\u5f0f\u548c\u4efb\u52a1\u7279\u5b9a\u9002\u914d\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5355\u4efb\u52a1IR\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4efb\u52a1\u7edf\u4e00IR\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u80fd\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709IR\u67b6\u6784\u4e2d\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6210\u529f\u5c06\u6269\u6563\u8bad\u7ec3\u8303\u5f0f\u878d\u5165\u666e\u901aIR\u8bad\u7ec3\uff0c\u63d0\u5347\u4e86\u5355\u4efb\u52a1\u548c\u591a\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.22426", "pdf": "https://arxiv.org/pdf/2506.22426", "abs": "https://arxiv.org/abs/2506.22426", "authors": ["Xiang Dai", "Kyrollos Yanny", "Kristina Monakhova", "Nicholas Antipa"], "title": "Single-shot HDR using conventional image sensor shutter functions and optical randomization", "categories": ["eess.IV", "cs.CV", "cs.GR", "eess.SP", "physics.optics"], "comment": null, "summary": "High-dynamic-range (HDR) imaging is an essential technique for overcoming the dynamic range limits of image sensors. The classic method relies on multiple exposures, which slows capture time, resulting in motion artifacts when imaging dynamic scenes. Single-shot HDR imaging alleviates this issue by encoding HDR data into a single exposure, then computationally recovering it. Many established methods use strong image priors to recover improperly exposed image detail. These approaches struggle with extended highlight regions. We utilize the global reset release (GRR) shutter mode of an off-the-shelf sensor. GRR shutter mode applies a longer exposure time to rows closer to the bottom of the sensor. We use optics that relay a randomly permuted (shuffled) image onto the sensor, effectively creating spatially randomized exposures across the scene. The exposure diversity allows us to recover HDR data by solving an optimization problem with a simple total variation image prior. In simulation, we demonstrate that our method outperforms other single-shot methods when many sensor pixels are saturated (10% or more), and is competitive at a modest saturation (1%). Finally, we demonstrate a physical lab prototype that uses an off-the-shelf random fiber bundle for the optical shuffling. The fiber bundle is coupled to a low-cost commercial sensor operating in GRR shutter mode. Our prototype achieves a dynamic range of up to 73dB using an 8-bit sensor with 48dB dynamic range.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5168\u5c40\u91cd\u7f6e\u91ca\u653e\uff08GRR\uff09\u5feb\u95e8\u6a21\u5f0f\u548c\u5149\u5b66\u968f\u673a\u6392\u5217\u7684\u5355\u6b21\u66dd\u5149HDR\u6210\u50cf\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u9ad8\u5149\u533a\u57df\u7684\u6062\u590d\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u591a\u66dd\u5149HDR\u6210\u50cf\u5728\u52a8\u6001\u573a\u666f\u4e2d\u6613\u4ea7\u751f\u8fd0\u52a8\u4f2a\u5f71\uff0c\u800c\u73b0\u6709\u5355\u6b21\u66dd\u5149\u65b9\u6cd5\u5728\u9ad8\u5149\u533a\u57df\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5229\u7528GRR\u5feb\u95e8\u6a21\u5f0f\u5b9e\u73b0\u4e0d\u540c\u533a\u57df\u7684\u5dee\u5f02\u5316\u66dd\u5149\uff0c\u5e76\u901a\u8fc7\u5149\u5b66\u968f\u673a\u6392\u5217\u521b\u5efa\u7a7a\u95f4\u968f\u673a\u66dd\u5149\uff0c\u7ed3\u5408\u603b\u53d8\u5206\u5148\u9a8c\u4f18\u5316\u6062\u590dHDR\u6570\u636e\u3002", "result": "\u4eff\u771f\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u9ad8\u50cf\u7d20\u9971\u548c\uff0810%\u4ee5\u4e0a\uff09\u65f6\u4f18\u4e8e\u5176\u4ed6\u5355\u6b21\u66dd\u5149\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u5ba4\u539f\u578b\u52a8\u6001\u8303\u56f4\u8fbe73dB\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7GRR\u548c\u5149\u5b66\u968f\u673a\u6392\u5217\u6709\u6548\u63d0\u5347\u5355\u6b21HDR\u6210\u50cf\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u9ad8\u5149\u573a\u666f\u3002"}}
{"id": "2506.21731", "pdf": "https://arxiv.org/pdf/2506.21731", "abs": "https://arxiv.org/abs/2506.21731", "authors": ["Chenqiu Zhao", "Anup Basu"], "title": "Exploring Image Generation via Mutually Exclusive Probability Spaces and Local Correlation Hypothesis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose two theoretical frameworks, the Mutually Exclusive Probability Space (MESP) and the Local Correlation Hypothesis (LCH), to explore a potential limitation in probabilistic generative models; namely that learning global distributions leads to memorization rather than generative behavior. MESP emerges from our rethinking of the Variational Autoencoder (VAE). We observe that latent variable distributions in VAE exhibit overlap, which leads to an optimization conflict between the reconstruction loss and KL-divergence loss. A lower bound based on the overlap coefficient is proposed. We refer to this phenomenon as Mutually Exclusive Probability Spaces. Based on MESP, a Binary Latent Autoencoder (BL-AE) is proposed to encode images into binary latent representations. These binary latents are used as the input to our Autoregressive Random Variable Model (ARVM), a modified autoregressive model outputting histograms. Our ARVM achieves competitive FID scores, outperforming state-of-the-art methods on standard datasets. However, such scores reflect memorization rather than generation. To address this issue, we propose the Local Correlation Hypothesis (LCH), which posits that generative capability arising from local correlations among latent variables. Comprehensive experiments and discussions are conducted to validate our frameworks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MESP\u548cLCH\u4e24\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u63a2\u8ba8\u6982\u7387\u751f\u6210\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5373\u5168\u5c40\u5206\u5e03\u5b66\u4e60\u5bfc\u81f4\u8bb0\u5fc6\u800c\u975e\u751f\u6210\u884c\u4e3a\u3002MESP\u6e90\u4e8e\u5bf9VAE\u7684\u91cd\u65b0\u601d\u8003\uff0c\u63d0\u51fa\u57fa\u4e8e\u91cd\u53e0\u7cfb\u6570\u7684\u4e0b\u754c\uff0c\u5e76\u8bbe\u8ba1BL-AE\u548cARVM\u6a21\u578b\u3002\u5b9e\u9a8c\u663e\u793aARVM\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u5b58\u5728\u8bb0\u5fc6\u95ee\u9898\uff0cLCH\u5047\u8bbe\u5c40\u90e8\u76f8\u5173\u6027\u53ef\u63d0\u5347\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u6982\u7387\u751f\u6210\u6a21\u578b\u5728\u5168\u5c40\u5206\u5e03\u5b66\u4e60\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0\u5176\u503e\u5411\u4e8e\u8bb0\u5fc6\u800c\u975e\u751f\u6210\u884c\u4e3a\uff0c\u8bd5\u56fe\u901a\u8fc7\u65b0\u7406\u8bba\u6846\u67b6\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faMESP\u6846\u67b6\uff0c\u57fa\u4e8eVAE\u7684\u6f5c\u5728\u53d8\u91cf\u91cd\u53e0\u95ee\u9898\u8bbe\u8ba1BL-AE\u548cARVM\u6a21\u578b\uff1b\u8fdb\u4e00\u6b65\u63d0\u51faLCH\u5047\u8bbe\uff0c\u5f3a\u8c03\u5c40\u90e8\u76f8\u5173\u6027\u5bf9\u751f\u6210\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "ARVM\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u7ade\u4e89\u6027FID\u5206\u6570\uff0c\u4f46\u53d1\u73b0\u5176\u8868\u73b0\u66f4\u591a\u6e90\u4e8e\u8bb0\u5fc6\u800c\u975e\u751f\u6210\uff1bLCH\u5047\u8bbe\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5c40\u90e8\u76f8\u5173\u6027\u7684\u91cd\u8981\u6027\u3002", "conclusion": "MESP\u548cLCH\u6846\u67b6\u63ed\u793a\u4e86\u6982\u7387\u751f\u6210\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u5c40\u90e8\u76f8\u5173\u6027\u63d0\u5347\u751f\u6210\u80fd\u529b\u7684\u53ef\u80fd\u65b9\u5411\u3002"}}
{"id": "2506.21832", "pdf": "https://arxiv.org/pdf/2506.21832", "abs": "https://arxiv.org/abs/2506.21832", "authors": ["Minh-Loi Nguyen", "Quang-Khai Le", "Tam V. Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "title": "TaleForge: Interactive Multimodal System for Personalized Story Creation", "categories": ["cs.CV"], "comment": null, "summary": "Storytelling is a deeply personal and creative process, yet existing methods often treat users as passive consumers, offering generic plots with limited personalization. This undermines engagement and immersion, especially where individual style or appearance is crucial. We introduce TaleForge, a personalized story-generation system that integrates large language models (LLMs) and text-to-image diffusion to embed users' facial images within both narratives and illustrations. TaleForge features three interconnected modules: Story Generation, where LLMs create narratives and character descriptions from user prompts; Personalized Image Generation, merging users' faces and outfit choices into character illustrations; and Background Generation, creating scene backdrops that incorporate personalized characters. A user study demonstrated heightened engagement and ownership when individuals appeared as protagonists. Participants praised the system's real-time previews and intuitive controls, though they requested finer narrative editing tools. TaleForge advances multimodal storytelling by aligning personalized text and imagery to create immersive, user-centric experiences.", "AI": {"tldr": "TaleForge\u662f\u4e00\u4e2a\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6280\u672f\u7684\u4e2a\u6027\u5316\u6545\u4e8b\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u7528\u6237\u7684\u9762\u90e8\u56fe\u50cf\u5d4c\u5165\u53d9\u4e8b\u548c\u63d2\u56fe\u4e2d\uff0c\u63d0\u5347\u6c89\u6d78\u611f\u548c\u53c2\u4e0e\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u7528\u6237\u89c6\u4e3a\u88ab\u52a8\u6d88\u8d39\u8005\uff0c\u63d0\u4f9b\u901a\u7528\u60c5\u8282\u4e14\u4e2a\u6027\u5316\u6709\u9650\uff0c\u5f71\u54cd\u4e86\u53c2\u4e0e\u5ea6\u548c\u6c89\u6d78\u611f\u3002", "method": "TaleForge\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u6545\u4e8b\u751f\u6210\uff08LLMs\u6839\u636e\u7528\u6237\u63d0\u793a\u521b\u5efa\u53d9\u4e8b\u548c\u89d2\u8272\u63cf\u8ff0\uff09\u3001\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\uff08\u5c06\u7528\u6237\u9762\u90e8\u548c\u670d\u88c5\u878d\u5165\u89d2\u8272\u63d2\u56fe\uff09\u3001\u80cc\u666f\u751f\u6210\uff08\u521b\u5efa\u5305\u542b\u4e2a\u6027\u5316\u89d2\u8272\u7684\u573a\u666f\uff09\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u5f53\u7528\u6237\u4f5c\u4e3a\u4e3b\u89d2\u51fa\u73b0\u65f6\uff0c\u53c2\u4e0e\u611f\u548c\u5f52\u5c5e\u611f\u663e\u8457\u63d0\u5347\u3002\u53c2\u4e0e\u8005\u8d5e\u8d4f\u7cfb\u7edf\u7684\u5b9e\u65f6\u9884\u89c8\u548c\u76f4\u89c2\u63a7\u5236\uff0c\u4f46\u5e0c\u671b\u6709\u66f4\u7cbe\u7ec6\u7684\u53d9\u4e8b\u7f16\u8f91\u5de5\u5177\u3002", "conclusion": "TaleForge\u901a\u8fc7\u4e2a\u6027\u5316\u6587\u672c\u548c\u56fe\u50cf\u7684\u7ed3\u5408\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u6545\u4e8b\u53d9\u8ff0\u7684\u53d1\u5c55\uff0c\u521b\u9020\u4e86\u6c89\u6d78\u5f0f\u3001\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u4f53\u9a8c\u3002"}}
{"id": "2506.21834", "pdf": "https://arxiv.org/pdf/2506.21834", "abs": "https://arxiv.org/abs/2506.21834", "authors": ["Duy-Bao Bui", "Hoang-Khang Nguyen", "Trung-Nghia Le"], "title": "PrefPaint: Enhancing Image Inpainting through Expert Human Feedback", "categories": ["cs.CV"], "comment": null, "summary": "Inpainting, the process of filling missing or corrupted image parts, has broad applications, including medical imaging. However, in specialized fields like medical polyps imaging, where accuracy and reliability are critical, inpainting models can generate inaccurate images, leading to significant errors in medical diagnosis and treatment. To ensure reliability, medical images should be annotated by experts like oncologists for effective model training. We propose PrefPaint, an approach that incorporates human feedback into the training process of Stable Diffusion Inpainting, bypassing the need for computationally expensive reward models. In addition, we develop a web-based interface streamlines training, fine-tuning, and inference. This interactive interface provides a smooth and intuitive user experience, making it easier to offer feedback and manage the fine-tuning process. User study on various domains shows that PrefPaint outperforms existing methods, reducing visual inconsistencies and improving image rendering, particularly in medical contexts, where our model generates more realistic polyps images.", "AI": {"tldr": "PrefPaint\u901a\u8fc7\u5f15\u5165\u4eba\u7c7b\u53cd\u9988\u4f18\u5316Stable Diffusion Inpainting\u8bad\u7ec3\uff0c\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u4fee\u590d\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u5c24\u5176\u5728\u606f\u8089\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u4fee\u590d\uff08\u5982\u606f\u8089\u56fe\u50cf\uff09\u7684\u51c6\u786e\u6027\u5bf9\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u6a21\u578b\u53ef\u80fd\u751f\u6210\u4e0d\u51c6\u786e\u56fe\u50cf\uff0c\u9700\u4e13\u5bb6\u6807\u6ce8\u548c\u53cd\u9988\u4ee5\u786e\u4fdd\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faPrefPaint\uff0c\u5c06\u4eba\u7c7b\u53cd\u9988\u76f4\u63a5\u878d\u5165Stable Diffusion Inpainting\u8bad\u7ec3\uff0c\u907f\u514d\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u5956\u52b1\u6a21\u578b\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8e\u7f51\u9875\u7684\u4ea4\u4e92\u754c\u9762\u7b80\u5316\u6d41\u7a0b\u3002", "result": "PrefPaint\u5728\u591a\u4e2a\u9886\u57df\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u51cf\u5c11\u89c6\u89c9\u4e0d\u4e00\u81f4\u6027\uff0c\u63d0\u5347\u56fe\u50cf\u6e32\u67d3\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u533b\u5b66\u606f\u8089\u56fe\u50cf\u751f\u6210\u4e2d\u66f4\u771f\u5b9e\u3002", "conclusion": "PrefPaint\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u53cd\u9988\u548c\u4ea4\u4e92\u754c\u9762\uff0c\u663e\u8457\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u4fee\u590d\u7684\u53ef\u9760\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.21843", "pdf": "https://arxiv.org/pdf/2506.21843", "abs": "https://arxiv.org/abs/2506.21843", "authors": ["Yuxiang Ge", "Jionghao Cheng", "Ruiquan Ge", "Zhaojie Fang", "Gangyong Jia", "Xiang Wan", "Nannan Li", "Ahmed Elazab", "Changmiao Wang"], "title": "3D-Telepathy: Reconstructing 3D Objects from EEG Signals", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing 3D visual stimuli from Electroencephalography (EEG) data holds significant potential for applications in Brain-Computer Interfaces (BCIs) and aiding individuals with communication disorders. Traditionally, efforts have focused on converting brain activity into 2D images, neglecting the translation of EEG data into 3D objects. This limitation is noteworthy, as the human brain inherently processes three-dimensional spatial information regardless of whether observing 2D images or the real world. The neural activities captured by EEG contain rich spatial information that is inevitably lost when reconstructing only 2D images, thus limiting its practical applications in BCI. The transition from EEG data to 3D object reconstruction faces considerable obstacles. These include the presence of extensive noise within EEG signals and a scarcity of datasets that include both EEG and 3D information, which complicates the extraction process of 3D visual data. Addressing this challenging task, we propose an innovative EEG encoder architecture that integrates a dual self-attention mechanism. We use a hybrid training strategy to train the EEG Encoder, which includes cross-attention, contrastive learning, and self-supervised learning techniques. Additionally, by employing stable diffusion as a prior distribution and utilizing Variational Score Distillation to train a neural radiation field, we successfully generate 3D objects with similar content and structure from EEG data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684EEG\u7f16\u7801\u5668\u67b6\u6784\uff0c\u7ed3\u5408\u53cc\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\uff0c\u6210\u529f\u4eceEEG\u6570\u636e\u751f\u62103D\u5bf9\u8c61\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4ec5\u5c06\u8111\u7535\u6d3b\u52a8\u8f6c\u6362\u4e3a2D\u56fe\u50cf\uff0c\u5ffd\u7565\u4e863D\u7a7a\u95f4\u4fe1\u606f\u7684\u91cd\u5efa\uff0c\u9650\u5236\u4e86BCI\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u53cc\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684EEG\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u4ea4\u53c9\u6ce8\u610f\u529b\u3001\u5bf9\u6bd4\u5b66\u4e60\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u5229\u7528\u7a33\u5b9a\u6269\u6563\u548c\u53d8\u5206\u5206\u6570\u84b8\u998f\u8bad\u7ec3\u795e\u7ecf\u8f90\u5c04\u573a\u3002", "result": "\u6210\u529f\u4eceEEG\u6570\u636e\u751f\u6210\u4e86\u5185\u5bb9\u548c\u7ed3\u6784\u76f8\u4f3c\u76843D\u5bf9\u8c61\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4eceEEG\u6570\u636e\u91cd\u5efa3D\u89c6\u89c9\u523a\u6fc0\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u6269\u5c55\u4e86BCI\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.21975", "pdf": "https://arxiv.org/pdf/2506.21975", "abs": "https://arxiv.org/abs/2506.21975", "authors": ["Meng Yu", "Te Cui", "Qitong Chu", "Wenjie Song", "Yi Yang", "Yufeng Yue"], "title": "TASeg: Text-aware RGB-T Semantic Segmentation based on Fine-tuning Vision Foundation Models", "categories": ["cs.CV"], "comment": "6 pages, accepted for publication in lEEE/RSJ international   Conference on Intelligent Robots and Systems (lROS 2025)", "summary": "Reliable semantic segmentation of open environments is essential for intelligent systems, yet significant problems remain: 1) Existing RGB-T semantic segmentation models mainly rely on low-level visual features and lack high-level textual information, which struggle with accurate segmentation when categories share similar visual characteristics. 2) While SAM excels in instance-level segmentation, integrating it with thermal images and text is hindered by modality heterogeneity and computational inefficiency. To address these, we propose TASeg, a text-aware RGB-T segmentation framework by using Low-Rank Adaptation (LoRA) fine-tuning technology to adapt vision foundation models. Specifically, we propose a Dynamic Feature Fusion Module (DFFM) in the image encoder, which effectively merges features from multiple visual modalities while freezing SAM's original transformer blocks. Additionally, we incorporate CLIP-generated text embeddings in the mask decoder to enable semantic alignment, which further rectifies the classification error and improves the semantic understanding accuracy. Experimental results across diverse datasets demonstrate that our method achieves superior performance in challenging scenarios with fewer trainable parameters.", "AI": {"tldr": "TASeg\u662f\u4e00\u4e2a\u57fa\u4e8e\u6587\u672c\u611f\u77e5\u7684RGB-T\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7LoRA\u5fae\u8c03\u6280\u672f\u548c\u52a8\u6001\u7279\u5f81\u878d\u5408\u6a21\u5757\uff08DFFM\uff09\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u89c6\u89c9\u7279\u5f81\u76f8\u4f3c\u7c7b\u522b\u4e2d\u7684\u5206\u5272\u95ee\u9898\uff0c\u5e76\u7ed3\u5408CLIP\u6587\u672c\u5d4c\u5165\u63d0\u5347\u8bed\u4e49\u7406\u89e3\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709RGB-T\u8bed\u4e49\u5206\u5272\u6a21\u578b\u4f9d\u8d56\u4f4e\u5c42\u89c6\u89c9\u7279\u5f81\uff0c\u7f3a\u4e4f\u9ad8\u5c42\u6587\u672c\u4fe1\u606f\uff0c\u96be\u4ee5\u533a\u5206\u89c6\u89c9\u7279\u5f81\u76f8\u4f3c\u7684\u7c7b\u522b\uff1b\u540c\u65f6\uff0cSAM\u5728\u5b9e\u4f8b\u7ea7\u5206\u5272\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4e0e\u70ed\u56fe\u50cf\u548c\u6587\u672c\u7684\u96c6\u6210\u5b58\u5728\u6a21\u6001\u5f02\u8d28\u6027\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51faTASeg\u6846\u67b6\uff0c\u91c7\u7528LoRA\u5fae\u8c03\u6280\u672f\u9002\u914d\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u8bbe\u8ba1DFFM\u6a21\u5757\u878d\u5408\u591a\u6a21\u6001\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u5728\u63a9\u7801\u89e3\u7801\u5668\u4e2d\u5f15\u5165CLIP\u751f\u6210\u7684\u6587\u672c\u5d4c\u5165\u4ee5\u5b9e\u73b0\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTASeg\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u8bad\u7ec3\u53c2\u6570\u8f83\u5c11\u3002", "conclusion": "TASeg\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86RGB-T\u8bed\u4e49\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2506.22007", "pdf": "https://arxiv.org/pdf/2506.22007", "abs": "https://arxiv.org/abs/2506.22007", "authors": ["Liudi Yang", "Yang Bai", "George Eskandar", "Fengyi Shen", "Mohammad Altillawi", "Dong Chen", "Soumajit Majumder", "Ziyuan Liu", "Gitta Kutyniok", "Abhinav Valada"], "title": "RoboEnvision: A Long-Horizon Video Generation Model for Multi-Task Robot Manipulation", "categories": ["cs.CV"], "comment": "8 pages, 6 figures", "summary": "We address the problem of generating long-horizon videos for robotic manipulation tasks. Text-to-video diffusion models have made significant progress in photorealism, language understanding, and motion generation but struggle with long-horizon robotic tasks. Recent works use video diffusion models for high-quality simulation data and predictive rollouts in robot planning. However, these works predict short sequences of the robot achieving one task and employ an autoregressive paradigm to extend to the long horizon, leading to error accumulations in the generated video and in the execution. To overcome these limitations, we propose a novel pipeline that bypasses the need for autoregressive generation. We achieve this through a threefold contribution: 1) we first decompose the high-level goals into smaller atomic tasks and generate keyframes aligned with these instructions. A second diffusion model then interpolates between each of the two generated frames, achieving the long-horizon video. 2) We propose a semantics preserving attention module to maintain consistency between the keyframes. 3) We design a lightweight policy model to regress the robot joint states from generated videos. Our approach achieves state-of-the-art results on two benchmarks in video quality and consistency while outperforming previous policy models on long-horizon tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u4efb\u52a1\u3001\u751f\u6210\u5173\u952e\u5e27\u548c\u63d2\u503c\u6765\u907f\u514d\u81ea\u56de\u5f52\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u89c6\u9891\u751f\u6210\u4e2d\u7684\u9519\u8bef\u7d2f\u79ef\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u907f\u514d\u81ea\u56de\u5f52\u751f\u6210\u5bfc\u81f4\u7684\u9519\u8bef\u7d2f\u79ef\u3002", "method": "1) \u5206\u89e3\u4efb\u52a1\u5e76\u751f\u6210\u5173\u952e\u5e27\uff1b2) \u4f7f\u7528\u6269\u6563\u6a21\u578b\u63d2\u503c\u5173\u952e\u5e27\uff1b3) \u8bbe\u8ba1\u8bed\u4e49\u4fdd\u6301\u6ce8\u610f\u529b\u6a21\u5757\u548c\u8f7b\u91cf\u7ea7\u7b56\u7565\u6a21\u578b\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u89c6\u9891\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u7684\u6700\u4f73\u7ed3\u679c\uff0c\u5e76\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4e4b\u524d\u7684\u7b56\u7565\u6a21\u578b\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u89c6\u9891\u751f\u6210\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u4efb\u52a1\u7684\u8868\u73b0\u3002"}}
{"id": "2506.22022", "pdf": "https://arxiv.org/pdf/2506.22022", "abs": "https://arxiv.org/abs/2506.22022", "authors": ["Zhanyi Lu", "Yue Zhou"], "title": "Advancing Facial Stylization through Semantic Preservation Constraint and Pseudo-Paired Supervision", "categories": ["cs.CV"], "comment": null, "summary": "Facial stylization aims to transform facial images into appealing, high-quality stylized portraits, with the critical challenge of accurately learning the target style while maintaining content consistency with the original image. Although previous StyleGAN-based methods have made significant advancements, the generated results still suffer from artifacts or insufficient fidelity to the source image. We argue that these issues stem from neglecting semantic shift of the generator during stylization. Therefore, we propose a facial stylization method that integrates semantic preservation constraint and pseudo-paired supervision to enhance the content correspondence and improve the stylization effect. Additionally, we develop a methodology for creating multi-level pseudo-paired datasets to implement supervisory constraint. Furthermore, building upon our facial stylization framework, we achieve more flexible multimodal and reference-guided stylization without complex network architecture designs or additional training. Experimental results demonstrate that our approach produces high-fidelity, aesthetically pleasing facial style transfer that surpasses previous methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u8bed\u4e49\u4fdd\u7559\u7ea6\u675f\u548c\u4f2a\u914d\u5bf9\u76d1\u7763\u7684\u4eba\u8138\u98ce\u683c\u5316\u65b9\u6cd5\uff0c\u63d0\u5347\u5185\u5bb9\u4e00\u81f4\u6027\u548c\u98ce\u683c\u5316\u6548\u679c\uff0c\u652f\u6301\u591a\u6a21\u6001\u548c\u53c2\u8003\u5f15\u5bfc\u98ce\u683c\u5316\u3002", "motivation": "\u73b0\u6709StyleGAN\u65b9\u6cd5\u5728\u98ce\u683c\u5316\u65f6\u5ffd\u7565\u751f\u6210\u5668\u7684\u8bed\u4e49\u504f\u79fb\uff0c\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u5b58\u5728\u4f2a\u5f71\u6216\u4e0e\u6e90\u56fe\u50cf\u5185\u5bb9\u4e0d\u4e00\u81f4\u3002", "method": "\u96c6\u6210\u8bed\u4e49\u4fdd\u7559\u7ea6\u675f\u548c\u4f2a\u914d\u5bf9\u76d1\u7763\uff0c\u5f00\u53d1\u591a\u7ea7\u4f2a\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u76d1\u7763\u7ea6\u675f\uff0c\u652f\u6301\u7075\u6d3b\u7684\u591a\u6a21\u6001\u548c\u53c2\u8003\u5f15\u5bfc\u98ce\u683c\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u7f8e\u89c2\u7684\u4eba\u8138\u98ce\u683c\u5316\u6548\u679c\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u8bed\u4e49\u4fdd\u7559\u548c\u4f2a\u914d\u5bf9\u76d1\u7763\u6709\u6548\u89e3\u51b3\u4e86\u98ce\u683c\u5316\u4e2d\u7684\u5185\u5bb9\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u4e86\u7075\u6d3b\u7684\u98ce\u683c\u5316\u6269\u5c55\u3002"}}
{"id": "2506.22065", "pdf": "https://arxiv.org/pdf/2506.22065", "abs": "https://arxiv.org/abs/2506.22065", "authors": ["Dechao Meng", "Steven Xiao", "Xindi Zhang", "Guangyuan Wang", "Peng Zhang", "Qi Wang", "Bang Zhang", "Liefeng Bo"], "title": "MirrorMe: Towards Realtime and High Fidelity Audio-Driven Halfbody Animation", "categories": ["cs.CV"], "comment": "8 pages, 6 figures", "summary": "Audio-driven portrait animation, which synthesizes realistic videos from reference images using audio signals, faces significant challenges in real-time generation of high-fidelity, temporally coherent animations. While recent diffusion-based methods improve generation quality by integrating audio into denoising processes, their reliance on frame-by-frame UNet architectures introduces prohibitive latency and struggles with temporal consistency. This paper introduces MirrorMe, a real-time, controllable framework built on the LTX video model, a diffusion transformer that compresses video spatially and temporally for efficient latent space denoising. To address LTX's trade-offs between compression and semantic fidelity, we propose three innovations: 1. A reference identity injection mechanism via VAE-encoded image concatenation and self-attention, ensuring identity consistency; 2. A causal audio encoder and adapter tailored to LTX's temporal structure, enabling precise audio-expression synchronization; and 3. A progressive training strategy combining close-up facial training, half-body synthesis with facial masking, and hand pose integration for enhanced gesture control. Extensive experiments on the EMTD Benchmark demonstrate MirrorMe's state-of-the-art performance in fidelity, lip-sync accuracy, and temporal stability.", "AI": {"tldr": "MirrorMe\u662f\u4e00\u4e2a\u57fa\u4e8eLTX\u89c6\u9891\u6a21\u578b\u7684\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u8096\u50cf\u52a8\u753b\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u548c\u65f6\u95f4\u538b\u7f29\u5b9e\u73b0\u9ad8\u6548\u6f5c\u5728\u7a7a\u95f4\u53bb\u566a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5ef6\u8fdf\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u867d\u7136\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u4f46\u7531\u4e8e\u9010\u5e27UNet\u67b6\u6784\u7684\u4f9d\u8d56\uff0c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4e0d\u8db3\u3002MirrorMe\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5b9e\u73b0\u5b9e\u65f6\u3001\u9ad8\u4fdd\u771f\u7684\u52a8\u753b\u751f\u6210\u3002", "method": "1. \u901a\u8fc7VAE\u7f16\u7801\u56fe\u50cf\u62fc\u63a5\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6ce8\u5165\u53c2\u8003\u8eab\u4efd\uff1b2. \u4e3aLTX\u65f6\u95f4\u7ed3\u6784\u8bbe\u8ba1\u7684\u56e0\u679c\u97f3\u9891\u7f16\u7801\u5668\u548c\u9002\u914d\u5668\uff1b3. \u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u9762\u90e8\u3001\u534a\u8eab\u548c\u624b\u52bf\u63a7\u5236\u3002", "result": "\u5728EMTD Benchmark\u4e0a\uff0cMirrorMe\u5728\u4fdd\u771f\u5ea6\u3001\u5507\u540c\u6b65\u51c6\u786e\u6027\u548c\u65f6\u95f4\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "MirrorMe\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u548c\u6e10\u8fdb\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u3001\u9ad8\u4fdd\u771f\u7684\u97f3\u9891\u9a71\u52a8\u8096\u50cf\u52a8\u753b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.22099", "pdf": "https://arxiv.org/pdf/2506.22099", "abs": "https://arxiv.org/abs/2506.22099", "authors": ["Zipei Ma", "Junzhe Jiang", "Yurui Chen", "Li Zhang"], "title": "B\u00e9zierGS: Dynamic Urban Scene Reconstruction with B\u00e9zier Curve Gaussian Splatting", "categories": ["cs.CV"], "comment": "Accepted at ICCV 2025, Project Page:   https://github.com/fudan-zvg/BezierGS", "summary": "The realistic reconstruction of street scenes is critical for developing real-world simulators in autonomous driving. Most existing methods rely on object pose annotations, using these poses to reconstruct dynamic objects and move them during the rendering process. This dependence on high-precision object annotations limits large-scale and extensive scene reconstruction. To address this challenge, we propose B\\'ezier curve Gaussian splatting (B\\'ezierGS), which represents the motion trajectories of dynamic objects using learnable B\\'ezier curves. This approach fully leverages the temporal information of dynamic objects and, through learnable curve modeling, automatically corrects pose errors. By introducing additional supervision on dynamic object rendering and inter-curve consistency constraints, we achieve reasonable and accurate separation and reconstruction of scene elements. Extensive experiments on the Waymo Open Dataset and the nuPlan benchmark demonstrate that B\\'ezierGS outperforms state-of-the-art alternatives in both dynamic and static scene components reconstruction and novel view synthesis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eB\u00e9zier\u66f2\u7ebf\u7684\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff08B\u00e9zierGS\uff09\uff0c\u7528\u4e8e\u52a8\u6001\u7269\u4f53\u7684\u8fd0\u52a8\u8f68\u8ff9\u5efa\u6a21\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u7cbe\u5ea6\u6807\u6ce8\u7684\u95ee\u9898\uff0c\u5e76\u5728\u573a\u666f\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u5408\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u7cbe\u5ea6\u7269\u4f53\u59ff\u6001\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u3002B\u00e9zierGS\u901a\u8fc7\u53ef\u5b66\u4e60\u7684B\u00e9zier\u66f2\u7ebf\u5efa\u6a21\u52a8\u6001\u7269\u4f53\u8fd0\u52a8\u8f68\u8ff9\uff0c\u51cf\u5c11\u5bf9\u6807\u6ce8\u7684\u4f9d\u8d56\u3002", "method": "\u4f7f\u7528B\u00e9zier\u66f2\u7ebf\u8868\u793a\u52a8\u6001\u7269\u4f53\u8fd0\u52a8\u8f68\u8ff9\uff0c\u7ed3\u5408\u53ef\u5b66\u4e60\u66f2\u7ebf\u5efa\u6a21\u548c\u52a8\u6001\u7269\u4f53\u6e32\u67d3\u76d1\u7763\uff0c\u5b9e\u73b0\u573a\u666f\u5143\u7d20\u7684\u5206\u79bb\u4e0e\u91cd\u5efa\u3002", "result": "\u5728Waymo Open Dataset\u548cnuPlan\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cB\u00e9zierGS\u5728\u52a8\u6001\u548c\u9759\u6001\u573a\u666f\u91cd\u5efa\u53ca\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "B\u00e9zierGS\u901a\u8fc7\u66f2\u7ebf\u5efa\u6a21\u548c\u989d\u5916\u76d1\u7763\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u573a\u666f\u91cd\u5efa\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4eff\u771f\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.22134", "pdf": "https://arxiv.org/pdf/2506.22134", "abs": "https://arxiv.org/abs/2506.22134", "authors": ["Zhengyun Cheng", "Changhao Wang", "Guanwen Zhang", "Yi Xu", "Wei Zhou", "Xiangyang Ji"], "title": "Low-Rank Implicit Neural Representation via Schatten-p Quasi-Norm and Jacobian Regularization", "categories": ["cs.CV"], "comment": "Submitted to IEEE Transactions on Circuits and Systems for Video   Technology", "summary": "Higher-order tensors are well-suited for representing multi-dimensional data, such as color images and videos. Low-rank tensor representation has become essential in machine learning and computer vision, but existing methods like Tucker decomposition offer flexibility at the expense of interpretability. In contrast, while the CANDECOMP/PARAFAC (CP) decomposition provides a more natural and interpretable tensor structure, obtaining sparse solutions remains challenging. Leveraging the rich properties of CP decomposition, we propose a CP-based low-rank tensor function parameterized by neural networks for implicit neural representation (CP-INR). This approach enables continuous data representation beyond structured grids, fully exploiting the non-linearity of tensor data with theoretical guarantees on excess risk bounds. To achieve a sparse CP decomposition, we introduce a variational form of the Schatten-p quasi-norm and prove its relationship to multilinear rank minimization. For smoothness, we propose a regularization term based on the spectral norm of the Jacobian and Hutchinson's trace estimator. Our proposed smoothness regularization is SVD-free and avoids explicit chain rule derivations. It can serve as an alternative to Total Variation (TV) regularization in image denoising tasks and is naturally applicable to continuous data. Extensive experiments on multi-dimensional data recovery tasks, including image inpainting, denoising, and point cloud upsampling, demonstrate the superiority and versatility of our method compared to state-of-the-art approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCP\u5206\u89e3\u7684\u4f4e\u79e9\u5f20\u91cf\u51fd\u6570\uff08CP-INR\uff09\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u8fde\u7eed\u6570\u636e\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u7a00\u758f\u6027\u548c\u5e73\u6ed1\u6027\u4f18\u5316\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f20\u91cf\u5206\u89e3\u65b9\u6cd5\uff08\u5982Tucker\u548cCP\uff09\u5728\u7075\u6d3b\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u4e14\u7a00\u758f\u89e3\u96be\u4ee5\u83b7\u5f97\u3002CP-INR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u540c\u65f6\u5229\u7528\u5f20\u91cf\u6570\u636e\u7684\u975e\u7ebf\u6027\u7279\u6027\u3002", "method": "\u91c7\u7528CP\u5206\u89e3\u53c2\u6570\u5316\u795e\u7ecf\u7f51\u7edc\uff0c\u5f15\u5165Schatten-p\u62df\u8303\u6570\u5b9e\u73b0\u7a00\u758f\u6027\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u8c31\u8303\u6570\u7684\u5e73\u6ed1\u6b63\u5219\u5316\u9879\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eCP-INR\u5728\u591a\u7ef4\u6570\u636e\u6062\u590d\u4efb\u52a1\uff08\u5982\u56fe\u50cf\u4fee\u590d\u3001\u53bb\u566a\u548c\u70b9\u4e91\u4e0a\u91c7\u6837\uff09\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CP-INR\u7ed3\u5408\u4e86CP\u5206\u89e3\u7684\u53ef\u89e3\u91ca\u6027\u548c\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4e3a\u8fde\u7eed\u6570\u636e\u8868\u793a\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7406\u8bba\u4fdd\u969c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22179", "pdf": "https://arxiv.org/pdf/2506.22179", "abs": "https://arxiv.org/abs/2506.22179", "authors": ["Wenhan Wu", "Zhishuai Guo", "Chen Chen", "Hongfei Xue", "Aidong Lu"], "title": "Frequency-Semantic Enhanced Variational Autoencoder for Zero-Shot Skeleton-based Action Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ICCV 2025", "summary": "Zero-shot skeleton-based action recognition aims to develop models capable of identifying actions beyond the categories encountered during training. Previous approaches have primarily focused on aligning visual and semantic representations but often overlooked the importance of fine-grained action patterns in the semantic space (e.g., the hand movements in drinking water and brushing teeth). To address these limitations, we propose a Frequency-Semantic Enhanced Variational Autoencoder (FS-VAE) to explore the skeleton semantic representation learning with frequency decomposition. FS-VAE consists of three key components: 1) a frequency-based enhancement module with high- and low-frequency adjustments to enrich the skeletal semantics learning and improve the robustness of zero-shot action recognition; 2) a semantic-based action description with multilevel alignment to capture both local details and global correspondence, effectively bridging the semantic gap and compensating for the inherent loss of information in skeleton sequences; 3) a calibrated cross-alignment loss that enables valid skeleton-text pairs to counterbalance ambiguous ones, mitigating discrepancies and ambiguities in skeleton and text features, thereby ensuring robust alignment. Evaluations on the benchmarks demonstrate the effectiveness of our approach, validating that frequency-enhanced semantic features enable robust differentiation of visually and semantically similar action clusters, improving zero-shot action recognition.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9891\u7387\u5206\u89e3\u7684\u9aa8\u67b6\u8bed\u4e49\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff08FS-VAE\uff09\uff0c\u901a\u8fc7\u9ad8\u9891\u548c\u4f4e\u9891\u8c03\u6574\u589e\u5f3a\u9aa8\u67b6\u8bed\u4e49\u5b66\u4e60\uff0c\u5e76\u6539\u8fdb\u96f6\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u4e2d\u5ffd\u89c6\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u6a21\u5f0f\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u8bed\u4e49\u7a7a\u95f4\u4e2d\u7c7b\u4f3c\u52a8\u4f5c\u7684\u533a\u5206\u3002", "method": "FS-VAE\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u9891\u7387\u589e\u5f3a\u6a21\u5757\u3001\u591a\u7ea7\u5bf9\u9f50\u7684\u8bed\u4e49\u52a8\u4f5c\u63cf\u8ff0\u548c\u6821\u51c6\u7684\u4ea4\u53c9\u5bf9\u9f50\u635f\u5931\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u9891\u7387\u589e\u5f3a\u7684\u8bed\u4e49\u7279\u5f81\u80fd\u591f\u533a\u5206\u89c6\u89c9\u548c\u8bed\u4e49\u76f8\u4f3c\u7684\u52a8\u4f5c\u7c07\u3002", "conclusion": "FS-VAE\u901a\u8fc7\u9891\u7387\u5206\u89e3\u548c\u8bed\u4e49\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.22216", "pdf": "https://arxiv.org/pdf/2506.22216", "abs": "https://arxiv.org/abs/2506.22216", "authors": ["Ming Zhao", "Pingping Liu", "Tongshun Zhang", "Zhe Zhang"], "title": "ReF-LLE: Personalized Low-Light Enhancement via Reference-Guided Deep Reinforcement Learning", "categories": ["cs.CV", "eess.IV"], "comment": "6 pages, 8 figures, accepted by ICME2025", "summary": "Low-light image enhancement presents two primary challenges: 1) Significant variations in low-light images across different conditions, and 2) Enhancement levels influenced by subjective preferences and user intent. To address these issues, we propose ReF-LLE, a novel personalized low-light image enhancement method that operates in the Fourier frequency domain and incorporates deep reinforcement learning. ReF-LLE is the first to integrate deep reinforcement learning into this domain. During training, a zero-reference image evaluation strategy is introduced to score enhanced images, providing reward signals that guide the model to handle varying degrees of low-light conditions effectively. In the inference phase, ReF-LLE employs a personalized adaptive iterative strategy, guided by the zero-frequency component in the Fourier domain, which represents the overall illumination level. This strategy enables the model to adaptively adjust low-light images to align with the illumination distribution of a user-provided reference image, ensuring personalized enhancement results. Extensive experiments on benchmark datasets demonstrate that ReF-LLE outperforms state-of-the-art methods, achieving superior perceptual quality and adaptability in personalized low-light image enhancement.", "AI": {"tldr": "ReF-LLE\u662f\u4e00\u79cd\u57fa\u4e8e\u5085\u91cc\u53f6\u9891\u57df\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u4e2a\u6027\u5316\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u9996\u6b21\u5c06\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5f15\u5165\u8be5\u9886\u57df\uff0c\u901a\u8fc7\u96f6\u53c2\u8003\u56fe\u50cf\u8bc4\u4f30\u7b56\u7565\u548c\u4e2a\u6027\u5316\u81ea\u9002\u5e94\u8fed\u4ee3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u589e\u5f3a\u6548\u679c\u3002", "motivation": "\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1) \u4e0d\u540c\u6761\u4ef6\u4e0b\u4f4e\u5149\u56fe\u50cf\u7684\u663e\u8457\u53d8\u5316\uff1b2) \u589e\u5f3a\u6c34\u5e73\u53d7\u4e3b\u89c2\u504f\u597d\u548c\u7528\u6237\u610f\u56fe\u5f71\u54cd\u3002", "method": "\u63d0\u51faReF-LLE\u65b9\u6cd5\uff0c\u7ed3\u5408\u5085\u91cc\u53f6\u9891\u57df\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u8bad\u7ec3\u65f6\u5f15\u5165\u96f6\u53c2\u8003\u56fe\u50cf\u8bc4\u4f30\u7b56\u7565\uff0c\u63a8\u7406\u65f6\u91c7\u7528\u4e2a\u6027\u5316\u81ea\u9002\u5e94\u8fed\u4ee3\u7b56\u7565\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cReF-LLE\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u611f\u77e5\u8d28\u91cf\u548c\u4e2a\u6027\u5316\u9002\u5e94\u6027\u3002", "conclusion": "ReF-LLE\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u89e3\u51b3\u4e86\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u7684\u6311\u6218\uff0c\u4e3a\u4e2a\u6027\u5316\u9700\u6c42\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22246", "pdf": "https://arxiv.org/pdf/2506.22246", "abs": "https://arxiv.org/abs/2506.22246", "authors": ["Yu-Cheng Lin", "Yu-Syuan Xu", "Hao-Wei Chen", "Hsien-Kai Kuo", "Chun-Yi Lee"], "title": "EAMamba: Efficient All-Around Vision State Space Model for Image Restoration", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "Image restoration is a key task in low-level computer vision that aims to reconstruct high-quality images from degraded inputs. The emergence of Vision Mamba, which draws inspiration from the advanced state space model Mamba, marks a significant advancement in this field. Vision Mamba demonstrates excellence in modeling long-range dependencies with linear complexity, a crucial advantage for image restoration tasks. Despite its strengths, Vision Mamba encounters challenges in low-level vision tasks, including computational complexity that scales with the number of scanning sequences and local pixel forgetting. To address these limitations, this study introduces Efficient All-Around Mamba (EAMamba), an enhanced framework that incorporates a Multi-Head Selective Scan Module (MHSSM) with an all-around scanning mechanism. MHSSM efficiently aggregates multiple scanning sequences, which avoids increases in computational complexity and parameter count. The all-around scanning strategy implements multiple patterns to capture holistic information and resolves the local pixel forgetting issue. Our experimental evaluations validate these innovations across several restoration tasks, including super resolution, denoising, deblurring, and dehazing. The results validate that EAMamba achieves a significant 31-89% reduction in FLOPs while maintaining favorable performance compared to existing low-level Vision Mamba methods.", "AI": {"tldr": "EAMamba\u901a\u8fc7\u591a\u5934\u90e8\u9009\u62e9\u6027\u626b\u63cf\u6a21\u5757\u548c\u5168\u65b9\u4f4d\u626b\u63cf\u673a\u5236\uff0c\u89e3\u51b3\u4e86Vision Mamba\u5728\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u5c40\u90e8\u50cf\u7d20\u9057\u5fd8\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86FLOPs\u5e76\u4fdd\u6301\u4e86\u6027\u80fd\u3002", "motivation": "Vision Mamba\u5728\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u9762\u4e34\u8ba1\u7b97\u590d\u6742\u6027\u548c\u5c40\u90e8\u50cf\u7d20\u9057\u5fd8\u7684\u6311\u6218\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faEAMamba\u6846\u67b6\uff0c\u5f15\u5165\u591a\u5934\u90e8\u9009\u62e9\u6027\u626b\u63cf\u6a21\u5757\uff08MHSSM\uff09\u548c\u5168\u65b9\u4f4d\u626b\u63cf\u673a\u5236\uff0c\u4ee5\u9ad8\u6548\u805a\u5408\u626b\u63cf\u5e8f\u5217\u5e76\u89e3\u51b3\u5c40\u90e8\u50cf\u7d20\u9057\u5fd8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEAMamba\u5728\u591a\u9879\u6062\u590d\u4efb\u52a1\u4e2d\u663e\u8457\u964d\u4f4eFLOPs\uff0831-89%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u4f18\u52bf\u3002", "conclusion": "EAMamba\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86Vision Mamba\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22298", "pdf": "https://arxiv.org/pdf/2506.22298", "abs": "https://arxiv.org/abs/2506.22298", "authors": ["Linhao Zhong", "Fan Li", "Yi Huang", "Jianzhuang Liu", "Renjing Pei", "Fenglong Song"], "title": "OutDreamer: Video Outpainting with a Diffusion Transformer", "categories": ["cs.CV"], "comment": null, "summary": "Video outpainting is a challenging task that generates new video content by extending beyond the boundaries of an original input video, requiring both temporal and spatial consistency. Many state-of-the-art methods utilize latent diffusion models with U-Net backbones but still struggle to achieve high quality and adaptability in generated content. Diffusion transformers (DiTs) have emerged as a promising alternative because of their superior performance. We introduce OutDreamer, a DiT-based video outpainting framework comprising two main components: an efficient video control branch and a conditional outpainting branch. The efficient video control branch effectively extracts masked video information, while the conditional outpainting branch generates missing content based on these extracted conditions. Additionally, we propose a mask-driven self-attention layer that dynamically integrates the given mask information, further enhancing the model's adaptability to outpainting tasks. Furthermore, we introduce a latent alignment loss to maintain overall consistency both within and between frames. For long video outpainting, we employ a cross-video-clip refiner to iteratively generate missing content, ensuring temporal consistency across video clips. Extensive evaluations demonstrate that our zero-shot OutDreamer outperforms state-of-the-art zero-shot methods on widely recognized benchmarks.", "AI": {"tldr": "OutDreamer\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u7684\u89c6\u9891\u5916\u7ed8\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u6548\u89c6\u9891\u63a7\u5236\u5206\u652f\u548c\u6761\u4ef6\u5916\u7ed8\u5206\u652f\uff0c\u7ed3\u5408\u52a8\u6001\u63a9\u7801\u81ea\u6ce8\u610f\u529b\u5c42\u548c\u6f5c\u5728\u5bf9\u9f50\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5916\u7ed8\u7684\u8d28\u91cf\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u89c6\u9891\u5916\u7ed8\u4efb\u52a1\u5728\u751f\u6210\u8d85\u51fa\u539f\u59cb\u89c6\u9891\u8fb9\u754c\u7684\u65b0\u5185\u5bb9\u65f6\uff0c\u9700\u540c\u65f6\u4fdd\u8bc1\u65f6\u7a7a\u4e00\u81f4\u6027\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u57fa\u4e8eU-Net\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff09\u5728\u751f\u6210\u8d28\u91cf\u548c\u9002\u5e94\u6027\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002", "method": "\u63d0\u51faOutDreamer\u6846\u67b6\uff0c\u5305\u542b\u9ad8\u6548\u89c6\u9891\u63a7\u5236\u5206\u652f\u548c\u6761\u4ef6\u5916\u7ed8\u5206\u652f\uff0c\u5f15\u5165\u52a8\u6001\u63a9\u7801\u81ea\u6ce8\u610f\u529b\u5c42\u548c\u6f5c\u5728\u5bf9\u9f50\u635f\u5931\uff0c\u5e76\u91c7\u7528\u8de8\u89c6\u9891\u7247\u6bb5\u7ec6\u5316\u5668\u5904\u7406\u957f\u89c6\u9891\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOutDreamer\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "OutDreamer\u901a\u8fc7\u521b\u65b0\u7684DiT\u67b6\u6784\u548c\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5916\u7ed8\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2506.22338", "pdf": "https://arxiv.org/pdf/2506.22338", "abs": "https://arxiv.org/abs/2506.22338", "authors": ["Luigi Russo", "Deodato Tapete", "Silvia Liberata Ullo", "Paolo Gamba"], "title": "A Deep Learning framework for building damage assessment using VHR SAR and geospatial data: demonstration on the 2023 Turkiye Earthquake", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 6 figures (plus 4 author photos), and 5 tables. Submitted   to IEEE Journal of Selected Topics in Applied Earth Observations and Remote   Sensing", "summary": "Building damage identification shortly after a disaster is crucial for guiding emergency response and recovery efforts. Although optical satellite imagery is commonly used for disaster mapping, its effectiveness is often hampered by cloud cover or the absence of pre-event acquisitions. To overcome these challenges, we introduce a novel multimodal deep learning (DL) framework for detecting building damage using single-date very high resolution (VHR) Synthetic Aperture Radar (SAR) imagery from the Italian Space Agency (ASI) COSMO SkyMed (CSK) constellation, complemented by auxiliary geospatial data. Our method integrates SAR image patches, OpenStreetMap (OSM) building footprints, digital surface model (DSM) data, and structural and exposure attributes from the Global Earthquake Model (GEM) to improve detection accuracy and contextual interpretation. Unlike existing approaches that depend on pre and post event imagery, our model utilizes only post event data, facilitating rapid deployment in critical scenarios. The framework effectiveness is demonstrated using a new dataset from the 2023 earthquake in Turkey, covering multiple cities with diverse urban settings. Results highlight that incorporating geospatial features significantly enhances detection performance and generalizability to previously unseen areas. By combining SAR imagery with detailed vulnerability and exposure information, our approach provides reliable and rapid building damage assessments without the dependency from available pre-event data. Moreover, the automated and scalable data generation process ensures the framework's applicability across diverse disaster-affected regions, underscoring its potential to support effective disaster management and recovery efforts. Code and data will be made available upon acceptance of the paper.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u65e5\u671f\u9ad8\u5206\u8fa8\u7387SAR\u56fe\u50cf\u548c\u591a\u6e90\u5730\u7406\u7a7a\u95f4\u6570\u636e\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5feb\u901f\u68c0\u6d4b\u5efa\u7b51\u7269\u635f\u574f\uff0c\u65e0\u9700\u4f9d\u8d56\u707e\u524d\u6570\u636e\u3002", "motivation": "\u707e\u5bb3\u540e\u5feb\u901f\u8bc6\u522b\u5efa\u7b51\u7269\u635f\u574f\u5bf9\u5e94\u6025\u54cd\u5e94\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u5149\u5b66\u536b\u661f\u56fe\u50cf\u5e38\u53d7\u4e91\u5c42\u6216\u7f3a\u4e4f\u707e\u524d\u6570\u636e\u9650\u5236\u3002", "method": "\u7ed3\u5408SAR\u56fe\u50cf\u3001OSM\u5efa\u7b51\u8db3\u8ff9\u3001DSM\u6570\u636e\u548cGEM\u7684\u7ed3\u6784\u4e0e\u66b4\u9732\u5c5e\u6027\uff0c\u6784\u5efa\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u5728\u571f\u8033\u51762023\u5e74\u5730\u9707\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u663e\u793a\u7ed3\u5408\u5730\u7406\u7a7a\u95f4\u7279\u5f81\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u707e\u524d\u6570\u636e\uff0c\u53ef\u5feb\u901f\u3001\u53ef\u9760\u5730\u8bc4\u4f30\u5efa\u7b51\u7269\u635f\u574f\uff0c\u652f\u6301\u707e\u5bb3\u7ba1\u7406\u548c\u6062\u590d\u3002"}}
{"id": "2506.22385", "pdf": "https://arxiv.org/pdf/2506.22385", "abs": "https://arxiv.org/abs/2506.22385", "authors": ["Yue Zhang", "Jilei Sun", "Yunhui Guo", "Vibhav Gogate"], "title": "Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Video Large Multimodal Models (VLMMs) have made impressive strides in understanding video content, but they often struggle with abstract and adaptive reasoning-the ability to revise their interpretations when new information emerges. In reality, conclusions are rarely set in stone; additional context can strengthen or weaken an initial inference. To address this, we introduce Defeasible Video Entailment (DVidE), a new task that challenges models to think like doubters, constantly updating their reasoning based on evolving evidence. In DVidE, given a video premise and a textual hypothesis, models must determine whether a new update strengthens or weakens the hypothesis (classification version) or generate a coherent update that modifies the entailment relationship (generation version). For solving the classification task, we propose the Chain of Counterfactual Thought framework, utilizing counterfactual reasoning, ASR-enhanced video content, and rationale refinement to reduce inference bias. For the generation task, we develop a framework that combines ASR output with a Large Language Model (LLM) to produce coherent, contextually relevant updates aligned with the intended strengthener or weakener goals. Additionally, we introduce a novel benchmark dataset, with strengthener/weakener annotations and an LLM-based evaluation metric specifically designed for assessing generative performance. Experimental results demonstrate significant improvements, highlighting our proposed method in enhancing dynamic reasoning capabilities of VLMMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u4efb\u52a1DVidE\uff0c\u65e8\u5728\u63d0\u5347\u89c6\u9891\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08VLMMs\uff09\u7684\u52a8\u6001\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u5f15\u5165\u53cd\u4e8b\u5b9e\u63a8\u7406\u548cASR\u589e\u5f3a\u6280\u672f\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u6a21\u578b\u7684\u9002\u5e94\u6027\u63a8\u7406\u8868\u73b0\u3002", "motivation": "\u73b0\u6709VLMMs\u5728\u62bd\u8c61\u548c\u9002\u5e94\u6027\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6839\u636e\u65b0\u4fe1\u606f\u52a8\u6001\u8c03\u6574\u63a8\u7406\u7ed3\u8bba\u3002DVidE\u4efb\u52a1\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faChain of Counterfactual Thought\u6846\u67b6\u7528\u4e8e\u5206\u7c7b\u4efb\u52a1\uff0c\u7ed3\u5408ASR\u589e\u5f3a\u89c6\u9891\u5185\u5bb9\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\uff1b\u751f\u6210\u4efb\u52a1\u5219\u7ed3\u5408ASR\u8f93\u51fa\u548cLLM\u751f\u6210\u8fde\u8d2f\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86VLMMs\u7684\u52a8\u6001\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "DVidE\u4efb\u52a1\u53ca\u76f8\u5173\u6846\u67b6\u4e3a\u63d0\u5347VLMMs\u7684\u9002\u5e94\u6027\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22432", "pdf": "https://arxiv.org/pdf/2506.22432", "abs": "https://arxiv.org/abs/2506.22432", "authors": ["Yuhao Liu", "Tengfei Wang", "Fang Liu", "Zhenwei Wang", "Rynson W. H. Lau"], "title": "Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in deep generative modeling have unlocked unprecedented opportunities for video synthesis. In real-world applications, however, users often seek tools to faithfully realize their creative editing intentions with precise and consistent control. Despite the progress achieved by existing methods, ensuring fine-grained alignment with user intentions remains an open and challenging problem. In this work, we present Shape-for-Motion, a novel framework that incorporates a 3D proxy for precise and consistent video editing. Shape-for-Motion achieves this by converting the target object in the input video to a time-consistent mesh, i.e., a 3D proxy, allowing edits to be performed directly on the proxy and then inferred back to the video frames. To simplify the editing process, we design a novel Dual-Propagation Strategy that allows users to perform edits on the 3D mesh of a single frame, and the edits are then automatically propagated to the 3D meshes of the other frames. The 3D meshes for different frames are further projected onto the 2D space to produce the edited geometry and texture renderings, which serve as inputs to a decoupled video diffusion model for generating edited results. Our framework supports various precise and physically-consistent manipulations across the video frames, including pose editing, rotation, scaling, translation, texture modification, and object composition. Our approach marks a key step toward high-quality, controllable video editing workflows. Extensive experiments demonstrate the superiority and effectiveness of our approach. Project page: https://shapeformotion.github.io/", "AI": {"tldr": "Shape-for-Motion \u662f\u4e00\u79cd\u65b0\u578b\u89c6\u9891\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc73D\u4ee3\u7406\u5b9e\u73b0\u7cbe\u786e\u4e14\u4e00\u81f4\u7684\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u73b0\u7528\u6237\u7cbe\u786e\u63a7\u5236\u89c6\u9891\u7f16\u8f91\u610f\u56fe\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u75283D\u4ee3\u7406\uff08\u65f6\u95f4\u4e00\u81f4\u7684\u7f51\u683c\uff09\u8fdb\u884c\u7f16\u8f91\uff0c\u5e76\u901a\u8fc7\u53cc\u4f20\u64ad\u7b56\u7565\u7b80\u5316\u7f16\u8f91\u8fc7\u7a0b\uff0c\u6700\u7ec8\u901a\u8fc7\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u7ed3\u679c\u3002", "result": "\u652f\u6301\u591a\u79cd\u7cbe\u786e\u4e14\u7269\u7406\u4e00\u81f4\u7684\u64cd\u4f5c\uff08\u5982\u59ff\u6001\u7f16\u8f91\u3001\u65cb\u8f6c\u3001\u7eb9\u7406\u4fee\u6539\u7b49\uff09\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "Shape-for-Motion \u662f\u9ad8\u8d28\u91cf\u3001\u53ef\u63a7\u89c6\u9891\u7f16\u8f91\u5de5\u4f5c\u6d41\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2506.21714", "pdf": "https://arxiv.org/pdf/2506.21714", "abs": "https://arxiv.org/abs/2506.21714", "authors": ["Denis Gudovskiy", "Wenzhao Zheng", "Tomoyuki Okuno", "Yohei Nakata", "Kurt Keutzer"], "title": "$\\textrm{ODE}_t \\left(\\textrm{ODE}_l \\right)$: Shortcutting the Time and Length in Diffusion and Flow Models for Faster Sampling", "categories": ["cs.LG", "cs.CV"], "comment": "Preprint. Github page: github.com/gudovskiy/odelt", "summary": "Recently, continuous normalizing flows (CNFs) and diffusion models (DMs) have been studied using the unified theoretical framework. Although such models can generate high-quality data points from a noise distribution, the sampling demands multiple iterations to solve an ordinary differential equation (ODE) with high computational complexity. Most existing methods focus on reducing the number of time steps during the sampling process to improve efficiency. In this work, we explore a complementary direction in which the quality-complexity tradeoff can be dynamically controlled in terms of time steps and in the length of the neural network. We achieve this by rewiring the blocks in the transformer-based architecture to solve an inner discretized ODE w.r.t. its length. Then, we employ time- and length-wise consistency terms during flow matching training, and as a result, the sampling can be performed with an arbitrary number of time steps and transformer blocks. Unlike others, our $\\textrm{ODE}_t \\left(\\textrm{ODE}_l \\right)$ approach is solver-agnostic in time dimension and decreases both latency and memory usage. Compared to the previous state of the art, image generation experiments on CelebA-HQ and ImageNet show a latency reduction of up to $3\\times$ in the most efficient sampling mode, and a FID score improvement of up to $3.5$ points for high-quality sampling. We release our code and model weights with fully reproducible experiments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u63a7\u5236\u8d28\u91cf-\u590d\u6742\u5ea6\u6743\u8861\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u65f6\u95f4\u6b65\u957f\u548c\u795e\u7ecf\u7f51\u7edc\u957f\u5ea6\uff0c\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u9ad8\u6548\u6027\u548c\u8d28\u91cf\u7684\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u51cf\u5c11\u91c7\u6837\u65f6\u95f4\u6b65\u6570\u4ee5\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u5ffd\u7565\u4e86\u52a8\u6001\u8c03\u6574\u7f51\u7edc\u957f\u5ea6\u7684\u6f5c\u529b\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u8fd9\u4e00\u4e92\u8865\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u91cd\u65b0\u8fde\u63a5\u57fa\u4e8eTransformer\u67b6\u6784\u7684\u5757\u6765\u6c42\u89e3\u5185\u90e8\u79bb\u6563ODE\uff0c\u5e76\u5728\u8bad\u7ec3\u4e2d\u5f15\u5165\u65f6\u95f4\u548c\u957f\u5ea6\u4e00\u81f4\u6027\u9879\uff0c\u5b9e\u73b0\u4efb\u610f\u65f6\u95f4\u6b65\u548c\u5757\u6570\u7684\u91c7\u6837\u3002", "result": "\u5728CelebA-HQ\u548cImageNet\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6700\u9ad8\u6548\u91c7\u6837\u6a21\u5f0f\u4e0b\u5ef6\u8fdf\u51cf\u5c113\u500d\uff0c\u9ad8\u8d28\u91cf\u91c7\u6837\u6a21\u5f0f\u4e0bFID\u5206\u6570\u63d0\u53473.5\u5206\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u65f6\u95f4\u7ef4\u5ea6\u548c\u957f\u5ea6\u7ef4\u5ea6\u4e0a\u5747\u5177\u6709\u7075\u6d3b\u6027\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u548c\u5185\u5b58\u4f7f\u7528\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2506.21748", "pdf": "https://arxiv.org/pdf/2506.21748", "abs": "https://arxiv.org/abs/2506.21748", "authors": ["Liav Hen", "Erez Yosef", "Dan Raviv", "Raja Giryes", "Jacob Scheuer"], "title": "Inverse Design of Diffractive Metasurfaces Using Diffusion Models", "categories": ["physics.optics", "cs.CV", "cs.LG"], "comment": null, "summary": "Metasurfaces are ultra-thin optical elements composed of engineered sub-wavelength structures that enable precise control of light. Their inverse design - determining a geometry that yields a desired optical response - is challenging due to the complex, nonlinear relationship between structure and optical properties. This often requires expert tuning, is prone to local minima, and involves significant computational overhead. In this work, we address these challenges by integrating the generative capabilities of diffusion models into computational design workflows. Using an RCWA simulator, we generate training data consisting of metasurface geometries and their corresponding far-field scattering patterns. We then train a conditional diffusion model to predict meta-atom geometry and height from a target spatial power distribution at a specified wavelength, sampled from a continuous supported band. Once trained, the model can generate metasurfaces with low error, either directly using RCWA-guided posterior sampling or by serving as an initializer for traditional optimization methods. We demonstrate our approach on the design of a spatially uniform intensity splitter and a polarization beam splitter, both produced with low error in under 30 minutes. To support further research in data-driven metasurface design, we publicly release our code and datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9006\u5411\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u8bbe\u8ba1\u8d85\u8868\u9762\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u5c40\u90e8\u6700\u4f18\u548c\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\u3002", "motivation": "\u8d85\u8868\u9762\u7684\u9006\u5411\u8bbe\u8ba1\u56e0\u7ed3\u6784\u4e0e\u5149\u5b66\u6027\u8d28\u7684\u975e\u7ebf\u6027\u5173\u7cfb\u800c\u590d\u6742\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u8c03\u53c2\u4e14\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u7ed3\u5408\u6269\u6563\u6a21\u578b\u4e0e\u8ba1\u7b97\u8bbe\u8ba1\u6d41\u7a0b\uff0c\u5229\u7528RCWA\u6a21\u62df\u5668\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u8bad\u7ec3\u6761\u4ef6\u6269\u6563\u6a21\u578b\u4ee5\u9884\u6d4b\u76ee\u6807\u5149\u5b66\u54cd\u5e94\u5bf9\u5e94\u7684\u8d85\u8868\u9762\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u6a21\u578b\u80fd\u9ad8\u6548\u751f\u6210\u4f4e\u8bef\u5dee\u8d85\u8868\u9762\uff0c\u5982\u5747\u5300\u5f3a\u5ea6\u5206\u675f\u5668\u548c\u504f\u632f\u5206\u675f\u5668\uff0c\u8bbe\u8ba1\u65f6\u95f4\u5c11\u4e8e30\u5206\u949f\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u4e3a\u8d85\u8868\u9762\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\uff0c\u516c\u5f00\u7684\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u63a8\u52a8\u6570\u636e\u9a71\u52a8\u8bbe\u8ba1\u7684\u7814\u7a76\u3002"}}
{"id": "2506.21884", "pdf": "https://arxiv.org/pdf/2506.21884", "abs": "https://arxiv.org/abs/2506.21884", "authors": ["Fabian Perez", "Sara Rojas", "Carlos Hinojosa", "Hoover Rueda-Chac\u00f3n", "Bernard Ghanem"], "title": "UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "eess.SP"], "comment": "Paper accepted at ICCV 2025 main conference", "summary": "Neural Radiance Field (NeRF)-based segmentation methods focus on object semantics and rely solely on RGB data, lacking intrinsic material properties. This limitation restricts accurate material perception, which is crucial for robotics, augmented reality, simulation, and other applications. We introduce UnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling joint hyperspectral novel view synthesis and unsupervised material segmentation. Our method models spectral reflectance via diffuse and specular components, where a learned dictionary of global endmembers represents pure material signatures, and per-point abundances capture their distribution. For material segmentation, we use spectral signature predictions along learned endmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF enables scene editing by modifying learned endmember dictionaries for flexible material-based appearance manipulation. Extensive experiments validate our approach, demonstrating superior spectral reconstruction and material segmentation to existing methods. Project page: https://www.factral.co/UnMix-NeRF.", "AI": {"tldr": "UnMix-NeRF\u7ed3\u5408\u5149\u8c31\u89e3\u6df7\u4e0eNeRF\uff0c\u5b9e\u73b0\u8054\u5408\u9ad8\u5149\u8c31\u65b0\u89c6\u89d2\u5408\u6210\u548c\u65e0\u76d1\u7763\u6750\u6599\u5206\u5272\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u6750\u6599\u5c5e\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709NeRF\u5206\u5272\u65b9\u6cd5\u4ec5\u4f9d\u8d56RGB\u6570\u636e\uff0c\u7f3a\u4e4f\u6750\u6599\u5c5e\u6027\uff0c\u9650\u5236\u4e86\u5728\u673a\u5668\u4eba\u3001\u589e\u5f3a\u73b0\u5b9e\u7b49\u5e94\u7528\u4e2d\u7684\u51c6\u786e\u6750\u6599\u611f\u77e5\u3002", "method": "\u901a\u8fc7\u5efa\u6a21\u5149\u8c31\u53cd\u5c04\u7684\u6f2b\u53cd\u5c04\u548c\u955c\u9762\u53cd\u5c04\u5206\u91cf\uff0c\u5229\u7528\u5168\u5c40\u7aef\u5143\u5b57\u5178\u8868\u793a\u7eaf\u6750\u6599\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u70b9\u7ea7\u4e30\u5ea6\u5206\u5e03\u5b9e\u73b0\u65e0\u76d1\u7763\u6750\u6599\u5206\u5272\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86UnMix-NeRF\u5728\u9ad8\u5149\u8c31\u91cd\u5efa\u548c\u6750\u6599\u5206\u5272\u4e0a\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "UnMix-NeRF\u4e3a\u6750\u6599\u611f\u77e5\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u65e0\u76d1\u7763\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u573a\u666f\u7f16\u8f91\u548c\u6750\u6599\u64cd\u4f5c\u3002"}}
{"id": "2506.21977", "pdf": "https://arxiv.org/pdf/2506.21977", "abs": "https://arxiv.org/abs/2506.21977", "authors": ["Tianyu Zhang", "Xin Luo", "Li Li", "Dong Liu"], "title": "StableCodec: Taming One-Step Diffusion for Extreme Image Compression", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Diffusion-based image compression has shown remarkable potential for achieving ultra-low bitrate coding (less than 0.05 bits per pixel) with high realism, by leveraging the generative priors of large pre-trained text-to-image diffusion models. However, current approaches require a large number of denoising steps at the decoder to generate realistic results under extreme bitrate constraints, limiting their application in real-time compression scenarios. Additionally, these methods often sacrifice reconstruction fidelity, as diffusion models typically fail to guarantee pixel-level consistency. To address these challenges, we introduce StableCodec, which enables one-step diffusion for high-fidelity and high-realism extreme image compression with improved coding efficiency. To achieve ultra-low bitrates, we first develop an efficient Deep Compression Latent Codec to transmit a noisy latent representation for a single-step denoising process. We then propose a Dual-Branch Coding Structure, consisting of a pair of auxiliary encoder and decoder, to enhance reconstruction fidelity. Furthermore, we adopt end-to-end optimization with joint bitrate and pixel-level constraints. Extensive experiments on the CLIC 2020, DIV2K, and Kodak dataset demonstrate that StableCodec outperforms existing methods in terms of FID, KID and DISTS by a significant margin, even at bitrates as low as 0.005 bits per pixel, while maintaining strong fidelity. Additionally, StableCodec achieves inference speeds comparable to mainstream transform coding schemes. All source code are available at https://github.com/LuizScarlet/StableCodec.", "AI": {"tldr": "StableCodec\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e00\u6b65\u6269\u6563\u5b9e\u73b0\u9ad8\u4fdd\u771f\u548c\u9ad8\u771f\u5b9e\u611f\u7684\u8d85\u4f4e\u6bd4\u7279\u7387\u538b\u7f29\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u591a\u6b65\u53bb\u566a\u548c\u4fdd\u771f\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u8d85\u4f4e\u6bd4\u7279\u7387\u4e0b\u9700\u8981\u5927\u91cf\u53bb\u566a\u6b65\u9aa4\u4e14\u96be\u4ee5\u4fdd\u8bc1\u50cf\u7d20\u7ea7\u4e00\u81f4\u6027\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u538b\u7f29\u5e94\u7528\u3002", "method": "\u63d0\u51faStableCodec\uff0c\u7ed3\u5408\u9ad8\u6548\u7684\u6df1\u5ea6\u538b\u7f29\u6f5c\u5728\u7f16\u89e3\u7801\u5668\u548c\u53cc\u5206\u652f\u7f16\u7801\u7ed3\u6784\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u4f18\u5316\u5b9e\u73b0\u4e00\u6b65\u6269\u6563\u538b\u7f29\u3002", "result": "\u5728CLIC 2020\u3001DIV2K\u548cKodak\u6570\u636e\u96c6\u4e0a\uff0cStableCodec\u5728FID\u3001KID\u548cDISTS\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6bd4\u7279\u7387\u4f4e\u81f30.005 bpp\u3002", "conclusion": "StableCodec\u5728\u8d85\u4f4e\u6bd4\u7279\u7387\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u771f\u5b9e\u611f\u548c\u9ad8\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u63a8\u7406\u901f\u5ea6\u4e0e\u4e3b\u6d41\u53d8\u6362\u7f16\u7801\u65b9\u6848\u76f8\u5f53\u3002"}}
{"id": "2506.22012", "pdf": "https://arxiv.org/pdf/2506.22012", "abs": "https://arxiv.org/abs/2506.22012", "authors": ["Qi Gao", "Zhihao Chen", "Dong Zeng", "Junping Zhang", "Jianhua Ma", "Hongming Shan"], "title": "Noise-Inspired Diffusion Model for Generalizable Low-Dose CT Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted for publication in Medical Image Analysis, 2025", "summary": "The generalization of deep learning-based low-dose computed tomography (CT) reconstruction models to doses unseen in the training data is important and remains challenging. Previous efforts heavily rely on paired data to improve the generalization performance and robustness through collecting either diverse CT data for re-training or a few test data for fine-tuning. Recently, diffusion models have shown promising and generalizable performance in low-dose CT (LDCT) reconstruction, however, they may produce unrealistic structures due to the CT image noise deviating from Gaussian distribution and imprecise prior information from the guidance of noisy LDCT images. In this paper, we propose a noise-inspired diffusion model for generalizable LDCT reconstruction, termed NEED, which tailors diffusion models for noise characteristics of each domain. First, we propose a novel shifted Poisson diffusion model to denoise projection data, which aligns the diffusion process with the noise model in pre-log LDCT projections. Second, we devise a doubly guided diffusion model to refine reconstructed images, which leverages LDCT images and initial reconstructions to more accurately locate prior information and enhance reconstruction fidelity. By cascading these two diffusion models for dual-domain reconstruction, our NEED requires only normal-dose data for training and can be effectively extended to various unseen dose levels during testing via a time step matching strategy. Extensive qualitative, quantitative, and segmentation-based evaluations on two datasets demonstrate that our NEED consistently outperforms state-of-the-art methods in reconstruction and generalization performance. Source code is made available at https://github.com/qgao21/NEED.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNEED\u7684\u566a\u58f0\u542f\u53d1\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u4f4e\u5242\u91cfCT\u91cd\u5efa\uff0c\u901a\u8fc7\u53cc\u57df\u6269\u6563\u6a21\u578b\u548c\u566a\u58f0\u7279\u6027\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u4f4e\u5242\u91cfCT\u91cd\u5efa\u4e2d\u5bf9\u672a\u89c1\u5242\u91cf\u6570\u636e\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u5bf9\u914d\u5bf9\u6570\u636e\u7684\u4f9d\u8d56\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u566a\u58f0\u7279\u6027\u7684\u53cc\u57df\u6269\u6563\u6a21\u578b\uff1a1) \u6295\u5f71\u57df\u7684\u53bb\u566a\u6269\u6563\u6a21\u578b\uff1b2) \u56fe\u50cf\u57df\u7684\u53cc\u91cd\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cNEED\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "NEED\u901a\u8fc7\u566a\u58f0\u5bf9\u9f50\u548c\u53cc\u57df\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u989d\u5916\u6570\u636e\u7684\u9ad8\u6548\u6cdb\u5316\u4f4e\u5242\u91cfCT\u91cd\u5efa\u3002"}}
{"id": "2506.22280", "pdf": "https://arxiv.org/pdf/2506.22280", "abs": "https://arxiv.org/abs/2506.22280", "authors": ["Yuliang Huang", "Imraj Singh", "Thomas Joyce", "Kris Thielemans", "Jamie R. McClelland"], "title": "DIGS: Dynamic CBCT Reconstruction using Deformation-Informed 4D Gaussian Splatting and a Low-Rank Free-Form Deformation Model", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by MICCAI 2025", "summary": "3D Cone-Beam CT (CBCT) is widely used in radiotherapy but suffers from motion artifacts due to breathing. A common clinical approach mitigates this by sorting projections into respiratory phases and reconstructing images per phase, but this does not account for breathing variability. Dynamic CBCT instead reconstructs images at each projection, capturing continuous motion without phase sorting. Recent advancements in 4D Gaussian Splatting (4DGS) offer powerful tools for modeling dynamic scenes, yet their application to dynamic CBCT remains underexplored. Existing 4DGS methods, such as HexPlane, use implicit motion representations, which are computationally expensive. While explicit low-rank motion models have been proposed, they lack spatial regularization, leading to inconsistencies in Gaussian motion. To address these limitations, we introduce a free-form deformation (FFD)-based spatial basis function and a deformation-informed framework that enforces consistency by coupling the temporal evolution of Gaussian's mean position, scale, and rotation under a unified deformation field. We evaluate our approach on six CBCT datasets, demonstrating superior image quality with a 6x speedup over HexPlane. These results highlight the potential of deformation-informed 4DGS for efficient, motion-compensated CBCT reconstruction. The code is available at https://github.com/Yuliang-Huang/DIGS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u7531\u5f62\u53d8\uff08FFD\uff09\u76844D\u9ad8\u65af\u6cfc\u6e85\uff084DGS\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001CBCT\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u8fd0\u52a8\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u52a8\u6001CBCT\u5728\u653e\u7597\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u547c\u5438\u8fd0\u52a8\u5bfc\u81f4\u56fe\u50cf\u4f2a\u5f71\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u76f8\u4f4d\u6392\u5e8f\u65e0\u6cd5\u5904\u7406\u547c\u5438\u53d8\u5f02\u6027\uff0c\u800c4DGS\u65b9\u6cd5\u5982HexPlane\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u7f3a\u4e4f\u7a7a\u95f4\u6b63\u5219\u5316\u3002", "method": "\u5f15\u5165FFD\u7a7a\u95f4\u57fa\u51fd\u6570\u548c\u53d8\u5f62\u611f\u77e5\u6846\u67b6\uff0c\u7edf\u4e00\u9ad8\u65af\u5747\u503c\u4f4d\u7f6e\u3001\u5c3a\u5ea6\u548c\u65cb\u8f6c\u7684\u65f6\u95f4\u6f14\u5316\uff0c\u786e\u4fdd\u8fd0\u52a8\u4e00\u81f4\u6027\u3002", "result": "\u5728\u516d\u4e2aCBCT\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u56fe\u50cf\u8d28\u91cf\u4f18\u4e8eHexPlane\uff0c\u901f\u5ea6\u63d0\u53476\u500d\u3002", "conclusion": "\u53d8\u5f62\u611f\u77e54DGS\u4e3a\u9ad8\u6548\u3001\u8fd0\u52a8\u8865\u507f\u7684CBCT\u91cd\u5efa\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2506.22397", "pdf": "https://arxiv.org/pdf/2506.22397", "abs": "https://arxiv.org/abs/2506.22397", "authors": ["Anirban Ray", "Ashesh", "Florian Jug"], "title": "Dehazing Light Microscopy Images with Guided Conditional Flow Matching: finding a sweet spot between fidelity and realism", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "supplement pending, 4 figures, 10 pages + refs", "summary": "Fluorescence microscopy is a major driver of scientific progress in the life sciences. Although high-end confocal microscopes are capable of filtering out-of-focus light, cheaper and more accessible microscopy modalities, such as widefield microscopy, can not, which consequently leads to hazy image data. Computational dehazing is trying to combine the best of both worlds, leading to cheap microscopy but crisp-looking images. The perception-distortion trade-off tells us that we can optimize either for data fidelity, e.g. low MSE or high PSNR, or for data realism, measured by perceptual metrics such as LPIPS or FID. Existing methods either prioritize fidelity at the expense of realism, or produce perceptually convincing results that lack quantitative accuracy. In this work, we propose HazeMatching, a novel iterative method for dehazing light microscopy images, which effectively balances these objectives. Our goal was to find a balanced trade-off between the fidelity of the dehazing results and the realism of individual predictions (samples). We achieve this by adapting the conditional flow matching framework by guiding the generative process with a hazy observation in the conditional velocity field. We evaluate HazeMatching on 5 datasets, covering both synthetic and real data, assessing both distortion and perceptual quality. Our method is compared against 7 baselines, achieving a consistent balance between fidelity and realism on average. Additionally, with calibration analysis, we show that HazeMatching produces well-calibrated predictions. Note that our method does not need an explicit degradation operator to exist, making it easily applicable on real microscopy data. All data used for training and evaluation and our code will be publicly available under a permissive license.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHazeMatching\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5e73\u8861\u8367\u5149\u663e\u5fae\u955c\u56fe\u50cf\u53bb\u96fe\u4e2d\u7684\u4fdd\u771f\u5ea6\u548c\u771f\u5b9e\u611f\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u53bb\u96fe\u8fc7\u7a0b\u4e2d\u65e0\u6cd5\u540c\u65f6\u517c\u987e\u6570\u636e\u4fdd\u771f\u5ea6\u548c\u611f\u77e5\u771f\u5b9e\u611f\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6761\u4ef6\u6d41\u5339\u914d\u6846\u67b6\uff0c\u901a\u8fc7\u6761\u4ef6\u901f\u5ea6\u573a\u5f15\u5bfc\u751f\u6210\u8fc7\u7a0b\uff0c\u5e73\u8861\u4fdd\u771f\u5ea6\u548c\u771f\u5b9e\u611f\u3002", "result": "\u57285\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cHazeMatching\u5728\u4fdd\u771f\u5ea6\u548c\u771f\u5b9e\u611f\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u4f18\u4e8e7\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "HazeMatching\u65e0\u9700\u663e\u5f0f\u9000\u5316\u7b97\u5b50\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u663e\u5fae\u955c\u6570\u636e\uff0c\u4e14\u9884\u6d4b\u7ed3\u679c\u6821\u51c6\u826f\u597d\u3002"}}
