<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 4]
- [cs.CV](#cs.CV) [Total: 23]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Perceive-Sample-Compress: Towards Real-Time 3D Gaussian Splatting](https://arxiv.org/abs/2508.04965)
*Zijian Wang,Beizhen Zhao,Hao Wang*

Main category: cs.GR

TL;DR: 提出了一种感知-采样-压缩框架，优化3D高斯泼溅技术，提升大场景管理和存储效率。


<details>
  <summary>Details</summary>
Motivation: 传统3D高斯泼溅技术在大场景管理和存储效率上表现不佳，尤其在复杂环境或资源有限时。

Method: 引入场景感知补偿算法和金字塔采样表示，结合广义高斯混合模型压缩算法。

Result: 实验表明，该方法显著提升内存效率和视觉质量，同时保持实时渲染速度。

Conclusion: 新框架有效解决了3D高斯泼溅技术的存储和效率问题，适用于复杂场景。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated remarkable capabilities in real-time and photorealistic novel view synthesis. However, traditional 3DGS representations often struggle with large-scale scene management and efficient storage, particularly when dealing with complex environments or limited computational resources. To address these limitations, we introduce a novel perceive-sample-compress framework for 3D Gaussian Splatting. Specifically, we propose a scene perception compensation algorithm that intelligently refines Gaussian parameters at each level. This algorithm intelligently prioritizes visual importance for higher fidelity rendering in critical areas, while optimizing resource usage and improving overall visible quality. Furthermore, we propose a pyramid sampling representation to manage Gaussian primitives across hierarchical levels. Finally, to facilitate efficient storage of proposed hierarchical pyramid representations, we develop a Generalized Gaussian Mixed model compression algorithm to achieve significant compression ratios without sacrificing visual fidelity. The extensive experiments demonstrate that our method significantly improves memory efficiency and high visual quality while maintaining real-time rendering speed.

</details>


### [2] [Laplacian Analysis Meets Dynamics Modelling: Gaussian Splatting for 4D Reconstruction](https://arxiv.org/abs/2508.04966)
*Yifan Zhou,Beizhen Zhao,Pengcheng Wu,Hao Wang*

Main category: cs.GR

TL;DR: 提出了一种新的动态3D高斯泼溅框架，结合显隐式函数，解决了现有方法在动态场景中的过平滑和特征碰撞问题。


<details>
  <summary>Details</summary>
Motivation: 现有动态3DGS方法在保留运动细节和变形一致性之间存在频谱冲突，导致过平滑或特征碰撞。

Method: 采用混合显隐式函数框架，包括频谱感知拉普拉斯编码架构、增强的高斯动态属性和自适应高斯分裂策略。

Result: 在复杂动态场景重建中表现优异，实现了更高的重建保真度。

Conclusion: 提出的方法在动态3DGS中达到了最先进的性能。

Abstract: While 3D Gaussian Splatting (3DGS) excels in static scene modeling, its extension to dynamic scenes introduces significant challenges. Existing dynamic 3DGS methods suffer from either over-smoothing due to low-rank decomposition or feature collision from high-dimensional grid sampling. This is because of the inherent spectral conflicts between preserving motion details and maintaining deformation consistency at different frequency. To address these challenges, we propose a novel dynamic 3DGS framework with hybrid explicit-implicit functions. Our approach contains three key innovations: a spectral-aware Laplacian encoding architecture which merges Hash encoding and Laplacian-based module for flexible frequency motion control, an enhanced Gaussian dynamics attribute that compensates for photometric distortions caused by geometric deformation, and an adaptive Gaussian split strategy guided by KDTree-based primitive control to efficiently query and optimize dynamic areas. Through extensive experiments, our method demonstrates state-of-the-art performance in reconstructing complex dynamic scenes, achieving better reconstruction fidelity.

</details>


### [3] [A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding](https://arxiv.org/abs/2508.05064)
*Mahmoud Chick Zaouali,Todd Charter,Yehor Karpichev,Brandon Haworth,Homayoun Najjjaran*

Main category: cs.GR

TL;DR: 该论文综述了结合语言引导与3D高斯泼溅技术的研究现状，探讨了理论基础、集成策略及实际应用，并指出了计算瓶颈、泛化性和语义标注数据不足等关键限制。


<details>
  <summary>Details</summary>
Motivation: 高斯泼溅技术为实时3D场景表示提供了高效且富有表现力的方法，但其与语言模型的结合尚未有全面综述。本文旨在填补这一空白。

Method: 通过结构化综述，分析当前结合语言引导与高斯泼溅的研究，包括理论基础、集成策略和实际用例。

Result: 总结了语言引导高斯泼溅技术的进展，并指出计算瓶颈、泛化性和数据稀缺等关键限制。

Conclusion: 未来研究方向包括提升语言引导的3D场景理解能力，克服现有技术限制。

Abstract: Gaussian Splatting has rapidly emerged as a transformative technique for real-time 3D scene representation, offering a highly efficient and expressive alternative to Neural Radiance Fields (NeRF). Its ability to render complex scenes with high fidelity has enabled progress across domains such as scene reconstruction, robotics, and interactive content creation. More recently, the integration of Large Language Models (LLMs) and language embeddings into Gaussian Splatting pipelines has opened new possibilities for text-conditioned generation, editing, and semantic scene understanding. Despite these advances, a comprehensive overview of this emerging intersection has been lacking. This survey presents a structured review of current research efforts that combine language guidance with 3D Gaussian Splatting, detailing theoretical foundations, integration strategies, and real-world use cases. We highlight key limitations such as computational bottlenecks, generalizability, and the scarcity of semantically annotated 3D Gaussian data and outline open challenges and future directions for advancing language-guided 3D scene understanding using Gaussian Splatting.

</details>


### [4] [Refining Gaussian Splatting: A Volumetric Densification Approach](https://arxiv.org/abs/2508.05187)
*Mohamed Abdul Gafoor,Marius Preda,Titus Zaharia*

Main category: cs.GR

TL;DR: 提出了一种基于惯性体积的新型密度控制方法，改进了3D高斯泼溅（3DGS）的点管理，提升了新视角合成的质量。


<details>
  <summary>Details</summary>
Motivation: 解决3DGS中自适应密度控制（ADC）的不足，优化点原语管理以提高合成质量。

Method: 利用高斯函数的惯性体积指导密度控制，并比较了传统SfM和深度图像匹配（DIM）的点云初始化效果。

Result: 在Mip-NeRF 360数据集上，新方法在重建质量上优于3DGS，适用于多样场景。

Conclusion: 提出的密度控制方法有效提升了3DGS的性能，为新视角合成提供了更高质量的解决方案。

Abstract: Achieving high-quality novel view synthesis in 3D Gaussian Splatting (3DGS) often depends on effective point primitive management. The underlying Adaptive Density Control (ADC) process addresses this issue by automating densification and pruning. Yet, the vanilla 3DGS densification strategy shows key shortcomings. To address this issue, in this paper we introduce a novel density control method, which exploits the volumes of inertia associated to each Gaussian function to guide the refinement process. Furthermore, we study the effect of both traditional Structure from Motion (SfM) and Deep Image Matching (DIM) methods for point cloud initialization. Extensive experimental evaluations on the Mip-NeRF 360 dataset demonstrate that our approach surpasses 3DGS in reconstruction quality, delivering encouraging performance across diverse scenes.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [5] [RetinexDual: Retinex-based Dual Nature Approach for Generalized Ultra-High-Definition Image Restoration](https://arxiv.org/abs/2508.04797)
*Mohab Kishawy,Ali Abdellatif Hussein,Jun Chen*

Main category: cs.CV

TL;DR: RetinexDual是一个基于Retinex理论的新型框架，用于超高清图像恢复任务，通过两个互补子网络（SAMBA和FIA）解决传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如下采样或频域转换）在超高清图像恢复中存在信息丢失或局部退化问题，RetinexDual旨在克服这些限制。

Method: RetinexDual包含两个子网络：SAMBA（用于反射分量校正）和FIA（用于频域中的颜色和光照校正）。

Result: 在四种超高清图像恢复任务中，RetinexDual在质量和数量上均优于现有方法。

Conclusion: RetinexDual通过互补设计和各组件的高效协作，显著提升了超高清图像恢复的性能。

Abstract: Advancements in image sensing have elevated the importance of Ultra-High-Definition Image Restoration (UHD IR). Traditional methods, such as extreme downsampling or transformation from the spatial to the frequency domain, encounter significant drawbacks: downsampling induces irreversible information loss in UHD images, while our frequency analysis reveals that pure frequency-domain approaches are ineffective for spatially confined image artifacts, primarily due to the loss of degradation locality. To overcome these limitations, we present RetinexDual, a novel Retinex theory-based framework designed for generalized UHD IR tasks. RetinexDual leverages two complementary sub-networks: the Scale-Attentive maMBA (SAMBA) and the Frequency Illumination Adaptor (FIA). SAMBA, responsible for correcting the reflectance component, utilizes a coarse-to-fine mechanism to overcome the causal modeling of mamba, which effectively reduces artifacts and restores intricate details. On the other hand, FIA ensures precise correction of color and illumination distortions by operating in the frequency domain and leveraging the global context provided by it. Evaluating RetinexDual on four UHD IR tasks, namely deraining, deblurring, dehazing, and Low-Light Image Enhancement (LLIE), shows that it outperforms recent methods qualitatively and quantitatively. Ablation studies demonstrate the importance of employing distinct designs for each branch in RetinexDual, as well as the effectiveness of its various components.

</details>


### [6] [Single-Step Reconstruction-Free Anomaly Detection and Segmentation via Diffusion Models](https://arxiv.org/abs/2508.04818)
*Mehrdad Moradi,Marco Grasso,Bianca Maria Colosimo,Kamran Paynabar*

Main category: cs.CV

TL;DR: RADAR是一种基于扩散模型的无重建实时异常检测方法，解决了传统重建方法的计算成本高、模式混淆和噪声选择问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于重建的扩散模型异常检测方法存在计算成本高、模式混淆和噪声选择困难的问题，限制了其实际应用。

Method: RADAR直接通过扩散模型生成异常图，避免了重建过程，提高了效率和准确性。

Result: 在MVTec-AD和3D打印材料数据集上，RADAR在准确率、精确率、召回率和F1分数上均优于现有方法，F1分数分别提高了7%和13%。

Conclusion: RADAR通过无重建方法显著提升了异常检测的性能和效率，适用于实时应用。

Abstract: Generative models have demonstrated significant success in anomaly detection and segmentation over the past decade. Recently, diffusion models have emerged as a powerful alternative, outperforming previous approaches such as GANs and VAEs. In typical diffusion-based anomaly detection, a model is trained on normal data, and during inference, anomalous images are perturbed to a predefined intermediate step in the forward diffusion process. The corresponding normal image is then reconstructed through iterative reverse sampling.   However, reconstruction-based approaches present three major challenges: (1) the reconstruction process is computationally expensive due to multiple sampling steps, making real-time applications impractical; (2) for complex or subtle patterns, the reconstructed image may correspond to a different normal pattern rather than the original input; and (3) Choosing an appropriate intermediate noise level is challenging because it is application-dependent and often assumes prior knowledge of anomalies, an assumption that does not hold in unsupervised settings.   We introduce Reconstruction-free Anomaly Detection with Attention-based diffusion models in Real-time (RADAR), which overcomes the limitations of reconstruction-based anomaly detection. Unlike current SOTA methods that reconstruct the input image, RADAR directly produces anomaly maps from the diffusion model, improving both detection accuracy and computational efficiency. We evaluate RADAR on real-world 3D-printed material and the MVTec-AD dataset. Our approach surpasses state-of-the-art diffusion-based and statistical machine learning models across all key metrics, including accuracy, precision, recall, and F1 score. Specifically, RADAR improves F1 score by 7% on MVTec-AD and 13% on the 3D-printed material dataset compared to the next best model.   Code available at: https://github.com/mehrdadmoradi124/RADAR

</details>


### [7] [UGOD: Uncertainty-Guided Differentiable Opacity and Soft Dropout for Enhanced Sparse-View 3DGS](https://arxiv.org/abs/2508.04968)
*Zhihao Guo,Peng Wang,Zidong Chen,Xiangyu Kong,Yan Lyu,Guanyu Gao,Liangxiu Han*

Main category: cs.CV

TL;DR: 3D高斯泼溅（3DGS）因其高效的渲染性能成为新视角合成（NVS）的竞争性方法。然而，现有方法中高斯权重均等化易导致过拟合，尤其在稀疏视角场景中。本文提出通过自适应高斯权重优化渲染质量，引入学习不确定性以指导高斯不透明度的更新，并通过软可微丢弃正则化提升性能。实验表明，该方法在稀疏视角3D合成中表现优异，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法中高斯权重均等化易导致过拟合，尤其在稀疏视角场景中，限制了渲染质量。

Method: 提出自适应高斯权重优化方法，通过学习不确定性指导高斯不透明度的更新，并采用软可微丢弃正则化策略。

Result: 在稀疏视角3D合成中表现优异，如MipNeRF 360数据集上PSNR提升3.27%，且使用更少的高斯数量。

Conclusion: 自适应高斯权重和不确定性学习显著提升稀疏视角下的渲染质量，为3DGS方法提供了新思路。

Abstract: 3D Gaussian Splatting (3DGS) has become a competitive approach for novel view synthesis (NVS) due to its advanced rendering efficiency through 3D Gaussian projection and blending. However, Gaussians are treated equally weighted for rendering in most 3DGS methods, making them prone to overfitting, which is particularly the case in sparse-view scenarios. To address this, we investigate how adaptive weighting of Gaussians affects rendering quality, which is characterised by learned uncertainties proposed. This learned uncertainty serves two key purposes: first, it guides the differentiable update of Gaussian opacity while preserving the 3DGS pipeline integrity; second, the uncertainty undergoes soft differentiable dropout regularisation, which strategically transforms the original uncertainty into continuous drop probabilities that govern the final Gaussian projection and blending process for rendering. Extensive experimental results over widely adopted datasets demonstrate that our method outperforms rivals in sparse-view 3D synthesis, achieving higher quality reconstruction with fewer Gaussians in most datasets compared to existing sparse-view approaches, e.g., compared to DropGaussian, our method achieves 3.27\% PSNR improvements on the MipNeRF 360 dataset.

</details>


### [8] [Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast Image Compression](https://arxiv.org/abs/2508.04979)
*Zheng Chen,Mingde Zhou,Jinpei Guo,Jiale Yuan,Yifei Ji,Yulun Zhang*

Main category: cs.CV

TL;DR: SODEC是一种单步扩散图像压缩模型，解决了传统扩散方法解码延迟高和保真度低的问题，通过预训练VAE和保真度引导模块实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 传统扩散图像压缩方法存在解码延迟高和保真度低的缺点，限制了其实际应用。

Method: 利用预训练的VAE生成信息丰富的潜在表示，并用单步解码替代迭代去噪过程；引入保真度引导模块和速率退火训练策略。

Result: SODEC在率失真感知性能上显著优于现有方法，解码速度提升20倍以上。

Conclusion: SODEC通过单步解码和保真度优化，实现了高效且高质量的图像压缩。

Abstract: Diffusion-based image compression has demonstrated impressive perceptual performance. However, it suffers from two critical drawbacks: (1) excessive decoding latency due to multi-step sampling, and (2) poor fidelity resulting from over-reliance on generative priors. To address these issues, we propose SODEC, a novel single-step diffusion image compression model. We argue that in image compression, a sufficiently informative latent renders multi-step refinement unnecessary. Based on this insight, we leverage a pre-trained VAE-based model to produce latents with rich information, and replace the iterative denoising process with a single-step decoding. Meanwhile, to improve fidelity, we introduce the fidelity guidance module, encouraging output that is faithful to the original image. Furthermore, we design the rate annealing training strategy to enable effective training under extremely low bitrates. Extensive experiments show that SODEC significantly outperforms existing methods, achieving superior rate-distortion-perception performance. Moreover, compared to previous diffusion-based compression models, SODEC improves decoding speed by more than 20$\times$. Code is released at: https://github.com/zhengchen1999/SODEC.

</details>


### [9] [Modeling Rapid Contextual Learning in the Visual Cortex with Fast-Weight Deep Autoencoder Networks](https://arxiv.org/abs/2508.04988)
*Yue Li,Weifan Wang,Tai Sing Lee*

Main category: cs.CV

TL;DR: 研究使用ViT自编码器探索快速学习如何通过快速权重（如LoRA）在早期层引入全局上下文敏感性，发现其效果与大脑熟悉效应类似。


<details>
  <summary>Details</summary>
Motivation: 探究早期视觉皮层如何快速学习全局图像上下文，以及快速权重（如LoRA）在此过程中的作用。

Method: 采用ViT自编码器，结合LoRA实现快速权重，分析熟悉性训练对自注意力机制和潜在表示的影响。

Result: 熟悉性训练使早期层与顶层全局上下文对齐，扩展自注意力范围，且LoRA显著增强这些效果。

Conclusion: 快速权重架构为研究大脑快速全局上下文学习提供了可行计算模型。

Abstract: Recent neurophysiological studies have revealed that the early visual cortex can rapidly learn global image context, as evidenced by a sparsification of population responses and a reduction in mean activity when exposed to familiar versus novel image contexts. This phenomenon has been attributed primarily to local recurrent interactions, rather than changes in feedforward or feedback pathways, supported by both empirical findings and circuit-level modeling. Recurrent neural circuits capable of simulating these effects have been shown to reshape the geometry of neural manifolds, enhancing robustness and invariance to irrelevant variations. In this study, we employ a Vision Transformer (ViT)-based autoencoder to investigate, from a functional perspective, how familiarity training can induce sensitivity to global context in the early layers of a deep neural network. We hypothesize that rapid learning operates via fast weights, which encode transient or short-term memory traces, and we explore the use of Low-Rank Adaptation (LoRA) to implement such fast weights within each Transformer layer. Our results show that (1) The proposed ViT-based autoencoder's self-attention circuit performs a manifold transform similar to a neural circuit model of the familiarity effect. (2) Familiarity training aligns latent representations in early layers with those in the top layer that contains global context information. (3) Familiarity training broadens the self-attention scope within the remembered image context. (4) These effects are significantly amplified by LoRA-based fast weights. Together, these findings suggest that familiarity training introduces global sensitivity to earlier layers in a hierarchical network, and that a hybrid fast-and-slow weight architecture may provide a viable computational model for studying rapid global context learning in the brain.

</details>


### [10] [AU-IQA: A Benchmark Dataset for Perceptual Quality Assessment of AI-Enhanced User-Generated Content](https://arxiv.org/abs/2508.05016)
*Shushi Wang,Chunyi Li,Zicheng Zhang,Han Zhou,Wei Dong,Jun Chen,Guangtao Zhai,Xiaohong Liu*

Main category: cs.CV

TL;DR: 论文提出了AU-IQA基准数据集，用于评估AI增强用户生成内容（AI-UGC）的质量评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对AI-UGC的专用质量评估模型，限制了用户体验和技术进步。

Method: 构建包含4,800张AI-UGC图像的AU-IQA数据集，涵盖超分辨率、低光增强和去噪三种增强类型，并评估现有质量评估模型。

Result: 论文对现有模型在AI-UGC质量评估中的表现进行了全面分析。

Conclusion: AU-IQA数据集填补了AI-UGC质量评估的空白，为未来研究提供了基准。

Abstract: AI-based image enhancement techniques have been widely adopted in various visual applications, significantly improving the perceptual quality of user-generated content (UGC). However, the lack of specialized quality assessment models has become a significant limiting factor in this field, limiting user experience and hindering the advancement of enhancement methods. While perceptual quality assessment methods have shown strong performance on UGC and AIGC individually, their effectiveness on AI-enhanced UGC (AI-UGC) which blends features from both, remains largely unexplored. To address this gap, we construct AU-IQA, a benchmark dataset comprising 4,800 AI-UGC images produced by three representative enhancement types which include super-resolution, low-light enhancement, and denoising. On this dataset, we further evaluate a range of existing quality assessment models, including traditional IQA methods and large multimodal models. Finally, we provide a comprehensive analysis of how well current approaches perform in assessing the perceptual quality of AI-UGC. The access link to the AU-IQA is https://github.com/WNNGGU/AU-IQA-Dataset.

</details>


### [11] [DualMat: PBR Material Estimation via Coherent Dual-Path Diffusion](https://arxiv.org/abs/2508.05060)
*Yifeng Huang,Zhang Chen,Yi Xu,Minh Hoai,Zhong Li*

Main category: cs.CV

TL;DR: DualMat是一种双路径扩散框架，用于从复杂光照条件下的单张图像估计基于物理的渲染（PBR）材料。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在单张图像PBR材料估计中的性能不足，尤其是在复杂光照条件下。

Method: 采用双路径设计：一条路径优化反照率（RGB潜在空间），另一条路径专注于金属和粗糙度估计（紧凑潜在空间）。通过特征蒸馏确保路径间一致性，并使用整流流提高效率。

Result: 在Objaverse和真实数据上表现最佳，反照率估计提升28%，金属-粗糙度预测误差减少39%。

Conclusion: DualMat通过双路径设计和特征蒸馏，显著提升了PBR材料估计的精度和效率。

Abstract: We present DualMat, a novel dual-path diffusion framework for estimating Physically Based Rendering (PBR) materials from single images under complex lighting conditions. Our approach operates in two distinct latent spaces: an albedo-optimized path leveraging pretrained visual knowledge through RGB latent space, and a material-specialized path operating in a compact latent space designed for precise metallic and roughness estimation. To ensure coherent predictions between the albedo-optimized and material-specialized paths, we introduce feature distillation during training. We employ rectified flow to enhance efficiency by reducing inference steps while maintaining quality. Our framework extends to high-resolution and multi-view inputs through patch-based estimation and cross-view attention, enabling seamless integration into image-to-3D pipelines. DualMat achieves state-of-the-art performance on both Objaverse and real-world data, significantly outperforming existing methods with up to 28% improvement in albedo estimation and 39% reduction in metallic-roughness prediction errors.

</details>


### [12] [Decoupling Continual Semantic Segmentation](https://arxiv.org/abs/2508.05065)
*Yifu Guo,Yuquan Lu,Wentao Zhang,Zishan Xu,Dexia Chen,Siyu Zhang,Yizhe Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: DecoupleCSS提出了一种两阶段框架，通过解耦类别感知检测和类别无关分割，解决了持续语义分割中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有CSS方法通常采用单阶段编码器-解码器架构，导致新旧类别学习干扰和次优的保留-可塑性平衡。

Method: DecoupleCSS分为两阶段：第一阶段使用预训练的文本和图像编码器（通过LoRA调整）生成位置感知提示；第二阶段利用SAM模型生成精确分割掩码。

Result: 该方法在多种挑战性任务中实现了最先进的性能，平衡了保留和适应性。

Conclusion: DecoupleCSS通过解耦设计有效解决了CSS中的灾难性遗忘问题，提升了性能。

Abstract: Continual Semantic Segmentation (CSS) requires learning new classes without forgetting previously acquired knowledge, addressing the fundamental challenge of catastrophic forgetting in dense prediction tasks. However, existing CSS methods typically employ single-stage encoder-decoder architectures where segmentation masks and class labels are tightly coupled, leading to interference between old and new class learning and suboptimal retention-plasticity balance. We introduce DecoupleCSS, a novel two-stage framework for CSS. By decoupling class-aware detection from class-agnostic segmentation, DecoupleCSS enables more effective continual learning, preserving past knowledge while learning new classes. The first stage leverages pre-trained text and image encoders, adapted using LoRA, to encode class-specific information and generate location-aware prompts. In the second stage, the Segment Anything Model (SAM) is employed to produce precise segmentation masks, ensuring that segmentation knowledge is shared across both new and previous classes. This approach improves the balance between retention and adaptability in CSS, achieving state-of-the-art performance across a variety of challenging tasks. Our code is publicly available at: https://github.com/euyis1019/Decoupling-Continual-Semantic-Segmentation.

</details>


### [13] [FLUX-Makeup: High-Fidelity, Identity-Consistent, and Robust Makeup Transfer via Diffusion Transformer](https://arxiv.org/abs/2508.05069)
*Jian Zhu,Shanyuan Liu,Liuzhuozheng Li,Yue Gong,He Wang,Bo Cheng,Yuhang Ma,Liebucha Wu,Xiaoyu Wu,Dawei Leng,Yuhui Yin,Yang Xu*

Main category: cs.CV

TL;DR: FLUX-Makeup是一种无需辅助面部控制组件的高保真、身份一致且鲁棒的化妆迁移框架，通过直接利用源-参考图像对实现优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有GAN和扩散方法依赖额外组件或算法来平衡迁移质量和身份一致性，但容易引入额外误差，导致效果不佳。

Method: 基于FLUX-Kontext框架，以源图像为条件输入，引入轻量级RefLoRAInjector解耦参考路径，并设计高效数据生成流程。

Result: 实验表明FLUX-Makeup在多样场景中表现鲁棒，性能优于现有方法。

Conclusion: FLUX-Makeup通过简化框架和优化数据生成，实现了高保真且身份一致的化妆迁移。

Abstract: Makeup transfer aims to apply the makeup style from a reference face to a target face and has been increasingly adopted in practical applications. Existing GAN-based approaches typically rely on carefully designed loss functions to balance transfer quality and facial identity consistency, while diffusion-based methods often depend on additional face-control modules or algorithms to preserve identity. However, these auxiliary components tend to introduce extra errors, leading to suboptimal transfer results. To overcome these limitations, we propose FLUX-Makeup, a high-fidelity, identity-consistent, and robust makeup transfer framework that eliminates the need for any auxiliary face-control components. Instead, our method directly leverages source-reference image pairs to achieve superior transfer performance. Specifically, we build our framework upon FLUX-Kontext, using the source image as its native conditional input. Furthermore, we introduce RefLoRAInjector, a lightweight makeup feature injector that decouples the reference pathway from the backbone, enabling efficient and comprehensive extraction of makeup-related information. In parallel, we design a robust and scalable data generation pipeline to provide more accurate supervision during training. The paired makeup datasets produced by this pipeline significantly surpass the quality of all existing datasets. Extensive experiments demonstrate that FLUX-Makeup achieves state-of-the-art performance, exhibiting strong robustness across diverse scenarios.

</details>


### [14] [PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human Video Generation](https://arxiv.org/abs/2508.05091)
*Jingxuan He,Busheng Su,Finn Wong*

Main category: cs.CV

TL;DR: PoseGen是一个新框架，通过单张参考图像和驱动姿态序列生成任意长度的视频，解决了扩散模型的身份漂移和时长限制问题。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在生成视频时存在身份漂移和短时长限制，难以实现长视频的时序一致性和精确控制。

Method: 采用in-context LoRA微调策略在token级别注入主题外观以保持身份，同时在通道级别条件化姿态信息以控制运动。通过分段生成和共享KV缓存机制实现无缝拼接。

Result: 在33小时小规模数据集上训练，PoseGen在身份保真度、姿态准确性和长视频生成能力上显著优于现有方法。

Conclusion: PoseGen通过创新方法解决了长视频生成中的身份和时序问题，展示了在小数据集上的高效表现。

Abstract: Generating long, temporally coherent videos with precise control over subject identity and motion is a formidable challenge for current diffusion models, which often suffer from identity drift and are limited to short clips. We introduce PoseGen, a novel framework that generates arbitrarily long videos of a specific subject from a single reference image and a driving pose sequence. Our core innovation is an in-context LoRA finetuning strategy that injects subject appearance at the token level for identity preservation, while simultaneously conditioning on pose information at the channel level for fine-grained motion control. To overcome duration limits, PoseGen pioneers an interleaved segment generation method that seamlessly stitches video clips together, using a shared KV cache mechanism and a specialized transition process to ensure background consistency and temporal smoothness. Trained on a remarkably small 33-hour video dataset, extensive experiments show that PoseGen significantly outperforms state-of-the-art methods in identity fidelity, pose accuracy, and its unique ability to produce coherent, artifact-free videos of unlimited duration.

</details>


### [15] [Rotation Equivariant Arbitrary-scale Image Super-Resolution](https://arxiv.org/abs/2508.05160)
*Qi Xie,Jiahong Fu,Zongben Xu,Deyu Meng*

Main category: cs.CV

TL;DR: 该论文提出了一种旋转等变的任意尺度图像超分辨率（ASISR）方法，通过改进编码器和隐式神经表示模块，实现了从输入到输出的端到端旋转等变性。


<details>
  <summary>Details</summary>
Motivation: 解决低分辨率图像中几何模式（如纹理、边缘）变形导致的高分辨率恢复中出现伪影的问题。

Method: 重新设计编码器和隐式神经表示模块，嵌入旋转等变性能力，并进行了理论分析。

Result: 在模拟和真实数据集上验证了方法的优越性，并可轻松集成到现有ASISR方法中。

Conclusion: 提出的方法首次实现了端到端旋转等变的ASISR，显著提升了恢复质量。

Abstract: The arbitrary-scale image super-resolution (ASISR), a recent popular topic in computer vision, aims to achieve arbitrary-scale high-resolution recoveries from a low-resolution input image. This task is realized by representing the image as a continuous implicit function through two fundamental modules, a deep-network-based encoder and an implicit neural representation (INR) module. Despite achieving notable progress, a crucial challenge of such a highly ill-posed setting is that many common geometric patterns, such as repetitive textures, edges, or shapes, are seriously warped and deformed in the low-resolution images, naturally leading to unexpected artifacts appearing in their high-resolution recoveries. Embedding rotation equivariance into the ASISR network is thus necessary, as it has been widely demonstrated that this enhancement enables the recovery to faithfully maintain the original orientations and structural integrity of geometric patterns underlying the input image. Motivated by this, we make efforts to construct a rotation equivariant ASISR method in this study. Specifically, we elaborately redesign the basic architectures of INR and encoder modules, incorporating intrinsic rotation equivariance capabilities beyond those of conventional ASISR networks. Through such amelioration, the ASISR network can, for the first time, be implemented with end-to-end rotational equivariance maintained from input to output. We also provide a solid theoretical analysis to evaluate its intrinsic equivariance error, demonstrating its inherent nature of embedding such an equivariance structure. The superiority of the proposed method is substantiated by experiments conducted on both simulated and real datasets. We also validate that the proposed framework can be readily integrated into current ASISR methods in a plug \& play manner to further enhance their performance.

</details>


### [16] [X-MoGen: Unified Motion Generation across Humans and Animals](https://arxiv.org/abs/2508.05162)
*Xuan Wang,Kai Ruan,Liyang Qian,Zhizhi Guo,Chang Su,Gaoang Wang*

Main category: cs.CV

TL;DR: X-MoGen是一个统一的跨物种文本驱动运动生成框架，通过两阶段架构和形态一致性模块解决形态差异问题，并在大规模数据集UniMo4D上验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常单独建模人类和动物运动，而联合跨物种方法能提供统一表示和更好的泛化能力，但形态差异仍是挑战。

Method: X-MoGen采用两阶段架构：条件图变分自编码器学习T姿态先验，自编码器将运动编码到共享潜在空间；第二阶段通过掩码运动建模生成文本条件运动嵌入。训练中使用形态一致性模块确保骨骼合理性。

Result: 在UniMo4D数据集上的实验表明，X-MoGen在已知和未知物种上均优于现有方法。

Conclusion: X-MoGen首次实现了人类和动物的统一文本驱动运动生成，解决了形态差异问题，并通过大规模数据集验证了其有效性。

Abstract: Text-driven motion generation has attracted increasing attention due to its broad applications in virtual reality, animation, and robotics. While existing methods typically model human and animal motion separately, a joint cross-species approach offers key advantages, such as a unified representation and improved generalization. However, morphological differences across species remain a key challenge, often compromising motion plausibility. To address this, we propose \textbf{X-MoGen}, the first unified framework for cross-species text-driven motion generation covering both humans and animals. X-MoGen adopts a two-stage architecture. First, a conditional graph variational autoencoder learns canonical T-pose priors, while an autoencoder encodes motion into a shared latent space regularized by morphological loss. In the second stage, we perform masked motion modeling to generate motion embeddings conditioned on textual descriptions. During training, a morphological consistency module is employed to promote skeletal plausibility across species. To support unified modeling, we construct \textbf{UniMo4D}, a large-scale dataset of 115 species and 119k motion sequences, which integrates human and animal motions under a shared skeletal topology for joint training. Extensive experiments on UniMo4D demonstrate that X-MoGen outperforms state-of-the-art methods on both seen and unseen species.

</details>


### [17] [ArbiViewGen: Controllable Arbitrary Viewpoint Camera Data Generation for Autonomous Driving via Stable Diffusion Models](https://arxiv.org/abs/2508.05236)
*Yatong Lan,Jingfeng Chen,Yiru Wang,Lei He*

Main category: cs.CV

TL;DR: Arbiviewgen是一个基于扩散模型的框架，用于从任意视角生成可控相机图像，解决了缺乏外推视图真实数据的问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中任意视角图像生成潜力巨大，但缺乏外推视图的真实数据，阻碍了高保真生成模型的训练。

Method: 提出Feature-Aware Adaptive View Stitching (FAVS)和Cross-View Consistency Self-Supervised Learning (CVC-SSL)，前者通过分层匹配策略实现几何对应，后者通过自监督学习强制跨视图一致性。

Result: 框架仅需多相机图像及其位姿进行训练，无需额外传感器或深度图，首次实现了多车辆配置下的可控任意视角图像生成。

Conclusion: Arbiviewgen为自动驾驶中的任意视角图像生成提供了高效且无需监督的解决方案。

Abstract: Arbitrary viewpoint image generation holds significant potential for autonomous driving, yet remains a challenging task due to the lack of ground-truth data for extrapolated views, which hampers the training of high-fidelity generative models. In this work, we propose Arbiviewgen, a novel diffusion-based framework for the generation of controllable camera images from arbitrary points of view. To address the absence of ground-truth data in unseen views, we introduce two key components: Feature-Aware Adaptive View Stitching (FAVS) and Cross-View Consistency Self-Supervised Learning (CVC-SSL). FAVS employs a hierarchical matching strategy that first establishes coarse geometric correspondences using camera poses, then performs fine-grained alignment through improved feature matching algorithms, and identifies high-confidence matching regions via clustering analysis. Building upon this, CVC-SSL adopts a self-supervised training paradigm where the model reconstructs the original camera views from the synthesized stitched images using a diffusion model, enforcing cross-view consistency without requiring supervision from extrapolated data. Our framework requires only multi-camera images and their associated poses for training, eliminating the need for additional sensors or depth maps. To our knowledge, Arbiviewgen is the first method capable of controllable arbitrary view camera image generation in multiple vehicle configurations.

</details>


### [18] [CF3: Compact and Fast 3D Feature Fields](https://arxiv.org/abs/2508.05254)
*Hyunjoon Lee,Joonkyu Min,Jaesik Park*

Main category: cs.CV

TL;DR: 提出了一种名为CF3的3D高斯特征场构建方法，通过自上而下的流程优化计算成本，并引入自适应稀疏化方法提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖自下而上的优化过程，计算成本高，且将2D特征视为绝对真值。

Method: 采用快速加权融合多视角2D特征与预训练高斯模型，直接在高斯域训练自编码器，并引入自适应稀疏化方法优化高斯属性。

Result: 仅需5%的高斯数量即可构建具有竞争力的3D特征场。

Conclusion: CF3方法在计算效率和几何细节保留上表现优异。

Abstract: 3D Gaussian Splatting (3DGS) has begun incorporating rich information from 2D foundation models. However, most approaches rely on a bottom-up optimization process that treats raw 2D features as ground truth, incurring increased computational costs. We propose a top-down pipeline for constructing compact and fast 3D Gaussian feature fields, namely, CF3. We first perform a fast weighted fusion of multi-view 2D features with pre-trained Gaussians. This approach enables training a per-Gaussian autoencoder directly on the lifted features, instead of training autoencoders in the 2D domain. As a result, the autoencoder better aligns with the feature distribution. More importantly, we introduce an adaptive sparsification method that optimizes the Gaussian attributes of the feature field while pruning and merging the redundant Gaussians, constructing an efficient representation with preserved geometric details. Our approach achieves a competitive 3D feature field using as little as 5% of the Gaussians compared to Feature-3DGS.

</details>


### [19] [SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible Image Fusion](https://arxiv.org/abs/2508.05264)
*Xiaoyang Zhang,Zhen Hua,Yakun Ju,Wei Zhou,Jun Liu,Alex C. Kot*

Main category: cs.CV

TL;DR: SGDFuse是一种基于Segment Anything Model（SAM）的条件扩散模型，用于红外与可见光图像融合，通过语义掩码指导融合过程，提升图像质量和任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法因缺乏对场景的深层语义理解，常导致关键目标丢失或引入伪影，影响图像质量和下游任务表现。

Method: 采用两阶段方法：先初步融合多模态特征，再利用SAM生成的语义掩码作为条件，通过扩散模型进行粗到细的去噪生成。

Result: SGDFuse在主观和客观评估中均达到最优性能，且适应性强，能有效解决图像融合的核心挑战。

Conclusion: SGDFuse通过语义引导的扩散模型，实现了高保真和语义感知的图像融合，为下游任务提供了强大支持。

Abstract: Infrared and visible image fusion (IVIF) aims to combine the thermal radiation information from infrared images with the rich texture details from visible images to enhance perceptual capabilities for downstream visual tasks. However, existing methods often fail to preserve key targets due to a lack of deep semantic understanding of the scene, while the fusion process itself can also introduce artifacts and detail loss, severely compromising both image quality and task performance. To address these issues, this paper proposes SGDFuse, a conditional diffusion model guided by the Segment Anything Model (SAM), to achieve high-fidelity and semantically-aware image fusion. The core of our method is to utilize high-quality semantic masks generated by SAM as explicit priors to guide the optimization of the fusion process via a conditional diffusion model. Specifically, the framework operates in a two-stage process: it first performs a preliminary fusion of multi-modal features, and then utilizes the semantic masks from SAM jointly with the preliminary fused image as a condition to drive the diffusion model's coarse-to-fine denoising generation. This ensures the fusion process not only has explicit semantic directionality but also guarantees the high fidelity of the final result. Extensive experiments demonstrate that SGDFuse achieves state-of-the-art performance in both subjective and objective evaluations, as well as in its adaptability to downstream tasks, providing a powerful solution to the core challenges in image fusion. The code of SGDFuse is available at https://github.com/boshizhang123/SGDFuse.

</details>


### [20] [Textual Inversion for Efficient Adaptation of Open-Vocabulary Object Detectors Without Forgetting](https://arxiv.org/abs/2508.05323)
*Frank Ruis,Gertjan Burghouts,Hugo Kuijf*

Main category: cs.CV

TL;DR: 提出一种基于文本反转（TI）的方法，用于扩展视觉语言模型（VLM）的词汇表，以检测新对象或细粒度对象，同时保留原始模型的零样本能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型预训练视觉语言模型在目标检测任务中表现出色，但针对特定目标仍需微调，而传统微调会丧失模型的自然语言查询和零样本能力。

Method: 借鉴文本反转（TI）的成功经验，提出一种类似的方法，通过学习和改进令牌来扩展VLM词汇表，仅需少量示例即可检测新对象。

Result: 该方法在保持原始模型性能的同时，显著减少了计算和存储需求，并在实验中优于基线方法。

Conclusion: 该方法为开放词汇目标检测提供了一种高效且兼容的解决方案，同时保留了模型的零样本能力。

Abstract: Recent progress in large pre-trained vision language models (VLMs) has reached state-of-the-art performance on several object detection benchmarks and boasts strong zero-shot capabilities, but for optimal performance on specific targets some form of finetuning is still necessary. While the initial VLM weights allow for great few-shot transfer learning, this usually involves the loss of the original natural language querying and zero-shot capabilities. Inspired by the success of Textual Inversion (TI) in personalizing text-to-image diffusion models, we propose a similar formulation for open-vocabulary object detection. TI allows extending the VLM vocabulary by learning new or improving existing tokens to accurately detect novel or fine-grained objects from as little as three examples. The learned tokens are completely compatible with the original VLM weights while keeping them frozen, retaining the original model's benchmark performance, and leveraging its existing capabilities such as zero-shot domain transfer (e.g., detecting a sketch of an object after training only on real photos). The storage and gradient calculations are limited to the token embedding dimension, requiring significantly less compute than full-model fine-tuning. We evaluated whether the method matches or outperforms the baseline methods that suffer from forgetting in a wide variety of quantitative and qualitative experiments.

</details>


### [21] [3DGabSplat: 3D Gabor Splatting for Frequency-adaptive Radiance Field Rendering](https://arxiv.org/abs/2508.05343)
*Junyu Zhou,Yuyang Huang,Wenrui Dai,Junni Zou,Ziyang Zheng,Nuowen Kan,Chenglin Li,Hongkai Xiong*

Main category: cs.CV

TL;DR: 3DGabSplat提出了一种基于3D Gabor的基元，用于提升3D场景高频细节表示，并通过高效CUDA光栅化器和频率自适应机制优化渲染质量与效率。


<details>
  <summary>Details</summary>
Motivation: 3DGS因高斯函数低通特性无法有效捕捉高频细节，且存在冗余基元导致效率低下。

Method: 采用多方向3D Gabor核构建滤波器组，结合CUDA光栅化器和频率自适应机制进行优化。

Result: 实验显示3DGabSplat在PSNR上提升1.35 dB，同时减少基元数量和内存消耗。

Conclusion: 3DGabSplat在渲染质量和效率上优于3DGS及其变体，适用于真实和合成场景。

Abstract: Recent prominence in 3D Gaussian Splatting (3DGS) has enabled real-time rendering while maintaining high-fidelity novel view synthesis. However, 3DGS resorts to the Gaussian function that is low-pass by nature and is restricted in representing high-frequency details in 3D scenes. Moreover, it causes redundant primitives with degraded training and rendering efficiency and excessive memory overhead. To overcome these limitations, we propose 3D Gabor Splatting (3DGabSplat) that leverages a novel 3D Gabor-based primitive with multiple directional 3D frequency responses for radiance field representation supervised by multi-view images. The proposed 3D Gabor-based primitive forms a filter bank incorporating multiple 3D Gabor kernels at different frequencies to enhance flexibility and efficiency in capturing fine 3D details. Furthermore, to achieve novel view rendering, an efficient CUDA-based rasterizer is developed to project the multiple directional 3D frequency components characterized by 3D Gabor-based primitives onto the 2D image plane, and a frequency-adaptive mechanism is presented for adaptive joint optimization of primitives. 3DGabSplat is scalable to be a plug-and-play kernel for seamless integration into existing 3DGS paradigms to enhance both efficiency and quality of novel view synthesis. Extensive experiments demonstrate that 3DGabSplat outperforms 3DGS and its variants using alternative primitives, and achieves state-of-the-art rendering quality across both real-world and synthetic scenes. Remarkably, we achieve up to 1.35 dB PSNR gain over 3DGS with simultaneously reduced number of primitives and memory consumption.

</details>


### [22] [UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation](https://arxiv.org/abs/2508.05399)
*Wonjun Kang,Byeongkeun Ahn,Minjae Lee,Kevin Galim,Seunghyuk Oh,Hyung Il Koo,Nam Ik Cho*

Main category: cs.CV

TL;DR: 论文提出了一种名为UNCAGE的无训练方法，通过利用注意力图优先解掩代表单个对象的标记，提升了文本到图像生成中的组合保真度。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到图像生成模型（如扩散模型和掩码生成变换器）在组合生成中属性绑定和文本图像对齐的不足。

Method: 提出UNCAGE方法，利用注意力图指导解掩过程，优先解掩明确代表单个对象的标记。

Result: UNCAGE在多个基准测试和指标上均显著提升了性能，且推理开销可忽略。

Conclusion: UNCAGE是一种高效且无需额外训练的方法，显著提升了组合文本到图像生成的质量。

Abstract: Text-to-image (T2I) generation has been actively studied using Diffusion Models and Autoregressive Models. Recently, Masked Generative Transformers have gained attention as an alternative to Autoregressive Models to overcome the inherent limitations of causal attention and autoregressive decoding through bidirectional attention and parallel decoding, enabling efficient and high-quality image generation. However, compositional T2I generation remains challenging, as even state-of-the-art Diffusion Models often fail to accurately bind attributes and achieve proper text-image alignment. While Diffusion Models have been extensively studied for this issue, Masked Generative Transformers exhibit similar limitations but have not been explored in this context. To address this, we propose Unmasking with Contrastive Attention Guidance (UNCAGE), a novel training-free method that improves compositional fidelity by leveraging attention maps to prioritize the unmasking of tokens that clearly represent individual objects. UNCAGE consistently improves performance in both quantitative and qualitative evaluations across multiple benchmarks and metrics, with negligible inference overhead. Our code is available at https://github.com/furiosa-ai/uncage.

</details>


### [23] [Physical Adversarial Camouflage through Gradient Calibration and Regularization](https://arxiv.org/abs/2508.05414)
*Jiawei Liang,Siyuan Liang,Jianjie Huang,Chenxi Si,Ming Zhang,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出了一种基于梯度优化的对抗性伪装框架，通过梯度校准和去相关方法，显著提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 物理对抗伪装对安全关键领域（如自动驾驶）构成威胁，现有方法在多变物理环境中效果不佳。

Method: 引入梯度校准策略和梯度去相关方法，优化多角度和距离下的梯度更新。

Result: 攻击成功率平均提升13.46%（距离）和11.03%（角度），显著优于现有技术。

Conclusion: 该方法有效解决了现有挑战，并强调了更鲁棒系统设计的必要性。

Abstract: The advancement of deep object detectors has greatly affected safety-critical fields like autonomous driving. However, physical adversarial camouflage poses a significant security risk by altering object textures to deceive detectors. Existing techniques struggle with variable physical environments, facing two main challenges: 1) inconsistent sampling point densities across distances hinder the gradient optimization from ensuring local continuity, and 2) updating texture gradients from multiple angles causes conflicts, reducing optimization stability and attack effectiveness. To address these issues, we propose a novel adversarial camouflage framework based on gradient optimization. First, we introduce a gradient calibration strategy, which ensures consistent gradient updates across distances by propagating gradients from sparsely to unsampled texture points. Additionally, we develop a gradient decorrelation method, which prioritizes and orthogonalizes gradients based on loss values, enhancing stability and effectiveness in multi-angle optimization by eliminating redundant or conflicting updates. Extensive experimental results on various detection models, angles and distances show that our method significantly exceeds the state of the art, with an average increase in attack success rate (ASR) of 13.46% across distances and 11.03% across angles. Furthermore, empirical evaluation in real-world scenarios highlights the need for more robust system design.

</details>


### [24] [WeTok: Powerful Discrete Tokenization for High-Fidelity Visual Reconstruction](https://arxiv.org/abs/2508.05599)
*Shaobin Zhuang,Yiwei Guo,Canmiao Fu,Zhipeng Huang,Zeyue Tian,Ying Zhang,Chen Li,Yali Wang*

Main category: cs.CV

TL;DR: WeTok tokenizer通过分组无查找量化和生成式解码，在视觉生成中实现了更高的压缩比和重建保真度。


<details>
  <summary>Details</summary>
Motivation: 现有视觉tokenizer在压缩比和重建保真度之间难以平衡，WeTok旨在解决这一问题。

Method: 提出两种创新：分组无查找量化（GQ）和生成式解码（GD），分别解决内存计算限制和视觉细节重建问题。

Result: 在ImageNet 50k验证集上，WeTok实现了最低的零样本rFID（0.12），并在高压缩比下表现优异。

Conclusion: WeTok在视觉tokenizer领域取得了突破性进展，尤其在压缩比和重建质量上显著优于现有方法。

Abstract: Visual tokenizer is a critical component for vision generation. However, the existing tokenizers often face unsatisfactory trade-off between compression ratios and reconstruction fidelity. To fill this gap, we introduce a powerful and concise WeTok tokenizer, which surpasses the previous leading tokenizers via two core innovations. (1) Group-wise lookup-free Quantization (GQ). We partition the latent features into groups, and perform lookup-free quantization for each group. As a result, GQ can efficiently overcome memory and computation limitations of prior tokenizers, while achieving a reconstruction breakthrough with more scalable codebooks. (2) Generative Decoding (GD). Different from prior tokenizers, we introduce a generative decoder with a prior of extra noise variable. In this case, GD can probabilistically model the distribution of visual data conditioned on discrete tokens, allowing WeTok to reconstruct visual details, especially at high compression ratios. Extensive experiments on mainstream benchmarks show superior performance of our WeTok. On the ImageNet 50k validation set, WeTok achieves a record-low zero-shot rFID (WeTok: 0.12 vs. FLUX-VAE: 0.18 vs. SD-VAE 3.5: 0.19). Furthermore, our highest compression model achieves a zero-shot rFID of 3.49 with a compression ratio of 768, outperforming Cosmos (384) 4.57 which has only 50% compression rate of ours. Code and models are available: https://github.com/zhuangshaobin/WeTok.

</details>


### [25] [Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision](https://arxiv.org/abs/2508.05606)
*Luozheng Qin,Jia Gong,Yuqing Sun,Tianjiao Li,Mengping Yang,Xiaomeng Yang,Chao Qu,Zhiyu Tan,Hao Li*

Main category: cs.CV

TL;DR: Uni-CoT是一个统一的链式思维框架，通过单模型实现连贯的多模态推理，解决了视觉语言任务中视觉状态转换的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视觉语言推理任务中难以建模视觉状态转换或产生不连贯的视觉轨迹，因此需要一种更高效且连贯的多模态推理框架。

Method: Uni-CoT采用两级推理范式：宏观级CoT用于任务规划，微观级CoT用于子任务执行，并结合结构化训练范式降低计算开销。

Result: 在WISE、RISE和KRIS基准测试中，Uni-CoT表现出SOTA性能和强泛化能力。

Conclusion: Uni-CoT为多模态推理提供了一种高效且连贯的解决方案，仅需8块A100 GPU即可完成实验。

Abstract: Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large Language Models (LLMs) by decomposing complex tasks into simpler, sequential subtasks. However, extending CoT to vision-language reasoning tasks remains challenging, as it often requires interpreting transitions of visual states to support reasoning. Existing methods often struggle with this due to limited capacity of modeling visual state transitions or incoherent visual trajectories caused by fragmented architectures.   To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought framework that enables coherent and grounded multimodal reasoning within a single unified model. The key idea is to leverage a model capable of both image understanding and generation to reason over visual content and model evolving visual states. However, empowering a unified model to achieve that is non-trivial, given the high computational cost and the burden of training. To address this, Uni-CoT introduces a novel two-level reasoning paradigm: A Macro-Level CoT for high-level task planning and A Micro-Level CoT for subtask execution. This design significantly reduces the computational overhead. Furthermore, we introduce a structured training paradigm that combines interleaved image-text supervision for macro-level CoT with multi-task objectives for micro-level CoT. Together, these innovations allow Uni-CoT to perform scalable and coherent multi-modal reasoning. Furthermore, thanks to our design, all experiments can be efficiently completed using only 8 A100 GPUs with 80GB VRAM each. Experimental results on reasoning-driven image generation benchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT demonstrates SOTA performance and strong generalization, establishing Uni-CoT as a promising solution for multi-modal reasoning. Project Page and Code: https://sais-fuxi.github.io/projects/uni-cot/

</details>


### [26] [Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity](https://arxiv.org/abs/2508.05609)
*Yuhan Zhang,Long Zhuo,Ziyang Chu,Tong Wu,Zhibing Li,Liang Pan,Dahua Lin,Ziwei Liu*

Main category: cs.CV

TL;DR: Hi3DEval是一个针对3D生成内容的分层评估框架，结合对象级和部件级评估，提升对空间一致性、材料真实性和局部细节的捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 现有3D内容质量评估方法主要依赖基于图像的指标，无法全面捕捉3D特性，如空间一致性和材料真实性。

Method: 提出Hi3DEval框架，结合对象级和部件级评估，并扩展纹理评估至材料真实性。构建Hi3DBench数据集，提出基于混合3D表示的自动评分系统。

Result: 实验表明，Hi3DEval优于现有基于图像的指标，更符合人类偏好。

Conclusion: Hi3DEval为3D生成内容提供了可扩展的质量评估方案。

Abstract: Despite rapid advances in 3D content generation, quality assessment for the generated 3D assets remains challenging. Existing methods mainly rely on image-based metrics and operate solely at the object level, limiting their ability to capture spatial coherence, material authenticity, and high-fidelity local details. 1) To address these challenges, we introduce Hi3DEval, a hierarchical evaluation framework tailored for 3D generative content. It combines both object-level and part-level evaluation, enabling holistic assessments across multiple dimensions as well as fine-grained quality analysis. Additionally, we extend texture evaluation beyond aesthetic appearance by explicitly assessing material realism, focusing on attributes such as albedo, saturation, and metallicness. 2) To support this framework, we construct Hi3DBench, a large-scale dataset comprising diverse 3D assets and high-quality annotations, accompanied by a reliable multi-agent annotation pipeline. We further propose a 3D-aware automated scoring system based on hybrid 3D representations. Specifically, we leverage video-based representations for object-level and material-subject evaluations to enhance modeling of spatio-temporal consistency and employ pretrained 3D features for part-level perception. Extensive experiments demonstrate that our approach outperforms existing image-based metrics in modeling 3D characteristics and achieves superior alignment with human preference, providing a scalable alternative to manual evaluations. The project page is available at https://zyh482.github.io/Hi3DEval/.

</details>


### [27] [GAP: Gaussianize Any Point Clouds with Text Guidance](https://arxiv.org/abs/2508.05631)
*Weiqi Zhang,Junsheng Zhou,Haotian Geng,Wenyuan Zhang,Yu-Shen Liu*

Main category: cs.CV

TL;DR: GAP方法通过多视角优化框架和文本引导，将无颜色点云转化为高保真3D高斯分布。


<details>
  <summary>Details</summary>
Motivation: 解决从无颜色点云直接生成高斯分布的挑战，提升3D渲染的质量和效率。

Method: 采用多视角优化框架，结合深度感知图像扩散模型和表面锚定机制，确保几何精度和一致性。

Result: GAP在合成点云、真实扫描和大规模场景中均表现出色，生成高保真3D高斯分布。

Conclusion: GAP为点云到高斯的转换提供了高效且高质量的解决方案，具有广泛的应用潜力。

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated its advantages in achieving fast and high-quality rendering. As point clouds serve as a widely-used and easily accessible form of 3D representation, bridging the gap between point clouds and Gaussians becomes increasingly important. Recent studies have explored how to convert the colored points into Gaussians, but directly generating Gaussians from colorless 3D point clouds remains an unsolved challenge. In this paper, we propose GAP, a novel approach that gaussianizes raw point clouds into high-fidelity 3D Gaussians with text guidance. Our key idea is to design a multi-view optimization framework that leverages a depth-aware image diffusion model to synthesize consistent appearances across different viewpoints. To ensure geometric accuracy, we introduce a surface-anchoring mechanism that effectively constrains Gaussians to lie on the surfaces of 3D shapes during optimization. Furthermore, GAP incorporates a diffuse-based inpainting strategy that specifically targets at completing hard-to-observe regions. We evaluate GAP on the Point-to-Gaussian generation task across varying complexity levels, from synthetic point clouds to challenging real-world scans, and even large-scale scenes. Project Page: https://weiqi-zhang.github.io/GAP.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [28] [Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation](https://arxiv.org/abs/2508.05635)
*Yue Liao,Pengfei Zhou,Siyuan Huang,Donglin Yang,Shengcong Chen,Yuxin Jiang,Yue Hu,Jingbin Cai,Si Liu,Jianlan Luo,Liliang Chen,Shuicheng Yan,Maoqing Yao,Guanghui Ren*

Main category: cs.RO

TL;DR: Genie Envisioner (GE) 是一个统一的机器人操作平台，集成了策略学习、评估和模拟，通过视频生成框架实现。


<details>
  <summary>Details</summary>
Motivation: 为机器人操作提供一个统一且可扩展的基础平台，支持指令驱动的通用智能体。

Method: GE-Base 是一个大规模的视频扩散模型，GE-Act 将潜在表示映射为可执行动作，GE-Sim 作为神经模拟器生成高保真模拟。

Result: 平台支持精确且通用的策略推断，并通过 EWMBench 标准化基准评估性能。

Conclusion: Genie Envisioner 是一个实用且可扩展的基础平台，适用于通用智能体的开发。

Abstract: We introduce Genie Envisioner (GE), a unified world foundation platform for robotic manipulation that integrates policy learning, evaluation, and simulation within a single video-generative framework. At its core, GE-Base is a large-scale, instruction-conditioned video diffusion model that captures the spatial, temporal, and semantic dynamics of real-world robotic interactions in a structured latent space. Built upon this foundation, GE-Act maps latent representations to executable action trajectories through a lightweight, flow-matching decoder, enabling precise and generalizable policy inference across diverse embodiments with minimal supervision. To support scalable evaluation and training, GE-Sim serves as an action-conditioned neural simulator, producing high-fidelity rollouts for closed-loop policy development. The platform is further equipped with EWMBench, a standardized benchmark suite measuring visual fidelity, physical consistency, and instruction-action alignment. Together, these components establish Genie Envisioner as a scalable and practical foundation for instruction-driven, general-purpose embodied intelligence. All code, models, and benchmarks will be released publicly.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [29] [LumiGen: An LVLM-Enhanced Iterative Framework for Fine-Grained Text-to-Image Generation](https://arxiv.org/abs/2508.04732)
*Xiaoqi Dong,Xiangyu Zhou,Nicholas Evans,Yujia Lin*

Main category: cs.LG

TL;DR: LumiGen是一个基于视觉语言模型（LVLM）的迭代框架，通过智能提示解析与增强（IPPA）和迭代视觉反馈与优化（IVFR）模块，提升文本到图像（T2I）生成的细粒度控制和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型在复杂指令、细粒度内容控制和语义一致性方面存在不足，而LVLM在跨模态理解和指令跟随方面表现出色，因此提出LumiGen以解决这些问题。

Method: LumiGen包含IPPA模块（主动增强提示）和IVFR模块（作为“视觉评论家”迭代优化图像），通过LVLM驱动的闭环反馈机制提升T2I生成质量。

Result: 在LongBench-T2I基准测试中，LumiGen以3.08的平均分数优于现有方法，尤其在文本渲染和姿势表达等关键维度表现突出。

Conclusion: LumiGen通过整合LVLM，显著提升了T2I生成的可控性和质量，验证了LVLM在图像生成中的有效性。

Abstract: Text-to-Image (T2I) generation has made significant advancements with diffusion models, yet challenges persist in handling complex instructions, ensuring fine-grained content control, and maintaining deep semantic consistency. Existing T2I models often struggle with tasks like accurate text rendering, precise pose generation, or intricate compositional coherence. Concurrently, Vision-Language Models (LVLMs) have demonstrated powerful capabilities in cross-modal understanding and instruction following. We propose LumiGen, a novel LVLM-enhanced iterative framework designed to elevate T2I model performance, particularly in areas requiring fine-grained control, through a closed-loop, LVLM-driven feedback mechanism. LumiGen comprises an Intelligent Prompt Parsing & Augmentation (IPPA) module for proactive prompt enhancement and an Iterative Visual Feedback & Refinement (IVFR) module, which acts as a "visual critic" to iteratively correct and optimize generated images. Evaluated on the challenging LongBench-T2I Benchmark, LumiGen achieves a superior average score of 3.08, outperforming state-of-the-art baselines. Notably, our framework demonstrates significant improvements in critical dimensions such as text rendering and pose expression, validating the effectiveness of LVLM integration for more controllable and higher-quality image generation.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [30] [CryoGS: Gaussian Splatting for Cryo-EM Homogeneous Reconstruction](https://arxiv.org/abs/2508.04929)
*Suyi Chen,Haibin Ling*

Main category: eess.IV

TL;DR: 论文提出了一种基于高斯混合模型（GMM）的cryoGS方法，用于直接从原始cryo-EM粒子图像进行3D重建，无需依赖外部初始化。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部共识图或原子模型初始化，限制了其在独立流程中的应用。

Method: 结合高斯投射与cryo-EM成像物理，开发了正交投影感知的高斯投射方法，包括归一化项和FFT对齐坐标系。

Result: 实验证明cryoGS在真实数据集上比基线方法更有效和稳健。

Conclusion: cryoGS为cryo-EM重建提供了一种稳定且高效的自包含解决方案。

Abstract: As a critical modality for structural biology, cryogenic electron microscopy (cryo-EM) facilitates the determination of macromolecular structures at near-atomic resolution. The core computational task in single-particle cryo-EM is to reconstruct the 3D electrostatic potential of a molecule from a large collection of noisy 2D projections acquired at unknown orientations. Gaussian mixture models (GMMs) provide a continuous, compact, and physically interpretable representation for molecular density and have recently gained interest in cryo-EM reconstruction. However, existing methods rely on external consensus maps or atomic models for initialization, limiting their use in self-contained pipelines. Addressing this issue, we introduce cryoGS, a GMM-based method that integrates Gaussian splatting with the physics of cryo-EM image formation. In particular, we develop an orthogonal projection-aware Gaussian splatting, with adaptations such as a normalization term and FFT-aligned coordinate system tailored for cryo-EM imaging. All these innovations enable stable and efficient homogeneous reconstruction directly from raw cryo-EM particle images using random initialization. Experimental results on real datasets validate the effectiveness and robustness of cryoGS over representative baselines. The code will be released upon publication.

</details>


### [31] [Coarse-to-Fine Joint Registration of MR and Ultrasound Images via Imaging Style Transfer](https://arxiv.org/abs/2508.05240)
*Junyi Wang,Xi Zhu,Yikun Guo,Zixi Wang,Haichuan Gao,Le Zhang,Fan Zhang*

Main category: eess.IV

TL;DR: 提出了一种基于3D CycleGAN的MR与US图像配准流程，通过生成合成T1图像提升配准性能。


<details>
  <summary>Details</summary>
Motivation: 解决术前MR与术后US图像配准的挑战，提升配准一致性。

Method: 使用3D CycleGAN进行非配对风格迁移生成合成T1图像，结合仿射和局部可变形变换实现粗到精配准。

Result: 在多数情况下提高了MR与US图像对的配准一致性。

Conclusion: 该方法有效提升了术前术后图像的配准性能。

Abstract: We developed a pipeline for registering pre-surgery Magnetic Resonance (MR) images and post-resection Ultrasound (US) images. Our approach leverages unpaired style transfer using 3D CycleGAN to generate synthetic T1 images, thereby enhancing registration performance. Additionally, our registration process employs both affine and local deformable transformations for a coarse-to-fine registration. The results demonstrate that our approach improves the consistency between MR and US image pairs in most cases.

</details>
