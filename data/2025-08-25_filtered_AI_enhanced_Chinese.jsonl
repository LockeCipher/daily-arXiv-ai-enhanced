{"id": "2508.16024", "pdf": "https://arxiv.org/pdf/2508.16024", "abs": "https://arxiv.org/abs/2508.16024", "authors": ["Prateek Poudel", "Prashant Aryal", "Kirtan Kunwar", "Navin Nepal", "Dinesh Bania Kshatri"], "title": "Wavelet-Space Super-Resolution for Real-Time Rendering", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We investigate the use of wavelet-space feature decomposition in neural super-resolution for rendering pipelines. Building on the DFASR framework, we introduce a wavelet-domain representation that separates low- and high-frequency details before reconstruction, enabling the network to better preserve fine textures while maintaining structural consistency. Unlike RGB-space regression, our approach leverages the stationary wavelet transform (SWT) to avoid spatial down-sampling, ensuring alignment across subbands and preserving shift invariance. The model predicts wavelet coefficients conditioned on spatial G-buffers and temporally warped history frames, which are then recombined through inverse wavelet synthesis. We conduct a comprehensive ablation study across wavelet families, transform types, and architectural variants, showing that incorporating SWT improves PSNR by up to 1.5 dB and reduces LPIPS by 17% on average, at a computational overhead of roughly +24 ms compared to out DFASR baseline. While absolute runtimes on our RTX 3050 mobile GPU are higher ( 141ms) than the original DFASR report on RTX 4090( 11ms), the relative overhead remains modest, suggesting that on higher-end GPUs our method would also remain real-time capable. Taken together, our results suggest that wavelet-domain representations are a principled and effective way to enhance perceptual quality in neural upscaling for graphics applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u7a7a\u95f4\u7279\u5f81\u5206\u89e3\u7684\u795e\u7ecf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c0f\u6ce2\u53d8\u6362\u5206\u79bb\u4f4e\u9891\u548c\u9ad8\u9891\u7ec6\u8282\uff0c\u5728\u4fdd\u6301\u7ed3\u6784\u4e00\u81f4\u6027\u7684\u540c\u65f6\u66f4\u597d\u5730\u4fdd\u7559\u7eb9\u7406\u7ec6\u8282\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728PSNR\u548cLPIPS\u6307\u6807\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684RGB\u7a7a\u95f4\u56de\u5f52\u65b9\u6cd5\u5728\u795e\u7ecf\u8d85\u5206\u8fa8\u7387\u4e2d\u96be\u4ee5\u540c\u65f6\u4fdd\u6301\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u7cbe\u7ec6\u7eb9\u7406\u7ec6\u8282\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u66f4\u597d\u5904\u7406\u591a\u5c3a\u5ea6\u7279\u5f81\u7684\u8868\u5f81\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eDFASR\u6846\u67b6\uff0c\u5f15\u5165\u5c0f\u6ce2\u57df\u8868\u793a\uff0c\u4f7f\u7528\u5e73\u7a33\u5c0f\u6ce2\u53d8\u6362(SWT)\u907f\u514d\u7a7a\u95f4\u4e0b\u91c7\u6837\uff0c\u4fdd\u6301\u5b50\u5e26\u5bf9\u9f50\u548c\u4f4d\u79fb\u4e0d\u53d8\u6027\u3002\u6a21\u578b\u57fa\u4e8e\u7a7a\u95f4G-buffers\u548c\u65f6\u95f4\u626d\u66f2\u5386\u53f2\u5e27\u9884\u6d4b\u5c0f\u6ce2\u7cfb\u6570\uff0c\u901a\u8fc7\u9006\u5c0f\u6ce2\u5408\u6210\u91cd\u5efa\u56fe\u50cf\u3002", "result": "\u76f8\u6bd4DFASR\u57fa\u7ebf\uff0cPSNR\u63d0\u5347\u6700\u9ad8\u8fbe1.5 dB\uff0cLPIPS\u5e73\u5747\u964d\u4f4e17%\uff0c\u8ba1\u7b97\u5f00\u9500\u589e\u52a0\u7ea624ms\uff0c\u5728\u9ad8\u7aefGPU\u4e0a\u4ecd\u80fd\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "\u5c0f\u6ce2\u57df\u8868\u793a\u662f\u63d0\u5347\u56fe\u5f62\u5e94\u7528\u4e2d\u795e\u7ecf\u4e0a\u91c7\u6837\u611f\u77e5\u8d28\u91cf\u7684\u6709\u6548\u4e14\u539f\u7406\u6027\u65b9\u6cd5\u3002"}}
{"id": "2508.15902", "pdf": "https://arxiv.org/pdf/2508.15902", "abs": "https://arxiv.org/abs/2508.15902", "authors": ["L\u00e9ore Bensabath", "Mathis Petrovich", "G\u00fcl Varol"], "title": "Text-Driven 3D Hand Motion Generation from Sign Language Data", "categories": ["cs.CV", "65-XX", "I.4.9; I.5.1"], "comment": "Project page: https://imagine.enpc.fr/~leore.bensabath/HandMDM/; 24   pages, 14 figures", "summary": "Our goal is to train a generative model of 3D hand motions, conditioned on natural language descriptions specifying motion characteristics such as handshapes, locations, finger/hand/arm movements. To this end, we automatically build pairs of 3D hand motions and their associated textual labels with unprecedented scale. Specifically, we leverage a large-scale sign language video dataset, along with noisy pseudo-annotated sign categories, which we translate into hand motion descriptions via an LLM that utilizes a dictionary of sign attributes, as well as our complementary motion-script cues. This data enables training a text-conditioned hand motion diffusion model HandMDM, that is robust across domains such as unseen sign categories from the same sign language, but also signs from another sign language and non-sign hand movements. We contribute extensive experimental investigation of these scenarios and will make our trained models and data publicly available to support future research in this relatively new field.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u81ea\u52a8\u5316\u65b9\u6cd5\u6784\u5efa\u5927\u89c4\u6a21\u76843D\u624b\u52bf\u52a8\u4f5c-\u6587\u672c\u63cf\u8ff0\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u4e86\u4e00\u4e2a\u6587\u672c\u6761\u4ef6\u5316\u76843D\u624b\u52bf\u52a8\u4f5c\u751f\u6210\u6a21\u578bHandMDM\uff0c\u80fd\u591f\u6839\u636e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u5404\u79cd\u624b\u52bf\u52a8\u4f5c\uff0c\u5305\u62ec\u4e0d\u540c\u624b\u8bed\u8a00\u548c\u975e\u624b\u8bed\u52a8\u4f5c\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u6839\u636e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u4e09\u7ef4\u624b\u52bf\u52a8\u4f5c\u7684\u76ee\u6807\uff0c\u9700\u8981\u5927\u89c4\u6a21\u7684\u8bad\u7ec3\u6570\u636e\u3002\u4f46\u76f8\u5173\u9886\u57df\u7684\u6807\u6ce8\u6570\u636e\u7f3a\u4e4f\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u6784\u5efa\u8fd9\u79cd\u6570\u636e\u96c6\u3002", "method": "\u5229\u7528\u5927\u89c4\u6a21\u624b\u8bed\u89c6\u9891\u6570\u636e\u96c6\u548c\u566a\u58f0\u4f2a\u6807\u6ce8\uff0c\u901a\u8fc7LLM\u5c06\u624b\u8bed\u5c5e\u6027\u8bcd\u5178\u548c\u52a8\u4f5c-\u811a\u672c\u7ebf\u7d22\u7ffb\u8bd1\u6210\u624b\u52bf\u52a8\u4f5c\u63cf\u8ff0\uff0c\u6784\u5efa\u6587\u672c-\u52a8\u4f5c\u6570\u636e\u5bf9\u3002\u7136\u540e\u8bad\u7ec3\u6587\u672c\u6761\u4ef6\u5316\u7684\u624b\u52bf\u52a8\u4f5c\u6a21\u578bHandMDM\uff08\u57fa\u4e8e\u6269\u6563\u6a21\u578b\uff09\u3002", "result": "\u8bad\u7ec3\u7684HandMDM\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u90fd\u8868\u73b0\u51fa\u826f\u597d\u7684\u7a81\u7834\u6027\u80fd\uff0c\u5305\u62ec\uff1a\u540c\u4e00\u624b\u8bed\u8a00\u4e2d\u672a\u89c1\u7684\u624b\u8bed\u7c7b\u522b\u3001\u53e6\u4e00\u79cd\u624b\u8bed\u8a00\u7684\u624b\u8bed\u4ee5\u53ca\u975e\u624b\u8bed\u52a8\u4f5c\u3002\u5b8c\u6574\u7684\u5b9e\u9a8c\u7814\u7a76\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u591a\u6837\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6587\u672c\u6761\u4ef6\u5316\u76843D\u624b\u52bf\u52a8\u4f5c\u751f\u6210\u9886\u57df\u5f00\u8fbe\u4e86\u65b0\u65b9\u5411\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6570\u636e\u6784\u5efa\u65b9\u6cd5\u89e3\u51b3\u4e86\u6570\u636e\u7f3a\u4e4f\u95ee\u9898\u3002\u6a21\u578b\u548c\u6570\u636e\u5c06\u516c\u5f00\u4ee5\u652f\u6301\u8be5\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2508.15988", "pdf": "https://arxiv.org/pdf/2508.15988", "abs": "https://arxiv.org/abs/2508.15988", "authors": ["Mohamed Ilyes Lakhal", "Richard Bowden"], "title": "Diverse Signer Avatars with Manual and Non-Manual Feature Modelling for Sign Language Production", "categories": ["cs.CV"], "comment": null, "summary": "The diversity of sign representation is essential for Sign Language Production (SLP) as it captures variations in appearance, facial expressions, and hand movements. However, existing SLP models are often unable to capture diversity while preserving visual quality and modelling non-manual attributes such as emotions. To address this problem, we propose a novel approach that leverages Latent Diffusion Model (LDM) to synthesise photorealistic digital avatars from a generated reference image. We propose a novel sign feature aggregation module that explicitly models the non-manual features (\\textit{e.g.}, the face) and the manual features (\\textit{e.g.}, the hands). We show that our proposed module ensures the preservation of linguistic content while seamlessly using reference images with different ethnic backgrounds to ensure diversity. Experiments on the YouTube-SL-25 sign language dataset show that our pipeline achieves superior visual quality compared to state-of-the-art methods, with significant improvements on perceptual metrics.", "AI": {"tldr": "\u901a\u8fc7\u6f5c\u5728\u6db2\u5316\u6a21\u578b\u548c\u65b0\u7684\u624b\u8bed\u7279\u5f81\u805a\u5408\u6a21\u5757\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u6301\u8bed\u8a00\u5185\u5bb9\u7684\u540c\u65f6\u751f\u6210\u89c6\u89c9\u8d28\u91cf\u66f4\u9ad8\u3001\u66f4\u591a\u6837\u5316\u7684\u624b\u8bed\u89c6\u9891\u3002", "motivation": "\u73b0\u6709\u7684\u624b\u8bed\u751f\u6210\u6a21\u578b\u65e0\u6cd5\u540c\u65f6\u6293\u53d6\u624b\u8bed\u8868\u8fbe\u7684\u591a\u6837\u6027\u548c\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u975e\u624b\u52bf\u5c5e\u6027\uff08\u5982\u60c5\u611f\u8868\u8fbe\uff09\u7684\u5efa\u6a21\u65b9\u9762\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u6f5c\u5728\u6db2\u5316\u6a21\u578b\uff08LDM\uff09\u4ece\u751f\u6210\u7684\u53c2\u8003\u56fe\u50cf\u5408\u6210\u8d85\u5b9e\u7684\u6570\u5b57\u5316\u8eab\uff1b\u8bbe\u8ba1\u4e86\u65b0\u7684\u624b\u8bed\u7279\u5f81\u805a\u5408\u6a21\u5757\uff0c\u660e\u786e\u5efa\u6a21\u975e\u624b\u52bf\u7279\u5f81\uff08\u5982\u9762\u90e8\uff09\u548c\u624b\u52bf\u7279\u5f81\uff08\u5982\u624b\u90e8\uff09\u3002", "result": "\u5728YouTube-SL-25\u624b\u8bed\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u8d85\u8fc7\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u611f\u77e5\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u624b\u8bed\u751f\u6210\u4e2d\u7684\u591a\u6837\u6027\u548c\u8d28\u91cf\u95ee\u9898\uff0c\u4e3a\u521b\u5efa\u66f4\u81ea\u7136\u3001\u66f4\u5305\u5bb9\u7684\u624b\u8bed\u4ea4\u6d41\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.16089", "pdf": "https://arxiv.org/pdf/2508.16089", "abs": "https://arxiv.org/abs/2508.16089", "authors": ["Sun Weikai", "Song Shijie", "Chi Wenjie"], "title": "Two-flow Feedback Multi-scale Progressive Generative Adversarial Network", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Although diffusion model has made good progress in the field of image generation, GAN\\cite{huang2023adaptive} still has a large development space due to its unique advantages, such as WGAN\\cite{liu2021comparing}, SSGAN\\cite{guibas2021adaptive} \\cite{zhang2022vsa} \\cite{zhou2024adapt} and so on. In this paper, we propose a novel two-flow feedback multi-scale progressive generative adversarial network (MSPG-SEN) for GAN models. This paper has four contributions: 1) : We propose a two-flow feedback multi-scale progressive Generative Adversarial network (MSPG-SEN), which not only improves image quality and human visual perception on the basis of retaining the advantages of the existing GAN model, but also simplifies the training process and reduces the training cost of GAN networks. Our experimental results show that, MSPG-SEN has achieved state-of-the-art generation results on the following five datasets,INKK The dataset is 89.7\\%,AWUN The dataset is 78.3\\%,IONJ The dataset is 85.5\\%,POKL The dataset is 88.7\\%,OPIN The dataset is 96.4\\%. 2) : We propose an adaptive perception-behavioral feedback loop (APFL), which effectively improves the robustness and training stability of the model and reduces the training cost. 3) : We propose a globally connected two-flow dynamic residual network(). After ablation experiments, it can effectively improve the training efficiency and greatly improve the generalization ability, with stronger flexibility. 4) : We propose a new dynamic embedded attention mechanism (DEMA). After experiments, the attention can be extended to a variety of image processing tasks, which can effectively capture global-local information, improve feature separation capability and feature expression capabilities, and requires minimal computing resources only 88.7\\% with INJK With strong cross-task capability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53cc\u6d41\u53cd\u9988\u591a\u5c3a\u5ea6\u6e10\u8fdb\u751f\u6210\u5bf9\u6297\u7f51\u7edc(MSPG-SEN)\uff0c\u901a\u8fc7\u56db\u9879\u6838\u5fc3\u6280\u672f\u63d0\u5347GAN\u6a21\u578b\u7684\u56fe\u50cf\u8d28\u91cf\u3001\u8bad\u7ec3\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u867d\u7136\u53d8\u5206\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u9886\u57df\u53d6\u5f97\u4e86\u8fdb\u6b65\uff0c\u4f46GAN\u56e0\u5176\u72ec\u7279\u4f18\u52bf\u4ecd\u6709\u5f88\u5927\u53d1\u5c55\u7a7a\u95f4\u3002\u672c\u6587\u65e8\u5728\u7ee7\u627fGAN\u73b0\u6709\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3001\u7b80\u5316\u8bad\u7ec3\u8fc7\u7a0b\u5e76\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e86\u53cc\u6d41\u53cd\u9988\u591a\u5c3a\u5ea6\u6e10\u8fdbGAN(MSPG-SEN)\u6a21\u578b\uff0c\u5305\u62ec\u56db\u4e2a\u6838\u5fc3\u6280\u672f\uff1a1)\u81ea\u9002\u5e94\u611f\u77e5-\u884c\u4e3a\u53cd\u9988\u5faa\u74b6(APFL)\u63d0\u9ad8\u7a33\u5b9a\u6027\uff1b2)\u5168\u5c40\u8fde\u63a5\u53cc\u6d41\u52a8\u6001\u6b8a\u5dee\u7f51\u7edc\u63d0\u5347\u6548\u7387\uff1b3)\u52a8\u6001\u5d4c\u5165\u5f0f\u6ce8\u610f\u529b\u673a\u5236(DEMA)\u63d0\u5347\u7279\u5f81\u8868\u8fbe\u80fd\u529b\uff1b4)\u7b80\u5316\u8bad\u7ec3\u6d41\u7a0b\u964d\u4f4e\u6210\u672c\u3002", "result": "\u57285\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\uff1aINKK(89.7%)\u3001AWUN(78.3%)\u3001IONJ(85.5%)\u3001POKL(88.7%)\u3001OPIN(96.4%)\u3002\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u3001\u8bad\u7ec3\u6548\u7387\u3001\u6a21\u578b\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017(88.7%)\u3002", "conclusion": "MSPG-SEN\u6a21\u578b\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u6d41\u53cd\u9988\u591a\u5c3a\u5ea6\u6e10\u8fdb\u7ed3\u6784\u548c\u56db\u9879\u6838\u5fc3\u6280\u672f\uff0c\u6210\u529f\u5b9e\u73b0\u4e86GAN\u6a21\u578b\u5728\u56fe\u50cf\u8d28\u91cf\u3001\u8bad\u7ec3\u6548\u7387\u3001\u7a33\u5b9a\u6027\u548c\u8ba1\u7b97\u6210\u672c\u65b9\u9762\u7684\u7efc\u5408\u63d0\u5347\uff0c\u4e3aGAN\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.16158", "pdf": "https://arxiv.org/pdf/2508.16158", "abs": "https://arxiv.org/abs/2508.16158", "authors": ["Haodong He", "Yancheng Bai", "Rui Lan", "Xu Duan", "Lei Sun", "Xiangxiang Chu", "Gui-Song Xia"], "title": "RAGSR: Regional Attention Guided Diffusion for Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "The rich textual information of large vision-language models (VLMs) combined with the powerful generative prior of pre-trained text-to-image (T2I) diffusion models has achieved impressive performance in single-image super-resolution (SISR). However, existing methods still face significant challenges in generating clear and accurate regional details, particularly in scenarios involving multiple objects. This challenge primarily stems from a lack of fine-grained regional descriptions and the models' insufficient ability to capture complex prompts. To address these limitations, we propose a Regional Attention Guided Super-Resolution (RAGSR) method that explicitly extracts localized fine-grained information and effectively encodes it through a novel regional attention mechanism, enabling both enhanced detail and overall visually coherent SR results. Specifically, RAGSR localizes object regions in an image and assigns fine-grained caption to each region, which are formatted as region-text pairs as textual priors for T2I models. A regional guided attention is then leveraged to ensure that each region-text pair is properly considered in the attention process while preventing unwanted interactions between unrelated region-text pairs. By leveraging this attention mechanism, our approach offers finer control over the integration of text and image information, thereby effectively overcoming limitations faced by traditional SISR techniques. Experimental results on benchmark datasets demonstrate that our approach exhibits superior performance in generating perceptually authentic visual details while maintaining contextual consistency compared to existing approaches.", "AI": {"tldr": "\u63d0\u51faRAGSR\u65b9\u6cd5\uff0c\u901a\u8fc7\u533a\u57df\u6ce8\u610f\u529b\u673a\u5236\u63d0\u53d6\u7ec6\u7c92\u5ea6\u533a\u57df\u63cf\u8ff0\u4fe1\u606f\uff0c\u89e3\u51b3\u591a\u5bf9\u8c61\u573a\u666f\u4e0b\u8d85\u5206\u8fa8\u7387\u7ec6\u8282\u751f\u6210\u95ee\u9898", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u6e05\u6670\u51c6\u786e\u533a\u57df\u7ec6\u8282\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u591a\u5bf9\u8c61\u573a\u666f\u4e2d\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u533a\u57df\u63cf\u8ff0\u548c\u6a21\u578b\u6355\u6349\u590d\u6742\u63d0\u793a\u80fd\u529b\u4e0d\u8db3", "method": "\u5b9a\u4f4d\u56fe\u50cf\u4e2d\u7684\u5bf9\u8c61\u533a\u57df\u5e76\u4e3a\u6bcf\u4e2a\u533a\u57df\u5206\u914d\u7ec6\u7c92\u5ea6\u6807\u9898\uff0c\u5f62\u6210\u533a\u57df-\u6587\u672c\u5bf9\u4f5c\u4e3a\u6587\u672c\u5148\u9a8c\uff0c\u5229\u7528\u533a\u57df\u5f15\u5bfc\u6ce8\u610f\u529b\u673a\u5236\u786e\u4fdd\u6bcf\u4e2a\u533a\u57df-\u6587\u672c\u5bf9\u5728\u6ce8\u610f\u529b\u8fc7\u7a0b\u4e2d\u5f97\u5230\u9002\u5f53\u8003\u8651", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u611f\u77e5\u771f\u5b9e\u89c6\u89c9\u7ec6\u8282\u7684\u540c\u65f6\u4fdd\u6301\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd", "conclusion": "RAGSR\u65b9\u6cd5\u901a\u8fc7\u533a\u57df\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u514b\u670d\u4e86\u4f20\u7edf\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u5bf9\u6587\u672c\u548c\u56fe\u50cf\u4fe1\u606f\u6574\u5408\u7684\u66f4\u7cbe\u7ec6\u63a7\u5236"}}
{"id": "2508.16211", "pdf": "https://arxiv.org/pdf/2508.16211", "abs": "https://arxiv.org/abs/2508.16211", "authors": ["Shikang Zheng", "Liang Feng", "Xinyu Wang", "Qinming Zhou", "Peiliang Cai", "Chang Zou", "Jiacheng Liu", "Yuqi Lin", "Junjie Chen", "Yue Ma", "Linfeng Zhang"], "title": "Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion Transformers (DiTs) have demonstrated exceptional performance in high-fidelity image and video generation. To reduce their substantial computational costs, feature caching techniques have been proposed to accelerate inference by reusing hidden representations from previous timesteps. However, current methods often struggle to maintain generation quality at high acceleration ratios, where prediction errors increase sharply due to the inherent instability of long-step forecasting. In this work, we adopt an ordinary differential equation (ODE) perspective on the hidden-feature sequence, modeling layer representations along the trajectory as a feature-ODE. We attribute the degradation of existing caching strategies to their inability to robustly integrate historical features under large skipping intervals. To address this, we propose FoCa (Forecast-then-Calibrate), which treats feature caching as a feature-ODE solving problem. Extensive experiments on image synthesis, video generation, and super-resolution tasks demonstrate the effectiveness of FoCa, especially under aggressive acceleration. Without additional training, FoCa achieves near-lossless speedups of 5.50 times on FLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high quality with a 4.53 times speedup on DiT.", "AI": {"tldr": "FoCa\u65b9\u6cd5\u901a\u8fc7\u5c06\u7279\u5f81\u7f13\u5b58\u89c6\u4e3a\u7279\u5f81-ODE\u6c42\u89e3\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u663e\u8457\u52a0\u901fDiffusion Transformers\u63a8\u7406\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5b9e\u73b0\u8fd1\u65e0\u635f\u76845-6\u500d\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u7279\u5f81\u7f13\u5b58\u65b9\u6cd5\u5728\u9ad8\u52a0\u901f\u6bd4\u4e0b\u96be\u4ee5\u4fdd\u6301\u751f\u6210\u8d28\u91cf\uff0c\u56e0\u4e3a\u957f\u6b65\u9884\u6d4b\u7684\u4e0d\u7a33\u5b9a\u6027\u5bfc\u81f4\u9884\u6d4b\u8bef\u5dee\u6025\u5267\u589e\u52a0\u3002", "method": "\u4eceODE\u89c6\u89d2\u5efa\u6a21\u9690\u85cf\u7279\u5f81\u5e8f\u5217\uff0c\u5c06\u7279\u5f81\u7f13\u5b58\u89c6\u4e3a\u7279\u5f81-ODE\u6c42\u89e3\u95ee\u9898\uff0c\u63d0\u51faForecast-then-Calibrate (FoCa)\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a33\u5065\u5730\u6574\u5408\u5386\u53f2\u7279\u5f81\u6765\u89e3\u51b3\u5927\u8df3\u8dc3\u95f4\u9694\u4e0b\u7684\u9000\u5316\u95ee\u9898\u3002", "result": "\u5728\u56fe\u50cf\u5408\u6210\u3001\u89c6\u9891\u751f\u6210\u548c\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e0a\uff0cFoCa\u5b9e\u73b0\u4e86\u663e\u8457\u52a0\u901f\uff1aFLUX 5.50\u500d\u3001HunyuanVideo 6.45\u500d\u3001Inf-DiT 3.17\u500d\u3001DiT 4.53\u500d\uff0c\u4e14\u4fdd\u6301\u9ad8\u8d28\u91cf\u3002", "conclusion": "FoCa\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u8fd1\u65e0\u635f\u7684\u9ad8\u901f\u63a8\u7406\uff0c\u4e3aDiffusion Transformers\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u52a0\u901f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.16212", "pdf": "https://arxiv.org/pdf/2508.16212", "abs": "https://arxiv.org/abs/2508.16212", "authors": ["Huanpeng Chu", "Wei Wu", "Guanyu Fen", "Yutao Zhang"], "title": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for Diffusion Transformer Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted by ICCV 2025", "summary": "Diffusion models have emerged as a powerful paradigm for generative tasks such as image synthesis and video generation, with Transformer architectures further enhancing performance. However, the high computational cost of diffusion Transformers-stemming from a large number of sampling steps and complex per-step computations-presents significant challenges for real-time deployment. In this paper, we introduce OmniCache, a training-free acceleration method that exploits the global redundancy inherent in the denoising process. Unlike existing methods that determine caching strategies based on inter-step similarities and tend to prioritize reusing later sampling steps, our approach originates from the sampling perspective of DIT models. We systematically analyze the model's sampling trajectories and strategically distribute cache reuse across the entire sampling process. This global perspective enables more effective utilization of cached computations throughout the diffusion trajectory, rather than concentrating reuse within limited segments of the sampling procedure.In addition, during cache reuse, we dynamically estimate the corresponding noise and filter it out to reduce its impact on the sampling direction.Extensive experiments demonstrate that our approach accelerates the sampling process while maintaining competitive generative quality, offering a promising and practical solution for efficient deployment of diffusion-based generative models.", "AI": {"tldr": "OmniCache\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u8f68\u8ff9\u548c\u5168\u5c40\u5197\u4f59\u6027\uff0c\u5728\u6574\u4e2a\u91c7\u6837\u8fc7\u7a0b\u4e2d\u667a\u80fd\u5206\u914d\u7f13\u5b58\u91cd\u7528\uff0c\u5e76\u52a8\u6001\u8fc7\u6ee4\u566a\u58f0\uff0c\u5b9e\u73b0\u52a0\u901f\u91c7\u6837\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6269\u6563Transformer\u6a21\u578b\u867d\u7136\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u91c7\u6837\u6b65\u9aa4\u591a\u3001\u8ba1\u7b97\u590d\u6742\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5b9e\u65f6\u90e8\u7f72\u3002\u73b0\u6709\u7f13\u5b58\u65b9\u6cd5\u57fa\u4e8e\u6b65\u9aa4\u95f4\u76f8\u4f3c\u6027\uff0c\u503e\u5411\u4e8e\u91cd\u7528\u540e\u671f\u91c7\u6837\u6b65\u9aa4\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u5168\u5c40\u5197\u4f59\u6027\u3002", "method": "\u4eceDIT\u6a21\u578b\u7684\u91c7\u6837\u89c6\u89d2\u51fa\u53d1\uff0c\u7cfb\u7edf\u5206\u6790\u6a21\u578b\u91c7\u6837\u8f68\u8ff9\uff0c\u5728\u6574\u4e2a\u91c7\u6837\u8fc7\u7a0b\u4e2d\u7b56\u7565\u6027\u5206\u914d\u7f13\u5b58\u91cd\u7528\u3002\u5728\u7f13\u5b58\u91cd\u7528\u671f\u95f4\u52a8\u6001\u4f30\u8ba1\u5e76\u8fc7\u6ee4\u76f8\u5e94\u566a\u58f0\uff0c\u51cf\u5c11\u5bf9\u91c7\u6837\u65b9\u5411\u7684\u5f71\u54cd\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u52a0\u901f\u4e86\u91c7\u6837\u8fc7\u7a0b\uff0c\u4e3a\u6269\u6563\u751f\u6210\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "OmniCache\u901a\u8fc7\u5168\u5c40\u89c6\u89d2\u7684\u7f13\u5b58\u91cd\u7528\u7b56\u7565\u548c\u566a\u58f0\u8fc7\u6ee4\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563Transformer\u6a21\u578b\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8bad\u7ec3\u514d\u8d39\u7684\u52a0\u901f\u6548\u679c\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2508.16217", "pdf": "https://arxiv.org/pdf/2508.16217", "abs": "https://arxiv.org/abs/2508.16217", "authors": ["Hohyun Na", "Seunghoo Hong", "Simon S. Woo"], "title": "PromptFlare: Prompt-Generalized Defense via Cross-Attention Decoy in Diffusion-Based Inpainting", "categories": ["cs.CV"], "comment": "Accepted to ACM MM 2025", "summary": "The success of diffusion models has enabled effortless, high-quality image modifications that precisely align with users' intentions, thereby raising concerns about their potential misuse by malicious actors. Previous studies have attempted to mitigate such misuse through adversarial attacks. However, these approaches heavily rely on image-level inconsistencies, which pose fundamental limitations in addressing the influence of textual prompts. In this paper, we propose PromptFlare, a novel adversarial protection method designed to protect images from malicious modifications facilitated by diffusion-based inpainting models. Our approach leverages the cross-attention mechanism to exploit the intrinsic properties of prompt embeddings. Specifically, we identify and target shared token of prompts that is invariant and semantically uninformative, injecting adversarial noise to suppress the sampling process. The injected noise acts as a cross-attention decoy, diverting the model's focus away from meaningful prompt-image alignments and thereby neutralizing the effect of prompt. Extensive experiments on the EditBench dataset demonstrate that our method achieves state-of-the-art performance across various metrics while significantly reducing computational overhead and GPU memory usage. These findings highlight PromptFlare as a robust and efficient protection against unauthorized image manipulations. The code is available at https://github.com/NAHOHYUN-SKKU/PromptFlare.", "AI": {"tldr": "PromptFlare\u662f\u4e00\u79cd\u9488\u5bf9\u6269\u6563\u6a21\u578b\u7684\u65b0\u578b\u5bf9\u6297\u4fdd\u62a4\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u548c\u63d0\u793a\u5d4c\u5165\u7684\u5185\u5728\u7279\u6027\uff0c\u9632\u6b62\u6076\u610f\u56fe\u50cf\u4fee\u6539\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u7684\u6210\u529f\u4f7f\u5f97\u9ad8\u8d28\u91cf\u56fe\u50cf\u4fee\u6539\u53d8\u5f97\u5bb9\u6613\uff0c\u5f15\u53d1\u4e86\u6076\u610f\u6ee5\u7528\u7684\u62c5\u5fe7\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u56fe\u50cf\u7ea7\u4e0d\u4e00\u81f4\u6027\uff0c\u96be\u4ee5\u6709\u6548\u5904\u7406\u6587\u672c\u63d0\u793a\u7684\u5f71\u54cd\u3002", "method": "\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8bc6\u522b\u5e76\u9488\u5bf9\u63d0\u793a\u4e2d\u4e0d\u53d8\u4e14\u8bed\u4e49\u65e0\u4fe1\u606f\u7684\u5171\u4eab\u6807\u8bb0\uff0c\u6ce8\u5165\u5bf9\u6297\u566a\u58f0\u6765\u6291\u5236\u91c7\u6837\u8fc7\u7a0b\uff0c\u4f5c\u4e3a\u4ea4\u53c9\u6ce8\u610f\u529b\u8bf1\u9975\u5206\u6563\u6a21\u578b\u6ce8\u610f\u529b\u3002", "result": "\u5728EditBench\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u548cGPU\u5185\u5b58\u4f7f\u7528\u3002", "conclusion": "PromptFlare\u4e3a\u672a\u7ecf\u6388\u6743\u7684\u56fe\u50cf\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5f3a\u5927\u800c\u9ad8\u6548\u7684\u4fdd\u62a4\u65b9\u6848\u3002"}}
{"id": "2508.16239", "pdf": "https://arxiv.org/pdf/2508.16239", "abs": "https://arxiv.org/abs/2508.16239", "authors": ["Nan wang", "Zhiyi Xia", "Yiming Li", "Shi Tang", "Zuxin Fan", "Xi Fang", "Haoyi Tao", "Xiaochen Cai", "Guolin Ke", "Linfeng Zhang", "Yanhui Hong"], "title": "UniEM-3M: A Universal Electron Micrograph Dataset for Microstructural Segmentation and Generation", "categories": ["cs.CV"], "comment": "15 pages, 13 figures, Submitted to AAAI2026", "summary": "Quantitative microstructural characterization is fundamental to materials science, where electron micrograph (EM) provides indispensable high-resolution insights. However, progress in deep learning-based EM characterization has been hampered by the scarcity of large-scale, diverse, and expert-annotated datasets, due to acquisition costs, privacy concerns, and annotation complexity. To address this issue, we introduce UniEM-3M, the first large-scale and multimodal EM dataset for instance-level understanding. It comprises 5,091 high-resolution EMs, about 3 million instance segmentation labels, and image-level attribute-disentangled textual descriptions, a subset of which will be made publicly available. Furthermore, we are also releasing a text-to-image diffusion model trained on the entire collection to serve as both a powerful data augmentation tool and a proxy for the complete data distribution. To establish a rigorous benchmark, we evaluate various representative instance segmentation methods on the complete UniEM-3M and present UniEM-Net as a strong baseline model. Quantitative experiments demonstrate that this flow-based model outperforms other advanced methods on this challenging benchmark. Our multifaceted release of a partial dataset, a generative model, and a comprehensive benchmark -- available at huggingface -- will significantly accelerate progress in automated materials analysis.", "AI": {"tldr": "UniEM-3M\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u7535\u5b50\u663e\u5fae\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5305\u542b5091\u5f20\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u3001\u7ea6300\u4e07\u4e2a\u5b9e\u4f8b\u5206\u5272\u6807\u7b7e\u548c\u56fe\u50cf\u7ea7\u6587\u672c\u63cf\u8ff0\uff0c\u65e8\u5728\u89e3\u51b3\u6750\u6599\u79d1\u5b66\u4e2d\u6df1\u5ea6\u5b66\u4e60\u8868\u5f81\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u7535\u5b50\u663e\u5fae\u56fe\u50cf\u7684\u9ad8\u5206\u8fa8\u7387\u8868\u5f81\u5bf9\u6750\u6599\u79d1\u5b66\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u8fdb\u5c55\u53d7\u5230\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u3001\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u9650\u5236\uff0c\u4e3b\u8981\u7531\u4e8e\u83b7\u53d6\u6210\u672c\u9ad8\u3001\u9690\u79c1\u95ee\u9898\u548c\u6807\u6ce8\u590d\u6742\u6027\u3002", "method": "\u6784\u5efa\u4e86UniEM-3M\u6570\u636e\u96c6\uff0c\u5305\u542b\u56fe\u50cf\u3001\u5b9e\u4f8b\u5206\u5272\u6807\u7b7e\u548c\u6587\u672c\u63cf\u8ff0\uff1b\u53d1\u5e03\u4e86\u57fa\u4e8e\u6574\u4e2a\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7528\u4e8e\u6570\u636e\u589e\u5f3a\uff1b\u63d0\u51fa\u4e86\u57fa\u4e8e\u6d41\u7684UniEM-Net\u4f5c\u4e3a\u5f3a\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u5b9e\u4f8b\u5206\u5272\u3002", "result": "\u5b9a\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u6d41\u7684UniEM-Net\u6a21\u578b\u5728\u8fd9\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5176\u4ed6\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u53d1\u5e03\u90e8\u5206\u6570\u636e\u96c6\u3001\u751f\u6210\u6a21\u578b\u548c\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8fd9\u9879\u5de5\u4f5c\u5c06\u663e\u8457\u52a0\u901f\u81ea\u52a8\u5316\u6750\u6599\u5206\u6790\u7684\u8fdb\u5c55\u3002"}}
{"id": "2508.16467", "pdf": "https://arxiv.org/pdf/2508.16467", "abs": "https://arxiv.org/abs/2508.16467", "authors": ["Huimin Zeng", "Yue Bai", "Yun Fu"], "title": "Arbitrary-Scale 3D Gaussian Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Existing 3D Gaussian Splatting (3DGS) super-resolution methods typically perform high-resolution (HR) rendering of fixed scale factors, making them impractical for resource-limited scenarios. Directly rendering arbitrary-scale HR views with vanilla 3DGS introduces aliasing artifacts due to the lack of scale-aware rendering ability, while adding a post-processing upsampler for 3DGS complicates the framework and reduces rendering efficiency. To tackle these issues, we build an integrated framework that incorporates scale-aware rendering, generative prior-guided optimization, and progressive super-resolving to enable 3D Gaussian super-resolution of arbitrary scale factors with a single 3D model. Notably, our approach supports both integer and non-integer scale rendering to provide more flexibility. Extensive experiments demonstrate the effectiveness of our model in rendering high-quality arbitrary-scale HR views (6.59 dB PSNR gain over 3DGS) with a single model. It preserves structural consistency with LR views and across different scales, while maintaining real-time rendering speed (85 FPS at 1080p).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u652f\u6301\u4efb\u610f\u5c3a\u5ea6\uff08\u5305\u62ec\u6574\u6570\u548c\u975e\u6574\u6570\uff09\u76843D\u9ad8\u65af\u6e85\u5c04\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u5c3a\u5ea6\u611f\u77e5\u6e32\u67d3\u3001\u751f\u6210\u5148\u9a8c\u5f15\u5bfc\u4f18\u5316\u548c\u6e10\u8fdb\u5f0f\u8d85\u5206\u8fa8\u7387\u6280\u672f\uff0c\u5b9e\u73b0\u5355\u4e00\u6a21\u578b\u7684\u9ad8\u8d28\u91cf\u5b9e\u65f6\u6e32\u67d3\u3002", "motivation": "\u73b0\u67093DGS\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u53ea\u80fd\u5904\u7406\u56fa\u5b9a\u5c3a\u5ea6\u56e0\u5b50\uff0c\u4e0d\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u573a\u666f\u3002\u76f4\u63a5\u6e32\u67d3\u4efb\u610f\u5c3a\u5ea6\u4f1a\u5f15\u5165\u6df7\u53e0\u4f2a\u5f71\uff0c\u800c\u6dfb\u52a0\u540e\u5904\u7406\u4e0a\u91c7\u6837\u5668\u4f1a\u964d\u4f4e\u6e32\u67d3\u6548\u7387\u3002", "method": "\u6784\u5efa\u96c6\u6210\u6846\u67b6\uff0c\u5305\u542b\u5c3a\u5ea6\u611f\u77e5\u6e32\u67d3\u3001\u751f\u6210\u5148\u9a8c\u5f15\u5bfc\u4f18\u5316\u548c\u6e10\u8fdb\u5f0f\u8d85\u5206\u8fa8\u7387\u6280\u672f\uff0c\u652f\u6301\u6574\u6570\u548c\u975e\u6574\u6570\u5c3a\u5ea6\u6e32\u67d3\u3002", "result": "\u5728\u6e32\u67d3\u4efb\u610f\u5c3a\u5ea6\u9ad8\u8d28\u91cfHR\u89c6\u56fe\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff08\u6bd43DGS\u63d0\u53476.59 dB PSNR\uff09\uff0c\u4fdd\u6301\u4e0eLR\u89c6\u56fe\u7684\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u7ef4\u6301\u5b9e\u65f6\u6e32\u67d3\u901f\u5ea6\uff081080p\u4e0b85 FPS\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e863DGS\u5728\u4efb\u610f\u5c3a\u5ea6\u8d85\u5206\u8fa8\u7387\u4e2d\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5355\u4e00\u6a21\u578b\u7684\u9ad8\u8d28\u91cf\u3001\u9ad8\u6548\u7387\u6e32\u67d3\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.16512", "pdf": "https://arxiv.org/pdf/2508.16512", "abs": "https://arxiv.org/abs/2508.16512", "authors": ["Chun-Peng Chang", "Chen-Yu Wang", "Julian Schmidt", "Holger Caesar", "Alain Pagani"], "title": "Seeing Clearly, Forgetting Deeply: Revisiting Fine-Tuned Video Generators for Driving Simulation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in video generation have substantially improved visual quality and temporal coherence, making these models increasingly appealing for applications such as autonomous driving, particularly in the context of driving simulation and so-called \"world models\". In this work, we investigate the effects of existing fine-tuning video generation approaches on structured driving datasets and uncover a potential trade-off: although visual fidelity improves, spatial accuracy in modeling dynamic elements may degrade. We attribute this degradation to a shift in the alignment between visual quality and dynamic understanding objectives. In datasets with diverse scene structures within temporal space, where objects or perspective shift in varied ways, these objectives tend to highly correlated. However, the very regular and repetitive nature of driving scenes allows visual quality to improve by modeling dominant scene motion patterns, without necessarily preserving fine-grained dynamic behavior. As a result, fine-tuning encourages the model to prioritize surface-level realism over dynamic accuracy. To further examine this phenomenon, we show that simple continual learning strategies, such as replay from diverse domains, can offer a balanced alternative by preserving spatial accuracy while maintaining strong visual quality.", "AI": {"tldr": "\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u9a7e\u9a76\u6570\u636e\u96c6\u5fae\u8c03\u65f6\u5b58\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u63d0\u5347\u4f46\u7a7a\u95f4\u52a8\u6001\u7cbe\u5ea6\u4e0b\u964d\u7684\u6743\u8861\u95ee\u9898\uff0c\u6301\u7eed\u5b66\u4e60\u7b56\u7565\u53ef\u4ee5\u5e73\u8861\u4e24\u8005", "motivation": "\u7814\u7a76\u73b0\u6709\u89c6\u9891\u751f\u6210\u5fae\u8c03\u65b9\u6cd5\u5728\u7ed3\u6784\u5316\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u7684\u6548\u679c\uff0c\u53d1\u73b0\u89c6\u89c9\u8d28\u91cf\u63d0\u5347\u53ef\u80fd\u5bfc\u81f4\u52a8\u6001\u5143\u7d20\u7a7a\u95f4\u7cbe\u5ea6\u4e0b\u964d\u7684\u6743\u8861\u95ee\u9898", "method": "\u5206\u6790\u9a7e\u9a76\u573a\u666f\u7684\u89c4\u5f8b\u6027\u548c\u91cd\u590d\u6027\u7279\u70b9\uff0c\u4f7f\u7528\u6301\u7eed\u5b66\u4e60\u7b56\u7565\uff08\u5982\u591a\u57df\u56de\u653e\uff09\u6765\u5e73\u8861\u89c6\u89c9\u8d28\u91cf\u548c\u52a8\u6001\u7cbe\u5ea6", "result": "\u53d1\u73b0\u5fae\u8c03\u4f1a\u4f7f\u6a21\u578b\u4f18\u5148\u8003\u8651\u8868\u9762\u771f\u5b9e\u611f\u800c\u975e\u52a8\u6001\u7cbe\u5ea6\uff0c\u800c\u6301\u7eed\u5b66\u4e60\u7b56\u7565\u80fd\u591f\u5728\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u7684\u540c\u65f6\u7ef4\u6301\u7a7a\u95f4\u51c6\u786e\u6027", "conclusion": "\u9a7e\u9a76\u573a\u666f\u7684\u7279\u6b8a\u6027\u5bfc\u81f4\u89c6\u89c9\u8d28\u91cf\u4e0e\u52a8\u6001\u7406\u89e3\u76ee\u6807\u4e4b\u95f4\u7684\u5bf9\u9f50\u504f\u79fb\uff0c\u9700\u8981\u91c7\u7528\u5e73\u8861\u7b56\u7565\u6765\u540c\u65f6\u4fdd\u8bc1\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u7a7a\u95f4\u51c6\u786e\u6027"}}
{"id": "2508.16577", "pdf": "https://arxiv.org/pdf/2508.16577", "abs": "https://arxiv.org/abs/2508.16577", "authors": ["Yosef Dayani", "Omer Benishu", "Sagie Benaim"], "title": "MV-RAG: Retrieval Augmented Multiview Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://yosefdayani.github.io/MV-RAG", "summary": "Text-to-3D generation approaches have advanced significantly by leveraging pretrained 2D diffusion priors, producing high-quality and 3D-consistent outputs. However, they often fail to produce out-of-domain (OOD) or rare concepts, yielding inconsistent or inaccurate results. To this end, we propose MV-RAG, a novel text-to-3D pipeline that first retrieves relevant 2D images from a large in-the-wild 2D database and then conditions a multiview diffusion model on these images to synthesize consistent and accurate multiview outputs. Training such a retrieval-conditioned model is achieved via a novel hybrid strategy bridging structured multiview data and diverse 2D image collections. This involves training on multiview data using augmented conditioning views that simulate retrieval variance for view-specific reconstruction, alongside training on sets of retrieved real-world 2D images using a distinctive held-out view prediction objective: the model predicts the held-out view from the other views to infer 3D consistency from 2D data. To facilitate a rigorous OOD evaluation, we introduce a new collection of challenging OOD prompts. Experiments against state-of-the-art text-to-3D, image-to-3D, and personalization baselines show that our approach significantly improves 3D consistency, photorealism, and text adherence for OOD/rare concepts, while maintaining competitive performance on standard benchmarks.", "AI": {"tldr": "MV-RAG\u662f\u4e00\u4e2a\u65b0\u7684\u6587\u672c\u52303D\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u7d22\u771f\u5b9e\u4e16\u754c\u76842D\u56fe\u50cf\u6765\u589e\u5f3a\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7f55\u89c1\u6982\u5ff5\u548c\u57df\u5916\u5185\u5bb9\u76843D\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u52303D\u751f\u6210\u65b9\u6cd5\u5728\u5904\u7406\u57df\u5916\u6216\u7f55\u89c1\u6982\u5ff5\u65f6\u5f80\u5f80\u4ea7\u751f\u4e0d\u4e00\u81f4\u6216\u4e0d\u51c6\u786e\u7684\u7ed3\u679c\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5229\u7528\u771f\u5b9e\u4e16\u754c2D\u56fe\u50cf\u6570\u636e\u6765\u589e\u5f3a3D\u751f\u6210\u8d28\u91cf\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMV-RAG\u7ba1\u9053\uff1a\u9996\u5148\u4ece\u5927\u89c4\u6a212D\u56fe\u50cf\u6570\u636e\u5e93\u4e2d\u68c0\u7d22\u76f8\u5173\u56fe\u50cf\uff0c\u7136\u540e\u7528\u8fd9\u4e9b\u56fe\u50cf\u6761\u4ef6\u5316\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u3002\u91c7\u7528\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u591a\u89c6\u89d2\u6570\u636e\u548c\u68c0\u7d22\u5230\u7684\u771f\u5b9e2D\u56fe\u50cf\uff0c\u901a\u8fc7\u4fdd\u6301\u89c6\u56fe\u9884\u6d4b\u76ee\u6807\u6765\u4ece2D\u6570\u636e\u63a8\u65ad3D\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u57df\u5916/\u7f55\u89c1\u6982\u5ff5\u4e0a\u663e\u8457\u63d0\u9ad8\u4e863D\u4e00\u81f4\u6027\u3001\u7167\u7247\u771f\u5b9e\u6027\u548c\u6587\u672c\u9075\u5faa\u6027\uff0c\u540c\u65f6\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "MV-RAG\u901a\u8fc7\u68c0\u7d22\u771f\u5b9e2D\u56fe\u50cf\u6765\u589e\u5f3a\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u52303D\u751f\u6210\u4e2d\u57df\u5916\u6982\u5ff5\u7684\u6311\u6218\uff0c\u4e3a3D\u5185\u5bb9\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.15972", "pdf": "https://arxiv.org/pdf/2508.15972", "abs": "https://arxiv.org/abs/2508.15972", "authors": ["Zhaodong Jiang", "Ashish Sinha", "Tongtong Cao", "Yuan Ren", "Bingbing Liu", "Binbin Xu"], "title": "UnPose: Uncertainty-Guided Diffusion Priors for Zero-Shot Pose Estimation", "categories": ["cs.RO", "cs.CV"], "comment": "Published at the Conference on Robot Learning (CoRL) 2025. For more   details please visit https://frankzhaodong.github.io/UnPose", "summary": "Estimating the 6D pose of novel objects is a fundamental yet challenging problem in robotics, often relying on access to object CAD models. However, acquiring such models can be costly and impractical. Recent approaches aim to bypass this requirement by leveraging strong priors from foundation models to reconstruct objects from single or multi-view images, but typically require additional training or produce hallucinated geometry. To this end, we propose UnPose, a novel framework for zero-shot, model-free 6D object pose estimation and reconstruction that exploits 3D priors and uncertainty estimates from a pre-trained diffusion model. Specifically, starting from a single-view RGB-D frame, UnPose uses a multi-view diffusion model to estimate an initial 3D model using 3D Gaussian Splatting (3DGS) representation, along with pixel-wise epistemic uncertainty estimates. As additional observations become available, we incrementally refine the 3DGS model by fusing new views guided by the diffusion model's uncertainty, thereby continuously improving the pose estimation accuracy and 3D reconstruction quality. To ensure global consistency, the diffusion prior-generated views and subsequent observations are further integrated in a pose graph and jointly optimized into a coherent 3DGS field. Extensive experiments demonstrate that UnPose significantly outperforms existing approaches in both 6D pose estimation accuracy and 3D reconstruction quality. We further showcase its practical applicability in real-world robotic manipulation tasks.", "AI": {"tldr": "UnPose\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u3001\u65e0\u6a21\u578b\u76846D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u548c\u91cd\u5efa\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u76843D\u5148\u9a8c\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u4ece\u5355\u89c6\u56feRGB-D\u56fe\u50cf\u5f00\u59cb\uff0c\u901a\u8fc7\u591a\u89c6\u56fe\u6269\u6563\u6a21\u578b\u548c3D\u9ad8\u65af\u6e85\u5c04\u8868\u793a\u9010\u6b65\u4f18\u5316\u59ff\u6001\u4f30\u8ba1\u548c3D\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf6D\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u7269\u4f53CAD\u6a21\u578b\uff0c\u4f46\u83b7\u53d6\u6210\u672c\u9ad8\u4e14\u4e0d\u5b9e\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u5c1d\u8bd5\u5229\u7528\u57fa\u7840\u6a21\u578b\u4ece\u56fe\u50cf\u91cd\u5efa\u7269\u4f53\uff0c\u4f46\u901a\u5e38\u9700\u8981\u989d\u5916\u8bad\u7ec3\u6216\u4ea7\u751f\u5e7b\u89c9\u51e0\u4f55\u3002", "method": "\u4ece\u5355\u89c6\u56feRGB-D\u5e27\u5f00\u59cb\uff0c\u4f7f\u7528\u591a\u89c6\u56fe\u6269\u6563\u6a21\u578b\u4f30\u8ba1\u521d\u59cb3D\u6a21\u578b\uff083D\u9ad8\u65af\u6e85\u5c04\u8868\u793a\uff09\u548c\u50cf\u7d20\u7ea7\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u3002\u968f\u7740\u65b0\u89c2\u6d4b\u53ef\u7528\uff0c\u5728\u6269\u6563\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u6307\u5bfc\u4e0b\u589e\u91cf\u878d\u5408\u65b0\u89c6\u56fe\uff0c\u5e76\u901a\u8fc7\u4f4d\u59ff\u56fe\u4f18\u5316\u786e\u4fdd\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cUnPose\u57286D\u59ff\u6001\u4f30\u8ba1\u7cbe\u5ea6\u548c3D\u91cd\u5efa\u8d28\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u6027\u3002", "conclusion": "UnPose\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u96f6\u6837\u672c\u89e3\u51b3\u65b9\u6848\uff0c\u6210\u529f\u5229\u7528\u6269\u6563\u6a21\u578b\u76843D\u5148\u9a8c\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6765\u5b9e\u73b0\u9ad8\u8d28\u91cf\u76846D\u59ff\u6001\u4f30\u8ba1\u548c3D\u91cd\u5efa\uff0c\u65e0\u9700\u7269\u4f53CAD\u6a21\u578b\u6216\u989d\u5916\u8bad\u7ec3\u3002"}}
{"id": "2508.16121", "pdf": "https://arxiv.org/pdf/2508.16121", "abs": "https://arxiv.org/abs/2508.16121", "authors": ["Wontae Kim", "Keuntek Lee", "Nam Ik Cho"], "title": "Lightweight and Fast Real-time Image Enhancement via Decomposition of the Spatial-aware Lookup Tables", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "The image enhancement methods based on 3D lookup tables (3D LUTs) efficiently reduce both model size and runtime by interpolating pre-calculated values at the vertices. However, the 3D LUT methods have a limitation due to their lack of spatial information, as they convert color values on a point-by-point basis. Although spatial-aware 3D LUT methods address this limitation, they introduce additional modules that require a substantial number of parameters, leading to increased runtime as image resolution increases. To address this issue, we propose a method for generating image-adaptive LUTs by focusing on the redundant parts of the tables. Our efficient framework decomposes a 3D LUT into a linear sum of low-dimensional LUTs and employs singular value decomposition (SVD). Furthermore, we enhance the modules for spatial feature fusion to be more cache-efficient. Extensive experimental results demonstrate that our model effectively decreases both the number of parameters and runtime while maintaining spatial awareness and performance.", "AI": {"tldr": "\u901a\u8fc7\u5bf93D LUT\u8fdb\u884c\u5947\u5f02\u503c\u5206\u89e3\u548c\u7f13\u5b58\u4f18\u5316\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7a7a\u95f4\u611f\u77e5\u4e14\u9ad8\u6548\u7684\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u548c\u8fd0\u884c\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u76843D LUT\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u5b58\u5728\u4e24\u5927\u95ee\u9898\uff1a\u7f3a\u4e4f\u7a7a\u95f4\u4fe1\u606f\uff0c\u4ee5\u53ca\u7a7a\u95f4\u611f\u77e5\u65b9\u6cd5\u53c2\u6570\u91cf\u5927\u3001\u8fd0\u884c\u65f6\u95f4\u968f\u56fe\u50cf\u5206\u8fa8\u7387\u589e\u52a0\u800c\u589e\u52a0\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u4fdd\u6301\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u7684\u540c\u65f6\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u4f7f\u7528\u5947\u5f02\u503c\u5206\u89e3(SVD)\u5c063D LUT\u5206\u89e3\u4e3a\u4f4e\u7ef4\u5ea6LUT\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u91cd\u70b9\u5173\u6ce8\u8868\u683c\u4e2d\u7684\u5197\u4f59\u90e8\u5206\u3002\u540c\u65f6\u6539\u8fdb\u7a7a\u95f4\u7279\u5f81\u878d\u5408\u6a21\u5757\uff0c\u4f7f\u5176\u66f4\u52a0\u7f13\u5b58\u6548\u679c\u3002\u901a\u8fc7\u8fd9\u79cd\u5206\u89e3\u548c\u4f18\u5316\u65b9\u5f0f\u751f\u6210\u56fe\u50cf\u9002\u914d\u6027LUT\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u6709\u6548\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u548c\u8fd0\u884c\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u548c\u56fe\u50cf\u589e\u5f3a\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf93D LUT\u8fdb\u884c\u6548\u7387\u4f18\u5316\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7a7a\u95f4\u611f\u77e53D LUT\u65b9\u6cd5\u7684\u53c2\u6570\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u56fe\u50cf\u589e\u5f3a\u3002"}}
{"id": "2508.16252", "pdf": "https://arxiv.org/pdf/2508.16252", "abs": "https://arxiv.org/abs/2508.16252", "authors": ["H\u00e9l\u00e8ne Corbaz", "Anh Nguyen", "Victor Schulze-Zachau", "Paul Friedrich", "Alicia Durrer", "Florentin Bieder", "Philippe C. Cattin", "Marios N Psychogios"], "title": "Towards Diagnostic Quality Flat-Panel Detector CT Imaging Using Diffusion Models", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Patients undergoing a mechanical thrombectomy procedure usually have a multi-detector CT (MDCT) scan before and after the intervention. The image quality of the flat panel detector CT (FDCT) present in the intervention room is generally much lower than that of a MDCT due to significant artifacts. However, using only FDCT images could improve patient management as the patient would not need to be moved to the MDCT room. Several studies have evaluated the potential use of FDCT imaging alone and the time that could be saved by acquiring the images before and/or after the intervention only with the FDCT. This study proposes using a denoising diffusion probabilistic model (DDPM) to improve the image quality of FDCT scans, making them comparable to MDCT scans. Clinicans evaluated FDCT, MDCT, and our model's predictions for diagnostic purposes using a questionnaire. The DDPM eliminated most artifacts and improved anatomical visibility without reducing bleeding detection, provided that the input FDCT image quality is not too low. Our code can be found on github.", "AI": {"tldr": "\u4f7f\u7528\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b(DDPM)\u63d0\u5347\u5e73\u677f\u63a2\u6d4b\u5668CT(FDCT)\u56fe\u50cf\u8d28\u91cf\uff0c\u4f7f\u5176\u63a5\u8fd1\u591a\u63a2\u6d4b\u5668CT(MDCT)\u6c34\u5e73\uff0c\u51cf\u5c11\u60a3\u8005\u79fb\u52a8\u9700\u6c42", "motivation": "\u673a\u68b0\u53d6\u6813\u624b\u672f\u60a3\u8005\u901a\u5e38\u9700\u8981\u5728MDCT\u548cFDCT\u4e4b\u95f4\u79fb\u52a8\uff0cFDCT\u56fe\u50cf\u8d28\u91cf\u8f83\u5dee\u4f46\u4f7f\u7528\u66f4\u65b9\u4fbf\uff0c\u5e0c\u671b\u901a\u8fc7AI\u6280\u672f\u63d0\u5347FDCT\u56fe\u50cf\u8d28\u91cf", "method": "\u91c7\u7528\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b(DDPM)\u5bf9FDCT\u56fe\u50cf\u8fdb\u884c\u53bb\u566a\u548c\u589e\u5f3a\u5904\u7406\uff0c\u6d88\u9664\u4f2a\u5f71\u5e76\u63d0\u9ad8\u89e3\u5256\u7ed3\u6784\u53ef\u89c1\u6027", "result": "DDPM\u6210\u529f\u6d88\u9664\u4e86\u5927\u90e8\u5206\u4f2a\u5f71\uff0c\u63d0\u9ad8\u4e86\u89e3\u5256\u53ef\u89c1\u6027\uff0c\u4e14\u4e0d\u5f71\u54cd\u51fa\u8840\u68c0\u6d4b\u80fd\u529b\uff08\u524d\u63d0\u662f\u8f93\u5165FDCT\u56fe\u50cf\u8d28\u91cf\u4e0d\u8fc7\u4f4e\uff09", "conclusion": "DDPM\u80fd\u6709\u6548\u63d0\u5347FDCT\u56fe\u50cf\u8d28\u91cf\uff0c\u4f7f\u5176\u5728\u8bca\u65ad\u4ef7\u503c\u4e0a\u63a5\u8fd1MDCT\uff0c\u6709\u671b\u51cf\u5c11\u60a3\u8005\u79fb\u52a8\u9700\u6c42\uff0c\u6539\u5584\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b"}}
{"id": "2508.16424", "pdf": "https://arxiv.org/pdf/2508.16424", "abs": "https://arxiv.org/abs/2508.16424", "authors": ["Hafeez Ur Rehman", "Sumaiya Fazal", "Moutaz Alazab", "Ali Baydoun"], "title": "Decoding MGMT Methylation: A Step Towards Precision Medicine in Glioblastoma", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Glioblastomas, constituting over 50% of malignant brain tumors, are highly aggressive brain tumors that pose substantial treatment challenges due to their rapid progression and resistance to standard therapies. The methylation status of the O-6-Methylguanine-DNA Methyltransferase (MGMT) gene is a critical biomarker for predicting patient response to treatment, particularly with the alkylating agent temozolomide. However, accurately predicting MGMT methylation status using non-invasive imaging techniques remains challenging due to the complex and heterogeneous nature of glioblastomas, that includes, uneven contrast, variability within lesions, and irregular enhancement patterns. This study introduces the Convolutional Autoencoders for MGMT Methylation Status Prediction (CAMP) framework, which is based on adaptive sparse penalties to enhance predictive accuracy. The CAMP framework operates in two phases: first, generating synthetic MRI slices through a tailored autoencoder that effectively captures and preserves intricate tissue and tumor structures across different MRI modalities; second, predicting MGMT methylation status using a convolutional neural network enhanced by adaptive sparse penalties. The adaptive sparse penalty dynamically adjusts to variations in the data, such as contrast differences and tumor locations in MR images. Our method excels in MRI image synthesis, preserving brain tissue, fat, and individual tumor structures across all MRI modalities. Validated on benchmark datasets, CAMP achieved an accuracy of 0.97, specificity of 0.98, and sensitivity of 0.97, significantly outperforming existing methods. These results demonstrate the potential of the CAMP framework to improve the interpretation of MRI data and contribute to more personalized treatment strategies for glioblastoma patients.", "AI": {"tldr": "CAMP\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u7a00\u758f\u60e9\u7f5a\u7684\u5377\u79ef\u81ea\u7f16\u7801\u5668\uff0c\u4eceMRI\u56fe\u50cf\u9884\u6d4b\u80f6\u8d28\u6bcd\u7ec6\u80de\u7624MGMT\u7532\u57fa\u5316\u72b6\u6001\uff0c\u51c6\u786e\u7387\u8fbe0.97\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u80f6\u8d28\u6bcd\u7ec6\u80de\u7624\u6cbb\u7597\u56f0\u96be\uff0cMGMT\u57fa\u56e0\u7532\u57fa\u5316\u72b6\u6001\u662f\u91cd\u8981\u751f\u7269\u6807\u5fd7\u7269\uff0c\u4f46\u73b0\u6709\u975e\u4fb5\u5165\u6027\u5f71\u50cf\u6280\u672f\u9884\u6d4b\u51c6\u786e\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u9884\u6d4b\u65b9\u6cd5", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u5b9a\u5236\u81ea\u7f16\u7801\u5668\u751f\u6210\u5408\u6210MRI\u5207\u7247\uff0c\u4fdd\u7559\u4e0d\u540c\u6a21\u6001\u7684\u7cbe\u7ec6\u7ec4\u7ec7\u7ed3\u6784\uff1b2) \u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\u81ea\u9002\u5e94\u7a00\u758f\u60e9\u7f5a\u9884\u6d4b\u7532\u57fa\u5316\u72b6\u6001\uff0c\u52a8\u6001\u9002\u5e94\u6570\u636e\u53d8\u5316", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u51c6\u786e\u73870.97\u3001\u7279\u5f02\u60270.98\u3001\u654f\u611f\u60270.97\uff0c\u5728MRI\u56fe\u50cf\u5408\u6210\u548c\u7532\u57fa\u5316\u72b6\u6001\u9884\u6d4b\u65b9\u9762\u5747\u8868\u73b0\u4f18\u5f02", "conclusion": "CAMP\u6846\u67b6\u80fd\u663e\u8457\u6539\u5584MRI\u6570\u636e\u89e3\u8bfb\uff0c\u4e3a\u80f6\u8d28\u6bcd\u7ec6\u80de\u7624\u60a3\u8005\u63d0\u4f9b\u66f4\u4e2a\u6027\u5316\u7684\u6cbb\u7597\u7b56\u7565\uff0c\u5177\u6709\u91cd\u8981\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c"}}
{"id": "2508.16557", "pdf": "https://arxiv.org/pdf/2508.16557", "abs": "https://arxiv.org/abs/2508.16557", "authors": ["Tainyi Zhang", "Zheng-Peng Duan", "Peng-Tao Jiang", "Bo Li", "Ming-Ming Cheng", "Chun-Le Guo", "Chongyi Li"], "title": "Time-Aware One Step Diffusion Network for Real-World Image Super-Resolution", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Diffusion-based real-world image super-resolution (Real-ISR) methods have demonstrated impressive performance. To achieve efficient Real-ISR, many works employ Variational Score Distillation (VSD) to distill pre-trained stable-diffusion (SD) model for one-step SR with a fixed timestep. However, due to the different noise injection timesteps, the SD will perform different generative priors. Therefore, a fixed timestep is difficult for these methods to fully leverage the generative priors in SD, leading to suboptimal performance. To address this, we propose a Time-Aware one-step Diffusion Network for Real-ISR (TADSR). We first introduce a Time-Aware VAE Encoder, which projects the same image into different latent features based on timesteps. Through joint dynamic variation of timesteps and latent features, the student model can better align with the input pattern distribution of the pre-trained SD, thereby enabling more effective utilization of SD's generative capabilities. To better activate the generative prior of SD at different timesteps, we propose a Time-Aware VSD loss that bridges the timesteps of the student model and those of the teacher model, thereby producing more consistent generative prior guidance conditioned on timesteps. Additionally, though utilizing the generative prior in SD at different timesteps, our method can naturally achieve controllable trade-offs between fidelity and realism by changing the timestep condition. Experimental results demonstrate that our method achieves both state-of-the-art performance and controllable SR results with only a single step.", "AI": {"tldr": "\u63d0\u51fa\u4e86TADSR\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u95f4\u611f\u77e5\u673a\u5236\u5145\u5206\u5229\u7528SD\u6a21\u578b\u5728\u4e0d\u540c\u65f6\u95f4\u6b65\u7684\u751f\u6210\u5148\u9a8c\uff0c\u5b9e\u73b0\u5355\u6b65\u8d85\u5206\u8fa8\u7387\u5e76\u652f\u6301\u4fdd\u771f\u5ea6\u4e0e\u771f\u5b9e\u6027\u7684\u53ef\u63a7\u6743\u8861", "motivation": "\u73b0\u6709\u57fa\u4e8eVSD\u7684\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u65f6\u95f4\u6b65\u96be\u4ee5\u5145\u5206\u5229\u7528SD\u6a21\u578b\u5728\u4e0d\u540c\u566a\u58f0\u6ce8\u5165\u65f6\u95f4\u6b65\u7684\u4e0d\u540c\u751f\u6210\u5148\u9a8c\uff0c\u5bfc\u81f4\u6027\u80fd\u6b21\u4f18", "method": "\u63d0\u51fa\u65f6\u95f4\u611f\u77e5VAE\u7f16\u7801\u5668\u5c06\u56fe\u50cf\u6295\u5f71\u5230\u4e0d\u540c\u65f6\u95f4\u6b65\u7684\u6f5c\u5728\u7279\u5f81\uff0c\u4ee5\u53ca\u65f6\u95f4\u611f\u77e5VSD\u635f\u5931\u6765\u6865\u63a5\u5b66\u751f\u6a21\u578b\u548c\u6559\u5e08\u6a21\u578b\u7684\u65f6\u95f4\u6b65", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5355\u6b65\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u548c\u53ef\u63a7\u7684\u8d85\u5206\u8fa8\u7387\u7ed3\u679c", "conclusion": "TADSR\u65b9\u6cd5\u901a\u8fc7\u65f6\u95f4\u611f\u77e5\u673a\u5236\u6709\u6548\u5229\u7528\u4e86SD\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u53ef\u63a7\u7684\u771f\u5b9e\u56fe\u50cf\u8d85\u5206\u8fa8\u7387"}}
