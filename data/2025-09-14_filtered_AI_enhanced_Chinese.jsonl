{"id": "2509.08947", "pdf": "https://arxiv.org/pdf/2509.08947", "abs": "https://arxiv.org/abs/2509.08947", "authors": ["Yancheng Cai", "Robert Wanat", "Rafal Mantiuk"], "title": "CameraVDP: Perceptual Display Assessment with Uncertainty Estimation via Camera and Visual Difference Prediction", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted by SIGGRAPH Asia 2025", "summary": "Accurate measurement of images produced by electronic displays is critical for the evaluation of both traditional and computational displays. Traditional display measurement methods based on sparse radiometric sampling and fitting a model are inadequate for capturing spatially varying display artifacts, as they fail to capture high-frequency and pixel-level distortions. While cameras offer sufficient spatial resolution, they introduce optical, sampling, and photometric distortions. Furthermore, the physical measurement must be combined with a model of a visual system to assess whether the distortions are going to be visible. To enable perceptual assessment of displays, we propose a combination of a camera-based reconstruction pipeline with a visual difference predictor, which account for both the inaccuracy of camera measurements and visual difference prediction. The reconstruction pipeline combines HDR image stacking, MTF inversion, vignetting correction, geometric undistortion, homography transformation, and color correction, enabling cameras to function as precise display measurement instruments. By incorporating a Visual Difference Predictor (VDP), our system models the visibility of various stimuli under different viewing conditions for the human visual system. We validate the proposed CameraVDP framework through three applications: defective pixel detection, color fringing awareness, and display non-uniformity evaluation. Our uncertainty analysis framework enables the estimation of the theoretical upper bound for defect pixel detection performance and provides confidence intervals for VDP quality scores.", "AI": {"tldr": "\u76f8\u673a+VDP\u6846\u67b6\u7528\u4e8e\u663e\u793a\u5668\u8bc4\u6d4b\uff0c\u7ed3\u5408\u4e86\u9ad8\u7cbe\u5ea6\u91cd\u5efa\u548c\u89c6\u89c9\u5dee\u5f02\u9884\u6d4b\uff0c\u80fd\u591f\u8bc4\u4f30\u5c4f\u5e55\u7f3a\u9677\u548c\u975e\u5747\u5300\u6027", "motivation": "\u4f20\u7edf\u663e\u793a\u5668\u6d4b\u91cf\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u7a7a\u95f4\u53d8\u5316\u5dee\u5f02\uff0c\u800c\u76f8\u673a\u6d4b\u91cf\u53c8\u5f15\u5165\u4e86\u5149\u5b66\u548c\u5149\u5ea6\u5f02\u5e38\uff0c\u9700\u8981\u7ed3\u5408\u89c6\u89c9\u7cfb\u7edf\u6a21\u578b\u8fdb\u884c\u611f\u77e5\u8bc4\u4f30", "method": "\u63d0\u51faCameraVDP\u6846\u67b6\uff0c\u7ed3\u5408HDR\u5806\u6808\u3001MTF\u9006\u53d8\u6362\u3001\u6697\u89d2\u6821\u6b63\u3001\u51e0\u4f55\u53bb\u5f02\u53d8\u3001\u540c\u80da\u53d8\u6362\u548c\u989c\u8272\u6821\u6b63\u7684\u91cd\u5efa\u6d41\u6c34\u7ebf\uff0c\u4ee5\u53caVisual Difference Predictor\u89c6\u89c9\u5dee\u5f02\u9884\u6d4b\u5668", "result": "\u901a\u8fc7\u7f3a\u9677\u50cf\u7d20\u68c0\u6d4b\u3001\u989c\u8272\u7f18\u6545\u8bc6\u522b\u548c\u663e\u793a\u975e\u5747\u5300\u6027\u8bc4\u4f30\u4e09\u4e2a\u5e94\u7528\u9a8c\u8bc1\u6846\u67b6\uff0c\u4e0d\u786e\u5b9a\u6027\u5206\u6790\u63d0\u4f9b\u4e86\u7406\u8bba\u6027\u80fd\u4e0a\u9650\u548c\u8d28\u91cf\u5206\u6570\u7684\u7f6e\u4fe1\u533a\u95f4", "conclusion": "CameraVDP\u6846\u67b6\u80fd\u591f\u4f7f\u666e\u901a\u76f8\u673a\u4f5c\u4e3a\u7cbe\u786e\u7684\u663e\u793a\u5668\u6d4b\u91cf\u4eea\u5668\uff0c\u901a\u8fc7\u7ed3\u5408\u91cd\u5efa\u6280\u672f\u548c\u89c6\u89c9\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u663e\u793a\u5668\u7f3a\u9677\u7684\u611f\u77e5\u8bc4\u4f30"}}
{"id": "2509.08908", "pdf": "https://arxiv.org/pdf/2509.08908", "abs": "https://arxiv.org/abs/2509.08908", "authors": ["Rogerio Guimaraes", "Frank Xiao", "Pietro Perona", "Markus Marks"], "title": "Diffusion-Based Action Recognition Generalizes to Untrained Domains", "categories": ["cs.CV"], "comment": null, "summary": "Humans can recognize the same actions despite large context and viewpoint variations, such as differences between species (walking in spiders vs. horses), viewpoints (egocentric vs. third-person), and contexts (real life vs movies). Current deep learning models struggle with such generalization. We propose using features generated by a Vision Diffusion Model (VDM), aggregated via a transformer, to achieve human-like action recognition across these challenging conditions. We find that generalization is enhanced by the use of a model conditioned on earlier timesteps of the diffusion process to highlight semantic information over pixel level details in the extracted features. We experimentally explore the generalization properties of our approach in classifying actions across animal species, across different viewing angles, and different recording contexts. Our model sets a new state-of-the-art across all three generalization benchmarks, bringing machine action recognition closer to human-like robustness. Project page: $\\href{https://www.vision.caltech.edu/actiondiff/}{\\texttt{vision.caltech.edu/actiondiff}}$ Code: $\\href{https://github.com/frankyaoxiao/ActionDiff}{\\texttt{github.com/frankyaoxiao/ActionDiff}}$", "AI": {"tldr": "\u4f7f\u7528\u89c6\u89c9\u6269\u6563\u6a21\u578b\u7279\u5f81\u548ctransformer\u805a\u5408\uff0c\u5b9e\u73b0\u8de8\u7269\u79cd\u3001\u89c6\u89d2\u548c\u4e0a\u4e0b\u6587\u7684\u4eba\u7c7b\u7ea7\u52a8\u4f5c\u8bc6\u522b\u6cdb\u5316\u80fd\u529b", "motivation": "\u4eba\u7c7b\u80fd\u591f\u8bc6\u522b\u4e0d\u540c\u7269\u79cd\u3001\u89c6\u89d2\u548c\u4e0a\u4e0b\u6587\u4e2d\u7684\u76f8\u540c\u52a8\u4f5c\uff0c\u4f46\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u6b64\u7c7b\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u56f0\u96be", "method": "\u5229\u7528\u89c6\u89c9\u6269\u6563\u6a21\u578b\u751f\u6210\u7279\u5f81\uff0c\u901a\u8fc7transformer\u8fdb\u884c\u805a\u5408\uff0c\u7279\u522b\u4f7f\u7528\u6269\u6563\u8fc7\u7a0b\u65e9\u671f\u65f6\u95f4\u6b65\u7684\u6761\u4ef6\u6a21\u578b\u6765\u7a81\u51fa\u8bed\u4e49\u4fe1\u606f\u800c\u975e\u50cf\u7d20\u7ec6\u8282", "result": "\u5728\u8de8\u7269\u79cd\u3001\u8de8\u89c6\u89d2\u548c\u8de8\u4e0a\u4e0b\u6587\u4e09\u4e2a\u6cdb\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73", "conclusion": "\u8be5\u65b9\u6cd5\u4f7f\u673a\u5668\u52a8\u4f5c\u8bc6\u522b\u66f4\u63a5\u8fd1\u4eba\u7c7b\u7ea7\u522b\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u8de8\u57df\u52a8\u4f5c\u8bc6\u522b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.08940", "pdf": "https://arxiv.org/pdf/2509.08940", "abs": "https://arxiv.org/abs/2509.08940", "authors": ["Lisa Dunlap", "Joseph E. Gonzalez", "Trevor Darrell", "Fabian Caba Heilbron", "Josef Sivic", "Bryan Russell"], "title": "Discovering Divergent Representations between Text-to-Image Models", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025. Code available at   https://github.com/adobe-research/CompCon", "summary": "In this paper, we investigate when and how visual representations learned by two different generative models diverge. Given two text-to-image models, our goal is to discover visual attributes that appear in images generated by one model but not the other, along with the types of prompts that trigger these attribute differences. For example, \"flames\" might appear in one model's outputs when given prompts expressing strong emotions, while the other model does not produce this attribute given the same prompts. We introduce CompCon (Comparing Concepts), an evolutionary search algorithm that discovers visual attributes more prevalent in one model's output than the other, and uncovers the prompt concepts linked to these visual differences. To evaluate CompCon's ability to find diverging representations, we create an automated data generation pipeline to produce ID2, a dataset of 60 input-dependent differences, and compare our approach to several LLM- and VLM-powered baselines. Finally, we use CompCon to compare popular text-to-image models, finding divergent representations such as how PixArt depicts prompts mentioning loneliness with wet streets and Stable Diffusion 3.5 depicts African American people in media professions. Code at: https://github.com/adobe-research/CompCon", "AI": {"tldr": "CompCon\u7b97\u6cd5\u7528\u4e8e\u6bd4\u8f83\u4e0d\u540c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u89c6\u89c9\u8868\u793a\u5dee\u5f02\uff0c\u901a\u8fc7\u8fdb\u5316\u641c\u7d22\u53d1\u73b0\u6a21\u578b\u95f4\u5dee\u5f02\u5316\u7684\u89c6\u89c9\u5c5e\u6027\u548c\u89e6\u53d1\u8fd9\u4e9b\u5dee\u5f02\u7684\u63d0\u793a\u6982\u5ff5", "motivation": "\u7814\u7a76\u4e0d\u540c\u751f\u6210\u6a21\u578b\u5728\u76f8\u540c\u6587\u672c\u63d0\u793a\u4e0b\u4ea7\u751f\u4e0d\u540c\u89c6\u89c9\u8868\u793a\u7684\u539f\u56e0\uff0c\u63a2\u7d22\u6a21\u578b\u95f4\u7684\u6982\u5ff5\u5dee\u5f02\u548c\u89e6\u53d1\u673a\u5236", "method": "\u63d0\u51faCompCon\u8fdb\u5316\u641c\u7d22\u7b97\u6cd5\uff0c\u81ea\u52a8\u53d1\u73b0\u4e00\u4e2a\u6a21\u578b\u6bd4\u53e6\u4e00\u4e2a\u6a21\u578b\u66f4\u5e38\u51fa\u73b0\u7684\u89c6\u89c9\u5c5e\u6027\uff0c\u5e76\u627e\u51fa\u4e0e\u8fd9\u4e9b\u89c6\u89c9\u5dee\u5f02\u76f8\u5173\u7684\u63d0\u793a\u6982\u5ff5\u3002\u5efa\u7acb\u4e86\u81ea\u52a8\u5316\u6570\u636e\u751f\u6210\u7ba1\u9053\u521b\u5efaID2\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1", "result": "\u6210\u529f\u6bd4\u8f83\u4e86\u6d41\u884c\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\uff08\u5982PixArt\u548cStable Diffusion 3.5\uff09\uff0c\u53d1\u73b0\u4e86\u6a21\u578b\u95f4\u7684\u8868\u793a\u5dee\u5f02\uff0c\u4f8b\u5982PixArt\u5728\u8868\u8fbe\u5b64\u72ec\u7684\u63d0\u793a\u4e2d\u503e\u5411\u4e8e\u63cf\u7ed8\u6e7f\u6f09\u8857\u9053\uff0c\u800cStable Diffusion 3.5\u5728\u63cf\u7ed8\u975e\u88d4\u7f8e\u56fd\u4eba\u65f6\u66f4\u503e\u5411\u4e8e\u5a92\u4f53\u804c\u4e1a\u5f62\u8c61", "conclusion": "CompCon\u80fd\u591f\u6709\u6548\u8bc6\u522b\u548c\u6bd4\u8f83\u4e0d\u540c\u751f\u6210\u6a21\u578b\u7684\u89c6\u89c9\u8868\u793a\u5dee\u5f02\uff0c\u4e3a\u7406\u89e3\u6a21\u578b\u95f4\u7684\u6982\u5ff5\u504f\u5dee\u548c\u751f\u6210\u7279\u6027\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177"}}
{"id": "2509.09054", "pdf": "https://arxiv.org/pdf/2509.09054", "abs": "https://arxiv.org/abs/2509.09054", "authors": ["Binxu Li", "Wei Peng", "Mingjie Li", "Ehsan Adeli", "Kilian M. Pohl"], "title": "Integrating Anatomical Priors into a Causal Diffusion Model", "categories": ["cs.CV"], "comment": "15 pages, 4 figures", "summary": "3D brain MRI studies often examine subtle morphometric differences between cohorts that are hard to detect visually. Given the high cost of MRI acquisition, these studies could greatly benefit from image syntheses, particularly counterfactual image generation, as seen in other domains, such as computer vision. However, counterfactual models struggle to produce anatomically plausible MRIs due to the lack of explicit inductive biases to preserve fine-grained anatomical details. This shortcoming arises from the training of the models aiming to optimize for the overall appearance of the images (e.g., via cross-entropy) rather than preserving subtle, yet medically relevant, local variations across subjects. To preserve subtle variations, we propose to explicitly integrate anatomical constraints on a voxel-level as prior into a generative diffusion framework. Called Probabilistic Causal Graph Model (PCGM), the approach captures anatomical constraints via a probabilistic graph module and translates those constraints into spatial binary masks of regions where subtle variations occur. The masks (encoded by a 3D extension of ControlNet) constrain a novel counterfactual denoising UNet, whose encodings are then transferred into high-quality brain MRIs via our 3D diffusion decoder. Extensive experiments on multiple datasets demonstrate that PCGM generates structural brain MRIs of higher quality than several baseline approaches. Furthermore, we show for the first time that brain measurements extracted from counterfactuals (generated by PCGM) replicate the subtle effects of a disease on cortical brain regions previously reported in the neuroscience literature. This achievement is an important milestone in the use of synthetic MRIs in studies investigating subtle morphological differences.", "AI": {"tldr": "PCGM\u662f\u4e00\u79cd\u65b0\u76843D\u8111MRI\u53cd\u4e8b\u5b9e\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6982\u7387\u56e0\u679c\u56fe\u6a21\u578b\u6574\u5408\u89e3\u5256\u7ea6\u675f\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u89e3\u5256\u5b66\u5408\u7406\u7684\u8111\u90e8MRI\u56fe\u50cf\uff0c\u80fd\u591f\u4fdd\u7559\u7ec6\u5fae\u7684\u533b\u5b66\u76f8\u5173\u5c40\u90e8\u53d8\u5316\u3002", "motivation": "\u73b0\u6709\u53cd\u4e8b\u5b9e\u6a21\u578b\u96be\u4ee5\u751f\u6210\u89e3\u5256\u5b66\u5408\u7406\u7684\u8111MRI\u56fe\u50cf\uff0c\u56e0\u4e3a\u5b83\u4eec\u7f3a\u4e4f\u660e\u786e\u7684\u5f52\u7eb3\u504f\u7f6e\u6765\u4fdd\u7559\u7ec6\u7c92\u5ea6\u7684\u89e3\u5256\u7ec6\u8282\uff0c\u800c\u4f18\u5316\u6574\u4f53\u5916\u89c2\u7684\u8bad\u7ec3\u76ee\u6807\u65e0\u6cd5\u4fdd\u7559\u533b\u5b66\u76f8\u5173\u7684\u5c40\u90e8\u7ec6\u5fae\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u6982\u7387\u56e0\u679c\u56fe\u6a21\u578b(PCGM)\uff0c\u5728\u751f\u6210\u6269\u6563\u6846\u67b6\u4e2d\u663e\u5f0f\u6574\u5408\u4f53\u7d20\u7ea7\u89e3\u5256\u7ea6\u675f\u4f5c\u4e3a\u5148\u9a8c\u3002\u901a\u8fc7\u6982\u7387\u56fe\u6a21\u5757\u6355\u83b7\u89e3\u5256\u7ea6\u675f\u5e76\u8f6c\u6362\u4e3a\u7a7a\u95f4\u4e8c\u8fdb\u5236\u63a9\u7801\uff0c\u4f7f\u75283D ControlNet\u7f16\u7801\u63a9\u7801\uff0c\u7ea6\u675f\u65b0\u578b\u53cd\u4e8b\u5b9e\u53bb\u566aUNet\uff0c\u6700\u540e\u901a\u8fc73D\u6269\u6563\u89e3\u7801\u5668\u751f\u6210\u9ad8\u8d28\u91cf\u8111MRI\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPCGM\u751f\u6210\u7684\u8111MRI\u8d28\u91cf\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u3002\u9996\u6b21\u8bc1\u660e\u4ecePCGM\u751f\u6210\u7684\u53cd\u4e8b\u5b9e\u56fe\u50cf\u4e2d\u63d0\u53d6\u7684\u8111\u6d4b\u91cf\u7ed3\u679c\u80fd\u591f\u590d\u73b0\u795e\u7ecf\u79d1\u5b66\u6587\u732e\u4e2d\u62a5\u544a\u7684\u75be\u75c5\u5bf9\u76ae\u5c42\u8111\u533a\u7684\u7ec6\u5fae\u5f71\u54cd\u3002", "conclusion": "PCGM\u5728\u5408\u6210MRI\u7528\u4e8e\u7814\u7a76\u7ec6\u5fae\u5f62\u6001\u5dee\u5f02\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u8981\u91cc\u7a0b\u7891\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u89e3\u5256\u5b66\u5408\u7406\u7684\u8111\u90e8MRI\u56fe\u50cf\uff0c\u6210\u529f\u4fdd\u7559\u4e86\u75be\u75c5\u76f8\u5173\u7684\u7ec6\u5fae\u5f62\u6001\u53d8\u5316\u3002"}}
{"id": "2509.09130", "pdf": "https://arxiv.org/pdf/2509.09130", "abs": "https://arxiv.org/abs/2509.09130", "authors": ["Bin Huang", "Kang Chen", "Bingxuan Li", "Huafeng Liu", "Qiegen Liu"], "title": "ALL-PET: A Low-resource and Low-shot PET Foundation Model in the Projection Domain", "categories": ["cs.CV"], "comment": null, "summary": "Building large-scale foundation model for PET imaging is hindered by limited access to labeled data and insufficient computational resources. To overcome data scarcity and efficiency limitations, we propose ALL-PET, a low-resource, low-shot PET foundation model operating directly in the projection domain. ALL-PET leverages a latent diffusion model (LDM) with three key innovations. First, we design a Radon mask augmentation strategy (RMAS) that generates over 200,000 structurally diverse training samples by projecting randomized image-domain masks into sinogram space, significantly improving generalization with minimal data. This is extended by a dynamic multi-mask (DMM) mechanism that varies mask quantity and distribution, enhancing data diversity without added model complexity. Second, we implement positive/negative mask constraints to embed strict geometric consistency, reducing parameter burden while preserving generation quality. Third, we introduce transparent medical attention (TMA), a parameter-free, geometry-driven mechanism that enhances lesion-related regions in raw projection data. Lesion-focused attention maps are derived from coarse segmentation, covering both hypermetabolic and hypometabolic areas, and projected into sinogram space for physically consistent guidance. The system supports clinician-defined ROI adjustments, ensuring flexible, interpretable, and task-adaptive emphasis aligned with PET acquisition physics. Experimental results show ALL-PET achieves high-quality sinogram generation using only 500 samples, with performance comparable to models trained on larger datasets. ALL-PET generalizes across tasks including low-dose reconstruction, attenuation correction, delayed-frame prediction, and tracer separation, operating efficiently with memory use under 24GB.", "AI": {"tldr": "ALL-PET\u662f\u4e00\u4e2a\u4f4e\u8d44\u6e90\u3001\u4f4e\u6837\u672c\u7684PET\u6295\u5f71\u57df\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u4e09\u9879\u5173\u952e\u521b\u65b0\u6280\u672f\uff0c\u4ec5\u7528500\u4e2a\u6837\u672c\u5c31\u80fd\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u6295\u5f71\u6570\u636e\u751f\u6210\uff0c\u5e76\u5728\u591a\u79cdPET\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u8272\u6027\u80fd\u3002", "motivation": "PET\u6210\u50cf\u9886\u57df\u6784\u5efa\u5927\u89c4\u6a21\u57fa\u7840\u6a21\u578b\u9762\u4e34\u6807\u6ce8\u6570\u636e\u6709\u9650\u548c\u8ba1\u7b97\u8d44\u6e90\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u6548\u7387\u9650\u5236\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b(LDM)\uff0c\u5305\u542b\u4e09\u9879\u521b\u65b0\uff1a1)Radon\u63a9\u7801\u589e\u5f3a\u7b56\u7565(RMAS)\u751f\u621020\u4e07+\u591a\u6837\u5316\u8bad\u7ec3\u6837\u672c\uff1b2)\u6b63\u8d1f\u63a9\u7801\u7ea6\u675f\u5d4c\u5165\u51e0\u4f55\u4e00\u81f4\u6027\uff1b3)\u900f\u660e\u533b\u5b66\u6ce8\u610f\u529b(TMA)\u673a\u5236\u589e\u5f3a\u75c5\u7076\u76f8\u5173\u533a\u57df\u3002", "result": "\u4ec5\u4f7f\u7528500\u4e2a\u6837\u672c\u5c31\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u6295\u5f71\u6570\u636e\u751f\u6210\uff0c\u6027\u80fd\u53ef\u4e0e\u5927\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u5185\u5b58\u4f7f\u7528\u4f4e\u4e8e24GB\uff0c\u5728\u4f4e\u5242\u91cf\u91cd\u5efa\u3001\u8870\u51cf\u6821\u6b63\u3001\u5ef6\u8fdf\u5e27\u9884\u6d4b\u548c\u793a\u8e2a\u5242\u5206\u79bb\u7b49\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u826f\u597d\u3002", "conclusion": "ALL-PET\u6210\u529f\u89e3\u51b3\u4e86PET\u6210\u50cf\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u7075\u6d3b\u4e14\u53ef\u89e3\u91ca\u7684\u57fa\u7840\u6a21\u578b\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u591a\u79cd\u4e34\u5e8a\u5e94\u7528\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.09157", "pdf": "https://arxiv.org/pdf/2509.09157", "abs": "https://arxiv.org/abs/2509.09157", "authors": ["Yuan Shufang"], "title": "RT-DETR++ for UAV Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Object detection in unmanned aerial vehicle (UAV) imagery presents significant challenges. Issues such as densely packed small objects, scale variations, and occlusion are commonplace. This paper introduces RT-DETR++, which enhances the encoder component of the RT-DETR model. Our improvements focus on two key aspects. First, we introduce a channel-gated attention-based upsampling/downsampling (AU/AD) mechanism. This dual-path system minimizes errors and preserves details during feature layer propagation. Second, we incorporate CSP-PAC during feature fusion. This technique employs parallel hollow convolutions to process local and contextual information within the same layer, facilitating the integration of multi-scale features. Evaluation demonstrates that our novel neck design achieves superior performance in detecting small and densely packed objects. The model maintains sufficient speed for real-time detection without increasing computational complexity. This study provides an effective approach for feature encoding design in real-time detection systems.", "AI": {"tldr": "RT-DETR++\u901a\u8fc7\u6539\u8fdbRT-DETR\u6a21\u578b\u7684\u7f16\u7801\u5668\uff0c\u5f15\u5165\u901a\u9053\u95e8\u63a7\u6ce8\u610f\u529b\u4e0a\u91c7\u6837/\u4e0b\u91c7\u6837\u673a\u5236\u548cCSP-PAC\u7279\u5f81\u878d\u5408\u6280\u672f\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u68c0\u6d4b\u901f\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u56fe\u50cf\u4e2d\u5c0f\u76ee\u6807\u548c\u5bc6\u96c6\u76ee\u6807\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u65e0\u4eba\u673a\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u9762\u4e34\u5c0f\u76ee\u6807\u5bc6\u96c6\u3001\u5c3a\u5ea6\u53d8\u5316\u5927\u3001\u906e\u6321\u4e25\u91cd\u7b49\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u4fdd\u6301\u5b9e\u65f6\u6027\u53c8\u80fd\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u5f15\u5165\u57fa\u4e8e\u901a\u9053\u95e8\u63a7\u6ce8\u610f\u529b\u7684\u4e0a\u91c7\u6837/\u4e0b\u91c7\u6837(AU/AD)\u53cc\u8def\u5f84\u673a\u5236\uff0c\u51cf\u5c11\u7279\u5f81\u5c42\u4f20\u64ad\u8bef\u5dee\u5e76\u4fdd\u7559\u7ec6\u8282\uff1b2. \u5728\u7279\u5f81\u878d\u5408\u4e2d\u91c7\u7528CSP-PAC\u6280\u672f\uff0c\u4f7f\u7528\u5e76\u884c\u7a7a\u6d1e\u5377\u79ef\u5728\u540c\u4e00\u5c42\u5904\u7406\u5c40\u90e8\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4fc3\u8fdb\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u3002", "result": "\u65b0\u578bneck\u8bbe\u8ba1\u5728\u5c0f\u76ee\u6807\u548c\u5bc6\u96c6\u76ee\u6807\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u6a21\u578b\u5728\u4fdd\u6301\u5b9e\u65f6\u68c0\u6d4b\u901f\u5ea6\u7684\u540c\u65f6\u4e0d\u589e\u52a0\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5b9e\u65f6\u68c0\u6d4b\u7cfb\u7edf\u4e2d\u7684\u7279\u5f81\u7f16\u7801\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u56fe\u50cf\u76ee\u6807\u68c0\u6d4b\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2509.09172", "pdf": "https://arxiv.org/pdf/2509.09172", "abs": "https://arxiv.org/abs/2509.09172", "authors": ["Chunxiao Li", "Xiaoxiao Wang", "Meiling Li", "Boming Miao", "Peng Sun", "Yunjian Zhang", "Xiangyang Ji", "Yao Zhu"], "title": "Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking AI-Generated Image Detection in Challenging Scenarios", "categories": ["cs.CV"], "comment": "ICCV2025", "summary": "With the rapid advancement of generative models, highly realistic image synthesis has posed new challenges to digital security and media credibility. Although AI-generated image detection methods have partially addressed these concerns, a substantial research gap remains in evaluating their performance under complex real-world conditions. This paper introduces the Real-World Robustness Dataset (RRDataset) for comprehensive evaluation of detection models across three dimensions: 1) Scenario Generalization: RRDataset encompasses high-quality images from seven major scenarios (War and Conflict, Disasters and Accidents, Political and Social Events, Medical and Public Health, Culture and Religion, Labor and Production, and everyday life), addressing existing dataset gaps from a content perspective. 2) Internet Transmission Robustness: examining detector performance on images that have undergone multiple rounds of sharing across various social media platforms. 3) Re-digitization Robustness: assessing model effectiveness on images altered through four distinct re-digitization methods. We benchmarked 17 detectors and 10 vision-language models (VLMs) on RRDataset and conducted a large-scale human study involving 192 participants to investigate human few-shot learning capabilities in detecting AI-generated images. The benchmarking results reveal the limitations of current AI detection methods under real-world conditions and underscore the importance of drawing on human adaptability to develop more robust detection algorithms.", "AI": {"tldr": "RRDataset\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u590d\u6742\u6761\u4ef6\u4e0b\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u5305\u542b\u573a\u666f\u6cdb\u5316\u3001\u7f51\u7edc\u4f20\u8f93\u9c81\u68d2\u6027\u548c\u91cd\u6570\u5b57\u5316\u9c81\u68d2\u6027\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u5f53\u524d\u68c0\u6d4b\u65b9\u6cd5\u7684\u5c40\u9650\u6027", "motivation": "\u968f\u7740\u751f\u6210\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9ad8\u5ea6\u903c\u771f\u7684\u56fe\u50cf\u5408\u6210\u5bf9\u6570\u5b57\u5b89\u5168\u548c\u5a92\u4f53\u53ef\u4fe1\u5ea6\u63d0\u51fa\u4e86\u65b0\u6311\u6218\u3002\u73b0\u6709AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u8bc4\u4f30\u5b58\u5728\u7814\u7a76\u7a7a\u767d", "method": "\u6784\u5efa\u5305\u542b7\u4e2a\u4e3b\u8981\u573a\u666f\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684RRDataset\uff0c\u8bc4\u4f3017\u4e2a\u68c0\u6d4b\u5668\u548c10\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u8fdb\u884c192\u540d\u53c2\u4e0e\u8005\u7684\u5927\u89c4\u6a21\u4eba\u7c7b\u7814\u7a76\uff0c\u63a2\u7d22\u4eba\u7c7b\u5728\u68c0\u6d4bAI\u751f\u6210\u56fe\u50cf\u65b9\u9762\u7684\u5c11\u6837\u672c\u5b66\u4e60\u80fd\u529b", "result": "\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u63ed\u793a\u4e86\u5f53\u524dAI\u68c0\u6d4b\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u501f\u9274\u4eba\u7c7b\u9002\u5e94\u6027\u5f00\u53d1\u66f4\u9c81\u68d2\u68c0\u6d4b\u7b97\u6cd5\u7684\u91cd\u8981\u6027", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u5728\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u65b9\u9762\u7684\u7a7a\u767d\uff0cRRDataset\u4e3a\u5168\u9762\u8bc4\u4f30\u68c0\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u4eba\u7c7b\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u68c0\u6d4b\u7b97\u6cd5\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2509.09183", "pdf": "https://arxiv.org/pdf/2509.09183", "abs": "https://arxiv.org/abs/2509.09183", "authors": ["Jiasheng Guo", "Xin Gao", "Yuxiang Yan", "Guanghao Li", "Jian Pu"], "title": "Dark-ISP: Enhancing RAW Image Processing for Low-Light Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 6 figures, conference", "summary": "Low-light Object detection is crucial for many real-world applications but remains challenging due to degraded image quality. While recent studies have shown that RAW images offer superior potential over RGB images, existing approaches either use RAW-RGB images with information loss or employ complex frameworks. To address these, we propose a lightweight and self-adaptive Image Signal Processing (ISP) plugin, Dark-ISP, which directly processes Bayer RAW images in dark environments, enabling seamless end-to-end training for object detection. Our key innovations are: (1) We deconstruct conventional ISP pipelines into sequential linear (sensor calibration) and nonlinear (tone mapping) sub-modules, recasting them as differentiable components optimized through task-driven losses. Each module is equipped with content-aware adaptability and physics-informed priors, enabling automatic RAW-to-RGB conversion aligned with detection objectives. (2) By exploiting the ISP pipeline's intrinsic cascade structure, we devise a Self-Boost mechanism that facilitates cooperation between sub-modules. Through extensive experiments on three RAW image datasets, we demonstrate that our method outperforms state-of-the-art RGB- and RAW-based detection approaches, achieving superior results with minimal parameters in challenging low-light environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86Dark-ISP\u8f7b\u91cf\u7ea7\u81ea\u9002\u5e94ISP\u63d2\u4ef6\uff0c\u76f4\u63a5\u5728\u6697\u73af\u5883\u4e0b\u5904\u7406Bayer RAW\u56fe\u50cf\uff0c\u901a\u8fc7\u89e3\u6784\u4f20\u7edfISP\u6d41\u6c34\u7ebf\u4e3a\u53ef\u5fae\u5206\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u4f4e\u5149\u76ee\u6807\u68c0\u6d4b\u4f18\u5316\u3002", "motivation": "\u4f4e\u5149\u76ee\u6807\u68c0\u6d4b\u56e0\u56fe\u50cf\u8d28\u91cf\u9000\u5316\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f7f\u7528\u6709\u4fe1\u606f\u635f\u5931\u7684RAW-RGB\u56fe\u50cf\uff0c\u8981\u4e48\u91c7\u7528\u590d\u6742\u6846\u67b6\uff0c\u9700\u8981\u66f4\u8f7b\u91cf\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u4f20\u7edfISP\u6d41\u6c34\u7ebf\u5206\u89e3\u4e3a\u987a\u5e8f\u7ebf\u6027\uff08\u4f20\u611f\u5668\u6821\u51c6\uff09\u548c\u975e\u7ebf\u6027\uff08\u8272\u8c03\u6620\u5c04\uff09\u5b50\u6a21\u5757\uff0c\u4f5c\u4e3a\u53ef\u5fae\u5206\u7ec4\u4ef6\u901a\u8fc7\u4efb\u52a1\u9a71\u52a8\u635f\u5931\u4f18\u5316\uff1b\u6bcf\u4e2a\u6a21\u5757\u5177\u6709\u5185\u5bb9\u611f\u77e5\u9002\u5e94\u6027\u548c\u7269\u7406\u5148\u9a8c\uff1b\u5229\u7528ISP\u7ea7\u8054\u7ed3\u6784\u8bbe\u8ba1\u81ea\u589e\u5f3a\u673a\u5236\u4fc3\u8fdb\u5b50\u6a21\u5757\u534f\u4f5c\u3002", "result": "\u5728\u4e09\u4e2aRAW\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u5149\u73af\u5883\u4e0b\u4ee5\u6700\u5c11\u7684\u53c2\u6570\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684RGB\u548cRAW\u68c0\u6d4b\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u4f18\u8d8a\u7ed3\u679c\u3002", "conclusion": "Dark-ISP\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u81ea\u9002\u5e94\u7684\u7aef\u5230\u7aef\u4f4e\u5149\u76ee\u6807\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u4f18\u5316ISP\u6d41\u6c34\u7ebf\u5b9e\u73b0\u66f4\u597d\u7684\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2509.09365", "pdf": "https://arxiv.org/pdf/2509.09365", "abs": "https://arxiv.org/abs/2509.09365", "authors": ["Xiaodong Wang", "Ping Wang", "Zhangyuan Li", "Xin Yuan"], "title": "Plug-and-play Diffusion Models for Image Compressive Sensing with Data Consistency Projection", "categories": ["cs.CV"], "comment": null, "summary": "We explore the connection between Plug-and-Play (PnP) methods and Denoising Diffusion Implicit Models (DDIM) for solving ill-posed inverse problems, with a focus on single-pixel imaging. We begin by identifying key distinctions between PnP and diffusion models-particularly in their denoising mechanisms and sampling procedures. By decoupling the diffusion process into three interpretable stages: denoising, data consistency enforcement, and sampling, we provide a unified framework that integrates learned priors with physical forward models in a principled manner. Building upon this insight, we propose a hybrid data-consistency module that linearly combines multiple PnP-style fidelity terms. This hybrid correction is applied directly to the denoised estimate, improving measurement consistency without disrupting the diffusion sampling trajectory. Experimental results on single-pixel imaging tasks demonstrate that our method achieves better reconstruction quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06PnP\u65b9\u6cd5\u4e0eDDIM\u6269\u6563\u6a21\u578b\u7ed3\u5408\u7528\u4e8e\u89e3\u51b3\u5355\u50cf\u7d20\u6210\u50cf\u7b49\u75c5\u6001\u9006\u95ee\u9898\uff0c\u901a\u8fc7\u89e3\u8026\u6269\u6563\u8fc7\u7a0b\u5e76\u5f15\u5165\u6df7\u5408\u6570\u636e\u4e00\u81f4\u6027\u6a21\u5757\u6765\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u63a2\u7d22PnP\u65b9\u6cd5\u4e0eDDIM\u6269\u6563\u6a21\u578b\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u7279\u522b\u662f\u5728\u89e3\u51b3\u75c5\u6001\u9006\u95ee\u9898\u65b9\u9762\u7684\u5e94\u7528\uff0c\u65e8\u5728\u6574\u5408\u5b66\u4e60\u5148\u9a8c\u4e0e\u7269\u7406\u524d\u5411\u6a21\u578b\u3002", "method": "\u5c06\u6269\u6563\u8fc7\u7a0b\u89e3\u8026\u4e3a\u4e09\u4e2a\u53ef\u89e3\u91ca\u9636\u6bb5\uff1a\u53bb\u566a\u3001\u6570\u636e\u4e00\u81f4\u6027\u5f3a\u5236\u548c\u91c7\u6837\uff1b\u63d0\u51fa\u6df7\u5408\u6570\u636e\u4e00\u81f4\u6027\u6a21\u5757\uff0c\u7ebf\u6027\u7ec4\u5408\u591a\u4e2aPnP\u5f0f\u4fdd\u771f\u5ea6\u9879\uff0c\u76f4\u63a5\u5728\u53bb\u566a\u4f30\u8ba1\u4e0a\u5e94\u7528\u6df7\u5408\u6821\u6b63\u3002", "result": "\u5728\u5355\u50cf\u7d20\u6210\u50cf\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u83b7\u5f97\u4e86\u66f4\u597d\u7684\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u6210\u529f\u6574\u5408\u4e86\u5b66\u4e60\u5148\u9a8c\u4e0e\u7269\u7406\u6a21\u578b\uff0c\u6df7\u5408\u6570\u636e\u4e00\u81f4\u6027\u6a21\u5757\u5728\u4e0d\u7834\u574f\u6269\u6563\u91c7\u6837\u8f68\u8ff9\u7684\u60c5\u51b5\u4e0b\u6539\u5584\u4e86\u6d4b\u91cf\u4e00\u81f4\u6027\u3002"}}
{"id": "2509.09397", "pdf": "https://arxiv.org/pdf/2509.09397", "abs": "https://arxiv.org/abs/2509.09397", "authors": ["Umaima Rahman", "Raza Imam", "Mohammad Yaqub", "Dwarikanath Mahapatra"], "title": "Decoupling Clinical and Class-Agnostic Features for Reliable Few-Shot Adaptation under Shift", "categories": ["cs.CV"], "comment": null, "summary": "Medical vision-language models (VLMs) offer promise for clinical decision support, yet their reliability under distribution shifts remains a major concern for safe deployment. These models often learn task-agnostic correlations due to variability in imaging protocols and free-text reports, limiting their generalizability and increasing the risk of failure in real-world settings. We propose DRiFt, a structured feature decoupling framework that explicitly separates clinically relevant signals from task-agnostic noise using parameter-efficient tuning (LoRA) and learnable prompt tokens. To enhance cross-modal alignment and reduce uncertainty, we curate high-quality, clinically grounded image-text pairs by generating captions for a diverse medical dataset. Our approach improves in-distribution performance by +11.4% Top-1 accuracy and +3.3% Macro-F1 over prior prompt-based methods, while maintaining strong robustness across unseen datasets. Ablation studies reveal that disentangling task-relevant features and careful alignment significantly enhance model generalization and reduce unpredictable behavior under domain shift. These insights contribute toward building safer, more trustworthy VLMs for clinical use. The code is available at https://github.com/rumaima/DRiFt.", "AI": {"tldr": "DRiFt\u662f\u4e00\u4e2a\u533b\u7597\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7279\u5f81\u89e3\u8026\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u4e34\u5e8a\u76f8\u5173\u4fe1\u53f7\u548c\u4efb\u52a1\u65e0\u5173\u566a\u58f0\uff0c\u63d0\u5347\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u533b\u7597\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u53ef\u9760\u6027\u4e0d\u8db3\uff0c\u5bb9\u6613\u5b66\u4e60\u4efb\u52a1\u65e0\u5173\u7684\u76f8\u5173\u6027\uff0c\u9650\u5236\u4e86\u5728\u771f\u5b9e\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u3002", "method": "\u4f7f\u7528\u53c2\u6570\u9ad8\u6548\u8c03\u4f18(LoRA)\u548c\u53ef\u5b66\u4e60\u63d0\u793a\u4ee4\u724c\uff0c\u663e\u5f0f\u5206\u79bb\u4e34\u5e8a\u76f8\u5173\u4fe1\u53f7\u548c\u4efb\u52a1\u65e0\u5173\u566a\u58f0\uff1b\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u4e34\u5e8a\u57fa\u7840\u56fe\u50cf-\u6587\u672c\u5bf9\u6765\u589e\u5f3a\u8de8\u6a21\u6001\u5bf9\u9f50\u3002", "result": "\u5728\u5206\u5e03\u5185\u6027\u80fd\u63d0\u534711.4% Top-1\u51c6\u786e\u7387\u548c3.3% Macro-F1\uff0c\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u7279\u5f81\u89e3\u8026\u548c\u7cbe\u5fc3\u5bf9\u9f50\u663e\u8457\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u51cf\u5c11\u9886\u57df\u504f\u79fb\u4e0b\u7684\u4e0d\u53ef\u9884\u6d4b\u884c\u4e3a\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u5b89\u5168\u53ef\u4fe1\u7684\u4e34\u5e8aVLM\u3002"}}
{"id": "2509.09427", "pdf": "https://arxiv.org/pdf/2509.09427", "abs": "https://arxiv.org/abs/2509.09427", "authors": ["Yuchan Jie", "Yushen Xu", "Xiaosong Li", "Fuqiang Zhou", "Jianming Lv", "Huafeng Li"], "title": "FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution", "categories": ["cs.CV"], "comment": null, "summary": "As an influential information fusion and low-level vision technique, image fusion integrates complementary information from source images to yield an informative fused image. A few attempts have been made in recent years to jointly realize image fusion and super-resolution. However, in real-world applications such as military reconnaissance and long-range detection missions, the target and background structures in multimodal images are easily corrupted, with low resolution and weak semantic information, which leads to suboptimal results in current fusion techniques. In response, we propose FS-Diff, a semantic guidance and clarity-aware joint image fusion and super-resolution method. FS-Diff unifies image fusion and super-resolution as a conditional generation problem. It leverages semantic guidance from the proposed clarity sensing mechanism for adaptive low-resolution perception and cross-modal feature extraction. Specifically, we initialize the desired fused result as pure Gaussian noise and introduce the bidirectional feature Mamba to extract the global features of the multimodal images. Moreover, utilizing the source images and semantics as conditions, we implement a random iterative denoising process via a modified U-Net network. This network istrained for denoising at multiple noise levels to produce high-resolution fusion results with cross-modal features and abundant semantic information. We also construct a powerful aerial view multiscene (AVMS) benchmark covering 600 pairs of images. Extensive joint image fusion and super-resolution experiments on six public and our AVMS datasets demonstrated that FS-Diff outperforms the state-of-the-art methods at multiple magnifications and can recover richer details and semantics in the fused images. The code is available at https://github.com/XylonXu01/FS-Diff.", "AI": {"tldr": "FS-Diff\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8054\u5408\u56fe\u50cf\u878d\u5408\u548c\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u5f15\u5bfc\u548c\u6e05\u6670\u5ea6\u611f\u77e5\u673a\u5236\uff0c\u5728\u4f4e\u5206\u8fa8\u7387\u611f\u77e5\u548c\u8de8\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u519b\u4e8b\u4fa6\u5bdf\u548c\u8fdc\u7a0b\u68c0\u6d4b\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u591a\u6a21\u6001\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u548c\u80cc\u666f\u7ed3\u6784\u5bb9\u6613\u635f\u574f\uff0c\u5206\u8fa8\u7387\u4f4e\u4e14\u8bed\u4e49\u4fe1\u606f\u5f31\uff0c\u5bfc\u81f4\u73b0\u6709\u878d\u5408\u6280\u672f\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u540c\u65f6\u89e3\u51b3\u56fe\u50cf\u878d\u5408\u548c\u8d85\u5206\u8fa8\u7387\u95ee\u9898\u3002", "method": "\u5c06\u56fe\u50cf\u878d\u5408\u548c\u8d85\u5206\u8fa8\u7387\u7edf\u4e00\u4e3a\u6761\u4ef6\u751f\u6210\u95ee\u9898\uff0c\u5229\u7528\u6e05\u6670\u5ea6\u611f\u77e5\u673a\u5236\u63d0\u4f9b\u8bed\u4e49\u5f15\u5bfc\uff0c\u91c7\u7528\u53cc\u5411\u7279\u5f81Mamba\u63d0\u53d6\u5168\u5c40\u7279\u5f81\uff0c\u901a\u8fc7\u6539\u8fdb\u7684U-Net\u7f51\u7edc\u5b9e\u73b0\u968f\u673a\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\uff0c\u5728\u591a\u4e2a\u566a\u58f0\u7ea7\u522b\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u516d\u4e2a\u516c\u5171\u6570\u636e\u96c6\u548c\u81ea\u5efa\u7684AVMS\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFS-Diff\u5728\u591a\u4e2a\u653e\u5927\u500d\u6570\u4e0b\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u80fd\u591f\u6062\u590d\u66f4\u4e30\u5bcc\u7684\u7ec6\u8282\u548c\u8bed\u4e49\u4fe1\u606f\u3002", "conclusion": "FS-Diff\u901a\u8fc7\u8bed\u4e49\u5f15\u5bfc\u548c\u6e05\u6670\u5ea6\u611f\u77e5\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u56fe\u50cf\u878d\u5408\u548c\u8d85\u5206\u8fa8\u7387\u7684\u8054\u5408\u95ee\u9898\uff0c\u5728\u771f\u5b9e\u573a\u666f\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2509.09456", "pdf": "https://arxiv.org/pdf/2509.09456", "abs": "https://arxiv.org/abs/2509.09456", "authors": ["Yushen Xu", "Xiaosong Li", "Yuchun Wang", "Xiaoqi Cheng", "Huafeng Li", "Haishu Tan"], "title": "FlexiD-Fuse: Flexible number of inputs multi-modal medical image fusion based on diffusion model", "categories": ["cs.CV"], "comment": null, "summary": "Different modalities of medical images provide unique physiological and anatomical information for diseases. Multi-modal medical image fusion integrates useful information from different complementary medical images with different modalities, producing a fused image that comprehensively and objectively reflects lesion characteristics to assist doctors in clinical diagnosis. However, existing fusion methods can only handle a fixed number of modality inputs, such as accepting only two-modal or tri-modal inputs, and cannot directly process varying input quantities, which hinders their application in clinical settings. To tackle this issue, we introduce FlexiD-Fuse, a diffusion-based image fusion network designed to accommodate flexible quantities of input modalities. It can end-to-end process two-modal and tri-modal medical image fusion under the same weight. FlexiD-Fuse transforms the diffusion fusion problem, which supports only fixed-condition inputs, into a maximum likelihood estimation problem based on the diffusion process and hierarchical Bayesian modeling. By incorporating the Expectation-Maximization algorithm into the diffusion sampling iteration process, FlexiD-Fuse can generate high-quality fused images with cross-modal information from source images, independently of the number of input images. We compared the latest two and tri-modal medical image fusion methods, tested them on Harvard datasets, and evaluated them using nine popular metrics. The experimental results show that our method achieves the best performance in medical image fusion with varying inputs. Meanwhile, we conducted extensive extension experiments on infrared-visible, multi-exposure, and multi-focus image fusion tasks with arbitrary numbers, and compared them with the perspective SOTA methods. The results of the extension experiments consistently demonstrate the effectiveness and superiority of our method.", "AI": {"tldr": "FlexiD-Fuse\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u533b\u5b66\u56fe\u50cf\u878d\u5408\u7f51\u7edc\uff0c\u80fd\u591f\u5904\u7406\u4efb\u610f\u6570\u91cf\u7684\u8f93\u5165\u6a21\u6001\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u53ea\u80fd\u5904\u7406\u56fa\u5b9a\u6570\u91cf\u6a21\u6001\u8f93\u5165\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u878d\u5408\u65b9\u6cd5\u53ea\u80fd\u5904\u7406\u56fa\u5b9a\u6570\u91cf\u7684\u6a21\u6001\u8f93\u5165\uff08\u5982\u53cc\u6a21\u6001\u6216\u4e09\u6a21\u6001\uff09\uff0c\u65e0\u6cd5\u76f4\u63a5\u5904\u7406\u53d8\u5316\u6570\u91cf\u7684\u8f93\u5165\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faFlexiD-Fuse\u6269\u6563\u878d\u5408\u7f51\u7edc\uff0c\u5c06\u6269\u6563\u878d\u5408\u95ee\u9898\u8f6c\u5316\u4e3a\u57fa\u4e8e\u6269\u6563\u8fc7\u7a0b\u548c\u5206\u5c42\u8d1d\u53f6\u65af\u5efa\u6a21\u7684\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u671f\u671b\u6700\u5927\u5316\u7b97\u6cd5\u878d\u5165\u6269\u6563\u91c7\u6837\u8fed\u4ee3\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4efb\u610f\u6570\u91cf\u8f93\u5165\u6a21\u6001\u7684\u9ad8\u8d28\u91cf\u878d\u5408\u3002", "result": "\u5728\u54c8\u4f5b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u53ef\u53d8\u8f93\u5165\u6570\u91cf\u7684\u533b\u5b66\u56fe\u50cf\u878d\u5408\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u540c\u65f6\u5728\u7ea2\u5916-\u53ef\u89c1\u5149\u3001\u591a\u66dd\u5149\u548c\u591a\u7126\u70b9\u56fe\u50cf\u878d\u5408\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "conclusion": "FlexiD-Fuse\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u878d\u5408\u4e2d\u53ef\u53d8\u8f93\u5165\u6570\u91cf\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u878d\u5408\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u5177\u6709\u5f88\u597d\u7684\u4e34\u5e8a\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.09547", "pdf": "https://arxiv.org/pdf/2509.09547", "abs": "https://arxiv.org/abs/2509.09547", "authors": ["Dohun Lee", "Hyeonho Jeong", "Jiwook Kim", "Duygu Ceylan", "Jong Chul Ye"], "title": "Improving Video Diffusion Transformer Training by Multi-Feature Fusion and Alignment from Self-Supervised Vision Encoders", "categories": ["cs.CV", "cs.AI"], "comment": "17 pages, 14 figures", "summary": "Video diffusion models have advanced rapidly in the recent years as a result of series of architectural innovations (e.g., diffusion transformers) and use of novel training objectives (e.g., flow matching). In contrast, less attention has been paid to improving the feature representation power of such models. In this work, we show that training video diffusion models can benefit from aligning the intermediate features of the video generator with feature representations of pre-trained vision encoders. We propose a new metric and conduct an in-depth analysis of various vision encoders to evaluate their discriminability and temporal consistency, thereby assessing their suitability for video feature alignment. Based on the analysis, we present Align4Gen which provides a novel multi-feature fusion and alignment method integrated into video diffusion model training. We evaluate Align4Gen both for unconditional and class-conditional video generation tasks and show that it results in improved video generation as quantified by various metrics. Full video results are available on our project page: https://align4gen.github.io/align4gen/", "AI": {"tldr": "\u63d0\u51fa\u4e86Align4Gen\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u89c6\u9891\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u4e2d\u5f15\u5165\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u7684\u7279\u5f81\u5bf9\u9f50\uff0c\u63d0\u5347\u89c6\u9891\u751f\u6210\u8d28\u91cf", "motivation": "\u5f53\u524d\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u67b6\u6784\u521b\u65b0\u548c\u8bad\u7ec3\u76ee\u6807\u6539\u8fdb\uff0c\u800c\u5ffd\u89c6\u4e86\u7279\u5f81\u8868\u793a\u80fd\u529b\u7684\u63d0\u5347\u3002\u7814\u7a76\u53d1\u73b0\u4e2d\u95f4\u7279\u5f81\u4e0e\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u7684\u5bf9\u9f50\u53ef\u4ee5\u6539\u5584\u89c6\u9891\u751f\u6210\u6548\u679c", "method": "\u63d0\u51fa\u591a\u7279\u5f81\u878d\u5408\u548c\u5bf9\u9f50\u65b9\u6cd5Align4Gen\uff0c\u96c6\u6210\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u4e2d\u3002\u9996\u5148\u5206\u6790\u8bc4\u4f30\u4e0d\u540c\u89c6\u89c9\u7f16\u7801\u5668\u7684\u5224\u522b\u6027\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\uff0c\u7136\u540e\u9009\u62e9\u5408\u9002\u7f16\u7801\u5668\u8fdb\u884c\u7279\u5f81\u5bf9\u9f50", "result": "\u5728\u65e0\u6761\u4ef6\u548c\u7c7b\u6761\u4ef6\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e0a\u90fd\u53d6\u5f97\u4e86\u6539\u8fdb\uff0c\u591a\u79cd\u91cf\u5316\u6307\u6807\u663e\u793a\u89c6\u9891\u751f\u6210\u8d28\u91cf\u5f97\u5230\u63d0\u5347", "conclusion": "\u7279\u5f81\u5bf9\u9f50\u662f\u63d0\u5347\u89c6\u9891\u6269\u6563\u6a21\u578b\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\uff0cAlign4Gen\u901a\u8fc7\u591a\u7279\u5f81\u878d\u5408\u548c\u5bf9\u9f50\u673a\u5236\u663e\u8457\u6539\u5584\u4e86\u89c6\u9891\u751f\u6210\u6548\u679c"}}
{"id": "2509.09572", "pdf": "https://arxiv.org/pdf/2509.09572", "abs": "https://arxiv.org/abs/2509.09572", "authors": ["Sijun Dong", "Yuxuan Hu", "LiBo Wang", "Geng Chen", "Xiaoliang Meng"], "title": "PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning for Remote Sensing Change Detection", "categories": ["cs.CV"], "comment": null, "summary": "To tackle the prevalence of pseudo changes, the scarcity of labeled samples, and the difficulty of cross-domain generalization in multi-temporal and multi-source remote sensing imagery, we propose PeftCD, a change detection framework built upon Vision Foundation Models (VFMs) with Parameter-Efficient Fine-Tuning (PEFT). At its core, PeftCD employs a weight-sharing Siamese encoder derived from a VFM, into which LoRA and Adapter modules are seamlessly integrated. This design enables highly efficient task adaptation by training only a minimal set of additional parameters. To fully unlock the potential of VFMs, we investigate two leading backbones: the Segment Anything Model v2 (SAM2), renowned for its strong segmentation priors, and DINOv3, a state-of-the-art self-supervised representation learner. The framework is complemented by a deliberately lightweight decoder, ensuring the focus remains on the powerful feature representations from the backbones. Extensive experiments demonstrate that PeftCD achieves state-of-the-art performance across multiple public datasets, including SYSU-CD (IoU 73.81%), WHUCD (92.05%), MSRSCD (64.07%), MLCD (76.89%), CDD (97.01%), S2Looking (52.25%) and LEVIR-CD (85.62%), with notably precise boundary delineation and strong suppression of pseudo-changes. In summary, PeftCD presents an optimal balance of accuracy, efficiency, and generalization. It offers a powerful and scalable paradigm for adapting large-scale VFMs to real-world remote sensing change detection applications. The code and pretrained models will be released at https://github.com/dyzy41/PeftCD.", "AI": {"tldr": "PeftCD\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u57fa\u7840\u6a21\u578b(VFMs)\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\u7684\u53d8\u5316\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7LoRA\u548cAdapter\u6a21\u5757\u5b9e\u73b0\u9ad8\u6548\u4efb\u52a1\u9002\u5e94\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u65f6\u76f8\u591a\u6e90\u9065\u611f\u5f71\u50cf\u4e2d\u4f2a\u53d8\u5316\u666e\u904d\u3001\u6807\u8bb0\u6837\u672c\u7a00\u7f3a\u548c\u8de8\u57df\u6cdb\u5316\u56f0\u96be\u7684\u95ee\u9898", "method": "\u4f7f\u7528\u6743\u91cd\u5171\u4eab\u7684Siamese\u7f16\u7801\u5668\uff0c\u96c6\u6210LoRA\u548cAdapter\u6a21\u5757\uff0c\u91c7\u7528SAM2\u548cDINOv3\u4f5c\u4e3a\u9aa8\u5e72\u7f51\u7edc\uff0c\u914d\u5408\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668", "result": "\u5728SYSU-CD(IoU 73.81%)\u3001WHUCD(92.05%)\u3001MSRSCD(64.07%)\u3001MLCD(76.89%)\u3001CDD(97.01%)\u3001S2Looking(52.25%)\u548cLEVIR-CD(85.62%)\u7b49\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd", "conclusion": "PeftCD\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u6cdb\u5316\u6027\u4e4b\u95f4\u5b9e\u73b0\u4e86\u6700\u4f73\u5e73\u8861\uff0c\u4e3a\u5c06\u5927\u89c4\u6a21VFM\u9002\u914d\u5230\u5b9e\u9645\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u5e94\u7528\u63d0\u4f9b\u4e86\u5f3a\u5927\u4e14\u53ef\u6269\u5c55\u7684\u8303\u5f0f"}}
{"id": "2509.09595", "pdf": "https://arxiv.org/pdf/2509.09595", "abs": "https://arxiv.org/abs/2509.09595", "authors": ["Yikang Ding", "Jiwen Liu", "Wenyuan Zhang", "Zekun Wang", "Wentao Hu", "Liyuan Cui", "Mingming Lao", "Yingchao Shao", "Hui Liu", "Xiaohan Li", "Ming Chen", "Xiaoqiang Liu", "Yu-Shen Liu", "Pengfei Wan"], "title": "Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis", "categories": ["cs.CV"], "comment": "Technical Report. Project Page: https://klingavatar.github.io/", "summary": "Recent advances in audio-driven avatar video generation have significantly enhanced audio-visual realism. However, existing methods treat instruction conditioning merely as low-level tracking driven by acoustic or visual cues, without modeling the communicative purpose conveyed by the instructions. This limitation compromises their narrative coherence and character expressiveness. To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that unifies multimodal instruction understanding with photorealistic portrait generation. Our approach adopts a two-stage pipeline. In the first stage, we design a multimodal large language model (MLLM) director that produces a blueprint video conditioned on diverse instruction signals, thereby governing high-level semantics such as character motion and emotions. In the second stage, guided by blueprint keyframes, we generate multiple sub-clips in parallel using a first-last frame strategy. This global-to-local framework preserves fine-grained details while faithfully encoding the high-level intent behind multimodal instructions. Our parallel architecture also enables fast and stable generation of long-duration videos, making it suitable for real-world applications such as digital human livestreaming and vlogging. To comprehensively evaluate our method, we construct a benchmark of 375 curated samples covering diverse instructions and challenging scenarios. Extensive experiments demonstrate that Kling-Avatar is capable of generating vivid, fluent, long-duration videos at up to 1080p and 48 fps, achieving superior performance in lip synchronization accuracy, emotion and dynamic expressiveness, instruction controllability, identity preservation, and cross-domain generalization. These results establish Kling-Avatar as a new benchmark for semantically grounded, high-fidelity audio-driven avatar synthesis.", "AI": {"tldr": "Kling-Avatar\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u7ea7\u8054\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6307\u4ee4\u7406\u89e3\u548c\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u4eba\u50cf\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u97f3\u9891\u9a71\u52a8\u865a\u62df\u4eba\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5728\u53d9\u4e8b\u8fde\u8d2f\u6027\u548c\u89d2\u8272\u8868\u73b0\u529b\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u5c06\u6307\u4ee4\u6761\u4ef6\u89c6\u4e3a\u58f0\u5b66\u6216\u89c6\u89c9\u7ebf\u7d22\u9a71\u52a8\u7684\u4f4e\u7ea7\u8ddf\u8e2a\uff0c\u6ca1\u6709\u5efa\u6a21\u6307\u4ee4\u4f20\u8fbe\u7684\u4ea4\u6d41\u76ee\u7684\uff0c\u8fd9\u5f71\u54cd\u4e86\u53d9\u4e8b\u8fde\u8d2f\u6027\u548c\u89d2\u8272\u8868\u73b0\u529b\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u6c34\u7ebf\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5bfc\u6f14\u751f\u6210\u84dd\u56fe\u89c6\u9891\uff0c\u63a7\u5236\u9ad8\u7ea7\u8bed\u4e49\u5982\u89d2\u8272\u52a8\u4f5c\u548c\u60c5\u611f\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5728\u84dd\u56fe\u5173\u952e\u5e27\u6307\u5bfc\u4e0b\uff0c\u4f7f\u7528\u9996\u5c3e\u5e27\u7b56\u7565\u5e76\u884c\u751f\u6210\u591a\u4e2a\u5b50\u7247\u6bb5\u3002", "result": "\u80fd\u591f\u751f\u6210\u751f\u52a8\u3001\u6d41\u7545\u3001\u957f\u65f6\u957f\u76841080p 48fps\u89c6\u9891\uff0c\u5728\u5507\u540c\u6b65\u51c6\u786e\u6027\u3001\u60c5\u611f\u548c\u52a8\u6001\u8868\u73b0\u529b\u3001\u6307\u4ee4\u53ef\u63a7\u6027\u3001\u8eab\u4efd\u4fdd\u6301\u548c\u8de8\u57df\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Kling-Avatar\u4e3a\u57fa\u4e8e\u8bed\u4e49\u7684\u9ad8\u4fdd\u771f\u97f3\u9891\u9a71\u52a8\u865a\u62df\u4eba\u5408\u6210\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\uff0c\u9002\u7528\u4e8e\u6570\u5b57\u4eba\u76f4\u64ad\u548c\u89c6\u9891\u535a\u5ba2\u7b49\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2509.09610", "pdf": "https://arxiv.org/pdf/2509.09610", "abs": "https://arxiv.org/abs/2509.09610", "authors": ["Daria Laslo", "Efthymios Georgiou", "Marius George Linguraru", "Andreas Rauschecker", "Sabine Muller", "Catherine R. Jutzeler", "Sarah Bruningk"], "title": "Mechanistic Learning with Guided Diffusion Models to Predict Spatio-Temporal Brain Tumor Growth", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 4 figures", "summary": "Predicting the spatio-temporal progression of brain tumors is essential for guiding clinical decisions in neuro-oncology. We propose a hybrid mechanistic learning framework that combines a mathematical tumor growth model with a guided denoising diffusion implicit model (DDIM) to synthesize anatomically feasible future MRIs from preceding scans. The mechanistic model, formulated as a system of ordinary differential equations, captures temporal tumor dynamics including radiotherapy effects and estimates future tumor burden. These estimates condition a gradient-guided DDIM, enabling image synthesis that aligns with both predicted growth and patient anatomy. We train our model on the BraTS adult and pediatric glioma datasets and evaluate on 60 axial slices of in-house longitudinal pediatric diffuse midline glioma (DMG) cases. Our framework generates realistic follow-up scans based on spatial similarity metrics. It also introduces tumor growth probability maps, which capture both clinically relevant extent and directionality of tumor growth as shown by 95th percentile Hausdorff Distance. The method enables biologically informed image generation in data-limited scenarios, offering generative-space-time predictions that account for mechanistic priors.", "AI": {"tldr": "\u4e00\u79cd\u7ed3\u5408\u673a\u5236\u6a21\u578b\u548c\u5bfc\u5411\u53bb\u566a\u6a21\u578b\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u8111\u7ed3\u6784\u7684\u65f6\u7a7a\u8fdb\u5c55\uff0c\u751f\u6210\u89e3\u5256\u5b66\u53ef\u884c\u7684\u672a\u6765MRI\u56fe\u50cf\u3002", "motivation": "\u9884\u6d4b\u8111\u7ed3\u6784\u7684\u65f6\u7a7a\u8fdb\u5c55\u5bf9\u795e\u7ecf\u8131\u5916\u79d1\u4e34\u5e8a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u65b9\u6cd5\u5728\u6570\u636e\u6709\u9650\u60c5\u51b5\u4e0b\u8fdb\u884c\u751f\u7269\u5b66\u4fe1\u606f\u7684\u56fe\u50cf\u751f\u6210\u3002", "method": "\u91c7\u7528\u6570\u5b66\u8111\u7ed3\u6784\u589e\u957f\u6a21\u578b\uff08\u5e38\u5fae\u5206\u65b9\u7a0b\uff09\u6355\u6349\u65f6\u95f4\u52a8\u6001\uff0c\u7ed3\u5408\u68c0\u67e5\u5f71\u54cd\u4f30\u8ba1\uff0c\u7136\u540e\u7528\u68af\u5ea6\u5bfc\u5411\u53bb\u566a\u6a21\u578b\uff08DDIM\uff09\u8fdb\u884c\u56fe\u50cf\u5408\u6210\uff0c\u786e\u4fdd\u4e0e\u9884\u6d4b\u589e\u957f\u548c\u75c5\u4eba\u89e3\u5256\u7ed3\u6784\u5bf9\u9f50\u3002", "result": "\u5728BraTS\u6210\u4eba\u548c\u513f\u7ae5\u7eea\u6742\u80de\u7624\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u572860\u4e2a\u7eb5\u5411\u513f\u7ae5DMG\u75c5\u4f8b\u4e0a\u8bc4\u4f30\u3002\u6a21\u578b\u751f\u6210\u4e86\u73b0\u5b9e\u7684\u968f\u8bbf\u626b\u63cf\uff0c\u901a\u8fc7\u7a7a\u95f4\u76f8\u4f3c\u6027\u6307\u6807\u9a8c\u8bc1\uff0c\u5e76\u63d0\u4f9b\u4e86\u6295\u5f71\u589e\u957f\u6982\u7387\u56fe\uff0c\u663e\u793a\u4e86\u4e34\u5e8a\u76f8\u5173\u7684\u7ed3\u6784\u8303\u56f4\u548c\u589e\u957f\u65b9\u5411\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u6570\u636e\u6709\u9650\u60c5\u51b5\u4e0b\u8fdb\u884c\u751f\u7269\u5b66\u4fe1\u606f\u7684\u56fe\u50cf\u751f\u6210\uff0c\u63d0\u4f9b\u8003\u8651\u673a\u5236\u524d\u77e5\u7684\u751f\u6210\u5f0f\u65f6\u7a7a\u9884\u6d4b\u3002"}}
{"id": "2509.09667", "pdf": "https://arxiv.org/pdf/2509.09667", "abs": "https://arxiv.org/abs/2509.09667", "authors": ["Zhengdi Yu", "Simone Foti", "Linguang Zhang", "Amy Zhao", "Cem Keskin", "Stefanos Zafeiriou", "Tolga Birdal"], "title": "Geometric Neural Distance Fields for Learning Human Motion Priors", "categories": ["cs.CV"], "comment": "8 pages", "summary": "We introduce Neural Riemannian Motion Fields (NRMF), a novel 3D generative human motion prior that enables robust, temporally consistent, and physically plausible 3D motion recovery. Unlike existing VAE or diffusion-based methods, our higher-order motion prior explicitly models the human motion in the zero level set of a collection of neural distance fields (NDFs) corresponding to pose, transition (velocity), and acceleration dynamics. Our framework is rigorous in the sense that our NDFs are constructed on the product space of joint rotations, their angular velocities, and angular accelerations, respecting the geometry of the underlying articulations. We further introduce: (i) a novel adaptive-step hybrid algorithm for projecting onto the set of plausible motions, and (ii) a novel geometric integrator to \"roll out\" realistic motion trajectories during test-time-optimization and generation. Our experiments show significant and consistent gains: trained on the AMASS dataset, NRMF remarkably generalizes across multiple input modalities and to diverse tasks ranging from denoising to motion in-betweening and fitting to partial 2D / 3D observations.", "AI": {"tldr": "NRMF\u662f\u4e00\u79cd\u65b0\u9896\u76843D\u751f\u6210\u5f0f\u4eba\u4f53\u8fd0\u52a8\u5148\u9a8c\u6a21\u578b\uff0c\u901a\u8fc7\u795e\u7ecf\u8ddd\u79bb\u573a\u5728\u5173\u8282\u65cb\u8f6c\u3001\u89d2\u901f\u5ea6\u548c\u89d2\u52a0\u901f\u5ea6\u7684\u4e58\u79ef\u7a7a\u95f4\u4e2d\u663e\u5f0f\u5efa\u6a21\u4eba\u4f53\u8fd0\u52a8\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u3001\u65f6\u95f4\u4e00\u81f4\u4e14\u7269\u7406\u5408\u7406\u76843D\u8fd0\u52a8\u6062\u590d\u3002", "motivation": "\u73b0\u6709VAE\u6216\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u57283D\u4eba\u4f53\u8fd0\u52a8\u6062\u590d\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u663e\u5f0f\u5efa\u6a21\u8fd0\u52a8\u52a8\u529b\u5b66\u5e76\u5c0a\u91cd\u5e95\u5c42\u5173\u8282\u51e0\u4f55\u7ed3\u6784\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u5728\u5173\u8282\u65cb\u8f6c\u3001\u89d2\u901f\u5ea6\u548c\u89d2\u52a0\u901f\u5ea6\u7684\u4e58\u79ef\u7a7a\u95f4\u4e0a\u6784\u5efa\u795e\u7ecf\u8ddd\u79bb\u573a\uff0c\u63d0\u51fa\u81ea\u9002\u5e94\u6b65\u957f\u6df7\u5408\u7b97\u6cd5\u8fdb\u884c\u6295\u5f71\uff0c\u4ee5\u53ca\u51e0\u4f55\u79ef\u5206\u5668\u6765\u5c55\u5f00\u8fd0\u52a8\u8f68\u8ff9\u3002", "result": "\u5728AMASS\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u540e\uff0cNRMF\u5728\u591a\u79cd\u8f93\u5165\u6a21\u6001\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4e14\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5305\u62ec\u53bb\u566a\u3001\u8fd0\u52a8\u63d2\u503c\u548c\u90e8\u52062D/3D\u89c2\u6d4b\u62df\u5408\u3002", "conclusion": "NRMF\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u8fd0\u52a8\u5efa\u6a21\u65b9\u6cd5\uff0c\u4e3a3D\u4eba\u4f53\u8fd0\u52a8\u6062\u590d\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u7269\u7406\u5408\u7406\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5f88\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.09672", "pdf": "https://arxiv.org/pdf/2509.09672", "abs": "https://arxiv.org/abs/2509.09672", "authors": ["Artem Lukoianov", "Chenyang Yuan", "Justin Solomon", "Vincent Sitzmann"], "title": "Locality in Image Diffusion Models Emerges from Data Statistics", "categories": ["cs.CV"], "comment": "30 pages, 18 figures, 6 tables", "summary": "Among generative models, diffusion models are uniquely intriguing due to the existence of a closed-form optimal minimizer of their training objective, often referred to as the optimal denoiser. However, diffusion using this optimal denoiser merely reproduces images in the training set and hence fails to capture the behavior of deep diffusion models. Recent work has attempted to characterize this gap between the optimal denoiser and deep diffusion models, proposing analytical, training-free models that can generate images that resemble those generated by a trained UNet. The best-performing method hypothesizes that shift equivariance and locality inductive biases of convolutional neural networks are the cause of the performance gap, hence incorporating these assumptions into its analytical model. In this work, we present evidence that the locality in deep diffusion models emerges as a statistical property of the image dataset, not due to the inductive bias of convolutional neural networks. Specifically, we demonstrate that an optimal parametric linear denoiser exhibits similar locality properties to the deep neural denoisers. We further show, both theoretically and experimentally, that this locality arises directly from the pixel correlations present in natural image datasets. Finally, we use these insights to craft an analytical denoiser that better matches scores predicted by a deep diffusion model than the prior expert-crafted alternative.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u53d1\u73b0\u6269\u6563\u6a21\u578b\u4e2d\u5c40\u90e8\u6027\u7279\u5f81\u6e90\u4e8e\u56fe\u50cf\u6570\u636e\u96c6\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u800c\u975e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efa\u4e86\u66f4\u5339\u914d\u6df1\u5ea6\u6269\u6563\u6a21\u578b\u7684\u89e3\u6790\u53bb\u566a\u5668", "motivation": "\u73b0\u6709\u7814\u7a76\u8ba4\u4e3a\u6df1\u5ea6\u6269\u6563\u6a21\u578b\u4e0e\u6700\u4f18\u53bb\u566a\u5668\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u6e90\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u5e73\u79fb\u7b49\u53d8\u6027\u548c\u5c40\u90e8\u6027\u5f52\u7eb3\u504f\u7f6e\uff0c\u4f46\u672c\u6587\u65e8\u5728\u8bc1\u660e\u8fd9\u79cd\u5c40\u90e8\u6027\u5b9e\u9645\u4e0a\u662f\u56fe\u50cf\u6570\u636e\u96c6\u672c\u8eab\u7684\u7edf\u8ba1\u7279\u6027", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u6700\u4f18\u53c2\u6570\u5316\u7ebf\u6027\u53bb\u566a\u5668\u8868\u73b0\u51fa\u4e0e\u6df1\u5ea6\u795e\u7ecf\u53bb\u566a\u5668\u76f8\u4f3c\u7684\u5c40\u90e8\u6027\u7279\u5f81\uff0c\u5e76\u5229\u7528\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u4e2d\u50cf\u7d20\u76f8\u5173\u6027\u7684\u6d1e\u5bdf\u6784\u5efa\u65b0\u7684\u89e3\u6790\u53bb\u566a\u5668", "result": "\u7814\u7a76\u53d1\u73b0\u5c40\u90e8\u6027\u76f4\u63a5\u6e90\u4e8e\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u4e2d\u7684\u50cf\u7d20\u76f8\u5173\u6027\uff0c\u6784\u5efa\u7684\u89e3\u6790\u53bb\u566a\u5668\u6bd4\u5148\u524d\u4e13\u5bb6\u8bbe\u8ba1\u7684\u66ff\u4ee3\u65b9\u6848\u66f4\u80fd\u5339\u914d\u6df1\u5ea6\u6269\u6563\u6a21\u578b\u9884\u6d4b\u7684\u5206\u6570", "conclusion": "\u6269\u6563\u6a21\u578b\u4e2d\u7684\u5c40\u90e8\u6027\u7279\u5f81\u672c\u8d28\u4e0a\u662f\u56fe\u50cf\u6570\u636e\u96c6\u7684\u7edf\u8ba1\u5c5e\u6027\uff0c\u8fd9\u4e00\u53d1\u73b0\u4e3a\u7406\u89e3\u6df1\u5ea6\u6269\u6563\u6a21\u578b\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u51c6\u786e\u7684\u89e3\u6790\u6a21\u578b"}}
{"id": "2509.09680", "pdf": "https://arxiv.org/pdf/2509.09680", "abs": "https://arxiv.org/abs/2509.09680", "authors": ["Rongyao Fang", "Aldrich Yu", "Chengqi Duan", "Linjiang Huang", "Shuai Bai", "Yuxuan Cai", "Kun Wang", "Si Liu", "Xihui Liu", "Hongsheng Li"], "title": "FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark", "categories": ["cs.CV", "cs.CL"], "comment": "Project page: https://flux-reason-6m.github.io/", "summary": "The advancement of open-source text-to-image (T2I) models has been hindered by the absence of large-scale, reasoning-focused datasets and comprehensive evaluation benchmarks, resulting in a performance gap compared to leading closed-source systems. To address this challenge, We introduce FLUX-Reason-6M and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark). FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality FLUX-generated images and 20 million bilingual (English and Chinese) descriptions specifically designed to teach complex reasoning. The image are organized according to six key characteristics: Imagination, Entity, Text rendering, Style, Affection, and Composition, and design explicit Generation Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation steps. The whole data curation takes 15,000 A100 GPU days, providing the community with a resource previously unattainable outside of large industrial labs. PRISM-Bench offers a novel evaluation standard with seven distinct tracks, including a formidable Long Text challenge using GCoT. Through carefully designed prompts, it utilizes advanced vision-language models for nuanced human-aligned assessment of prompt-image alignment and image aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench reveals critical performance gaps and highlights specific areas requiring improvement. Our dataset, benchmark, and evaluation code are released to catalyze the next wave of reasoning-oriented T2I generation. Project page: https://flux-reason-6m.github.io/ .", "AI": {"tldr": "FLUX-Reason-6M\u662f\u4e00\u4e2a\u5305\u542b600\u4e07\u9ad8\u8d28\u91cf\u56fe\u50cf\u548c2000\u4e07\u53cc\u8bed\u63cf\u8ff0\u7684\u63a8\u7406\u6570\u636e\u96c6\uff0cPRISM-Bench\u63d0\u4f9b7\u4e2a\u8bc4\u4f30\u8d5b\u9053\u7684\u65b0\u57fa\u51c6\uff0c\u65e8\u5728\u63d0\u5347\u5f00\u6e90\u6587\u672c\u751f\u6210\u56fe\u50cf\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f00\u6e90\u6587\u672c\u751f\u6210\u56fe\u50cf\u6a21\u578b\u56e0\u7f3a\u4e4f\u5927\u89c4\u6a21\u63a8\u7406\u6570\u636e\u96c6\u548c\u5168\u9762\u8bc4\u4f30\u57fa\u51c6\u800c\u843d\u540e\u4e8e\u95ed\u6e90\u7cfb\u7edf\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u6027\u80fd\u5dee\u8ddd\u3002", "method": "\u6784\u5efaFLUX-Reason-6M\u6570\u636e\u96c6\uff086M\u56fe\u50cf+20M\u53cc\u8bed\u63cf\u8ff0\uff09\uff0c\u91c7\u7528\u516d\u7ef4\u7279\u5f81\u7ec4\u7ec7\u548c\u751f\u6210\u601d\u7ef4\u94fe\uff1b\u521b\u5efaPRISM-Bench\u8bc4\u4f30\u57fa\u51c6\uff087\u4e2a\u8d5b\u9053\uff09\uff1b\u4f7f\u752815000 A100 GPU\u5929\u8fdb\u884c\u6570\u636e\u6574\u7406\u3002", "result": "\u5bf919\u4e2a\u9886\u5148\u6a21\u578b\u7684\u8bc4\u4f30\u63ed\u793a\u4e86\u5173\u952e\u6027\u80fd\u5dee\u8ddd\u548c\u6539\u8fdb\u9700\u6c42\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u5de5\u4e1a\u7ea7\u8d44\u6e90\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u548c\u57fa\u51c6\u5c06\u63a8\u52a8\u4e0b\u4e00\u4ee3\u9762\u5411\u63a8\u7406\u7684\u6587\u672c\u751f\u6210\u56fe\u50cf\u6280\u672f\u7684\u53d1\u5c55\uff0c\u6240\u6709\u8d44\u6e90\u5df2\u5f00\u6e90\u53d1\u5e03\u3002"}}
