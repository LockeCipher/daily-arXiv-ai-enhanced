{"id": "2509.20400", "pdf": "https://arxiv.org/pdf/2509.20400", "abs": "https://arxiv.org/abs/2509.20400", "authors": ["Yiyu Li", "Haoyuan Wang", "Ke Xu", "Gerhard Petrus Hancke", "Rynson W. H. Lau"], "title": "SeHDR: Single-Exposure HDR Novel View Synthesis via 3D Gaussian Bracketing", "categories": ["cs.GR"], "comment": "ICCV 2025 accepted paper", "summary": "This paper presents SeHDR, a novel high dynamic range 3D Gaussian Splatting (HDR-3DGS) approach for generating HDR novel views given multi-view LDR images. Unlike existing methods that typically require the multi-view LDR input images to be captured from different exposures, which are tedious to capture and more likely to suffer from errors (e.g., object motion blurs and calibration/alignment inaccuracies), our approach learns the HDR scene representation from multi-view LDR images of a single exposure. Our key insight to this ill-posed problem is that by first estimating Bracketed 3D Gaussians (i.e., with different exposures) from single-exposure multi-view LDR images, we may then be able to merge these bracketed 3D Gaussians into an HDR scene representation. Specifically, SeHDR first learns base 3D Gaussians from single-exposure LDR inputs, where the spherical harmonics parameterize colors in a linear color space. We then estimate multiple 3D Gaussians with identical geometry but varying linear colors conditioned on exposure manipulations. Finally, we propose the Differentiable Neural Exposure Fusion (NeEF) to integrate the base and estimated 3D Gaussians into HDR Gaussians for novel view rendering. Extensive experiments demonstrate that SeHDR outperforms existing methods as well as carefully designed baselines.", "AI": {"tldr": "SeHDR\u662f\u4e00\u79cd\u4ece\u5355\u66dd\u5149\u591a\u89c6\u89d2LDR\u56fe\u50cf\u751f\u6210HDR\u65b0\u89c6\u89d2\u76843D\u9ad8\u65af\u6e85\u5c04\u65b9\u6cd5\uff0c\u65e0\u9700\u591a\u66dd\u5149\u8f93\u5165\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u4ece\u4e0d\u540c\u66dd\u5149\u7684\u591a\u89c6\u89d2LDR\u56fe\u50cf\u4e2d\u5b66\u4e60HDR\u573a\u666f\u8868\u793a\uff0c\u8fd9\u65e2\u7e41\u7410\u53c8\u5bb9\u6613\u4ea7\u751f\u8bef\u5dee\uff08\u5982\u7269\u4f53\u8fd0\u52a8\u6a21\u7cca\u548c\u6821\u51c6\u4e0d\u51c6\u786e\uff09\u3002", "method": "\u9996\u5148\u4ece\u5355\u66dd\u5149LDR\u8f93\u5165\u5b66\u4e60\u57fa\u78403D\u9ad8\u65af\uff0c\u7136\u540e\u4f30\u8ba1\u5177\u6709\u76f8\u540c\u51e0\u4f55\u4f46\u4e0d\u540c\u7ebf\u6027\u989c\u8272\u7684\u591a\u4e2a3D\u9ad8\u65af\uff0c\u6700\u540e\u901a\u8fc7\u53ef\u5fae\u5206\u795e\u7ecf\u66dd\u5149\u878d\u5408\u5c06\u57fa\u7840\u548c\u9ad8\u65af\u4f30\u8ba1\u6574\u5408\u4e3aHDR\u9ad8\u65af\u8fdb\u884c\u65b0\u89c6\u89d2\u6e32\u67d3\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eSeHDR\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u57fa\u7ebf\u3002", "conclusion": "SeHDR\u80fd\u591f\u4ece\u5355\u66dd\u5149\u591a\u89c6\u89d2LDR\u56fe\u50cf\u6210\u529f\u5b66\u4e60HDR\u573a\u666f\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u591a\u66dd\u5149\u8f93\u5165\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.20427", "pdf": "https://arxiv.org/pdf/2509.20427", "abs": "https://arxiv.org/abs/2509.20427", "authors": ["Team Seedream", "Yunpeng Chen", "Yu Gao", "Lixue Gong", "Meng Guo", "Qiushan Guo", "Zhiyao Guo", "Xiaoxia Hou", "Weilin Huang", "Yixuan Huang", "Xiaowen Jian", "Huafeng Kuang", "Zhichao Lai", "Fanshi Li", "Liang Li", "Xiaochen Lian", "Chao Liao", "Liyang Liu", "Wei Liu", "Yanzuo Lu", "Zhengxiong Luo", "Tongtong Ou", "Guang Shi", "Yichun Shi", "Shiqi Sun", "Yu Tian", "Zhi Tian", "Peng Wang", "Rui Wang", "Xun Wang", "Ye Wang", "Guofeng Wu", "Jie Wu", "Wenxu Wu", "Yonghui Wu", "Xin Xia", "Xuefeng Xiao", "Shuang Xu", "Xin Yan", "Ceyuan Yang", "Jianchao Yang", "Zhonghua Zhai", "Chenlin Zhang", "Heng Zhang", "Qi Zhang", "Xinyu Zhang", "Yuwei Zhang", "Shijia Zhao", "Wenliang Zhao", "Wenjia Zhu"], "title": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "categories": ["cs.CV"], "comment": "Seedream 4.0 Technical Report", "summary": "We introduce Seedream 4.0, an efficient and high-performance multimodal image generation system that unifies text-to-image (T2I) synthesis, image editing, and multi-image composition within a single framework. We develop a highly efficient diffusion transformer with a powerful VAE which also can reduce the number of image tokens considerably. This allows for efficient training of our model, and enables it to fast generate native high-resolution images (e.g., 1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning diverse taxonomies and knowledge-centric concepts. Comprehensive data collection across hundreds of vertical scenarios, coupled with optimized strategies, ensures stable and large-scale training, with strong generalization. By incorporating a carefully fine-tuned VLM model, we perform multi-modal post-training for training both T2I and image editing tasks jointly. For inference acceleration, we integrate adversarial distillation, distribution matching, and quantization, as well as speculative decoding. It achieves an inference time of up to 1.8 seconds for generating a 2K image (without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream 4.0 can achieve state-of-the-art results on both T2I and multimodal image editing. In particular, it demonstrates exceptional multimodal capabilities in complex tasks, including precise image editing and in-context reasoning, and also allows for multi-image reference, and can generate multiple output images. This extends traditional T2I systems into an more interactive and multidimensional creative tool, pushing the boundary of generative AI for both creativity and professional applications. Seedream 4.0 is now accessible on https://www.volcengine.com/experience/ark?launch=seedream.", "AI": {"tldr": "Seedream 4.0\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u591a\u6a21\u6001\u56fe\u50cf\u751f\u6210\u7cfb\u7edf\uff0c\u7edf\u4e00\u4e86\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u3001\u56fe\u50cf\u7f16\u8f91\u548c\u591a\u56fe\u50cf\u7ec4\u5408\u529f\u80fd\uff0c\u91c7\u7528\u9ad8\u6548\u6269\u6563\u53d8\u6362\u5668\u548c\u5f3a\u5927VAE\uff0c\u652f\u6301\u539f\u751f\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\uff0c\u5728T2I\u548c\u591a\u6a21\u6001\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u5904\u7406\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u3001\u56fe\u50cf\u7f16\u8f91\u548c\u591a\u56fe\u50cf\u7ec4\u5408\u4efb\u52a1\uff0c\u6269\u5c55\u4f20\u7edfT2I\u7cfb\u7edf\u4e3a\u66f4\u4ea4\u4e92\u548c\u591a\u7ef4\u7684\u521b\u610f\u5de5\u5177\uff0c\u63a8\u52a8\u751f\u6210AI\u5728\u521b\u610f\u548c\u4e13\u4e1a\u5e94\u7528\u4e2d\u7684\u8fb9\u754c\u3002", "method": "\u4f7f\u7528\u9ad8\u6548\u6269\u6563\u53d8\u6362\u5668\u548c\u5f3a\u5927VAE\u51cf\u5c11\u56fe\u50cftoken\u6570\u91cf\uff0c\u5728\u6570\u5341\u4ebf\u6587\u672c-\u56fe\u50cf\u5bf9\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u91c7\u7528\u591a\u6a21\u6001\u540e\u8bad\u7ec3\u8054\u5408\u8bad\u7ec3T2I\u548c\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\uff0c\u96c6\u6210\u5bf9\u6297\u84b8\u998f\u3001\u5206\u5e03\u5339\u914d\u3001\u91cf\u5316\u548c\u63a8\u6d4b\u89e3\u7801\u8fdb\u884c\u63a8\u7406\u52a0\u901f\u3002", "result": "\u7cfb\u7edf\u80fd\u57281.8\u79d2\u5185\u751f\u62102K\u56fe\u50cf\uff08\u65e0LLM/VLM\u4f5c\u4e3aPE\u6a21\u578b\uff09\uff0c\u5728T2I\u548c\u591a\u6a21\u6001\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5c55\u793a\u51fa\u8272\u7684\u591a\u6a21\u6001\u80fd\u529b\uff0c\u5305\u62ec\u7cbe\u786e\u56fe\u50cf\u7f16\u8f91\u548c\u4e0a\u4e0b\u6587\u63a8\u7406\u3002", "conclusion": "Seedream 4.0\u5c06\u4f20\u7edfT2I\u7cfb\u7edf\u6269\u5c55\u4e3a\u66f4\u4ea4\u4e92\u548c\u591a\u7ef4\u7684\u521b\u610f\u5de5\u5177\uff0c\u5728\u521b\u610f\u548c\u4e13\u4e1a\u5e94\u7528\u4e2d\u63a8\u52a8\u4e86\u751f\u6210AI\u7684\u8fb9\u754c\uff0c\u73b0\u5df2\u53ef\u5728\u6307\u5b9a\u7f51\u5740\u8bbf\u95ee\u3002"}}
{"id": "2509.20481", "pdf": "https://arxiv.org/pdf/2509.20481", "abs": "https://arxiv.org/abs/2509.20481", "authors": ["Jing Li", "Oskar Bartosz", "Chengyu Wang", "Michal Wnuczynski", "Dilshan Godaliyadda", "Michael Polley"], "title": "Shared Neural Space: Unified Precomputed Feature Encoding for Multi-Task and Cross Domain Vision", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The majority of AI models in imaging and vision are customized to perform on specific high-precision task. However, this strategy is inefficient for applications with a series of modular tasks, since each requires a mapping into a disparate latent domain. To address this inefficiency, we proposed a universal Neural Space (NS), where an encoder-decoder framework pre-computes features across vision and imaging tasks. Our encoder learns transformation aware, generalizable representations, which enable multiple downstream AI modules to share the same feature space. This architecture reduces redundancy, improves generalization across domain shift, and establishes a foundation for effecient multi-task vision pipelines. Furthermore, as opposed to larger transformer backbones, our backbone is lightweight and CNN-based, allowing for wider across hardware. We furthur demonstrate that imaging and vision modules, such as demosaicing, denoising, depth estimation and semantic segmentation can be performed efficiently in the NS.", "AI": {"tldr": "\u63d0\u51fa\u901a\u7528\u795e\u7ecf\u7a7a\u95f4\uff08NS\uff09\uff0c\u901a\u8fc7\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6846\u67b6\u9884\u8ba1\u7b97\u8de8\u89c6\u89c9\u548c\u6210\u50cf\u4efb\u52a1\u7684\u7279\u5f81\uff0c\u4f7f\u591a\u4e2a\u4e0b\u6e38AI\u6a21\u5757\u5171\u4eab\u540c\u4e00\u7279\u5f81\u7a7a\u95f4\uff0c\u51cf\u5c11\u5197\u4f59\u5e76\u63d0\u9ad8\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u9488\u5bf9\u5f53\u524dAI\u6a21\u578b\u5728\u6210\u50cf\u548c\u89c6\u89c9\u5e94\u7528\u4e2d\u4e3a\u7279\u5b9a\u9ad8\u7cbe\u5ea6\u4efb\u52a1\u5b9a\u5236\u5316\u8bbe\u8ba1\uff0c\u5bfc\u81f4\u5728\u6a21\u5757\u5316\u4efb\u52a1\u5e8f\u5217\u4e2d\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7CNN\u9aa8\u5e72\u7f51\u7edc\u6784\u5efa\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6846\u67b6\uff0c\u5b66\u4e60\u53d8\u6362\u611f\u77e5\u3001\u53ef\u6cdb\u5316\u7684\u8868\u793a\uff0c\u5728\u901a\u7528\u795e\u7ecf\u7a7a\u95f4\u4e2d\u9884\u8ba1\u7b97\u7279\u5f81\uff0c\u652f\u6301\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u6267\u884c\u53bb\u9a6c\u8d5b\u514b\u3001\u53bb\u566a\u3001\u6df1\u5ea6\u4f30\u8ba1\u548c\u8bed\u4e49\u5206\u5272\u7b49\u591a\u79cd\u6210\u50cf\u548c\u89c6\u89c9\u4efb\u52a1\uff0c\u540c\u65f6\u5177\u6709\u66f4\u597d\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u7528\u795e\u7ecf\u7a7a\u95f4\u67b6\u6784\u4e3a\u9ad8\u6548\u591a\u4efb\u52a1\u89c6\u89c9\u6d41\u6c34\u7ebf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u76f8\u6bd4\u5927\u578bTransformer\u9aa8\u5e72\u7f51\u7edc\u66f4\u8f7b\u91cf\uff0c\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u786c\u4ef6\u9002\u7528\u6027\u3002"}}
{"id": "2509.20524", "pdf": "https://arxiv.org/pdf/2509.20524", "abs": "https://arxiv.org/abs/2509.20524", "authors": ["Julien Han", "Shuwen Qiu", "Qi Li", "Xingzi Xu", "Mehmet Saygin Seyfioglu", "Kavosh Asadi", "Karim Bouyarmane"], "title": "InstructVTON: Optimal Auto-Masking and Natural-Language-Guided Interactive Style Control for Inpainting-Based Virtual Try-On", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to CVPR 2025 and Published at CVPR 2025 AI for Content   Creation workshop", "summary": "We present InstructVTON, an instruction-following interactive virtual try-on system that allows fine-grained and complex styling control of the resulting generation, guided by natural language, on single or multiple garments. A computationally efficient and scalable formulation of virtual try-on formulates the problem as an image-guided or image-conditioned inpainting task. These inpainting-based virtual try-on models commonly use a binary mask to control the generation layout. Producing a mask that yields desirable result is difficult, requires background knowledge, might be model dependent, and in some cases impossible with the masking-based approach (e.g. trying on a long-sleeve shirt with \"sleeves rolled up\" styling on a person wearing long-sleeve shirt with sleeves down, where the mask will necessarily cover the entire sleeve). InstructVTON leverages Vision Language Models (VLMs) and image segmentation models for automated binary mask generation. These masks are generated based on user-provided images and free-text style instructions. InstructVTON simplifies the end-user experience by removing the necessity of a precisely drawn mask, and by automating execution of multiple rounds of image generation for try-on scenarios that cannot be achieved with masking-based virtual try-on models alone. We show that InstructVTON is interoperable with existing virtual try-on models to achieve state-of-the-art results with styling control.", "AI": {"tldr": "InstructVTON\u662f\u4e00\u4e2a\u57fa\u4e8e\u6307\u4ee4\u7684\u4ea4\u4e92\u5f0f\u865a\u62df\u8bd5\u7a7f\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u5bfc\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u548c\u590d\u6742\u7684\u98ce\u683c\u63a7\u5236\uff0c\u652f\u6301\u5355\u4ef6\u6216\u591a\u4ef6\u670d\u88c5\u8bd5\u7a7f\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u63a9\u7801\u7684\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\u9700\u8981\u7cbe\u786e\u7ed8\u5236\u4e8c\u8fdb\u5236\u63a9\u7801\uff0c\u8fd9\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u4e14\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u65e0\u6cd5\u5b9e\u73b0\uff08\u5982\u5c06\u957f\u8896\u5377\u8d77\uff09\uff0c\u9650\u5236\u4e86\u7528\u6237\u4f53\u9a8c\u548c\u529f\u80fd\u3002", "method": "\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u56fe\u50cf\u5206\u5272\u6a21\u578b\u6839\u636e\u7528\u6237\u63d0\u4f9b\u7684\u56fe\u50cf\u548c\u81ea\u7531\u6587\u672c\u98ce\u683c\u6307\u4ee4\u81ea\u52a8\u751f\u6210\u4e8c\u8fdb\u5236\u63a9\u7801\uff0c\u7b80\u5316\u7528\u6237\u64cd\u4f5c\u5e76\u652f\u6301\u4f20\u7edf\u63a9\u7801\u65b9\u6cd5\u65e0\u6cd5\u5b9e\u73b0\u7684\u8bd5\u7a7f\u573a\u666f\u3002", "result": "InstructVTON\u80fd\u591f\u4e0e\u73b0\u6709\u865a\u62df\u8bd5\u7a7f\u6a21\u578b\u4e92\u64cd\u4f5c\uff0c\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u548c\u98ce\u683c\u63a7\u5236\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u81ea\u52a8\u5316\u63a9\u7801\u751f\u6210\u548c\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u5347\u4e86\u865a\u62df\u8bd5\u7a7f\u7684\u6613\u7528\u6027\u548c\u529f\u80fd\u6027\uff0c\u652f\u6301\u66f4\u590d\u6742\u7684\u8bd5\u7a7f\u573a\u666f\u3002"}}
{"id": "2509.20756", "pdf": "https://arxiv.org/pdf/2509.20756", "abs": "https://arxiv.org/abs/2509.20756", "authors": ["Yuhong Zhang", "Han Wang", "Yiwen Wang", "Rong Xie", "Li Song"], "title": "FreeInsert: Personalized Object Insertion with Geometric and Style Control", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image diffusion models have made significant progress in image generation, allowing for effortless customized generation. However, existing image editing methods still face certain limitations when dealing with personalized image composition tasks. First, there is the issue of lack of geometric control over the inserted objects. Current methods are confined to 2D space and typically rely on textual instructions, making it challenging to maintain precise geometric control over the objects. Second, there is the challenge of style consistency. Existing methods often overlook the style consistency between the inserted object and the background, resulting in a lack of realism. In addition, the challenge of inserting objects into images without extensive training remains significant. To address these issues, we propose \\textit{FreeInsert}, a novel training-free framework that customizes object insertion into arbitrary scenes by leveraging 3D geometric information. Benefiting from the advances in existing 3D generation models, we first convert the 2D object into 3D, perform interactive editing at the 3D level, and then re-render it into a 2D image from a specified view. This process introduces geometric controls such as shape or view. The rendered image, serving as geometric control, is combined with style and content control achieved through diffusion adapters, ultimately producing geometrically controlled, style-consistent edited images via the diffusion model.", "AI": {"tldr": "FreeInsert\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u75283D\u51e0\u4f55\u4fe1\u606f\u5c06\u5bf9\u8c61\u63d2\u5165\u4efb\u610f\u573a\u666f\uff0c\u5b9e\u73b0\u51e0\u4f55\u63a7\u5236\u548c\u98ce\u683c\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5728\u4e2a\u6027\u5316\u56fe\u50cf\u5408\u6210\u4efb\u52a1\u4e2d\u5b58\u5728\u51e0\u4f55\u63a7\u5236\u4e0d\u8db3\u3001\u98ce\u683c\u4e0d\u4e00\u81f4\u4ee5\u53ca\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u7684\u95ee\u9898\u3002", "method": "\u9996\u5148\u5c062D\u5bf9\u8c61\u8f6c\u6362\u4e3a3D\uff0c\u57283D\u5c42\u9762\u8fdb\u884c\u4ea4\u4e92\u7f16\u8f91\uff0c\u7136\u540e\u4ece\u6307\u5b9a\u89c6\u89d2\u91cd\u65b0\u6e32\u67d3\u4e3a2D\u56fe\u50cf\uff0c\u7ed3\u5408\u6269\u6563\u9002\u914d\u5668\u5b9e\u73b0\u51e0\u4f55\u3001\u98ce\u683c\u548c\u5185\u5bb9\u63a7\u5236\u3002", "result": "\u80fd\u591f\u751f\u6210\u51e0\u4f55\u63a7\u5236\u7cbe\u786e\u3001\u98ce\u683c\u4e00\u81f4\u7684\u7f16\u8f91\u56fe\u50cf\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "conclusion": "FreeInsert\u901a\u8fc73D\u51e0\u4f55\u4fe1\u606f\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u51e0\u4f55\u63a7\u5236\u548c\u98ce\u683c\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u7f16\u8f91\u65b9\u6848\u3002"}}
{"id": "2509.20775", "pdf": "https://arxiv.org/pdf/2509.20775", "abs": "https://arxiv.org/abs/2509.20775", "authors": ["Maoye Ren", "Praneetha Vaddamanu", "Jianjin Xu", "Fernando De la Torre Frade"], "title": "CusEnhancer: A Zero-Shot Scene and Controllability Enhancement Method for Photo Customization via ResInversion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recently remarkable progress has been made in synthesizing realistic human photos using text-to-image diffusion models. However, current approaches face degraded scenes, insufficient control, and suboptimal perceptual identity. We introduce CustomEnhancer, a novel framework to augment existing identity customization models. CustomEnhancer is a zero-shot enhancement pipeline that leverages face swapping techniques, pretrained diffusion model, to obtain additional representations in a zeroshot manner for encoding into personalized models. Through our proposed triple-flow fused PerGeneration approach, which identifies and combines two compatible counter-directional latent spaces to manipulate a pivotal space of personalized model, we unify the generation and reconstruction processes, realizing generation from three flows. Our pipeline also enables comprehensive training-free control over the generation process of personalized models, offering precise controlled personalization for them and eliminating the need for controller retraining for per-model. Besides, to address the high time complexity of null-text inversion (NTI), we introduce ResInversion, a novel inversion method that performs noise rectification via a pre-diffusion mechanism, reducing the inversion time by 129 times. Experiments demonstrate that CustomEnhancer reach SOTA results at scene diversity, identity fidelity, training-free controls, while also showing the efficiency of our ResInversion over NTI. The code will be made publicly available upon paper acceptance.", "AI": {"tldr": "CustomEnhancer\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u91cd\u6d41\u878d\u5408\u751f\u6210\u65b9\u6cd5\u589e\u5f3a\u8eab\u4efd\u5b9a\u5236\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u4eba\u50cf\u751f\u6210\u548c\u7cbe\u786e\u63a7\u5236\uff0c\u540c\u65f6\u63d0\u51faResInversion\u65b9\u6cd5\u5c06\u53cd\u6f14\u65f6\u95f4\u51cf\u5c11129\u500d\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u5408\u6210\u771f\u5b9e\u4eba\u50cf\u65f6\u9762\u4e34\u573a\u666f\u9000\u5316\u3001\u63a7\u5236\u4e0d\u8db3\u548c\u611f\u77e5\u8eab\u4efd\u4e0d\u4f18\u7684\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u8eab\u4efd\u5b9a\u5236\u6a21\u578b\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faCustomEnhancer\u6846\u67b6\uff0c\u5229\u7528\u4eba\u8138\u4ea4\u6362\u6280\u672f\u548c\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u83b7\u53d6\u989d\u5916\u8868\u793a\uff0c\u901a\u8fc7\u4e09\u91cd\u6d41\u878d\u5408\u751f\u6210\u65b9\u6cd5\u7ed3\u5408\u4e24\u4e2a\u517c\u5bb9\u7684\u53cd\u5411\u6f5c\u5728\u7a7a\u95f4\u6765\u64cd\u7eb5\u4e2a\u6027\u5316\u6a21\u578b\u7684\u5173\u952e\u7a7a\u95f4\uff0c\u7edf\u4e00\u751f\u6210\u548c\u91cd\u5efa\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCustomEnhancer\u5728\u573a\u666f\u591a\u6837\u6027\u3001\u8eab\u4efd\u4fdd\u771f\u5ea6\u548c\u514d\u8bad\u7ec3\u63a7\u5236\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0cResInversion\u65b9\u6cd5\u6bd4NTI\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "CustomEnhancer\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u8eab\u4efd\u5b9a\u5236\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u53ef\u63a7\u7684\u4eba\u50cf\u751f\u6210\uff0c\u540c\u65f6\u5927\u5e45\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2509.20792", "pdf": "https://arxiv.org/pdf/2509.20792", "abs": "https://arxiv.org/abs/2509.20792", "authors": ["Ved Umrajkar"], "title": "DAC-LoRA: Dynamic Adversarial Curriculum for Efficient and Robust Few-Shot Adaptation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted at ICCV2025 Workshop on Safe and Trustworthy Multimodal AI   Systems", "summary": "Vision-Language Models (VLMs) are foundational to critical applications like autonomous driving, medical diagnosis, and content moderation. While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA enable their efficient adaptation to specialized tasks, these models remain vulnerable to adversarial attacks that can compromise safety-critical decisions. CLIP, the backbone for numerous downstream VLMs, is a high-value target whose vulnerabilities can cascade across the multimodal AI ecosystem. We propose Dynamic Adversarial Curriculum DAC-LoRA, a novel framework that integrates adversarial training into PEFT. The core principle of our method i.e. an intelligent curriculum of progressively challenging attack, is general and can potentially be applied to any iterative attack method. Guided by the First-Order Stationary Condition (FOSC) and a TRADES-inspired loss, DAC-LoRA achieves substantial improvements in adversarial robustness without significantly compromising clean accuracy. Our work presents an effective, lightweight, and broadly applicable method to demonstrate that the DAC-LoRA framework can be easily integrated into a standard PEFT pipeline to significantly enhance robustness.", "AI": {"tldr": "DAC-LoRA\u662f\u4e00\u79cd\u5c06\u5bf9\u6297\u8bad\u7ec3\u96c6\u6210\u5230\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\u4e2d\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5bf9\u6297\u8bfe\u7a0b\u9010\u6b65\u589e\u5f3aVision-Language Models\u7684\u9c81\u68d2\u6027\uff0c\u5728\u4e0d\u663e\u8457\u5f71\u54cd\u6e05\u6d01\u51c6\u786e\u7387\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u5bf9\u6297\u653b\u51fb\u62b5\u6297\u529b\u3002", "motivation": "Vision-Language Models\u5728\u5173\u952e\u5e94\u7528\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5373\u4f7f\u4f7f\u7528LoRA\u7b49PEFT\u65b9\u6cd5\u8fdb\u884c\u9ad8\u6548\u9002\u914d\uff0c\u8fd9\u4e9b\u6a21\u578b\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u653b\u51fb\uff0c\u53ef\u80fd\u5371\u53ca\u5b89\u5168\u5173\u952e\u51b3\u7b56\u3002CLIP\u4f5c\u4e3a\u4f17\u591a\u4e0b\u6e38VLM\u7684\u9aa8\u5e72\u7f51\u7edc\uff0c\u5176\u6f0f\u6d1e\u53ef\u80fd\u5728\u6574\u4e2a\u591a\u6a21\u6001AI\u751f\u6001\u7cfb\u7edf\u4e2d\u4f20\u64ad\u3002", "method": "\u63d0\u51faDynamic Adversarial Curriculum (DAC-LoRA)\u6846\u67b6\uff0c\u6838\u5fc3\u539f\u7406\u662f\u667a\u80fd\u7684\u6e10\u8fdb\u5f0f\u6311\u6218\u653b\u51fb\u8bfe\u7a0b\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u4e00\u9636\u5e73\u7a33\u6761\u4ef6(FOSC)\u548cTRADES\u542f\u53d1\u5f0f\u635f\u5931\u51fd\u6570\uff0c\u53ef\u5e94\u7528\u4e8e\u4efb\u4f55\u8fed\u4ee3\u653b\u51fb\u65b9\u6cd5\u3002", "result": "DAC-LoRA\u5728\u5bf9\u6297\u9c81\u68d2\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u5b9e\u8d28\u6027\u6539\u8fdb\uff0c\u540c\u65f6\u6ca1\u6709\u663e\u8457\u635f\u5bb3\u6e05\u6d01\u51c6\u786e\u7387\u3002\u8be5\u65b9\u6cd5\u8f7b\u91cf\u7ea7\u4e14\u5e7f\u6cdb\u9002\u7528\u3002", "conclusion": "DAC-LoRA\u6846\u67b6\u53ef\u4ee5\u8f7b\u677e\u96c6\u6210\u5230\u6807\u51c6PEFT\u6d41\u7a0b\u4e2d\uff0c\u663e\u8457\u589e\u5f3a\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u5173\u952e\u5e94\u7528\u4e2d\u7684Vision-Language Models\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b89\u5168\u4fdd\u62a4\u65b9\u6848\u3002"}}
{"id": "2509.20884", "pdf": "https://arxiv.org/pdf/2509.20884", "abs": "https://arxiv.org/abs/2509.20884", "authors": ["Zhifei Li", "Feng Qiu", "Yiran Wang", "Yujing Xia", "Kui Xiao", "Miao Zhang", "Yan Zhang"], "title": "Integrating Object Interaction Self-Attention and GAN-Based Debiasing for Visual Question Answering", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages, 6 figures. ACCEPTED for publication as a REGULAR paper in   the IEEE Transactions on Multimedia 2025", "summary": "Visual Question Answering (VQA) presents a unique challenge by requiring models to understand and reason about visual content to answer questions accurately. Existing VQA models often struggle with biases introduced by the training data, leading to over-reliance on superficial patterns and inadequate generalization to diverse questions and images. This paper presents a novel model, IOG-VQA, which integrates Object Interaction Self-Attention and GAN-Based Debiasing to enhance VQA model performance. The self-attention mechanism allows our model to capture complex interactions between objects within an image, providing a more comprehensive understanding of the visual context. Meanwhile, the GAN-based debiasing framework generates unbiased data distributions, helping the model to learn more robust and generalizable features. By leveraging these two components, IOG-VQA effectively combines visual and textual information to address the inherent biases in VQA datasets. Extensive experiments on the VQA-CP v1 and VQA-CP v2 datasets demonstrate that our model shows excellent performance compared with the existing methods, particularly in handling biased and imbalanced data distributions highlighting the importance of addressing both object interactions and dataset biases in advancing VQA tasks. Our code is available at https://github.com/HubuKG/IOG-VQA.", "AI": {"tldr": "IOG-VQA\u6a21\u578b\u901a\u8fc7\u5bf9\u8c61\u4ea4\u4e92\u81ea\u6ce8\u610f\u529b\u548cGAN\u53bb\u504f\u6280\u672f\u63d0\u5347VQA\u6027\u80fd\uff0c\u6709\u6548\u89e3\u51b3\u6570\u636e\u96c6\u504f\u89c1\u95ee\u9898\u3002", "motivation": "\u73b0\u6709VQA\u6a21\u578b\u5b58\u5728\u8bad\u7ec3\u6570\u636e\u504f\u89c1\u95ee\u9898\uff0c\u8fc7\u5ea6\u4f9d\u8d56\u8868\u9762\u6a21\u5f0f\uff0c\u5bf9\u591a\u6837\u5316\u95ee\u9898\u548c\u56fe\u50cf\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u5bf9\u8c61\u4ea4\u4e92\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u56fe\u50cf\u5185\u5bf9\u8c61\u590d\u6742\u4ea4\u4e92\uff0c\u4ee5\u53caGAN\u53bb\u504f\u6846\u67b6\u751f\u6210\u65e0\u504f\u6570\u636e\u5206\u5e03\u3002", "result": "\u5728VQA-CP v1\u548cVQA-CP v2\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u5728\u5904\u7406\u6709\u504f\u548c\u4e0d\u5e73\u8861\u6570\u636e\u5206\u5e03\u65b9\u9762\u3002", "conclusion": "\u89e3\u51b3\u5bf9\u8c61\u4ea4\u4e92\u548c\u6570\u636e\u96c6\u504f\u89c1\u5bf9\u63a8\u8fdbVQA\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.20886", "pdf": "https://arxiv.org/pdf/2509.20886", "abs": "https://arxiv.org/abs/2509.20886", "authors": ["Tristan S. W. Stevens", "Ois\u00edn Nolan", "Jean-Luc Robert", "Ruud J. G. van Sloun"], "title": "Nuclear Diffusion Models for Low-Rank Background Suppression in Videos", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "5 pages, 4 figures, preprint", "summary": "Video sequences often contain structured noise and background artifacts that obscure dynamic content, posing challenges for accurate analysis and restoration. Robust principal component methods address this by decomposing data into low-rank and sparse components. Still, the sparsity assumption often fails to capture the rich variability present in real video data. To overcome this limitation, a hybrid framework that integrates low-rank temporal modeling with diffusion posterior sampling is proposed. The proposed method, Nuclear Diffusion, is evaluated on a real-world medical imaging problem, namely cardiac ultrasound dehazing, and demonstrates improved dehazing performance compared to traditional RPCA concerning contrast enhancement (gCNR) and signal preservation (KS statistic). These results highlight the potential of combining model-based temporal models with deep generative priors for high-fidelity video restoration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4f4e\u79e9\u65f6\u95f4\u5efa\u6a21\u548c\u6269\u6563\u540e\u9a8c\u91c7\u6837\u7684\u6df7\u5408\u6846\u67b6Nuclear Diffusion\uff0c\u7528\u4e8e\u89c6\u9891\u53bb\u96fe\uff0c\u5728\u5fc3\u810f\u8d85\u58f0\u53bb\u96fe\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfRPCA\u65b9\u6cd5\u3002", "motivation": "\u89c6\u9891\u5e8f\u5217\u5e38\u5305\u542b\u7ed3\u6784\u5316\u566a\u58f0\u548c\u80cc\u666f\u4f2a\u5f71\uff0c\u4f20\u7edfRPCA\u65b9\u6cd5\u7684\u7a00\u758f\u6027\u5047\u8bbe\u96be\u4ee5\u6355\u6349\u771f\u5b9e\u89c6\u9891\u6570\u636e\u7684\u4e30\u5bcc\u53d8\u5f02\u6027\u3002", "method": "\u96c6\u6210\u4f4e\u79e9\u65f6\u95f4\u5efa\u6a21\u4e0e\u6269\u6563\u540e\u9a8c\u91c7\u6837\u7684\u6df7\u5408\u6846\u67b6\uff0c\u5229\u7528\u6a21\u578b\u57fa\u7840\u7684\u65f6\u95f4\u6a21\u578b\u548c\u6df1\u5ea6\u751f\u6210\u5148\u9a8c\u3002", "result": "\u5728\u5fc3\u810f\u8d85\u58f0\u53bb\u96fe\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u4f20\u7edfRPCA\uff0c\u5728\u5bf9\u6bd4\u5ea6\u589e\u5f3a(gCNR)\u548c\u4fe1\u53f7\u4fdd\u6301(KS\u7edf\u8ba1\u91cf)\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u7ed3\u5408\u6a21\u578b\u57fa\u7840\u7684\u65f6\u95f4\u6a21\u578b\u4e0e\u6df1\u5ea6\u751f\u6210\u5148\u9a8c\u5728\u9ad8\u8d28\u91cf\u89c6\u9891\u6062\u590d\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2509.20927", "pdf": "https://arxiv.org/pdf/2509.20927", "abs": "https://arxiv.org/abs/2509.20927", "authors": ["Akihisa Watanabe", "Jiawei Ren", "Li Siyao", "Yichen Peng", "Erwin Wu", "Edgar Simo-Serra"], "title": "SimDiff: Simulator-constrained Diffusion Model for Physically Plausible Motion Generation", "categories": ["cs.CV"], "comment": null, "summary": "Generating physically plausible human motion is crucial for applications such as character animation and virtual reality. Existing approaches often incorporate a simulator-based motion projection layer to the diffusion process to enforce physical plausibility. However, such methods are computationally expensive due to the sequential nature of the simulator, which prevents parallelization. We show that simulator-based motion projection can be interpreted as a form of guidance, either classifier-based or classifier-free, within the diffusion process. Building on this insight, we propose SimDiff, a Simulator-constrained Diffusion Model that integrates environment parameters (e.g., gravity, wind) directly into the denoising process. By conditioning on these parameters, SimDiff generates physically plausible motions efficiently, without repeated simulator calls at inference, and also provides fine-grained control over different physical coefficients. Moreover, SimDiff successfully generalizes to unseen combinations of environmental parameters, demonstrating compositional generalization.", "AI": {"tldr": "SimDiff\uff1a\u4e00\u79cd\u6a21\u62df\u5668\u7ea6\u675f\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u73af\u5883\u53c2\u6570\u76f4\u63a5\u96c6\u6210\u5230\u53bb\u566a\u8fc7\u7a0b\u4e2d\uff0c\u65e0\u9700\u63a8\u7406\u65f6\u7684\u91cd\u590d\u6a21\u62df\u5668\u8c03\u7528\u5373\u53ef\u9ad8\u6548\u751f\u6210\u7269\u7406\u5408\u7406\u7684\u4eba\u4f53\u8fd0\u52a8\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u52a0\u5165\u57fa\u4e8e\u6a21\u62df\u5668\u7684\u8fd0\u52a8\u6295\u5f71\u5c42\u6765\u786e\u4fdd\u7269\u7406\u5408\u7406\u6027\uff0c\u4f46\u7531\u4e8e\u6a21\u62df\u5668\u7684\u987a\u5e8f\u6027\u8d28\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u4e14\u65e0\u6cd5\u5e76\u884c\u5316\u3002", "method": "\u5c06\u6a21\u62df\u5668\u8fd0\u52a8\u6295\u5f71\u89e3\u91ca\u4e3a\u6269\u6563\u8fc7\u7a0b\u4e2d\u7684\u5f15\u5bfc\u5f62\u5f0f\uff0c\u63d0\u51faSimDiff\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u73af\u5883\u53c2\u6570\uff08\u5982\u91cd\u529b\u3001\u98ce\u529b\uff09\u76f4\u63a5\u6761\u4ef6\u5316\u5230\u53bb\u566a\u8fc7\u7a0b\u4e2d\u6765\u751f\u6210\u7269\u7406\u5408\u7406\u7684\u8fd0\u52a8\u3002", "result": "SimDiff\u80fd\u591f\u9ad8\u6548\u751f\u6210\u7269\u7406\u5408\u7406\u7684\u8fd0\u52a8\uff0c\u65e0\u9700\u63a8\u7406\u65f6\u7684\u91cd\u590d\u6a21\u62df\u5668\u8c03\u7528\uff0c\u5e76\u63d0\u4f9b\u5bf9\u4e0d\u540c\u7269\u7406\u7cfb\u6570\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u540c\u65f6\u6210\u529f\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u73af\u5883\u53c2\u6570\u7ec4\u5408\u3002", "conclusion": "SimDiff\u901a\u8fc7\u5c06\u73af\u5883\u53c2\u6570\u76f4\u63a5\u96c6\u6210\u5230\u6269\u6563\u8fc7\u7a0b\u4e2d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7269\u7406\u5408\u7406\u8fd0\u52a8\u751f\u6210\uff0c\u5e76\u5c55\u793a\u4e86\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.20986", "pdf": "https://arxiv.org/pdf/2509.20986", "abs": "https://arxiv.org/abs/2509.20986", "authors": ["Geunhyeok Yu", "Sunjae Jeong", "Yoonyoung Choi", "Jaeseung Kim", "Hyoseok Hwang"], "title": "SiNGER: A Clearer Voice Distills Vision Transformers Further", "categories": ["cs.CV", "cs.AI"], "comment": "Main paper: 12 pages (including 3 pages of references), 6 figures, 6   tables. Appendix: 9 pages, 7 figures", "summary": "Vision Transformers are widely adopted as the backbone of vision foundation models, but they are known to produce high-norm artifacts that degrade representation quality. When knowledge distillation transfers these features to students, high-norm artifacts dominate the objective, so students overfit to artifacts and underweight informative signals, diminishing the gains from larger models. Prior work attempted to remove artifacts but encountered an inherent trade-off between artifact suppression and preserving informative signals from teachers. To address this, we introduce Singular Nullspace-Guided Energy Reallocation (SiNGER), a novel distillation framework that suppresses artifacts while preserving informative signals. The key idea is principled teacher feature refinement: during refinement, we leverage the nullspace-guided perturbation to preserve information while suppressing artifacts. Then, the refined teacher's features are distilled to a student. We implement this perturbation efficiently with a LoRA-based adapter that requires minimal structural modification. Extensive experiments show that \\oursname consistently improves student models, achieving state-of-the-art performance in multiple downstream tasks and producing clearer and more interpretable representations.", "AI": {"tldr": "\u63d0\u51faSiNGER\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u96f6\u7a7a\u95f4\u5f15\u5bfc\u7684\u80fd\u91cf\u91cd\u5206\u914d\u6765\u6291\u5236Vision Transformers\u4e2d\u7684\u9ad8\u8303\u6570\u4f2a\u5f71\uff0c\u540c\u65f6\u4fdd\u7559\u4fe1\u606f\u4fe1\u53f7\uff0c\u63d0\u5347\u5b66\u751f\u6a21\u578b\u6027\u80fd\u3002", "motivation": "Vision Transformers\u4f1a\u4ea7\u751f\u9ad8\u8303\u6570\u4f2a\u5f71\uff0c\u5728\u77e5\u8bc6\u84b8\u998f\u8fc7\u7a0b\u4e2d\u8fd9\u4e9b\u4f2a\u5f71\u4f1a\u4e3b\u5bfc\u76ee\u6807\u51fd\u6570\uff0c\u5bfc\u81f4\u5b66\u751f\u6a21\u578b\u8fc7\u5ea6\u62df\u5408\u4f2a\u5f71\u800c\u5ffd\u89c6\u4fe1\u606f\u4fe1\u53f7\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u6a21\u578b\u7684\u589e\u76ca\u3002", "method": "\u4f7f\u7528\u96f6\u7a7a\u95f4\u5f15\u5bfc\u7684\u6270\u52a8\u65b9\u6cd5\u8fdb\u884c\u6559\u5e08\u7279\u5f81\u7cbe\u70bc\uff0c\u5728\u7cbe\u70bc\u8fc7\u7a0b\u4e2d\u4fdd\u7559\u4fe1\u606f\u540c\u65f6\u6291\u5236\u4f2a\u5f71\uff0c\u7136\u540e\u901a\u8fc7LoRA\u9002\u914d\u5668\u5c06\u7cbe\u70bc\u540e\u7684\u6559\u5e08\u7279\u5f81\u84b8\u998f\u7ed9\u5b66\u751f\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u751f\u6210\u4e86\u66f4\u6e05\u6670\u548c\u53ef\u89e3\u91ca\u7684\u8868\u5f81\u3002", "conclusion": "SiNGER\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u6559\u5e08\u7279\u5f81\u4e2d\u4f2a\u5f71\u548c\u4fe1\u606f\u4fe1\u53f7\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5b66\u751f\u6a21\u578b\u7684\u8868\u73b0\u3002"}}
{"id": "2509.21008", "pdf": "https://arxiv.org/pdf/2509.21008", "abs": "https://arxiv.org/abs/2509.21008", "authors": ["Qinqin He", "Jiaqi Weng", "Jialing Tao", "Hui Xue"], "title": "A Single Neuron Works: Precise Concept Erasure in Text-to-Image Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image models exhibit remarkable capabilities in image generation. However, they also pose safety risks of generating harmful content. A key challenge of existing concept erasure methods is the precise removal of target concepts while minimizing degradation of image quality. In this paper, we propose Single Neuron-based Concept Erasure (SNCE), a novel approach that can precisely prevent harmful content generation by manipulating only a single neuron. Specifically, we train a Sparse Autoencoder (SAE) to map text embeddings into a sparse, disentangled latent space, where individual neurons align tightly with atomic semantic concepts. To accurately locate neurons responsible for harmful concepts, we design a novel neuron identification method based on the modulated frequency scoring of activation patterns. By suppressing activations of the harmful concept-specific neuron, SNCE achieves surgical precision in concept erasure with minimal disruption to image quality. Experiments on various benchmarks demonstrate that SNCE achieves state-of-the-art results in target concept erasure, while preserving the model's generation capabilities for non-target concepts. Additionally, our method exhibits strong robustness against adversarial attacks, significantly outperforming existing methods.", "AI": {"tldr": "SNCE\u662f\u4e00\u79cd\u901a\u8fc7\u64cd\u7eb5\u5355\u4e2a\u795e\u7ecf\u5143\u6765\u7cbe\u786e\u9632\u6b62\u6709\u5bb3\u5185\u5bb9\u751f\u6210\u7684\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5c06\u6587\u672c\u5d4c\u5165\u6620\u5c04\u5230\u89e3\u8026\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u901a\u8fc7\u8c03\u5236\u9891\u7387\u8bc4\u5206\u8bc6\u522b\u6709\u5bb3\u6982\u5ff5\u76f8\u5173\u795e\u7ecf\u5143\uff0c\u5b9e\u73b0\u7cbe\u51c6\u6982\u5ff5\u64e6\u9664\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4e5f\u5b58\u5728\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u7684\u5b89\u5168\u98ce\u9669\u3002\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u7684\u5173\u952e\u6311\u6218\u662f\u5728\u7cbe\u786e\u79fb\u9664\u76ee\u6807\u6982\u5ff5\u7684\u540c\u65f6\u6700\u5c0f\u5316\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u8bad\u7ec3\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5c06\u6587\u672c\u5d4c\u5165\u6620\u5c04\u5230\u7a00\u758f\u89e3\u8026\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u8c03\u5236\u9891\u7387\u8bc4\u5206\u7684\u795e\u7ecf\u5143\u8bc6\u522b\u65b9\u6cd5\u51c6\u786e\u5b9a\u4f4d\u6709\u5bb3\u6982\u5ff5\u76f8\u5173\u795e\u7ecf\u5143\uff0c\u901a\u8fc7\u6291\u5236\u6709\u5bb3\u6982\u5ff5\u7279\u5b9a\u795e\u7ecf\u5143\u7684\u6fc0\u6d3b\u5b9e\u73b0\u6982\u5ff5\u64e6\u9664\u3002", "result": "\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSNCE\u5728\u76ee\u6807\u6982\u5ff5\u64e6\u9664\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u5bf9\u975e\u76ee\u6807\u6982\u5ff5\u7684\u751f\u6210\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5bf9\u5bf9\u6297\u653b\u51fb\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SNCE\u901a\u8fc7\u5355\u795e\u7ecf\u5143\u64cd\u4f5c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u6982\u5ff5\u64e6\u9664\uff0c\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u6709\u6548\u9632\u6b62\u6709\u5bb3\u5185\u5bb9\u751f\u6210\uff0c\u5177\u6709\u4f18\u5f02\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.21086", "pdf": "https://arxiv.org/pdf/2509.21086", "abs": "https://arxiv.org/abs/2509.21086", "authors": ["Guojun Lei", "Rong Zhang", "Chi Wang", "Tianhang Liu", "Hong Li", "Zhiyuan Ma", "Weiwei Xu"], "title": "UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep Decomposition", "categories": ["cs.CV"], "comment": "NeuriIPS 2025", "summary": "We propose a novel architecture UniTransfer, which introduces both spatial and diffusion timestep decomposition in a progressive paradigm, achieving precise and controllable video concept transfer. Specifically, in terms of spatial decomposition, we decouple videos into three key components: the foreground subject, the background, and the motion flow. Building upon this decomposed formulation, we further introduce a dual-to-single-stream DiT-based architecture for supporting fine-grained control over different components in the videos. We also introduce a self-supervised pretraining strategy based on random masking to enhance the decomposed representation learning from large-scale unlabeled video data. Inspired by the Chain-of-Thought reasoning paradigm, we further revisit the denoising diffusion process and propose a Chain-of-Prompt (CoP) mechanism to achieve the timestep decomposition. We decompose the denoising process into three stages of different granularity and leverage large language models (LLMs) for stage-specific instructions to guide the generation progressively. We also curate an animal-centric video dataset called OpenAnimal to facilitate the advancement and benchmarking of research in video concept transfer. Extensive experiments demonstrate that our method achieves high-quality and controllable video concept transfer across diverse reference images and scenes, surpassing existing baselines in both visual fidelity and editability. Web Page: https://yu-shaonian.github.io/UniTransfer-Web/", "AI": {"tldr": "UniTransfer\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u89c6\u9891\u6982\u5ff5\u8fc1\u79fb\u67b6\u6784\uff0c\u901a\u8fc7\u7a7a\u95f4\u548c\u6269\u6563\u65f6\u95f4\u6b65\u5206\u89e3\u5b9e\u73b0\u7cbe\u786e\u53ef\u63a7\u7684\u89c6\u9891\u6982\u5ff5\u8fc1\u79fb\uff0c\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u53ef\u7f16\u8f91\u6027\u65b9\u9762\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u6982\u5ff5\u8fc1\u79fb\u65b9\u6cd5\u5728\u7cbe\u786e\u63a7\u5236\u548c\u7f16\u8f91\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5bf9\u89c6\u9891\u4e0d\u540c\u7ec4\u4ef6\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63a7\u5236\u7684\u65b0\u67b6\u6784\u3002", "method": "\u63d0\u51fa\u7a7a\u95f4\u5206\u89e3\uff08\u524d\u666f\u4e3b\u4f53\u3001\u80cc\u666f\u3001\u8fd0\u52a8\u6d41\uff09\u548c\u6269\u6563\u65f6\u95f4\u6b65\u5206\u89e3\u7684\u53cc\u91cd\u5206\u89e3\u7b56\u7565\uff0c\u91c7\u7528\u53cc\u5230\u5355\u6d41DiT\u67b6\u6784\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548cChain-of-Prompt\u673a\u5236\uff0c\u5229\u7528LLM\u8fdb\u884c\u6e10\u8fdb\u5f0f\u751f\u6210\u6307\u5bfc\u3002", "result": "\u5728OpenAnimal\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u548c\u53ef\u63a7\u7684\u89c6\u9891\u6982\u5ff5\u8fc1\u79fb\uff0c\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u53ef\u7f16\u8f91\u6027\u65b9\u9762\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "UniTransfer\u901a\u8fc7\u7a7a\u95f4\u548c\u65f6\u95f4\u6b65\u5206\u89e3\u7684\u6e10\u8fdb\u5f0f\u8303\u5f0f\uff0c\u4e3a\u89c6\u9891\u6982\u5ff5\u8fc1\u79fb\u63d0\u4f9b\u4e86\u7cbe\u786e\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u7f16\u8f91\u80fd\u529b\u3002"}}
{"id": "2509.21119", "pdf": "https://arxiv.org/pdf/2509.21119", "abs": "https://arxiv.org/abs/2509.21119", "authors": ["Guojun Lei", "Chi Wang", "Yikai Wang", "Hong Li", "Ying Song", "Weiwei Xu"], "title": "MotionFlow:Learning Implicit Motion Flow for Complex Camera Trajectory Control in Video Generation", "categories": ["cs.CV"], "comment": "ICME2025", "summary": "Generating videos guided by camera trajectories poses significant challenges in achieving consistency and generalizability, particularly when both camera and object motions are present. Existing approaches often attempt to learn these motions separately, which may lead to confusion regarding the relative motion between the camera and the objects. To address this challenge, we propose a novel approach that integrates both camera and object motions by converting them into the motion of corresponding pixels. Utilizing a stable diffusion network, we effectively learn reference motion maps in relation to the specified camera trajectory. These maps, along with an extracted semantic object prior, are then fed into an image-to-video network to generate the desired video that can accurately follow the designated camera trajectory while maintaining consistent object motions. Extensive experiments verify that our model outperforms SOTA methods by a large margin.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u76f8\u673a\u548c\u7269\u4f53\u8fd0\u52a8\u6574\u5408\u4e3a\u50cf\u7d20\u8fd0\u52a8\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a33\u5b9a\u6269\u6563\u7f51\u7edc\u5b66\u4e60\u53c2\u8003\u8fd0\u52a8\u56fe\uff0c\u7ed3\u5408\u8bed\u4e49\u5bf9\u8c61\u5148\u9a8c\u751f\u6210\u7b26\u5408\u6307\u5b9a\u76f8\u673a\u8f68\u8ff9\u7684\u89c6\u9891\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5206\u522b\u5b66\u4e60\u76f8\u673a\u548c\u7269\u4f53\u8fd0\u52a8\u53ef\u80fd\u5bfc\u81f4\u76f8\u5bf9\u8fd0\u52a8\u6df7\u6dc6\uff0c\u96be\u4ee5\u5728\u76f8\u673a\u548c\u7269\u4f53\u8fd0\u52a8\u540c\u65f6\u5b58\u5728\u65f6\u4fdd\u6301\u89c6\u9891\u4e00\u81f4\u6027\u548c\u6cdb\u5316\u6027\u3002", "method": "\u5c06\u76f8\u673a\u548c\u7269\u4f53\u8fd0\u52a8\u8f6c\u6362\u4e3a\u50cf\u7d20\u8fd0\u52a8\uff0c\u4f7f\u7528\u7a33\u5b9a\u6269\u6563\u7f51\u7edc\u5b66\u4e60\u53c2\u8003\u8fd0\u52a8\u56fe\uff0c\u7ed3\u5408\u8bed\u4e49\u5bf9\u8c61\u5148\u9a8c\u8f93\u5165\u56fe\u50cf\u5230\u89c6\u9891\u7f51\u7edc\u751f\u6210\u89c6\u9891\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u672c\u6a21\u578b\u5927\u5e45\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u7b26\u5408\u6307\u5b9a\u76f8\u673a\u8f68\u8ff9\u7684\u89c6\u9891\uff0c\u540c\u65f6\u4fdd\u6301\u7269\u4f53\u8fd0\u52a8\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2509.21227", "pdf": "https://arxiv.org/pdf/2509.21227", "abs": "https://arxiv.org/abs/2509.21227", "authors": ["Seyed Amir Kasaei", "Ali Aghayari", "Arash Marioriyad", "Niki Sepasian", "MohammadAmin Fazli", "Mahdieh Soleymani Baghshah", "Mohammad Hossein Rohban"], "title": "Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted at GenProCC NeurIPS 2025 Workshop", "summary": "Text-image generation has advanced rapidly, but assessing whether outputs truly capture the objects, attributes, and relations described in prompts remains a central challenge. Evaluation in this space relies heavily on automated metrics, yet these are often adopted by convention or popularity rather than validated against human judgment. Because evaluation and reported progress in the field depend directly on these metrics, it is critical to understand how well they reflect human preferences. To address this, we present a broad study of widely used metrics for compositional text-image evaluation. Our analysis goes beyond simple correlation, examining their behavior across diverse compositional challenges and comparing how different metric families align with human judgments. The results show that no single metric performs consistently across tasks: performance varies with the type of compositional problem. Notably, VQA-based metrics, though popular, are not uniformly superior, while certain embedding-based metrics prove stronger in specific cases. Image-only metrics, as expected, contribute little to compositional evaluation, as they are designed for perceptual quality rather than alignment. These findings underscore the importance of careful and transparent metric selection, both for trustworthy evaluation and for their use as reward models in generation. Project page is available at \\href{https://amirkasaei.com/eval-the-evals/}{this URL}.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6587\u672c-\u56fe\u50cf\u751f\u6210\u8bc4\u4f30\u6307\u6807\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7814\u7a76\uff0c\u53d1\u73b0\u6ca1\u6709\u5355\u4e00\u6307\u6807\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u8868\u73b0\u4e00\u81f4\uff0c\u4e0d\u540c\u6307\u6807\u5728\u4e0d\u540c\u7c7b\u578b\u7684\u7ec4\u5408\u95ee\u9898\u4e0a\u8868\u73b0\u5404\u5f02\uff0cVQA\u7c7b\u6307\u6807\u5e76\u975e\u603b\u662f\u6700\u4f18\uff0c\u800c\u67d0\u4e9b\u57fa\u4e8e\u5d4c\u5165\u7684\u6307\u6807\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u6587\u672c-\u56fe\u50cf\u751f\u6210\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u8bc4\u4f30\u8f93\u51fa\u662f\u5426\u771f\u6b63\u6355\u6349\u63d0\u793a\u4e2d\u7684\u5bf9\u8c61\u3001\u5c5e\u6027\u548c\u5173\u7cfb\u4ecd\u662f\u6838\u5fc3\u6311\u6218\u3002\u5f53\u524d\u8bc4\u4f30\u4e25\u91cd\u4f9d\u8d56\u81ea\u52a8\u5316\u6307\u6807\uff0c\u4f46\u8fd9\u4e9b\u6307\u6807\u5f80\u5f80\u57fa\u4e8e\u60ef\u4f8b\u6216\u6d41\u884c\u5ea6\u91c7\u7528\uff0c\u800c\u975e\u7ecf\u8fc7\u4eba\u7c7b\u5224\u65ad\u9a8c\u8bc1\u3002", "method": "\u5bf9\u5e7f\u6cdb\u4f7f\u7528\u7684\u7ec4\u5408\u6587\u672c-\u56fe\u50cf\u8bc4\u4f30\u6307\u6807\u8fdb\u884c\u5e7f\u6cdb\u7814\u7a76\uff0c\u8d85\u8d8a\u7b80\u5355\u76f8\u5173\u6027\u5206\u6790\uff0c\u68c0\u67e5\u5b83\u4eec\u5728\u4e0d\u540c\u7ec4\u5408\u6311\u6218\u4e2d\u7684\u884c\u4e3a\uff0c\u5e76\u6bd4\u8f83\u4e0d\u540c\u6307\u6807\u5bb6\u65cf\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\u6ca1\u6709\u5355\u4e00\u6307\u6807\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u8868\u73b0\u4e00\u81f4\uff1a\u6027\u80fd\u968f\u7ec4\u5408\u95ee\u9898\u7c7b\u578b\u800c\u53d8\u5316\u3002VQA\u7c7b\u6307\u6807\u867d\u7136\u6d41\u884c\u4f46\u5e76\u975e\u603b\u662f\u6700\u4f18\uff0c\u800c\u67d0\u4e9b\u57fa\u4e8e\u5d4c\u5165\u7684\u6307\u6807\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u5f3a\u3002\u4ec5\u56fe\u50cf\u6307\u6807\u5bf9\u7ec4\u5408\u8bc4\u4f30\u8d21\u732e\u5f88\u5c0f\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u4ed4\u7ec6\u548c\u900f\u660e\u9009\u62e9\u6307\u6807\u7684\u91cd\u8981\u6027\uff0c\u65e2\u662f\u4e3a\u4e86\u53ef\u4fe1\u7684\u8bc4\u4f30\uff0c\u4e5f\u662f\u4e3a\u4e86\u5b83\u4eec\u5728\u751f\u6210\u4e2d\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\u7684\u4f7f\u7528\u3002"}}
{"id": "2509.21263", "pdf": "https://arxiv.org/pdf/2509.21263", "abs": "https://arxiv.org/abs/2509.21263", "authors": ["Songlin Yang", "Tianyi Wei", "Yushi Lan", "Zeqi Xiao", "Anyi Rao", "Xingang Pan"], "title": "Dense Semantic Matching with VGGT Prior", "categories": ["cs.CV"], "comment": null, "summary": "Semantic matching aims to establish pixel-level correspondences between instances of the same category and represents a fundamental task in computer vision. Existing approaches suffer from two limitations: (i) Geometric Ambiguity: Their reliance on 2D foundation model features (e.g., Stable Diffusion, DINO) often fails to disambiguate symmetric structures, requiring extra fine-tuning yet lacking generalization; (ii) Nearest-Neighbor Rule: Their pixel-wise matching ignores cross-image invisibility and neglects manifold preservation. These challenges call for geometry-aware pixel descriptors and holistic dense correspondence mechanisms. Inspired by recent advances in 3D geometric foundation models, we turn to VGGT, which provides geometry-grounded features and holistic dense matching capabilities well aligned with these needs. However, directly transferring VGGT is challenging, as it was originally designed for geometry matching within cross views of a single instance, misaligned with cross-instance semantic matching, and further hindered by the scarcity of dense semantic annotations. To address this, we propose an approach that (i) retains VGGT's intrinsic strengths by reusing early feature stages, fine-tuning later ones, and adding a semantic head for bidirectional correspondences; and (ii) adapts VGGT to the semantic matching scenario under data scarcity through cycle-consistent training strategy, synthetic data augmentation, and progressive training recipe with aliasing artifact mitigation. Extensive experiments demonstrate that our approach achieves superior geometry awareness, matching reliability, and manifold preservation, outperforming previous baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u51e0\u4f55\u57fa\u7840\u6a21\u578bVGGT\u7684\u8bed\u4e49\u5339\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fdd\u7559VGGT\u65e9\u671f\u7279\u5f81\u3001\u5fae\u8c03\u540e\u671f\u5c42\u3001\u6dfb\u52a0\u8bed\u4e49\u5934\uff0c\u5e76\u7ed3\u5408\u5faa\u73af\u4e00\u81f4\u6027\u8bad\u7ec3\u3001\u5408\u6210\u6570\u636e\u589e\u5f3a\u7b49\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u51e0\u4f55\u6a21\u7cca\u6027\u548c\u6700\u8fd1\u90bb\u89c4\u5219\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49\u5339\u914d\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u51e0\u4f55\u6a21\u7cca\u6027\uff08\u4f9d\u8d562D\u57fa\u7840\u6a21\u578b\u7279\u5f81\u96be\u4ee5\u533a\u5206\u5bf9\u79f0\u7ed3\u6784\uff09\u548c\u6700\u8fd1\u90bb\u89c4\u5219\u9650\u5236\uff08\u50cf\u7d20\u7ea7\u5339\u914d\u5ffd\u7565\u8de8\u56fe\u50cf\u4e0d\u53ef\u89c1\u6027\u548c\u6d41\u5f62\u4fdd\u6301\uff09\u3002\u9700\u8981\u51e0\u4f55\u611f\u77e5\u7684\u50cf\u7d20\u63cf\u8ff0\u7b26\u548c\u6574\u4f53\u5bc6\u96c6\u5bf9\u5e94\u673a\u5236\u3002", "method": "\u5229\u7528VGGT\u7684\u51e0\u4f55\u57fa\u7840\u7279\u5f81\u548c\u6574\u4f53\u5bc6\u96c6\u5339\u914d\u80fd\u529b\uff0c\u4fdd\u7559\u5176\u65e9\u671f\u7279\u5f81\u9636\u6bb5\uff0c\u5fae\u8c03\u540e\u671f\u5c42\uff0c\u6dfb\u52a0\u8bed\u4e49\u5934\u5b9e\u73b0\u53cc\u5411\u5bf9\u5e94\u3002\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u91c7\u7528\u5faa\u73af\u4e00\u81f4\u6027\u8bad\u7ec3\u7b56\u7565\u3001\u5408\u6210\u6570\u636e\u589e\u5f3a\u548c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u51e0\u4f55\u611f\u77e5\u3001\u5339\u914d\u53ef\u9760\u6027\u548c\u6d41\u5f62\u4fdd\u6301\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u6709\u6548\u5229\u75283D\u51e0\u4f55\u57fa\u7840\u6a21\u578bVGGT\u5e76\u7ed3\u5408\u9002\u5f53\u7684\u9002\u5e94\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8bed\u4e49\u5339\u914d\u4e2d\u7684\u51e0\u4f55\u6a21\u7cca\u6027\u548c\u6700\u8fd1\u90bb\u89c4\u5219\u9650\u5236\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u7684\u50cf\u7d20\u7ea7\u5bf9\u5e94\u5173\u7cfb\u3002"}}
{"id": "2509.21265", "pdf": "https://arxiv.org/pdf/2509.21265", "abs": "https://arxiv.org/abs/2509.21265", "authors": ["Xinyu Liu", "Guolei Sun", "Cheng Wang", "Yixuan Yuan", "Ender Konukoglu"], "title": "MedVSR: Medical Video Super-Resolution with Cross State-Space Propagation", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV 2025", "summary": "High-resolution (HR) medical videos are vital for accurate diagnosis, yet are hard to acquire due to hardware limitations and physiological constraints. Clinically, the collected low-resolution (LR) medical videos present unique challenges for video super-resolution (VSR) models, including camera shake, noise, and abrupt frame transitions, which result in significant optical flow errors and alignment difficulties. Additionally, tissues and organs exhibit continuous and nuanced structures, but current VSR models are prone to introducing artifacts and distorted features that can mislead doctors. To this end, we propose MedVSR, a tailored framework for medical VSR. It first employs Cross State-Space Propagation (CSSP) to address the imprecise alignment by projecting distant frames as control matrices within state-space models, enabling the selective propagation of consistent and informative features to neighboring frames for effective alignment. Moreover, we design an Inner State-Space Reconstruction (ISSR) module that enhances tissue structures and reduces artifacts with joint long-range spatial feature learning and large-kernel short-range information aggregation. Experiments across four datasets in diverse medical scenarios, including endoscopy and cataract surgeries, show that MedVSR significantly outperforms existing VSR models in reconstruction performance and efficiency. Code released at https://github.com/CUHK-AIM-Group/MedVSR.", "AI": {"tldr": "\u63d0\u51faMedVSR\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u533b\u5b66\u89c6\u9891\u8d85\u5206\u8fa8\u7387\uff0c\u901a\u8fc7\u8de8\u72b6\u6001\u7a7a\u95f4\u4f20\u64ad\u89e3\u51b3\u5bf9\u9f50\u95ee\u9898\uff0c\u5185\u90e8\u72b6\u6001\u7a7a\u95f4\u91cd\u5efa\u6a21\u5757\u589e\u5f3a\u7ec4\u7ec7\u7ed3\u6784\u5e76\u51cf\u5c11\u4f2a\u5f71\uff0c\u5728\u591a\u4e2a\u533b\u5b66\u573a\u666f\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u533b\u5b66\u89c6\u9891\u5bf9\u51c6\u786e\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u786c\u4ef6\u9650\u5236\u548c\u751f\u7406\u7ea6\u675f\u4f7f\u5176\u96be\u4ee5\u83b7\u53d6\u3002\u73b0\u6709VSR\u6a21\u578b\u5728\u5904\u7406\u533b\u5b66\u89c6\u9891\u65f6\u9762\u4e34\u76f8\u673a\u6296\u52a8\u3001\u566a\u58f0\u3001\u5e27\u95f4\u7a81\u53d8\u7b49\u72ec\u7279\u6311\u6218\uff0c\u5bfc\u81f4\u5149\u6d41\u8bef\u5dee\u548c\u5bf9\u9f50\u56f0\u96be\uff0c\u4e14\u5bb9\u6613\u5f15\u5165\u4f2a\u5f71\u548c\u626d\u66f2\u7279\u5f81\u8bef\u5bfc\u533b\u751f\u8bca\u65ad\u3002", "method": "\u63d0\u51faMedVSR\u6846\u67b6\uff1a1\uff09\u8de8\u72b6\u6001\u7a7a\u95f4\u4f20\u64ad(CSSP)\u901a\u8fc7\u5c06\u8fdc\u8ddd\u79bb\u5e27\u6295\u5f71\u4e3a\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e2d\u7684\u63a7\u5236\u77e9\u9635\uff0c\u9009\u62e9\u6027\u5730\u4f20\u64ad\u4e00\u81f4\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u7279\u5f81\u5230\u76f8\u90bb\u5e27\u5b9e\u73b0\u6709\u6548\u5bf9\u9f50\uff1b2\uff09\u5185\u90e8\u72b6\u6001\u7a7a\u95f4\u91cd\u5efa(ISSR)\u6a21\u5757\u901a\u8fc7\u8054\u5408\u957f\u8ddd\u79bb\u7a7a\u95f4\u7279\u5f81\u5b66\u4e60\u548c\u5927\u6838\u77ed\u8ddd\u79bb\u4fe1\u606f\u805a\u5408\u6765\u589e\u5f3a\u7ec4\u7ec7\u7ed3\u6784\u5e76\u51cf\u5c11\u4f2a\u5f71\u3002", "result": "\u5728\u5305\u62ec\u5185\u7aa5\u955c\u548c\u767d\u5185\u969c\u624b\u672f\u5728\u5185\u7684\u591a\u4e2a\u533b\u5b66\u573a\u666f\u7684\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0cMedVSR\u5728\u91cd\u5efa\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684VSR\u6a21\u578b\u3002", "conclusion": "MedVSR\u662f\u9488\u5bf9\u533b\u5b66\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u5b9a\u5236\u7684\u6709\u6548\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u533b\u5b66\u89c6\u9891\u7279\u6709\u7684\u5bf9\u9f50\u95ee\u9898\u548c\u4f2a\u5f71\u95ee\u9898\uff0c\u5728\u591a\u79cd\u533b\u5b66\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2509.21309", "pdf": "https://arxiv.org/pdf/2509.21309", "abs": "https://arxiv.org/abs/2509.21309", "authors": ["Yu Yuan", "Xijun Wang", "Tharindu Wickremasinghe", "Zeeshan Nadir", "Bole Ma", "Stanley H. Chan"], "title": "NewtonGen: Physics-Consistent and Controllable Text-to-Video Generation via Neural Newtonian Dynamics", "categories": ["cs.CV"], "comment": "All data and code is available at   https://github.com/pandayuanyu/NewtonGen", "summary": "A primary bottleneck in large-scale text-to-video generation today is physical consistency and controllability. Despite recent advances, state-of-the-art models often produce unrealistic motions, such as objects falling upward, or abrupt changes in velocity and direction. Moreover, these models lack precise parameter control, struggling to generate physically consistent dynamics under different initial conditions. We argue that this fundamental limitation stems from current models learning motion distributions solely from appearance, while lacking an understanding of the underlying dynamics. In this work, we propose NewtonGen, a framework that integrates data-driven synthesis with learnable physical principles. At its core lies trainable Neural Newtonian Dynamics (NND), which can model and predict a variety of Newtonian motions, thereby injecting latent dynamical constraints into the video generation process. By jointly leveraging data priors and dynamical guidance, NewtonGen enables physically consistent video synthesis with precise parameter control.", "AI": {"tldr": "\u63d0\u51fa\u4e86NewtonGen\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u8bad\u7ec3\u7684\u795e\u7ecf\u725b\u987f\u52a8\u529b\u5b66\u5c06\u7269\u7406\u7ea6\u675f\u6ce8\u5165\u89c6\u9891\u751f\u6210\u8fc7\u7a0b\uff0c\u89e3\u51b3\u73b0\u6709\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u7269\u7406\u4e00\u81f4\u6027\u548c\u53ef\u63a7\u6027\u65b9\u9762\u7684\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u5b58\u5728\u7269\u7406\u4e00\u81f4\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5982\u7269\u4f53\u5411\u4e0a\u5760\u843d\u3001\u901f\u5ea6\u548c\u65b9\u5411\u7a81\u53d8\u7b49\uff0c\u4e14\u7f3a\u4e4f\u7cbe\u786e\u7684\u53c2\u6570\u63a7\u5236\u80fd\u529b\u3002\u8fd9\u4e9b\u9650\u5236\u6e90\u4e8e\u6a21\u578b\u4ec5\u4ece\u5916\u89c2\u5b66\u4e60\u8fd0\u52a8\u5206\u5e03\uff0c\u800c\u7f3a\u4e4f\u5bf9\u5e95\u5c42\u52a8\u529b\u5b66\u7684\u7406\u89e3\u3002", "method": "\u63d0\u51faNewtonGen\u6846\u67b6\uff0c\u6574\u5408\u6570\u636e\u9a71\u52a8\u5408\u6210\u4e0e\u53ef\u5b66\u4e60\u7269\u7406\u539f\u7406\u3002\u6838\u5fc3\u662f\u53ef\u8bad\u7ec3\u7684\u795e\u7ecf\u725b\u987f\u52a8\u529b\u5b66(NND)\uff0c\u80fd\u591f\u5efa\u6a21\u548c\u9884\u6d4b\u5404\u79cd\u725b\u987f\u8fd0\u52a8\uff0c\u4ece\u800c\u5c06\u6f5c\u5728\u52a8\u529b\u5b66\u7ea6\u675f\u6ce8\u5165\u89c6\u9891\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u901a\u8fc7\u8054\u5408\u5229\u7528\u6570\u636e\u5148\u9a8c\u548c\u52a8\u529b\u5b66\u6307\u5bfc\uff0cNewtonGen\u5b9e\u73b0\u4e86\u7269\u7406\u4e00\u81f4\u7684\u89c6\u9891\u5408\u6210\uff0c\u5e76\u5177\u5907\u7cbe\u786e\u7684\u53c2\u6570\u63a7\u5236\u80fd\u529b\u3002", "conclusion": "NewtonGen\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u7269\u7406\u539f\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u7684\u7269\u7406\u4e00\u81f4\u6027\u548c\u53ef\u63a7\u6027\uff0c\u4e3a\u89e3\u51b3\u5f53\u524d\u6a21\u578b\u5728\u52a8\u529b\u5b66\u7406\u89e3\u65b9\u9762\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2509.21318", "pdf": "https://arxiv.org/pdf/2509.21318", "abs": "https://arxiv.org/abs/2509.21318", "authors": ["Hmrishav Bandyopadhyay", "Rahim Entezari", "Jim Scott", "Reshinth Adithyan", "Yi-Zhe Song", "Varun Jampani"], "title": "SD3.5-Flash: Distribution-Guided Distillation of Generative Flows", "categories": ["cs.CV", "cs.AI"], "comment": "Project Page: https://hmrishavbandy.github.io/sd35flash/", "summary": "We present SD3.5-Flash, an efficient few-step distillation framework that brings high-quality image generation to accessible consumer devices. Our approach distills computationally prohibitive rectified flow models through a reformulated distribution matching objective tailored specifically for few-step generation. We introduce two key innovations: \"timestep sharing\" to reduce gradient noise and \"split-timestep fine-tuning\" to improve prompt alignment. Combined with comprehensive pipeline optimizations like text encoder restructuring and specialized quantization, our system enables both rapid generation and memory-efficient deployment across different hardware configurations. This democratizes access across the full spectrum of devices, from mobile phones to desktop computers. Through extensive evaluation including large-scale user studies, we demonstrate that SD3.5-Flash consistently outperforms existing few-step methods, making advanced generative AI truly accessible for practical deployment.", "AI": {"tldr": "SD3.5-Flash\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u5c11\u6b65\u84b8\u998f\u6846\u67b6\uff0c\u53ef\u5c06\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u5e26\u5230\u6d88\u8d39\u7ea7\u8bbe\u5907\u3002\u901a\u8fc7\u91cd\u65b0\u5236\u5b9a\u7684\u5206\u5e03\u5339\u914d\u76ee\u6807\u3001\u65f6\u95f4\u6b65\u5171\u4eab\u548c\u5206\u65f6\u6b65\u5fae\u8c03\u7b49\u521b\u65b0\uff0c\u7ed3\u5408\u7ba1\u9053\u4f18\u5316\uff0c\u5b9e\u73b0\u5728\u4e0d\u540c\u786c\u4ef6\u4e0a\u7684\u5feb\u901f\u751f\u6210\u548c\u5185\u5b58\u9ad8\u6548\u90e8\u7f72\u3002", "motivation": "\u5c06\u8ba1\u7b97\u5bc6\u96c6\u7684\u6574\u6d41\u6d41\u6a21\u578b\u84b8\u998f\u5230\u6d88\u8d39\u7ea7\u8bbe\u5907\uff0c\u4f7f\u5148\u8fdb\u7684\u751f\u6210AI\u771f\u6b63\u5b9e\u7528\u5316\u90e8\u7f72\uff0c\u4ece\u624b\u673a\u5230\u684c\u9762\u7535\u8111\u90fd\u80fd\u4f7f\u7528\u3002", "method": "\u91c7\u7528\u5c11\u6b65\u84b8\u998f\u6846\u67b6\uff0c\u5f15\u5165\u65f6\u95f4\u6b65\u5171\u4eab\u51cf\u5c11\u68af\u5ea6\u566a\u58f0\uff0c\u5206\u65f6\u6b65\u5fae\u8c03\u6539\u8fdb\u63d0\u793a\u5bf9\u9f50\uff0c\u7ed3\u5408\u6587\u672c\u7f16\u7801\u5668\u91cd\u6784\u548c\u4e13\u7528\u91cf\u5316\u7b49\u7ba1\u9053\u4f18\u5316\u3002", "result": "\u901a\u8fc7\u5927\u89c4\u6a21\u7528\u6237\u7814\u7a76\u8bc4\u4f30\uff0cSD3.5-Flash\u5728\u5c11\u6b65\u65b9\u6cd5\u4e2d\u8868\u73b0\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u751f\u6210AI\u7684\u6c11\u4e3b\u5316\u8bbf\u95ee\uff0c\u4f7f\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u771f\u6b63\u5b9e\u7528\u5316\u90e8\u7f72\u5230\u5404\u7c7b\u8bbe\u5907\u3002"}}
{"id": "2509.20501", "pdf": "https://arxiv.org/pdf/2509.20501", "abs": "https://arxiv.org/abs/2509.20501", "authors": ["Kishor Datta Gupta", "Mohd Ariful Haque", "Marufa Kamal", "Ahmed Rafi Hasan", "Md. Mahfuzur Rahman", "Roy George"], "title": "Beyond Visual Similarity: Rule-Guided Multimodal Clustering with explicit domain rules", "categories": ["cs.LG", "cs.CV"], "comment": "12 pages, 9 figures", "summary": "Traditional clustering techniques often rely solely on similarity in the input data, limiting their ability to capture structural or semantic constraints that are critical in many domains. We introduce the Domain Aware Rule Triggered Variational Autoencoder (DARTVAE), a rule guided multimodal clustering framework that incorporates domain specific constraints directly into the representation learning process. DARTVAE extends the VAE architecture by embedding explicit rules, semantic representations, and data driven features into a unified latent space, while enforcing constraint compliance through rule consistency and violation penalties in the loss function. Unlike conventional clustering methods that rely only on visual similarity or apply rules as post hoc filters, DARTVAE treats rules as first class learning signals. The rules are generated by LLMs, structured into knowledge graphs, and enforced through a loss function combining reconstruction, KL divergence, consistency, and violation penalties. Experiments on aircraft and automotive datasets demonstrate that rule guided clustering produces more operationally meaningful and interpretable clusters for example, isolating UAVs, unifying stealth aircraft, or separating SUVs from sedans while improving traditional clustering metrics. However, the framework faces challenges: LLM generated rules may hallucinate or conflict, excessive rules risk overfitting, and scaling to complex domains increases computational and consistency difficulties. By combining rule encodings with learned representations, DARTVAE achieves more meaningful and consistent clustering outcomes than purely data driven models, highlighting the utility of constraint guided multimodal clustering for complex, knowledge intensive settings.", "AI": {"tldr": "DARTVAE\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c4\u5219\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u805a\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9886\u57df\u7279\u5b9a\u7ea6\u675f\u76f4\u63a5\u5d4c\u5165\u8868\u793a\u5b66\u4e60\u8fc7\u7a0b\uff0c\u7ed3\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548cLLM\u751f\u6210\u7684\u89c4\u5219\uff0c\u5b9e\u73b0\u66f4\u7b26\u5408\u9886\u57df\u8bed\u4e49\u7684\u805a\u7c7b\u3002", "motivation": "\u4f20\u7edf\u805a\u7c7b\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u8f93\u5165\u6570\u636e\u7684\u76f8\u4f3c\u6027\uff0c\u96be\u4ee5\u6355\u6349\u8bb8\u591a\u9886\u57df\u4e2d\u5173\u952e\u7684\u7ed3\u6784\u6216\u8bed\u4e49\u7ea6\u675f\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u76f4\u63a5\u6574\u5408\u9886\u57df\u77e5\u8bc6\u7684\u805a\u7c7b\u65b9\u6cd5\u3002", "method": "\u6269\u5c55VAE\u67b6\u6784\uff0c\u5c06\u663e\u5f0f\u89c4\u5219\u3001\u8bed\u4e49\u8868\u793a\u548c\u6570\u636e\u9a71\u52a8\u7279\u5f81\u5d4c\u5165\u7edf\u4e00\u6f5c\u5728\u7a7a\u95f4\uff0c\u901a\u8fc7\u89c4\u5219\u4e00\u81f4\u6027\u548c\u8fdd\u53cd\u60e9\u7f5a\u5728\u635f\u5931\u51fd\u6570\u4e2d\u5f3a\u5236\u7ea6\u675f\u5408\u89c4\u3002\u89c4\u5219\u7531LLM\u751f\u6210\u5e76\u6784\u5efa\u4e3a\u77e5\u8bc6\u56fe\u8c31\u3002", "result": "\u5728\u98de\u673a\u548c\u6c7d\u8f66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u89c4\u5219\u5f15\u5bfc\u7684\u805a\u7c7b\u4ea7\u751f\u4e86\u66f4\u5177\u64cd\u4f5c\u610f\u4e49\u548c\u53ef\u89e3\u91ca\u6027\u7684\u805a\u7c7b\u7ed3\u679c\uff08\u5982\u5206\u79bb\u65e0\u4eba\u673a\u3001\u7edf\u4e00\u9690\u5f62\u98de\u673a\u3001\u533a\u5206SUV\u548c\u8f7f\u8f66\uff09\uff0c\u540c\u65f6\u6539\u8fdb\u4e86\u4f20\u7edf\u805a\u7c7b\u6307\u6807\u3002", "conclusion": "DARTVAE\u901a\u8fc7\u5c06\u89c4\u5219\u7f16\u7801\u4e0e\u5b66\u4e60\u8868\u793a\u76f8\u7ed3\u5408\uff0c\u6bd4\u7eaf\u6570\u636e\u9a71\u52a8\u6a21\u578b\u5b9e\u73b0\u4e86\u66f4\u6709\u610f\u4e49\u548c\u4e00\u81f4\u7684\u805a\u7c7b\u7ed3\u679c\uff0c\u7a81\u663e\u4e86\u7ea6\u675f\u5f15\u5bfc\u591a\u6a21\u6001\u805a\u7c7b\u5728\u590d\u6742\u77e5\u8bc6\u5bc6\u96c6\u578b\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.20852", "pdf": "https://arxiv.org/pdf/2509.20852", "abs": "https://arxiv.org/abs/2509.20852", "authors": ["Kjersti Engan", "Neel Kanwal", "Anita Yeconia", "Ladislaus Blacy", "Yuda Munyaw", "Estomih Mduma", "Hege Ersdal"], "title": "FHRFormer: A Self-supervised Transformer Approach for Fetal Heart Rate Inpainting and Forecasting", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CV"], "comment": "Submitted to IEEE JBHI", "summary": "Approximately 10\\% of newborns require assistance to initiate breathing at birth, and around 5\\% need ventilation support. Fetal heart rate (FHR) monitoring plays a crucial role in assessing fetal well-being during prenatal care, enabling the detection of abnormal patterns and supporting timely obstetric interventions to mitigate fetal risks during labor. Applying artificial intelligence (AI) methods to analyze large datasets of continuous FHR monitoring episodes with diverse outcomes may offer novel insights into predicting the risk of needing breathing assistance or interventions. Recent advances in wearable FHR monitors have enabled continuous fetal monitoring without compromising maternal mobility. However, sensor displacement during maternal movement, as well as changes in fetal or maternal position, often lead to signal dropouts, resulting in gaps in the recorded FHR data. Such missing data limits the extraction of meaningful insights and complicates automated (AI-based) analysis. Traditional approaches to handle missing data, such as simple interpolation techniques, often fail to preserve the spectral characteristics of the signals. In this paper, we propose a masked transformer-based autoencoder approach to reconstruct missing FHR signals by capturing both spatial and frequency components of the data. The proposed method demonstrates robustness across varying durations of missing data and can be used for signal inpainting and forecasting. The proposed approach can be applied retrospectively to research datasets to support the development of AI-based risk algorithms. In the future, the proposed method could be integrated into wearable FHR monitoring devices to achieve earlier and more robust risk detection.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u63a9\u7801\u53d8\u6362\u5668\u81ea\u7f16\u7801\u5668\u7684\u65b9\u6cd5\u6765\u91cd\u5efa\u7f3a\u5931\u7684\u80ce\u513f\u5fc3\u7387\u4fe1\u53f7\uff0c\u8be5\u65b9\u6cd5\u80fd\u6355\u6349\u6570\u636e\u7684\u7a7a\u95f4\u548c\u9891\u7387\u7279\u5f81\uff0c\u9002\u7528\u4e8e\u4fe1\u53f7\u4fee\u590d\u548c\u9884\u6d4b\u3002", "motivation": "\u7ea610%\u7684\u65b0\u751f\u513f\u9700\u8981\u547c\u5438\u8f85\u52a9\uff0c5%\u9700\u8981\u901a\u6c14\u652f\u6301\u3002\u80ce\u513f\u5fc3\u7387\u76d1\u6d4b\u5728\u4ea7\u524d\u62a4\u7406\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u611f\u5668\u4f4d\u79fb\u548c\u4f4d\u7f6e\u53d8\u5316\u5e38\u5bfc\u81f4\u4fe1\u53f7\u4e22\u5931\uff0c\u9650\u5236AI\u5206\u6790\u7684\u51c6\u786e\u6027\u3002\u4f20\u7edf\u63d2\u503c\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u6301\u4fe1\u53f7\u7684\u9891\u8c31\u7279\u6027\u3002", "method": "\u4f7f\u7528\u63a9\u7801\u53d8\u6362\u5668\u81ea\u7f16\u7801\u5668\u65b9\u6cd5\u91cd\u5efa\u7f3a\u5931\u7684\u80ce\u513f\u5fc3\u7387\u4fe1\u53f7\uff0c\u8be5\u65b9\u6cd5\u80fd\u540c\u65f6\u6355\u6349\u6570\u636e\u7684\u7a7a\u95f4\u548c\u9891\u7387\u6210\u5206\uff0c\u5bf9\u4e0d\u540c\u7a0b\u5ea6\u7684\u7f3a\u5931\u6570\u636e\u5177\u6709\u9c81\u68d2\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u7f3a\u5931\u6301\u7eed\u65f6\u95f4\u4e0b\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u53ef\u7528\u4e8e\u4fe1\u53f7\u4fee\u590d\u548c\u9884\u6d4b\uff0c\u652f\u6301AI\u98ce\u9669\u7b97\u6cd5\u5f00\u53d1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u56de\u987e\u6027\u7814\u7a76\u6570\u636e\u96c6\uff0c\u672a\u6765\u53ef\u96c6\u6210\u5230\u53ef\u7a7f\u6234\u80ce\u513f\u5fc3\u7387\u76d1\u6d4b\u8bbe\u5907\u4e2d\uff0c\u5b9e\u73b0\u66f4\u65e9\u3001\u66f4\u7a33\u5065\u7684\u98ce\u9669\u68c0\u6d4b\u3002"}}
{"id": "2509.21027", "pdf": "https://arxiv.org/pdf/2509.21027", "abs": "https://arxiv.org/abs/2509.21027", "authors": ["Sibo Li", "Qianyue Hao", "Yu Shang", "Yong Li"], "title": "KeyWorld: Key Frame Reasoning Enables Effective and Efficient World Models", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Robotic world models are a promising paradigm for forecasting future environment states, yet their inference speed and the physical plausibility of generated trajectories remain critical bottlenecks, limiting their real-world applications. This stems from the redundancy of the prevailing frame-to-frame generation approach, where the model conducts costly computation on similar frames, as well as neglecting the semantic importance of key transitions. To address this inefficiency, we propose KeyWorld, a framework that improves text-conditioned robotic world models by concentrating transformers computation on a few semantic key frames while employing a lightweight convolutional model to fill the intermediate frames. Specifically, KeyWorld first identifies significant transitions by iteratively simplifying the robot's motion trajectories, obtaining the ground truth key frames. Then, a DiT model is trained to reason and generate these physically meaningful key frames from textual task descriptions. Finally, a lightweight interpolator efficiently reconstructs the full video by inpainting all intermediate frames. Evaluations on the LIBERO benchmark demonstrate that KeyWorld achieves a 5.68$\\times$ acceleration compared to the frame-to-frame generation baseline, and focusing on the motion-aware key frames further contributes to the physical validity of the generated videos, especially on complex tasks. Our approach highlights a practical path toward deploying world models in real-time robotic control and other domains requiring both efficient and effective world models. Code is released at https://anonymous.4open.science/r/Keyworld-E43D.", "AI": {"tldr": "KeyWorld\u662f\u4e00\u4e2a\u6539\u8fdb\u6587\u672c\u6761\u4ef6\u673a\u5668\u4eba\u4e16\u754c\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06Transformer\u8ba1\u7b97\u96c6\u4e2d\u5728\u5c11\u91cf\u8bed\u4e49\u5173\u952e\u5e27\u4e0a\uff0c\u540c\u65f6\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5377\u79ef\u6a21\u578b\u586b\u5145\u4e2d\u95f4\u5e27\uff0c\u5b9e\u73b0\u4e865.68\u500d\u7684\u52a0\u901f\uff0c\u5e76\u63d0\u9ad8\u4e86\u751f\u6210\u89c6\u9891\u7684\u7269\u7406\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u4e16\u754c\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\u548c\u751f\u6210\u8f68\u8ff9\u7684\u7269\u7406\u5408\u7406\u6027\u662f\u9650\u5236\u5176\u5b9e\u9645\u5e94\u7528\u7684\u5173\u952e\u74f6\u9888\uff0c\u4e3b\u8981\u6e90\u4e8e\u9010\u5e27\u751f\u6210\u65b9\u6cd5\u7684\u5197\u4f59\u8ba1\u7b97\u548c\u5bf9\u5173\u952e\u8f6c\u6362\u8bed\u4e49\u91cd\u8981\u6027\u7684\u5ffd\u89c6\u3002", "method": "KeyWorld\u9996\u5148\u901a\u8fc7\u8fed\u4ee3\u7b80\u5316\u673a\u5668\u4eba\u8fd0\u52a8\u8f68\u8ff9\u8bc6\u522b\u91cd\u8981\u8f6c\u6362\uff0c\u83b7\u53d6\u771f\u5b9e\u5173\u952e\u5e27\uff1b\u7136\u540e\u8bad\u7ec3DiT\u6a21\u578b\u4ece\u6587\u672c\u4efb\u52a1\u63cf\u8ff0\u63a8\u7406\u751f\u6210\u8fd9\u4e9b\u7269\u7406\u610f\u4e49\u5173\u952e\u5e27\uff1b\u6700\u540e\u4f7f\u7528\u8f7b\u91cf\u7ea7\u63d2\u503c\u5668\u9ad8\u6548\u91cd\u5efa\u5b8c\u6574\u89c6\u9891\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKeyWorld\u76f8\u6bd4\u9010\u5e27\u751f\u6210\u57fa\u7ebf\u5b9e\u73b0\u4e865.68\u500d\u52a0\u901f\uff0c\u5173\u6ce8\u8fd0\u52a8\u611f\u77e5\u5173\u952e\u5e27\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u751f\u6210\u89c6\u9891\u7684\u7269\u7406\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5728\u5b9e\u65f6\u673a\u5668\u4eba\u63a7\u5236\u548c\u5176\u4ed6\u9700\u8981\u9ad8\u6548\u6709\u6548\u4e16\u754c\u6a21\u578b\u7684\u9886\u57df\u4e2d\u90e8\u7f72\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2509.21167", "pdf": "https://arxiv.org/pdf/2509.21167", "abs": "https://arxiv.org/abs/2509.21167", "authors": ["Nicola Novello", "Federico Fontana", "Luigi Cinque", "Deniz Gunduz", "Andrea M. Tonello"], "title": "A Unified Framework for Diffusion Model Unlearning with f-Divergence", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Machine unlearning aims to remove specific knowledge from a trained model. While diffusion models (DMs) have shown remarkable generative capabilities, existing unlearning methods for text-to-image (T2I) models often rely on minimizing the mean squared error (MSE) between the output distribution of a target and an anchor concept. We show that this MSE-based approach is a special case of a unified $f$-divergence-based framework, in which any $f$-divergence can be utilized. We analyze the benefits of using different $f$-divergences, that mainly impact the convergence properties of the algorithm and the quality of unlearning. The proposed unified framework offers a flexible paradigm that allows to select the optimal divergence for a specific application, balancing different trade-offs between aggressive unlearning and concept preservation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8ef-\u6563\u5ea6\u7684\u7edf\u4e00\u6846\u67b6\u7528\u4e8e\u6269\u6563\u6a21\u578b\u9057\u5fd8\uff0c\u8bc1\u660e\u4f20\u7edfMSE\u65b9\u6cd5\u662f\u8be5\u6846\u67b6\u7684\u7279\u4f8b\uff0c\u5206\u6790\u4e86\u4e0d\u540cf-\u6563\u5ea6\u5bf9\u9057\u5fd8\u8d28\u91cf\u548c\u6536\u655b\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u9057\u5fd8\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u6700\u5c0f\u5316\u76ee\u6807\u6982\u5ff5\u548c\u951a\u6982\u5ff5\u8f93\u51fa\u5206\u5e03\u4e4b\u95f4\u7684\u5747\u65b9\u8bef\u5dee\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8ef-\u6563\u5ea6\u7684\u7edf\u4e00\u9057\u5fd8\u6846\u67b6\uff0c\u4efb\u4f55f-\u6563\u5ea6\u90fd\u53ef\u4ee5\u88ab\u5229\u7528\uff0c\u63d0\u4f9b\u4e86\u9009\u62e9\u6700\u4f18\u6563\u5ea6\u7684\u7075\u6d3b\u6027\u3002", "result": "\u5206\u6790\u8868\u660e\u4e0d\u540cf-\u6563\u5ea6\u4e3b\u8981\u5f71\u54cd\u7b97\u6cd5\u7684\u6536\u655b\u6027\u548c\u9057\u5fd8\u8d28\u91cf\uff0c\u5728\u6fc0\u8fdb\u9057\u5fd8\u548c\u6982\u5ff5\u4fdd\u7559\u4e4b\u95f4\u63d0\u4f9b\u4e0d\u540c\u6743\u8861\u3002", "conclusion": "\u8be5\u7edf\u4e00\u6846\u67b6\u4e3a\u7279\u5b9a\u5e94\u7528\u9009\u62e9\u6700\u4f18\u6563\u5ea6\u63d0\u4f9b\u4e86\u7075\u6d3b\u8303\u5f0f\uff0c\u5e73\u8861\u4e86\u4e0d\u540c\u9057\u5fd8\u9700\u6c42\u3002"}}
