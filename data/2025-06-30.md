<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 3]
- [cs.CV](#cs.CV) [Total: 19]
- [cs.LG](#cs.LG) [Total: 1]
- [eess.IV](#eess.IV) [Total: 6]
- [physics.optics](#physics.optics) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded Scenes](https://arxiv.org/abs/2506.21629)
*Chenhao Zhang,Yezhi Shen,Fengqing Zhu*

Main category: cs.GR

TL;DR: 提出了一种结合ICP与优化的方法，用于大范围场景下的相机姿态估计和场景重建，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决神经渲染方法（如NeRF和3DGS）依赖预处理相机姿态和3D结构先验的问题，尤其是在户外场景中。

Method: 结合迭代最近点（ICP）和基于优化的细化方法进行相机姿态估计，并引入基于体素的场景密集化方法指导重建。

Result: 实验表明，ICP-3DGS在相机姿态估计和新视角合成方面优于现有方法，适用于不同规模的室内外场景。

Conclusion: 提出的方法有效解决了大范围场景下的相机姿态估计和重建问题，代码已开源。

Abstract: In recent years, neural rendering methods such as NeRFs and 3D Gaussian Splatting (3DGS) have made significant progress in scene reconstruction and novel view synthesis. However, they heavily rely on preprocessed camera poses and 3D structural priors from structure-from-motion (SfM), which are challenging to obtain in outdoor scenarios. To address this challenge, we propose to incorporate Iterative Closest Point (ICP) with optimization-based refinement to achieve accurate camera pose estimation under large camera movements. Additionally, we introduce a voxel-based scene densification approach to guide the reconstruction in large-scale scenes. Experiments demonstrate that our approach ICP-3DGS outperforms existing methods in both camera pose estimation and novel view synthesis across indoor and outdoor scenes of various scales. Source code is available at https://github.com/Chenhao-Z/ICP-3DGS.

</details>


### [2] [SkinningGS: Editable Dynamic Human Scene Reconstruction Using Gaussian Splatting Based on a Skinning Model](https://arxiv.org/abs/2506.21632)
*Da Li,Donggang Jia,Markus Hadwiger,Ivan Viola*

Main category: cs.GR

TL;DR: 提出了一种基于点云解耦和联合优化的方法，用于从单目视频中重建交互式人体和背景，同时保留人体运动的交互性。


<details>
  <summary>Details</summary>
Motivation: 解决从单目视频中重建动态人体场景时，背景和人体解耦重建的挑战，同时保持人体运动的交互性。

Method: 采用点云解耦和联合优化策略，引入位置纹理细分SMPL模型表面，并结合CNN预测人体点云特征。

Result: 在重建质量上超越HUGS，减少GPU资源消耗，实时渲染速度达100 FPS，并可扩展至动物场景重建。

Conclusion: 该方法高效且通用，适用于人体和动物场景的高质量重建。

Abstract: Reconstructing an interactive human avatar and the background from a monocular video of a dynamic human scene is highly challenging. In this work we adopt a strategy of point cloud decoupling and joint optimization to achieve the decoupled reconstruction of backgrounds and human bodies while preserving the interactivity of human motion. We introduce a position texture to subdivide the Skinned Multi-Person Linear (SMPL) body model's surface and grow the human point cloud. To capture fine details of human dynamics and deformations, we incorporate a convolutional neural network structure to predict human body point cloud features based on texture. This strategy makes our approach free of hyperparameter tuning for densification and efficiently represents human points with half the point cloud of HUGS. This approach ensures high-quality human reconstruction and reduces GPU resource consumption during training. As a result, our method surpasses the previous state-of-the-art HUGS in reconstruction metrics while maintaining the ability to generalize to novel poses and views. Furthermore, our technique achieves real-time rendering at over 100 FPS, $\sim$6$\times$ the HUGS speed using only Linear Blend Skinning (LBS) weights for human transformation. Additionally, this work demonstrates that this framework can be extended to animal scene reconstruction when an accurately-posed model of an animal is available.

</details>


### [3] [SAR-GS: 3D Gaussian Splatting for Synthetic Aperture Radar Target Reconstruction](https://arxiv.org/abs/2506.21633)
*Aobo Li,Zhengxin Lei,Jiangtao Wei,Feng Xu*

Main category: cs.GR

TL;DR: 本文提出了一种名为SDGR的新方法，结合高斯泼溅和映射投影算法，用于SAR目标的三维重建，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: SAR图像中复杂的电磁散射机制给目标重建带来挑战，而3D高斯泼溅在光学领域的成功启发了该方法的设计。

Method: 结合高斯泼溅与映射投影算法，通过SDGR生成模拟SAR图像，并优化高斯基元参数，使用自定义CUDA梯度流加速计算。

Result: 实验表明，SDGR能有效重建目标的几何结构和散射特性，在模拟和真实数据集上均表现良好。

Conclusion: SDGR为SAR成像领域的3D重建提供了一种新解决方案。

Abstract: Three-dimensional target reconstruction from synthetic aperture radar (SAR) imagery is crucial for interpreting complex scattering information in SAR data. However, the intricate electromagnetic scattering mechanisms inherent to SAR imaging pose significant reconstruction challenges. Inspired by the remarkable success of 3D Gaussian Splatting (3D-GS) in optical domain reconstruction, this paper presents a novel SAR Differentiable Gaussian Splatting Rasterizer (SDGR) specifically designed for SAR target reconstruction. Our approach combines Gaussian splatting with the Mapping and Projection Algorithm to compute scattering intensities of Gaussian primitives and generate simulated SAR images through SDGR. Subsequently, the loss function between the rendered image and the ground truth image is computed to optimize the Gaussian primitive parameters representing the scene, while a custom CUDA gradient flow is employed to replace automatic differentiation for accelerated gradient computation. Through experiments involving the rendering of simplified architectural targets and SAR images of multiple vehicle targets, we validate the imaging rationality of SDGR on simulated SAR imagery. Furthermore, the effectiveness of our method for target reconstruction is demonstrated on both simulated and real-world datasets containing multiple vehicle targets, with quantitative evaluations conducted to assess its reconstruction performance. Experimental results indicate that our approach can effectively reconstruct the geometric structures and scattering properties of targets, thereby providing a novel solution for 3D reconstruction in the field of SAR imaging.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [TanDiT: Tangent-Plane Diffusion Transformer for High-Quality 360° Panorama Generation](https://arxiv.org/abs/2506.21681)
*Hakan Çapuk,Andrew Bond,Muhammed Burak Kızıl,Emir Göçen,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

TL;DR: TanDiT是一种通过生成覆盖360度视图的切平面图像网格来合成全景场景的方法，解决了现有模型在全景图像生成中的几何失真和循环一致性挑战。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型在全景图像生成中存在几何失真和循环一致性问题，TanDiT旨在解决这些问题并提升全景图像质量。

Method: TanDiT使用统一的扩散模型在单次去噪迭代中同时生成切平面图像，并提出模型无关的后处理步骤以增强全局一致性。

Result: 实验表明，TanDiT能有效泛化到训练数据之外，处理复杂文本提示，并与多种生成模型结合生成高质量全景图像。

Conclusion: TanDiT为全景图像生成提供了一种高效且通用的解决方案，并通过新指标和基准推动了该领域的发展。

Abstract: Recent advances in image generation have led to remarkable improvements in synthesizing perspective images. However, these models still struggle with panoramic image generation due to unique challenges, including varying levels of geometric distortion and the requirement for seamless loop-consistency. To address these issues while leveraging the strengths of the existing models, we introduce TanDiT, a method that synthesizes panoramic scenes by generating grids of tangent-plane images covering the entire 360$^\circ$ view. Unlike previous methods relying on multiple diffusion branches, TanDiT utilizes a unified diffusion model trained to produce these tangent-plane images simultaneously within a single denoising iteration. Furthermore, we propose a model-agnostic post-processing step specifically designed to enhance global coherence across the generated panoramas. To accurately assess panoramic image quality, we also present two specialized metrics, TangentIS and TangentFID, and provide a comprehensive benchmark comprising captioned panoramic datasets and standardized evaluation scripts. Extensive experiments demonstrate that our method generalizes effectively beyond its training data, robustly interprets detailed and complex text prompts, and seamlessly integrates with various generative models to yield high-quality, diverse panoramic images.

</details>


### [5] [Elucidating and Endowing the Diffusion Training Paradigm for General Image Restoration](https://arxiv.org/abs/2506.21722)
*Xin Lu,Xueyang Fu,Jie Xiao,Zihao Fan,Yurui Zhu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 本文提出了一种将扩散训练范式融入普通图像修复（IR）框架的新方法，通过系统分析时间步依赖、网络层次、噪声级别关系和多任务相关性，优化了IR网络的泛化能力和多任务性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像修复任务中表现出强大的生成能力，但其复杂架构和迭代过程限制了实际应用，现有方法忽视了将扩散训练范式融入普通IR框架的潜力。

Method: 通过分析时间步依赖、网络层次等关键因素，提出扩散训练支持的IR框架，并引入正则化策略和对齐扩散目标与IR任务的方法，同时开发增量训练范式和任务特定适配器。

Result: 实验表明，该方法显著提高了单任务IR的泛化能力，并在多任务统一IR中表现优异，且能无缝集成到现有IR架构中。

Conclusion: 提出的框架成功将扩散训练范式融入普通IR训练，提升了单任务和多任务的性能，具有广泛的应用潜力。

Abstract: While diffusion models demonstrate strong generative capabilities in image restoration (IR) tasks, their complex architectures and iterative processes limit their practical application compared to mainstream reconstruction-based general ordinary IR networks. Existing approaches primarily focus on optimizing network architecture and diffusion paths but overlook the integration of the diffusion training paradigm within general ordinary IR frameworks. To address these challenges, this paper elucidates key principles for adapting the diffusion training paradigm to general IR training through systematic analysis of time-step dependencies, network hierarchies, noise-level relationships, and multi-restoration task correlations, proposing a new IR framework supported by diffusion-based training. To enable IR networks to simultaneously restore images and model generative representations, we introduce a series of regularization strategies that align diffusion objectives with IR tasks, improving generalization in single-task scenarios. Furthermore, recognizing that diffusion-based generation exerts varying influences across different IR tasks, we develop an incremental training paradigm and task-specific adaptors, further enhancing performance in multi-task unified IR. Experiments demonstrate that our method significantly improves the generalization of IR networks in single-task IR and achieves superior performance in multi-task unified IR. Notably, the proposed framework can be seamlessly integrated into existing general IR architectures.

</details>


### [6] [Exploring Image Generation via Mutually Exclusive Probability Spaces and Local Correlation Hypothesis](https://arxiv.org/abs/2506.21731)
*Chenqiu Zhao,Anup Basu*

Main category: cs.CV

TL;DR: 论文提出了MESP和LCH两个理论框架，探讨概率生成模型的局限性，即全局分布学习导致记忆而非生成行为。MESP源于对VAE的重新思考，提出基于重叠系数的下界，并设计BL-AE和ARVM模型。实验显示ARVM性能优越，但存在记忆问题，LCH假设局部相关性可提升生成能力。


<details>
  <summary>Details</summary>
Motivation: 研究概率生成模型在全局分布学习中的局限性，发现其倾向于记忆而非生成行为，试图通过新理论框架解决这一问题。

Method: 提出MESP框架，基于VAE的潜在变量重叠问题设计BL-AE和ARVM模型；进一步提出LCH假设，强调局部相关性对生成能力的影响。

Result: ARVM在标准数据集上取得竞争性FID分数，但发现其表现更多源于记忆而非生成；LCH假设通过实验验证局部相关性的重要性。

Conclusion: MESP和LCH框架揭示了概率生成模型的局限性，并提出通过局部相关性提升生成能力的可能方向。

Abstract: We propose two theoretical frameworks, the Mutually Exclusive Probability Space (MESP) and the Local Correlation Hypothesis (LCH), to explore a potential limitation in probabilistic generative models; namely that learning global distributions leads to memorization rather than generative behavior. MESP emerges from our rethinking of the Variational Autoencoder (VAE). We observe that latent variable distributions in VAE exhibit overlap, which leads to an optimization conflict between the reconstruction loss and KL-divergence loss. A lower bound based on the overlap coefficient is proposed. We refer to this phenomenon as Mutually Exclusive Probability Spaces. Based on MESP, a Binary Latent Autoencoder (BL-AE) is proposed to encode images into binary latent representations. These binary latents are used as the input to our Autoregressive Random Variable Model (ARVM), a modified autoregressive model outputting histograms. Our ARVM achieves competitive FID scores, outperforming state-of-the-art methods on standard datasets. However, such scores reflect memorization rather than generation. To address this issue, we propose the Local Correlation Hypothesis (LCH), which posits that generative capability arising from local correlations among latent variables. Comprehensive experiments and discussions are conducted to validate our frameworks.

</details>


### [7] [TaleForge: Interactive Multimodal System for Personalized Story Creation](https://arxiv.org/abs/2506.21832)
*Minh-Loi Nguyen,Quang-Khai Le,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: TaleForge是一个结合大语言模型和文本到图像扩散技术的个性化故事生成系统，通过将用户的面部图像嵌入叙事和插图中，提升沉浸感和参与度。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将用户视为被动消费者，提供通用情节且个性化有限，影响了参与度和沉浸感。

Method: TaleForge包含三个模块：故事生成（LLMs根据用户提示创建叙事和角色描述）、个性化图像生成（将用户面部和服装融入角色插图）、背景生成（创建包含个性化角色的场景）。

Result: 用户研究表明，当用户作为主角出现时，参与感和归属感显著提升。参与者赞赏系统的实时预览和直观控制，但希望有更精细的叙事编辑工具。

Conclusion: TaleForge通过个性化文本和图像的结合，推动了多模态故事叙述的发展，创造了沉浸式、以用户为中心的体验。

Abstract: Storytelling is a deeply personal and creative process, yet existing methods often treat users as passive consumers, offering generic plots with limited personalization. This undermines engagement and immersion, especially where individual style or appearance is crucial. We introduce TaleForge, a personalized story-generation system that integrates large language models (LLMs) and text-to-image diffusion to embed users' facial images within both narratives and illustrations. TaleForge features three interconnected modules: Story Generation, where LLMs create narratives and character descriptions from user prompts; Personalized Image Generation, merging users' faces and outfit choices into character illustrations; and Background Generation, creating scene backdrops that incorporate personalized characters. A user study demonstrated heightened engagement and ownership when individuals appeared as protagonists. Participants praised the system's real-time previews and intuitive controls, though they requested finer narrative editing tools. TaleForge advances multimodal storytelling by aligning personalized text and imagery to create immersive, user-centric experiences.

</details>


### [8] [PrefPaint: Enhancing Image Inpainting through Expert Human Feedback](https://arxiv.org/abs/2506.21834)
*Duy-Bao Bui,Hoang-Khang Nguyen,Trung-Nghia Le*

Main category: cs.CV

TL;DR: PrefPaint通过引入人类反馈优化Stable Diffusion Inpainting训练，提升医学图像修复的准确性和可靠性，尤其在息肉图像生成中表现优异。


<details>
  <summary>Details</summary>
Motivation: 医学图像修复（如息肉图像）的准确性对诊断至关重要，现有模型可能生成不准确图像，需专家标注和反馈以确保可靠性。

Method: 提出PrefPaint，将人类反馈直接融入Stable Diffusion Inpainting训练，避免高计算成本的奖励模型，并开发基于网页的交互界面简化流程。

Result: PrefPaint在多个领域优于现有方法，减少视觉不一致性，提升图像渲染质量，尤其在医学息肉图像生成中更真实。

Conclusion: PrefPaint通过结合人类反馈和交互界面，显著提升医学图像修复的可靠性和实用性。

Abstract: Inpainting, the process of filling missing or corrupted image parts, has broad applications, including medical imaging. However, in specialized fields like medical polyps imaging, where accuracy and reliability are critical, inpainting models can generate inaccurate images, leading to significant errors in medical diagnosis and treatment. To ensure reliability, medical images should be annotated by experts like oncologists for effective model training. We propose PrefPaint, an approach that incorporates human feedback into the training process of Stable Diffusion Inpainting, bypassing the need for computationally expensive reward models. In addition, we develop a web-based interface streamlines training, fine-tuning, and inference. This interactive interface provides a smooth and intuitive user experience, making it easier to offer feedback and manage the fine-tuning process. User study on various domains shows that PrefPaint outperforms existing methods, reducing visual inconsistencies and improving image rendering, particularly in medical contexts, where our model generates more realistic polyps images.

</details>


### [9] [3D-Telepathy: Reconstructing 3D Objects from EEG Signals](https://arxiv.org/abs/2506.21843)
*Yuxiang Ge,Jionghao Cheng,Ruiquan Ge,Zhaojie Fang,Gangyong Jia,Xiang Wan,Nannan Li,Ahmed Elazab,Changmiao Wang*

Main category: cs.CV

TL;DR: 提出了一种创新的EEG编码器架构，结合双自注意力机制和混合训练策略，成功从EEG数据生成3D对象。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅将脑电活动转换为2D图像，忽略了3D空间信息的重建，限制了BCI的实际应用。

Method: 采用双自注意力机制的EEG编码器，结合交叉注意力、对比学习和自监督学习的混合训练策略，并利用稳定扩散和变分分数蒸馏训练神经辐射场。

Result: 成功从EEG数据生成了内容和结构相似的3D对象。

Conclusion: 该方法为从EEG数据重建3D视觉刺激提供了新思路，扩展了BCI的应用潜力。

Abstract: Reconstructing 3D visual stimuli from Electroencephalography (EEG) data holds significant potential for applications in Brain-Computer Interfaces (BCIs) and aiding individuals with communication disorders. Traditionally, efforts have focused on converting brain activity into 2D images, neglecting the translation of EEG data into 3D objects. This limitation is noteworthy, as the human brain inherently processes three-dimensional spatial information regardless of whether observing 2D images or the real world. The neural activities captured by EEG contain rich spatial information that is inevitably lost when reconstructing only 2D images, thus limiting its practical applications in BCI. The transition from EEG data to 3D object reconstruction faces considerable obstacles. These include the presence of extensive noise within EEG signals and a scarcity of datasets that include both EEG and 3D information, which complicates the extraction process of 3D visual data. Addressing this challenging task, we propose an innovative EEG encoder architecture that integrates a dual self-attention mechanism. We use a hybrid training strategy to train the EEG Encoder, which includes cross-attention, contrastive learning, and self-supervised learning techniques. Additionally, by employing stable diffusion as a prior distribution and utilizing Variational Score Distillation to train a neural radiation field, we successfully generate 3D objects with similar content and structure from EEG data.

</details>


### [10] [TASeg: Text-aware RGB-T Semantic Segmentation based on Fine-tuning Vision Foundation Models](https://arxiv.org/abs/2506.21975)
*Meng Yu,Te Cui,Qitong Chu,Wenjie Song,Yi Yang,Yufeng Yue*

Main category: cs.CV

TL;DR: TASeg是一个基于文本感知的RGB-T语义分割框架，通过LoRA微调技术和动态特征融合模块（DFFM）解决现有模型在视觉特征相似类别中的分割问题，并结合CLIP文本嵌入提升语义理解准确性。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-T语义分割模型依赖低层视觉特征，缺乏高层文本信息，难以区分视觉特征相似的类别；同时，SAM在实例级分割中表现优异，但与热图像和文本的集成存在模态异质性和计算效率问题。

Method: 提出TASeg框架，采用LoRA微调技术适配视觉基础模型，设计DFFM模块融合多模态视觉特征，并在掩码解码器中引入CLIP生成的文本嵌入以实现语义对齐。

Result: 在多个数据集上的实验表明，TASeg在挑战性场景中表现优异，且训练参数较少。

Conclusion: TASeg通过结合视觉和文本信息，显著提升了RGB-T语义分割的准确性和效率。

Abstract: Reliable semantic segmentation of open environments is essential for intelligent systems, yet significant problems remain: 1) Existing RGB-T semantic segmentation models mainly rely on low-level visual features and lack high-level textual information, which struggle with accurate segmentation when categories share similar visual characteristics. 2) While SAM excels in instance-level segmentation, integrating it with thermal images and text is hindered by modality heterogeneity and computational inefficiency. To address these, we propose TASeg, a text-aware RGB-T segmentation framework by using Low-Rank Adaptation (LoRA) fine-tuning technology to adapt vision foundation models. Specifically, we propose a Dynamic Feature Fusion Module (DFFM) in the image encoder, which effectively merges features from multiple visual modalities while freezing SAM's original transformer blocks. Additionally, we incorporate CLIP-generated text embeddings in the mask decoder to enable semantic alignment, which further rectifies the classification error and improves the semantic understanding accuracy. Experimental results across diverse datasets demonstrate that our method achieves superior performance in challenging scenarios with fewer trainable parameters.

</details>


### [11] [RoboEnvision: A Long-Horizon Video Generation Model for Multi-Task Robot Manipulation](https://arxiv.org/abs/2506.22007)
*Liudi Yang,Yang Bai,George Eskandar,Fengyi Shen,Mohammad Altillawi,Dong Chen,Soumajit Majumder,Ziyuan Liu,Gitta Kutyniok,Abhinav Valada*

Main category: cs.CV

TL;DR: 提出了一种新方法，通过分解任务、生成关键帧和插值来避免自回归生成，解决了长时程视频生成中的错误累积问题。


<details>
  <summary>Details</summary>
Motivation: 解决文本到视频扩散模型在长时程机器人任务中的局限性，避免自回归生成导致的错误累积。

Method: 1) 分解任务并生成关键帧；2) 使用扩散模型插值关键帧；3) 设计语义保持注意力模块和轻量级策略模型。

Result: 在两个基准测试中取得了视频质量和一致性的最佳结果，并在长时程任务中优于之前的策略模型。

Conclusion: 新方法有效解决了长时程视频生成的挑战，提升了机器人任务的表现。

Abstract: We address the problem of generating long-horizon videos for robotic manipulation tasks. Text-to-video diffusion models have made significant progress in photorealism, language understanding, and motion generation but struggle with long-horizon robotic tasks. Recent works use video diffusion models for high-quality simulation data and predictive rollouts in robot planning. However, these works predict short sequences of the robot achieving one task and employ an autoregressive paradigm to extend to the long horizon, leading to error accumulations in the generated video and in the execution. To overcome these limitations, we propose a novel pipeline that bypasses the need for autoregressive generation. We achieve this through a threefold contribution: 1) we first decompose the high-level goals into smaller atomic tasks and generate keyframes aligned with these instructions. A second diffusion model then interpolates between each of the two generated frames, achieving the long-horizon video. 2) We propose a semantics preserving attention module to maintain consistency between the keyframes. 3) We design a lightweight policy model to regress the robot joint states from generated videos. Our approach achieves state-of-the-art results on two benchmarks in video quality and consistency while outperforming previous policy models on long-horizon tasks.

</details>


### [12] [Advancing Facial Stylization through Semantic Preservation Constraint and Pseudo-Paired Supervision](https://arxiv.org/abs/2506.22022)
*Zhanyi Lu,Yue Zhou*

Main category: cs.CV

TL;DR: 提出一种结合语义保留约束和伪配对监督的人脸风格化方法，提升内容一致性和风格化效果，支持多模态和参考引导风格化。


<details>
  <summary>Details</summary>
Motivation: 现有StyleGAN方法在风格化时忽略生成器的语义偏移，导致生成结果存在伪影或与源图像内容不一致。

Method: 集成语义保留约束和伪配对监督，开发多级伪配对数据集，实现监督约束，支持灵活的多模态和参考引导风格化。

Result: 实验结果表明，该方法生成高保真、美观的人脸风格化效果，优于现有方法。

Conclusion: 提出的方法通过语义保留和伪配对监督有效解决了风格化中的内容一致性问题，并实现了灵活的风格化扩展。

Abstract: Facial stylization aims to transform facial images into appealing, high-quality stylized portraits, with the critical challenge of accurately learning the target style while maintaining content consistency with the original image. Although previous StyleGAN-based methods have made significant advancements, the generated results still suffer from artifacts or insufficient fidelity to the source image. We argue that these issues stem from neglecting semantic shift of the generator during stylization. Therefore, we propose a facial stylization method that integrates semantic preservation constraint and pseudo-paired supervision to enhance the content correspondence and improve the stylization effect. Additionally, we develop a methodology for creating multi-level pseudo-paired datasets to implement supervisory constraint. Furthermore, building upon our facial stylization framework, we achieve more flexible multimodal and reference-guided stylization without complex network architecture designs or additional training. Experimental results demonstrate that our approach produces high-fidelity, aesthetically pleasing facial style transfer that surpasses previous methods.

</details>


### [13] [MirrorMe: Towards Realtime and High Fidelity Audio-Driven Halfbody Animation](https://arxiv.org/abs/2506.22065)
*Dechao Meng,Steven Xiao,Xindi Zhang,Guangyuan Wang,Peng Zhang,Qi Wang,Bang Zhang,Liefeng Bo*

Main category: cs.CV

TL;DR: MirrorMe是一个基于LTX视频模型的实时音频驱动肖像动画框架，通过空间和时间压缩实现高效潜在空间去噪，解决了现有方法在延迟和时间一致性上的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的方法虽然提高了生成质量，但由于逐帧UNet架构的依赖，导致高延迟和时间一致性不足。MirrorMe旨在解决这些问题，实现实时、高保真的动画生成。

Method: 1. 通过VAE编码图像拼接和自注意力机制注入参考身份；2. 为LTX时间结构设计的因果音频编码器和适配器；3. 渐进式训练策略，结合面部、半身和手势控制。

Result: 在EMTD Benchmark上，MirrorMe在保真度、唇同步准确性和时间稳定性方面表现最佳。

Conclusion: MirrorMe通过创新设计和渐进训练，实现了实时、高保真的音频驱动肖像动画，显著优于现有方法。

Abstract: Audio-driven portrait animation, which synthesizes realistic videos from reference images using audio signals, faces significant challenges in real-time generation of high-fidelity, temporally coherent animations. While recent diffusion-based methods improve generation quality by integrating audio into denoising processes, their reliance on frame-by-frame UNet architectures introduces prohibitive latency and struggles with temporal consistency. This paper introduces MirrorMe, a real-time, controllable framework built on the LTX video model, a diffusion transformer that compresses video spatially and temporally for efficient latent space denoising. To address LTX's trade-offs between compression and semantic fidelity, we propose three innovations: 1. A reference identity injection mechanism via VAE-encoded image concatenation and self-attention, ensuring identity consistency; 2. A causal audio encoder and adapter tailored to LTX's temporal structure, enabling precise audio-expression synchronization; and 3. A progressive training strategy combining close-up facial training, half-body synthesis with facial masking, and hand pose integration for enhanced gesture control. Extensive experiments on the EMTD Benchmark demonstrate MirrorMe's state-of-the-art performance in fidelity, lip-sync accuracy, and temporal stability.

</details>


### [14] [BézierGS: Dynamic Urban Scene Reconstruction with Bézier Curve Gaussian Splatting](https://arxiv.org/abs/2506.22099)
*Zipei Ma,Junzhe Jiang,Yurui Chen,Li Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于Bézier曲线的高斯泼溅方法（BézierGS），用于动态物体的运动轨迹建模，解决了现有方法依赖高精度标注的问题，并在场景重建和新视角合成中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖高精度物体姿态标注，限制了大规模场景重建。BézierGS通过可学习的Bézier曲线建模动态物体运动轨迹，减少对标注的依赖。

Method: 使用Bézier曲线表示动态物体运动轨迹，结合可学习曲线建模和动态物体渲染监督，实现场景元素的分离与重建。

Result: 在Waymo Open Dataset和nuPlan基准测试中，BézierGS在动态和静态场景重建及新视角合成方面优于现有方法。

Conclusion: BézierGS通过曲线建模和额外监督，实现了高效且准确的场景重建，为自动驾驶仿真提供了新思路。

Abstract: The realistic reconstruction of street scenes is critical for developing real-world simulators in autonomous driving. Most existing methods rely on object pose annotations, using these poses to reconstruct dynamic objects and move them during the rendering process. This dependence on high-precision object annotations limits large-scale and extensive scene reconstruction. To address this challenge, we propose B\'ezier curve Gaussian splatting (B\'ezierGS), which represents the motion trajectories of dynamic objects using learnable B\'ezier curves. This approach fully leverages the temporal information of dynamic objects and, through learnable curve modeling, automatically corrects pose errors. By introducing additional supervision on dynamic object rendering and inter-curve consistency constraints, we achieve reasonable and accurate separation and reconstruction of scene elements. Extensive experiments on the Waymo Open Dataset and the nuPlan benchmark demonstrate that B\'ezierGS outperforms state-of-the-art alternatives in both dynamic and static scene components reconstruction and novel view synthesis.

</details>


### [15] [Low-Rank Implicit Neural Representation via Schatten-p Quasi-Norm and Jacobian Regularization](https://arxiv.org/abs/2506.22134)
*Zhengyun Cheng,Changhao Wang,Guanwen Zhang,Yi Xu,Wei Zhou,Xiangyang Ji*

Main category: cs.CV

TL;DR: 提出了一种基于CP分解的低秩张量函数（CP-INR），结合神经网络实现连续数据表示，并通过稀疏性和平滑性优化提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有张量分解方法（如Tucker和CP）在灵活性和可解释性之间存在权衡，且稀疏解难以获得。CP-INR旨在解决这些问题，同时利用张量数据的非线性特性。

Method: 采用CP分解参数化神经网络，引入Schatten-p拟范数实现稀疏性，并提出基于谱范数的平滑正则化项。

Result: 实验证明CP-INR在多维数据恢复任务（如图像修复、去噪和点云上采样）中优于现有方法。

Conclusion: CP-INR结合了CP分解的可解释性和神经网络的表达能力，为连续数据表示提供了高效且理论保障的解决方案。

Abstract: Higher-order tensors are well-suited for representing multi-dimensional data, such as color images and videos. Low-rank tensor representation has become essential in machine learning and computer vision, but existing methods like Tucker decomposition offer flexibility at the expense of interpretability. In contrast, while the CANDECOMP/PARAFAC (CP) decomposition provides a more natural and interpretable tensor structure, obtaining sparse solutions remains challenging. Leveraging the rich properties of CP decomposition, we propose a CP-based low-rank tensor function parameterized by neural networks for implicit neural representation (CP-INR). This approach enables continuous data representation beyond structured grids, fully exploiting the non-linearity of tensor data with theoretical guarantees on excess risk bounds. To achieve a sparse CP decomposition, we introduce a variational form of the Schatten-p quasi-norm and prove its relationship to multilinear rank minimization. For smoothness, we propose a regularization term based on the spectral norm of the Jacobian and Hutchinson's trace estimator. Our proposed smoothness regularization is SVD-free and avoids explicit chain rule derivations. It can serve as an alternative to Total Variation (TV) regularization in image denoising tasks and is naturally applicable to continuous data. Extensive experiments on multi-dimensional data recovery tasks, including image inpainting, denoising, and point cloud upsampling, demonstrate the superiority and versatility of our method compared to state-of-the-art approaches.

</details>


### [16] [Frequency-Semantic Enhanced Variational Autoencoder for Zero-Shot Skeleton-based Action Recognition](https://arxiv.org/abs/2506.22179)
*Wenhan Wu,Zhishuai Guo,Chen Chen,Hongfei Xue,Aidong Lu*

Main category: cs.CV

TL;DR: 提出了一种基于频率分解的骨架语义表示学习方法（FS-VAE），通过高频和低频调整增强骨架语义学习，并改进零样本动作识别的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在零样本骨架动作识别中忽视细粒度动作模式的问题，特别是语义空间中类似动作的区分。

Method: FS-VAE包含三个关键组件：频率增强模块、多级对齐的语义动作描述和校准的交叉对齐损失。

Result: 在基准测试中验证了方法的有效性，频率增强的语义特征能够区分视觉和语义相似的动作簇。

Conclusion: FS-VAE通过频率分解和语义对齐，显著提升了零样本动作识别的性能。

Abstract: Zero-shot skeleton-based action recognition aims to develop models capable of identifying actions beyond the categories encountered during training. Previous approaches have primarily focused on aligning visual and semantic representations but often overlooked the importance of fine-grained action patterns in the semantic space (e.g., the hand movements in drinking water and brushing teeth). To address these limitations, we propose a Frequency-Semantic Enhanced Variational Autoencoder (FS-VAE) to explore the skeleton semantic representation learning with frequency decomposition. FS-VAE consists of three key components: 1) a frequency-based enhancement module with high- and low-frequency adjustments to enrich the skeletal semantics learning and improve the robustness of zero-shot action recognition; 2) a semantic-based action description with multilevel alignment to capture both local details and global correspondence, effectively bridging the semantic gap and compensating for the inherent loss of information in skeleton sequences; 3) a calibrated cross-alignment loss that enables valid skeleton-text pairs to counterbalance ambiguous ones, mitigating discrepancies and ambiguities in skeleton and text features, thereby ensuring robust alignment. Evaluations on the benchmarks demonstrate the effectiveness of our approach, validating that frequency-enhanced semantic features enable robust differentiation of visually and semantically similar action clusters, improving zero-shot action recognition.

</details>


### [17] [ReF-LLE: Personalized Low-Light Enhancement via Reference-Guided Deep Reinforcement Learning](https://arxiv.org/abs/2506.22216)
*Ming Zhao,Pingping Liu,Tongshun Zhang,Zhe Zhang*

Main category: cs.CV

TL;DR: ReF-LLE是一种基于傅里叶频域和深度强化学习的个性化低光图像增强方法，首次将深度强化学习引入该领域，通过零参考图像评估策略和个性化自适应迭代策略，显著提升了增强效果。


<details>
  <summary>Details</summary>
Motivation: 低光图像增强面临两个主要挑战：1) 不同条件下低光图像的显著变化；2) 增强水平受主观偏好和用户意图影响。

Method: 提出ReF-LLE方法，结合傅里叶频域和深度强化学习，训练时引入零参考图像评估策略，推理时采用个性化自适应迭代策略。

Result: 在基准数据集上，ReF-LLE优于现有方法，实现了更高的感知质量和个性化适应性。

Conclusion: ReF-LLE通过创新方法解决了低光图像增强的挑战，为个性化需求提供了有效解决方案。

Abstract: Low-light image enhancement presents two primary challenges: 1) Significant variations in low-light images across different conditions, and 2) Enhancement levels influenced by subjective preferences and user intent. To address these issues, we propose ReF-LLE, a novel personalized low-light image enhancement method that operates in the Fourier frequency domain and incorporates deep reinforcement learning. ReF-LLE is the first to integrate deep reinforcement learning into this domain. During training, a zero-reference image evaluation strategy is introduced to score enhanced images, providing reward signals that guide the model to handle varying degrees of low-light conditions effectively. In the inference phase, ReF-LLE employs a personalized adaptive iterative strategy, guided by the zero-frequency component in the Fourier domain, which represents the overall illumination level. This strategy enables the model to adaptively adjust low-light images to align with the illumination distribution of a user-provided reference image, ensuring personalized enhancement results. Extensive experiments on benchmark datasets demonstrate that ReF-LLE outperforms state-of-the-art methods, achieving superior perceptual quality and adaptability in personalized low-light image enhancement.

</details>


### [18] [EAMamba: Efficient All-Around Vision State Space Model for Image Restoration](https://arxiv.org/abs/2506.22246)
*Yu-Cheng Lin,Yu-Syuan Xu,Hao-Wei Chen,Hsien-Kai Kuo,Chun-Yi Lee*

Main category: cs.CV

TL;DR: EAMamba通过多头部选择性扫描模块和全方位扫描机制，解决了Vision Mamba在图像恢复任务中的计算复杂性和局部像素遗忘问题，显著降低了FLOPs并保持了性能。


<details>
  <summary>Details</summary>
Motivation: Vision Mamba在图像恢复任务中表现出色，但仍面临计算复杂性和局部像素遗忘的挑战，需要改进。

Method: 提出EAMamba框架，引入多头部选择性扫描模块（MHSSM）和全方位扫描机制，以高效聚合扫描序列并解决局部像素遗忘。

Result: 实验表明，EAMamba在多项恢复任务中显著降低FLOPs（31-89%），同时保持性能优势。

Conclusion: EAMamba通过创新设计有效解决了Vision Mamba的局限性，为图像恢复任务提供了高效解决方案。

Abstract: Image restoration is a key task in low-level computer vision that aims to reconstruct high-quality images from degraded inputs. The emergence of Vision Mamba, which draws inspiration from the advanced state space model Mamba, marks a significant advancement in this field. Vision Mamba demonstrates excellence in modeling long-range dependencies with linear complexity, a crucial advantage for image restoration tasks. Despite its strengths, Vision Mamba encounters challenges in low-level vision tasks, including computational complexity that scales with the number of scanning sequences and local pixel forgetting. To address these limitations, this study introduces Efficient All-Around Mamba (EAMamba), an enhanced framework that incorporates a Multi-Head Selective Scan Module (MHSSM) with an all-around scanning mechanism. MHSSM efficiently aggregates multiple scanning sequences, which avoids increases in computational complexity and parameter count. The all-around scanning strategy implements multiple patterns to capture holistic information and resolves the local pixel forgetting issue. Our experimental evaluations validate these innovations across several restoration tasks, including super resolution, denoising, deblurring, and dehazing. The results validate that EAMamba achieves a significant 31-89% reduction in FLOPs while maintaining favorable performance compared to existing low-level Vision Mamba methods.

</details>


### [19] [OutDreamer: Video Outpainting with a Diffusion Transformer](https://arxiv.org/abs/2506.22298)
*Linhao Zhong,Fan Li,Yi Huang,Jianzhuang Liu,Renjing Pei,Fenglong Song*

Main category: cs.CV

TL;DR: OutDreamer是一种基于扩散变换器（DiT）的视频外绘框架，通过高效视频控制分支和条件外绘分支，结合动态掩码自注意力层和潜在对齐损失，显著提升了视频外绘的质量和适应性。


<details>
  <summary>Details</summary>
Motivation: 视频外绘任务在生成超出原始视频边界的新内容时，需同时保证时空一致性。现有方法（如基于U-Net的潜在扩散模型）在生成质量和适应性上仍有不足。

Method: 提出OutDreamer框架，包含高效视频控制分支和条件外绘分支，引入动态掩码自注意力层和潜在对齐损失，并采用跨视频片段细化器处理长视频。

Result: 实验表明，OutDreamer在零样本设置下优于现有方法。

Conclusion: OutDreamer通过创新的DiT架构和优化策略，显著提升了视频外绘的性能和适应性。

Abstract: Video outpainting is a challenging task that generates new video content by extending beyond the boundaries of an original input video, requiring both temporal and spatial consistency. Many state-of-the-art methods utilize latent diffusion models with U-Net backbones but still struggle to achieve high quality and adaptability in generated content. Diffusion transformers (DiTs) have emerged as a promising alternative because of their superior performance. We introduce OutDreamer, a DiT-based video outpainting framework comprising two main components: an efficient video control branch and a conditional outpainting branch. The efficient video control branch effectively extracts masked video information, while the conditional outpainting branch generates missing content based on these extracted conditions. Additionally, we propose a mask-driven self-attention layer that dynamically integrates the given mask information, further enhancing the model's adaptability to outpainting tasks. Furthermore, we introduce a latent alignment loss to maintain overall consistency both within and between frames. For long video outpainting, we employ a cross-video-clip refiner to iteratively generate missing content, ensuring temporal consistency across video clips. Extensive evaluations demonstrate that our zero-shot OutDreamer outperforms state-of-the-art zero-shot methods on widely recognized benchmarks.

</details>


### [20] [A Deep Learning framework for building damage assessment using VHR SAR and geospatial data: demonstration on the 2023 Turkiye Earthquake](https://arxiv.org/abs/2506.22338)
*Luigi Russo,Deodato Tapete,Silvia Liberata Ullo,Paolo Gamba*

Main category: cs.CV

TL;DR: 提出了一种基于单日期高分辨率SAR图像和多源地理空间数据的深度学习框架，用于快速检测建筑物损坏，无需依赖灾前数据。


<details>
  <summary>Details</summary>
Motivation: 灾害后快速识别建筑物损坏对应急响应至关重要，但传统光学卫星图像常受云层或缺乏灾前数据限制。

Method: 结合SAR图像、OSM建筑足迹、DSM数据和GEM的结构与暴露属性，构建多模态深度学习模型。

Result: 在土耳其2023年地震数据集上验证，显示结合地理空间特征显著提升检测性能和泛化能力。

Conclusion: 该方法无需灾前数据，可快速、可靠地评估建筑物损坏，支持灾害管理和恢复。

Abstract: Building damage identification shortly after a disaster is crucial for guiding emergency response and recovery efforts. Although optical satellite imagery is commonly used for disaster mapping, its effectiveness is often hampered by cloud cover or the absence of pre-event acquisitions. To overcome these challenges, we introduce a novel multimodal deep learning (DL) framework for detecting building damage using single-date very high resolution (VHR) Synthetic Aperture Radar (SAR) imagery from the Italian Space Agency (ASI) COSMO SkyMed (CSK) constellation, complemented by auxiliary geospatial data. Our method integrates SAR image patches, OpenStreetMap (OSM) building footprints, digital surface model (DSM) data, and structural and exposure attributes from the Global Earthquake Model (GEM) to improve detection accuracy and contextual interpretation. Unlike existing approaches that depend on pre and post event imagery, our model utilizes only post event data, facilitating rapid deployment in critical scenarios. The framework effectiveness is demonstrated using a new dataset from the 2023 earthquake in Turkey, covering multiple cities with diverse urban settings. Results highlight that incorporating geospatial features significantly enhances detection performance and generalizability to previously unseen areas. By combining SAR imagery with detailed vulnerability and exposure information, our approach provides reliable and rapid building damage assessments without the dependency from available pre-event data. Moreover, the automated and scalable data generation process ensures the framework's applicability across diverse disaster-affected regions, underscoring its potential to support effective disaster management and recovery efforts. Code and data will be made available upon acceptance of the paper.

</details>


### [21] [Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment](https://arxiv.org/abs/2506.22385)
*Yue Zhang,Jilei Sun,Yunhui Guo,Vibhav Gogate*

Main category: cs.CV

TL;DR: 论文提出了一种新任务DVidE，旨在提升视频大型多模态模型（VLMMs）的动态推理能力，通过引入反事实推理和ASR增强技术，显著改进了模型的适应性推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLMMs在抽象和适应性推理方面表现不足，无法根据新信息动态调整推理结论。DVidE任务旨在解决这一问题。

Method: 提出Chain of Counterfactual Thought框架用于分类任务，结合ASR增强视频内容和反事实推理；生成任务则结合ASR输出和LLM生成连贯更新。

Result: 实验结果表明，所提方法显著提升了VLMMs的动态推理能力。

Conclusion: DVidE任务及相关框架为提升VLMMs的适应性推理能力提供了有效解决方案。

Abstract: Video Large Multimodal Models (VLMMs) have made impressive strides in understanding video content, but they often struggle with abstract and adaptive reasoning-the ability to revise their interpretations when new information emerges. In reality, conclusions are rarely set in stone; additional context can strengthen or weaken an initial inference. To address this, we introduce Defeasible Video Entailment (DVidE), a new task that challenges models to think like doubters, constantly updating their reasoning based on evolving evidence. In DVidE, given a video premise and a textual hypothesis, models must determine whether a new update strengthens or weakens the hypothesis (classification version) or generate a coherent update that modifies the entailment relationship (generation version). For solving the classification task, we propose the Chain of Counterfactual Thought framework, utilizing counterfactual reasoning, ASR-enhanced video content, and rationale refinement to reduce inference bias. For the generation task, we develop a framework that combines ASR output with a Large Language Model (LLM) to produce coherent, contextually relevant updates aligned with the intended strengthener or weakener goals. Additionally, we introduce a novel benchmark dataset, with strengthener/weakener annotations and an LLM-based evaluation metric specifically designed for assessing generative performance. Experimental results demonstrate significant improvements, highlighting our proposed method in enhancing dynamic reasoning capabilities of VLMMs.

</details>


### [22] [Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy](https://arxiv.org/abs/2506.22432)
*Yuhao Liu,Tengfei Wang,Fang Liu,Zhenwei Wang,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: Shape-for-Motion 是一种新型视频编辑框架，通过3D代理实现精确且一致的编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法在实现用户精确控制视频编辑意图方面存在挑战，需要一种更可靠的解决方案。

Method: 使用3D代理（时间一致的网格）进行编辑，并通过双传播策略简化编辑过程，最终通过视频扩散模型生成结果。

Result: 支持多种精确且物理一致的操作（如姿态编辑、旋转、纹理修改等），实验证明其优越性。

Conclusion: Shape-for-Motion 是高质量、可控视频编辑工作流的重要进展。

Abstract: Recent advances in deep generative modeling have unlocked unprecedented opportunities for video synthesis. In real-world applications, however, users often seek tools to faithfully realize their creative editing intentions with precise and consistent control. Despite the progress achieved by existing methods, ensuring fine-grained alignment with user intentions remains an open and challenging problem. In this work, we present Shape-for-Motion, a novel framework that incorporates a 3D proxy for precise and consistent video editing. Shape-for-Motion achieves this by converting the target object in the input video to a time-consistent mesh, i.e., a 3D proxy, allowing edits to be performed directly on the proxy and then inferred back to the video frames. To simplify the editing process, we design a novel Dual-Propagation Strategy that allows users to perform edits on the 3D mesh of a single frame, and the edits are then automatically propagated to the 3D meshes of the other frames. The 3D meshes for different frames are further projected onto the 2D space to produce the edited geometry and texture renderings, which serve as inputs to a decoupled video diffusion model for generating edited results. Our framework supports various precise and physically-consistent manipulations across the video frames, including pose editing, rotation, scaling, translation, texture modification, and object composition. Our approach marks a key step toward high-quality, controllable video editing workflows. Extensive experiments demonstrate the superiority and effectiveness of our approach. Project page: https://shapeformotion.github.io/

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [$\textrm{ODE}_t \left(\textrm{ODE}_l \right)$: Shortcutting the Time and Length in Diffusion and Flow Models for Faster Sampling](https://arxiv.org/abs/2506.21714)
*Denis Gudovskiy,Wenzhao Zheng,Tomoyuki Okuno,Yohei Nakata,Kurt Keutzer*

Main category: cs.LG

TL;DR: 论文提出了一种动态控制质量-复杂度权衡的方法，通过调整时间步长和神经网络长度，在采样过程中实现高效性和质量的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注减少采样时间步数以提高效率，但忽略了动态调整网络长度的潜力。本文旨在探索这一互补方向。

Method: 通过重新连接基于Transformer架构的块来求解内部离散ODE，并在训练中引入时间和长度一致性项，实现任意时间步和块数的采样。

Result: 在CelebA-HQ和ImageNet上的实验表明，最高效采样模式下延迟减少3倍，高质量采样模式下FID分数提升3.5分。

Conclusion: 提出的方法在时间维度和长度维度上均具有灵活性，显著降低了延迟和内存使用，同时提升了生成质量。

Abstract: Recently, continuous normalizing flows (CNFs) and diffusion models (DMs) have been studied using the unified theoretical framework. Although such models can generate high-quality data points from a noise distribution, the sampling demands multiple iterations to solve an ordinary differential equation (ODE) with high computational complexity. Most existing methods focus on reducing the number of time steps during the sampling process to improve efficiency. In this work, we explore a complementary direction in which the quality-complexity tradeoff can be dynamically controlled in terms of time steps and in the length of the neural network. We achieve this by rewiring the blocks in the transformer-based architecture to solve an inner discretized ODE w.r.t. its length. Then, we employ time- and length-wise consistency terms during flow matching training, and as a result, the sampling can be performed with an arbitrary number of time steps and transformer blocks. Unlike others, our $\textrm{ODE}_t \left(\textrm{ODE}_l \right)$ approach is solver-agnostic in time dimension and decreases both latency and memory usage. Compared to the previous state of the art, image generation experiments on CelebA-HQ and ImageNet show a latency reduction of up to $3\times$ in the most efficient sampling mode, and a FID score improvement of up to $3.5$ points for high-quality sampling. We release our code and model weights with fully reproducible experiments.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [24] [Single-shot HDR using conventional image sensor shutter functions and optical randomization](https://arxiv.org/abs/2506.22426)
*Xiang Dai,Kyrollos Yanny,Kristina Monakhova,Nicholas Antipa*

Main category: eess.IV

TL;DR: 提出一种基于全局重置释放（GRR）快门模式和光学随机排列的单次曝光HDR成像方法，显著提升高光区域的恢复能力。


<details>
  <summary>Details</summary>
Motivation: 传统多曝光HDR成像在动态场景中易产生运动伪影，而现有单次曝光方法在高光区域表现不佳。

Method: 利用GRR快门模式实现不同区域的差异化曝光，并通过光学随机排列创建空间随机曝光，结合总变分先验优化恢复HDR数据。

Result: 仿真显示该方法在高像素饱和（10%以上）时优于其他单次曝光方法，实验室原型动态范围达73dB。

Conclusion: 该方法通过GRR和光学随机排列有效提升单次HDR成像性能，适用于高光场景。

Abstract: High-dynamic-range (HDR) imaging is an essential technique for overcoming the dynamic range limits of image sensors. The classic method relies on multiple exposures, which slows capture time, resulting in motion artifacts when imaging dynamic scenes. Single-shot HDR imaging alleviates this issue by encoding HDR data into a single exposure, then computationally recovering it. Many established methods use strong image priors to recover improperly exposed image detail. These approaches struggle with extended highlight regions. We utilize the global reset release (GRR) shutter mode of an off-the-shelf sensor. GRR shutter mode applies a longer exposure time to rows closer to the bottom of the sensor. We use optics that relay a randomly permuted (shuffled) image onto the sensor, effectively creating spatially randomized exposures across the scene. The exposure diversity allows us to recover HDR data by solving an optimization problem with a simple total variation image prior. In simulation, we demonstrate that our method outperforms other single-shot methods when many sensor pixels are saturated (10% or more), and is competitive at a modest saturation (1%). Finally, we demonstrate a physical lab prototype that uses an off-the-shelf random fiber bundle for the optical shuffling. The fiber bundle is coupled to a low-cost commercial sensor operating in GRR shutter mode. Our prototype achieves a dynamic range of up to 73dB using an 8-bit sensor with 48dB dynamic range.

</details>


### [25] [UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields](https://arxiv.org/abs/2506.21884)
*Fabian Perez,Sara Rojas,Carlos Hinojosa,Hoover Rueda-Chacón,Bernard Ghanem*

Main category: eess.IV

TL;DR: UnMix-NeRF结合光谱解混与NeRF，实现联合高光谱新视角合成和无监督材料分割，解决了现有方法缺乏材料属性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF分割方法仅依赖RGB数据，缺乏材料属性，限制了在机器人、增强现实等应用中的准确材料感知。

Method: 通过建模光谱反射的漫反射和镜面反射分量，利用全局端元字典表示纯材料特征，并通过点级丰度分布实现无监督材料分割。

Result: 实验验证了UnMix-NeRF在高光谱重建和材料分割上的优越性。

Conclusion: UnMix-NeRF为材料感知提供了灵活且无监督的解决方案，支持场景编辑和材料操作。

Abstract: Neural Radiance Field (NeRF)-based segmentation methods focus on object semantics and rely solely on RGB data, lacking intrinsic material properties. This limitation restricts accurate material perception, which is crucial for robotics, augmented reality, simulation, and other applications. We introduce UnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling joint hyperspectral novel view synthesis and unsupervised material segmentation. Our method models spectral reflectance via diffuse and specular components, where a learned dictionary of global endmembers represents pure material signatures, and per-point abundances capture their distribution. For material segmentation, we use spectral signature predictions along learned endmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF enables scene editing by modifying learned endmember dictionaries for flexible material-based appearance manipulation. Extensive experiments validate our approach, demonstrating superior spectral reconstruction and material segmentation to existing methods. Project page: https://www.factral.co/UnMix-NeRF.

</details>


### [26] [StableCodec: Taming One-Step Diffusion for Extreme Image Compression](https://arxiv.org/abs/2506.21977)
*Tianyu Zhang,Xin Luo,Li Li,Dong Liu*

Main category: eess.IV

TL;DR: StableCodec是一种基于扩散模型的图像压缩方法，通过一步扩散实现高保真和高真实感的超低比特率压缩，解决了现有方法需要多步去噪和保真度不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在超低比特率下需要大量去噪步骤且难以保证像素级一致性，限制了实时压缩应用。

Method: 提出StableCodec，结合高效的深度压缩潜在编解码器和双分支编码结构，通过端到端优化实现一步扩散压缩。

Result: 在CLIC 2020、DIV2K和Kodak数据集上，StableCodec在FID、KID和DISTS指标上显著优于现有方法，比特率低至0.005 bpp。

Conclusion: StableCodec在超低比特率下实现了高真实感和高保真度，同时推理速度与主流变换编码方案相当。

Abstract: Diffusion-based image compression has shown remarkable potential for achieving ultra-low bitrate coding (less than 0.05 bits per pixel) with high realism, by leveraging the generative priors of large pre-trained text-to-image diffusion models. However, current approaches require a large number of denoising steps at the decoder to generate realistic results under extreme bitrate constraints, limiting their application in real-time compression scenarios. Additionally, these methods often sacrifice reconstruction fidelity, as diffusion models typically fail to guarantee pixel-level consistency. To address these challenges, we introduce StableCodec, which enables one-step diffusion for high-fidelity and high-realism extreme image compression with improved coding efficiency. To achieve ultra-low bitrates, we first develop an efficient Deep Compression Latent Codec to transmit a noisy latent representation for a single-step denoising process. We then propose a Dual-Branch Coding Structure, consisting of a pair of auxiliary encoder and decoder, to enhance reconstruction fidelity. Furthermore, we adopt end-to-end optimization with joint bitrate and pixel-level constraints. Extensive experiments on the CLIC 2020, DIV2K, and Kodak dataset demonstrate that StableCodec outperforms existing methods in terms of FID, KID and DISTS by a significant margin, even at bitrates as low as 0.005 bits per pixel, while maintaining strong fidelity. Additionally, StableCodec achieves inference speeds comparable to mainstream transform coding schemes. All source code are available at https://github.com/LuizScarlet/StableCodec.

</details>


### [27] [Noise-Inspired Diffusion Model for Generalizable Low-Dose CT Reconstruction](https://arxiv.org/abs/2506.22012)
*Qi Gao,Zhihao Chen,Dong Zeng,Junping Zhang,Jianhua Ma,Hongming Shan*

Main category: eess.IV

TL;DR: 论文提出了一种名为NEED的噪声启发扩散模型，用于低剂量CT重建，通过双域扩散模型和噪声特性对齐，显著提升了泛化性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在低剂量CT重建中对未见剂量数据的泛化问题，避免传统方法对配对数据的依赖。

Method: 提出基于噪声特性的双域扩散模型：1) 投影域的去噪扩散模型；2) 图像域的双重引导扩散模型。

Result: 在多个数据集上，NEED在重建质量和泛化能力上均优于现有方法。

Conclusion: NEED通过噪声对齐和双域扩散模型，实现了无需额外数据的高效泛化低剂量CT重建。

Abstract: The generalization of deep learning-based low-dose computed tomography (CT) reconstruction models to doses unseen in the training data is important and remains challenging. Previous efforts heavily rely on paired data to improve the generalization performance and robustness through collecting either diverse CT data for re-training or a few test data for fine-tuning. Recently, diffusion models have shown promising and generalizable performance in low-dose CT (LDCT) reconstruction, however, they may produce unrealistic structures due to the CT image noise deviating from Gaussian distribution and imprecise prior information from the guidance of noisy LDCT images. In this paper, we propose a noise-inspired diffusion model for generalizable LDCT reconstruction, termed NEED, which tailors diffusion models for noise characteristics of each domain. First, we propose a novel shifted Poisson diffusion model to denoise projection data, which aligns the diffusion process with the noise model in pre-log LDCT projections. Second, we devise a doubly guided diffusion model to refine reconstructed images, which leverages LDCT images and initial reconstructions to more accurately locate prior information and enhance reconstruction fidelity. By cascading these two diffusion models for dual-domain reconstruction, our NEED requires only normal-dose data for training and can be effectively extended to various unseen dose levels during testing via a time step matching strategy. Extensive qualitative, quantitative, and segmentation-based evaluations on two datasets demonstrate that our NEED consistently outperforms state-of-the-art methods in reconstruction and generalization performance. Source code is made available at https://github.com/qgao21/NEED.

</details>


### [28] [DIGS: Dynamic CBCT Reconstruction using Deformation-Informed 4D Gaussian Splatting and a Low-Rank Free-Form Deformation Model](https://arxiv.org/abs/2506.22280)
*Yuliang Huang,Imraj Singh,Thomas Joyce,Kris Thielemans,Jamie R. McClelland*

Main category: eess.IV

TL;DR: 论文提出了一种基于自由形变（FFD）的4D高斯泼溅（4DGS）方法，用于动态CBCT重建，解决了现有方法计算成本高和运动不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 动态CBCT在放疗中广泛应用，但呼吸运动导致图像伪影。现有方法如相位排序无法处理呼吸变异性，而4DGS方法如HexPlane计算成本高且缺乏空间正则化。

Method: 引入FFD空间基函数和变形感知框架，统一高斯均值位置、尺度和旋转的时间演化，确保运动一致性。

Result: 在六个CBCT数据集上验证，图像质量优于HexPlane，速度提升6倍。

Conclusion: 变形感知4DGS为高效、运动补偿的CBCT重建提供了潜力。

Abstract: 3D Cone-Beam CT (CBCT) is widely used in radiotherapy but suffers from motion artifacts due to breathing. A common clinical approach mitigates this by sorting projections into respiratory phases and reconstructing images per phase, but this does not account for breathing variability. Dynamic CBCT instead reconstructs images at each projection, capturing continuous motion without phase sorting. Recent advancements in 4D Gaussian Splatting (4DGS) offer powerful tools for modeling dynamic scenes, yet their application to dynamic CBCT remains underexplored. Existing 4DGS methods, such as HexPlane, use implicit motion representations, which are computationally expensive. While explicit low-rank motion models have been proposed, they lack spatial regularization, leading to inconsistencies in Gaussian motion. To address these limitations, we introduce a free-form deformation (FFD)-based spatial basis function and a deformation-informed framework that enforces consistency by coupling the temporal evolution of Gaussian's mean position, scale, and rotation under a unified deformation field. We evaluate our approach on six CBCT datasets, demonstrating superior image quality with a 6x speedup over HexPlane. These results highlight the potential of deformation-informed 4DGS for efficient, motion-compensated CBCT reconstruction. The code is available at https://github.com/Yuliang-Huang/DIGS.

</details>


### [29] [Dehazing Light Microscopy Images with Guided Conditional Flow Matching: finding a sweet spot between fidelity and realism](https://arxiv.org/abs/2506.22397)
*Anirban Ray,Ashesh,Florian Jug*

Main category: eess.IV

TL;DR: 提出了一种名为HazeMatching的新方法，用于平衡荧光显微镜图像去雾中的保真度和真实感。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在去雾过程中无法同时兼顾数据保真度和感知真实感的问题。

Method: 采用条件流匹配框架，通过条件速度场引导生成过程，平衡保真度和真实感。

Result: 在5个数据集上评估，HazeMatching在保真度和真实感之间取得了平衡，优于7个基线方法。

Conclusion: HazeMatching无需显式退化算子，适用于真实显微镜数据，且预测结果校准良好。

Abstract: Fluorescence microscopy is a major driver of scientific progress in the life sciences. Although high-end confocal microscopes are capable of filtering out-of-focus light, cheaper and more accessible microscopy modalities, such as widefield microscopy, can not, which consequently leads to hazy image data. Computational dehazing is trying to combine the best of both worlds, leading to cheap microscopy but crisp-looking images. The perception-distortion trade-off tells us that we can optimize either for data fidelity, e.g. low MSE or high PSNR, or for data realism, measured by perceptual metrics such as LPIPS or FID. Existing methods either prioritize fidelity at the expense of realism, or produce perceptually convincing results that lack quantitative accuracy. In this work, we propose HazeMatching, a novel iterative method for dehazing light microscopy images, which effectively balances these objectives. Our goal was to find a balanced trade-off between the fidelity of the dehazing results and the realism of individual predictions (samples). We achieve this by adapting the conditional flow matching framework by guiding the generative process with a hazy observation in the conditional velocity field. We evaluate HazeMatching on 5 datasets, covering both synthetic and real data, assessing both distortion and perceptual quality. Our method is compared against 7 baselines, achieving a consistent balance between fidelity and realism on average. Additionally, with calibration analysis, we show that HazeMatching produces well-calibrated predictions. Note that our method does not need an explicit degradation operator to exist, making it easily applicable on real microscopy data. All data used for training and evaluation and our code will be publicly available under a permissive license.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [30] [Inverse Design of Diffractive Metasurfaces Using Diffusion Models](https://arxiv.org/abs/2506.21748)
*Liav Hen,Erez Yosef,Dan Raviv,Raja Giryes,Jacob Scheuer*

Main category: physics.optics

TL;DR: 本文提出了一种基于扩散模型的逆向设计方法，用于高效设计超表面结构，解决了传统方法中的局部最优和计算开销问题。


<details>
  <summary>Details</summary>
Motivation: 超表面的逆向设计因结构与光学性质的非线性关系而复杂，传统方法依赖专家调参且易陷入局部最优，计算成本高。

Method: 结合扩散模型与计算设计流程，利用RCWA模拟器生成训练数据，训练条件扩散模型以预测目标光学响应对应的超表面几何结构。

Result: 模型能高效生成低误差超表面，如均匀强度分束器和偏振分束器，设计时间少于30分钟。

Conclusion: 扩散模型为超表面设计提供了高效工具，公开的代码和数据集将推动数据驱动设计的研究。

Abstract: Metasurfaces are ultra-thin optical elements composed of engineered sub-wavelength structures that enable precise control of light. Their inverse design - determining a geometry that yields a desired optical response - is challenging due to the complex, nonlinear relationship between structure and optical properties. This often requires expert tuning, is prone to local minima, and involves significant computational overhead. In this work, we address these challenges by integrating the generative capabilities of diffusion models into computational design workflows. Using an RCWA simulator, we generate training data consisting of metasurface geometries and their corresponding far-field scattering patterns. We then train a conditional diffusion model to predict meta-atom geometry and height from a target spatial power distribution at a specified wavelength, sampled from a continuous supported band. Once trained, the model can generate metasurfaces with low error, either directly using RCWA-guided posterior sampling or by serving as an initializer for traditional optimization methods. We demonstrate our approach on the design of a spatially uniform intensity splitter and a polarization beam splitter, both produced with low error in under 30 minutes. To support further research in data-driven metasurface design, we publicly release our code and datasets.

</details>
