<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 19]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [BANG: Dividing 3D Assets via Generative Exploded Dynamics](https://arxiv.org/abs/2507.21493)
*Longwen Zhang,Qixuan Zhang,Haoran Jiang,Yinuo Bai,Wei Yang,Lan Xu,Jingyi Yu*

Main category: cs.GR

TL;DR: BANG是一种新型生成方法，通过“生成爆炸动力学”实现3D对象的部分级分解，结合预训练扩散模型和空间提示，提供直观的3D创作工具。


<details>
  <summary>Details</summary>
Motivation: 当前3D设计工具需要大量专业知识和手动操作，难以模拟人类自然的分解与重组能力。BANG旨在提供更直观、灵活的3D创作方式。

Method: BANG利用预训练的大规模潜在扩散模型，结合轻量级爆炸视图适配器和时间注意力模块，实现几何和语义一致的部分分解。支持空间提示和多模态模型（如GPT-4）辅助交互。

Result: BANG能生成详细的部分级几何结构，关联功能描述，并支持3D打印中的可分离部件生成。

Conclusion: BANG为3D创作提供了更符合人类直觉的工具，扩展了从概念到详细3D资产的转换能力。

Abstract: 3D creation has always been a unique human strength, driven by our ability to deconstruct and reassemble objects using our eyes, mind and hand. However, current 3D design tools struggle to replicate this natural process, requiring considerable artistic expertise and manual labor. This paper introduces BANG, a novel generative approach that bridges 3D generation and reasoning, allowing for intuitive and flexible part-level decomposition of 3D objects. At the heart of BANG is "Generative Exploded Dynamics", which creates a smooth sequence of exploded states for an input geometry, progressively separating parts while preserving their geometric and semantic coherence.   BANG utilizes a pre-trained large-scale latent diffusion model, fine-tuned for exploded dynamics with a lightweight exploded view adapter, allowing precise control over the decomposition process. It also incorporates a temporal attention module to ensure smooth transitions and consistency across time. BANG enhances control with spatial prompts, such as bounding boxes and surface regions, enabling users to specify which parts to decompose and how. This interaction can be extended with multimodal models like GPT-4, enabling 2D-to-3D manipulations for more intuitive and creative workflows.   The capabilities of BANG extend to generating detailed part-level geometry, associating parts with functional descriptions, and facilitating component-aware 3D creation and manufacturing workflows. Additionally, BANG offers applications in 3D printing, where separable parts are generated for easy printing and reassembly. In essence, BANG enables seamless transformation from imaginative concepts to detailed 3D assets, offering a new perspective on creation that resonates with human intuition.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [PanoGAN A Deep Generative Model for Panoramic Dental Radiographs](https://arxiv.org/abs/2507.21200)
*Soren Pedersen,Sanyam Jain,Mikkel Chavez,Viktor Ladehoff,Bruna Neves de Freitas,Ruben Pauwels*

Main category: cs.CV

TL;DR: 本文开发了一种用于合成牙科全景X光片的生成对抗网络（GAN），旨在解决牙科研究和教育中数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 解决牙科研究和教育中数据稀缺的问题。

Method: 使用Wasserstein损失和梯度惩罚（WGANGP）训练深度卷积GAN（DCGAN），数据集包含2322张质量不一的X光片。对输入进行了预处理和数据清理，探索了四种候选模型。

Result: 生成的X光片在解剖可视性和真实性方面表现中等，部分图像存在伪影。未去噪数据训练的模型在细节表现上更优，而去噪数据训练的模型整体清晰度更高。

Conclusion: 为未来基于GAN的牙科影像方法研究奠定了基础。

Abstract: This paper presents the development of a generative adversarial network (GAN) for synthesizing dental panoramic radiographs. Although exploratory in nature, the study aims to address the scarcity of data in dental research and education. We trained a deep convolutional GAN (DCGAN) using a Wasserstein loss with gradient penalty (WGANGP) on a dataset of 2322 radiographs of varying quality. The focus was on the dentoalveolar regions, other anatomical structures were cropped out. Extensive preprocessing and data cleaning were performed to standardize the inputs while preserving anatomical variability. We explored four candidate models by varying critic iterations, feature depth, and the use of denoising prior to training. A clinical expert evaluated the generated radiographs based on anatomical visibility and realism, using a 5-point scale (1 very poor 5 excellent). Most images showed moderate anatomical depiction, although some were degraded by artifacts. A trade-off was observed the model trained on non-denoised data yielded finer details especially in structures like the mandibular canal and trabecular bone, while a model trained on denoised data offered superior overall image clarity and sharpness. These findings provide a foundation for future work on GAN-based methods in dental imaging.

</details>


### [3] [HDR Environment Map Estimation with Latent Diffusion Models](https://arxiv.org/abs/2507.21261)
*Jack Hilliard,Adrian Hilton,Jean-Yves Guillemaut*

Main category: cs.CV

TL;DR: 提出了一种基于潜在扩散模型（LDM）的新方法，用于从单视图图像估计HDR环境贴图，解决了ERP格式的边界伪影问题，并提出了全景适应的扩散变换器架构（PanoDiT）。


<details>
  <summary>Details</summary>
Motivation: 解决ERP格式在环境贴图中的极点和边界伪影问题，提升单视图图像生成环境贴图的质量和准确性。

Method: 使用潜在扩散模型（LDM）生成环境贴图，提出ERP卷积填充消除边界伪影，并设计全景适应的扩散变换器架构（PanoDiT）以减少ERP失真。

Result: 模型在标准基准测试中表现优异，生成的环境贴图在图像质量和光照准确性上与最先进方法竞争。

Conclusion: 提出的方法有效解决了ERP格式的伪影问题，并通过PanoDiT提升了环境贴图的质量，尽管在图像真实感上有所牺牲。

Abstract: We advance the field of HDR environment map estimation from a single-view image by establishing a novel approach leveraging the Latent Diffusion Model (LDM) to produce high-quality environment maps that can plausibly light mirror-reflective surfaces. A common issue when using the ERP representation, the format used by the vast majority of approaches, is distortions at the poles and a seam at the sides of the environment map. We remove the border seam artefact by proposing an ERP convolutional padding in the latent autoencoder. Additionally, we investigate whether adapting the diffusion network architecture to the ERP format can improve the quality and accuracy of the estimated environment map by proposing a panoramically-adapted Diffusion Transformer architecture. Our proposed PanoDiT network reduces ERP distortions and artefacts, but at the cost of image quality and plausibility. We evaluate with standard benchmarks to demonstrate that our models estimate high-quality environment maps that perform competitively with state-of-the-art approaches in both image quality and lighting accuracy.

</details>


### [4] [Group Relative Augmentation for Data Efficient Action Detection](https://arxiv.org/abs/2507.21353)
*Deep Anil Patel,Iain Melvin,Zachary Izzo,Martin Renqiang Min*

Main category: cs.CV

TL;DR: 提出了一种结合参数高效调优（LoRA）和可学习内部特征增强的策略，用于高效适应大型视频-语言模型（VLMs）进行动作检测，解决了过拟合和粒度不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型VLMs在动作检测任务中因少量样本导致的过拟合问题，以及场景级预训练与人物中心理解之间的粒度不匹配问题。

Method: 结合LoRA和可学习的内部特征增强（通过FiLM实现），并引入基于预测差异动态调整样本训练贡献的组加权损失函数。

Result: 在复杂多标签、多人动作检测数据集（AVA、MOMA）上表现出色，实现了较高的mAP性能，并展示了从少量样本中高效适应VLMs的能力。

Conclusion: 该方法通过特征增强和动态损失函数，显著提升了VLMs在动作检测任务中的适应效率和性能。

Abstract: Adapting large Video-Language Models (VLMs) for action detection using only a few examples poses challenges like overfitting and the granularity mismatch between scene-level pre-training and required person-centric understanding. We propose an efficient adaptation strategy combining parameter-efficient tuning (LoRA) with a novel learnable internal feature augmentation. Applied within the frozen VLM backbone using FiLM, these augmentations generate diverse feature variations directly relevant to the task. Additionally, we introduce a group-weighted loss function that dynamically modulates the training contribution of each augmented sample based on its prediction divergence relative to the group average. This promotes robust learning by prioritizing informative yet reasonable augmentations. We demonstrate our method's effectiveness on complex multi-label, multi-person action detection datasets (AVA, MOMA), achieving strong mAP performance and showcasing significant data efficiency for adapting VLMs from limited examples.

</details>


### [5] [Top2Pano: Learning to Generate Indoor Panoramas from Top-Down View](https://arxiv.org/abs/2507.21371)
*Zitong Zhang,Suranjan Gautam,Rui Yu*

Main category: cs.CV

TL;DR: Top2Pano是一个端到端模型，用于从2D俯视图生成逼真的360度室内全景图，解决了缺乏显式3D结构和几何一致性的挑战。


<details>
  <summary>Details</summary>
Motivation: 从2D俯视图生成360度室内全景图在虚拟现实、室内设计、房地产和机器人技术中有广泛应用，但缺乏3D结构和几何一致性是主要挑战。

Method: Top2Pano通过估计体积占用率推断3D结构，使用体积渲染生成粗糙的颜色和深度全景图，再通过ControlNet进行基于扩散的细化，提升真实感和结构保真度。

Result: 在两个数据集上的评估表明，Top2Pano优于基线方法，能有效重建几何、遮挡和空间布局，并能从示意性平面图生成高质量全景图。

Conclusion: Top2Pano在连接俯视图与沉浸式室内合成方面具有潜力，展示了其在实际应用中的价值。

Abstract: Generating immersive 360{\deg} indoor panoramas from 2D top-down views has applications in virtual reality, interior design, real estate, and robotics. This task is challenging due to the lack of explicit 3D structure and the need for geometric consistency and photorealism. We propose Top2Pano, an end-to-end model for synthesizing realistic indoor panoramas from top-down views. Our method estimates volumetric occupancy to infer 3D structures, then uses volumetric rendering to generate coarse color and depth panoramas. These guide a diffusion-based refinement stage using ControlNet, enhancing realism and structural fidelity. Evaluations on two datasets show Top2Pano outperforms baselines, effectively reconstructing geometry, occlusions, and spatial arrangements. It also generalizes well, producing high-quality panoramas from schematic floorplans. Our results highlight Top2Pano's potential in bridging top-down views with immersive indoor synthesis.

</details>


### [6] [Multimodal LLMs as Customized Reward Models for Text-to-Image Generation](https://arxiv.org/abs/2507.21391)
*Shijie Zhou,Ruiyi Zhang,Huaisheng Zhu,Branislav Kveton,Yufan Zhou,Jiuxiang Gu,Jian Chen,Changyou Chen*

Main category: cs.CV

TL;DR: LLaVA-Reward是一种高效的奖励模型，利用预训练的多模态大语言模型（MLLMs）自动评估文本到图像（T2I）生成的多个方面。通过Skip-connection Cross Attention模块增强视觉与文本交互，支持多种偏好数据微调，在多个评估维度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLM的方法需要指令跟随数据进行监督微调，且评估生成质量时依赖文本响应分析，耗时且训练困难。LLaVA-Reward旨在解决这些问题。

Method: 直接利用MLLMs的隐藏状态处理文本-图像对，并引入SkipCA模块增强视觉与文本的双向交互。支持配对和非配对偏好数据微调。

Result: 在文本-图像对齐、保真度/伪影、安全性和整体排名四个评估维度上，LLaVA-Reward优于传统和基于MLLM的方法。

Conclusion: LLaVA-Reward提供了一种高效、多视角的自动评估方法，显著提升了文本到图像生成的评估质量和效率。

Abstract: We introduce LLaVA-Reward, an efficient reward model designed to automatically evaluate text-to-image (T2I) generations across multiple perspectives, leveraging pretrained multimodal large language models (MLLMs). Existing MLLM-based approaches require instruction-following data for supervised fine-tuning and evaluate generation quality on analyzing text response, which is time-consuming and difficult to train. To address this problem, we propose LLaVA-Reward, which directly utilizes the hidden states of MLLMs given text-image pairs. To enhance the bidirectional interaction between visual and textual representations in decoder-only MLLMs, we further propose adding a Skip-connection Cross Attention (SkipCA) module. This design enhances text-image correlation reasoning by connecting early-layer visual features with later-layer hidden representations.In addition, LLaVA-Reward supports different types of preference data for efficient fine-tuning, including paired preference data and unpaired data. We train LLaVA-Reward on four evaluation perspectives: text-image alignment, fidelity/artifact, safety, and overall ranking. Empirical results demonstrate that LLaVA-Reward outperforms conventional and MLLM-based methods in generating human-aligned scores for automatic evaluations and inference-time scaling in text-to-image generations.

</details>


### [7] [Describe, Adapt and Combine: Empowering CLIP Encoders for Open-set 3D Object Retrieval](https://arxiv.org/abs/2507.21489)
*Zhichuan Wang,Yang Zhou,Zhe Liu,Rui Yu,Song Bai,Yulong Wang,Xinwei He,Xiang Bai*

Main category: cs.CV

TL;DR: DAC框架利用CLIP和多模态大语言模型（MLLM）提升开放集3D物体检索性能，仅需多视角图像，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在开放集3D物体检索中因数据不足导致泛化能力差的问题。

Method: 结合CLIP和MLLM，利用AB-LoRA适应技术，通过描述和外部提示学习泛化表示。

Result: 在四个数据集上平均提升10.01% mAP，验证了泛化能力。

Conclusion: DAC框架简单有效，显著提升开放集3D物体检索性能。

Abstract: Open-set 3D object retrieval (3DOR) is an emerging task aiming to retrieve 3D objects of unseen categories beyond the training set. Existing methods typically utilize all modalities (i.e., voxels, point clouds, multi-view images) and train specific backbones before fusion. However, they still struggle to produce generalized representations due to insufficient 3D training data. Being contrastively pre-trained on web-scale image-text pairs, CLIP inherently produces generalized representations for a wide range of downstream tasks. Building upon it, we present a simple yet effective framework named Describe, Adapt and Combine (DAC) by taking only multi-view images for open-set 3DOR. DAC innovatively synergizes a CLIP model with a multi-modal large language model (MLLM) to learn generalized 3D representations, where the MLLM is used for dual purposes. First, it describes the seen category information to align with CLIP's training objective for adaptation during training. Second, it provides external hints about unknown objects complementary to visual cues during inference. To improve the synergy, we introduce an Additive-Bias Low-Rank adaptation (AB-LoRA), which alleviates overfitting and further enhances the generalization to unseen categories. With only multi-view images, DAC significantly surpasses prior arts by an average of +10.01\% mAP on four open-set 3DOR datasets. Moreover, its generalization is also validated on image-based and cross-dataset setups. Code is available at https://github.com/wangzhichuan123/DAC.

</details>


### [8] [Optimizing Active Learning in Vision-Language Models via Parameter-Efficient Uncertainty Calibration](https://arxiv.org/abs/2507.21521)
*Athmanarayanan Lakshmi Narayanan,Amrutha Machireddy,Ranganath Krishnan*

Main category: cs.CV

TL;DR: 本文提出了一种参数高效学习方法，通过不确定性校准损失改进主动学习框架，显著减少标注成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言模型的主动学习面临不确定性估计和高效采样的挑战，需要一种参数高效的方法。

Method: 引入可微损失函数，结合不确定性校准，选择信息量最大的样本进行微调。

Result: 实验表明，该方法在多个数据集和视觉骨干上性能优于复杂特征采样技术，且计算高效。

Conclusion: Prompt学习与LoRA在样本选择中的效果比较为高效主动学习提供了新视角。

Abstract: Active Learning (AL) has emerged as a powerful approach for minimizing labeling costs by selectively sampling the most informative data for neural network model development. Effective AL for large-scale vision-language models necessitates addressing challenges in uncertainty estimation and efficient sampling given the vast number of parameters involved. In this work, we introduce a novel parameter-efficient learning methodology that incorporates uncertainty calibration loss within the AL framework. We propose a differentiable loss function that promotes uncertainty calibration for effectively selecting fewer and most informative data samples for fine-tuning. Through extensive experiments across several datasets and vision backbones, we demonstrate that our solution can match and exceed the performance of complex feature-based sampling techniques while being computationally very efficient. Additionally, we investigate the efficacy of Prompt learning versus Low-rank adaptation (LoRA) in sample selection, providing a detailed comparative analysis of these methods in the context of efficient AL.

</details>


### [9] [Chain-of-Cooking:Cooking Process Visualization via Bidirectional Chain-of-Thought Guidance](https://arxiv.org/abs/2507.21529)
*Mengling Xu,Ming Tao,Bing-Kun Bao*

Main category: cs.CV

TL;DR: 提出了一种名为Chain-of-Cooking的模型，用于解决烹饪过程可视化中的语义一致性和上下文连贯性问题，通过动态补丁选择模块和双向思维链引导，显著提升了生成图像的质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注生成成品食物的图像，难以处理烹饪过程中食材外观变化和步骤间的依赖关系，导致语义不一致和上下文不连贯。

Method: 采用动态补丁选择模块检索相关图像补丁，结合语义进化模块和双向思维链引导，确保生成图像的正确外观和连贯性。

Result: 实验表明，该方法在生成连贯且语义一致的烹饪过程图像方面优于现有方法。

Conclusion: Chain-of-Cooking模型通过创新的模块设计，有效解决了烹饪过程可视化中的关键挑战。

Abstract: Cooking process visualization is a promising task in the intersection of image generation and food analysis, which aims to generate an image for each cooking step of a recipe. However, most existing works focus on generating images of finished foods based on the given recipes, and face two challenges to visualize the cooking process. First, the appearance of ingredients changes variously across cooking steps, it is difficult to generate the correct appearances of foods that match the textual description, leading to semantic inconsistency. Second, the current step might depend on the operations of previous step, it is crucial to maintain the contextual coherence of images in sequential order. In this work, we present a cooking process visualization model, called Chain-of-Cooking. Specifically, to generate correct appearances of ingredients, we present a Dynamic Patch Selection Module to retrieve previously generated image patches as references, which are most related to current textual contents. Furthermore, to enhance the coherence and keep the rational order of generated images, we propose a Semantic Evolution Module and a Bidirectional Chain-of-Thought (CoT) Guidance. To better utilize the semantics of previous texts, the Semantic Evolution Module establishes the semantical association between latent prompts and current cooking step, and merges it with the latent features. Then the CoT Guidance updates the merged features to guide the current cooking step remain coherent with the previous step. Moreover, we construct a dataset named CookViz, consisting of intermediate image-text pairs for the cooking process. Quantitative and qualitative experiments show that our method outperforms existing methods in generating coherent and semantic consistent cooking process.

</details>


### [10] [Decoupled Spatio-Temporal Consistency Learning for Self-Supervised Tracking](https://arxiv.org/abs/2507.21606)
*Yaozong Zheng,Bineng Zhong,Qihua Liang,Ning Li,Shuxiang Song*

Main category: cs.CV

TL;DR: 论文提出了一种名为SSTrack的自监督跟踪框架，无需手动标注框即可学习通用跟踪表示，通过解耦的时空一致性训练框架和实例对比损失，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉跟踪数据集依赖人工标注框，成本高且限制了数据规模和多样性，因此需要一种无需标注的自监督方法。

Method: 提出解耦的时空一致性训练框架，结合全局空间定位和局部时间关联，以及多视角的实例对比损失。

Result: 在九个基准数据集上，SSTrack优于现有自监督跟踪方法，AUC（AO）得分提升显著。

Conclusion: SSTrack通过自监督学习有效减少对标注框的依赖，为视觉跟踪提供了新范式。

Abstract: The success of visual tracking has been largely driven by datasets with manual box annotations. However, these box annotations require tremendous human effort, limiting the scale and diversity of existing tracking datasets. In this work, we present a novel Self-Supervised Tracking framework named \textbf{{\tracker}}, designed to eliminate the need of box annotations. Specifically, a decoupled spatio-temporal consistency training framework is proposed to learn rich target information across timestamps through global spatial localization and local temporal association. This allows for the simulation of appearance and motion variations of instances in real-world scenarios. Furthermore, an instance contrastive loss is designed to learn instance-level correspondences from a multi-view perspective, offering robust instance supervision without additional labels. This new design paradigm enables {\tracker} to effectively learn generic tracking representations in a self-supervised manner, while reducing reliance on extensive box annotations. Extensive experiments on nine benchmark datasets demonstrate that {\tracker} surpasses \textit{SOTA} self-supervised tracking methods, achieving an improvement of more than 25.3\%, 20.4\%, and 14.8\% in AUC (AO) score on the GOT10K, LaSOT, TrackingNet datasets, respectively. Code: https://github.com/GXNU-ZhongLab/SSTrack.

</details>


### [11] [GuidPaint: Class-Guided Image Inpainting with Diffusion Models](https://arxiv.org/abs/2507.21627)
*Qimin Wang,Xinda Liu,Guohua Geng*

Main category: cs.CV

TL;DR: GuidPaint是一种无需训练的、基于分类器引导的图像修复框架，通过调整去噪过程实现精细控制，提升语义一致性和视觉真实感。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的多模态修复方法需修改架构和重新训练，计算成本高；而上下文感知方法虽无需训练，但缺乏对掩码区域的精细控制，导致语义不一致或视觉不合理。

Method: 提出GuidPaint框架，结合分类器引导和随机与确定性采样，实现对掩码区域的精确控制和结果优化。

Result: 实验表明，GuidPaint在定性和定量评估上均优于现有上下文感知修复方法。

Conclusion: GuidPaint通过分类器引导和采样策略，显著提升了图像修复的质量和可控性，无需额外训练。

Abstract: In recent years, diffusion models have been widely adopted for image inpainting tasks due to their powerful generative capabilities, achieving impressive results. Existing multimodal inpainting methods based on diffusion models often require architectural modifications and retraining, resulting in high computational cost. In contrast, context-aware diffusion inpainting methods leverage the model's inherent priors to adjust intermediate denoising steps, enabling high-quality inpainting without additional training and significantly reducing computation. However, these methods lack fine-grained control over the masked regions, often leading to semantically inconsistent or visually implausible content. To address this issue, we propose GuidPaint, a training-free, class-guided image inpainting framework. By incorporating classifier guidance into the denoising process, GuidPaint enables precise control over intermediate generations within the masked areas, ensuring both semantic consistency and visual realism. Furthermore, it integrates stochastic and deterministic sampling, allowing users to select preferred intermediate results and deterministically refine them. Experimental results demonstrate that GuidPaint achieves clear improvements over existing context-aware inpainting methods in both qualitative and quantitative evaluations.

</details>


### [12] [APT: Improving Diffusion Models for High Resolution Image Generation with Adaptive Path Tracing](https://arxiv.org/abs/2507.21690)
*Sangmin Han,Jinho Jeong,Jinwoo Kim,Seon Joo Kim*

Main category: cs.CV

TL;DR: APT框架通过统计匹配和尺度感知调度解决了基于补丁方法中的分布偏移和单调性问题，提升了高分辨率图像生成的细节和速度。


<details>
  <summary>Details</summary>
Motivation: 固定分辨率的潜在扩散模型在高分辨率图像生成中存在局限性，而基于补丁的方法虽然高效，但存在分布偏移和单调性问题。

Method: 提出APT框架，结合统计匹配和尺度感知调度，优化补丁分布和调度策略。

Result: APT生成的高分辨率图像细节更清晰，推理速度更快。

Conclusion: APT为高分辨率图像生成提供了一种实用且高效的解决方案。

Abstract: Latent Diffusion Models (LDMs) are generally trained at fixed resolutions, limiting their capability when scaling up to high-resolution images. While training-based approaches address this limitation by training on high-resolution datasets, they require large amounts of data and considerable computational resources, making them less practical. Consequently, training-free methods, particularly patch-based approaches, have become a popular alternative. These methods divide an image into patches and fuse the denoising paths of each patch, showing strong performance on high-resolution generation. However, we observe two critical issues for patch-based approaches, which we call ``patch-level distribution shift" and ``increased patch monotonicity." To address these issues, we propose Adaptive Path Tracing (APT), a framework that combines Statistical Matching to ensure patch distributions remain consistent in upsampled latents and Scale-aware Scheduling to deal with the patch monotonicity. As a result, APT produces clearer and more refined details in high-resolution images. In addition, APT enables a shortcut denoising process, resulting in faster sampling with minimal quality degradation. Our experimental results confirm that APT produces more detailed outputs with improved inference speed, providing a practical approach to high-resolution image generation.

</details>


### [13] [Impact of Underwater Image Enhancement on Feature Matching](https://arxiv.org/abs/2507.21715)
*Jason M. Summers,Mark W. Jones*

Main category: cs.CV

TL;DR: 提出了一种用于评估水下图像增强效果的新框架，包括局部匹配稳定性和最远可匹配帧作为量化指标。


<details>
  <summary>Details</summary>
Motivation: 水下图像因光线吸收、散射、海洋生物和碎片等问题导致视觉退化，增强后的图像对水下机器人的路径检测和自主导航等任务至关重要。

Method: 提出了一种针对水下环境的评估框架，通过度量分析评估现有方法的优缺点，并结合实际匹配策略。

Result: 框架为增强方法提供了上下文感知的基准，并通过SLAM算法验证了视觉改进对实际应用的影响。

Conclusion: 该框架为水下图像增强技术的评估提供了实用且鲁棒的标准，强调了其在真实场景中的适用性。

Abstract: We introduce local matching stability and furthest matchable frame as quantitative measures for evaluating the success of underwater image enhancement. This enhancement process addresses visual degradation caused by light absorption, scattering, marine growth, and debris. Enhanced imagery plays a critical role in downstream tasks such as path detection and autonomous navigation for underwater vehicles, relying on robust feature extraction and frame matching. To assess the impact of enhancement techniques on frame-matching performance, we propose a novel evaluation framework tailored to underwater environments. Through metric-based analysis, we identify strengths and limitations of existing approaches and pinpoint gaps in their assessment of real-world applicability. By incorporating a practical matching strategy, our framework offers a robust, context-aware benchmark for comparing enhancement methods. Finally, we demonstrate how visual improvements affect the performance of a complete real-world algorithm -- Simultaneous Localization and Mapping (SLAM) -- reinforcing the framework's relevance to operational underwater scenarios.

</details>


### [14] [Detection Transformers Under the Knife: A Neuroscience-Inspired Approach to Ablations](https://arxiv.org/abs/2507.21723)
*Nils Hütten,Florian Hölken,Hasan Tercan,Tobias Meisen*

Main category: cs.CV

TL;DR: 该论文通过神经科学启发的消融研究，分析了三种检测变换器模型（DETR、DDETR、DINO）内部组件的作用，揭示了模型特定的鲁棒性模式，并提出了优化透明度和效率的见解。


<details>
  <summary>Details</summary>
Motivation: 尽管可解释AI在复杂模型中取得进展，但对内部组件作用的理解仍存在研究空白，这对提高透明度和效率至关重要。

Method: 通过消融研究，系统地分析了三种检测变换器模型的关键组件（如查询嵌入、编码器和解码器的多头自注意力层）对性能的影响。

Result: 研究发现DETR对编码器和解码器注意力层消融敏感，DDETR因多尺度可变形注意力更鲁棒，DINO因知识分布机制最具韧性。同时揭示了结构冗余。

Conclusion: 该研究通过明确内部组件对性能的贡献，推动了DETR的可解释性，为优化透明度和效率提供了方向。

Abstract: In recent years, Explainable AI has gained traction as an approach to enhancing model interpretability and transparency, particularly in complex models such as detection transformers. Despite rapid advancements, a substantial research gap remains in understanding the distinct roles of internal components - knowledge that is essential for improving transparency and efficiency. Inspired by neuroscientific ablation studies, which investigate the functions of brain regions through selective impairment, we systematically analyze the impact of ablating key components in three state-of-the-art detection transformer models: Detection transformer (DETR), deformable detection transformer (DDETR), and DETR with improved denoising anchor boxes (DINO). The ablations target query embeddings, encoder and decoder multi-head self-attentions (MHSA) as well as decoder multi-head cross-attention (MHCA) layers. We evaluate the effects of these ablations on the performance metrics gIoU and F1-score, quantifying effects on both the classification and regression sub-tasks on the COCO dataset. To facilitate reproducibility and future research, we publicly release the DeepDissect library. Our findings reveal model-specific resilience patterns: while DETR is particularly sensitive to ablations in encoder MHSA and decoder MHCA, DDETR's multi-scale deformable attention enhances robustness, and DINO exhibits the greatest resilience due to its look-forward twice update rule, which helps distributing knowledge across blocks. These insights also expose structural redundancies, particularly in DDETR's and DINO's decoder MHCA layers, highlighting opportunities for model simplification without sacrificing performance. This study advances XAI for DETRs by clarifying the contributions of internal components to model performance, offering insights to optimize and improve transparency and efficiency in critical applications.

</details>


### [15] [Low-Cost Test-Time Adaptation for Robust Video Editing](https://arxiv.org/abs/2507.21858)
*Jianhui Wang,Yinda Chen,Yangfan He,Xinyuan Song,Yi Xin,Dapeng Zhang,Zhongwei Wan,Bin Li,Rongchao Zhang*

Main category: cs.CV

TL;DR: Vid-TTA是一种轻量级的测试时自适应框架，通过自监督辅助任务优化视频编辑，解决时间不一致性和提示过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法面临时间不一致性和提示过拟合的挑战，且计算资源需求高。

Method: 采用运动感知帧重建和提示扰动重建策略，结合元学习动态损失平衡机制。

Result: 显著提升视频时间一致性，减少提示过拟合，同时保持低计算开销。

Conclusion: Vid-TTA为现有视频编辑模型提供即插即用的性能提升。

Abstract: Video editing is a critical component of content creation that transforms raw footage into coherent works aligned with specific visual and narrative objectives. Existing approaches face two major challenges: temporal inconsistencies due to failure in capturing complex motion patterns, and overfitting to simple prompts arising from limitations in UNet backbone architectures. While learning-based methods can enhance editing quality, they typically demand substantial computational resources and are constrained by the scarcity of high-quality annotated data. In this paper, we present Vid-TTA, a lightweight test-time adaptation framework that personalizes optimization for each test video during inference through self-supervised auxiliary tasks. Our approach incorporates a motion-aware frame reconstruction mechanism that identifies and preserves crucial movement regions, alongside a prompt perturbation and reconstruction strategy that strengthens model robustness to diverse textual descriptions. These innovations are orchestrated by a meta-learning driven dynamic loss balancing mechanism that adaptively adjusts the optimization process based on video characteristics. Extensive experiments demonstrate that Vid-TTA significantly improves video temporal consistency and mitigates prompt overfitting while maintaining low computational overhead, offering a plug-and-play performance boost for existing video editing models.

</details>


### [16] [Evaluating Deepfake Detectors in the Wild](https://arxiv.org/abs/2507.21905)
*Viacheslav Pirogov,Maksim Artemev*

Main category: cs.CV

TL;DR: 本文评估了现代深度伪造检测器在真实场景中的表现，发现检测效果不佳，仅少数检测器AUC超过60%。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术对数字媒体真实性构成威胁，现有检测器在真实场景中的有效性尚未验证。

Method: 提出一种模拟真实场景的测试方法，使用先进深度伪造生成技术创建包含50万张高质量深度伪造图像的数据集。

Result: 检测深度伪造仍具挑战性，仅不到一半检测器AUC超过60%，最低为50%。基础图像处理（如JPEG压缩）会显著降低性能。

Conclusion: 深度伪造检测在真实场景中效果有限，需进一步改进检测方法。

Abstract: Deepfakes powered by advanced machine learning models present a significant and evolving threat to identity verification and the authenticity of digital media. Although numerous detectors have been developed to address this problem, their effectiveness has yet to be tested when applied to real-world data. In this work we evaluate modern deepfake detectors, introducing a novel testing procedure designed to mimic real-world scenarios for deepfake detection. Using state-of-the-art deepfake generation methods, we create a comprehensive dataset containing more than 500,000 high-quality deepfake images. Our analysis shows that detecting deepfakes still remains a challenging task. The evaluation shows that in fewer than half of the deepfake detectors tested achieved an AUC score greater than 60%, with the lowest being 50%. We demonstrate that basic image manipulations, such as JPEG compression or image enhancement, can significantly reduce model performance. All code and data are publicly available at https://github.com/messlav/Deepfake-Detectors-in-the-Wild.

</details>


### [17] [Enhancing Generalization in Data-free Quantization via Mixup-class Prompting](https://arxiv.org/abs/2507.21947)
*Jiwoong Park,Chaeun Lee,Yongseok Choi,Sein Park,Deokki Hong,Jungwook Choi*

Main category: cs.CV

TL;DR: 提出了一种基于mixup-class prompt的数据自由量化方法，通过融合多类别标签生成多样化的合成数据，提升了量化模型的泛化能力和优化稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决传统数据自由量化方法中因单类别提示导致的合成图像多义性问题，提升量化模型的性能。

Method: 使用mixup-class prompt策略，在文本提示层面融合多个类别标签，生成多样且鲁棒的合成数据，并结合现有PTQ算法。

Result: 在CNN和ViT上实验表明，该方法优于现有DFQ方法，尤其在极低位量化（如W2A4）中达到新SOTA。

Conclusion: mixup-class prompt策略有效提升了数据自由量化的性能，为极低位量化提供了新解决方案。

Abstract: Post-training quantization (PTQ) improves efficiency but struggles with limited calibration data, especially under privacy constraints. Data-free quantization (DFQ) mitigates this by generating synthetic images using generative models such as generative adversarial networks (GANs) and text-conditioned latent diffusion models (LDMs), while applying existing PTQ algorithms. However, the relationship between generated synthetic images and the generalizability of the quantized model during PTQ remains underexplored. Without investigating this relationship, synthetic images generated by previous prompt engineering methods based on single-class prompts suffer from issues such as polysemy, leading to performance degradation. We propose \textbf{mixup-class prompt}, a mixup-based text prompting strategy that fuses multiple class labels at the text prompt level to generate diverse, robust synthetic data. This approach enhances generalization, and improves optimization stability in PTQ. We provide quantitative insights through gradient norm and generalization error analysis. Experiments on convolutional neural networks (CNNs) and vision transformers (ViTs) show that our method consistently outperforms state-of-the-art DFQ methods like GenQ. Furthermore, it pushes the performance boundary in extremely low-bit scenarios, achieving new state-of-the-art accuracy in challenging 2-bit weight, 4-bit activation (W2A4) quantization.

</details>


### [18] [See Different, Think Better: Visual Variations Mitigating Hallucinations in LVLMs](https://arxiv.org/abs/2507.22003)
*Ziyun Dai,Xiaoqiang Li,Shaohua Zhang,Yuanchen Wu,Jide Li*

Main category: cs.CV

TL;DR: ViHallu是一个视觉中心的幻觉缓解框架，通过视觉变化图像生成和视觉指令构建增强视觉语义对齐，显著减少大视觉语言模型的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉缓解方法以文本为中心，视觉语义对齐的挑战限制了其效果，尤其是在细粒度视觉理解场景中。

Method: ViHallu通过生成视觉变化图像和构建视觉指令，增强模型的细粒度视觉理解能力。

Result: 在多个基准测试中，ViHallu显著减少了幻觉现象并提升了模型的视觉语义对齐能力。

Conclusion: ViHallu有效缓解了LVLMs的幻觉问题，并发布了专门用于幻觉缓解的视觉指令数据集ViHallu-Instruction。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in visual understanding and multimodal reasoning. However, LVLMs frequently exhibit hallucination phenomena, manifesting as the generated textual responses that demonstrate inconsistencies with the provided visual content. Existing hallucination mitigation methods are predominantly text-centric, the challenges of visual-semantic alignment significantly limit their effectiveness, especially when confronted with fine-grained visual understanding scenarios. To this end, this paper presents ViHallu, a Vision-Centric Hallucination mitigation framework that enhances visual-semantic alignment through Visual Variation Image Generation and Visual Instruction Construction. ViHallu introduces \textbf{\textit{visual variation images}} with controllable visual alterations while maintaining the overall image structure. These images, combined with carefully constructed visual instructions, enable LVLMs to better understand fine-grained visual content through fine-tuning, allowing models to more precisely capture the correspondence between visual content and text, thereby enhancing visual-semantic alignment. Extensive experiments on multiple benchmarks show that ViHallu effectively enhances models' fine-grained visual understanding while significantly reducing hallucination tendencies. Furthermore, we release ViHallu-Instruction, a visual instruction dataset specifically designed for hallucination mitigation and visual-semantic alignment. Code is available at https://github.com/oliviadzy/ViHallu.

</details>


### [19] [VeS: Teaching Pixels to Listen Without Supervision](https://arxiv.org/abs/2507.22008)
*Sajay Raj*

Main category: cs.CV

TL;DR: 论文研究了多语言环境下密集音频-视觉模型的性能，发现密集目标函数在低资源、多语言和嘈杂环境中表现优异，显著优于全局池化方法。


<details>
  <summary>Details</summary>
Motivation: 探讨密集音频-视觉模型在低资源、多语言和嘈杂环境中的表现，验证其是否适用于非英语为主的场景。

Method: 使用多语言数据集Project Vaani，对比三种对比目标函数：全局均值池化、密集最大-均值标记匹配器和混合方法。

Result: 密集目标函数在音频-视觉检索中相对R@1提升59%，并生成清晰的零样本定位热图，即使视觉主干完全冻结。

Conclusion: 密集标记路由不仅适用于高资源英语语料库，在低资源环境中更具决定性作用。

Abstract: Recent dense audio-visual (AV) models achieve impressive retrieval and emergent localization, but almost all evidence comes from English-centric, caption-rich web video. It is unclear whether these objectives survive in low-resource, code-switched, and noisy multilingual settings that typify developing regions. We show they do**-**and that the choice of aggregation function becomes even more critical. Using a multilingual subset of Project Vaani spanning dozens of Indian languages and dialectal variants, we compare three contrastive objectives: (i) a global mean-pooled loss (CLIP-style), (ii) a dense max-mean token matcher (DenseAV-style), and (iii) a simple hybrid (motivated by frozen-vision alignment strategies). The dense objective delivers a +59% relative R@1 (Audio Visual) improvement over global pooling and substantially lower mean/median ranks, while consistently producing sharp zero-shot localization heatmaps of spoken objects-despite keeping the vision backbone entirely frozen (no LoRA / partial fine-tuning). Our results demonstrate that dense token routing is not a luxury of high-resource English corpora; it is more decisive when annotations and acoustic cleanliness are scarce. We release the codebase and trained models.

</details>


### [20] [X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again](https://arxiv.org/abs/2507.22058)
*Zigang Geng,Yibing Wang,Yeyao Ma,Chen Li,Yongming Rao,Shuyang Gu,Zhao Zhong,Qinglin Lu,Han Hu,Xiaosong Zhang,Linus,Di Wang,Jie Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种基于强化学习的框架X-Omni，通过结合离散自回归模型和扩散解码器，解决了图像生成中的视觉保真度低和指令遵循能力差的问题，实现了图像与语言生成的无缝集成。


<details>
  <summary>Details</summary>
Motivation: 当前基于离散自回归模型的图像生成方法存在视觉保真度低、输出扭曲和复杂指令遵循能力差的问题，主要原因是自回归推理中的累积误差或离散化过程中的信息丢失。

Method: 提出X-Omni框架，包括语义图像分词器、统一的自回归模型（用于语言和图像）和离线扩散解码器，利用强化学习优化生成质量。

Result: X-Omni在图像生成任务中表现优异，使用7B语言模型生成了具有高美学质量的图像，并展现了强大的指令遵循和长文本渲染能力。

Conclusion: 强化学习可以有效提升离散自回归模型的生成质量，X-Omni框架为图像与语言生成的统一建模提供了可行方案。

Abstract: Numerous efforts have been made to extend the ``next token prediction'' paradigm to visual contents, aiming to create a unified approach for both image generation and understanding. Nevertheless, attempts to generate images through autoregressive modeling with discrete tokens have been plagued by issues such as low visual fidelity, distorted outputs, and failure to adhere to complex instructions when rendering intricate details. These shortcomings are likely attributed to cumulative errors during autoregressive inference or information loss incurred during the discretization process. Probably due to this challenge, recent research has increasingly shifted toward jointly training image generation with diffusion objectives and language generation with autoregressive objectives, moving away from unified modeling approaches. In this work, we demonstrate that reinforcement learning can effectively mitigate artifacts and largely enhance the generation quality of a discrete autoregressive modeling method, thereby enabling seamless integration of image and language generation. Our framework comprises a semantic image tokenizer, a unified autoregressive model for both language and images, and an offline diffusion decoder for image generation, termed X-Omni. X-Omni achieves state-of-the-art performance in image generation tasks using a 7B language model, producing images with high aesthetic quality while exhibiting strong capabilities in following instructions and rendering long texts.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [21] [Hot-Swap MarkBoard: An Efficient Black-box Watermarking Approach for Large-scale Model Distribution](https://arxiv.org/abs/2507.20650)
*Zhicheng Zhang,Peizhuo Lv,Mengke Wan,Jiang Fang,Diandian Guo,Yezeng Chen,Yinlong Liu,Wei Ma,Jiyan Sun,Liru Geng*

Main category: cs.CR

TL;DR: 提出了一种名为Hot-Swap MarkBoard的高效水印方法，用于保护分布式深度学习模型的IP，支持黑盒验证且无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型在终端设备上的部署增加，IP风险加剧，现有水印方法无法满足大规模分发场景的需求。

Method: 通过多分支LoRA模块独立嵌入多个水印，支持分支交换实现高效定制，并采用参数混淆机制防止水印移除。

Result: 在六种骨干模型和三类任务上的实验表明，该方法验证准确率达100%，效率与适应性优于现有方法。

Conclusion: Hot-Swap MarkBoard为分布式深度学习模型提供了高效、灵活且安全的IP保护解决方案。

Abstract: Recently, Deep Learning (DL) models have been increasingly deployed on end-user devices as On-Device AI, offering improved efficiency and privacy. However, this deployment trend poses more serious Intellectual Property (IP) risks, as models are distributed on numerous local devices, making them vulnerable to theft and redistribution. Most existing ownership protection solutions (e.g., backdoor-based watermarking) are designed for cloud-based AI-as-a-Service (AIaaS) and are not directly applicable to large-scale distribution scenarios, where each user-specific model instance must carry a unique watermark. These methods typically embed a fixed watermark, and modifying the embedded watermark requires retraining the model. To address these challenges, we propose Hot-Swap MarkBoard, an efficient watermarking method. It encodes user-specific $n$-bit binary signatures by independently embedding multiple watermarks into a multi-branch Low-Rank Adaptation (LoRA) module, enabling efficient watermark customization without retraining through branch swapping. A parameter obfuscation mechanism further entangles the watermark weights with those of the base model, preventing removal without degrading model performance. The method supports black-box verification and is compatible with various model architectures and DL tasks, including classification, image generation, and text generation. Extensive experiments across three types of tasks and six backbone models demonstrate our method's superior efficiency and adaptability compared to existing approaches, achieving 100\% verification accuracy.

</details>


### [22] [PRISM: Programmatic Reasoning with Image Sequence Manipulation for LVLM Jailbreaking](https://arxiv.org/abs/2507.21540)
*Quanchen Zou,Zonghao Ying,Moyang Chen,Wenzhuo Xu,Yisong Xiao,Yakai Li,Deyue Zhang,Dongdong Yang,Zhao Liu,Xiangzheng Zhang*

Main category: cs.CR

TL;DR: 论文提出了一种基于软件安全中ROP技术的新型越狱框架，通过将有害指令分解为多个看似无害的视觉片段，利用模型的推理能力生成有害内容，显著提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的大视觉语言模型（LVLM）安全对齐机制仍易受复杂对抗攻击，传统越狱方法忽略了模型在多步推理中的潜在漏洞。

Method: 提出了一种受ROP启发的越狱框架，将有害指令分解为多个无害视觉片段，并通过精心设计的文本提示引导模型整合这些片段生成有害输出。

Result: 在SafeBench和MM-SafetyBench基准测试中，该方法攻击成功率显著提升（SafeBench上超过0.90），ASR最高提升0.39。

Conclusion: 研究揭示了LVLM在组合推理能力中的关键漏洞，强调了保护整个推理过程的紧迫性。

Abstract: The increasing sophistication of large vision-language models (LVLMs) has been accompanied by advances in safety alignment mechanisms designed to prevent harmful content generation. However, these defenses remain vulnerable to sophisticated adversarial attacks. Existing jailbreak methods typically rely on direct and semantically explicit prompts, overlooking subtle vulnerabilities in how LVLMs compose information over multiple reasoning steps. In this paper, we propose a novel and effective jailbreak framework inspired by Return-Oriented Programming (ROP) techniques from software security. Our approach decomposes a harmful instruction into a sequence of individually benign visual gadgets. A carefully engineered textual prompt directs the sequence of inputs, prompting the model to integrate the benign visual gadgets through its reasoning process to produce a coherent and harmful output. This makes the malicious intent emergent and difficult to detect from any single component. We validate our method through extensive experiments on established benchmarks including SafeBench and MM-SafetyBench, targeting popular LVLMs. Results show that our approach consistently and substantially outperforms existing baselines on state-of-the-art models, achieving near-perfect attack success rates (over 0.90 on SafeBench) and improving ASR by up to 0.39. Our findings reveal a critical and underexplored vulnerability that exploits the compositional reasoning abilities of LVLMs, highlighting the urgent need for defenses that secure the entire reasoning process.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [23] [A Tactical Behaviour Recognition Framework Based on Causal Multimodal Reasoning: A Study on Covert Audio-Video Analysis Combining GAN Structure Enhancement and Phonetic Accent Modelling](https://arxiv.org/abs/2507.21100)
*Wei Meng*

Main category: cs.CY

TL;DR: TACTIC-GRAPHS结合谱图理论和多模态图神经网络推理，用于战术视频中的语义理解和威胁检测，在高噪声和弱结构下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决战术视频中高噪声和弱结构条件下的语义理解和威胁检测问题。

Method: 结合谱嵌入、时间因果边建模和异构模态的判别路径推理，采用语义感知关键帧提取方法融合视觉、声学和动作线索构建时间图。

Result: 在TACTIC-AVS和TACTIC-Voice数据集上，时间对齐准确率达89.3%，完整威胁链识别率超过85%，节点延迟在±150毫秒内。

Conclusion: 该方法提升了结构可解释性，适用于监视、防御和智能安全系统。

Abstract: This paper introduces TACTIC-GRAPHS, a system that combines spectral graph theory and multimodal graph neural reasoning for semantic understanding and threat detection in tactical video under high noise and weak structure. The framework incorporates spectral embedding, temporal causal edge modeling, and discriminative path inference across heterogeneous modalities. A semantic-aware keyframe extraction method fuses visual, acoustic, and action cues to construct temporal graphs. Using graph attention and Laplacian spectral mapping, the model performs cross-modal weighting and causal signal analysis. Experiments on TACTIC-AVS and TACTIC-Voice datasets show 89.3 percent accuracy in temporal alignment and over 85 percent recognition of complete threat chains, with node latency within plus-minus 150 milliseconds. The approach enhances structural interpretability and supports applications in surveillance, defense, and intelligent security systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [24] [MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE](https://arxiv.org/abs/2507.21802)
*Junzhe Li,Yutao Cui,Tao Huang,Yinping Ma,Chun Fan,Miles Yang,Zhao Zhong*

Main category: cs.AI

TL;DR: MixGRPO是一种新颖的框架，通过结合SDE和ODE的混合采样策略，优化了FlowGRPO的效率问题，显著提升了训练速度和性能。


<details>
  <summary>Details</summary>
Motivation: FlowGRPO等方法在图像生成的人类偏好对齐中效率低下，因为需要在MDP的所有去噪步骤中进行采样和优化。

Method: MixGRPO引入滑动窗口机制，窗口内使用SDE采样和GRPO优化，窗口外使用ODE采样，减少优化开销并加速收敛。

Result: MixGRPO在人类偏好对齐的多个维度上表现优异，训练时间减少50%，MixGRPO-Flash进一步减少71%。

Conclusion: MixGRPO通过混合采样策略显著提升了效率和性能，优于现有方法。

Abstract: Although GRPO substantially enhances flow matching models in human preference alignment of image generation, methods such as FlowGRPO still exhibit inefficiency due to the necessity of sampling and optimizing over all denoising steps specified by the Markov Decision Process (MDP). In this paper, we propose $\textbf{MixGRPO}$, a novel framework that leverages the flexibility of mixed sampling strategies through the integration of stochastic differential equations (SDE) and ordinary differential equations (ODE). This streamlines the optimization process within the MDP to improve efficiency and boost performance. Specifically, MixGRPO introduces a sliding window mechanism, using SDE sampling and GRPO-guided optimization only within the window, while applying ODE sampling outside. This design confines sampling randomness to the time-steps within the window, thereby reducing the optimization overhead, and allowing for more focused gradient updates to accelerate convergence. Additionally, as time-steps beyond the sliding window are not involved in optimization, higher-order solvers are supported for sampling. So we present a faster variant, termed $\textbf{MixGRPO-Flash}$, which further improves training efficiency while achieving comparable performance. MixGRPO exhibits substantial gains across multiple dimensions of human preference alignment, outperforming DanceGRPO in both effectiveness and efficiency, with nearly 50% lower training time. Notably, MixGRPO-Flash further reduces training time by 71%. Codes and models are available at $\href{https://github.com/Tencent-Hunyuan/MixGRPO}{MixGRPO}$.

</details>
