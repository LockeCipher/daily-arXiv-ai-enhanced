{"id": "2507.06109", "pdf": "https://arxiv.org/pdf/2507.06109", "abs": "https://arxiv.org/abs/2507.06109", "authors": ["Seungoh Han", "Jaehoon Jang", "Hyunsu Kim", "Jaeheung Surh", "Junhyung Kwak", "Hyowon Ha", "Kyungdon Joo"], "title": "LighthouseGS: Indoor Structure-aware 3D Gaussian Splatting for Panorama-Style Mobile Captures", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Preprint", "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time novel view synthesis (NVS) with impressive quality in indoor scenes. However, achieving high-fidelity rendering requires meticulously captured images covering the entire scene, limiting accessibility for general users. We aim to develop a practical 3DGS-based NVS framework using simple panorama-style motion with a handheld camera (e.g., mobile device). While convenient, this rotation-dominant motion and narrow baseline make accurate camera pose and 3D point estimation challenging, especially in textureless indoor scenes. To address these challenges, we propose LighthouseGS, a novel framework inspired by the lighthouse-like sweeping motion of panoramic views. LighthouseGS leverages rough geometric priors, such as mobile device camera poses and monocular depth estimation, and utilizes the planar structures often found in indoor environments. We present a new initialization method called plane scaffold assembly to generate consistent 3D points on these structures, followed by a stable pruning strategy to enhance geometry and optimization stability. Additionally, we introduce geometric and photometric corrections to resolve inconsistencies from motion drift and auto-exposure in mobile devices. Tested on collected real and synthetic indoor scenes, LighthouseGS delivers photorealistic rendering, surpassing state-of-the-art methods and demonstrating the potential for panoramic view synthesis and object placement."}
{"id": "2507.05300", "pdf": "https://arxiv.org/pdf/2507.05300", "abs": "https://arxiv.org/abs/2507.05300", "authors": ["Nicholas Merchant", "Haitz S\u00e1ez de Oc\u00e1riz Borde", "Andrei Cristian Popescu", "Carlos Garcia Jurado Suarez"], "title": "Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption 19M)", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "7-page main paper + appendix, 18 figures", "summary": "We argue that generative text-to-image models often struggle with prompt adherence due to the noisy and unstructured nature of large-scale datasets like LAION-5B. This forces users to rely heavily on prompt engineering to elicit desirable outputs. In this work, we propose that enforcing a consistent caption structure during training can significantly improve model controllability and alignment. We introduce Re-LAION-Caption 19M, a high-quality subset of Re-LAION-5B, comprising 19 million 1024x1024 images with captions generated by a Mistral 7B Instruct-based LLaVA-Next model. Each caption follows a four-part template: subject, setting, aesthetics, and camera details. We fine-tune PixArt-$\\Sigma$ and Stable Diffusion 2 using both structured and randomly shuffled captions, and show that structured versions consistently yield higher text-image alignment scores using visual question answering (VQA) models. The dataset is publicly available at https://huggingface.co/datasets/supermodelresearch/Re-LAION-Caption19M."}
{"id": "2507.05906", "pdf": "https://arxiv.org/pdf/2507.05906", "abs": "https://arxiv.org/abs/2507.05906", "authors": ["Chenhao Li", "Marco Hutter", "Andreas Krause"], "title": "Feature-Based vs. GAN-Based Learning from Demonstrations: When and Why", "categories": ["cs.LG", "cs.AI", "cs.GR", "cs.RO"], "comment": null, "summary": "This survey provides a comparative analysis of feature-based and GAN-based approaches to learning from demonstrations, with a focus on the structure of reward functions and their implications for policy learning. Feature-based methods offer dense, interpretable rewards that excel at high-fidelity motion imitation, yet often require sophisticated representations of references and struggle with generalization in unstructured settings. GAN-based methods, in contrast, use implicit, distributional supervision that enables scalability and adaptation flexibility, but are prone to training instability and coarse reward signals. Recent advancements in both paradigms converge on the importance of structured motion representations, which enable smoother transitions, controllable synthesis, and improved task integration. We argue that the dichotomy between feature-based and GAN-based methods is increasingly nuanced: rather than one paradigm dominating the other, the choice should be guided by task-specific priorities such as fidelity, diversity, interpretability, and adaptability. This work outlines the algorithmic trade-offs and design considerations that underlie method selection, offering a framework for principled decision-making in learning from demonstrations."}
{"id": "2507.05302", "pdf": "https://arxiv.org/pdf/2507.05302", "abs": "https://arxiv.org/abs/2507.05302", "authors": ["Binjia Zhou", "Hengrui Lou", "Lizhe Chen", "Haoyuan Li", "Dawei Luo", "Shuai Chen", "Jie Lei", "Zunlei Feng", "Yijun Bei"], "title": "CorrDetail: Visual Detail Enhanced Self-Correction for Face Forgery Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the swift progression of image generation technology, the widespread emergence of facial deepfakes poses significant challenges to the field of security, thus amplifying the urgent need for effective deepfake detection.Existing techniques for face forgery detection can broadly be categorized into two primary groups: visual-based methods and multimodal approaches. The former often lacks clear explanations for forgery details, while the latter, which merges visual and linguistic modalities, is more prone to the issue of hallucinations.To address these shortcomings, we introduce a visual detail enhanced self-correction framework, designated CorrDetail, for interpretable face forgery detection. CorrDetail is meticulously designed to rectify authentic forgery details when provided with error-guided questioning, with the aim of fostering the ability to uncover forgery details rather than yielding hallucinated responses. Additionally, to bolster the reliability of its findings, a visual fine-grained detail enhancement module is incorporated, supplying CorrDetail with more precise visual forgery details. Ultimately, a fusion decision strategy is devised to further augment the model's discriminative capacity in handling extreme samples, through the integration of visual information compensation and model bias reduction.Experimental results demonstrate that CorrDetail not only achieves state-of-the-art performance compared to the latest methodologies but also excels in accurately identifying forged details, all while exhibiting robust generalization capabilities."}
{"id": "2507.05393", "pdf": "https://arxiv.org/pdf/2507.05393", "abs": "https://arxiv.org/abs/2507.05393", "authors": ["Jose M. Montero", "Jose-Luis Lisani"], "title": "Enhancing Underwater Images Using Deep Learning with Subjective Image Quality Integration", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Recent advances in deep learning, particularly neural networks, have significantly impacted a wide range of fields, including the automatic enhancement of underwater images. This paper presents a deep learning-based approach to improving underwater image quality by integrating human subjective assessments into the training process. To this end, we utilize publicly available datasets containing underwater images labeled by experts as either high or low quality. Our method involves first training a classifier network to distinguish between high- and low-quality images. Subsequently, generative adversarial networks (GANs) are trained using various enhancement criteria to refine the low-quality images. The performance of the GAN models is evaluated using quantitative metrics such as PSNR, SSIM, and UIQM, as well as through qualitative analysis. Results demonstrate that the proposed model -- particularly when incorporating criteria such as color fidelity and image sharpness -- achieves substantial improvements in both perceived and measured image quality."}
{"id": "2507.05426", "pdf": "https://arxiv.org/pdf/2507.05426", "abs": "https://arxiv.org/abs/2507.05426", "authors": ["Lanqing Guo", "Yufei Wang", "Hezhen Hu", "Yan Zheng", "Yeying Jin", "Siyu Huang", "Zhangyang Wang"], "title": "Mastering Regional 3DGS: Locating, Initializing, and Editing with Diverse 2D Priors", "categories": ["cs.CV"], "comment": null, "summary": "Many 3D scene editing tasks focus on modifying local regions rather than the entire scene, except for some global applications like style transfer, and in the context of 3D Gaussian Splatting (3DGS), where scenes are represented by a series of Gaussians, this structure allows for precise regional edits, offering enhanced control over specific areas of the scene; however, the challenge lies in the fact that 3D semantic parsing often underperforms compared to its 2D counterpart, making targeted manipulations within 3D spaces more difficult and limiting the fidelity of edits, which we address by leveraging 2D diffusion editing to accurately identify modification regions in each view, followed by inverse rendering for 3D localization, then refining the frontal view and initializing a coarse 3DGS with consistent views and approximate shapes derived from depth maps predicted by a 2D foundation model, thereby supporting an iterative, view-consistent editing process that gradually enhances structural details and textures to ensure coherence across perspectives. Experiments demonstrate that our method achieves state-of-the-art performance while delivering up to a $4\\times$ speedup, providing a more efficient and effective approach to 3D scene local editing."}
{"id": "2507.05496", "pdf": "https://arxiv.org/pdf/2507.05496", "abs": "https://arxiv.org/abs/2507.05496", "authors": ["Andrew Randono"], "title": "Cloud Diffusion Part 1: Theory and Motivation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "39 pages, 21 figures. Associated code:   https://github.com/arandono/Cloud-Diffusion", "summary": "Diffusion models for image generation function by progressively adding noise to an image set and training a model to separate out the signal from the noise. The noise profile used by these models is white noise -- that is, noise based on independent normal distributions at each point whose mean and variance is independent of the scale. By contrast, most natural image sets exhibit a type of scale invariance in their low-order statistical properties characterized by a power-law scaling. Consequently, natural images are closer (in a quantifiable sense) to a different probability distribution that emphasizes large scale correlations and de-emphasizes small scale correlations. These scale invariant noise profiles can be incorporated into diffusion models in place of white noise to form what we will call a ``Cloud Diffusion Model\". We argue that these models can lead to faster inference, improved high-frequency details, and greater controllability. In a follow-up paper, we will build and train a Cloud Diffusion Model that uses scale invariance at a fundamental level and compare it to classic, white noise diffusion models."}
{"id": "2507.05499", "pdf": "https://arxiv.org/pdf/2507.05499", "abs": "https://arxiv.org/abs/2507.05499", "authors": ["Giulio Federico", "Fabio Carrara", "Claudio Gennaro", "Giuseppe Amato", "Marco Di Benedetto"], "title": "LoomNet: Enhancing Multi-View Image Generation via Latent Space Weaving", "categories": ["cs.CV"], "comment": null, "summary": "Generating consistent multi-view images from a single image remains challenging. Lack of spatial consistency often degrades 3D mesh quality in surface reconstruction. To address this, we propose LoomNet, a novel multi-view diffusion architecture that produces coherent images by applying the same diffusion model multiple times in parallel to collaboratively build and leverage a shared latent space for view consistency. Each viewpoint-specific inference generates an encoding representing its own hypothesis of the novel view from a given camera pose, which is projected onto three orthogonal planes. For each plane, encodings from all views are fused into a single aggregated plane. These aggregated planes are then processed to propagate information and interpolate missing regions, combining the hypotheses into a unified, coherent interpretation. The final latent space is then used to render consistent multi-view images. LoomNet generates 16 high-quality and coherent views in just 15 seconds. In our experiments, LoomNet outperforms state-of-the-art methods on both image quality and reconstruction metrics, also showing creativity by producing diverse, plausible novel views from the same input."}
{"id": "2507.05536", "pdf": "https://arxiv.org/pdf/2507.05536", "abs": "https://arxiv.org/abs/2507.05536", "authors": ["Moseli Mots'oehli", "Feimei Chen", "Hok Wai Chan", "Itumeleng Tlali", "Thulani Babeli", "Kyungim Baek", "Huaijin Chen"], "title": "Simulating Refractive Distortions and Weather-Induced Artifacts for Resource-Constrained Autonomous Perception", "categories": ["cs.CV", "cs.ET", "cs.LG"], "comment": "This paper has been submitted to the ICCV 2025 Workshop on Computer   Vision for Developing Countries (CV4DC) for review", "summary": "The scarcity of autonomous vehicle datasets from developing regions, particularly across Africa's diverse urban, rural, and unpaved roads, remains a key obstacle to robust perception in low-resource settings. We present a procedural augmentation pipeline that enhances low-cost monocular dashcam footage with realistic refractive distortions and weather-induced artifacts tailored to challenging African driving scenarios. Our refractive module simulates optical effects from low-quality lenses and air turbulence, including lens distortion, Perlin noise, Thin-Plate Spline (TPS), and divergence-free (incompressible) warps. The weather module adds homogeneous fog, heterogeneous fog, and lens flare. To establish a benchmark, we provide baseline performance using three image restoration models. To support perception research in underrepresented African contexts, without costly data collection, labeling, or simulation, we release our distortion toolkit, augmented dataset splits, and benchmark results."}
{"id": "2507.05588", "pdf": "https://arxiv.org/pdf/2507.05588", "abs": "https://arxiv.org/abs/2507.05588", "authors": ["Shuai Li", "Shihan Chen", "Wanru Geng", "Zhaohua Xu", "Xiaolu Liu", "Can Dong", "Zhen Tian", "Changlin Chen"], "title": "Semi-Supervised Defect Detection via Conditional Diffusion and CLIP-Guided Noise Filtering", "categories": ["cs.CV"], "comment": null, "summary": "In the realm of industrial quality inspection, defect detection stands as a critical component, particularly in high-precision, safety-critical sectors such as automotive components aerospace, and medical devices. Traditional methods, reliant on manual inspection or early image processing algorithms, suffer from inefficiencies, high costs, and limited robustness. This paper introduces a semi-supervised defect detection framework based on conditional diffusion (DSYM), leveraging a two-stage collaborative training mechanism and a staged joint optimization strategy. The framework utilizes labeled data for initial training and subsequently incorporates unlabeled data through the generation of pseudo-labels. A conditional diffusion model synthesizes multi-scale pseudo-defect samples, while a CLIP cross-modal feature-based noise filtering mechanism mitigates label contamination. Experimental results on the NEU-DET dataset demonstrate a 78.4% mAP@0.5 with the same amount of labeled data as traditional supervised methods, and 75.1% mAP@0.5 with only 40% of the labeled data required by the original supervised model, showcasing significant advantages in data efficiency. This research provides a high-precision, low-labeling-dependent solution for defect detection in industrial quality inspection scenarios. The work of this article has been open-sourced at https://github.com/cLin-c/Semisupervised-DSYM."}
{"id": "2507.05601", "pdf": "https://arxiv.org/pdf/2507.05601", "abs": "https://arxiv.org/abs/2507.05601", "authors": ["Jingye Chen", "Zhaowen Wang", "Nanxuan Zhao", "Li Zhang", "Difan Liu", "Jimei Yang", "Qifeng Chen"], "title": "Rethinking Layered Graphic Design Generation with a Top-Down Approach", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "Graphic design is crucial for conveying ideas and messages. Designers usually organize their work into objects, backgrounds, and vectorized text layers to simplify editing. However, this workflow demands considerable expertise. With the rise of GenAI methods, an endless supply of high-quality graphic designs in pixel format has become more accessible, though these designs often lack editability. Despite this, non-layered designs still inspire human designers, influencing their choices in layouts and text styles, ultimately guiding the creation of layered designs. Motivated by this observation, we propose Accordion, a graphic design generation framework taking the first attempt to convert AI-generated designs into editable layered designs, meanwhile refining nonsensical AI-generated text with meaningful alternatives guided by user prompts. It is built around a vision language model (VLM) playing distinct roles in three curated stages. For each stage, we design prompts to guide the VLM in executing different tasks. Distinct from existing bottom-up methods (e.g., COLE and Open-COLE) that gradually generate elements to create layered designs, our approach works in a top-down manner by using the visually harmonious reference image as global guidance to decompose each layer. Additionally, it leverages multiple vision experts such as SAM and element removal models to facilitate the creation of graphic layers. We train our method using the in-house graphic design dataset Design39K, augmented with AI-generated design images coupled with refined ground truth created by a customized inpainting model. Experimental results and user studies by designers show that Accordion generates favorable results on the DesignIntention benchmark, including tasks such as text-to-template, adding text to background, and text de-rendering, and also excels in creating design variations."}
{"id": "2507.05604", "pdf": "https://arxiv.org/pdf/2507.05604", "abs": "https://arxiv.org/abs/2507.05604", "authors": ["Yuyang Hu", "Kangfu Mei", "Mojtaba Sahraee-Ardakan", "Ulugbek S. Kamilov", "Peyman Milanfar", "Mauricio Delbracio"], "title": "Kernel Density Steering: Inference-Time Scaling via Mode Seeking for Image Restoration", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Diffusion models show promise for image restoration, but existing methods often struggle with inconsistent fidelity and undesirable artifacts. To address this, we introduce Kernel Density Steering (KDS), a novel inference-time framework promoting robust, high-fidelity outputs through explicit local mode-seeking. KDS employs an $N$-particle ensemble of diffusion samples, computing patch-wise kernel density estimation gradients from their collective outputs. These gradients steer patches in each particle towards shared, higher-density regions identified within the ensemble. This collective local mode-seeking mechanism, acting as \"collective wisdom\", steers samples away from spurious modes prone to artifacts, arising from independent sampling or model imperfections, and towards more robust, high-fidelity structures. This allows us to obtain better quality samples at the expense of higher compute by simultaneously sampling multiple particles. As a plug-and-play framework, KDS requires no retraining or external verifiers, seamlessly integrating with various diffusion samplers. Extensive numerical validations demonstrate KDS substantially improves both quantitative and qualitative performance on challenging real-world super-resolution and image inpainting tasks."}
{"id": "2507.05621", "pdf": "https://arxiv.org/pdf/2507.05621", "abs": "https://arxiv.org/abs/2507.05621", "authors": ["Suoxiang Zhang", "Xiaxi Li", "Hongrui Chang", "Zhuoyan Hou", "Guoxin Wu", "Ronghua Ji"], "title": "AdaptaGen: Domain-Specific Image Generation through Hierarchical Semantic Optimization Framework", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Domain-specific image generation aims to produce high-quality visual content for specialized fields while ensuring semantic accuracy and detail fidelity. However, existing methods exhibit two critical limitations: First, current approaches address prompt engineering and model adaptation separately, overlooking the inherent dependence between semantic understanding and visual representation in specialized domains. Second, these techniques inadequately incorporate domain-specific semantic constraints during content synthesis, resulting in generation outcomes that exhibit hallucinations and semantic deviations. To tackle these issues, we propose AdaptaGen, a hierarchical semantic optimization framework that integrates matrix-based prompt optimization with multi-perspective understanding, capturing comprehensive semantic relationships from both global and local perspectives. To mitigate hallucinations in specialized domains, we design a cross-modal adaptation mechanism, which, when combined with intelligent content synthesis, enables preserving core thematic elements while incorporating diverse details across images. Additionally, we introduce a two-phase caption semantic transformation during the generation phase. This approach maintains semantic coherence while enhancing visual diversity, ensuring the generated images adhere to domain-specific constraints. Experimental results confirm our approach's effectiveness, with our framework achieving superior performance across 40 categories from diverse datasets using only 16 images per category, demonstrating significant improvements in image quality, diversity, and semantic consistency."}
{"id": "2507.05666", "pdf": "https://arxiv.org/pdf/2507.05666", "abs": "https://arxiv.org/abs/2507.05666", "authors": ["Junfei Shi", "Yu Cheng", "Haiyan Jin", "Junhuai Li", "Zhaolin Xiao", "Maoguo Gong", "Weisi Lin"], "title": "Knowledge-guided Complex Diffusion Model for PolSAR Image Classification in Contourlet Domain", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Diffusion models have demonstrated exceptional performance across various domains due to their ability to model and generate complicated data distributions. However, when applied to PolSAR data, traditional real-valued diffusion models face challenges in capturing complex-valued phase information.Moreover, these models often struggle to preserve fine structural details. To address these limitations, we leverage the Contourlet transform, which provides rich multiscale and multidirectional representations well-suited for PolSAR imagery. We propose a structural knowledge-guided complex diffusion model for PolSAR image classification in the Contourlet domain. Specifically, the complex Contourlet transform is first applied to decompose the data into low- and high-frequency subbands, enabling the extraction of statistical and boundary features. A knowledge-guided complex diffusion network is then designed to model the statistical properties of the low-frequency components. During the process, structural information from high-frequency coefficients is utilized to guide the diffusion process, improving edge preservation. Furthermore, multiscale and multidirectional high-frequency features are jointly learned to further boost classification accuracy. Experimental results on three real-world PolSAR datasets demonstrate that our approach surpasses state-of-the-art methods, particularly in preserving edge details and maintaining region homogeneity in complex terrain."}
{"id": "2507.05670", "pdf": "https://arxiv.org/pdf/2507.05670", "abs": "https://arxiv.org/abs/2507.05670", "authors": ["Omar Zamzam", "Haleh Akrami", "Anand Joshi", "Richard Leahy"], "title": "Modeling and Reversing Brain Lesions Using Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Brain lesions are abnormalities or injuries in brain tissue that are often detectable using magnetic resonance imaging (MRI), which reveals structural changes in the affected areas. This broad definition of brain lesions includes areas of the brain that are irreversibly damaged, as well as areas of brain tissue that are deformed as a result of lesion growth or swelling. Despite the importance of differentiating between damaged and deformed tissue, existing lesion segmentation methods overlook this distinction, labeling both of them as a single anomaly. In this work, we introduce a diffusion model-based framework for analyzing and reversing the brain lesion process. Our pipeline first segments abnormal regions in the brain, then estimates and reverses tissue deformations by restoring displaced tissue to its original position, isolating the core lesion area representing the initial damage. Finally, we inpaint the core lesion area to arrive at an estimation of the pre-lesion healthy brain. This proposed framework reverses a forward lesion growth process model that is well-established in biomechanical studies that model brain lesions. Our results demonstrate improved accuracy in lesion segmentation, characterization, and brain labeling compared to traditional methods, offering a robust tool for clinical and research applications in brain lesion analysis. Since pre-lesion healthy versions of abnormal brains are not available in any public dataset for validation of the reverse process, we simulate a forward model to synthesize multiple lesioned brain images."}
{"id": "2507.05675", "pdf": "https://arxiv.org/pdf/2507.05675", "abs": "https://arxiv.org/abs/2507.05675", "authors": ["Rongsheng Wang", "Junying Chen", "Ke Ji", "Zhenyang Cai", "Shunian Chen", "Yunjin Yang", "Benyou Wang"], "title": "MedGen: Unlocking Medical Video Generation by Scaling Granularly-annotated Medical Videos", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in video generation have shown remarkable progress in open-domain settings, yet medical video generation remains largely underexplored. Medical videos are critical for applications such as clinical training, education, and simulation, requiring not only high visual fidelity but also strict medical accuracy. However, current models often produce unrealistic or erroneous content when applied to medical prompts, largely due to the lack of large-scale, high-quality datasets tailored to the medical domain. To address this gap, we introduce MedVideoCap-55K, the first large-scale, diverse, and caption-rich dataset for medical video generation. It comprises over 55,000 curated clips spanning real-world medical scenarios, providing a strong foundation for training generalist medical video generation models. Built upon this dataset, we develop MedGen, which achieves leading performance among open-source models and rivals commercial systems across multiple benchmarks in both visual quality and medical accuracy. We hope our dataset and model can serve as a valuable resource and help catalyze further research in medical video generation. Our code and data is available at https://github.com/FreedomIntelligence/MedGen"}
{"id": "2507.05678", "pdf": "https://arxiv.org/pdf/2507.05678", "abs": "https://arxiv.org/abs/2507.05678", "authors": ["Yisu Zhang", "Chenjie Cao", "Chaohui Yu", "Jianke Zhu"], "title": "LiON-LoRA: Rethinking LoRA Fusion to Unify Controllable Spatial and Temporal Generation for Video Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Video Diffusion Models (VDMs) have demonstrated remarkable capabilities in synthesizing realistic videos by learning from large-scale data. Although vanilla Low-Rank Adaptation (LoRA) can learn specific spatial or temporal movement to driven VDMs with constrained data, achieving precise control over both camera trajectories and object motion remains challenging due to the unstable fusion and non-linear scalability. To address these issues, we propose LiON-LoRA, a novel framework that rethinks LoRA fusion through three core principles: Linear scalability, Orthogonality, and Norm consistency. First, we analyze the orthogonality of LoRA features in shallow VDM layers, enabling decoupled low-level controllability. Second, norm consistency is enforced across layers to stabilize fusion during complex camera motion combinations. Third, a controllable token is integrated into the diffusion transformer (DiT) to linearly adjust motion amplitudes for both cameras and objects with a modified self-attention mechanism to ensure decoupled control. Additionally, we extend LiON-LoRA to temporal generation by leveraging static-camera videos, unifying spatial and temporal controllability. Experiments demonstrate that LiON-LoRA outperforms state-of-the-art methods in trajectory control accuracy and motion strength adjustment, achieving superior generalization with minimal training data. Project Page: https://fuchengsu.github.io/lionlora.github.io/"}
{"id": "2507.05763", "pdf": "https://arxiv.org/pdf/2507.05763", "abs": "https://arxiv.org/abs/2507.05763", "authors": ["Ruijie Lu", "Yu Liu", "Jiaxiang Tang", "Junfeng Ni", "Yuxiang Wang", "Diwen Wan", "Gang Zeng", "Yixin Chen", "Siyuan Huang"], "title": "DreamArt: Generating Interactable Articulated Objects from a Single Image", "categories": ["cs.CV"], "comment": "Technical Report", "summary": "Generating articulated objects, such as laptops and microwaves, is a crucial yet challenging task with extensive applications in Embodied AI and AR/VR. Current image-to-3D methods primarily focus on surface geometry and texture, neglecting part decomposition and articulation modeling. Meanwhile, neural reconstruction approaches (e.g., NeRF or Gaussian Splatting) rely on dense multi-view or interaction data, limiting their scalability. In this paper, we introduce DreamArt, a novel framework for generating high-fidelity, interactable articulated assets from single-view images. DreamArt employs a three-stage pipeline: firstly, it reconstructs part-segmented and complete 3D object meshes through a combination of image-to-3D generation, mask-prompted 3D segmentation, and part amodal completion. Second, we fine-tune a video diffusion model to capture part-level articulation priors, leveraging movable part masks as prompt and amodal images to mitigate ambiguities caused by occlusion. Finally, DreamArt optimizes the articulation motion, represented by a dual quaternion, and conducts global texture refinement and repainting to ensure coherent, high-quality textures across all parts. Experimental results demonstrate that DreamArt effectively generates high-quality articulated objects, possessing accurate part shape, high appearance fidelity, and plausible articulation, thereby providing a scalable solution for articulated asset generation. Our project page is available at https://dream-art-0.github.io/DreamArt/."}
{"id": "2507.05798", "pdf": "https://arxiv.org/pdf/2507.05798", "abs": "https://arxiv.org/abs/2507.05798", "authors": ["Xin Hu", "Ke Qin", "Guiduo Duan", "Ming Li", "Yuan-Fang Li", "Tao He"], "title": "SPADE: Spatial-Aware Denoising Network for Open-vocabulary Panoptic Scene Graph Generation with Long- and Local-range Context Reasoning", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "Panoptic Scene Graph Generation (PSG) integrates instance segmentation with relation understanding to capture pixel-level structural relationships in complex scenes. Although recent approaches leveraging pre-trained vision-language models (VLMs) have significantly improved performance in the open-vocabulary setting, they commonly ignore the inherent limitations of VLMs in spatial relation reasoning, such as difficulty in distinguishing object relative positions, which results in suboptimal relation prediction. Motivated by the denoising diffusion model's inversion process in preserving the spatial structure of input images, we propose SPADE (SPatial-Aware Denoising-nEtwork) framework -- a novel approach for open-vocabulary PSG. SPADE consists of two key steps: (1) inversion-guided calibration for the UNet adaptation, and (2) spatial-aware context reasoning. In the first step, we calibrate a general pre-trained teacher diffusion model into a PSG-specific denoising network with cross-attention maps derived during inversion through a lightweight LoRA-based fine-tuning strategy. In the second step, we develop a spatial-aware relation graph transformer that captures both local and long-range contextual information, facilitating the generation of high-quality relation queries. Extensive experiments on benchmark PSG and Visual Genome datasets demonstrate that SPADE outperforms state-of-the-art methods in both closed- and open-set scenarios, particularly for spatial relationship prediction."}
{"id": "2507.05859", "pdf": "https://arxiv.org/pdf/2507.05859", "abs": "https://arxiv.org/abs/2507.05859", "authors": ["Wenkang Zhang", "Yan Zhao", "Qiang Wang", "Li Song", "Zhengxue Cheng"], "title": "D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for Free-Viewpoint Videos", "categories": ["cs.CV", "cs.MM"], "comment": "12 pages, 9 figures, 8 tables", "summary": "Free-viewpoint video (FVV) enables immersive 3D experiences, but efficient compression of dynamic 3D representations remains a major challenge. Recent advances in 3D Gaussian Splatting (3DGS) and its dynamic extensions have enabled high-fidelity scene modeling. However, existing methods often couple scene reconstruction with optimization-dependent coding, which limits generalizability. This paper presents Feedforward Compression of Dynamic Gaussian Splatting (D-FCGS), a novel feedforward framework for compressing temporally correlated Gaussian point cloud sequences. Our approach introduces a Group-of-Frames (GoF) structure with I-P frame coding, where inter-frame motions are extracted via sparse control points. The resulting motion tensors are compressed in a feedforward manner using a dual prior-aware entropy model that combines hyperprior and spatial-temporal priors for accurate rate estimation. For reconstruction, we perform control-point-guided motion compensation and employ a refinement network to enhance view-consistent fidelity. Trained on multi-view video-derived Gaussian frames, D-FCGS generalizes across scenes without per-scene optimization. Experiments show that it matches the rate-distortion performance of optimization-based methods, achieving over 40 times compression in under 2 seconds while preserving visual quality across viewpoints. This work advances feedforward compression for dynamic 3DGS, paving the way for scalable FVV transmission and storage in immersive applications."}
{"id": "2507.05899", "pdf": "https://arxiv.org/pdf/2507.05899", "abs": "https://arxiv.org/abs/2507.05899", "authors": ["Yuedong Tan", "Jiawei Shao", "Eduard Zamfir", "Ruanjun Li", "Zhaochong An", "Chao Ma", "Danda Paudel", "Luc Van Gool", "Radu Timofte", "Zongwei Wu"], "title": "What You Have is What You Track: Adaptive and Robust Multimodal Tracking", "categories": ["cs.CV"], "comment": "ICCV2025 accepted", "summary": "Multimodal data is known to be helpful for visual tracking by improving robustness to appearance variations. However, sensor synchronization challenges often compromise data availability, particularly in video settings where shortages can be temporal. Despite its importance, this area remains underexplored. In this paper, we present the first comprehensive study on tracker performance with temporally incomplete multimodal data. Unsurprisingly, under such a circumstance, existing trackers exhibit significant performance degradation, as their rigid architectures lack the adaptability needed to effectively handle missing modalities. To address these limitations, we propose a flexible framework for robust multimodal tracking. We venture that a tracker should dynamically activate computational units based on missing data rates. This is achieved through a novel Heterogeneous Mixture-of-Experts fusion mechanism with adaptive complexity, coupled with a video-level masking strategy that ensures both temporal consistency and spatial completeness which is critical for effective video tracking. Surprisingly, our model not only adapts to varying missing rates but also adjusts to scene complexity. Extensive experiments show that our model achieves SOTA performance across 9 benchmarks, excelling in both conventional complete and missing modality settings. The code and benchmark will be publicly available at https://github.com/supertyd/FlexTrack/tree/main."}
{"id": "2507.05952", "pdf": "https://arxiv.org/pdf/2507.05952", "abs": "https://arxiv.org/abs/2507.05952", "authors": ["Aoxiang Fan", "Corentin Dumery", "Nicolas Talabot", "Hieu Le", "Pascal Fua"], "title": "High-Fidelity and Generalizable Neural Surface Reconstruction with Sparse Feature Volumes", "categories": ["cs.CV"], "comment": null, "summary": "Generalizable neural surface reconstruction has become a compelling technique to reconstruct from few images without per-scene optimization, where dense 3D feature volume has proven effective as a global representation of scenes. However, the dense representation does not scale well to increasing voxel resolutions, severely limiting the reconstruction quality. We thus present a sparse representation method, that maximizes memory efficiency and enables significantly higher resolution reconstructions on standard hardware. We implement this through a two-stage approach: First training a network to predict voxel occupancies from posed images and associated depth maps, then computing features and performing volume rendering only in voxels with sufficiently high occupancy estimates. To support this sparse representation, we developed custom algorithms for efficient sampling, feature aggregation, and querying from sparse volumes-overcoming the dense-volume assumptions inherent in existing works. Experiments on public datasets demonstrate that our approach reduces storage requirements by more than 50 times without performance degradation, enabling reconstructions at $512^3$ resolution compared to the typical $128^3$ on similar hardware, and achieving superior reconstruction accuracy over current state-of-the-art methods."}
{"id": "2507.05963", "pdf": "https://arxiv.org/pdf/2507.05963", "abs": "https://arxiv.org/abs/2507.05963", "authors": ["Zhenghao Zhang", "Junchao Liao", "Xiangyu Meng", "Long Qin", "Weizhi Wang"], "title": "Tora2: Motion and Appearance Customized Diffusion Transformer for Multi-Entity Video Generation", "categories": ["cs.CV"], "comment": "ACM MM25 Conference Proceedings", "summary": "Recent advances in diffusion transformer models for motion-guided video generation, such as Tora, have shown significant progress. In this paper, we present Tora2, an enhanced version of Tora, which introduces several design improvements to expand its capabilities in both appearance and motion customization. Specifically, we introduce a decoupled personalization extractor that generates comprehensive personalization embeddings for multiple open-set entities, better preserving fine-grained visual details compared to previous methods. Building on this, we design a gated self-attention mechanism to integrate trajectory, textual description, and visual information for each entity. This innovation significantly reduces misalignment in multimodal conditioning during training. Moreover, we introduce a contrastive loss that jointly optimizes trajectory dynamics and entity consistency through explicit mapping between motion and personalization embeddings. Tora2 is, to our best knowledge, the first method to achieve simultaneous multi-entity customization of appearance and motion for video generation. Experimental results demonstrate that Tora2 achieves competitive performance with state-of-the-art customization methods while providing advanced motion control capabilities, which marks a critical advancement in multi-condition video generation. Project page: https://github.com/alibaba/Tora ."}
{"id": "2507.05964", "pdf": "https://arxiv.org/pdf/2507.05964", "abs": "https://arxiv.org/abs/2507.05964", "authors": ["Vera Soboleva", "Aibek Alanov", "Andrey Kuznetsov", "Konstantin Sobolev"], "title": "T-LoRA: Single Image Diffusion Model Customization Without Overfitting", "categories": ["cs.CV"], "comment": null, "summary": "While diffusion model fine-tuning offers a powerful approach for customizing pre-trained models to generate specific objects, it frequently suffers from overfitting when training samples are limited, compromising both generalization capability and output diversity. This paper tackles the challenging yet most impactful task of adapting a diffusion model using just a single concept image, as single-image customization holds the greatest practical potential. We introduce T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework specifically designed for diffusion model personalization. In our work we show that higher diffusion timesteps are more prone to overfitting than lower ones, necessitating a timestep-sensitive fine-tuning strategy. T-LoRA incorporates two key innovations: (1) a dynamic fine-tuning strategy that adjusts rank-constrained updates based on diffusion timesteps, and (2) a weight parametrization technique that ensures independence between adapter components through orthogonal initialization. Extensive experiments show that T-LoRA and its individual components outperform standard LoRA and other diffusion model personalization techniques. They achieve a superior balance between concept fidelity and text alignment, highlighting the potential of T-LoRA in data-limited and resource-constrained scenarios. Code is available at https://github.com/ControlGenAI/T-LoRA."}
{"id": "2507.06033", "pdf": "https://arxiv.org/pdf/2507.06033", "abs": "https://arxiv.org/abs/2507.06033", "authors": ["Syeda Anshrah Gillani", "Mirza Samad Ahmed Baig", "Osama Ahmed Khan", "Shahid Munir Shah", "Umema Mujeeb", "Maheen Ali"], "title": "TextPixs: Glyph-Conditioned Diffusion with Character-Aware Attention and OCR-Guided Supervision", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "30 pages", "summary": "The modern text-to-image diffusion models boom has opened a new era in digital content production as it has proven the previously unseen ability to produce photorealistic and stylistically diverse imagery based on the semantics of natural-language descriptions. However, the consistent disadvantage of these models is that they cannot generate readable, meaningful, and correctly spelled text in generated images, which significantly limits the use of practical purposes like advertising, learning, and creative design. This paper introduces a new framework, namely Glyph-Conditioned Diffusion with Character-Aware Attention (GCDA), using which a typical diffusion backbone is extended by three well-designed modules. To begin with, the model has a dual-stream text encoder that encodes both semantic contextual information and explicit glyph representations, resulting in a character-aware representation of the input text that is rich in nature. Second, an attention mechanism that is aware of the character is proposed with a new attention segregation loss that aims to limit the attention distribution of each character independently in order to avoid distortion artifacts. Lastly, GCDA has an OCR-in-the-loop fine-tuning phase, where a full text perceptual loss, directly optimises models to be legible and accurately spell. Large scale experiments to benchmark datasets, such as MARIO-10M and T2I-CompBench, reveal that GCDA sets a new state-of-the-art on all metrics, with better character based metrics on text rendering (Character Error Rate: 0.08 vs 0.21 for the previous best; Word Error Rate: 0.15 vs 0.25), human perception, and comparable image synthesis quality on high-fidelity (FID: 14.3)."}
{"id": "2507.06060", "pdf": "https://arxiv.org/pdf/2507.06060", "abs": "https://arxiv.org/abs/2507.06060", "authors": ["Alexandre Symeonidis-Herzig", "\u00d6zge Mercano\u011flu Sincan", "Richard Bowden"], "title": "VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Realistic, high-fidelity 3D facial animations are crucial for expressive avatar systems in human-computer interaction and accessibility. Although prior methods show promising quality, their reliance on the mesh domain limits their ability to fully leverage the rapid visual innovations seen in 2D computer vision and graphics. We propose VisualSpeaker, a novel method that bridges this gap using photorealistic differentiable rendering, supervised by visual speech recognition, for improved 3D facial animation. Our contribution is a perceptual lip-reading loss, derived by passing photorealistic 3D Gaussian Splatting avatar renders through a pre-trained Visual Automatic Speech Recognition model during training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker improves both the standard Lip Vertex Error metric by 56.1% and the perceptual quality of the generated animations, while retaining the controllability of mesh-driven animation. This perceptual focus naturally supports accurate mouthings, essential cues that disambiguate similar manual signs in sign language avatars."}
{"id": "2507.06078", "pdf": "https://arxiv.org/pdf/2507.06078", "abs": "https://arxiv.org/abs/2507.06078", "authors": ["Chihan Huang", "Hao Tang"], "title": "ScoreAdv: Score-based Targeted Generation of Natural Adversarial Examples via Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Despite the success of deep learning across various domains, it remains vulnerable to adversarial attacks. Although many existing adversarial attack methods achieve high success rates, they typically rely on $\\ell_{p}$-norm perturbation constraints, which do not align with human perceptual capabilities. Consequently, researchers have shifted their focus toward generating natural, unrestricted adversarial examples (UAEs). GAN-based approaches suffer from inherent limitations, such as poor image quality due to instability and mode collapse. Meanwhile, diffusion models have been employed for UAE generation, but they still rely on iterative PGD perturbation injection, without fully leveraging their central denoising capabilities. In this paper, we introduce a novel approach for generating UAEs based on diffusion models, named ScoreAdv. This method incorporates an interpretable adversarial guidance mechanism to gradually shift the sampling distribution towards the adversarial distribution, while using an interpretable saliency map to inject the visual information of a reference image into the generated samples. Notably, our method is capable of generating an unlimited number of natural adversarial examples and can attack not only classification models but also retrieval models. We conduct extensive experiments on ImageNet and CelebA datasets, validating the performance of ScoreAdv across ten target models in both black-box and white-box settings. Our results demonstrate that ScoreAdv achieves state-of-the-art attack success rates and image quality. Furthermore, the dynamic balance between denoising and adversarial perturbation enables ScoreAdv to remain robust even under defensive measures."}
{"id": "2507.06103", "pdf": "https://arxiv.org/pdf/2507.06103", "abs": "https://arxiv.org/abs/2507.06103", "authors": ["Jiayi Song", "Zihan Ye", "Qingyuan Zhou", "Weidong Yang", "Ben Fei", "Jingyi Xu", "Ying He", "Wanli Ouyang"], "title": "Reflections Unlock: Geometry-Aware Reflection Disentanglement in 3D Gaussian Splatting for Photorealistic Scenes Rendering", "categories": ["cs.CV"], "comment": null, "summary": "Accurately rendering scenes with reflective surfaces remains a significant challenge in novel view synthesis, as existing methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) often misinterpret reflections as physical geometry, resulting in degraded reconstructions. Previous methods rely on incomplete and non-generalizable geometric constraints, leading to misalignment between the positions of Gaussian splats and the actual scene geometry. When dealing with real-world scenes containing complex geometry, the accumulation of Gaussians further exacerbates surface artifacts and results in blurred reconstructions. To address these limitations, in this work, we propose Ref-Unlock, a novel geometry-aware reflection modeling framework based on 3D Gaussian Splatting, which explicitly disentangles transmitted and reflected components to better capture complex reflections and enhance geometric consistency in real-world scenes. Our approach employs a dual-branch representation with high-order spherical harmonics to capture high-frequency reflective details, alongside a reflection removal module providing pseudo reflection-free supervision to guide clean decomposition. Additionally, we incorporate pseudo-depth maps and a geometry-aware bilateral smoothness constraint to enhance 3D geometric consistency and stability in decomposition. Extensive experiments demonstrate that Ref-Unlock significantly outperforms classical GS-based reflection methods and achieves competitive results with NeRF-based models, while enabling flexible vision foundation models (VFMs) driven reflection editing. Our method thus offers an efficient and generalizable solution for realistic rendering of reflective scenes. Our code is available at https://ref-unlock.github.io/."}
{"id": "2507.06119", "pdf": "https://arxiv.org/pdf/2507.06119", "abs": "https://arxiv.org/abs/2507.06119", "authors": ["Zhiyu Tan", "Hao Yang", "Luozheng Qin", "Jia Gong", "Mengping Yang", "Hao Li"], "title": "Omni-Video: Democratizing Unified Video Understanding and Generation", "categories": ["cs.CV"], "comment": "Technical report, project page:   https://sais-fuxi.github.io/Omni-Video/", "summary": "Notable breakthroughs in unified understanding and generation modeling have led to remarkable advancements in image understanding, reasoning, production and editing, yet current foundational models predominantly focus on processing images, creating a gap in the development of unified models for video understanding and generation. This report presents Omni-Video, an efficient and effective unified framework for video understanding, generation, as well as instruction-based editing. Our key insight is to teach existing multimodal large language models (MLLMs) to produce continuous visual clues that are used as the input of diffusion decoders, which produce high-quality videos conditioned on these visual clues. To fully unlock the potential of our system for unified video modeling, we integrate several technical improvements: 1) a lightweight architectural design that respectively attaches a vision head on the top of MLLMs and a adapter before the input of diffusion decoders, the former produce visual tokens for the latter, which adapts these visual tokens to the conditional space of diffusion decoders; and 2) an efficient multi-stage training scheme that facilitates a fast connection between MLLMs and diffusion decoders with limited data and computational resources. We empirically demonstrate that our model exhibits satisfactory generalization abilities across video generation, editing and understanding tasks."}
{"id": "2507.06146", "pdf": "https://arxiv.org/pdf/2507.06146", "abs": "https://arxiv.org/abs/2507.06146", "authors": ["Haoyu Wang", "Lei Zhang", "Wei Wei", "Chen Ding", "Yanning Zhang"], "title": "Prompt-Free Conditional Diffusion for Multi-object Image Augmentation", "categories": ["cs.CV"], "comment": "Accepted at IJCAI 2025", "summary": "Diffusion models has underpinned much recent advances of dataset augmentation in various computer vision tasks. However, when involving generating multi-object images as real scenarios, most existing methods either rely entirely on text condition, resulting in a deviation between the generated objects and the original data, or rely too much on the original images, resulting in a lack of diversity in the generated images, which is of limited help to downstream tasks. To mitigate both problems with one stone, we propose a prompt-free conditional diffusion framework for multi-object image augmentation. Specifically, we introduce a local-global semantic fusion strategy to extract semantics from images to replace text, and inject knowledge into the diffusion model through LoRA to alleviate the category deviation between the original model and the target dataset. In addition, we design a reward model based counting loss to assist the traditional reconstruction loss for model training. By constraining the object counts of each category instead of pixel-by-pixel constraints, bridging the quantity deviation between the generated data and the original data while improving the diversity of the generated data. Experimental results demonstrate the superiority of the proposed method over several representative state-of-the-art baselines and showcase strong downstream task gain and out-of-domain generalization capabilities. Code is available at \\href{https://github.com/00why00/PFCD}{here}."}
{"id": "2507.05317", "pdf": "https://arxiv.org/pdf/2507.05317", "abs": "https://arxiv.org/abs/2507.05317", "authors": ["Yi Liu", "Yiyang Wen", "Zekun Zhou", "Junqi Ma", "Linghang Wang", "Yucheng Yao", "Liu Shi", "Qiegen Liu"], "title": "PWD: Prior-Guided and Wavelet-Enhanced Diffusion Model for Limited-Angle CT", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Generative diffusion models have received increasing attention in medical imaging, particularly in limited-angle computed tomography (LACT). Standard diffusion models achieve high-quality image reconstruction but require a large number of sampling steps during inference, resulting in substantial computational overhead. Although skip-sampling strategies have been proposed to improve efficiency, they often lead to loss of fine structural details. To address this issue, we propose a prior information embedding and wavelet feature fusion fast sampling diffusion model for LACT reconstruction. The PWD enables efficient sampling while preserving reconstruction fidelity in LACT, and effectively mitigates the degradation typically introduced by skip-sampling. Specifically, during the training phase, PWD maps the distribution of LACT images to that of fully sampled target images, enabling the model to learn structural correspondences between them. During inference, the LACT image serves as an explicit prior to guide the sampling trajectory, allowing for high-quality reconstruction with significantly fewer steps. In addition, PWD performs multi-scale feature fusion in the wavelet domain, effectively enhancing the reconstruction of fine details by leveraging both low-frequency and high-frequency information. Quantitative and qualitative evaluations on clinical dental arch CBCT and periapical datasets demonstrate that PWD outperforms existing methods under the same sampling condition. Using only 50 sampling steps, PWD achieves at least 1.7 dB improvement in PSNR and 10% gain in SSIM."}
{"id": "2507.05451", "pdf": "https://arxiv.org/pdf/2507.05451", "abs": "https://arxiv.org/abs/2507.05451", "authors": ["Lijie Huang", "Jingyi Yin", "Jingke Zhang", "U-Wai Lok", "Ryan M. DeRuiter", "Jieyang Jin", "Kate M. Knoll", "Kendra E. Petersen", "James D. Krier", "Xiang-yang Zhu", "Gina K. Hesley", "Kathryn A. Robinson", "Andrew J. Bentall", "Thomas D. Atwell", "Andrew D. Rule", "Lilach O. Lerman", "Shigao Chen", "Chengwu Huang"], "title": "Self-supervised Deep Learning for Denoising in Ultrasound Microvascular Imaging", "categories": ["eess.IV", "cs.CV", "eess.SP"], "comment": "12 pages, 10 figures. Supplementary materials are available at   https://zenodo.org/records/15832003", "summary": "Ultrasound microvascular imaging (UMI) is often hindered by low signal-to-noise ratio (SNR), especially in contrast-free or deep tissue scenarios, which impairs subsequent vascular quantification and reliable disease diagnosis. To address this challenge, we propose Half-Angle-to-Half-Angle (HA2HA), a self-supervised denoising framework specifically designed for UMI. HA2HA constructs training pairs from complementary angular subsets of beamformed radio-frequency (RF) blood flow data, across which vascular signals remain consistent while noise varies. HA2HA was trained using in-vivo contrast-free pig kidney data and validated across diverse datasets, including contrast-free and contrast-enhanced data from pig kidneys, as well as human liver and kidney. An improvement exceeding 15 dB in both contrast-to-noise ratio (CNR) and SNR was observed, indicating a substantial enhancement in image quality. In addition to power Doppler imaging, denoising directly in the RF domain is also beneficial for other downstream processing such as color Doppler imaging (CDI). CDI results of human liver derived from the HA2HA-denoised signals exhibited improved microvascular flow visualization, with a suppressed noisy background. HA2HA offers a label-free, generalizable, and clinically applicable solution for robust vascular imaging in both contrast-free and contrast-enhanced UMI."}
{"id": "2507.05647", "pdf": "https://arxiv.org/pdf/2507.05647", "abs": "https://arxiv.org/abs/2507.05647", "authors": ["Jiaqi Guo", "Santiago L\u00f3pez-Tapia"], "title": "Diffusion-Based Limited-Angle CT Reconstruction under Noisy Conditions", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted at the 2025 IEEE International Conference on Image   Processing (ICIP), Workshop", "summary": "Limited-Angle Computed Tomography (LACT) is a challenging inverse problem where missing angular projections lead to incomplete sinograms and severe artifacts in the reconstructed images. While recent learning-based methods have demonstrated effectiveness, most of them assume ideal, noise-free measurements and fail to address the impact of measurement noise. To overcome this limitation, we treat LACT as a sinogram inpainting task and propose a diffusion-based framework that completes missing angular views using a Mean-Reverting Stochastic Differential Equation (MR-SDE) formulation. To improve robustness under realistic noise, we propose RNSD$^+$, a novel noise-aware rectification mechanism that explicitly models inference-time uncertainty, enabling reliable and robust reconstruction. Extensive experiments demonstrate that our method consistently surpasses baseline models in data consistency and perceptual quality, and generalizes well across varying noise intensity and acquisition scenarios."}
{"id": "2507.05661", "pdf": "https://arxiv.org/pdf/2507.05661", "abs": "https://arxiv.org/abs/2507.05661", "authors": ["Haitao Lu", "Haijier Chen", "Haoze Liu", "Shoujian Zhang", "Bo Xu", "Ziao Liu"], "title": "3DGS_LSR:Large_Scale Relocation for Autonomous Driving Based on 3D Gaussian Splatting", "categories": ["cs.RO", "cs.CV"], "comment": "13 pages,7 figures,4 tables", "summary": "In autonomous robotic systems, precise localization is a prerequisite for safe navigation. However, in complex urban environments, GNSS positioning often suffers from signal occlusion and multipath effects, leading to unreliable absolute positioning. Traditional mapping approaches are constrained by storage requirements and computational inefficiency, limiting their applicability to resource-constrained robotic platforms. To address these challenges, we propose 3DGS-LSR: a large-scale relocalization framework leveraging 3D Gaussian Splatting (3DGS), enabling centimeter-level positioning using only a single monocular RGB image on the client side. We combine multi-sensor data to construct high-accuracy 3DGS maps in large outdoor scenes, while the robot-side localization requires just a standard camera input. Using SuperPoint and SuperGlue for feature extraction and matching, our core innovation is an iterative optimization strategy that refines localization results through step-by-step rendering, making it suitable for real-time autonomous navigation. Experimental validation on the KITTI dataset demonstrates our 3DGS-LSR achieves average positioning accuracies of 0.026m, 0.029m, and 0.081m in town roads, boulevard roads, and traffic-dense highways respectively, significantly outperforming other representative methods while requiring only monocular RGB input. This approach provides autonomous robots with reliable localization capabilities even in challenging urban environments where GNSS fails."}
{"id": "2507.06109", "pdf": "https://arxiv.org/pdf/2507.06109", "abs": "https://arxiv.org/abs/2507.06109", "authors": ["Seungoh Han", "Jaehoon Jang", "Hyunsu Kim", "Jaeheung Surh", "Junhyung Kwak", "Hyowon Ha", "Kyungdon Joo"], "title": "LighthouseGS: Indoor Structure-aware 3D Gaussian Splatting for Panorama-Style Mobile Captures", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Preprint", "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time novel view synthesis (NVS) with impressive quality in indoor scenes. However, achieving high-fidelity rendering requires meticulously captured images covering the entire scene, limiting accessibility for general users. We aim to develop a practical 3DGS-based NVS framework using simple panorama-style motion with a handheld camera (e.g., mobile device). While convenient, this rotation-dominant motion and narrow baseline make accurate camera pose and 3D point estimation challenging, especially in textureless indoor scenes. To address these challenges, we propose LighthouseGS, a novel framework inspired by the lighthouse-like sweeping motion of panoramic views. LighthouseGS leverages rough geometric priors, such as mobile device camera poses and monocular depth estimation, and utilizes the planar structures often found in indoor environments. We present a new initialization method called plane scaffold assembly to generate consistent 3D points on these structures, followed by a stable pruning strategy to enhance geometry and optimization stability. Additionally, we introduce geometric and photometric corrections to resolve inconsistencies from motion drift and auto-exposure in mobile devices. Tested on collected real and synthetic indoor scenes, LighthouseGS delivers photorealistic rendering, surpassing state-of-the-art methods and demonstrating the potential for panoramic view synthesis and object placement."}
{"id": "2507.06137", "pdf": "https://arxiv.org/pdf/2507.06137", "abs": "https://arxiv.org/abs/2507.06137", "authors": ["Mohammad Mahdi Derakhshani", "Dheeraj Varghese", "Marzieh Fadaee", "Cees G. M. Snoek"], "title": "NeoBabel: A Multilingual Open Tower for Visual Generation", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "34 pages, 12 figures", "summary": "Text-to-image generation advancements have been predominantly English-centric, creating barriers for non-English speakers and perpetuating digital inequities. While existing systems rely on translation pipelines, these introduce semantic drift, computational overhead, and cultural misalignment. We introduce NeoBabel, a novel multilingual image generation framework that sets a new Pareto frontier in performance, efficiency and inclusivity, supporting six languages: English, Chinese, Dutch, French, Hindi, and Persian. The model is trained using a combination of large-scale multilingual pretraining and high-resolution instruction tuning. To evaluate its capabilities, we expand two English-only benchmarks to multilingual equivalents: m-GenEval and m-DPG. NeoBabel achieves state-of-the-art multilingual performance while retaining strong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG. Notably, it performs on par with leading models on English tasks while outperforming them by +0.11 and +0.09 on multilingual benchmarks, even though these models are built on multilingual base LLMs. This demonstrates the effectiveness of our targeted alignment training for preserving and extending crosslingual generalization. We further introduce two new metrics to rigorously assess multilingual alignment and robustness to code-mixed prompts. Notably, NeoBabel matches or exceeds English-only models while being 2-4x smaller. We release an open toolkit, including all code, model checkpoints, a curated dataset of 124M multilingual text-image pairs, and standardized multilingual evaluation protocols, to advance inclusive AI research. Our work demonstrates that multilingual capability is not a trade-off but a catalyst for improved robustness, efficiency, and cultural fidelity in generative AI."}
{"id": "2507.06140", "pdf": "https://arxiv.org/pdf/2507.06140", "abs": "https://arxiv.org/abs/2507.06140", "authors": ["Zhihao Chen", "Tao Chen", "Chenhui Wang", "Qi Gao", "Huidong Xie", "Chuang Niu", "Ge Wang", "Hongming Shan"], "title": "LangMamba: A Language-driven Mamba Framework for Low-dose CT Denoising with Vision-language Models", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "11 pages, 8 figures", "summary": "Low-dose computed tomography (LDCT) reduces radiation exposure but often degrades image quality, potentially compromising diagnostic accuracy. Existing deep learning-based denoising methods focus primarily on pixel-level mappings, overlooking the potential benefits of high-level semantic guidance. Recent advances in vision-language models (VLMs) suggest that language can serve as a powerful tool for capturing structured semantic information, offering new opportunities to improve LDCT reconstruction. In this paper, we introduce LangMamba, a Language-driven Mamba framework for LDCT denoising that leverages VLM-derived representations to enhance supervision from normal-dose CT (NDCT). LangMamba follows a two-stage learning strategy. First, we pre-train a Language-guided AutoEncoder (LangAE) that leverages frozen VLMs to map NDCT images into a semantic space enriched with anatomical information. Second, we synergize LangAE with two key components to guide LDCT denoising: Semantic-Enhanced Efficient Denoiser (SEED), which enhances NDCT-relevant local semantic while capturing global features with efficient Mamba mechanism, and Language-engaged Dual-space Alignment (LangDA) Loss, which ensures that denoised images align with NDCT in both perceptual and semantic spaces. Extensive experiments on two public datasets demonstrate that LangMamba outperforms conventional state-of-the-art methods, significantly improving detail preservation and visual fidelity. Remarkably, LangAE exhibits strong generalizability to unseen datasets, thereby reducing training costs. Furthermore, LangDA loss improves explainability by integrating language-guided insights into image reconstruction and offers a plug-and-play fashion. Our findings shed new light on the potential of language as a supervisory signal to advance LDCT denoising. The code is publicly available on https://github.com/hao1635/LangMamba."}
