<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 8]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Fidelity-preserving enhancement of ptychography with foundational text-to-image models](https://arxiv.org/abs/2509.04513)
*Ming Du,Volker Rose,Junjing Deng,Dileep Singh,Si Chen,Mathew J. Cherukara*

Main category: cs.GR

TL;DR: 提出了一种结合物理模型相位检索与文本引导扩散模型的即插即用框架，通过ADMM算法实现数据保真度与伪影去除的共识，显著提升衍射成像质量。


<details>
  <summary>Details</summary>
Motivation: 解决ptychographic相位检索中的网格病理学和多层串扰等伪影问题，这些伪影会降低重建图像质量，需要一种既能保持物理一致性又能有效去除伪影的方法。

Method: 采用交替方向乘子法(ADMM)框架，将物理模型相位检索与文本引导扩散模型(LEDITS++)相结合，用户可以通过自然语言指定需要去除的伪影类型。

Result: 在模拟和实验数据集上展示了显著的伪影抑制和结构保真度提升，通过PSNR和衍射图案一致性等指标验证了方法的有效性。

Conclusion: 文本引导生成模型与基于模型的相位检索算法相结合，为高质量衍射成像提供了一种可迁移且保真度高的方法。

Abstract: Ptychographic phase retrieval enables high-resolution imaging of complex samples but often suffers from artifacts such as grid pathology and multislice crosstalk, which degrade reconstructed images. We propose a plug-and-play (PnP) framework that integrates physics model-based phase retrieval with text-guided image editing using foundational diffusion models. By employing the alternating direction method of multipliers (ADMM), our approach ensures consensus between data fidelity and artifact removal subproblems, maintaining physics consistency while enhancing image quality. Artifact removal is achieved using a text-guided diffusion image editing method (LEDITS++) with a pre-trained foundational diffusion model, allowing users to specify artifacts for removal in natural language. Demonstrations on simulated and experimental datasets show significant improvements in artifact suppression and structural fidelity, validated by metrics such as peak signal-to-noise ratio (PSNR) and diffraction pattern consistency. This work highlights the combination of text-guided generative models and model-based phase retrieval algorithms as a transferable and fidelity-preserving method for high-quality diffraction imaging.

</details>


### [2] [Improved 3D Scene Stylization via Text-Guided Generative Image Editing with Region-Based Control](https://arxiv.org/abs/2509.05285)
*Haruo Fujiwara,Yusuke Mukuta,Tatsuya Harada*

Main category: cs.GR

TL;DR: 提出了一种改进的文本驱动3D场景风格化方法，通过参考注意力机制和深度图增强视角一致性，并支持多区域风格控制。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动3D场景编辑方法在保证高质量风格化和视角一致性方面存在挑战，特别是对场景中不同区域进行语义对应的风格迁移较为困难。

Method: 扩展风格对齐的深度条件视角生成框架，使用参考注意力机制替代共享注意力；利用多深度图网格增强视角一致性；提出多区域重要性加权切片Wasserstein距离损失实现区域控制风格迁移。

Result: 实验评估表明，该方法有效提升了文本驱动3D风格化的结果质量，在保持视角一致性的同时实现了更好的风格化效果。

Conclusion: 所提出的技术能够同时确保风格一致性和视角一致性，并提供可选的多区域风格控制功能，为3D场景风格化提供了更高质量的解决方案。

Abstract: Recent advances in text-driven 3D scene editing and stylization, which leverage the powerful capabilities of 2D generative models, have demonstrated promising outcomes. However, challenges remain in ensuring high-quality stylization and view consistency simultaneously. Moreover, applying style consistently to different regions or objects in the scene with semantic correspondence is a challenging task. To address these limitations, we introduce techniques that enhance the quality of 3D stylization while maintaining view consistency and providing optional region-controlled style transfer. Our method achieves stylization by re-training an initial 3D representation using stylized multi-view 2D images of the source views. Therefore, ensuring both style consistency and view consistency of stylized multi-view images is crucial. We achieve this by extending the style-aligned depth-conditioned view generation framework, replacing the fully shared attention mechanism with a single reference-based attention-sharing mechanism, which effectively aligns style across different viewpoints. Additionally, inspired by recent 3D inpainting methods, we utilize a grid of multiple depth maps as a single-image reference to further strengthen view consistency among stylized images. Finally, we propose Multi-Region Importance-Weighted Sliced Wasserstein Distance Loss, allowing styles to be applied to distinct image regions using segmentation masks from off-the-shelf models. We demonstrate that this optional feature enhances the faithfulness of style transfer and enables the mixing of different styles across distinct regions of the scene. Experimental evaluations, both qualitative and quantitative, demonstrate that our pipeline effectively improves the results of text-driven 3D stylization.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [3] [Skywork UniPic 2.0: Building Kontext Model with Online RL for Unified Multimodal Model](https://arxiv.org/abs/2509.04548)
*Hongyang Wei,Baixin Xu,Hongbo Liu,Cyrus Wu,Jie Liu,Yi Peng,Peiyu Wang,Zexiang Liu,Jingwen He,Yidan Xietian,Chuanxin Tang,Zidong Wang,Yichen Wei,Liang Hu,Boyi Jiang,William Li,Ying He,Yang Liu,Xuchen Song,Eric Li,Yahui Zhou*

Main category: cs.CV

TL;DR: UniPic2-SD3.5M-Kontext是一个2B参数的DiT模型，基于SD3.5-Medium架构改进，通过大规模预训练和创新的渐进式双任务强化策略，在图像生成和编辑任务上达到SOTA性能，并扩展到统一多模态框架UniPic2-Metaquery。


<details>
  <summary>Details</summary>
Motivation: 现有开源多模态模型过于注重参数规模扩展而忽视训练策略优化，限制了效率和性能。需要开发更高效的训练范式来提升模型能力。

Method: 1. 对SD3.5-Medium进行架构修改和大规模高质量数据预训练；2. 提出渐进式双任务强化策略(PDTR)分阶段增强指令跟随和编辑一致性；3. 通过连接器将图像模型与Qwen2.5-VL-7B结合进行联合训练。

Result: 模型在图像生成和编辑能力上超越参数规模更大的模型(BAGEL 7B和Flux-Kontext 12B)，UniPic2-Metaquery在多种任务上达到顶级性能。

Conclusion: 提出的训练范式Skywork UniPic 2.0被证明有效且具有良好泛化性，能够以简单可扩展的方式实现理解、生成和编辑的统一集成。

Abstract: Recent advances in multimodal models have demonstrated impressive capabilities in unified image generation and editing. However, many prominent open-source models prioritize scaling model parameters over optimizing training strategies, limiting their efficiency and performance. In this work, we present UniPic2-SD3.5M-Kontext, a 2B-parameter DiT model based on SD3.5-Medium, which achieves state-of-the-art image generation and editing while extending seamlessly into a unified multimodal framework. Our approach begins with architectural modifications to SD3.5-Medium and large-scale pre-training on high-quality data, enabling joint text-to-image generation and editing capabilities. To enhance instruction following and editing consistency, we propose a novel Progressive Dual-Task Reinforcement strategy (PDTR), which effectively strengthens both tasks in a staged manner. We empirically validate that the reinforcement phases for different tasks are mutually beneficial and do not induce negative interference. After pre-training and reinforcement strategies, UniPic2-SD3.5M-Kontext demonstrates stronger image generation and editing capabilities than models with significantly larger generation parameters-including BAGEL (7B) and Flux-Kontext (12B). Furthermore, following the MetaQuery, we connect the UniPic2-SD3.5M-Kontext and Qwen2.5-VL-7B via a connector and perform joint training to launch a unified multimodal model UniPic2-Metaquery. UniPic2-Metaquery integrates understanding, generation, and editing, achieving top-tier performance across diverse tasks with a simple and scalable training paradigm. This consistently validates the effectiveness and generalizability of our proposed training paradigm, which we formalize as Skywork UniPic 2.0.

</details>


### [4] [Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing via Bidirectional Warping](https://arxiv.org/abs/2509.04582)
*Jingyi Lu,Kai Han*

Main category: cs.CV

TL;DR: Inpaint4Drag是一个基于像素空间双向变形和图像修复的拖拽式图像编辑框架，实现了实时变形预览和高效修复，显著提升了交互体验。


<details>
  <summary>Details</summary>
Motivation: 现有拖拽式图像编辑方法主要依赖生成模型的潜在空间操作，存在精度有限、反馈延迟和模型特定限制等问题，需要更直接高效的解决方案。

Method: 将拖拽编辑分解为像素空间双向变形和图像修复两个步骤，将图像区域视为可变形材料，通过直接转换拖拽输入为标准修复格式，可作为任何修复模型的通用适配器。

Result: 在512x512分辨率下实现实时变形预览(0.01秒)和高效修复(0.3秒)，相比需要数分钟编辑的现有方法显著提升交互体验，实验证明具有优越的视觉质量和精确控制。

Conclusion: Inpaint4Drag提供了一个无需架构修改的通用适配器方案，能够自动继承未来修复技术的所有改进，在保持实时性能的同时实现高质量的拖拽式图像编辑。

Abstract: Drag-based image editing has emerged as a powerful paradigm for intuitive image manipulation. However, existing approaches predominantly rely on manipulating the latent space of generative models, leading to limited precision, delayed feedback, and model-specific constraints. Accordingly, we present Inpaint4Drag, a novel framework that decomposes drag-based editing into pixel-space bidirectional warping and image inpainting. Inspired by elastic object deformation in the physical world, we treat image regions as deformable materials that maintain natural shape under user manipulation. Our method achieves real-time warping previews (0.01s) and efficient inpainting (0.3s) at 512x512 resolution, significantly improving the interaction experience compared to existing methods that require minutes per edit. By transforming drag inputs directly into standard inpainting formats, our approach serves as a universal adapter for any inpainting model without architecture modification, automatically inheriting all future improvements in inpainting technology. Extensive experiments demonstrate that our method achieves superior visual quality and precise control while maintaining real-time performance. Project page: https://visual-ai.github.io/inpaint4drag/

</details>


### [5] [Comparative Evaluation of Traditional and Deep Learning Feature Matching Algorithms using Chandrayaan-2 Lunar Data](https://arxiv.org/abs/2509.04775)
*R. Makharia,J. G. Singla,Amitabh,N. Dube,H. Sharma*

Main category: cs.CV

TL;DR: 这篇论文评估了五种特征匹配算法在月球多模态图像注册中的性能，发现深度学习方法SuperGlue表现最优，经典算法在极地条件下性能下降，强调预处理流程和学习方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 月球图像注册对于表面成图、资源定位和任务规划至关重要，但多模态传感器数据的对齐遇到了分辨率、照明和传感器形变等挑战。

Method: 评估SIFT、ASIFT、AKAZE、RIFT2和SuperGlue五种算法，使用赤道和极地区域的交叉模态图像对。提出预处理流程包括地理参考、分辨率对齐、强度归一化、适应性直方图均衡化、主成分分析和阴影缩正。

Result: SuperGlue持续获得最低的根均方误差和最快的运行时间。SIFT和AKAZE等经典方法在赤道附近表现良好，但在极地光照条件下性能下降。

Conclusion: 结果强调了预处理和基于学习的方法对于在多样化条件下实现稳健的月球图像注册的重要性。

Abstract: Accurate image registration is critical for lunar exploration, enabling surface mapping, resource localization, and mission planning. Aligning data from diverse lunar sensors -- optical (e.g., Orbital High Resolution Camera, Narrow and Wide Angle Cameras), hyperspectral (Imaging Infrared Spectrometer), and radar (e.g., Dual-Frequency Synthetic Aperture Radar, Selene/Kaguya mission) -- is challenging due to differences in resolution, illumination, and sensor distortion. We evaluate five feature matching algorithms: SIFT, ASIFT, AKAZE, RIFT2, and SuperGlue (a deep learning-based matcher), using cross-modality image pairs from equatorial and polar regions. A preprocessing pipeline is proposed, including georeferencing, resolution alignment, intensity normalization, and enhancements like adaptive histogram equalization, principal component analysis, and shadow correction. SuperGlue consistently yields the lowest root mean square error and fastest runtimes. Classical methods such as SIFT and AKAZE perform well near the equator but degrade under polar lighting. The results highlight the importance of preprocessing and learning-based approaches for robust lunar image registration across diverse conditions.

</details>


### [6] [Exploring Non-Local Spatial-Angular Correlations with a Hybrid Mamba-Transformer Framework for Light Field Super-Resolution](https://arxiv.org/abs/2509.04824)
*Haosong Liu,Xiancheng Zhu,Huanqiang Zeng,Jianqing Zhu,Jiuwen Cao,Junhui Hou*

Main category: cs.CV

TL;DR: 提出LFMT框架，结合Mamba和Transformer优势，通过子空间简单扫描策略和双阶段建模，在光场图像超分辨率任务中实现高效特征提取和性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有Mamba方法在光场超分辨率中存在多方向扫描策略效率低下和特征冗余问题，且状态空间模型在保持空间-角度和视差信息方面存在局限。

Method: 提出子空间简单扫描策略(Sub-SS)和子空间简单Mamba块(SSMB)；采用双阶段建模：阶段I使用空间-角度残差子空间Mamba块(SA-RSMB)进行浅层特征提取，阶段II使用双分支并行结构结合极平面Mamba块(EPMB)和极平面Transformer块(EPTB)进行深度特征精炼。

Result: 在真实和合成光场数据集上显著超越当前最先进方法，在保持低计算复杂度的同时实现了性能的实质性提升。

Conclusion: LFMT框架成功整合了Mamba和Transformer的优势，能够全面探索空间、角度和极平面域的信息，为光场超分辨率提供了高效且高性能的解决方案。

Abstract: Recently, Mamba-based methods, with its advantage in long-range information modeling and linear complexity, have shown great potential in optimizing both computational cost and performance of light field image super-resolution (LFSR). However, current multi-directional scanning strategies lead to inefficient and redundant feature extraction when applied to complex LF data. To overcome this challenge, we propose a Subspace Simple Scanning (Sub-SS) strategy, based on which we design the Subspace Simple Mamba Block (SSMB) to achieve more efficient and precise feature extraction. Furthermore, we propose a dual-stage modeling strategy to address the limitation of state space in preserving spatial-angular and disparity information, thereby enabling a more comprehensive exploration of non-local spatial-angular correlations. Specifically, in stage I, we introduce the Spatial-Angular Residual Subspace Mamba Block (SA-RSMB) for shallow spatial-angular feature extraction; in stage II, we use a dual-branch parallel structure combining the Epipolar Plane Mamba Block (EPMB) and Epipolar Plane Transformer Block (EPTB) for deep epipolar feature refinement. Building upon meticulously designed modules and strategies, we introduce a hybrid Mamba-Transformer framework, termed LFMT. LFMT integrates the strengths of Mamba and Transformer models for LFSR, enabling comprehensive information exploration across spatial, angular, and epipolar-plane domains. Experimental results demonstrate that LFMT significantly outperforms current state-of-the-art methods in LFSR, achieving substantial improvements in performance while maintaining low computational complexity on both real-word and synthetic LF datasets.

</details>


### [7] [CoRe-GS: Coarse-to-Refined Gaussian Splatting with Semantic Object Focus](https://arxiv.org/abs/2509.04859)
*Hannah Schieber,Dominik Frischmann,Simon Boche,Victor Schaack,Angela Schoellig,Stefan Leutenegger,Daniel Roth*

Main category: cs.CV

TL;DR: CoRe-GS是一种用于移动自主空中机器人的语义3D高斯重建方法，通过粗粒度语义分割和颜色过滤快速隔离感兴趣对象，将训练时间减少约四分之一，同时保持高质量的新视角合成效果。


<details>
  <summary>Details</summary>
Motivation: 移动机器人应用如远程指导和灾难响应需要快速准确的3D重建，但传统方法重建整个场景效率低下。通过专注于特定感兴趣对象(PoIs)，可以显著减少计算需求和提高处理速度。

Method: 首先使用语义高斯泼溅生成粗粒度可分割场景，然后通过新颖的基于颜色的有效过滤方法对语义对象进行精细化处理，实现有效的对象隔离。

Result: 在两个数据集(SCRREAM真实户外和NeRDS 360合成室内)上评估显示，运行时间减少，新视角合成质量提高，训练时间比完整语义高斯泼溅训练周期减少约四分之一。

Conclusion: CoRe-GS方法在保持重建质量的同时显著减少了训练时间，为移动机器人应用提供了高效的语义3D重建解决方案。

Abstract: Mobile reconstruction for autonomous aerial robotics holds strong potential for critical applications such as tele-guidance and disaster response. These tasks demand both accurate 3D reconstruction and fast scene processing. Instead of reconstructing the entire scene in detail, it is often more efficient to focus on specific objects, i.e., points of interest (PoIs). Mobile robots equipped with advanced sensing can usually detect these early during data acquisition or preliminary analysis, reducing the need for full-scene optimization. Gaussian Splatting (GS) has recently shown promise in delivering high-quality novel view synthesis and 3D representation by an incremental learning process. Extending GS with scene editing, semantics adds useful per-splat features to isolate objects effectively.   Semantic 3D Gaussian editing can already be achieved before the full training cycle is completed, reducing the overall training time. Moreover, the semantically relevant area, the PoI, is usually already known during capturing. To balance high-quality reconstruction with reduced training time, we propose CoRe-GS. We first generate a coarse segmentation-ready scene with semantic GS and then refine it for the semantic object using our novel color-based effective filtering for effective object isolation. This is speeding up the training process to be about a quarter less than a full training cycle for semantic GS. We evaluate our approach on two datasets, SCRREAM (real-world, outdoor) and NeRDS 360 (synthetic, indoor), showing reduced runtime and higher novel-view-synthesis quality.

</details>


### [8] [Efficient Video-to-Audio Generation via Multiple Foundation Models Mapper](https://arxiv.org/abs/2509.04957)
*Gehui Chen,Guan'an Wang,Xiaowen Huang,Jitao Sang*

Main category: cs.CV

TL;DR: MFM-Mapper是一个高效的多模态映射器，通过融合双视觉编码器特征和使用GPT-2进行特征对齐，显著减少了V2A生成的训练成本，仅需之前方法16%的训练量就能达到竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频到音频生成方法需要大量资源训练，而基于基础模型的映射方法虽然减少了训练成本，但语义和时间信息提取不够充分。本文旨在通过融合多个基础模型的特征来提升生成质量，同时保持训练效率。

Method: 提出MFM-Mapper方法：1）使用双视觉编码器融合更丰富的语义和时间特征；2）用GPT-2替代线性映射器，将跨模态特征映射视为自回归翻译任务；3）保持轻量级映射网络架构。

Result: 方法仅需之前映射器方法16%的训练规模，在语义和时间一致性方面表现更好，达到了与更大规模训练模型相竞争的性能水平。

Conclusion: MFM-Mapper通过有效利用多个基础模型的特征融合和GPT-2的序列建模能力，在视频到音频生成任务中实现了训练效率和生成质量的显著提升。

Abstract: Recent Video-to-Audio (V2A) generation relies on extracting semantic and temporal features from video to condition generative models. Training these models from scratch is resource intensive. Consequently, leveraging foundation models (FMs) has gained traction due to their cross-modal knowledge transfer and generalization capabilities. One prior work has explored fine-tuning a lightweight mapper network to connect a pre-trained visual encoder with a text-to-audio generation model for V2A. Inspired by this, we introduce the Multiple Foundation Model Mapper (MFM-Mapper). Compared to the previous mapper approach, MFM-Mapper benefits from richer semantic and temporal information by fusing features from dual visual encoders. Furthermore, by replacing a linear mapper with GPT-2, MFM-Mapper improves feature alignment, drawing parallels between cross-modal features mapping and autoregressive translation tasks. Our MFM-Mapper exhibits remarkable training efficiency. It achieves better performance in semantic and temporal consistency with fewer training consuming, requiring only 16\% of the training scale compared to previous mapper-based work, yet achieves competitive performance with models trained on a much larger scale.

</details>


### [9] [LUIVITON: Learned Universal Interoperable VIrtual Try-ON](https://arxiv.org/abs/2509.05030)
*Cong Cao,Xianhang Cheng,Jingyuan Liu,Yujian Zheng,Zhenhui Lin,Meriem Chkir,Hao Li*

Main category: cs.CV

TL;DR: LUIVITON是一个端到端的全自动虚拟试穿系统，能够将复杂的多层服装覆盖到多样化且任意姿态的人形角色上，无需人工干预即可生成高质量的3D服装拟合效果。


<details>
  <summary>Details</summary>
Motivation: 为了解决复杂服装与任意多样化体型对齐的挑战，特别是处理复杂几何形状、非流形网格以及泛化到广泛人形角色的需求。

Method: 使用SMPL作为代理表示，将服装到身体的覆盖问题分解为两个对应任务：1）基于几何学习的服装到SMPL对应方法，2）基于扩散模型的身体到SMPL对应方法，利用多视角一致的外观特征和预训练的2D基础模型。

Result: 系统能够处理复杂几何形状，有效泛化到各种人形角色（包括人类、机器人、卡通角色、生物和外星人），并保持计算效率，支持服装尺寸和材质的快速定制。

Conclusion: LUIVITON提供了一个完全自动化的服装拟合解决方案，即使在2D服装缝制图案不可用的情况下，也能产生高质量的3D服装拟合效果，具有实际应用的潜力。

Abstract: We present LUIVITON, an end-to-end system for fully automated virtual try-on, capable of draping complex, multi-layer clothing onto diverse and arbitrarily posed humanoid characters. To address the challenge of aligning complex garments with arbitrary and highly diverse body shapes, we use SMPL as a proxy representation and separate the clothing-to-body draping problem into two correspondence tasks: 1) clothing-to-SMPL and 2) body-to-SMPL correspondence, where each has its unique challenges. While we address the clothing-to-SMPL fitting problem using a geometric learning-based approach for partial-to-complete shape correspondence prediction, we introduce a diffusion model-based approach for body-to-SMPL correspondence using multi-view consistent appearance features and a pre-trained 2D foundation model. Our method can handle complex geometries, non-manifold meshes, and generalizes effectively to a wide range of humanoid characters -- including humans, robots, cartoon subjects, creatures, and aliens, while maintaining computational efficiency for practical adoption. In addition to offering a fully automatic fitting solution, LUIVITON supports fast customization of clothing size, allowing users to adjust clothing sizes and material properties after they have been draped. We show that our system can produce high-quality 3D clothing fittings without any human labor, even when 2D clothing sewing patterns are not available.

</details>


### [10] [GeoSplat: A Deep Dive into Geometry-Constrained Gaussian Splatting](https://arxiv.org/abs/2509.05075)
*Yangming Li,Chaoyu Liu,Lihao Liu,Simon Masnou,Carola-Bibian Schönlieb*

Main category: cs.CV

TL;DR: GeoSplat是一个几何约束优化框架，利用一阶和二阶几何量改进高斯溅射的整个训练流程，包括初始化、梯度更新和致密化，显著提升了性能


<details>
  <summary>Details</summary>
Motivation: 现有方法主要使用低阶几何先验（如法向量），且通过噪声敏感方法估计不可靠。需要更鲁棒的几何约束来改进高斯溅射优化

Method: 提出几何约束优化框架，利用主曲率初始化高斯尺度，基于局部流形等几何结构引入高效且噪声鲁棒的估计方法，提供动态几何先验

Result: 在多个数据集上的新颖视图合成实验中，GeoSplat显著提升了高斯溅射性能，优于之前的基线方法

Conclusion: GeoSplat通过引入高阶几何约束和鲁棒估计方法，有效改进了高斯溅射的优化过程，为3D重建提供了更好的几何先验

Abstract: A few recent works explored incorporating geometric priors to regularize the optimization of Gaussian splatting, further improving its performance. However, those early studies mainly focused on the use of low-order geometric priors (e.g., normal vector), and they are also unreliably estimated by noise-sensitive methods, like local principal component analysis. To address their limitations, we first present GeoSplat, a general geometry-constrained optimization framework that exploits both first-order and second-order geometric quantities to improve the entire training pipeline of Gaussian splatting, including Gaussian initialization, gradient update, and densification. As an example, we initialize the scales of 3D Gaussian primitives in terms of principal curvatures, leading to a better coverage of the object surface than random initialization. Secondly, based on certain geometric structures (e.g., local manifold), we introduce efficient and noise-robust estimation methods that provide dynamic geometric priors for our framework. We conduct extensive experiments on multiple datasets for novel view synthesis, showing that our framework: GeoSplat, significantly improves the performance of Gaussian splatting and outperforms previous baselines.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation](https://arxiv.org/abs/2509.05263)
*Yinglin Duan,Zhengxia Zou,Tongwei Gu,Wei Jia,Zhan Zhao,Luyi Xu,Xinzhu Liu,Hao Jiang,Kang Chen,Shuang Qiu*

Main category: cs.AI

TL;DR: LatticeWorld是一个基于轻量级LLM和游戏引擎的3D世界生成框架，通过多模态输入创建大规模动态交互环境，显著提升工业生产效率90倍以上


<details>
  <summary>Details</summary>
Motivation: 传统手动建模3D场景效率低下，需要开发能够基于用户指令自动生成逼真3D虚拟世界的系统，以缩小仿真与现实差距并丰富真实世界信息获取

Method: 结合轻量级LLM(LLaMA-2-7B)和工业级渲染引擎(如Unreal Engine 5)，接受文本描述和视觉指令作为多模态输入，生成包含动态代理的大规模3D交互世界

Result: 在场景布局生成和视觉保真度方面达到优异精度，相比传统手动生产方式实现90倍以上的工业生产效率提升，同时保持高质量创意

Conclusion: LatticeWorld提供了一个简单有效的3D世界生成框架，能够高效创建具有竞争性多代理交互、高保真物理模拟和实时渲染的动态环境，显著优化工业生产流程

Abstract: Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. A more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real world conveniently. While traditional manual modeling has enabled the creation of virtual 3D scenes, modern approaches have leveraged advanced machine learning algorithms for 3D world generation, with most recent advances focusing on generative methods that can create virtual worlds based on user instructions. This work explores such a research direction by proposing LatticeWorld, a simple yet effective 3D world generation framework that streamlines the industrial production pipeline of 3D environments. LatticeWorld leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed framework accepts textual descriptions and visual instructions as multimodal inputs and creates large-scale 3D interactive worlds with dynamic agents, featuring competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. We conduct comprehensive experiments to evaluate LatticeWorld, showing that it achieves superior accuracy in scene layout generation and visual fidelity. Moreover, LatticeWorld achieves over a $90\times$ increase in industrial production efficiency while maintaining high creative quality compared with traditional manual production methods. Our demo video is available at https://youtu.be/8VWZXpERR18

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [12] [STADI: Fine-Grained Step-Patch Diffusion Parallelism for Heterogeneous GPUs](https://arxiv.org/abs/2509.04719)
*Han Liang,Jiahui Zhou,Zicheng Zhou,Xiaoxi Zhang,Xu Chen*

Main category: cs.DC

TL;DR: STADI是一个针对异构多GPU环境的扩散模型推理加速框架，通过时空自适应调度机制显著减少推理延迟并提高资源利用率


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型并行推理方案在异构多GPU环境中资源利用不足，硬件能力差异或后台任务导致工作负载不平衡

Method: 采用混合调度器实现时空维度的细粒度并行：时间维度使用计算感知的步数分配器和最小公倍数最小化量化技术；空间维度使用弹性补丁并行机制按GPU计算能力分配不同大小的图像块

Result: 在负载不平衡和异构多GPU集群上的实验验证了STADI的有效性，相比最先进的补丁并行框架，端到端推理延迟最多减少45%，异构GPU资源利用率显著提高

Conclusion: STADI通过时空自适应调度成功解决了异构环境中扩散模型推理的负载平衡问题，为高效并行推理提供了有效解决方案

Abstract: The escalating adoption of diffusion models for applications such as image generation demands efficient parallel inference techniques to manage their substantial computational cost. However, existing diffusion parallelism inference schemes often underutilize resources in heterogeneous multi-GPU environments, where varying hardware capabilities or background tasks cause workload imbalance. This paper introduces Spatio-Temporal Adaptive Diffusion Inference (STADI), a novel framework to accelerate diffusion model inference in such settings. At its core is a hybrid scheduler that orchestrates fine-grained parallelism across both temporal and spatial dimensions. Temporally, STADI introduces a novel computation-aware step allocator applied after warmup phases, using a least-common-multiple-minimizing quantization technique to reduce denoising steps on slower GPUs and execution synchronization. To further minimize GPU idle periods, STADI executes an elastic patch parallelism mechanism that allocates variably sized image patches to GPUs according to their computational capability, ensuring balanced workload distribution through a complementary spatial mechanism. Extensive experiments on both load-imbalanced and heterogeneous multi-GPU clusters validate STADI's efficacy, demonstrating improved load balancing and mitigation of performance bottlenecks. Compared to patch parallelism, a state-of-the-art diffusion inference framework, our method significantly reduces end-to-end inference latency by up to 45% and significantly improves resource utilization on heterogeneous GPUs.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [13] [AURAD: Anatomy-Pathology Unified Radiology Synthesis with Progressive Representations](https://arxiv.org/abs/2509.04819)
*Shuhan Ding,Jingjing Fu,Yu Gu,Naiteek Sangani,Mu Wei,Paul Vozila,Nan Liu,Jiang Bian,Hoifung Poon*

Main category: eess.IV

TL;DR: AURAD是一个可控的放射学合成框架，能够联合生成高保真胸部X光片和伪语义掩码，通过临床提示生成解剖结构相关的多病理共存掩码来指导图像合成，显著提升了医学图像合成的可控性和临床相关性。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像合成中由于高质量标注有限和数据集间域偏移导致的细粒度可控合成困难，特别是在胸部X光片中疾病模式形态多样且与解剖结构紧密交织的挑战。

Method: 采用渐进式流程：首先基于解剖结构条件从临床提示生成伪掩码，然后用这些掩码指导图像合成；利用预训练专家医学模型过滤输出以确保临床合理性；生成的掩码还可作为下游任务的标签。

Result: 78%的合成图像被认证放射科医生分类为真实图像，超过40%的预测分割覆盖被评定为临床有用；在多个任务和数据集上展示了方法的有效性和泛化能力。

Conclusion: AURAD框架不仅实现了视觉真实性，还通过生成的掩码连接了生成建模与真实世界临床应用，为数据稀缺的临床环境提供了有效的解决方案。

Abstract: Medical image synthesis has become an essential strategy for augmenting datasets and improving model generalization in data-scarce clinical settings. However, fine-grained and controllable synthesis remains difficult due to limited high-quality annotations and domain shifts across datasets. Existing methods, often designed for natural images or well-defined tumors, struggle to generalize to chest radiographs, where disease patterns are morphologically diverse and tightly intertwined with anatomical structures. To address these challenges, we propose AURAD, a controllable radiology synthesis framework that jointly generates high-fidelity chest X-rays and pseudo semantic masks. Unlike prior approaches that rely on randomly sampled masks-limiting diversity, controllability, and clinical relevance-our method learns to generate masks that capture multi-pathology coexistence and anatomical-pathological consistency. It follows a progressive pipeline: pseudo masks are first generated from clinical prompts conditioned on anatomical structures, and then used to guide image synthesis. We also leverage pretrained expert medical models to filter outputs and ensure clinical plausibility. Beyond visual realism, the synthesized masks also serve as labels for downstream tasks such as detection and segmentation, bridging the gap between generative modeling and real-world clinical applications. Extensive experiments and blinded radiologist evaluations demonstrate the effectiveness and generalizability of our method across tasks and datasets. In particular, 78% of our synthesized images are classified as authentic by board-certified radiologists, and over 40% of predicted segmentation overlays are rated as clinically useful. All code, pre-trained models, and the synthesized dataset will be released upon publication.

</details>


### [14] [Multi-modal Uncertainty Robust Tree Cover Segmentation For High-Resolution Remote Sensing Images](https://arxiv.org/abs/2509.04870)
*Yuanyuan Gui,Wei Li,Yinjian Wang,Xiang-Gen Xia,Mauro Marty,Christian Ginzler,Zuyuan Wang*

Main category: eess.IV

TL;DR: MURTreeFormer是一个新颖的多模态分割框架，通过概率潜在表示和VAE重采样机制处理多模态遥感图像中的时间错位不确定性，显著提高了树冠覆盖分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 多模态遥感图像（光学、LiDAR、SAR）通常在不同时间采集，时间错位会导致植被扰动和成像质量变化，引入跨模态不确定性，严重影响分割精度。

Method: 提出MURTreeFormer框架：1）将一种模态作为主要模态，其他作为辅助模态；2）通过概率潜在表示建模辅助模态的补丁级不确定性；3）使用VAE重采样机制重建不确定补丁；4）在解码器中集成梯度幅度注意力模块和轻量级细化头。

Result: 在上海和苏黎世的多模态数据集上进行广泛实验，证明MURTreeFormer显著提高了分割性能，有效减少了时间引起的随机不确定性影响。

Conclusion: 该方法成功解决了多模态遥感图像中的时间错位问题，为树冠覆盖制图提供了更鲁棒的解决方案，在城规、森林监测和生态评估中具有重要应用价值。

Abstract: Recent advances in semantic segmentation of multi-modal remote sensing images have significantly improved the accuracy of tree cover mapping, supporting applications in urban planning, forest monitoring, and ecological assessment. Integrating data from multiple modalities-such as optical imagery, light detection and ranging (LiDAR), and synthetic aperture radar (SAR)-has shown superior performance over single-modality methods. However, these data are often acquired days or even months apart, during which various changes may occur, such as vegetation disturbances (e.g., logging, and wildfires) and variations in imaging quality. Such temporal misalignments introduce cross-modal uncertainty, especially in high-resolution imagery, which can severely degrade segmentation accuracy. To address this challenge, we propose MURTreeFormer, a novel multi-modal segmentation framework that mitigates and leverages aleatoric uncertainty for robust tree cover mapping. MURTreeFormer treats one modality as primary and others as auxiliary, explicitly modeling patch-level uncertainty in the auxiliary modalities via a probabilistic latent representation. Uncertain patches are identified and reconstructed from the primary modality's distribution through a VAE-based resampling mechanism, producing enhanced auxiliary features for fusion. In the decoder, a gradient magnitude attention (GMA) module and a lightweight refinement head (RH) are further integrated to guide attention toward tree-like structures and to preserve fine-grained spatial details. Extensive experiments on multi-modal datasets from Shanghai and Zurich demonstrate that MURTreeFormer significantly improves segmentation performance and effectively reduces the impact of temporally induced aleatoric uncertainty.

</details>


### [15] [VLSM-Ensemble: Ensembling CLIP-based Vision-Language Models for Enhanced Medical Image Segmentation](https://arxiv.org/abs/2509.05154)
*Julia Dietlmeier,Oluwabukola Grace Adegboro,Vayangi Ganepola,Claudia Mazo,Noel E. O'Connor*

Main category: eess.IV

TL;DR: 通过将视觉语言分割模型(VLSMs)与低复杂度CNN集成，显著提升了医学图像分割性能，在BKAI息肉数据集上Dice分数提高了6.3%，其他数据集也有1-6%的提升


<details>
  <summary>Details</summary>
Motivation: 当前基于CLIP和BiomedCLIP的视觉语言分割模型性能仍落后于CRIS等复杂架构，希望通过模型集成方法缩小这一差距

Method: 采用视觉语言分割模型(VLSMs)与低复杂度CNN进行集成，而非传统的文本提示工程方法

Result: 在BKAI息肉数据集上Dice分数提升6.3%，其他四个放射学和非放射学数据集也有1-6%的性能提升，集成效果因数据集而异

Conclusion: 模型集成方法在不同数据集上表现差异明显，有时优于CRIS模型有时不如，这为社区提供了未来研究的重要方向

Abstract: Vision-language models and their adaptations to image segmentation tasks present enormous potential for producing highly accurate and interpretable results. However, implementations based on CLIP and BiomedCLIP are still lagging behind more sophisticated architectures such as CRIS. In this work, instead of focusing on text prompt engineering as is the norm, we attempt to narrow this gap by showing how to ensemble vision-language segmentation models (VLSMs) with a low-complexity CNN. By doing so, we achieve a significant Dice score improvement of 6.3% on the BKAI polyp dataset using the ensembled BiomedCLIPSeg, while other datasets exhibit gains ranging from 1% to 6%. Furthermore, we provide initial results on additional four radiology and non-radiology datasets. We conclude that ensembling works differently across these datasets (from outperforming to underperforming the CRIS model), indicating a topic for future investigation by the community. The code is available at https://github.com/juliadietlmeier/VLSM-Ensemble.

</details>
