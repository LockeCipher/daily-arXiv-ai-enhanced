<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 7]
- [cs.CV](#cs.CV) [Total: 33]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [SpotDiff: Spotting and Disentangling Interference in Feature Space for Subject-Preserving Image Generation](https://arxiv.org/abs/2510.07340)
*Yongzhi Li,Saining Zhang,Yibing Chen,Boying Li,Yanxin Zhang,Xiaoyu Du*

Main category: cs.GR

TL;DR: SpotDiff是一种基于学习的个性化图像生成方法，通过识别和解缠干扰来提取特定主题特征，在保持主题身份的同时实现高效生成和可控编辑。


<details>
  <summary>Details</summary>
Motivation: 现有优化方法计算成本高，而学习方法存在特征纠缠问题，受干扰因素影响。需要一种既能保持高保真度又高效的方法。

Method: 利用预训练的CLIP图像编码器和专门的姿态、背景专家网络，通过特征空间正交性约束来隔离主题身份。构建SpotDiff10k数据集进行训练。

Result: SpotDiff比现有方法实现更鲁棒的主题保持和可控编辑，仅用1万训练样本就达到竞争性性能。

Conclusion: SpotDiff通过解缠干扰的方法在个性化图像生成中实现了高效且高保真的表现。

Abstract: Personalized image generation aims to faithfully preserve a reference subject's identity while adapting to diverse text prompts. Existing optimization-based methods ensure high fidelity but are computationally expensive, while learning-based approaches offer efficiency at the cost of entangled representations influenced by nuisance factors. We introduce SpotDiff, a novel learning-based method that extracts subject-specific features by spotting and disentangling interference. Leveraging a pre-trained CLIP image encoder and specialized expert networks for pose and background, SpotDiff isolates subject identity through orthogonality constraints in the feature space. To enable principled training, we introduce SpotDiff10k, a curated dataset with consistent pose and background variations. Experiments demonstrate that SpotDiff achieves more robust subject preservation and controllable editing than prior methods, while attaining competitive performance with only 10k training samples.

</details>


### [2] [Local MAP Sampling for Diffusion Models](https://arxiv.org/abs/2510.07343)
*Shaorong Zhang,Rob Brekelmans,Greg Ver Steeg*

Main category: cs.GR

TL;DR: 提出了Local MAP Sampling (LMAPS)框架，通过沿扩散轨迹迭代求解局部MAP子问题，为基于优化的扩散求解器提供统一的概率解释，并在多个图像恢复和科学任务中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于优化的扩散求解器在逆问题中表现优异但缺乏清晰的概率基础，而DPS虽然提供贝叶斯方法但实际目标是最准确重建而非覆盖后验分布。

Method: 引入LMAPS框架，迭代求解扩散轨迹上的局部MAP子问题；开发具有概率可解释的协方差近似、稳定可解释的目标函数重构，以及针对不可微算子的梯度近似。

Result: 在广泛的图像恢复和科学任务中实现最先进性能，包括运动去模糊、JPEG恢复和量化任务上≥2dB增益，逆散射基准上>1.5dB改进。

Conclusion: LMAPS为基于优化的扩散方法提供了统一的概率解释，并在实践中显著优于现有方法，建立了优化方法与概率方法之间的桥梁。

Abstract: Diffusion Posterior Sampling (DPS) provides a principled Bayesian approach to inverse problems by sampling from $p(x_0 \mid y)$. However, in practice, the goal of inverse problem solving is not to cover the posterior but to recover the most accurate reconstruction, where optimization-based diffusion solvers often excel despite lacking a clear probabilistic foundation. We introduce Local MAP Sampling (LMAPS), a new inference framework that iteratively solving local MAP subproblems along the diffusion trajectory. This perspective clarifies their connection to global MAP estimation and DPS, offering a unified probabilistic interpretation for optimization-based methods. Building on this foundation, we develop practical algorithms with a probabilistically interpretable covariance approximation, a reformulated objective for stability and interpretability, and a gradient approximation for non-differentiable operators. Across a broad set of image restoration and scientific tasks, LMAPS achieves state-of-the-art performance, including $\geq 2$ dB gains on motion deblurring, JPEG restoration, and quantization, and $>1.5$ dB improvements on inverse scattering benchmarks.

</details>


### [3] [NRRS: Neural Russian Roulette and Splitting](https://arxiv.org/abs/2510.07868)
*Haojie Jin,Jierui Ren,Yisong Chen,Guoping Wang,Sheng Li*

Main category: cs.GR

TL;DR: 提出了一种专为波前路径追踪设计的俄罗斯轮盘赌和分裂（RRS）框架，通过归一化RRS公式和有界路径计数解决了传统方法与波前架构的兼容性问题，并引入神经网络学习RRS因子。


<details>
  <summary>Details</summary>
Motivation: 传统RRS方法由于路径数量不可预测，与波前路径追踪的预分配内存和调度需求不兼容，需要开发适合该架构的稳定内存高效方法。

Method: 提出归一化RRS公式确保路径计数有界；设计NRRS和AID-NRRS两种神经网络模型学习RRS因子；引入Mix-Depth机制自适应调节神经评估以平衡计算成本和推理精度。

Result: 在多种复杂场景下，该方法在渲染质量和性能方面均优于传统启发式方法和近期RRS技术。

Conclusion: 所提出的框架成功解决了波前路径追踪中RRS的兼容性问题，通过神经网络学习RRS因子和自适应调节机制，实现了更好的渲染效果和效率。

Abstract: We propose a novel framework for Russian Roulette and Splitting (RRS) tailored to wavefront path tracing, a highly parallel rendering architecture that processes path states in batched, stage-wise execution for efficient GPU utilization. Traditional RRS methods, with unpredictable path counts, are fundamentally incompatible with wavefront's preallocated memory and scheduling requirements. To resolve this, we introduce a normalized RRS formulation with a bounded path count, enabling stable and memory-efficient execution.   Furthermore, we pioneer the use of neural networks to learn RRS factors, presenting two models: NRRS and AID-NRRS. At a high level, both feature a carefully designed RRSNet that explicitly incorporates RRS normalization, with only subtle differences in their implementation. To balance computational cost and inference accuracy, we introduce Mix-Depth, a path-depth-aware mechanism that adaptively regulates neural evaluation, further improving efficiency.   Extensive experiments demonstrate that our method outperforms traditional heuristics and recent RRS techniques in both rendering quality and performance across a variety of complex scenes.

</details>


### [4] [Variable-Rate Texture Compression: Real-Time Rendering with JPEG](https://arxiv.org/abs/2510.08166)
*Elias Kristmann,Markus Schütz,Michael Wimmer*

Main category: cs.GR

TL;DR: 该论文研究了在现代GPU上使用JPEG格式进行可变速率纹理压缩的可行性，并与固定速率压缩方法BC1和ASTC进行比较。通过延迟渲染管线，能够识别所需纹理块并解码，在RTX 4090上仅增加0.3ms渲染时间。


<details>
  <summary>Details</summary>
Motivation: 虽然可变速率压缩图像格式如JPEG广泛用于高效编码图像，但由于需要随机访问单个纹素等特殊要求，尚未在实时渲染中得到应用。

Method: 使用延迟渲染管线识别每帧所需的纹理块子集，解码这些块并为帧缓冲像素着色。

Result: 尽管需要额外约0.17比特每像素，JPEG在质量和压缩率方面显著优于BC1，根据图像类型，可超越或与ASTC竞争。

Conclusion: 复杂的可变速率压缩方案在现代GPU上是可行的，即使在VR环境中，JPEG渲染管线在RTX 4090上仅增加不到0.3ms的渲染时间。

Abstract: Although variable-rate compressed image formats such as JPEG are widely used to efficiently encode images, they have not found their way into real-time rendering due to special requirements such as random access to individual texels. In this paper, we investigate the feasibility of variable-rate texture compression on modern GPUs using the JPEG format, and how it compares to the GPU-friendly fixed-rate compression approaches BC1 and ASTC. Using a deferred rendering pipeline, we are able to identify the subset of blocks that are needed for a given frame, decode these, and colorize the framebuffer's pixels. Despite the additional $\sim$0.17 bit per pixel that we require for our approach, JPEG maintains significantly better quality and compression rates compared to BC1, and depending on the type of image, outperforms or competes with ASTC. The JPEG rendering pipeline increases rendering duration by less than 0.3 ms on an RTX 4090, demonstrating that sophisticated variable-rate compression schemes are feasible on modern GPUs, even in VR. Source code and data sets are available at: https://github.com/elias1518693/jpeg_textures

</details>


### [5] [SViM3D: Stable Video Material Diffusion for Single Image 3D Generation](https://arxiv.org/abs/2510.08271)
*Andreas Engelhardt,Mark Boss,Vikram Voletti,Chun-Han Yao,Hendrik P. A. Lensch,Varun Jampani*

Main category: cs.GR

TL;DR: SViM3D是一个从单张图像预测多视角一致PBR材质的框架，通过扩展视频扩散模型输出空间变化的PBR参数和法线，实现可重光照的3D资产生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么使用简单的材质模型，要么需要额外步骤估计反射率，限制了重光照和外观编辑能力。

Method: 扩展潜在视频扩散模型，基于显式相机控制联合输出PBR参数和表面法线，引入多种机制改善病态设置下的质量。

Result: 在多个以物体为中心的数据集上实现了最先进的重光照和新视角合成性能，能泛化到多样化输入。

Conclusion: 该方法能够生成适用于AR/VR、电影、游戏等视觉媒体的可重光照3D资产。

Abstract: We present Stable Video Materials 3D (SViM3D), a framework to predict multi-view consistent physically based rendering (PBR) materials, given a single image. Recently, video diffusion models have been successfully used to reconstruct 3D objects from a single image efficiently. However, reflectance is still represented by simple material models or needs to be estimated in additional steps to enable relighting and controlled appearance edits. We extend a latent video diffusion model to output spatially varying PBR parameters and surface normals jointly with each generated view based on explicit camera control. This unique setup allows for relighting and generating a 3D asset using our model as neural prior. We introduce various mechanisms to this pipeline that improve quality in this ill-posed setting. We show state-of-the-art relighting and novel view synthesis performance on multiple object-centric datasets. Our method generalizes to diverse inputs, enabling the generation of relightable 3D assets useful in AR/VR, movies, games and other visual media.

</details>


### [6] [Splat the Net: Radiance Fields with Splattable Neural Primitives](https://arxiv.org/abs/2510.08491)
*Xilong Zhou,Bao-Huy Nguyen,Loïc Magne,Vladislav Golyanik,Thomas Leimkühler,Christian Theobalt*

Main category: cs.GR

TL;DR: 提出了一种可拼接的神经基元表示方法，将神经模型的表达能力与基元拼接的效率相结合，在保持实时渲染质量的同时，使用更少的基元和参数。


<details>
  <summary>Details</summary>
Motivation: 现有神经辐射场方法渲染成本高，而基元拼接方法虽然实时但表达能力有限。希望结合两者的优势，开发既能保持高质量表达又能实现高效渲染的新表示方法。

Method: 使用有界神经密度场的基元，每个基元由浅层神经网络参数化。通过精确解析线积分计算，实现透视准确的拼接核，无需光线行进。

Result: 在新视角合成基准测试中，匹配3D高斯拼接的质量和速度，同时使用10倍更少的基元和6倍更少的参数。

Conclusion: 该方法成功结合了神经模型的表达能力与基元拼接的效率，提供了一种高效且表达力强的3D场景表示方案。

Abstract: Radiance fields have emerged as a predominant representation for modeling 3D scene appearance. Neural formulations such as Neural Radiance Fields provide high expressivity but require costly ray marching for rendering, whereas primitive-based methods such as 3D Gaussian Splatting offer real-time efficiency through splatting, yet at the expense of representational power. Inspired by advances in both these directions, we introduce splattable neural primitives, a new volumetric representation that reconciles the expressivity of neural models with the efficiency of primitive-based splatting. Each primitive encodes a bounded neural density field parameterized by a shallow neural network. Our formulation admits an exact analytical solution for line integrals, enabling efficient computation of perspectively accurate splatting kernels. As a result, our representation supports integration along view rays without the need for costly ray marching. The primitives flexibly adapt to scene geometry and, being larger than prior analytic primitives, reduce the number required per scene. On novel-view synthesis benchmarks, our approach matches the quality and speed of 3D Gaussian Splatting while using $10\times$ fewer primitives and $6\times$ fewer parameters. These advantages arise directly from the representation itself, without reliance on complex control or adaptation frameworks. The project page is https://vcai.mpi-inf.mpg.de/projects/SplatNet/.

</details>


### [7] [X2Video: Adapting Diffusion Models for Multimodal Controllable Neural Video Rendering](https://arxiv.org/abs/2510.08530)
*Zhitong Huang,Mohan Zhang,Renhan Wang,Rui Tang,Hao Zhu,Jing Liao*

Main category: cs.GR

TL;DR: X2Video是首个基于内在通道（反照率、法线、粗糙度、金属性和辐照度）生成照片级真实感视频的扩散模型，支持参考图像和文本提示的多模态控制，并能通过混合自注意力确保时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型缺乏对颜色、材质、几何和光照的精确控制能力，需要开发能够利用内在通道指导并支持多模态控制的视频生成方法。

Method: 扩展XRGB图像生成模型到视频领域，采用混合自注意力确保时间一致性，使用掩码交叉注意力分离全局和局部文本提示，并通过递归采样方法生成长视频。

Result: X2Video能够生成长时间、时间一致、照片级真实的视频，有效支持多模态控制，并实现了对颜色、材质、几何和光照的参数化编辑。

Conclusion: X2Video通过内在通道指导和多模态控制，为视频生成提供了精确的编辑能力和高质量的输出结果，在定性和定量评估中均表现出色。

Abstract: We present X2Video, the first diffusion model for rendering photorealistic videos guided by intrinsic channels including albedo, normal, roughness, metallicity, and irradiance, while supporting intuitive multi-modal controls with reference images and text prompts for both global and local regions. The intrinsic guidance allows accurate manipulation of color, material, geometry, and lighting, while reference images and text prompts provide intuitive adjustments in the absence of intrinsic information. To enable these functionalities, we extend the intrinsic-guided image generation model XRGB to video generation by employing a novel and efficient Hybrid Self-Attention, which ensures temporal consistency across video frames and also enhances fidelity to reference images. We further develop a Masked Cross-Attention to disentangle global and local text prompts, applying them effectively onto respective local and global regions. For generating long videos, our novel Recursive Sampling method incorporates progressive frame sampling, combining keyframe prediction and frame interpolation to maintain long-range temporal consistency while preventing error accumulation. To support the training of X2Video, we assembled a video dataset named InteriorVideo, featuring 1,154 rooms from 295 interior scenes, complete with reliable ground-truth intrinsic channel sequences and smooth camera trajectories. Both qualitative and quantitative evaluations demonstrate that X2Video can produce long, temporally consistent, and photorealistic videos guided by intrinsic conditions. Additionally, X2Video effectively accommodates multi-modal controls with reference images, global and local text prompts, and simultaneously supports editing on color, material, geometry, and lighting through parametric tuning. Project page: https://luckyhzt.github.io/x2video

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [8] [DynamicEval: Rethinking Evaluation for Dynamic Text-to-Video Synthesis](https://arxiv.org/abs/2510.07441)
*Nithin C. Babu,Aniruddha Mahapatra,Harsh Rangwani,Rajiv Soundararajan,Kuldeep Kulkarni*

Main category: cs.CV

TL;DR: DynamicEval是一个新的文本到视频评估基准，专注于动态相机运动评估，包含系统策划的提示语和45K人工标注，提出了新的背景一致性和前景一致性指标。


<details>
  <summary>Details</summary>
Motivation: 现有T2V评估基准存在两个局限：(i) 忽视动态相机运动评估，(ii) 仅关注模型级评分而忽略视频级评估。

Method: 提出DynamicEval基准，包含动态相机运动提示语和45K人工标注；开发新的背景一致性指标（基于VBench运动平滑度但修正遮挡问题）和前景一致性指标（跟踪对象实例内的点）。

Result: 新指标在视频级和模型级与人类偏好相关性提升超过2%，在动态相机运动下提供更全面的T2V模型评估。

Conclusion: DynamicEval建立了更全面的T2V模型评估基准，特别针对动态相机运动场景，提出的新指标显著提升了与人类判断的一致性。

Abstract: Existing text-to-video (T2V) evaluation benchmarks, such as VBench and EvalCrafter, suffer from two limitations. (i) While the emphasis is on subject-centric prompts or static camera scenes, camera motion essential for producing cinematic shots and existing metrics under dynamic motion are largely unexplored. (ii) These benchmarks typically aggregate video-level scores into a single model-level score for ranking generative models. Such aggregation, however, overlook video-level evaluation, which is vital to selecting the better video among the candidate videos generated for a given prompt. To address these gaps, we introduce DynamicEval, a benchmark consisting of systematically curated prompts emphasizing dynamic camera motion, paired with 45k human annotations on video pairs from 3k videos generated by ten T2V models. DynamicEval evaluates two key dimensions of video quality: background scene consistency and foreground object consistency. For background scene consistency, we obtain the interpretable error maps based on the Vbench motion smoothness metric. We observe that while the Vbench motion smoothness metric shows promising alignment with human judgments, it fails in two cases: occlusions/disocclusions arising from camera and foreground object movements. Building on this, we propose a new background consistency metric that leverages object error maps to correct two failure cases in a principled manner. Our second innovation is the introduction of a foreground consistency metric that tracks points and their neighbors within each object instance to assess object fidelity. Extensive experiments demonstrate that our proposed metrics achieve stronger correlations with human preferences at both the video level and the model level (an improvement of more than 2% points), establishing DynamicEval as a more comprehensive benchmark for evaluating T2V models under dynamic camera motion.

</details>


### [9] [Provably Accelerated Imaging with Restarted Inertia and Score-based Image Priors](https://arxiv.org/abs/2510.07470)
*Marien Renaud,Julien Hermant,Deliang Wei,Yu Sun*

Main category: cs.CV

TL;DR: RISP是一种改进的RED算法，通过引入重启惯性机制实现快速收敛，同时保持基于分数的图像先验以获得高质量重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法如RED主要关注设计复杂的图像先验来提高重建质量，但收敛加速通常依赖启发式方法。需要一种既能快速收敛又能保证高质量重建的原则性方法。

Method: 提出RISP算法，结合重启惯性机制和基于分数的图像先验，从RED扩展而来。通过理论分析和连续时间动力学系统推导，建立了与重球ODE的联系。

Result: 实验证明RISP在多种成像逆问题中实现了快速收敛，同时获得高质量重建。理论分析表明RISP比RED具有更快的驻点收敛速度。

Conclusion: RISP成功地将快速收敛与高质量重建相结合，为成像逆问题提供了一种有效的解决方案，无需图像先验的凸性假设。

Abstract: Fast convergence and high-quality image recovery are two essential features of algorithms for solving ill-posed imaging inverse problems. Existing methods, such as regularization by denoising (RED), often focus on designing sophisticated image priors to improve reconstruction quality, while leaving convergence acceleration to heuristics. To bridge the gap, we propose Restarted Inertia with Score-based Priors (RISP) as a principled extension of RED. RISP incorporates a restarting inertia for fast convergence, while still allowing score-based image priors for high-quality reconstruction. We prove that RISP attains a faster stationary-point convergence rate than RED, without requiring the convexity of the image prior. We further derive and analyze the associated continuous-time dynamical system, offering insight into the connection between RISP and the heavy-ball ordinary differential equation (ODE). Experiments across a range of imaging inverse problems demonstrate that RISP enables fast convergence while achieving high-quality reconstructions.

</details>


### [10] [A Denoising Framework for Real-World Ultra-Low Dose Lung CT Images Based on an Image Purification Strategy](https://arxiv.org/abs/2510.07492)
*Guoliang Gong,Man Yu*

Main category: cs.CV

TL;DR: 提出基于图像纯化策略的创新去噪框架，解决超低剂量CT图像中的噪声、伪影和空间错位问题，通过频率域流匹配模型在真实临床数据集上取得最先进结果。


<details>
  <summary>Details</summary>
Motivation: 超低剂量CT显著降低辐射暴露但引入严重噪声和伪影，导致uLDCT与正常剂量CT图像对之间存在显著空间错位，现有去噪网络难以直接应用。

Method: 构建真实临床uLDCT肺部数据集，提出图像纯化策略生成结构对齐的图像对，并开发频率域流匹配模型协同工作以保持解剖结构完整性。

Result: 在真实临床数据集上，IP策略显著提升多个主流去噪模型性能，FFM模型结合IP策略在解剖结构保持方面达到最先进水平。

Conclusion: 本研究为真实世界uLDCT去噪中的数据不匹配问题提供了有效解决方案。

Abstract: Ultra-low dose CT (uLDCT) significantly reduces radiation exposure but introduces severe noise and artifacts. It also leads to substantial spatial misalignment between uLDCT and normal dose CT (NDCT) image pairs. This poses challenges for directly applying existing denoising networks trained on synthetic noise or aligned data. To address this core challenge in uLDCT denoising, this paper proposes an innovative denoising framework based on an Image Purification (IP) strategy. First, we construct a real clinical uLDCT lung dataset. Then, we propose an Image Purification strategy that generates structurally aligned uLDCT-NDCT image pairs, providing a high-quality data foundation for network training. Building upon this, we propose a Frequency-domain Flow Matching (FFM) model, which works synergistically with the IP strategy to excellently preserve the anatomical structure integrity of denoised images. Experiments on the real clinical dataset demonstrate that our IP strategy significantly enhances the performance of multiple mainstream denoising models on the uLDCT task. Notably, our proposed FFM model combined with the IP strategy achieves state-of-the-art (SOTA) results in anatomical structure preservation. This study provides an effective solution to the data mismatch problem in real-world uLDCT denoising. Code and dataset are available at https://github.com/MonkeyDadLufy/flow-matching.

</details>


### [11] [PickStyle: Video-to-Video Style Transfer with Context-Style Adapters](https://arxiv.org/abs/2510.07546)
*Soroush Mehraban,Vida Adeli,Jacob Rommann,Babak Taati,Kyryl Truskovskyi*

Main category: cs.CV

TL;DR: PickStyle是一个视频风格迁移框架，通过向预训练视频扩散模型添加风格适配器，利用成对静态图像数据进行训练，实现内容保留和风格转换的视频风格迁移。


<details>
  <summary>Details</summary>
Motivation: 解决视频风格迁移任务中缺乏配对视频数据监督的挑战，目标是保持输入视频内容的同时根据文本提示渲染目标风格。

Method: 在条件模块的自注意力层插入低秩适配器，利用共享增强构建合成训练剪辑模拟相机运动，并提出上下文-风格分类器自由引导(CS-CFG)方法。

Result: 在多个基准测试中实现了时间一致、风格忠实且内容保留的视频转换，在定性和定量评估中均优于现有基线方法。

Conclusion: PickStyle框架通过有效的适配器设计和训练策略，成功解决了视频风格迁移中的数据稀缺问题，实现了高质量的动态视频风格转换。

Abstract: We address the task of video style transfer with diffusion models, where the goal is to preserve the context of an input video while rendering it in a target style specified by a text prompt. A major challenge is the lack of paired video data for supervision. We propose PickStyle, a video-to-video style transfer framework that augments pretrained video diffusion backbones with style adapters and benefits from paired still image data with source-style correspondences for training. PickStyle inserts low-rank adapters into the self-attention layers of conditioning modules, enabling efficient specialization for motion-style transfer while maintaining strong alignment between video content and style. To bridge the gap between static image supervision and dynamic video, we construct synthetic training clips from paired images by applying shared augmentations that simulate camera motion, ensuring temporal priors are preserved. In addition, we introduce Context-Style Classifier-Free Guidance (CS-CFG), a novel factorization of classifier-free guidance into independent text (style) and video (context) directions. CS-CFG ensures that context is preserved in generated video while the style is effectively transferred. Experiments across benchmarks show that our approach achieves temporally coherent, style-faithful, and content-preserving video translations, outperforming existing baselines both qualitatively and quantitatively.

</details>


### [12] [Rectified-CFG++ for Flow Based Models](https://arxiv.org/abs/2510.07631)
*Shreshth Saini,Shashank Gupta,Alan C. Bovik*

Main category: cs.CV

TL;DR: 提出Rectified-CFG++方法，解决标准CFG在整流流模型中的离流形漂移问题，通过自适应预测-校正引导确保轨迹保持在数据流形附近


<details>
  <summary>Details</summary>
Motivation: 标准CFG在整流流模型中会引起严重的离流形漂移，导致视觉伪影、文本不对齐和脆弱行为

Method: 自适应预测-校正引导：先执行条件RF更新锚定样本，然后应用加权条件校正，在条件和无条件速度场之间插值

Result: 在Flux、Stable Diffusion 3/3.5、Lumina等大规模文本到图像模型上，在MS-COCO、LAION-Aesthetic、T2I-CompBench基准数据集上持续优于标准CFG

Conclusion: Rectified-CFG++将整流流的确定性效率与几何感知条件规则相结合，确保轨迹保持在数据流形的有界管状邻域内，实现稳定引导

Abstract: Classifier-free guidance (CFG) is the workhorse for steering large diffusion models toward text-conditioned targets, yet its native application to rectified flow (RF) based models provokes severe off-manifold drift, yielding visual artifacts, text misalignment, and brittle behaviour. We present Rectified-CFG++, an adaptive predictor-corrector guidance that couples the deterministic efficiency of rectified flows with a geometry-aware conditioning rule. Each inference step first executes a conditional RF update that anchors the sample near the learned transport path, then applies a weighted conditional correction that interpolates between conditional and unconditional velocity fields. We prove that the resulting velocity field is marginally consistent and that its trajectories remain within a bounded tubular neighbourhood of the data manifold, ensuring stability across a wide range of guidance strengths. Extensive experiments on large-scale text-to-image models (Flux, Stable Diffusion 3/3.5, Lumina) show that Rectified-CFG++ consistently outperforms standard CFG on benchmark datasets such as MS-COCO, LAION-Aesthetic, and T2I-CompBench. Project page: https://rectified-cfgpp.github.io/

</details>


### [13] [Once Is Enough: Lightweight DiT-Based Video Virtual Try-On via One-Time Garment Appearance Injection](https://arxiv.org/abs/2510.07654)
*Yanjie Pan,Qingdong He,Lidong Wang,Bo Peng,Mingmin Chi*

Main category: cs.CV

TL;DR: 提出OIE方法，基于首帧服装替换的视频虚拟试穿策略，通过图像模型替换首帧服装，在编辑后的首帧内容控制下，利用姿态和掩码信息引导视频生成模型合成后续帧。


<details>
  <summary>Details</summary>
Motivation: 当前基于U-Net的双分支架构在扩散模型中取得成功，但难以适应基于Diffusion Transformer的模型。引入服装参考分支的潜空间特征需要修改主干网络，导致大量可训练参数，且服装潜空间特征缺乏时间特性需要额外学习。

Method: 采用首帧服装替换策略：使用基于图像的服装转移模型替换初始帧服装，在编辑后首帧的内容控制下，利用姿态和掩码信息引导视频生成模型按顺序合成剩余帧。

Result: 实验表明该方法在参数效率和计算效率方面表现优异，同时在约束条件下仍保持领先性能。

Conclusion: OIE方法通过首帧替换策略有效解决了基于Diffusion Transformer的虚拟试穿挑战，实现了高效且高性能的视频服装替换。

Abstract: Video virtual try-on aims to replace the clothing of a person in a video with a target garment. Current dual-branch architectures have achieved significant success in diffusion models based on the U-Net; however, adapting them to diffusion models built upon the Diffusion Transformer remains challenging. Initially, introducing latent space features from the garment reference branch requires adding or modifying the backbone network, leading to a large number of trainable parameters. Subsequently, the latent space features of garments lack inherent temporal characteristics and thus require additional learning. To address these challenges, we propose a novel approach, OIE (Once is Enough), a virtual try-on strategy based on first-frame clothing replacement: specifically, we employ an image-based clothing transfer model to replace the clothing in the initial frame, and then, under the content control of the edited first frame, utilize pose and mask information to guide the temporal prior of the video generation model in synthesizing the remaining frames sequentially. Experiments show that our method achieves superior parameter efficiency and computational efficiency while still maintaining leading performance under these constraints.

</details>


### [14] [Controllable Video Synthesis via Variational Inference](https://arxiv.org/abs/2510.07670)
*Haoyi Duan,Yunzhi Zhang,Yilun Du,Jiajun Wu*

Main category: cs.CV

TL;DR: 提出一种视频合成方法，支持从精确4D对象轨迹到粗粒度文本提示的多粒度用户控制，通过变分推理和KL散度最小化实现高可控性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型通常针对固定输入格式训练，无法满足需要混合粒度用户控制（从精确4D对象轨迹到粗粒度文本提示）的视频工作流需求。

Method: 将任务建模为变分推理来近似组合分布，利用多个视频生成主干网络共同处理所有任务约束。通过逐步KL散度最小化和上下文条件分解技术解决优化挑战。

Result: 实验表明，与现有方法相比，该方法生成的样本在可控性、多样性和3D一致性方面都有所提升。

Conclusion: 该方法能够为指定元素提供高可控性，同时为未指定元素保持多样性，解决了现有视频生成模型在混合控制粒度方面的局限性。

Abstract: Many video workflows benefit from a mixture of user controls with varying granularity, from exact 4D object trajectories and camera paths to coarse text prompts, while existing video generative models are typically trained for fixed input formats. We develop a video synthesis method that addresses this need and generates samples with high controllability for specified elements while maintaining diversity for under-specified ones. We cast the task as variational inference to approximate a composed distribution, leveraging multiple video generation backbones to account for all task constraints collectively. To address the optimization challenge, we break down the problem into step-wise KL divergence minimization over an annealed sequence of distributions, and further propose a context-conditioned factorization technique that reduces modes in the solution space to circumvent local optima. Experiments suggest that our method produces samples with improved controllability, diversity, and 3D consistency compared to prior works.

</details>


### [15] [RePainter: Empowering E-commerce Object Removal via Spatial-matting Reinforcement Learning](https://arxiv.org/abs/2510.07721)
*Zipeng Guo,Lichen Ma,Xiaolong Fu,Gaojing Zhou,Lan Yang,Yuchen Zhou,Linkai Liu,Yu He,Ximan Liu,Shiping Dong,Jingling Fu,Zhen Chen,Yu Shi,Junshi Huang,Jason Li,Chao Gou*

Main category: cs.CV

TL;DR: 提出了Repainter强化学习框架，通过空间遮罩轨迹优化和GRPO策略，解决电商图像中水印和促销文本的去除问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 电商平台中产品图像的水印和促销文本等侵入性元素阻碍了清晰视觉效果的呈现，现有扩散修复方法在商业场景中面临对象去除不可靠和领域适应性有限的问题。

Method: 结合空间遮罩轨迹优化与Group Relative Policy Optimization(GRPO)，通过调节注意力机制强调背景上下文，生成高奖励样本并减少不需要的对象插入，同时引入平衡全局、局部和语义约束的复合奖励机制。

Result: 在复杂场景中显著优于现有最先进方法，创建了包含10万张高质量电商图像的EcomPaint-100K数据集和标准化基准EcomPaint-Bench。

Conclusion: Repainter框架有效解决了电商图像修复中的关键挑战，为商业应用提供了可靠的解决方案。

Abstract: In web data, product images are central to boosting user engagement and advertising efficacy on e-commerce platforms, yet the intrusive elements such as watermarks and promotional text remain major obstacles to delivering clear and appealing product visuals. Although diffusion-based inpainting methods have advanced, they still face challenges in commercial settings due to unreliable object removal and limited domain-specific adaptation. To tackle these challenges, we propose Repainter, a reinforcement learning framework that integrates spatial-matting trajectory refinement with Group Relative Policy Optimization (GRPO). Our approach modulates attention mechanisms to emphasize background context, generating higher-reward samples and reducing unwanted object insertion. We also introduce a composite reward mechanism that balances global, local, and semantic constraints, effectively reducing visual artifacts and reward hacking. Additionally, we contribute EcomPaint-100K, a high-quality, large-scale e-commerce inpainting dataset, and a standardized benchmark EcomPaint-Bench for fair evaluation. Extensive experiments demonstrate that Repainter significantly outperforms state-of-the-art methods, especially in challenging scenes with intricate compositions. We will release our code and weights upon acceptance.

</details>


### [16] [ComGS: Efficient 3D Object-Scene Composition via Surface Octahedral Probes](https://arxiv.org/abs/2510.07729)
*Jian Gao,Mengqi Yuan,Yifei Zeng,Chang Zeng,Zhihao Li,Zhenyu Chen,Weichao Qiu,Xiao-Xiao Long,Hao Zhu,Xun Cao,Yao Yao*

Main category: cs.CV

TL;DR: 提出ComGS框架，通过表面八面体探针(SOPs)实现高效的3D物体-场景合成，避免昂贵的光线追踪，提供实时阴影计算和高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 解决高斯泼溅(GS)中物体-场景合成时烘焙外观和阴影信息导致的不一致问题，需要可重照明的物体重建和场景光照估计。

Method: 使用表面八面体探针(SOPs)存储光照和遮挡信息，通过插值实现高效3D查询；简化场景光照估计，专注于物体放置位置的环境光照，使用扩散模型补全光照。

Result: 实现约28 FPS的高质量实时渲染，产生视觉和谐的合成结果和生动的阴影，编辑仅需36秒。

Conclusion: ComGS框架成功解决了3D物体-场景合成中的光照一致性问题，实现了高效、高质量的实时渲染。

Abstract: Gaussian Splatting (GS) enables immersive rendering, but realistic 3D object-scene composition remains challenging. Baked appearance and shadow information in GS radiance fields cause inconsistencies when combining objects and scenes. Addressing this requires relightable object reconstruction and scene lighting estimation. For relightable object reconstruction, existing Gaussian-based inverse rendering methods often rely on ray tracing, leading to low efficiency. We introduce Surface Octahedral Probes (SOPs), which store lighting and occlusion information and allow efficient 3D querying via interpolation, avoiding expensive ray tracing. SOPs provide at least a 2x speedup in reconstruction and enable real-time shadow computation in Gaussian scenes. For lighting estimation, existing Gaussian-based inverse rendering methods struggle to model intricate light transport and often fail in complex scenes, while learning-based methods predict lighting from a single image and are viewpoint-sensitive. We observe that 3D object-scene composition primarily concerns the object's appearance and nearby shadows. Thus, we simplify the challenging task of full scene lighting estimation by focusing on the environment lighting at the object's placement. Specifically, we capture a 360 degrees reconstructed radiance field of the scene at the location and fine-tune a diffusion model to complete the lighting. Building on these advances, we propose ComGS, a novel 3D object-scene composition framework. Our method achieves high-quality, real-time rendering at around 28 FPS, produces visually harmonious results with vivid shadows, and requires only 36 seconds for editing. Code and dataset are available at https://nju-3dv.github.io/projects/ComGS/.

</details>


### [17] [UltraLED: Learning to See Everything in Ultra-High Dynamic Range Scenes](https://arxiv.org/abs/2510.07741)
*Yuang Meng,Xin Jin,Lina Lei,Chun-Le Guo,Chongyi Li*

Main category: cs.CV

TL;DR: 提出UltraLED框架，仅使用单张短曝光RAW图像进行超高动态范围重建，通过曝光校正和亮度感知RAW去噪来恢复暗部细节，避免重影和运动模糊问题。


<details>
  <summary>Details</summary>
Motivation: 解决超高动态范围场景中亮暗区域曝光差异大、难以同时保留高光和阴影细节的问题，避免RGB包围曝光方法的重影和对齐问题。

Method: 两阶段框架：首先通过比率图进行曝光校正以平衡动态范围，然后使用亮度感知RAW去噪器增强暗部细节恢复。基于9档包围曝光合成真实UHDR图像构建数据集。

Result: 在广泛实验中，UltraLED显著优于现有的单帧方法，能够有效恢复暗部细节同时避免重影和运动模糊。

Conclusion: 仅使用单张短曝光RAW图像即可实现高质量的UHDR重建，该方法在动态场景中表现稳健，代码和数据集已公开。

Abstract: Ultra-high dynamic range (UHDR) scenes exhibit significant exposure disparities between bright and dark regions. Such conditions are commonly encountered in nighttime scenes with light sources. Even with standard exposure settings, a bimodal intensity distribution with boundary peaks often emerges, making it difficult to preserve both highlight and shadow details simultaneously. RGB-based bracketing methods can capture details at both ends using short-long exposure pairs, but are susceptible to misalignment and ghosting artifacts. We found that a short-exposure image already retains sufficient highlight detail. The main challenge of UHDR reconstruction lies in denoising and recovering information in dark regions. In comparison to the RGB images, RAW images, thanks to their higher bit depth and more predictable noise characteristics, offer greater potential for addressing this challenge. This raises a key question: can we learn to see everything in UHDR scenes using only a single short-exposure RAW image? In this study, we rely solely on a single short-exposure frame, which inherently avoids ghosting and motion blur, making it particularly robust in dynamic scenes. To achieve that, we introduce UltraLED, a two-stage framework that performs exposure correction via a ratio map to balance dynamic range, followed by a brightness-aware RAW denoiser to enhance detail recovery in dark regions. To support this setting, we design a 9-stop bracketing pipeline to synthesize realistic UHDR images and contribute a corresponding dataset based on diverse scenes, using only the shortest exposure as input for reconstruction. Extensive experiments show that UltraLED significantly outperforms existing single-frame approaches. Our code and dataset are made publicly available at https://srameo.github.io/projects/ultraled.

</details>


### [18] [DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event Stream](https://arxiv.org/abs/2510.07752)
*Junhao He,Jiaxu Wang,Jia Li,Mingyuan Sun,Qiang Zhang,Jiahang Cao,Ziyi Zhang,Yi Gu,Jingkai Sun,Renjing Xu*

Main category: cs.CV

TL;DR: 提出一种结合低帧率RGB视频和高帧率事件流来重建动态3D高斯泼溅的新框架，通过事件运动先验指导变形场优化，解决大帧间运动带来的不确定性挑战。


<details>
  <summary>Details</summary>
Motivation: 从低帧率RGB视频重建动态3D高斯泼溅具有挑战性，因为大帧间运动会增加解空间的不确定性。事件相机能异步捕捉快速视觉变化且对运动模糊鲁棒，但不提供颜色信息。结合两种模态可以解决这一挑战。

Method: 采用事件运动先验指导变形场优化：1）使用LoCM无监督微调框架从事件流提取运动先验；2）提出几何感知数据关联方法建立事件-高斯运动对应关系；3）配合运动分解和帧间伪标签策略。

Result: 在合成和真实场景的广泛实验中，该方法优于现有的基于图像和事件的方法，证明能有效利用事件数据优化动态3D高斯泼溅。

Conclusion: 提出的框架成功解决了联合优化RGB和事件模态动态3D高斯泼溅的挑战，通过事件运动先验显著提升了重建性能。

Abstract: Reconstructing Dynamic 3D Gaussian Splatting (3DGS) from low-framerate RGB videos is challenging. This is because large inter-frame motions will increase the uncertainty of the solution space. For example, one pixel in the first frame might have more choices to reach the corresponding pixel in the second frame. Event cameras can asynchronously capture rapid visual changes and are robust to motion blur, but they do not provide color information. Intuitively, the event stream can provide deterministic constraints for the inter-frame large motion by the event trajectories. Hence, combining low-temporal-resolution images with high-framerate event streams can address this challenge. However, it is challenging to jointly optimize Dynamic 3DGS using both RGB and event modalities due to the significant discrepancy between these two data modalities. This paper introduces a novel framework that jointly optimizes dynamic 3DGS from the two modalities. The key idea is to adopt event motion priors to guide the optimization of the deformation fields. First, we extract the motion priors encoded in event streams by using the proposed LoCM unsupervised fine-tuning framework to adapt an event flow estimator to a certain unseen scene. Then, we present the geometry-aware data association method to build the event-Gaussian motion correspondence, which is the primary foundation of the pipeline, accompanied by two useful strategies, namely motion decomposition and inter-frame pseudo-label. Extensive experiments show that our method outperforms existing image and event-based approaches across synthetic and real scenes and prove that our method can effectively optimize dynamic 3DGS with the help of event data.

</details>


### [19] [PrismGS: Physically-Grounded Anti-Aliasing for High-Fidelity Large-Scale 3D Gaussian Splatting](https://arxiv.org/abs/2510.07830)
*Houqiang Zhong,Zhenglong Wu,Sihua Fu,Zihan Zheng,Xin Jin,Xiaoyun Zhang,Li Song,Qiang Hu*

Main category: cs.CV

TL;DR: PrismGS是一个基于物理正则化的框架，通过金字塔多尺度监督和显式尺寸正则化来解决3D高斯溅射在大规模城市环境中出现的锯齿伪影和优化不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射在紧凑场景中能实现实时逼真渲染，但在大规模城市环境中会出现严重的锯齿伪影和优化不稳定问题，尤其是在4K高分辨率渲染下。现有方法虽然解决了可扩展性问题，但未能解决渲染质量差距。

Method: PrismGS集成了两个协同正则化器：1）金字塔多尺度监督，通过对预过滤图像金字塔进行监督来强制模型学习抗锯齿表示；2）显式尺寸正则化，对3D高斯的尺寸施加物理基础的下界约束，防止形成退化的视图相关基元。

Result: 在MatrixCity、Mill-19和UrbanScene3D数据集上的广泛实验表明，PrismGS实现了最先进的性能，相比CityGaussian获得了约1.5 dB的PSNR增益，同时在苛刻的4K渲染下保持了优越的质量和鲁棒性。

Conclusion: PrismGS是一个即插即用的框架，与现有管道兼容，通过物理基础的正则化显著提高了3D高斯溅射在大规模城市环境中的渲染质量和稳定性。

Abstract: 3D Gaussian Splatting (3DGS) has recently enabled real-time photorealistic rendering in compact scenes, but scaling to large urban environments introduces severe aliasing artifacts and optimization instability, especially under high-resolution (e.g., 4K) rendering. These artifacts, manifesting as flickering textures and jagged edges, arise from the mismatch between Gaussian primitives and the multi-scale nature of urban geometry. While existing ``divide-and-conquer'' pipelines address scalability, they fail to resolve this fidelity gap. In this paper, we propose PrismGS, a physically-grounded regularization framework that improves the intrinsic rendering behavior of 3D Gaussians. PrismGS integrates two synergistic regularizers. The first is pyramidal multi-scale supervision, which enforces consistency by supervising the rendering against a pre-filtered image pyramid. This compels the model to learn an inherently anti-aliased representation that remains coherent across different viewing scales, directly mitigating flickering textures. This is complemented by an explicit size regularization that imposes a physically-grounded lower bound on the dimensions of the 3D Gaussians. This prevents the formation of degenerate, view-dependent primitives, leading to more stable and plausible geometric surfaces and reducing jagged edges. Our method is plug-and-play and compatible with existing pipelines. Extensive experiments on MatrixCity, Mill-19, and UrbanScene3D demonstrate that PrismGS achieves state-of-the-art performance, yielding significant PSNR gains around 1.5 dB against CityGaussian, while maintaining its superior quality and robustness under demanding 4K rendering.

</details>


### [20] [TTOM: Test-Time Optimization and Memorization for Compositional Video Generation](https://arxiv.org/abs/2510.07940)
*Leigang Qu,Ziyang Wang,Na Zheng,Wenjie Wang,Liqiang Nie,Tat-Seng Chua*

Main category: cs.CV

TL;DR: TTOM是一个无需训练的框架，通过在推理时优化新参数并利用参数化记忆机制，提升视频基础模型在组合场景中的文本-图像对齐能力。


<details>
  <summary>Details</summary>
Motivation: 视频基础模型在视觉生成方面表现出色，但在组合场景（如运动、数字和空间关系）中表现不佳，需要改进跨模态对齐。

Method: 提出测试时优化和记忆框架，通过通用布局-注意力目标优化新参数，并采用参数化记忆机制维护历史优化上下文，支持插入、读取、更新和删除等操作。

Result: 在T2V-CompBench和Vbench基准测试中，TTOM表现出强大的可迁移性和泛化能力，有效提升了组合视频生成的跨模态对齐效果。

Conclusion: TTOM是一个有效、实用、可扩展且高效的框架，能够实时实现组合视频生成的跨模态对齐。

Abstract: Video Foundation Models (VFMs) exhibit remarkable visual generation performance, but struggle in compositional scenarios (e.g., motion, numeracy, and spatial relation). In this work, we introduce Test-Time Optimization and Memorization (TTOM), a training-free framework that aligns VFM outputs with spatiotemporal layouts during inference for better text-image alignment. Rather than direct intervention to latents or attention per-sample in existing work, we integrate and optimize new parameters guided by a general layout-attention objective. Furthermore, we formulate video generation within a streaming setting, and maintain historical optimization contexts with a parametric memory mechanism that supports flexible operations, such as insert, read, update, and delete. Notably, we found that TTOM disentangles compositional world knowledge, showing powerful transferability and generalization. Experimental results on the T2V-CompBench and Vbench benchmarks establish TTOM as an effective, practical, scalable, and efficient framework to achieve cross-modal alignment for compositional video generation on the fly.

</details>


### [21] [CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving](https://arxiv.org/abs/2510.07944)
*Tianrui Zhang,Yichen Liu,Zilin Guo,Yuxin Guo,Jingcheng Ni,Chenjing Ding,Dan Xu,Lewei Lu,Zehuan Wu*

Main category: cs.CV

TL;DR: CVD-STORM是一个基于空间-时间重建VAE的跨视角视频扩散模型，能够生成具有4D重建能力的多视角长视频，在各种控制输入下提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶的发展，不仅需要高保真度的视频生成，还需要产生多样且有意义的深度估计等信息。

Method: 首先通过辅助4D重建任务微调VAE，增强其编码3D结构和时间动态的能力，然后将该VAE集成到视频扩散过程中。

Result: 实验结果显示模型在FID和FVD指标上都有显著提升，联合训练的高斯泼溅解码器能有效重建动态场景。

Conclusion: 该方法能够生成高质量的多视角视频，并提供有价值的几何信息，实现全面的场景理解。

Abstract: Generative models have been widely applied to world modeling for environment simulation and future state prediction. With advancements in autonomous driving, there is a growing demand not only for high-fidelity video generation under various controls, but also for producing diverse and meaningful information such as depth estimation. To address this, we propose CVD-STORM, a cross-view video diffusion model utilizing a spatial-temporal reconstruction Variational Autoencoder (VAE) that generates long-term, multi-view videos with 4D reconstruction capabilities under various control inputs. Our approach first fine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its ability to encode 3D structures and temporal dynamics. Subsequently, we integrate this VAE into the video diffusion process to significantly improve generation quality. Experimental results demonstrate that our model achieves substantial improvements in both FID and FVD metrics. Additionally, the jointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic scenes, providing valuable geometric information for comprehensive scene understanding.

</details>


### [22] [Latent Harmony: Synergistic Unified UHD Image Restoration via Latent Space Regularization and Controllable Refinement](https://arxiv.org/abs/2510.07961)
*Yidi Liu,Xueyang Fu,Jie Huang,Jie Xiao,Dong Li,Wenlong Zhang,Lei Bai,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 提出了Latent Harmony框架，通过两阶段方法解决UHD图像修复中计算效率与高频细节保留的权衡问题。第一阶段增强VAE的语义鲁棒性和高频重建能力，第二阶段结合HF-LoRA技术恢复真实细节和合成逼真纹理。


<details>
  <summary>Details</summary>
Motivation: UHD图像修复面临计算效率与高频细节保留的权衡。传统VAE的高斯约束会丢弃退化特定的高频信息，影响重建保真度。

Method: 两阶段框架：第一阶段提出LH-VAE，通过视觉语义约束和渐进退化扰动增强语义鲁棒性，潜在等变性强化高频重建；第二阶段联合训练精炼VAE与修复模型，使用HF-LoRA技术（编码器LoRA由保真导向的高频对齐损失引导，解码器LoRA由感知导向损失驱动）。

Result: 在UHD和标准分辨率任务上达到最先进性能，有效平衡效率、感知质量和重建精度。

Conclusion: Latent Harmony框架成功解决了UHD图像修复中的关键挑战，通过重新定义VAE和HF-LoRA技术实现了计算效率与高频细节保留的良好平衡。

Abstract: Ultra-High Definition (UHD) image restoration faces a trade-off between computational efficiency and high-frequency detail retention. While Variational Autoencoders (VAEs) improve efficiency via latent-space processing, their Gaussian constraint often discards degradation-specific high-frequency information, hurting reconstruction fidelity. To overcome this, we propose Latent Harmony, a two-stage framework that redefines VAEs for UHD restoration by jointly regularizing the latent space and enforcing high-frequency-aware reconstruction.In Stage One, we introduce LH-VAE, which enhances semantic robustness through visual semantic constraints and progressive degradation perturbations, while latent equivariance strengthens high-frequency reconstruction.Stage Two jointly trains this refined VAE with a restoration model using High-Frequency Low-Rank Adaptation (HF-LoRA): an encoder LoRA guided by a fidelity-oriented high-frequency alignment loss to recover authentic details, and a decoder LoRA driven by a perception-oriented loss to synthesize realistic textures. Both LoRA modules are trained via alternating optimization with selective gradient propagation to preserve the pretrained latent structure.At inference, a tunable parameter {\alpha} enables flexible fidelity-perception trade-offs.Experiments show Latent Harmony achieves state-of-the-art performance across UHD and standard-resolution tasks, effectively balancing efficiency, perceptual quality, and reconstruction accuracy.

</details>


### [23] [Efficient Label Refinement for Face Parsing Under Extreme Poses Using 3D Gaussian Splatting](https://arxiv.org/abs/2510.08096)
*Ankit Gahlawat,Anirban Mukherjee,Dinesh Babu Jayagopi*

Main category: cs.CV

TL;DR: 提出了一种利用3D高斯溅射(3DGS)从噪声多视图预测中生成精确分割掩码的标签细化管道，通过联合拟合RGB图像和初始分割图的3DGS模型，实现多视图一致性，显著提升极端视角下的人脸解析精度。


<details>
  <summary>Details</summary>
Motivation: 极端视角下的人脸解析由于缺乏标注数据而面临挑战，手动标注成本高且难以规模化。

Method: 联合拟合两个3DGS模型（一个用于RGB图像，一个用于初始分割图），通过共享几何结构强制多视图一致性，合成姿态多样的训练数据。

Result: 在具有挑战性的头部姿态上显著提高了人脸解析精度，同时在标准视图上保持强性能，无需真实3D标注且仅使用少量初始图像。

Conclusion: 该方法为在真实场景中提高人脸解析鲁棒性提供了可扩展且有效的解决方案。

Abstract: Accurate face parsing under extreme viewing angles remains a significant challenge due to limited labeled data in such poses. Manual annotation is costly and often impractical at scale. We propose a novel label refinement pipeline that leverages 3D Gaussian Splatting (3DGS) to generate accurate segmentation masks from noisy multiview predictions. By jointly fitting two 3DGS models, one to RGB images and one to their initial segmentation maps, our method enforces multiview consistency through shared geometry, enabling the synthesis of pose-diverse training data with only minimal post-processing. Fine-tuning a face parsing model on this refined dataset significantly improves accuracy on challenging head poses, while maintaining strong performance on standard views. Extensive experiments, including human evaluations, demonstrate that our approach achieves superior results compared to state-of-the-art methods, despite requiring no ground-truth 3D annotations and using only a small set of initial images. Our method offers a scalable and effective solution for improving face parsing robustness in real-world settings.

</details>


### [24] [Real-Time Motion-Controllable Autoregressive Video Diffusion](https://arxiv.org/abs/2510.08131)
*Kesen Zhao,Jiaxin Shi,Beier Zhu,Junbao Zhou,Xiaolong Shen,Yuan Zhou,Qianru Sun,Hanwang Zhang*

Main category: cs.CV

TL;DR: AR-Drag是首个RL增强的少步自回归视频扩散模型，用于实时图像到视频生成，支持多样化运动控制，显著降低了延迟并提高了生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决双向扩散模型的固有延迟问题，以及现有自回归视频扩散模型在少步生成中的质量下降和运动伪影问题。

Method: 首先微调基础I2V模型支持基本运动控制，然后通过基于轨迹的奖励模型进行强化学习改进，通过自展开机制保持马尔可夫性，并在去噪步骤中选择性引入随机性来加速训练。

Result: AR-Drag实现了高视觉保真度和精确的运动对齐，与最先进的运动可控VDMs相比显著减少了延迟，仅使用13亿参数。

Conclusion: 该方法在实时运动可控视频生成方面取得了显著进展，平衡了生成质量和计算效率。

Abstract: Real-time motion-controllable video generation remains challenging due to the inherent latency of bidirectional diffusion models and the lack of effective autoregressive (AR) approaches. Existing AR video diffusion models are limited to simple control signals or text-to-video generation, and often suffer from quality degradation and motion artifacts in few-step generation. To address these challenges, we propose AR-Drag, the first RL-enhanced few-step AR video diffusion model for real-time image-to-video generation with diverse motion control. We first fine-tune a base I2V model to support basic motion control, then further improve it via reinforcement learning with a trajectory-based reward model. Our design preserves the Markov property through a Self-Rollout mechanism and accelerates training by selectively introducing stochasticity in denoising steps. Extensive experiments demonstrate that AR-Drag achieves high visual fidelity and precise motion alignment, significantly reducing latency compared with state-of-the-art motion-controllable VDMs, while using only 1.3B parameters. Additional visualizations can be found on our project page: https://kesenzhao.github.io/AR-Drag.github.io/.

</details>


### [25] [UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video Super-Resolution](https://arxiv.org/abs/2510.08143)
*Shian Du,Menghan Xia,Chang Liu,Quande Liu,Xintao Wang,Pengfei Wan,Xiangyang Ji*

Main category: cs.CV

TL;DR: UniMMVSR是首个统一的多模态视频超分辨率框架，能够整合文本、图像和视频等多种生成条件，显著提升视频细节和条件一致性，并能生成4K视频。


<details>
  <summary>Details</summary>
Motivation: 现有级联视频超分辨率方法主要局限于文本到视频任务，未能充分利用文本之外的多模态生成条件，而这些条件对于确保多模态视频生成的保真度至关重要。

Method: 在潜在视频扩散模型中，通过探索条件注入策略、训练方案和数据混合技术，设计了不同的数据构建和条件利用方法，使模型能够精确利用所有条件类型。

Result: UniMMVSR显著优于现有方法，生成的视频具有更优的细节和更高的多模态条件一致性，并验证了与基础模型结合实现4K视频多模态引导生成的可行性。

Conclusion: UniMMVSR成功解决了多模态条件利用的挑战，为高质量视频生成提供了统一框架，突破了现有技术的限制。

Abstract: Cascaded video super-resolution has emerged as a promising technique for decoupling the computational burden associated with generating high-resolution videos using large foundation models. Existing studies, however, are largely confined to text-to-video tasks and fail to leverage additional generative conditions beyond text, which are crucial for ensuring fidelity in multi-modal video generation. We address this limitation by presenting UniMMVSR, the first unified generative video super-resolution framework to incorporate hybrid-modal conditions, including text, images, and videos. We conduct a comprehensive exploration of condition injection strategies, training schemes, and data mixture techniques within a latent video diffusion model. A key challenge was designing distinct data construction and condition utilization methods to enable the model to precisely utilize all condition types, given their varied correlations with the target video. Our experiments demonstrate that UniMMVSR significantly outperforms existing methods, producing videos with superior detail and a higher degree of conformity to multi-modal conditions. We also validate the feasibility of combining UniMMVSR with a base model to achieve multi-modal guided generation of 4K video, a feat previously unattainable with existing techniques.

</details>


### [26] [One Stone with Two Birds: A Null-Text-Null Frequency-Aware Diffusion Models for Text-Guided Image Inpainting](https://arxiv.org/abs/2510.08273)
*Haipeng Liu,Yang Wang,Meng Wang*

Main category: cs.CV

TL;DR: 提出NTN-Diff方法，通过频率感知的扩散模型解决文本引导图像修复中的两个关键挑战：保持未掩码区域和实现掩码与未掩码区域的语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法同时解决文本引导图像修复中的两个关键问题：保持未掩码区域和实现掩码与未掩码区域的语义一致性。这源于混合频率波段（如中低频）在去噪过程中对文本提示的不同鲁棒性。

Method: 提出空文本-空频率感知扩散模型（NTN-Diff），将去噪过程分为早期（高级噪声）和晚期（低级噪声）阶段，在去噪过程中解耦中低频波段。通过渐进式去噪实现中频波段的语义对齐，并指导低频波段的空文本去噪。

Result: 大量实验验证了NTN-Diff在文本引导扩散模型中的优越性，能够有效保持未掩码区域同时实现跨区域的语义一致性。

Conclusion: NTN-Diff通过频率感知的方法成功解决了文本引导图像修复中的关键挑战，在保持未掩码区域的同时实现了掩码与未掩码区域的语义一致性。

Abstract: Text-guided image inpainting aims at reconstructing the masked regions as per text prompts, where the longstanding challenges lie in the preservation for unmasked regions, while achieving the semantics consistency between unmasked and inpainted masked regions. Previous arts failed to address both of them, always with either of them to be remedied. Such facts, as we observed, stem from the entanglement of the hybrid (e.g., mid-and-low) frequency bands that encode varied image properties, which exhibit different robustness to text prompts during the denoising process. In this paper, we propose a null-text-null frequency-aware diffusion models, dubbed \textbf{NTN-Diff}, for text-guided image inpainting, by decomposing the semantics consistency across masked and unmasked regions into the consistencies as per each frequency band, while preserving the unmasked regions, to circumvent two challenges in a row. Based on the diffusion process, we further divide the denoising process into early (high-level noise) and late (low-level noise) stages, where the mid-and-low frequency bands are disentangled during the denoising process. As observed, the stable mid-frequency band is progressively denoised to be semantically aligned during text-guided denoising process, which, meanwhile, serves as the guidance to the null-text denoising process to denoise low-frequency band for the masked regions, followed by a subsequent text-guided denoising process at late stage, to achieve the semantics consistency for mid-and-low frequency bands across masked and unmasked regions, while preserve the unmasked regions. Extensive experiments validate the superiority of NTN-Diff over the state-of-the-art diffusion models to text-guided diffusion models. Our code can be accessed from https://github.com/htyjers/NTN-Diff.

</details>


### [27] [LinVideo: A Post-Training Framework towards O(n) Attention in Efficient Video Generation](https://arxiv.org/abs/2510.08318)
*Yushi Huang,Xingtong Ge,Ruihao Gong,Chengtao Lv,Jun Zhang*

Main category: cs.CV

TL;DR: LinVideo是一个高效的无数据后训练框架，通过将自注意力模块替换为线性注意力来加速视频扩散模型，同时保持原始模型性能。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型的计算成本随序列长度呈二次方增长，而线性注意力虽然能降低成本，但由于其表达能力有限和视频生成中时空建模的复杂性，完全替换二次注意力需要昂贵的预训练。

Method: 提出选择性迁移方法，将层选择视为二分类问题，自动渐进地将层转换为线性注意力；引入任意时间分布匹配（ADM）目标，在采样轨迹的任何时间步对齐样本分布。

Result: 实验显示该方法实现了1.25-2.00倍加速，同时保持生成质量；4步蒸馏模型进一步实现15.92倍延迟降低，视觉质量下降最小。

Conclusion: LinVideo框架成功解决了视频扩散模型计算效率问题，通过智能层替换和高效训练目标实现了显著加速而不牺牲性能。

Abstract: Video diffusion models (DMs) have enabled high-quality video synthesis. However, their computation costs scale quadratically with sequence length because self-attention has quadratic complexity. While linear attention lowers the cost, fully replacing quadratic attention requires expensive pretraining due to the limited expressiveness of linear attention and the complexity of spatiotemporal modeling in video generation. In this paper, we present LinVideo, an efficient data-free post-training framework that replaces a target number of self-attention modules with linear attention while preserving the original model's performance. First, we observe a significant disparity in the replaceability of different layers. Instead of manual or heuristic choices, we frame layer selection as a binary classification problem and propose selective transfer, which automatically and progressively converts layers to linear attention with minimal performance impact. Additionally, to overcome the ineffectiveness and inefficiency of existing objectives for this transfer process, we introduce an anytime distribution matching (ADM) objective that aligns the distributions of samples across any timestep along the sampling trajectory. This objective is efficient and recovers model performance. Extensive experiments show that our method achieves a 1.25-2.00x speedup while preserving generation quality, and our 4-step distilled model further delivers a 15.92x latency reduction with minimal visual quality drop.

</details>


### [28] [SPICE: Simple and Practical Image Clarification and Enhancement](https://arxiv.org/abs/2510.08358)
*Alexander Belyaev,Pierre-Alain Fayolle,Michael Cohen*

Main category: cs.CV

TL;DR: 提出一种简单高效的图像增强方法，专门处理低光照图像增强和雾霾图像（雾天、沙尘、水下图像）的清晰化。通过构建图像滤波器模拟低光或雾霾条件，并推导近似反向滤波器来最小化增强图像中的失真。


<details>
  <summary>Details</summary>
Motivation: 针对低光照和雾霾条件下图像质量下降的问题，需要一种简单而有效的增强方法，能够在处理极暗图像和雾霾图像时超越现有技术，同时保持实现的简洁性。

Method: 构建图像滤波器模拟低光或雾霾条件，然后推导近似反向滤波器来最小化增强图像中的失真。该方法仅需几行MATLAB代码即可实现。

Result: 实验结果表明，该方法在处理极暗图像和增强雾霾图像方面具有高度竞争力，通常超越最先进的技术。

Conclusion: 该方法提供了一种简单高效的图像增强解决方案，在处理低光照和雾霾图像时表现出色，且实现简单，仅需少量代码。

Abstract: We introduce a simple and efficient method to enhance and clarify images. More specifically, we deal with low light image enhancement and clarification of hazy imagery (hazy/foggy images, images containing sand dust, and underwater images). Our method involves constructing an image filter to simulate low-light or hazy conditions and deriving approximate reverse filters to minimize distortions in the enhanced images. Experimental results show that our approach is highly competitive and often surpasses state-of-the-art techniques in handling extremely dark images and in enhancing hazy images. A key advantage of our approach lies in its simplicity: Our method is implementable with just a few lines of MATLAB code.

</details>


### [29] [Hyperspectral data augmentation with transformer-based diffusion models](https://arxiv.org/abs/2510.08363)
*Mattia Ferrari,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 提出一种基于引导扩散模型的数据增强技术，结合轻量级Transformer网络、改进的加权损失函数和优化的余弦方差调度器，用于小样本高光谱图像森林分类任务。


<details>
  <summary>Details</summary>
Motivation: 深度学习在中小规模地物分类中面临小样本过拟合问题，需要有效的数据增强方法来提升模型性能。

Method: 使用引导扩散模型进行数据增强，结合轻量级Transformer网络、改进的加权损失函数和优化的余弦方差调度器，在PRISMA卫星高光谱图像上进行10类森林分类。

Result: 该方法在平均精度和加权平均精度上均优于其他数据增强技术，且模型训练稳定，解决了深度生成模型在实际应用中的常见限制。

Conclusion: 提出的数据增强方法能有效缓解小样本过拟合问题，提升高光谱图像分类性能，具有实际应用价值。

Abstract: The introduction of new generation hyperspectral satellite sensors, combined with advancements in deep learning methodologies, has significantly enhanced the ability to discriminate detailed land-cover classes at medium-large scales. However, a significant challenge in deep learning methods is the risk of overfitting when training networks with small labeled datasets. In this work, we propose a data augmentation technique that leverages a guided diffusion model. To effectively train the model with a limited number of labeled samples and to capture complex patterns in the data, we implement a lightweight transformer network. Additionally, we introduce a modified weighted loss function and an optimized cosine variance scheduler, which facilitate fast and effective training on small datasets. We evaluate the effectiveness of the proposed method on a forest classification task with 10 different forest types using hyperspectral images acquired by the PRISMA satellite. The results demonstrate that the proposed method outperforms other data augmentation techniques in both average and weighted average accuracy. The effectiveness of the method is further highlighted by the stable training behavior of the model, which addresses a common limitation in the practical application of deep generative models for data augmentation.

</details>


### [30] [UniVideo: Unified Understanding, Generation, and Editing for Videos](https://arxiv.org/abs/2510.08377)
*Cong Wei,Quande Liu,Zixuan Ye,Qiulin Wang,Xintao Wang,Pengfei Wan,Kun Gai,Wenhu Chen*

Main category: cs.CV

TL;DR: UniVideo是一个统一的多模态视频生成和编辑框架，采用双流设计结合MLLM和MMDiT，支持多种视频任务并实现任务组合和跨任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的统一多模态模型主要局限于图像领域，需要扩展到视频领域以实现复杂的多模态指令理解和视觉一致性保持。

Method: 采用双流架构：多模态大语言模型(MLLM)负责指令理解，多模态DiT(MMDiT)负责视频生成，通过联合训练统一多种视频任务。

Result: 在文本/图像到视频生成、上下文视频生成和编辑等任务上达到或超越最先进的特定任务基线，支持任务组合和未见指令的泛化。

Conclusion: UniVideo成功将统一建模扩展到视频领域，实现了强大的多任务能力和泛化性能，为未来研究提供了有价值的框架。

Abstract: Unified multimodal models have shown promising results in multimodal content generation and editing but remain largely limited to the image domain. In this work, we present UniVideo, a versatile framework that extends unified modeling to the video domain. UniVideo adopts a dual-stream design, combining a Multimodal Large Language Model (MLLM) for instruction understanding with a Multimodal DiT (MMDiT) for video generation. This design enables accurate interpretation of complex multimodal instructions while preserving visual consistency. Built on this architecture, UniVideo unifies diverse video generation and editing tasks under a single multimodal instruction paradigm and is jointly trained across them. Extensive experiments demonstrate that UniVideo matches or surpasses state-of-the-art task-specific baselines in text/image-to-video generation, in-context video generation and in-context video editing. Notably, the unified design of UniVideo enables two forms of generalization. First, UniVideo supports task composition, such as combining editing with style transfer, by integrating multiple capabilities within a single instruction. Second, even without explicit training on free-form video editing, UniVideo transfers its editing capability from large-scale image editing data to this setting, handling unseen instructions such as green-screening characters or changing materials within a video. Beyond these core capabilities, UniVideo also supports visual-prompt-based video generation, where the MLLM interprets visual prompts and guides the MMDiT during synthesis. To foster future research, we will release our model and code.

</details>


### [31] [VideoVerse: How Far is Your T2V Generator from a World Model?](https://arxiv.org/abs/2510.08398)
*Zeqing Wang,Xinyu Wei,Bairui Li,Zhen Guo,Jinrui Zhang,Hongyang Wei,Keze Wang,Lei Zhang*

Main category: cs.CV

TL;DR: VideoVerse是一个新的文本到视频生成基准测试，专注于评估模型对复杂时间因果关系和世界知识的理解能力，填补了现有基准在事件级时间因果性和系统化世界知识评估方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成基准无法充分评估最先进模型，主要问题包括：当前评估维度无法区分顶级模型、事件级时间因果关系研究不足、缺乏对世界知识的系统评估。

Method: 收集跨领域代表性视频，提取具有时间因果关系的事件级描述，由独立标注者改写为文本到视频提示，设计包含10个维度的二元评估问题，开发基于视觉语言模型的人类偏好对齐评估流程。

Result: 构建了包含300个精心策划提示、815个事件和793个评估问题的VideoVerse基准，并对开源和闭源T2V模型进行了系统评估。

Conclusion: VideoVerse为评估T2V模型构建世界模型的能力提供了全面基准，揭示了当前T2V生成器与世界模型之间的差距。

Abstract: The recent rapid advancement of Text-to-Video (T2V) generation technologies, which are critical to build ``world models'', makes the existing benchmarks increasingly insufficient to evaluate state-of-the-art T2V models. First, current evaluation dimensions, such as per-frame aesthetic quality and temporal consistency, are no longer able to differentiate state-of-the-art T2V models. Second, event-level temporal causality, which not only distinguishes video from other modalities but also constitutes a crucial component of world models, is severely underexplored in existing benchmarks. Third, existing benchmarks lack a systematic assessment of world knowledge, which are essential capabilities for building world models. To address these issues, we introduce VideoVerse, a comprehensive benchmark that focuses on evaluating whether a T2V model could understand complex temporal causality and world knowledge in the real world. We collect representative videos across diverse domains (e.g., natural landscapes, sports, indoor scenes, science fiction, chemical and physical experiments) and extract their event-level descriptions with inherent temporal causality, which are then rewritten into text-to-video prompts by independent annotators. For each prompt, we design a suite of binary evaluation questions from the perspective of dynamic and static properties, with a total of ten carefully defined evaluation dimensions. In total, our VideoVerse comprises 300 carefully curated prompts, involving 815 events and 793 binary evaluation questions. Consequently, a human preference aligned QA-based evaluation pipeline is developed by using modern vision-language models. Finally, we perform a systematic evaluation of state-of-the-art open-source and closed-source T2V models on VideoVerse, providing in-depth analysis on how far the current T2V generators are from world models.

</details>


### [32] [Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency](https://arxiv.org/abs/2510.08431)
*Kaiwen Zheng,Yuji Wang,Qianli Ma,Huayu Chen,Jintao Zhang,Yogesh Balaji,Jianfei Chen,Ming-Yu Liu,Jun Zhu,Qinsheng Zhang*

Main category: cs.CV

TL;DR: 本文首次将连续时间一致性蒸馏扩展到通用应用级图像和视频扩散模型，提出了分数正则化连续时间一致性模型(rCM)，在保持高多样性的同时显著提升视觉质量，实现15-50倍加速。


<details>
  <summary>Details</summary>
Motivation: 虽然连续时间一致性模型(sCM)在学术规模扩散加速方面表现优异，但其在大规模文本到图像和视频任务中的适用性仍不明确，主要面临Jacobian-向量积计算的基础设施挑战和标准评估基准的局限性。

Method: 开发并行兼容的FlashAttention-2 JVP核，支持10B+参数模型训练；提出rCM模型，将分数蒸馏作为长跳跃正则化器，结合sCM的前向散度和反向散度的模式寻求特性。

Result: 在14B参数模型和5秒视频任务上验证，rCM在质量指标上匹配或超越最先进蒸馏方法DMD2，同时保持多样性优势，仅需1-4步生成高保真样本，实现15-50倍加速。

Conclusion: rCM为推进大规模扩散蒸馏提供了一个实用且理论基础的框架，无需GAN调优或大量超参数搜索。

Abstract: This work represents the first effort to scale up continuous-time consistency distillation to general application-level image and video diffusion models. Although continuous-time consistency model (sCM) is theoretically principled and empirically powerful for accelerating academic-scale diffusion, its applicability to large-scale text-to-image and video tasks remains unclear due to infrastructure challenges in Jacobian-vector product (JVP) computation and the limitations of standard evaluation benchmarks. We first develop a parallelism-compatible FlashAttention-2 JVP kernel, enabling sCM training on models with over 10 billion parameters and high-dimensional video tasks. Our investigation reveals fundamental quality limitations of sCM in fine-detail generation, which we attribute to error accumulation and the "mode-covering" nature of its forward-divergence objective. To remedy this, we propose the score-regularized continuous-time consistency model (rCM), which incorporates score distillation as a long-skip regularizer. This integration complements sCM with the "mode-seeking" reverse divergence, effectively improving visual quality while maintaining high generation diversity. Validated on large-scale models (Cosmos-Predict2, Wan2.1) up to 14B parameters and 5-second videos, rCM matches or surpasses the state-of-the-art distillation method DMD2 on quality metrics while offering notable advantages in diversity, all without GAN tuning or extensive hyperparameter searches. The distilled models generate high-fidelity samples in only $1\sim4$ steps, accelerating diffusion sampling by $15\times\sim50\times$. These results position rCM as a practical and theoretically grounded framework for advancing large-scale diffusion distillation.

</details>


### [33] [Hierarchical Spatial Algorithms for High-Resolution Image Quantization and Feature Extraction](https://arxiv.org/abs/2510.08449)
*Noor Islam S. Mohammad*

Main category: cs.CV

TL;DR: 提出了一个模块化空间图像处理框架，包含灰度量化、色彩增强、图像锐化、双向变换流水线和几何特征提取，在多个数据集上表现出稳健性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个综合性的图像处理框架，能够处理从基础量化到高级特征提取的多种任务，满足实时图像分析和计算机视觉应用的需求。

Method: 采用模块化设计，包括八级灰度量化、RGB/YCrCb色彩空间直方图均衡化、HSV亮度调整、3×3卷积核锐化、双向变换流水线（反锐化掩模、伽马校正、噪声放大）以及Canny边缘检测、Hough线估计、Harris角点检测等几何特征提取方法。

Result: 双向变换流水线正反向准确率分别达到76.10%和74.80%，球杆对齐角度估计为51.50度，球杆隔离与真实图像相似度达81.87%。

Conclusion: 该框架在多样化数据集上表现出稳健和确定性的性能，显示出在实时图像分析和计算机视觉应用中的潜力。

Abstract: This study introduces a modular framework for spatial image processing, integrating grayscale quantization, color and brightness enhancement, image sharpening, bidirectional transformation pipelines, and geometric feature extraction. A stepwise intensity transformation quantizes grayscale images into eight discrete levels, producing a posterization effect that simplifies representation while preserving structural detail. Color enhancement is achieved via histogram equalization in both RGB and YCrCb color spaces, with the latter improving contrast while maintaining chrominance fidelity. Brightness adjustment is implemented through HSV value-channel manipulation, and image sharpening is performed using a 3 * 3 convolution kernel to enhance high-frequency details. A bidirectional transformation pipeline that integrates unsharp masking, gamma correction, and noise amplification achieved accuracy levels of 76.10% and 74.80% for the forward and reverse processes, respectively. Geometric feature extraction employed Canny edge detection, Hough-based line estimation (e.g., 51.50{\deg} for billiard cue alignment), Harris corner detection, and morphological window localization. Cue isolation further yielded 81.87\% similarity against ground truth images. Experimental evaluation across diverse datasets demonstrates robust and deterministic performance, highlighting its potential for real-time image analysis and computer vision.

</details>


### [34] [MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration](https://arxiv.org/abs/2510.08508)
*Lu Liu,Chunlei Cai,Shaocheng Shen,Jianfeng Liang,Weimin Ouyang,Tianxiao Ye,Jian Mao,Huiyu Duan,Jiangchao Yao,Xiaoyun Zhang,Qiang Hu,Guangtao Zhai*

Main category: cs.CV

TL;DR: MoA-VR是一个基于多智能体协作的视频修复系统，通过三个协调的智能体（退化识别、路由修复、质量评估）来模拟人类专家的修复流程，有效处理复杂的复合退化问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界视频常因采集和传输条件不同而遭受噪声、压缩伪影、低光失真等多种复杂退化。现有方法需要人工选择专用模型或使用单一架构，难以泛化处理各种退化类型。

Method: 提出三智能体协作系统：1）基于视觉语言模型的退化识别器；2）大语言模型驱动的自适应路由器，学习修复策略；3）专门为修复任务设计的VLM视频质量评估模型。

Result: 大量实验表明，MoA-VR能有效处理多样化和复合退化，在客观指标和感知质量上均优于现有基线方法。

Conclusion: 该研究展示了多模态智能和模块化推理在通用视频修复系统中的潜力，为处理复杂视频退化问题提供了新思路。

Abstract: Real-world videos often suffer from complex degradations, such as noise, compression artifacts, and low-light distortions, due to diverse acquisition and transmission conditions. Existing restoration methods typically require professional manual selection of specialized models or rely on monolithic architectures that fail to generalize across varying degradations. Inspired by expert experience, we propose MoA-VR, the first \underline{M}ixture-\underline{o}f-\underline{A}gents \underline{V}ideo \underline{R}estoration system that mimics the reasoning and processing procedures of human professionals through three coordinated agents: Degradation Identification, Routing and Restoration, and Restoration Quality Assessment. Specifically, we construct a large-scale and high-resolution video degradation recognition benchmark and build a vision-language model (VLM) driven degradation identifier. We further introduce a self-adaptive router powered by large language models (LLMs), which autonomously learns effective restoration strategies by observing tool usage patterns. To assess intermediate and final processed video quality, we construct the \underline{Res}tored \underline{V}ideo \underline{Q}uality (Res-VQ) dataset and design a dedicated VLM-based video quality assessment (VQA) model tailored for restoration tasks. Extensive experiments demonstrate that MoA-VR effectively handles diverse and compound degradations, consistently outperforming existing baselines in terms of both objective metrics and perceptual quality. These results highlight the potential of integrating multimodal intelligence and modular reasoning in general-purpose video restoration systems.

</details>


### [35] [FlexTraj: Image-to-Video Generation with Flexible Point Trajectory Control](https://arxiv.org/abs/2510.08527)
*Zhiyuan Zhang,Can Wang,Dongdong Chen,Jing Liao*

Main category: cs.CV

TL;DR: FlexTraj是一个用于图像到视频生成的框架，通过灵活的点轨迹控制实现多粒度运动控制，支持密集和稀疏轨迹，采用序列拼接方案实现高效训练和推理。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成方法在精确运动控制方面存在局限，需要开发能够灵活控制点轨迹的统一框架，支持各种应用场景。

Method: 引入基于点的统一运动表示，使用分割ID、时间一致轨迹ID和可选颜色通道编码每个点；采用高效的序列拼接方案而非token拼接或ControlNet；使用退火训练策略逐步减少对完整监督和对齐条件的依赖。

Result: 实验结果表明FlexTraj能够实现多粒度、对齐无关的轨迹控制，支持运动克隆、拖拽式图像到视频、运动插值、相机重定向、灵活动作控制和网格动画等多种应用。

Conclusion: FlexTraj框架通过统一的点轨迹表示和高效的训练策略，实现了灵活可控的视频生成，在收敛速度、可控性和推理效率方面表现出色。

Abstract: We present FlexTraj, a framework for image-to-video generation with flexible point trajectory control. FlexTraj introduces a unified point-based motion representation that encodes each point with a segmentation ID, a temporally consistent trajectory ID, and an optional color channel for appearance cues, enabling both dense and sparse trajectory control. Instead of injecting trajectory conditions into the video generator through token concatenation or ControlNet, FlexTraj employs an efficient sequence-concatenation scheme that achieves faster convergence, stronger controllability, and more efficient inference, while maintaining robustness under unaligned conditions. To train such a unified point trajectory-controlled video generator, FlexTraj adopts an annealing training strategy that gradually reduces reliance on complete supervision and aligned condition. Experimental results demonstrate that FlexTraj enables multi-granularity, alignment-agnostic trajectory control for video generation, supporting various applications such as motion cloning, drag-based image-to-video, motion interpolation, camera redirection, flexible action control and mesh animations.

</details>


### [36] [VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning](https://arxiv.org/abs/2510.08555)
*Minghong Cai,Qiulin Wang,Zongli Ye,Wenze Liu,Quande Liu,Weicai Ye,Xintao Wang,Pengfei Wan,Kun Gai,Xiangyu Yue*

Main category: cs.CV

TL;DR: 提出了VideoCanvas框架，通过零参数修改实现任意时空视频补全，统一了图像到视频、修复、扩展和插值等任务，解决了因果VAE带来的时间模糊问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型由于因果VAE的压缩机制导致时间模糊，难以实现精确的帧级控制。需要一种能够统一多种可控视频生成任务的灵活范式。

Method: 采用混合条件策略：空间控制通过零填充处理，时间对齐通过时间RoPE插值实现，为每个条件分配连续分数位置，无需新增参数。

Result: 在VideoCanvasBench基准测试中显著优于现有条件范式，在帧内保真度和帧间创造性方面都建立了新的最先进水平。

Conclusion: VideoCanvas成功解决了因果VAE的时间模糊问题，实现了灵活统一的视频生成，为任意时空视频补全任务提供了有效解决方案。

Abstract: We introduce the task of arbitrary spatio-temporal video completion, where a video is generated from arbitrary, user-specified patches placed at any spatial location and timestamp, akin to painting on a video canvas. This flexible formulation naturally unifies many existing controllable video generation tasks--including first-frame image-to-video, inpainting, extension, and interpolation--under a single, cohesive paradigm. Realizing this vision, however, faces a fundamental obstacle in modern latent video diffusion models: the temporal ambiguity introduced by causal VAEs, where multiple pixel frames are compressed into a single latent representation, making precise frame-level conditioning structurally difficult. We address this challenge with VideoCanvas, a novel framework that adapts the In-Context Conditioning (ICC) paradigm to this fine-grained control task with zero new parameters. We propose a hybrid conditioning strategy that decouples spatial and temporal control: spatial placement is handled via zero-padding, while temporal alignment is achieved through Temporal RoPE Interpolation, which assigns each condition a continuous fractional position within the latent sequence. This resolves the VAE's temporal ambiguity and enables pixel-frame-aware control on a frozen backbone. To evaluate this new capability, we develop VideoCanvasBench, the first benchmark for arbitrary spatio-temporal video completion, covering both intra-scene fidelity and inter-scene creativity. Experiments demonstrate that VideoCanvas significantly outperforms existing conditioning paradigms, establishing a new state of the art in flexible and unified video generation.

</details>


### [37] [MultiCOIN: Multi-Modal COntrollable Video INbetweening](https://arxiv.org/abs/2510.08561)
*Maham Tanveer,Yang Zhou,Simon Niklaus,Ali Mahdavi Amiri,Hao Zhang,Krishna Kumar Singh,Nanxuan Zhao*

Main category: cs.CV

TL;DR: 提出了一种名为\modelname{}的视频插帧框架，支持多模态控制（深度过渡、运动轨迹、文本提示等），采用DiT架构和分阶段训练策略，实现灵活、精确的视频插值。


<details>
  <summary>Details</summary>
Motivation: 现有视频插帧方法无法生成大型、复杂或精细的运动，缺乏用户意图的多样性和对中间帧细节的精细控制，难以满足创意需求。

Method: 采用Diffusion Transformer (DiT)架构，将运动控制映射为稀疏点表示，分离内容和运动控制为两个分支，使用分阶段训练策略。

Result: 定性和定量实验表明，多模态控制能够实现更动态、可定制和上下文准确的视觉叙事。

Conclusion: \modelname{}框架通过多模态控制填补了现有视频插帧方法的不足，在灵活性、易用性和精度之间取得了平衡。

Abstract: Video inbetweening creates smooth and natural transitions between two image frames, making it an indispensable tool for video editing and long-form video synthesis. Existing works in this domain are unable to generate large, complex, or intricate motions. In particular, they cannot accommodate the versatility of user intents and generally lack fine control over the details of intermediate frames, leading to misalignment with the creative mind. To fill these gaps, we introduce \modelname{}, a video inbetweening framework that allows multi-modal controls, including depth transition and layering, motion trajectories, text prompts, and target regions for movement localization, while achieving a balance between flexibility, ease of use, and precision for fine-grained video interpolation. To achieve this, we adopt the Diffusion Transformer (DiT) architecture as our video generative model, due to its proven capability to generate high-quality long videos. To ensure compatibility between DiT and our multi-modal controls, we map all motion controls into a common sparse and user-friendly point-based representation as the video/noise input. Further, to respect the variety of controls which operate at varying levels of granularity and influence, we separate content controls and motion controls into two branches to encode the required features before guiding the denoising process, resulting in two generators, one for motion and the other for content. Finally, we propose a stage-wise training strategy to ensure that our model learns the multi-modal controls smoothly. Extensive qualitative and quantitative experiments demonstrate that multi-modal controls enable a more dynamic, customizable, and contextually accurate visual narrative.

</details>


### [38] [ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous Driving](https://arxiv.org/abs/2510.08562)
*Zhiyu Zheng,Shaoyu Chen,Haoran Yin,Xinbang Zhang,Jialv Zou,Xinggang Wang,Qian Zhang,Lefei Zhang*

Main category: cs.CV

TL;DR: 提出ResAD框架，通过归一化残差轨迹建模解决端到端自动驾驶中的时空不平衡问题，将轨迹预测任务重构为预测相对于确定性惯性参考的残差偏差。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶系统面临轨迹数据的固有时空不平衡问题，这导致模型学习虚假相关性而非因果推理，并优先考虑不确定的远距离预测，从而危及即时安全。

Method: ResAD框架将学习任务重构为预测相对于确定性惯性参考的残差偏差，并使用点归一化处理优化不平衡问题，防止大误差主导学习信号。

Result: 在NAVSIM基准测试中，ResAD使用仅两步去噪的普通扩散策略实现了88.6的最先进PDMS分数，显著简化了学习任务并提升了模型性能。

Conclusion: ResAD通过残差轨迹建模和点归一化有效解决了端到端自动驾驶中的优化不平衡问题，提高了模型性能和安全性。

Abstract: End-to-end autonomous driving (E2EAD) systems, which learn to predict future trajectories directly from sensor data, are fundamentally challenged by the inherent spatio-temporal imbalance of trajectory data. This imbalance creates a significant optimization burden, causing models to learn spurious correlations instead of causal inference, while also prioritizing uncertain, distant predictions, thereby compromising immediate safety. To address these issues, we propose ResAD, a novel Normalized Residual Trajectory Modeling framework. Instead of predicting the future trajectory directly, our approach reframes the learning task to predict the residual deviation from a deterministic inertial reference. The inertial reference serves as a counterfactual, forcing the model to move beyond simple pattern recognition and instead identify the underlying causal factors (e.g., traffic rules, obstacles) that necessitate deviations from a default, inertially-guided path. To deal with the optimization imbalance caused by uncertain, long-term horizons, ResAD further incorporates Point-wise Normalization of the predicted residual. It re-weights the optimization objective, preventing large-magnitude errors associated with distant, uncertain waypoints from dominating the learning signal. Extensive experiments validate the effectiveness of our framework. On the NAVSIM benchmark, ResAD achieves a state-of-the-art PDMS of 88.6 using a vanilla diffusion policy with only two denoising steps, demonstrating that our approach significantly simplifies the learning task and improves model performance. The code will be released to facilitate further research.

</details>


### [39] [D$^2$GS: Depth-and-Density Guided Gaussian Splatting for Stable and Accurate Sparse-View Reconstruction](https://arxiv.org/abs/2510.08566)
*Meixi Song,Xin Lin,Dizhe Zhang,Haodong Li,Xiangtai Li,Bo Du,Lu Qi*

Main category: cs.CV

TL;DR: 提出D²GS框架解决稀疏视图下3D高斯溅射的过拟合和欠拟合问题，通过深度密度引导的dropout策略和距离感知保真度增强模块，显著提升稀疏视图条件下的重建质量和稳定性。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射在稀疏视图条件下存在性能下降和不稳定性问题，主要表现为相机附近高斯密度过高的过拟合区域和远处高斯覆盖不足的欠拟合区域。

Method: 1. 深度密度引导的dropout策略：基于密度和深度自适应掩码冗余高斯来抑制过拟合；2. 距离感知保真度增强模块：通过针对性监督改善远处欠拟合区域的重建质量；3. 提出新的评估指标量化学习高斯分布的稳定性。

Result: 在多个数据集上的广泛实验表明，该方法在稀疏视图条件下显著提升了视觉质量和鲁棒性。

Conclusion: D²GS框架有效解决了稀疏视图3D高斯溅射中的过拟合和欠拟合问题，为稀疏视图条件下的高质量3D重建提供了有效解决方案。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) enable real-time, high-fidelity novel view synthesis (NVS) with explicit 3D representations. However, performance degradation and instability remain significant under sparse-view conditions. In this work, we identify two key failure modes under sparse-view conditions: overfitting in regions with excessive Gaussian density near the camera, and underfitting in distant areas with insufficient Gaussian coverage. To address these challenges, we propose a unified framework D$^2$GS, comprising two key components: a Depth-and-Density Guided Dropout strategy that suppresses overfitting by adaptively masking redundant Gaussians based on density and depth, and a Distance-Aware Fidelity Enhancement module that improves reconstruction quality in under-fitted far-field areas through targeted supervision. Moreover, we introduce a new evaluation metric to quantify the stability of learned Gaussian distributions, providing insights into the robustness of the sparse-view 3DGS. Extensive experiments on multiple datasets demonstrate that our method significantly improves both visual quality and robustness under sparse view conditions. The project page can be found at: https://insta360-research-team.github.io/DDGS-website/.

</details>


### [40] [ReSplat: Learning Recurrent Gaussian Splats](https://arxiv.org/abs/2510.08575)
*Haofei Xu,Daniel Barath,Andreas Geiger,Marc Pollefeys*

Main category: cs.CV

TL;DR: ReSplat是一种前馈循环高斯溅射模型，通过迭代细化3D高斯而无需显式计算梯度，利用渲染误差作为反馈信号来指导高斯更新，在减少高斯数量和提升渲染速度的同时实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统前馈高斯溅射模型依赖单次前向推理，性能受限于此。需要一种能够迭代优化且适应未见数据分布的鲁棒方法。

Method: 提出循环高斯溅射模型，使用渲染误差作为反馈信号指导高斯更新；引入紧凑重建模型在16倍降采样空间初始化，大幅减少高斯数量。

Result: 在多种输入视图(2,8,16)、分辨率(256×256到540×960)和数据集(DL3DV、RealEstate10K)上的实验表明，该方法在减少高斯数量和提升渲染速度的同时达到最先进性能。

Conclusion: ReSplat通过循环反馈机制有效提升了高斯溅射的性能和泛化能力，在计算效率和渲染质量方面都取得了显著改进。

Abstract: While feed-forward Gaussian splatting models provide computational efficiency and effectively handle sparse input settings, their performance is fundamentally limited by the reliance on a single forward pass during inference. We propose ReSplat, a feed-forward recurrent Gaussian splatting model that iteratively refines 3D Gaussians without explicitly computing gradients. Our key insight is that the Gaussian splatting rendering error serves as a rich feedback signal, guiding the recurrent network to learn effective Gaussian updates. This feedback signal naturally adapts to unseen data distributions at test time, enabling robust generalization. To initialize the recurrent process, we introduce a compact reconstruction model that operates in a $16 \times$ subsampled space, producing $16 \times$ fewer Gaussians than previous per-pixel Gaussian models. This substantially reduces computational overhead and allows for efficient Gaussian updates. Extensive experiments across varying of input views (2, 8, 16), resolutions ($256 \times 256$ to $540 \times 960$), and datasets (DL3DV and RealEstate10K) demonstrate that our method achieves state-of-the-art performance while significantly reducing the number of Gaussians and improving the rendering speed. Our project page is at https://haofeixu.github.io/resplat/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [41] [Biology-driven assessment of deep learning super-resolution imaging of the porosity network in dentin](https://arxiv.org/abs/2510.08407)
*Lauren Anderson,Lucas Chatelain,Nicolas Tremblay,Kathryn Grandfield,David Rousseau,Aurélien Gourrier*

Main category: cs.LG

TL;DR: 本研究测试了多种深度学习超分辨率模型，用于从低分辨率共聚焦图像恢复牙本质孔隙网络的高分辨率图像，并通过基于生物学特性的评估方法验证模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前牙本质孔隙网络的可视化受限于共聚焦显微镜的分辨率，需要在小区域内获取高分辨率图像。为了克服这一限制，研究旨在通过深度学习超分辨率技术实现更快的低分辨率采集和后期处理恢复。

Method: 使用三种监督式2D超分辨率模型（RCAN、pix2pix、FSRCNN）和一种无监督模型（CycleGAN），在实验配对的低-高分辨率共聚焦图像上进行测试，像素尺寸增加倍数为x2、x4、x8。通过图像分割、连通组件分析和图分析评估3D孔隙连通性。

Result: 标准图像质量评估指标与视觉感知不一致，无法有效评估牙本质孔隙结构的特异性。基于生物学特性的评估方法（连通组件分析和图分析）能更好地解释超分辨率模型的性能差异，揭示了模型对弱强度特征的敏感性和图像生成非线性的影响。

Conclusion: 标准图像质量评估指标不适用于牙本质孔隙网络的特异性结构评估，基于生物学特性的评估方法能提供更准确的性能解释，为超分辨率技术在生物医学成像中的应用提供了重要参考。

Abstract: The mechanosensory system of teeth is currently believed to partly rely on Odontoblast cells stimulation by fluid flow through a porosity network extending through dentin. Visualizing the smallest sub-microscopic porosity vessels therefore requires the highest achievable resolution from confocal fluorescence microscopy, the current gold standard. This considerably limits the extent of the field of view to very small sample regions. To overcome this limitation, we tested different deep learning (DL) super-resolution (SR) models to allow faster experimental acquisitions of lower resolution images and restore optimal image quality by post-processing. Three supervised 2D SR models (RCAN, pix2pix, FSRCNN) and one unsupervised (CycleGAN) were applied to a unique set of experimentally paired high- and low-resolution confocal images acquired with different sampling schemes, resulting in a pixel size increase of x2, x4, x8. Model performance was quantified using a broad set of similarity and distribution-based image quality assessment (IQA) metrics, which yielded inconsistent results that mostly contradicted our visual perception. This raises the question of the relevance of such generic metrics to efficiently target the specific structure of dental porosity. To resolve this conflicting information, the generated SR images were segmented taking into account the specific scales and morphology of the porosity network and analysed by comparing connected components. Additionally, the capacity of the SR models to preserve 3D porosity connectivity throughout the confocal image stacks was evaluated using graph analysis. This biology-driven assessment allowed a far better mechanistic interpretation of SR performance, highlighting differences in model sensitivity to weak intensity features and the impact of non-linearity in image generation, which explains the failure of standard IQA metrics.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [42] [NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos](https://arxiv.org/abs/2510.08568)
*Hongyu Li,Lingfeng Sun,Yafei Hu,Duy Ta,Jennifer Barry,George Konidaris,Jiahui Fu*

Main category: cs.RO

TL;DR: NovaFlow是一个零样本机器人操作框架，通过视频生成和3D物体流分析实现跨平台的自主操作，无需演示或特定机器人训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要任务分布内数据或特定机器人的微调，限制了跨平台迁移能力。目标是实现机器人零样本执行新任务，无需演示。

Method: 将任务描述转换为视频，通过感知模块提取3D物体流，对刚性物体计算相对位姿并规划抓取和轨迹，对可变形物体使用基于粒子的动力学模型进行跟踪规划。

Result: 在桌面Franka机械臂和Spot四足移动机器人上验证了刚性、关节和可变形物体操作任务，实现了有效的零样本执行。

Conclusion: 通过将任务理解与底层控制解耦，NovaFlow实现了跨平台的零样本操作能力，无需演示或特定机器人训练。

Abstract: Enabling robots to execute novel manipulation tasks zero-shot is a central goal in robotics. Most existing methods assume in-distribution tasks or rely on fine-tuning with embodiment-matched data, limiting transfer across platforms. We present NovaFlow, an autonomous manipulation framework that converts a task description into an actionable plan for a target robot without any demonstrations. Given a task description, NovaFlow synthesizes a video using a video generation model and distills it into 3D actionable object flow using off-the-shelf perception modules. From the object flow, it computes relative poses for rigid objects and realizes them as robot actions via grasp proposals and trajectory optimization. For deformable objects, this flow serves as a tracking objective for model-based planning with a particle-based dynamics model. By decoupling task understanding from low-level control, NovaFlow naturally transfers across embodiments. We validate on rigid, articulated, and deformable object manipulation tasks using a table-top Franka arm and a Spot quadrupedal mobile robot, and achieve effective zero-shot execution without demonstrations or embodiment-specific training. Project website: https://novaflow.lhy.xyz/.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [43] [FlowLensing: Simulating Gravitational Lensing with Flow Matching](https://arxiv.org/abs/2510.07878)
*Hamees Sayed,Pranath Reddy,Michael W. Toomey,Sergei Gleyzer*

Main category: astro-ph.IM

TL;DR: FlowLensing是一个基于扩散变换器的紧凑高效流匹配模型，用于强引力透镜模拟，相比传统模拟器实现200倍加速，支持离散和连续参数，确保物理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有引力透镜模拟工具依赖光线追踪或前向建模管道，虽然精确但速度极慢，成为暗物质研究的瓶颈。

Method: 使用基于扩散变换器的流匹配模型，在离散和连续参数域中操作，处理不同暗物质模型和连续模型参数。

Result: 模型在密集暗物质模型中相比经典模拟器实现200倍以上的加速，具有高保真度和低推理延迟。

Conclusion: FlowLensing能够实现快速、可扩展且物理一致的图像合成，为传统前向建模管道提供了实用替代方案。

Abstract: Gravitational lensing is one of the most powerful probes of dark matter, yet creating high-fidelity lensed images at scale remains a bottleneck. Existing tools rely on ray-tracing or forward-modeling pipelines that, while precise, are prohibitively slow. We introduce FlowLensing, a Diffusion Transformer-based compact and efficient flow-matching model for strong gravitational lensing simulation. FlowLensing operates in both discrete and continuous regimes, handling classes such as different dark matter models as well as continuous model parameters ensuring physical consistency. By enabling scalable simulations, our model can advance dark matter studies, specifically for probing dark matter substructure in cosmological surveys. We find that our model achieves a speedup of over 200$\times$ compared to classical simulators for intensive dark matter models, with high fidelity and low inference latency. FlowLensing enables rapid, scalable, and physically consistent image synthesis, offering a practical alternative to traditional forward-modeling pipelines.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [44] [SatFusion: A Unified Framework for Enhancing Satellite IoT Images via Multi-Temporal and Multi-Source Data Fusion](https://arxiv.org/abs/2510.07905)
*Yufei Tong,Guanjie Cheng,Peihan Wu,Yicheng Zhu,Kexu Lu,Feiyi Chen,Meng Xi,Junqin Huang,Shuiguang Deng*

Main category: eess.IV

TL;DR: SatFusion是一个统一的卫星物联网图像增强框架，通过多时相和多源数据融合来解决现有方法的局限性。它结合了多图像超分辨率的时间互补性和全色锐化的空间信息注入优势，显著提升了融合质量和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着卫星物联网的快速发展，大规模多时相多源图像不断积累，但现有方法未能充分利用时间和源维度的互补信息。多图像超分辨率受限于输入图像的纹理细节，而全色锐化对噪声和配准误差敏感。

Method: SatFusion包含三个模块：多时相图像融合模块实现与全色图像的深度特征对齐，多源图像融合模块注入全色数据的细粒度纹理信息，融合组合模块自适应整合两种模态的优势并动态优化光谱一致性。

Result: 在WorldStrat、WV3、QB和GF2数据集上的广泛实验表明，SatFusion显著提高了融合质量、在挑战性条件下的鲁棒性以及对真实卫星物联网场景的泛化能力。

Conclusion: SatFusion通过统一的多时相和多源数据融合框架，有效解决了卫星图像增强中的关键问题，为卫星物联网应用提供了高质量的图像增强解决方案。

Abstract: With the rapid advancement of the digital society, the proliferation of satellites in the Satellite Internet of Things (Sat-IoT) has led to the continuous accumulation of large-scale multi-temporal and multi-source images across diverse application scenarios. However, existing methods fail to fully exploit the complementary information embedded in both temporal and source dimensions. For example, Multi-Image Super-Resolution (MISR) enhances reconstruction quality by leveraging temporal complementarity across multiple observations, yet the limited fine-grained texture details in input images constrain its performance. Conversely, pansharpening integrates multi-source images by injecting high-frequency spatial information from panchromatic data, but typically relies on pre-interpolated low-resolution inputs and assumes noise-free alignment, making it highly sensitive to noise and misregistration. To address these issues, we propose SatFusion: A Unified Framework for Enhancing Satellite IoT Images via Multi-Temporal and Multi-Source Data Fusion. Specifically, SatFusion first employs a Multi-Temporal Image Fusion (MTIF) module to achieve deep feature alignment with the panchromatic image. Then, a Multi-Source Image Fusion (MSIF) module injects fine-grained texture information from the panchromatic data. Finally, a Fusion Composition module adaptively integrates the complementary advantages of both modalities while dynamically refining spectral consistency, supervised by a weighted combination of multiple loss functions. Extensive experiments on the WorldStrat, WV3, QB, and GF2 datasets demonstrate that SatFusion significantly improves fusion quality, robustness under challenging conditions, and generalizability to real-world Sat-IoT scenarios. The code is available at: https://github.com/dllgyufei/SatFusion.git.

</details>
