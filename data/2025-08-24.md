<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 19]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [eess.IV](#eess.IV) [Total: 5]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Inference Time Debiasing Concepts in Diffusion Models](https://arxiv.org/abs/2508.14933)
*Lucas S. Kupssinskü,Marco N. Bochernitsan,Jordan Kopper,Otávio Parraga,Rodrigo C. Barros*

Main category: cs.GR

TL;DR: DeCoDi是一种用于文本到图像扩散模型的去偏方法，通过改变推理过程避免潜在维度中的偏见概念区域，无需复杂干预或显著计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习去偏方法通常需要复杂或计算密集的干预，而DeCoDi旨在仅改变推理过程，使其对更广泛的实践者更加可访问。

Method: 改变扩散过程以避免潜在维度中的偏见概念区域，该方法可应用于任何基于扩散的图像生成模型。

Result: 通过对护士、消防员和CEO等概念进行性别、种族和年龄去偏，人工评估1200张生成图像显示方法有效。GPT4o的自动偏见评估与人工评估无显著统计差异。

Conclusion: DeCoDi方法有潜力显著改善基于扩散的文本到图像生成模型生成图像的多样性，评估结果显示评估者间一致性可靠，对受保护属性的覆盖更全面。

Abstract: We propose DeCoDi, a debiasing procedure for text-to-image diffusion-based models that changes the inference procedure, does not significantly change image quality, has negligible compute overhead, and can be applied in any diffusion-based image generation model. DeCoDi changes the diffusion process to avoid latent dimension regions of biased concepts. While most deep learning debiasing methods require complex or compute-intensive interventions, our method is designed to change only the inference procedure. Therefore, it is more accessible to a wide range of practitioners. We show the effectiveness of the method by debiasing for gender, ethnicity, and age for the concepts of nurse, firefighter, and CEO. Two distinct human evaluators manually inspect 1,200 generated images. Their evaluation results provide evidence that our method is effective in mitigating biases based on gender, ethnicity, and age. We also show that an automatic bias evaluation performed by the GPT4o is not significantly statistically distinct from a human evaluation. Our evaluation shows promising results, with reliable levels of agreement between evaluators and more coverage of protected attributes. Our method has the potential to significantly improve the diversity of images it generates by diffusion-based text-to-image generative models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [Image-Conditioned 3D Gaussian Splat Quantization](https://arxiv.org/abs/2508.15372)
*Xinshuang Liu,Runfa Blark Li,Keito Suzuki,Truong Nguyen*

Main category: cs.CV

TL;DR: 提出ICGS-Quantizer方法，通过联合利用高斯间和属性间相关性、使用跨场景共享码本，将3D高斯溅射压缩到千字节级别，并支持基于图像的条件解码以适应场景变化。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS压缩方法存在两个主要限制：(1)只能将中等规模场景压缩到兆字节级别，对大规模场景或场景集合仍不实用；(2)缺乏适应长期归档后场景变化的机制。

Method: 提出图像条件高斯溅射量化器(ICGS-Quantizer)，通过联合利用高斯间和属性间相关性、使用固定共享码本，并基于解码时捕获的图像进行条件解码。编码、量化和解码过程联合训练。

Result: 实验结果表明，ICGS-Quantizer在压缩效率和场景变化适应性方面持续优于最先进方法，将3DGS存储需求降低到千字节范围同时保持视觉保真度。

Conclusion: ICGS-Quantizer显著提升了压缩效率并提供了归档后场景变化的适应性，解决了现有3DGS压缩方法的局限性。

Abstract: 3D Gaussian Splatting (3DGS) has attracted considerable attention for enabling high-quality real-time rendering. Although 3DGS compression methods have been proposed for deployment on storage-constrained devices, two limitations hinder archival use: (1) they compress medium-scale scenes only to the megabyte range, which remains impractical for large-scale scenes or extensive scene collections; and (2) they lack mechanisms to accommodate scene changes after long-term archival. To address these limitations, we propose an Image-Conditioned Gaussian Splat Quantizer (ICGS-Quantizer) that substantially enhances compression efficiency and provides adaptability to scene changes after archiving. ICGS-Quantizer improves quantization efficiency by jointly exploiting inter-Gaussian and inter-attribute correlations and by using shared codebooks across all training scenes, which are then fixed and applied to previously unseen test scenes, eliminating the overhead of per-scene codebooks. This approach effectively reduces the storage requirements for 3DGS to the kilobyte range while preserving visual fidelity. To enable adaptability to post-archival scene changes, ICGS-Quantizer conditions scene decoding on images captured at decoding time. The encoding, quantization, and decoding processes are trained jointly, ensuring that the codes, which are quantized representations of the scene, are effective for conditional decoding. We evaluate ICGS-Quantizer on 3D scene compression and 3D scene updating. Experimental results show that ICGS-Quantizer consistently outperforms state-of-the-art methods in compression efficiency and adaptability to scene changes. Our code, model, and data will be publicly available on GitHub.

</details>


### [3] [Scaling Group Inference for Diverse and High-Quality Generation](https://arxiv.org/abs/2508.15773)
*Gaurav Parmar,Or Patashnik,Daniil Ostashev,Kuan-Chieh Wang,Kfir Aberman,Srinivasa Narasimhan,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: 提出了一种可扩展的组推理方法，通过将组推理建模为二次整数分配问题，在提高样本质量的同时最大化组多样性，解决了传统独立采样导致的冗余问题。


<details>
  <summary>Details</summary>
Motivation: 在真实应用中，用户通常需要获得一组多样化的输出（如4-8张图像），但传统生成模型的独立采样往往导致结果冗余，限制了用户选择和创意探索。

Method: 将组推理建模为二次整数分配问题：候选输出作为图节点，通过优化样本质量（一元项）和最大化组多样性（二元项）来选择子集；采用渐进式剪枝策略提高运行效率，可扩展到大规模候选集。

Result: 大量实验表明，该方法相比独立采样基线和最新推理算法，显著提高了组多样性和质量；框架可泛化到文本到图像、图像到图像、图像提示和视频生成等多种任务。

Conclusion: 该框架使生成模型能够将多个输出视为有凝聚力的组而非独立样本，为实际应用提供了更丰富和多样化的生成结果选择。

Abstract: Generative models typically sample outputs independently, and recent inference-time guidance and scaling algorithms focus on improving the quality of individual samples. However, in real-world applications, users are often presented with a set of multiple images (e.g., 4-8) for each prompt, where independent sampling tends to lead to redundant results, limiting user choices and hindering idea exploration. In this work, we introduce a scalable group inference method that improves both the diversity and quality of a group of samples. We formulate group inference as a quadratic integer assignment problem: candidate outputs are modeled as graph nodes, and a subset is selected to optimize sample quality (unary term) while maximizing group diversity (binary term). To substantially improve runtime efficiency, we progressively prune the candidate set using intermediate predictions, allowing our method to scale up to large candidate sets. Extensive experiments show that our method significantly improves group diversity and quality compared to independent sampling baselines and recent inference algorithms. Our framework generalizes across a wide range of tasks, including text-to-image, image-to-image, image prompting, and video generation, enabling generative models to treat multiple outputs as cohesive groups rather than independent samples.

</details>


### [4] [TAIGen: Training-Free Adversarial Image Generation via Diffusion Models](https://arxiv.org/abs/2508.15020)
*Susim Roy,Anubhooti Jain,Mayank Vatsa,Richa Singh*

Main category: cs.CV

TL;DR: TAIGen是一种无需训练的黑盒对抗图像生成方法，通过仅使用3-20个采样步骤从无条件扩散模型中高效生成高质量对抗样本，比现有方法快10倍


<details>
  <summary>Details</summary>
Motivation: 解决生成模型对抗攻击中图像质量低、计算资源消耗大的问题，特别是扩散模型需要数百个采样步骤的局限性

Method: 在混合步骤区间注入扰动，采用选择性RGB通道策略：对红色通道应用注意力图，对绿色和蓝色通道使用GradCAM引导的扰动，保持图像结构的同时最大化目标模型的误分类

Result: 在ImageNet数据集上，针对VGGNet源模型，对ResNet成功率为70.6%，对MNASNet为80.8%，对ShuffleNet为97.8%；PSNR超过30dB，视觉质量优秀

Conclusion: TAIGen是最具影响力的攻击方法，防御机制对其生成的图像净化效果最差，实现了最低的鲁棒准确率

Abstract: Adversarial attacks from generative models often produce low-quality images and require substantial computational resources. Diffusion models, though capable of high-quality generation, typically need hundreds of sampling steps for adversarial generation. This paper introduces TAIGen, a training-free black-box method for efficient adversarial image generation. TAIGen produces adversarial examples using only 3-20 sampling steps from unconditional diffusion models. Our key finding is that perturbations injected during the mixing step interval achieve comparable attack effectiveness without processing all timesteps. We develop a selective RGB channel strategy that applies attention maps to the red channel while using GradCAM-guided perturbations on green and blue channels. This design preserves image structure while maximizing misclassification in target models. TAIGen maintains visual quality with PSNR above 30 dB across all tested datasets. On ImageNet with VGGNet as source, TAIGen achieves 70.6% success against ResNet, 80.8% against MNASNet, and 97.8% against ShuffleNet. The method generates adversarial examples 10x faster than existing diffusion-based attacks. Our method achieves the lowest robust accuracy, indicating it is the most impactful attack as the defense mechanism is least successful in purifying the images generated by TAIGen.

</details>


### [5] [Reversible Unfolding Network for Concealed Visual Perception with Generative Refinement](https://arxiv.org/abs/2508.15027)
*Chunming He,Fengyang Xiao,Rihan Zhang,Chengyu Fang,Deng-Ping Fan,Sina Farsiu*

Main category: cs.CV

TL;DR: RUN++是一个可逆展开网络，通过生成式精炼解决隐蔽视觉感知问题，在掩码和RGB双域应用可逆建模，并使用针对性扩散模型优化不确定区域


<details>
  <summary>Details</summary>
Motivation: 现有隐蔽视觉感知方法主要局限于掩码域的可逆策略，未能充分利用RGB域的潜力，需要一种能够在双域同时应用可逆建模并有效处理不确定性的方法

Method: 将CVP任务公式化为数学优化问题并展开为多阶段深度网络，包含CORE模块（掩码域可逆建模）、CARE模块（RGB域可逆建模）和FINE模块（针对性Bernoulli扩散模型精炼）

Result: 提出的方法通过展开网络提供强不确定性先验，使扩散模型能高效聚焦模糊区域，显著减少假阳性和假阴性，在真实世界退化条件下保持鲁棒性

Conclusion: RUN++通过可逆展开网络与生成式精炼的独特协同，建立了新的CVP系统范式，扩展为双层优化框架，为隐蔽视觉感知提供了有效解决方案

Abstract: Existing methods for concealed visual perception (CVP) often leverage reversible strategies to decrease uncertainty, yet these are typically confined to the mask domain, leaving the potential of the RGB domain underexplored. To address this, we propose a reversible unfolding network with generative refinement, termed RUN++. Specifically, RUN++ first formulates the CVP task as a mathematical optimization problem and unfolds the iterative solution into a multi-stage deep network. This approach provides a principled way to apply reversible modeling across both mask and RGB domains while leveraging a diffusion model to resolve the resulting uncertainty. Each stage of the network integrates three purpose-driven modules: a Concealed Object Region Extraction (CORE) module applies reversible modeling to the mask domain to identify core object regions; a Context-Aware Region Enhancement (CARE) module extends this principle to the RGB domain to foster better foreground-background separation; and a Finetuning Iteration via Noise-based Enhancement (FINE) module provides a final refinement. The FINE module introduces a targeted Bernoulli diffusion model that refines only the uncertain regions of the segmentation mask, harnessing the generative power of diffusion for fine-detail restoration without the prohibitive computational cost of a full-image process. This unique synergy, where the unfolding network provides a strong uncertainty prior for the diffusion model, allows RUN++ to efficiently direct its focus toward ambiguous areas, significantly mitigating false positives and negatives. Furthermore, we introduce a new paradigm for building robust CVP systems that remain effective under real-world degradations and extend this concept into a broader bi-level optimization framework.

</details>


### [6] [CurveFlow: Curvature-Guided Flow Matching for Image Generation](https://arxiv.org/abs/2508.15093)
*Yan Luo,Drake Du,Hao Huang,Yi Fang,Mengyu Wang*

Main category: cs.CV

TL;DR: CurveFlow通过引入曲率引导的非线性轨迹来改进文本到图像生成，相比线性轨迹的rectified flow模型能更好地保持语义一致性和指令遵循能力


<details>
  <summary>Details</summary>
Motivation: 现有的rectified flow模型使用线性轨迹，强制零曲率可能导致生成过程经过数据流形的低概率区域，影响语义对齐和指令遵循能力

Method: 提出CurveFlow框架，通过曲率正则化技术学习平滑的非线性轨迹，惩罚轨迹内在动态的突变

Result: 在MS COCO数据集上达到SOTA性能，在BLEU、METEOR、ROUGE、CLAIR等语义一致性指标上显著优于标准rectified flow变体和其他非线性基线

Conclusion: 曲率感知建模显著增强了模型忠实遵循复杂指令的能力，同时保持高图像质量

Abstract: Existing rectified flow models are based on linear trajectories between data and noise distributions. This linearity enforces zero curvature, which can inadvertently force the image generation process through low-probability regions of the data manifold. A key question remains underexplored: how does the curvature of these trajectories correlate with the semantic alignment between generated images and their corresponding captions, i.e., instructional compliance? To address this, we introduce CurveFlow, a novel flow matching framework designed to learn smooth, non-linear trajectories by directly incorporating curvature guidance into the flow path. Our method features a robust curvature regularization technique that penalizes abrupt changes in the trajectory's intrinsic dynamics.Extensive experiments on MS COCO 2014 and 2017 demonstrate that CurveFlow achieves state-of-the-art performance in text-to-image generation, significantly outperforming both standard rectified flow variants and other non-linear baselines like Rectified Diffusion. The improvements are especially evident in semantic consistency metrics such as BLEU, METEOR, ROUGE, and CLAIR. This confirms that our curvature-aware modeling substantially enhances the model's ability to faithfully follow complex instructions while simultaneously maintaining high image quality. The code is made publicly available at https://github.com/Harvard-AI-and-Robotics-Lab/CurveFlow.

</details>


### [7] [XDR-LVLM: An Explainable Vision-Language Large Model for Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2508.15168)
*Masato Ito,Kaito Tanaka,Keisuke Matsuda,Aya Nakayama*

Main category: cs.CV

TL;DR: XDR-LVLM是一个基于视觉语言大模型的可解释性糖尿病视网膜病变诊断框架，通过自然语言解释实现高精度诊断和透明化报告生成


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在医疗诊断中缺乏透明度和可解释性的问题，促进临床采用

Method: 整合专用医学视觉编码器和LVLM核心，采用多任务提示工程和多阶段微调，从眼底图像中理解病理特征并生成包含DR分级、关键病理概念识别和详细解释的诊断报告

Result: 在DDR数据集上达到84.55%的平衡准确率和79.92%的F1分数，概念检测达到77.95% BACC和66.88% F1，人类评估确认生成解释的高流畅性、准确性和临床实用性

Conclusion: XDR-LVLM能够通过提供稳健且可解释的见解，弥合自动化诊断与临床需求之间的差距

Abstract: Diabetic Retinopathy (DR) is a major cause of global blindness, necessitating early and accurate diagnosis. While deep learning models have shown promise in DR detection, their black-box nature often hinders clinical adoption due to a lack of transparency and interpretability. To address this, we propose XDR-LVLM (eXplainable Diabetic Retinopathy Diagnosis with LVLM), a novel framework that leverages Vision-Language Large Models (LVLMs) for high-precision DR diagnosis coupled with natural language-based explanations. XDR-LVLM integrates a specialized Medical Vision Encoder, an LVLM Core, and employs Multi-task Prompt Engineering and Multi-stage Fine-tuning to deeply understand pathological features within fundus images and generate comprehensive diagnostic reports. These reports explicitly include DR severity grading, identification of key pathological concepts (e.g., hemorrhages, exudates, microaneurysms), and detailed explanations linking observed features to the diagnosis. Extensive experiments on the Diabetic Retinopathy (DDR) dataset demonstrate that XDR-LVLM achieves state-of-the-art performance, with a Balanced Accuracy of 84.55% and an F1 Score of 79.92% for disease diagnosis, and superior results for concept detection (77.95% BACC, 66.88% F1). Furthermore, human evaluations confirm the high fluency, accuracy, and clinical utility of the generated explanations, showcasing XDR-LVLM's ability to bridge the gap between automated diagnosis and clinical needs by providing robust and interpretable insights.

</details>


### [8] [MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View Consistent Diffusion](https://arxiv.org/abs/2508.15169)
*Xuyang Chen,Zhijun Zhai,Kaixuan Zhou,Zengmao Wang,Jianan He,Dong Wang,Yanfeng Zhang,mingwei Sun,Rüdiger Westermann,Konrad Schindler,Liqiu Meng*

Main category: cs.CV

TL;DR: MeSS方法利用城市网格模型作为几何先验，通过改进的图像扩散模型生成高质量、风格一致的室外场景纹理，解决了现有3D城市模型缺乏真实纹理的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的城市网格模型缺乏真实纹理，限制了在虚拟城市导航和自动驾驶中的应用。虽然图像和视频扩散模型可以利用空间布局生成街景视角，但无法直接用于3D场景生成或保证视图一致性。

Method: 采用三阶段管道：1) 使用级联外绘ControlNet生成几何一致的稀疏视图；2) 通过AGInpaint组件传播更密集的中间视图；3) 使用GCAlign模块全局消除视觉不一致性。同时基于网格表面初始化3D高斯溅射场景重建。

Result: 该方法在几何对齐和生成质量方面优于现有方法，能够生成高质量、风格一致的室外场景纹理。

Conclusion: MeSS成功解决了城市网格模型的纹理生成问题，生成的场景可通过重照明和风格迁移技术以不同风格渲染，为虚拟城市导航和自动驾驶提供了实用的解决方案。

Abstract: Mesh models have become increasingly accessible for numerous cities; however, the lack of realistic textures restricts their application in virtual urban navigation and autonomous driving. To address this, this paper proposes MeSS (Meshbased Scene Synthesis) for generating high-quality, styleconsistent outdoor scenes with city mesh models serving as the geometric prior. While image and video diffusion models can leverage spatial layouts (such as depth maps or HD maps) as control conditions to generate street-level perspective views, they are not directly applicable to 3D scene generation. Video diffusion models excel at synthesizing consistent view sequences that depict scenes but often struggle to adhere to predefined camera paths or align accurately with rendered control videos. In contrast, image diffusion models, though unable to guarantee cross-view visual consistency, can produce more geometry-aligned results when combined with ControlNet. Building on this insight, our approach enhances image diffusion models by improving cross-view consistency. The pipeline comprises three key stages: first, we generate geometrically consistent sparse views using Cascaded Outpainting ControlNets; second, we propagate denser intermediate views via a component dubbed AGInpaint; and third, we globally eliminate visual inconsistencies (e.g., varying exposure) using the GCAlign module. Concurrently with generation, a 3D Gaussian Splatting (3DGS) scene is reconstructed by initializing Gaussian balls on the mesh surface. Our method outperforms existing approaches in both geometric alignment and generation quality. Once synthesized, the scene can be rendered in diverse styles through relighting and style transfer techniques.

</details>


### [9] [Collaborative Multi-Modal Coding for High-Quality 3D Generation](https://arxiv.org/abs/2508.15228)
*Ziang Cao,Zhaoxi Chen,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: TriMM是首个前馈式3D原生生成模型，通过多模态协作编码（RGB、RGBD、点云）和triplane潜在扩散模型，有效利用多模态数据提升3D资产生成的纹理和几何细节质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成架构大多局限于单一模态或3D结构，无法充分利用多模态数据的互补优势，限制了训练数据集的范围和生成质量。

Method: 1）引入协作多模态编码整合模态特定特征；2）采用辅助2D和3D监督提升鲁棒性；3）基于triplane潜在扩散模型生成高质量3D资产

Result: 在多个知名数据集上的实验表明，TriMM通过有效利用多模态数据，在使用少量训练数据的情况下实现了与大规模数据集训练模型相竞争的性能

Conclusion: TriMM证明了多模态数据在3D生成中的有效性，并为整合其他多模态数据集提供了可行性验证

Abstract: 3D content inherently encompasses multi-modal characteristics and can be projected into different modalities (e.g., RGB images, RGBD, and point clouds). Each modality exhibits distinct advantages in 3D asset modeling: RGB images contain vivid 3D textures, whereas point clouds define fine-grained 3D geometries. However, most existing 3D-native generative architectures either operate predominantly within single-modality paradigms-thus overlooking the complementary benefits of multi-modality data-or restrict themselves to 3D structures, thereby limiting the scope of available training datasets. To holistically harness multi-modalities for 3D modeling, we present TriMM, the first feed-forward 3D-native generative model that learns from basic multi-modalities (e.g., RGB, RGBD, and point cloud). Specifically, 1) TriMM first introduces collaborative multi-modal coding, which integrates modality-specific features while preserving their unique representational strengths. 2) Furthermore, auxiliary 2D and 3D supervision are introduced to raise the robustness and performance of multi-modal coding. 3) Based on the embedded multi-modal code, TriMM employs a triplane latent diffusion model to generate 3D assets of superior quality, enhancing both the texture and the geometric detail. Extensive experiments on multiple well-known datasets demonstrate that TriMM, by effectively leveraging multi-modality, achieves competitive performance with models trained on large-scale datasets, despite utilizing a small amount of training data. Furthermore, we conduct additional experiments on recent RGB-D datasets, verifying the feasibility of incorporating other multi-modal datasets into 3D generation.

</details>


### [10] [Pretrained Diffusion Models Are Inherently Skipped-Step Samplers](https://arxiv.org/abs/2508.15233)
*Wenju Xu*

Main category: cs.CV

TL;DR: 本文提出了一种跳过步采样机制，可以直接绕过扩散模型生成过程中的多个中间去噪步骤，实现加速采样，且该方法与标准扩散模型具有相同的训练目标。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然在各生成任务中取得最优效果，但其顺序生成过程需要多步迭代，效率较低。现有方法如DDIM通过非马尔可夫过程减少采样步数，但缺乏对原始扩散过程能否以马尔可夫方式实现相同效率的理解。

Method: 提出跳过步采样机制，允许在迭代生成过程中绕过多个中间去噪步骤，而不是传统的逐步细化。该方法从与标准扩散模型相同的训练目标推导而来，表明通过马尔可夫方式的加速采样是预训练扩散模型的内在属性。

Result: 在OpenAI ADM、Stable Diffusion和Open Sora等流行预训练扩散模型上的大量实验表明，该方法能够以显著减少的采样步骤实现高质量生成。

Conclusion: 跳过步采样是扩散模型的内在特性，可以通过马尔可夫方式实现高效采样，该方法还可与DDIM结合进一步提升生成效果。

Abstract: Diffusion models have been achieving state-of-the-art results across various generation tasks. However, a notable drawback is their sequential generation process, requiring long-sequence step-by-step generation. Existing methods, such as DDIM, attempt to reduce sampling steps by constructing a class of non-Markovian diffusion processes that maintain the same training objective. However, there remains a gap in understanding whether the original diffusion process can achieve the same efficiency without resorting to non-Markovian processes. In this paper, we provide a confirmative answer and introduce skipped-step sampling, a mechanism that bypasses multiple intermediate denoising steps in the iterative generation process, in contrast with the traditional step-by-step refinement of standard diffusion inference. Crucially, we demonstrate that this skipped-step sampling mechanism is derived from the same training objective as the standard diffusion model, indicating that accelerated sampling via skipped-step sampling via a Markovian way is an intrinsic property of pretrained diffusion models. Additionally, we propose an enhanced generation method by integrating our accelerated sampling technique with DDIM. Extensive experiments on popular pretrained diffusion models, including the OpenAI ADM, Stable Diffusion, and Open Sora models, show that our method achieves high-quality generation with significantly reduced sampling steps.

</details>


### [11] [TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect Classification](https://arxiv.org/abs/2508.15298)
*Darya Taratynova,Alya Almsouti,Beknur Kalmakhanbet,Numan Saeed,Mohammad Yaqub*

Main category: cs.CV

TL;DR: TPA是一个用于胎儿先天性心脏病超声视频分类的框架，结合时序建模、提示感知对比学习和不确定性量化，在CHD检测和心脏功能评估方面达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前超声视频CHD检测方法存在图像噪声、探头位置变异性问题，现有机器学习方法往往忽略时序信息、仅限于二元分类且缺乏预测校准。

Method: TPA利用基础图像-文本模型和提示感知对比学习，通过图像编码器提取视频子片段特征，可训练时序提取器聚合特征捕获心脏运动，并通过边界铰链对比损失将视频表示与类别特定文本提示对齐。引入CVAESM模块学习潜在风格向量来调节嵌入并量化分类不确定性。

Result: 在CHD检测私有数据集上达到85.40%的宏F1分数，同时将预期校准误差降低5.38%，自适应ECE降低6.8%。在EchoNet-Dynamic三分类任务上，宏F1提升4.73%（从53.89%到58.62%）。

Conclusion: TPA框架通过整合时序建模、提示感知对比学习和不确定性量化，在胎儿先天性心脏病超声视频分类中表现出色，提高了临床可靠性。

Abstract: Congenital heart defect (CHD) detection in ultrasound videos is hindered by image noise and probe positioning variability. While automated methods can reduce operator dependence, current machine learning approaches often neglect temporal information, limit themselves to binary classification, and do not account for prediction calibration. We propose Temporal Prompt Alignment (TPA), a method leveraging foundation image-text model and prompt-aware contrastive learning to classify fetal CHD on cardiac ultrasound videos. TPA extracts features from each frame of video subclips using an image encoder, aggregates them with a trainable temporal extractor to capture heart motion, and aligns the video representation with class-specific text prompts via a margin-hinge contrastive loss. To enhance calibration for clinical reliability, we introduce a Conditional Variational Autoencoder Style Modulation (CVAESM) module, which learns a latent style vector to modulate embeddings and quantifies classification uncertainty. Evaluated on a private dataset for CHD detection and on a large public dataset, EchoNet-Dynamic, for systolic dysfunction, TPA achieves state-of-the-art macro F1 scores of 85.40% for CHD diagnosis, while also reducing expected calibration error by 5.38% and adaptive ECE by 6.8%. On EchoNet-Dynamic's three-class task, it boosts macro F1 by 4.73% (from 53.89% to 58.62%). Temporal Prompt Alignment (TPA) is a framework for fetal congenital heart defect (CHD) classification in ultrasound videos that integrates temporal modeling, prompt-aware contrastive learning, and uncertainty quantification.

</details>


### [12] [Transfer learning optimization based on evolutionary selective fine tuning](https://arxiv.org/abs/2508.15367)
*Jacinto Colan,Ana Davila,Yasuhisa Hasegawa*

Main category: cs.CV

TL;DR: BioTune是一种进化自适应微调技术，通过进化算法选择性地微调特定层来提升迁移学习效率，在多个图像分类数据集上表现出优于传统方法的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法需要更新所有模型参数，容易导致过拟合和计算成本高，需要一种更高效的迁移学习策略。

Method: 使用进化算法识别需要微调的层子集，只对相关层进行选择性微调，减少可训练参数数量。

Result: 在9个不同领域的图像分类数据集上测试，BioTune在准确性和效率方面都优于AutoRGN和LoRA等现有微调方法。

Conclusion: 通过集中微调相关层子集，BioTune降低了计算成本，促进了跨不同数据特征和分布的高效迁移学习。

Abstract: Deep learning has shown substantial progress in image analysis. However, the computational demands of large, fully trained models remain a consideration. Transfer learning offers a strategy for adapting pre-trained models to new tasks. Traditional fine-tuning often involves updating all model parameters, which can potentially lead to overfitting and higher computational costs. This paper introduces BioTune, an evolutionary adaptive fine-tuning technique that selectively fine-tunes layers to enhance transfer learning efficiency. BioTune employs an evolutionary algorithm to identify a focused set of layers for fine-tuning, aiming to optimize model performance on a given target task. Evaluation across nine image classification datasets from various domains indicates that BioTune achieves competitive or improved accuracy and efficiency compared to existing fine-tuning methods such as AutoRGN and LoRA. By concentrating the fine-tuning process on a subset of relevant layers, BioTune reduces the number of trainable parameters, potentially leading to decreased computational cost and facilitating more efficient transfer learning across diverse data characteristics and distributions.

</details>


### [13] [DriveSplat: Decoupled Driving Scene Reconstruction with Geometry-enhanced Partitioned Neural Gaussians](https://arxiv.org/abs/2508.15376)
*Cong Wang,Xianda Guo,Wenbo Xu,Wei Tian,Ruiqi Song,Chenming Zhang,Lingxi Li,Long Chen*

Main category: cs.CV

TL;DR: DriveSplat是一种基于神经高斯表示的动态-静态解耦方法，专门针对驾驶场景的高质量3D重建，通过区域化体素初始化、可变形神经高斯和深度/法线先验监督，在Waymo和KITTI数据集上实现了最先进的新视角合成性能。


<details>
  <summary>Details</summary>
Motivation: 驾驶场景中存在快速移动的车辆、行人和大规模静态背景，给3D场景重建带来挑战。现有基于3D高斯泼溅的方法通过解耦动态和静态组件来解决运动模糊问题，但这些方法忽视了具有充分几何关系的背景优化，仅通过添加高斯来拟合每个训练视图，导致新视角渲染鲁棒性有限且缺乏准确的几何表示。

Method: 1) 采用区域化体素初始化方案，将场景划分为近、中、远区域以增强近距离细节表示；2) 引入可变形神经高斯来建模非刚性动态对象，其参数通过可学习变形网络进行时间调整；3) 整个框架通过预训练模型的深度和法线先验进行监督，提高几何结构的准确性。

Result: 在Waymo和KITTI数据集上进行了严格评估，证明了在驾驶场景新视角合成方面的最先进性能。

Conclusion: DriveSplat通过创新的动态-静态解耦策略和几何先验监督，有效解决了驾驶场景3D重建中的挑战，实现了高质量的新视角合成和准确的几何表示。

Abstract: In the realm of driving scenarios, the presence of rapidly moving vehicles, pedestrians in motion, and large-scale static backgrounds poses significant challenges for 3D scene reconstruction. Recent methods based on 3D Gaussian Splatting address the motion blur problem by decoupling dynamic and static components within the scene. However, these decoupling strategies overlook background optimization with adequate geometry relationships and rely solely on fitting each training view by adding Gaussians. Therefore, these models exhibit limited robustness in rendering novel views and lack an accurate geometric representation. To address the above issues, we introduce DriveSplat, a high-quality reconstruction method for driving scenarios based on neural Gaussian representations with dynamic-static decoupling. To better accommodate the predominantly linear motion patterns of driving viewpoints, a region-wise voxel initialization scheme is employed, which partitions the scene into near, middle, and far regions to enhance close-range detail representation. Deformable neural Gaussians are introduced to model non-rigid dynamic actors, whose parameters are temporally adjusted by a learnable deformation network. The entire framework is further supervised by depth and normal priors from pre-trained models, improving the accuracy of geometric structures. Our method has been rigorously evaluated on the Waymo and KITTI datasets, demonstrating state-of-the-art performance in novel-view synthesis for driving scenarios.

</details>


### [14] [Enhancing Novel View Synthesis from extremely sparse views with SfM-free 3D Gaussian Splatting Framework](https://arxiv.org/abs/2508.15457)
*Zongqi He,Hanmin Li,Kin-Chung Chan,Yushen Zuo,Hao Xie,Zhe Xiao,Jun Xiao,Kin-Man Lam*

Main category: cs.CV

TL;DR: 提出了一种无需SfM的3D高斯泼溅方法，能够在极稀疏视角输入下联合估计相机位姿和重建3D场景，显著提升了渲染质量


<details>
  <summary>Details</summary>
Motivation: 传统3DGS方法严重依赖密集多视角输入和精确相机位姿，在极稀疏视角下SfM初始化失败导致渲染质量下降

Method: 使用稠密立体模块逐步估计相机位姿并重建全局稠密点云初始化；提出连贯视角插值模块生成额外监督信号；引入多尺度拉普拉斯一致性正则化和自适应空间感知多尺度几何正则化

Result: 在极稀疏视角条件下（仅使用2个训练视角）PSNR显著提升2.75dB，合成图像失真最小且保留丰富高频细节

Conclusion: 该方法在极稀疏视角输入下实现了卓越的3D重建和渲染性能，为实际应用场景提供了有效解决方案

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated remarkable real-time performance in novel view synthesis, yet its effectiveness relies heavily on dense multi-view inputs with precisely known camera poses, which are rarely available in real-world scenarios. When input views become extremely sparse, the Structure-from-Motion (SfM) method that 3DGS depends on for initialization fails to accurately reconstruct the 3D geometric structures of scenes, resulting in degraded rendering quality. In this paper, we propose a novel SfM-free 3DGS-based method that jointly estimates camera poses and reconstructs 3D scenes from extremely sparse-view inputs. Specifically, instead of SfM, we propose a dense stereo module to progressively estimates camera pose information and reconstructs a global dense point cloud for initialization. To address the inherent problem of information scarcity in extremely sparse-view settings, we propose a coherent view interpolation module that interpolates camera poses based on training view pairs and generates viewpoint-consistent content as additional supervision signals for training. Furthermore, we introduce multi-scale Laplacian consistent regularization and adaptive spatial-aware multi-scale geometry regularization to enhance the quality of geometrical structures and rendered content. Experiments show that our method significantly outperforms other state-of-the-art 3DGS-based approaches, achieving a remarkable 2.75dB improvement in PSNR under extremely sparse-view conditions (using only 2 training views). The images synthesized by our method exhibit minimal distortion while preserving rich high-frequency details, resulting in superior visual quality compared to existing techniques.

</details>


### [15] [Multi-Object Sketch Animation with Grouping and Motion Trajectory Priors](https://arxiv.org/abs/2508.15535)
*Guotao Liang,Juncheng Hu,Ximing Xing,Jing Zhang,Qian Yu*

Main category: cs.CV

TL;DR: GroupSketch是一种新颖的矢量草图动画方法，通过两阶段流程（运动初始化和运动细化）有效处理多目标交互和复杂运动，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多目标交互和复杂运动时存在局限，要么仅限于单目标情况，要么存在时间不一致性和泛化能力差的问题。

Method: 采用两阶段流程：1）运动初始化阶段交互式分割语义组并定义关键帧，通过插值生成粗略动画；2）运动细化阶段使用基于组的位移网络（GDN）预测组特定位移场，并整合上下文条件特征增强（CCFE）等模块提升时间一致性。

Result: 大量实验表明，该方法在生成复杂多目标草图的高质量、时间一致动画方面显著优于现有方法。

Conclusion: GroupSketch扩展了草图动画的实际应用范围，为处理多目标交互和复杂运动提供了有效解决方案。

Abstract: We introduce GroupSketch, a novel method for vector sketch animation that effectively handles multi-object interactions and complex motions. Existing approaches struggle with these scenarios, either being limited to single-object cases or suffering from temporal inconsistency and poor generalization. To address these limitations, our method adopts a two-stage pipeline comprising Motion Initialization and Motion Refinement. In the first stage, the input sketch is interactively divided into semantic groups and key frames are defined, enabling the generation of a coarse animation via interpolation. In the second stage, we propose a Group-based Displacement Network (GDN), which refines the coarse animation by predicting group-specific displacement fields, leveraging priors from a text-to-video model. GDN further incorporates specialized modules, such as Context-conditioned Feature Enhancement (CCFE), to improve temporal consistency. Extensive experiments demonstrate that our approach significantly outperforms existing methods in generating high-quality, temporally consistent animations for complex, multi-object sketches, thus expanding the practical applications of sketch animation.

</details>


### [16] [When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding](https://arxiv.org/abs/2508.15641)
*Pengcheng Fang,Yuxia Chen,Rui Guo*

Main category: cs.CV

TL;DR: Grounded VideoDiT是一个视频大语言模型，通过扩散时间潜在编码器、对象接地表示和混合令牌方案，显著提升了视频时间感知和实体交互的精确度。


<details>
  <summary>Details</summary>
Motivation: 现有视频LLM在时间感知方面存在局限：时间戳编码隐式、帧级特征连续性差、语言视觉对齐漂移，需要更精确的时间定位和实体交互理解能力。

Method: 提出三个关键创新：1）扩散时间潜在编码器增强边界敏感性和时间一致性；2）对象接地表示将查询实体显式绑定到局部视觉证据；3）混合令牌方案使用离散时间令牌进行显式时间戳建模。

Result: 在Charades STA、NExT GQA和多个VideoQA基准测试中取得了最先进的结果，验证了模型的强大接地能力。

Conclusion: Grounded VideoDiT通过创新的时间编码和实体绑定机制，成功解决了视频理解中的时间感知精度问题，为细粒度时间推理提供了有效解决方案。

Abstract: Understanding videos requires more than answering open ended questions, it demands the ability to pinpoint when events occur and how entities interact across time. While recent Video LLMs have achieved remarkable progress in holistic reasoning, they remain coarse in temporal perception: timestamps are encoded only implicitly, frame level features are weak in capturing continuity, and language vision alignment often drifts from the entities of interest. In this paper, we present Grounded VideoDiT, a Video LLM designed to overcome these limitations by introducing three key innovations. First, a Diffusion Temporal Latent (DTL) encoder enhances boundary sensitivity and maintains temporal consistency. Second, object grounded representations explicitly bind query entities to localized visual evidence, strengthening alignment. Third, a mixed token scheme with discrete temporal tokens provides explicit timestamp modeling, enabling fine grained temporal reasoning. Together, these designs equip Grounded VideoDiT with robust grounding capabilities, as validated by state of the art results on Charades STA, NExT GQA, and multiple VideoQA benchmarks.

</details>


### [17] [WorldWeaver: Generating Long-Horizon Video Worlds via Rich Perception](https://arxiv.org/abs/2508.15720)
*Zhiheng Liu,Xueqing Deng,Shoufa Chen,Angtian Wang,Qiushan Guo,Mingfei Han,Zeyue Xue,Mengzhao Chen,Ping Luo,Linjie Yang*

Main category: cs.CV

TL;DR: WorldWeaver是一个用于生成长视频的框架，通过联合建模RGB帧和感知条件来增强时间一致性和运动动态，利用深度线索减少漂移，并采用分段噪声调度降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前生成视频建模方法主要依赖RGB信号，在长序列中会导致对象结构和运动误差累积，需要解决时间一致性和结构保持的问题。

Method: 提出统一的长时域建模方案，联合预测感知条件和颜色信息；利用深度线索构建记忆库保持上下文信息；采用分段噪声调度训练预测组。

Result: 在基于扩散和整流流的模型上进行广泛实验，证明WorldWeaver能有效减少时间漂移并提高生成视频的保真度。

Conclusion: 该框架通过多模态联合建模和创新的训练策略，显著提升了长视频生成的质量和时间一致性。

Abstract: Generative video modeling has made significant strides, yet ensuring structural and temporal consistency over long sequences remains a challenge. Current methods predominantly rely on RGB signals, leading to accumulated errors in object structure and motion over extended durations. To address these issues, we introduce WorldWeaver, a robust framework for long video generation that jointly models RGB frames and perceptual conditions within a unified long-horizon modeling scheme. Our training framework offers three key advantages. First, by jointly predicting perceptual conditions and color information from a unified representation, it significantly enhances temporal consistency and motion dynamics. Second, by leveraging depth cues, which we observe to be more resistant to drift than RGB, we construct a memory bank that preserves clearer contextual information, improving quality in long-horizon video generation. Third, we employ segmented noise scheduling for training prediction groups, which further mitigates drift and reduces computational cost. Extensive experiments on both diffusion- and rectified flow-based models demonstrate the effectiveness of WorldWeaver in reducing temporal drift and improving the fidelity of generated videos.

</details>


### [18] [Waver: Wave Your Way to Lifelike Video Generation](https://arxiv.org/abs/2508.15761)
*Yifu Zhang,Hao Yang,Yuqi Zhang,Yifei Hu,Fengda Zhu,Chuang Lin,Xiaofeng Mei,Yi Jiang,Zehuan Yuan,Bingyue Peng*

Main category: cs.CV

TL;DR: Waver是一个高性能的基础模型，能够统一进行图像和视频生成，支持文本到视频、图像到视频和文本到图像生成，在多个基准测试中排名前三。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有视频生成模型在运动捕捉、时间一致性和多模态统一生成方面的挑战，开发一个能够同时处理图像和视频生成的高性能基础模型。

Method: 采用混合流DiT架构增强模态对齐和加速训练收敛，建立全面的数据筛选流程，使用MLLM视频质量模型过滤高质量样本，并提供详细的训练和推理方案。

Result: 模型能够直接生成5-10秒的720p原生分辨率视频（可升级至1080p），在复杂运动捕捉方面表现优异，在T2V和I2V排行榜上均位列前三，超越现有开源模型并媲美商业解决方案。

Conclusion: Waver为社区提供了高效训练高质量视频生成模型的技术路径，有望加速视频生成技术的发展，相关代码已开源供研究使用。

Abstract: We present Waver, a high-performance foundation model for unified image and video generation. Waver can directly generate videos with durations ranging from 5 to 10 seconds at a native resolution of 720p, which are subsequently upscaled to 1080p. The model simultaneously supports text-to-video (T2V), image-to-video (I2V), and text-to-image (T2I) generation within a single, integrated framework. We introduce a Hybrid Stream DiT architecture to enhance modality alignment and accelerate training convergence. To ensure training data quality, we establish a comprehensive data curation pipeline and manually annotate and train an MLLM-based video quality model to filter for the highest-quality samples. Furthermore, we provide detailed training and inference recipes to facilitate the generation of high-quality videos. Building on these contributions, Waver excels at capturing complex motion, achieving superior motion amplitude and temporal consistency in video synthesis. Notably, it ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial Analysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming existing open-source models and matching or surpassing state-of-the-art commercial solutions. We hope this technical report will help the community more efficiently train high-quality video generation models and accelerate progress in video generation technologies. Official page: https://github.com/FoundationVision/Waver.

</details>


### [19] [Visual Autoregressive Modeling for Instruction-Guided Image Editing](https://arxiv.org/abs/2508.15772)
*Qingyang Mao,Qi Cai,Yehao Li,Yingwei Pan,Mingyue Cheng,Ting Yao,Qi Liu,Tao Mei*

Main category: cs.CV

TL;DR: VAREdit是一个基于视觉自回归模型的图像编辑框架，通过多尺度特征预测和尺度对齐参考模块，解决了扩散模型在指令引导编辑中的全局纠缠问题，实现了更高的编辑精度和效率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在指令引导图像编辑中存在全局去噪过程导致编辑区域与整个图像上下文纠缠的问题，会产生意外的伪修改并影响对编辑指令的遵循。自回归模型通过序列化合成过程提供了更好的组合机制。

Method: 提出VAREdit框架，将图像编辑重新定义为下一尺度预测问题。通过源图像特征和文本指令条件化，生成多尺度目标特征。引入尺度对齐参考(SAR)模块，在第一个自注意力层注入尺度匹配的条件信息。

Result: 在标准基准测试中，VAREdit比领先的基于扩散的方法高出30%以上的GPT-Balance分数。完成512×512图像编辑仅需1.2秒，比同等规模的UltraEdit快2.2倍。

Conclusion: VAREdit通过自回归范式和多尺度条件化机制，显著提升了图像编辑的指令遵循性和效率，为指令引导图像编辑提供了新的解决方案。

Abstract: Recent advances in diffusion models have brought remarkable visual fidelity to instruction-guided image editing. However, their global denoising process inherently entangles the edited region with the entire image context, leading to unintended spurious modifications and compromised adherence to editing instructions. In contrast, autoregressive models offer a distinct paradigm by formulating image synthesis as a sequential process over discrete visual tokens. Their causal and compositional mechanism naturally circumvents the adherence challenges of diffusion-based methods. In this paper, we present VAREdit, a visual autoregressive (VAR) framework that reframes image editing as a next-scale prediction problem. Conditioned on source image features and text instructions, VAREdit generates multi-scale target features to achieve precise edits. A core challenge in this paradigm is how to effectively condition the source image tokens. We observe that finest-scale source features cannot effectively guide the prediction of coarser target features. To bridge this gap, we introduce a Scale-Aligned Reference (SAR) module, which injects scale-matched conditioning information into the first self-attention layer. VAREdit demonstrates significant advancements in both editing adherence and efficiency. On standard benchmarks, it outperforms leading diffusion-based methods by 30\%+ higher GPT-Balance score. Moreover, it completes a $512\times512$ editing in 1.2 seconds, making it 2.2$\times$ faster than the similarly sized UltraEdit. The models are available at https://github.com/HiDream-ai/VAREdit.

</details>


### [20] [CineScale: Free Lunch in High-Resolution Cinematic Visual Generation](https://arxiv.org/abs/2508.15774)
*Haonan Qiu,Ning Yu,Ziqi Huang,Paul Debevec,Ziwei Liu*

Main category: cs.CV

TL;DR: CineScale是一种新的推理范式，能够实现更高分辨率的视觉生成，无需微调即可生成8K图像，仅需少量LoRA微调即可生成4K视频。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉扩散模型由于缺乏高分辨率数据和计算资源限制，通常在有限分辨率下训练，导致生成高分辨率内容时出现重复模式和低质量问题。

Method: 针对两种视频生成架构提出专门的变体，构建在开源视频生成框架之上，通过新的推理范式解决高分辨率生成时高频信息增加导致的累积误差问题。

Result: 实验验证了该范式在扩展图像和视频模型高分辨率生成能力方面的优越性，实现了8K图像生成和4K视频生成。

Conclusion: CineScale成功扩展了预训练模型的高分辨率视觉生成能力，为T2I、T2V、I2V和V2V合成提供了有效的解决方案。

Abstract: Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained models. However, these methods are still prone to producing low-quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. In this work, we propose CineScale, a novel inference paradigm to enable higher-resolution visual generation. To tackle the various issues introduced by the two types of video generation architectures, we propose dedicated variants tailored to each. Unlike existing baseline methods that are confined to high-resolution T2I and T2V generation, CineScale broadens the scope by enabling high-resolution I2V and V2V synthesis, built atop state-of-the-art open-source video generation frameworks. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Remarkably, our approach enables 8k image generation without any fine-tuning, and achieves 4k video generation with only minimal LoRA fine-tuning. Generated video samples are available at our website: https://eyeline-labs.github.io/CineScale/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [21] [Probability Density from Latent Diffusion Models for Out-of-Distribution Detection](https://arxiv.org/abs/2508.15737)
*Joonas Järve,Karl Kaspar Haavel,Meelis Kull*

Main category: cs.LG

TL;DR: 本文探讨了生成模型中基于似然的OOD检测方法，通过在预训练ResNet-18表示空间上训练变分扩散模型来评估性能，并与OpenOOD套件中的最先进方法进行比较。


<details>
  <summary>Details</summary>
Motivation: 尽管AI快速发展，安全性仍是部署机器学习系统的主要瓶颈。OOD检测是关键安全组件，而似然在理论上是最优OOD检测器，但实践中经常失败，需要探究这是表示空间的问题还是像素空间的问题。

Method: 在预训练ResNet-18的表示空间上训练变分扩散模型，使用基于似然的OOD检测方法，并与OpenOOD套件中的最先进方法进行性能比较。

Result: 论文通过实验验证了在表示空间中基于似然的OOD检测方法的性能表现，但具体结果数据未在摘要中提供。

Conclusion: 研究旨在确定表示空间是否也存在无法学习良好密度估计的问题，或者仅仅是生成模型中通常使用的像素空间的问题，为改进OOD检测提供新的视角。

Abstract: Despite rapid advances in AI, safety remains the main bottleneck to deploying machine-learning systems. A critical safety component is out-of-distribution detection: given an input, decide whether it comes from the same distribution as the training data. In generative models, the most natural OOD score is the data likelihood. Actually, under the assumption of uniformly distributed OOD data, the likelihood is even the optimal OOD detector, as we show in this work. However, earlier work reported that likelihood often fails in practice, raising doubts about its usefulness. We explore whether, in practice, the representation space also suffers from the inability to learn good density estimation for OOD detection, or if it is merely a problem of the pixel space typically used in generative models. To test this, we trained a Variational Diffusion Model not on images, but on the representation space of a pre-trained ResNet-18 to assess the performance of our likelihood-based detector in comparison to state-of-the-art methods from the OpenOOD suite.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [22] [See it. Say it. Sorted: Agentic System for Compositional Diagram Generation](https://arxiv.org/abs/2508.15222)
*Hantao Zhang,Jingyang Liu,Ed Li*

Main category: cs.AI

TL;DR: 这是一个训练免费的代理系统See it. Say it. Sorted.，通过VLM和LLM协作将手绘草图转换为精确的可编辑SVG图表，在流程图生成上超越了GPT-5和Gemini-2.5-Pro。


<details>
  <summary>Details</summary>
Motivation: 散布模型在照片实际效果上表现优异，但在空间精度、对齐和符号结构方面遇到困难，特别是流程图生成。

Method: 设计了一个迭代循环系统：Critic VLM提出定性关系编辑建议，多个LLM候选生成不同策略的SVG更新，Judge VLM选择最佳方案，确保稳定改进。

Result: 在10个来自发表论文的流程图草图上，该方法比GPT-5和Gemini-2.5-Pro更准确地重建了布局和结构，准确组合原语（如多头箭头）而不插入不应有的文本。

Conclusion: 该系统重视定性推理而非脆弱的数值估计，保持全局约束，支持人在循环中的修正，且由于输出为程序化SVG，方法易于扩展到展示工具。

Abstract: We study sketch-to-diagram generation: converting rough hand sketches into precise, compositional diagrams. Diffusion models excel at photorealism but struggle with the spatial precision, alignment, and symbolic structure required for flowcharts. We introduce See it. Say it. Sorted., a training-free agentic system that couples a Vision-Language Model (VLM) with Large Language Models (LLMs) to produce editable Scalable Vector Graphics (SVG) programs. The system runs an iterative loop in which a Critic VLM proposes a small set of qualitative, relational edits; multiple candidate LLMs synthesize SVG updates with diverse strategies (conservative->aggressive, alternative, focused); and a Judge VLM selects the best candidate, ensuring stable improvement. This design prioritizes qualitative reasoning over brittle numerical estimates, preserves global constraints (e.g., alignment, connectivity), and naturally supports human-in-the-loop corrections. On 10 sketches derived from flowcharts in published papers, our method more faithfully reconstructs layout and structure than two frontier closed-source image generation LLMs (GPT-5 and Gemini-2.5-Pro), accurately composing primitives (e.g., multi-headed arrows) without inserting unwanted text. Because outputs are programmatic SVGs, the approach is readily extensible to presentation tools (e.g., PowerPoint) via APIs and can be specialized with improved prompts and task-specific tools. The codebase is open-sourced at https://github.com/hantaoZhangrichard/see_it_say_it_sorted.git.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [23] [Scalable FPGA Framework for Real-Time Denoising in High-Throughput Imaging: A DRAM-Optimized Pipeline using High-Level Synthesis](https://arxiv.org/abs/2508.14917)
*Weichien Liao*

Main category: cs.AR

TL;DR: FPGA实时预处理流水线，用于高通量成像数据的去噪和降维，通过HLS实现低延迟处理


<details>
  <summary>Details</summary>
Motivation: 高通量成像技术（如PRISM）产生数据的速度超过传统实时处理能力，需要专门的硬件加速解决方案

Method: 基于FPGA的可扩展预处理流水线，使用高级综合（HLS）实现，采用DRAM缓冲和突发模式AXI4接口进行帧减法和平均操作

Result: 内核操作时间低于帧间间隔，实现内联去噪并减少下游CPU/GPU分析的数据集大小，在PRISM规模采集下验证有效

Conclusion: 该模块化FPGA框架为光谱学和显微镜学中的延迟敏感成像工作流程提供了实用解决方案

Abstract: High-throughput imaging workflows, such as Parallel Rapid Imaging with Spectroscopic Mapping (PRISM), generate data at rates that exceed conventional real-time processing capabilities. We present a scalable FPGA-based preprocessing pipeline for real-time denoising, implemented via High-Level Synthesis (HLS) and optimized for DRAM-backed buffering. Our architecture performs frame subtraction and averaging directly on streamed image data, minimizing latency through burst-mode AXI4 interfaces. The resulting kernel operates below the inter-frame interval, enabling inline denoising and reducing dataset size for downstream CPU/GPU analysis. Validated under PRISM-scale acquisition, this modular FPGA framework offers a practical solution for latency-sensitive imaging workflows in spectroscopy and microscopy.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [24] [Pixels Under Pressure: Exploring Fine-Tuning Paradigms for Foundation Models in High-Resolution Medical Imaging](https://arxiv.org/abs/2508.14931)
*Zahra TehraniNasab,Amar Kumar,Tal Arbel*

Main category: eess.IV

TL;DR: 本文系统研究了不同微调技术对512x512高分辨率图像生成质量的影响，比较了全微调和参数高效微调方法，发现特定微调策略能同时提升生成保真度和下游分类任务性能。


<details>
  <summary>Details</summary>
Motivation: 随着高分辨率图像合成在医疗影像等领域的重要性日益增加，需要研究如何通过微调技术将预训练的扩散基础模型适配到特定任务和数据分布，以提升高分辨率图像生成质量。

Method: 系统性地评估了多种微调技术，包括全微调策略和参数高效微调(PEFT)方法，分析了不同微调方法对FID、Vendi分数和提示-图像对齐等关键质量指标的影响，并在数据稀缺条件下评估生成图像在下游分类任务中的实用性。

Result: 研究发现特定的微调策略能够同时改善生成保真度和下游性能，当使用合成图像进行分类器训练并在真实图像上评估时，这些策略表现出更好的效果。

Conclusion: 微调是提升高分辨率图像生成质量的有效方法，特定微调策略能够显著改善生成图像的保真度和下游任务性能，为医疗影像等领域的应用提供了重要参考。

Abstract: Advancements in diffusion-based foundation models have improved text-to-image generation, yet most efforts have been limited to low-resolution settings. As high-resolution image synthesis becomes increasingly essential for various applications, particularly in medical imaging domains, fine-tuning emerges as a crucial mechanism for adapting these powerful pre-trained models to task-specific requirements and data distributions. In this work, we present a systematic study, examining the impact of various fine-tuning techniques on image generation quality when scaling to high resolution 512x512 pixels. We benchmark a diverse set of fine-tuning methods, including full fine-tuning strategies and parameter-efficient fine-tuning (PEFT). We dissect how different fine-tuning methods influence key quality metrics, including Fr\'echet Inception Distance (FID), Vendi score, and prompt-image alignment. We also evaluate the utility of generated images in a downstream classification task under data-scarce conditions, demonstrating that specific fine-tuning strategies improve both generation fidelity and downstream performance when synthetic images are used for classifier training and evaluation on real images. Our code is accessible through the project website - https://tehraninasab.github.io/PixelUPressure/.

</details>


### [25] [Zero-shot Volumetric CT Super-Resolution using 3D Gaussian Splatting with Upsampled 2D X-ray Projection Priors](https://arxiv.org/abs/2508.15151)
*Jeonghyun Noh,Hyun-Jic Oh,Byungju Chae,Won-Ki Jeong*

Main category: eess.IV

TL;DR: 一种新的无监督3D CT超分辨玟构框架，利用激光扩散2D X光投影作为外部先验知识，通过3D高斯拟合和负值融合技术提高细节恢复效果


<details>
  <summary>Details</summary>
Motivation: 解决传统监督方法需要大量成对LR-HR数据集以及零检查方法内部信息有限无法恢复细微解剖细节的问题

Method: 使用激光扩散模型生成高分辨率2D X光投影作为外部先验，通过每张投影适配采样策略选择生成过程，然后用3D高斯拟合重建3D CT体积，并提出支持负值的负值融合技术以促进高频结构恢复

Result: 在两个数据集上进行实验，该方法在数量和质量上都获得了优异的3D CT超分辨玟构结果

Conclusion: 该框架成功结合了外部先验知识和高级重建技术，为无监督3D CT超分辨玟构提供了有效解决方案，能够恢复细微的解剖细节

Abstract: Computed tomography (CT) is widely used in clinical diagnosis, but acquiring high-resolution (HR) CT is limited by radiation exposure risks. Deep learning-based super-resolution (SR) methods have been studied to reconstruct HR from low-resolution (LR) inputs. While supervised SR approaches have shown promising results, they require large-scale paired LR-HR volume datasets that are often unavailable. In contrast, zero-shot methods alleviate the need for paired data by using only a single LR input, but typically struggle to recover fine anatomical details due to limited internal information. To overcome these, we propose a novel zero-shot 3D CT SR framework that leverages upsampled 2D X-ray projection priors generated by a diffusion model. Exploiting the abundance of HR 2D X-ray data, we train a diffusion model on large-scale 2D X-ray projection and introduce a per-projection adaptive sampling strategy. It selects the generative process for each projection, thus providing HR projections as strong external priors for 3D CT reconstruction. These projections serve as inputs to 3D Gaussian splatting for reconstructing a 3D CT volume. Furthermore, we propose negative alpha blending (NAB-GS) that allows negative values in Gaussian density representation. NAB-GS enables residual learning between LR and diffusion-based projections, thereby enhancing high-frequency structure reconstruction. Experiments on two datasets show that our method achieves superior quantitative and qualitative results for 3D CT SR.

</details>


### [26] [Pathology-Informed Latent Diffusion Model for Anomaly Detection in Lymph Node Metastasis](https://arxiv.org/abs/2508.15236)
*Jiamu Wang,Keunho Byeon,Jinsol Song,Anh Nguyen,Sangjeong Ahn,Sung Hak Lee,Jin Tae Kwak*

Main category: eess.IV

TL;DR: 提出了一种结合视觉语言模型和扩散模型的数字病理学无监督异常检测方法，利用组织病理学提示指导重建过程，在胃和乳腺淋巴结数据集上验证了有效性


<details>
  <summary>Details</summary>
Motivation: 数字病理学中监督学习方法需要大量标注数据，但数据稀缺问题严重。无监督异常检测能够识别正常组织分布的偏差，而无需详尽标注。扩散概率模型在无监督异常检测中表现出色，但尚未充分结合视觉语言模型的能力

Method: 将视觉语言模型与扩散模型结合，在重建过程中使用一组与正常组织相关的病理学关键词作为提示，引导重建过程，从而区分正常和异常组织

Result: 在本地医院胃淋巴结数据集和公共乳腺淋巴结数据集上的实验表明，该方法在不同器官的数字病理学无监督异常检测中具有良好性能，并展现出在域偏移情况下的泛化能力

Conclusion: 所提出的方法展示了在数字病理学中跨器官无监督异常检测的潜力，为利用视觉语言模型和扩散模型的结合提供了新的思路，代码已开源

Abstract: Anomaly detection is an emerging approach in digital pathology for its ability to efficiently and effectively utilize data for disease diagnosis. While supervised learning approaches deliver high accuracy, they rely on extensively annotated datasets, suffering from data scarcity in digital pathology. Unsupervised anomaly detection, however, offers a viable alternative by identifying deviations from normal tissue distributions without requiring exhaustive annotations. Recently, denoising diffusion probabilistic models have gained popularity in unsupervised anomaly detection, achieving promising performance in both natural and medical imaging datasets. Building on this, we incorporate a vision-language model with a diffusion model for unsupervised anomaly detection in digital pathology, utilizing histopathology prompts during reconstruction. Our approach employs a set of pathology-related keywords associated with normal tissues to guide the reconstruction process, facilitating the differentiation between normal and abnormal tissues. To evaluate the effectiveness of the proposed method, we conduct experiments on a gastric lymph node dataset from a local hospital and assess its generalization ability under domain shift using a public breast lymph node dataset. The experimental results highlight the potential of the proposed method for unsupervised anomaly detection across various organs in digital pathology. Code: https://github.com/QuIIL/AnoPILaD.

</details>


### [27] [Deep Equilibrium Convolutional Sparse Coding for Hyperspectral Image Denoising](https://arxiv.org/abs/2508.15553)
*Jin Ye,Jingran Wang,Fengchao Xiong,Jingzhou Chen,Yuntao Qian*

Main category: eess.IV

TL;DR: 提出基于深度平衡模型的卷积稀疏编码框架DECSC，结合局部空间-光谱相关性、非局部空间自相似性和全局空间一致性，实现高光谱图像去噪


<details>
  <summary>Details</summary>
Motivation: 现有深度展开方法缺乏收敛保证，而深度平衡模型自然符合优化过程，能够提供无限深度网络表示

Method: 在卷积稀疏编码框架中，使用共享2D卷积稀疏表示确保频带间全局空间一致性，非共享3D卷积稀疏表示捕获局部空间-光谱细节，嵌入Transformer块利用非局部自相似性，并集成细节增强模块

Result: 实验结果表明DECSC方法相比最先进方法实现了更优的去噪性能

Conclusion: 基于深度平衡模型的DECSC框架有效统一了多种特征表示，为高光谱图像去噪提供了强大解决方案

Abstract: Hyperspectral images (HSIs) play a crucial role in remote sensing but are often degraded by complex noise patterns. Ensuring the physical property of the denoised HSIs is vital for robust HSI denoising, giving the rise of deep unfolding-based methods. However, these methods map the optimization of a physical model to a learnable network with a predefined depth, which lacks convergence guarantees. In contrast, Deep Equilibrium (DEQ) models treat the hidden layers of deep networks as the solution to a fixed-point problem and models them as infinite-depth networks, naturally consistent with the optimization. Under the framework of DEQ, we propose a Deep Equilibrium Convolutional Sparse Coding (DECSC) framework that unifies local spatial-spectral correlations, nonlocal spatial self-similarities, and global spatial consistency for robust HSI denoising. Within the convolutional sparse coding (CSC) framework, we enforce shared 2D convolutional sparse representation to ensure global spatial consistency across bands, while unshared 3D convolutional sparse representation captures local spatial-spectral details. To further exploit nonlocal self-similarities, a transformer block is embedded after the 2D CSC. Additionally, a detail enhancement module is integrated with the 3D CSC to promote image detail preservation. We formulate the proximal gradient descent of the CSC model as a fixed-point problem and transform the iterative updates into a learnable network architecture within the framework of DEQ. Experimental results demonstrate that our DECSC method achieves superior denoising performance compared to state-of-the-art methods.

</details>


### [28] [Are Virtual DES Images a Valid Alternative to the Real Ones?](https://arxiv.org/abs/2508.15594)
*Ana C. Perre,Luís A. Alexandre,Luís C. Freire*

Main category: eess.IV

TL;DR: 本研究评估了三种模型（预训练U-Net、端到端训练U-Net、CycleGAN）从低能量图像生成虚拟双能量减影图像的能力，并测试了这些虚拟图像在乳腺病变良恶性分类中的效果。


<details>
  <summary>Details</summary>
Motivation: 通过图像到图像转换技术从低能量图像生成双能量减影图像，可以减少患者在高能量图像采集过程中的辐射暴露，具有重要的临床应用价值。

Method: 使用三种不同的深度学习模型（预训练U-Net、端到端训练U-Net、CycleGAN）生成虚拟DES图像，并评估这些图像在CESM检查分类任务中的性能表现。

Result: 预训练U-Net模型表现最佳，使用虚拟DES图像获得85.59%的F1分数，而使用真实DES图像为90.35%，存在4.76%的性能差距。

Conclusion: 虚拟DES图像生成具有巨大潜力，虽然目前与真实图像存在性能差距，但未来技术进步可能使其达到临床可用水平，从而减少患者辐射暴露。

Abstract: Contrast-enhanced spectral mammography (CESM) is an imaging modality that provides two types of images, commonly known as low-energy (LE) and dual-energy subtracted (DES) images. In many domains, particularly in medicine, the emergence of image-to-image translation techniques has enabled the artificial generation of images using other images as input. Within CESM, applying such techniques to generate DES images from LE images could be highly beneficial, potentially reducing patient exposure to radiation associated with high-energy image acquisition. In this study, we investigated three models for the artificial generation of DES images (virtual DES): a pre-trained U-Net model, a U-Net trained end-to-end model, and a CycleGAN model. We also performed a series of experiments to assess the impact of using virtual DES images on the classification of CESM examinations into malignant and non-malignant categories. To our knowledge, this is the first study to evaluate the impact of virtual DES images on CESM lesion classification. The results demonstrate that the best performance was achieved with the pre-trained U-Net model, yielding an F1 score of 85.59% when using the virtual DES images, compared to 90.35% with the real DES images. This discrepancy likely results from the additional diagnostic information in real DES images, which contributes to a higher classification accuracy. Nevertheless, the potential for virtual DES image generation is considerable and future advancements may narrow this performance gap to a level where exclusive reliance on virtual DES images becomes clinically viable.

</details>
