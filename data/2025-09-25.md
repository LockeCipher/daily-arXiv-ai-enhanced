<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 19]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Enhancing Transformer-Based Vision Models: Addressing Feature Map Anomalies Through Novel Optimization Strategies](https://arxiv.org/abs/2509.19687)
*Sumit Mamtani*

Main category: cs.CV

TL;DR: 提出了两种轻量级优化技术STA和ANF，用于改善ViT的特征图质量，减少结构化噪声伪影，提升下游任务性能


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在特征图中存在结构化噪声伪影，这阻碍了分割和深度估计等下游应用的效果

Method: 结构化令牌增强(STA)在令牌化过程中通过空间扰动增强令牌多样性，自适应噪声滤波(ANF)在transformer层间应用可学习的在线去噪

Result: 在ImageNet、Ade20k和NYUv2等标准基准测试中，实验结果显示视觉质量和任务性能均获得一致提升

Conclusion: 所提出的STA和ANF方法是架构无关的，具有实际有效性，能够显著改善ViT的特征表示质量

Abstract: Vision Transformers (ViTs) have demonstrated superior performance across a wide range of computer vision tasks. However, structured noise artifacts in their feature maps hinder downstream applications such as segmentation and depth estimation. We propose two novel and lightweight optimisation techniques- Structured Token Augmentation (STA) and Adaptive Noise Filtering (ANF)- to improve interpretability and mitigate these artefacts. STA enhances token diversity through spatial perturbations during tokenisation, while ANF applies learnable inline denoising between transformer layers. These methods are architecture-agnostic and evaluated across standard benchmarks, including ImageNet, Ade20k, and NYUv2. Experimental results show consistent improvements in visual quality and task performance, highlighting the practical effectiveness of our approach.

</details>


### [2] [From Prompt to Progression: Taming Video Diffusion Models for Seamless Attribute Transition](https://arxiv.org/abs/2509.19690)
*Ling Lo,Kelvin C. K. Chan,Wen-Huang Cheng,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 本文提出了一种通过引入帧级引导来扩展现有模型的方法，以实现平滑一致的属性过渡，解决了现有模型在处理复杂时间变化时的不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型在处理复杂时间变化时存在困难，特别是在生成具有逐渐属性过渡的视频时。常见的提示插值方法往往无法处理逐渐属性过渡，导致不一致性更加明显。

Method: 通过在去噪过程中引入帧级引导，为每个噪声潜在空间构建数据特定的过渡方向，逐帧引导从初始属性到最终属性的逐渐转变，同时保持视频的运动动态。

Result: 实验结果表明，该方法在视觉保真度、与文本提示的对齐以及无缝属性过渡方面优于现有基线。

Conclusion: 提出的方法简单有效，能够实现平滑一致的属性过渡，并发布了CAT-Bench基准测试和评估指标来全面评估模型性能。

Abstract: Existing models often struggle with complex temporal changes, particularly when generating videos with gradual attribute transitions. The most common prompt interpolation approach for motion transitions often fails to handle gradual attribute transitions, where inconsistencies tend to become more pronounced. In this work, we propose a simple yet effective method to extend existing models for smooth and consistent attribute transitions, through introducing frame-wise guidance during the denoising process. Our approach constructs a data-specific transitional direction for each noisy latent, guiding the gradual shift from initial to final attributes frame by frame while preserving the motion dynamics of the video. Moreover, we present the Controlled-Attribute-Transition Benchmark (CAT-Bench), which integrates both attribute and motion dynamics, to comprehensively evaluate the performance of different models. We further propose two metrics to assess the accuracy and smoothness of attribute transitions. Experimental results demonstrate that our approach performs favorably against existing baselines, achieving visual fidelity, maintaining alignment with text prompts, and delivering seamless attribute transitions. Code and CATBench are released: https://github.com/lynn-ling-lo/Prompt2Progression.

</details>


### [3] [PolGS: Polarimetric Gaussian Splatting for Fast Reflective Surface Reconstruction](https://arxiv.org/abs/2509.19726)
*Yufei Han,Bowen Tie,Heng Guo,Youwei Lyu,Si Li,Boxin Shi,Yunpeng Jia,Zhanyu Ma*

Main category: cs.CV

TL;DR: PolGS是一种基于偏振高斯溅射的快速反射表面重建方法，能在10分钟内完成重建，通过偏振约束有效分离镜面和漫反射分量。


<details>
  <summary>Details</summary>
Motivation: 解决3D高斯溅射方法在复杂反射表面重建质量上落后于隐式神经表示的问题，特别是对于具有复杂反射特性的表面。

Method: 将偏振约束集成到3D高斯溅射框架中，有效分离镜面和漫反射分量。

Result: 在合成和真实世界数据集上的实验验证了该方法的有效性。

Conclusion: PolGS通过偏振约束显著提升了复杂反射表面的重建质量，同时保持了快速重建的优势。

Abstract: Efficient shape reconstruction for surfaces with complex reflectance properties is crucial for real-time virtual reality. While 3D Gaussian Splatting (3DGS)-based methods offer fast novel view rendering by leveraging their explicit surface representation, their reconstruction quality lags behind that of implicit neural representations, particularly in the case of recovering surfaces with complex reflective reflectance. To address these problems, we propose PolGS, a Polarimetric Gaussian Splatting model allowing fast reflective surface reconstruction in 10 minutes. By integrating polarimetric constraints into the 3DGS framework, PolGS effectively separates specular and diffuse components, enhancing reconstruction quality for challenging reflective materials. Experimental results on the synthetic and real-world dataset validate the effectiveness of our method.

</details>


### [4] [EfficienT-HDR: An Efficient Transformer-Based Framework via Multi-Exposure Fusion for HDR Reconstruction](https://arxiv.org/abs/2509.19779)
*Yu-Shen Huang,Tzu-Han Chen,Cheng-Yen Hsiao,Shaou-Gang Miaou*

Main category: cs.CV

TL;DR: 提出一种轻量级视觉Transformer架构，用于在边缘设备上实现高质量HDR成像，通过多曝光融合技术解决计算成本高和重影问题


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上实现高质量HDR成像对智能监控和自动驾驶等下游任务至关重要，现有方法存在计算成本高和重影伪影的双重瓶颈

Method: 基于上下文感知视觉Transformer，将输入图像转换为YCbCr色彩空间，使用交叉感知自适应融合模块抑制重影，引入倒置残差嵌入、动态Tanh和增强多尺度扩张卷积实现轻量化设计

Result: 主要版本相比基线减少约67%的FLOPS，CPU推理速度提升5倍以上，边缘设备提升2.5倍，提供高效无重影的HDR成像解决方案

Conclusion: 该方法在性能和图像质量之间达到良好平衡，为边缘设备提供了多功能且实用的HDR成像方案

Abstract: Achieving high-quality High Dynamic Range (HDR) imaging on resource-constrained edge devices is a critical challenge in computer vision, as its performance directly impacts downstream tasks such as intelligent surveillance and autonomous driving. Multi-Exposure Fusion (MEF) is a mainstream technique to achieve this goal; however, existing methods generally face the dual bottlenecks of high computational costs and ghosting artifacts, hindering their widespread deployment. To this end, this study proposes a light-weight Vision Transformer architecture designed explicitly for HDR reconstruction to overcome these limitations. This study is based on the Context-Aware Vision Transformer and begins by converting input images to the YCbCr color space to separate luminance and chrominance information. It then employs an Intersection-Aware Adaptive Fusion (IAAF) module to suppress ghosting effectively. To further achieve a light-weight design, we introduce Inverted Residual Embedding (IRE), Dynamic Tanh (DyT), and propose Enhanced Multi-Scale Dilated Convolution (E-MSDC) to reduce computational complexity at multiple levels. Our study ultimately contributes two model versions: a main version for high visual quality and a light-weight version with advantages in computational efficiency, both of which achieve an excellent balance between performance and image quality. Experimental results demonstrate that, compared to the baseline, the main version reduces FLOPS by approximately 67% and increases inference speed by more than fivefold on CPU and 2.5 times on an edge device. These results confirm that our method provides an efficient and ghost-free HDR imaging solution for edge devices, demonstrating versatility and practicality across various dynamic scenarios.

</details>


### [5] [BiTAA: A Bi-Task Adversarial Attack for Object Detection and Depth Estimation via 3D Gaussian Splatting](https://arxiv.org/abs/2509.19793)
*Yixun Zhang,Feng Zhou,Jianqin Yin*

Main category: cs.CV

TL;DR: BiTAA是一种基于3D高斯泼溅的双任务对抗攻击方法，能够同时降低目标检测性能并偏置单目深度估计，揭示了自动驾驶多任务感知中的安全风险


<details>
  <summary>Details</summary>
Motivation: 现有2D/3D攻击方法存在任务孤岛问题，缺乏可控深度偏置机制，且没有标准化协议来量化跨任务转移，导致检测与深度任务之间的交互关系未被充分探索

Method: 基于3D高斯泼溅构建双模型攻击框架，支持全图像和补丁设置，兼容常见检测器和深度估计器；设计复合损失函数，将检测抑制与带符号、幅度可控的对数深度偏置耦合，在感兴趣区域内实现可控的远近误感知

Result: 实验显示BiTAA能够实现一致的跨任务性能下降，并揭示了从检测到深度和从深度到检测的明显不对称转移特性

Conclusion: 该方法突显了多任务相机感知的实际风险，并推动了自动驾驶场景中跨任务感知防御机制的发展

Abstract: Camera-based perception is critical to autonomous driving yet remains vulnerable to task-specific adversarial manipulations in object detection and monocular depth estimation. Most existing 2D/3D attacks are developed in task silos, lack mechanisms to induce controllable depth bias, and offer no standardized protocol to quantify cross-task transfer, leaving the interaction between detection and depth underexplored. We present BiTAA, a bi-task adversarial attack built on 3D Gaussian Splatting that yields a single perturbation capable of simultaneously degrading detection and biasing monocular depth. Specifically, we introduce a dual-model attack framework that supports both full-image and patch settings and is compatible with common detectors and depth estimators, with optional expectation-over-transformation (EOT) for physical reality. In addition, we design a composite loss that couples detection suppression with a signed, magnitude-controlled log-depth bias within regions of interest (ROIs) enabling controllable near or far misperception while maintaining stable optimization across tasks. We also propose a unified evaluation protocol with cross-task transfer metrics and real-world evaluations, showing consistent cross-task degradation and a clear asymmetry between Det to Depth and from Depth to Det transfer. The results highlight practical risks for multi-task camera-only perception and motivate cross-task-aware defenses in autonomous driving scenarios.

</details>


### [6] [StrCGAN: A Generative Framework for Stellar Image Restoration](https://arxiv.org/abs/2509.19805)
*Shantanusinh Parmar*

Main category: cs.CV

TL;DR: StrCGAN是一个用于增强低分辨率天体摄影图像的生成模型，通过3D卷积层、多光谱融合和天体物理正则化模块来重建高质量的天体图像。


<details>
  <summary>Details</summary>
Motivation: 解决小型望远镜观测图像分辨率低、质量差的问题，传统CycleGAN模型局限于2D映射且会扭曲恒星和星系的形态。

Method: 扩展CycleGAN框架，引入3D卷积层捕捉体积空间相关性，多光谱融合对齐光学和近红外域，天体物理正则化模块保持恒星形态。

Result: StrCGAN生成的图像不仅视觉上更清晰，而且在物理上更一致，在天体物理图像增强任务中优于标准GAN模型。

Conclusion: StrCGAN通过创新的3D卷积、多光谱融合和物理正则化方法，成功实现了高质量天体图像的重建，为低分辨率天体摄影提供了有效解决方案。

Abstract: We introduce StrCGAN (Stellar Cyclic GAN), a generative model designed to enhance low-resolution astrophotography images. Our goal is to reconstruct high-fidelity ground truth-like representations of celestial objects, a task that is challenging due to the limited resolution and quality of small-telescope observations such as the MobilTelesco dataset. Traditional models such as CycleGAN provide a foundation for image-to-image translation but are restricted to 2D mappings and often distort the morphology of stars and galaxies. To overcome these limitations, we extend the CycleGAN framework with three key innovations: 3D convolutional layers to capture volumetric spatial correlations, multi-spectral fusion to align optical and near-infrared (NIR) domains, and astrophysical regularization modules to preserve stellar morphology. Ground-truth references from multi-mission all-sky surveys spanning optical to NIR guide the training process, ensuring that reconstructions remain consistent across spectral bands. Together, these components allow StrCGAN to generate reconstructions that are not only visually sharper but also physically consistent, outperforming standard GAN models in the task of astrophysical image enhancement.

</details>


### [7] [Aerial-Ground Image Feature Matching via 3D Gaussian Splatting-based Intermediate View Rendering](https://arxiv.org/abs/2509.19898)
*Jiangxue Yu,Hui Wang,San Jiang,Xing Zhang,Dejin Zhang,Qingquan Li*

Main category: cs.CV

TL;DR: 提出一种基于中间视图生成的特征匹配算法，用于解决航空和地面图像之间的视角差异问题，通过3D高斯泼溅生成中间图像来桥接视角差异，提高匹配可靠性。


<details>
  <summary>Details</summary>
Motivation: 航空和地面图像集成在复杂场景3D建模中具有重要应用，但由于视角差异大导致特征匹配困难，需要解决视角畸变问题。

Method: 1. 使用航空图像通过增量SfM重建稀疏模型；2. 采用3D高斯泼溅进行场景渲染；3. 设计渲染视点确定算法生成高质量中间图像；4. 通过中间图像进行可靠的特征匹配。

Result: 实验验证表明该方法能显著增加初始和精炼匹配数量，为准确的ISfM重建和完整的3DGS场景渲染提供足够匹配点。

Conclusion: 所提出的解决方案能够为航空和地面图像提供可靠的特征匹配，有效解决视角差异带来的匹配困难问题。

Abstract: The integration of aerial and ground images has been a promising solution in 3D modeling of complex scenes, which is seriously restricted by finding reliable correspondences. The primary contribution of this study is a feature matching algorithm for aerial and ground images, whose core idea is to generate intermediate views to alleviate perspective distortions caused by the extensive viewpoint changes. First, by using aerial images only, sparse models are reconstructed through an incremental SfM (Structure from Motion) engine due to their large scene coverage. Second, 3D Gaussian Splatting is then adopted for scene rendering by taking as inputs sparse points and oriented images. For accurate view rendering, a render viewpoint determination algorithm is designed by using the oriented camera poses of aerial images, which is used to generate high-quality intermediate images that can bridge the gap between aerial and ground images. Third, with the aid of intermediate images, reliable feature matching is conducted for match pairs from render-aerial and render-ground images, and final matches can be generated by transmitting correspondences through intermediate views. By using real aerial and ground datasets, the validation of the proposed solution has been verified in terms of feature matching and scene rendering and compared comprehensively with widely used methods. The experimental results demonstrate that the proposed solution can provide reliable feature matches for aerial and ground images with an obvious increase in the number of initial and refined matches, and it can provide enough matches to achieve accurate ISfM reconstruction and complete 3DGS-based scene rendering.

</details>


### [8] [GS-RoadPatching: Inpainting Gaussians via 3D Searching and Placing for Driving Scenes](https://arxiv.org/abs/2509.19937)
*Guo Chen,Jiarun Liu,Sicong Du,Chenming Wu,Deqi Li,Shi-Sheng Huang,Guofeng Zhang,Sheng Yang*

Main category: cs.CV

TL;DR: GS-RoadPatching是一种基于3D高斯泼溅的驾驶场景修复方法，通过参考完全重建区域进行替换式场景修复，无需依赖2D跨模态的时空一致性或耗时的高斯重训练。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS修复方法依赖2D视角的扩散或GAN模型预测缺失区域的外观或深度线索，存在时空一致性问题和训练耗时长的局限性。

Method: 构建特征嵌入的3DGS场景，采用多尺度局部上下文抽象和3D空间结构搜索方法，结合简单有效的替换融合优化实现视觉和谐。

Result: 在多个公开数据集上的实验表明，该方法在驾驶场景中实现了最先进的性能，在质量和互操作性方面优于基线方法。

Conclusion: 该方法为3DGS模态的直接场景修复和编辑提供了有效解决方案，并在通用场景中也展示了适用性。

Abstract: This paper presents GS-RoadPatching, an inpainting method for driving scene completion by referring to completely reconstructed regions, which are represented by 3D Gaussian Splatting (3DGS). Unlike existing 3DGS inpainting methods that perform generative completion relying on 2D perspective-view-based diffusion or GAN models to predict limited appearance or depth cues for missing regions, our approach enables substitutional scene inpainting and editing directly through the 3DGS modality, extricating it from requiring spatial-temporal consistency of 2D cross-modals and eliminating the need for time-intensive retraining of Gaussians. Our key insight is that the highly repetitive patterns in driving scenes often share multi-modal similarities within the implicit 3DGS feature space and are particularly suitable for structural matching to enable effective 3DGS-based substitutional inpainting. Practically, we construct feature-embedded 3DGS scenes to incorporate a patch measurement method for abstracting local context at different scales and, subsequently, propose a structural search method to find candidate patches in 3D space effectively. Finally, we propose a simple yet effective substitution-and-fusion optimization for better visual harmony. We conduct extensive experiments on multiple publicly available datasets to demonstrate the effectiveness and efficiency of our proposed method in driving scenes, and the results validate that our method achieves state-of-the-art performance compared to the baseline methods in terms of both quality and interoperability. Additional experiments in general scenes also demonstrate the applicability of the proposed 3D inpainting strategy. The project page and code are available at: https://shanzhaguoo.github.io/GS-RoadPatching/

</details>


### [9] [SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding](https://arxiv.org/abs/2509.19965)
*Phyo Thet Yee,Dimitrios Kollias,Sudeepta Mishra,Abhinav Dhall*

Main category: cs.CV

TL;DR: SynchroRaMa是一个新颖的音频驱动说话人脸生成框架，通过多模态情感嵌入和场景描述来提高情感表达的真实性和动态一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖单模态（音频或图像）进行情感嵌入，且仅基于单张参考图像，难以捕捉细微的情感变化和动态属性变化。

Method: 结合文本情感分析和音频情感识别进行多模态情感嵌入；使用音频到运动模块确保唇部同步；引入LLM生成的场景描述来增强动态动作和语义属性捕捉。

Result: 在基准数据集上的定量和定性实验表明，SynchroRaMa在图像质量、表情保持和运动真实性方面优于现有方法。用户研究也证实其在自然度、运动多样性和视频流畅度方面获得更高主观评分。

Conclusion: SynchroRaMa通过多模态情感嵌入和场景描述条件化，显著提升了说话人脸生成的情感真实性和动态一致性，为人类-虚拟形象交互提供了更自然的解决方案。

Abstract: Audio-driven talking face generation has received growing interest, particularly for applications requiring expressive and natural human-avatar interaction. However, most existing emotion-aware methods rely on a single modality (either audio or image) for emotion embedding, limiting their ability to capture nuanced affective cues. Additionally, most methods condition on a single reference image, restricting the model's ability to represent dynamic changes in actions or attributes across time. To address these issues, we introduce SynchroRaMa, a novel framework that integrates a multi-modal emotion embedding by combining emotional signals from text (via sentiment analysis) and audio (via speech-based emotion recognition and audio-derived valence-arousal features), enabling the generation of talking face videos with richer and more authentic emotional expressiveness and fidelity. To ensure natural head motion and accurate lip synchronization, SynchroRaMa includes an audio-to-motion (A2M) module that generates motion frames aligned with the input audio. Finally, SynchroRaMa incorporates scene descriptions generated by Large Language Model (LLM) as additional textual input, enabling it to capture dynamic actions and high-level semantic attributes. Conditioning the model on both visual and textual cues enhances temporal consistency and visual realism. Quantitative and qualitative experiments on benchmark datasets demonstrate that SynchroRaMa outperforms the state-of-the-art, achieving improvements in image quality, expression preservation, and motion realism. A user study further confirms that SynchroRaMa achieves higher subjective ratings than competing methods in overall naturalness, motion diversity, and video smoothness. Our project page is available at <https://novicemm.github.io/synchrorama>.

</details>


### [10] [CamPVG: Camera-Controlled Panoramic Video Generation with Epipolar-Aware Diffusion](https://arxiv.org/abs/2509.19979)
*Chenhao Ji,Chaohui Yu,Junyao Gao,Fan Wang,Cairong Zhao*

Main category: cs.CV

TL;DR: CamPVG是首个基于扩散模型的、通过精确相机位姿引导的全景视频生成框架，解决了现有方法在几何一致的全景视频生成方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注透视投影视频中的相机控制，而几何一致的全景视频生成由于全景位姿表示和球面投影的复杂性仍然具有挑战性。

Method: 提出全景Plücker嵌入通过球坐标变换编码相机外参，并引入球面极线模块通过沿极线的自适应注意力掩码实现细粒度跨视图特征聚合。

Result: 大量实验表明，该方法能生成与相机轨迹一致的高质量全景视频，在全景视频生成方面远超现有方法。

Conclusion: CamPVG框架通过创新的位姿编码和几何约束模块，成功实现了几何一致的高质量全景视频生成。

Abstract: Recently, camera-controlled video generation has seen rapid development, offering more precise control over video generation. However, existing methods predominantly focus on camera control in perspective projection video generation, while geometrically consistent panoramic video generation remains challenging. This limitation is primarily due to the inherent complexities in panoramic pose representation and spherical projection. To address this issue, we propose CamPVG, the first diffusion-based framework for panoramic video generation guided by precise camera poses. We achieve camera position encoding for panoramic images and cross-view feature aggregation based on spherical projection. Specifically, we propose a panoramic Pl\"ucker embedding that encodes camera extrinsic parameters through spherical coordinate transformation. This pose encoder effectively captures panoramic geometry, overcoming the limitations of traditional methods when applied to equirectangular projections. Additionally, we introduce a spherical epipolar module that enforces geometric constraints through adaptive attention masking along epipolar lines. This module enables fine-grained cross-view feature aggregation, substantially enhancing the quality and consistency of generated panoramic videos. Extensive experiments demonstrate that our method generates high-quality panoramic videos consistent with camera trajectories, far surpassing existing methods in panoramic video generation.

</details>


### [11] [Generative Adversarial Networks Applied for Privacy Preservation in Biometric-Based Authentication and Identification](https://arxiv.org/abs/2509.20024)
*Lubos Mjachky,Ivan Homoliak*

Main category: cs.CV

TL;DR: 提出一种基于GAN的隐私保护人脸认证方法，将人脸图像转换为视觉私有域（如花朵或鞋子），在保护隐私的同时保持认证功能


<details>
  <summary>Details</summary>
Motivation: 传统生物认证系统不允许用户控制数据使用方式，且数据可能泄露被滥用，需要保护个人隐私

Method: 使用生成对抗网络(GAN)将人脸图像转换到视觉私有域，在该域上训练分类器进行认证

Result: 方法对攻击具有鲁棒性，同时保持有意义的实用性

Conclusion: 该方法在保护隐私的同时实现了有效的认证功能

Abstract: Biometric-based authentication systems are getting broadly adopted in many areas. However, these systems do not allow participating users to influence the way their data is used. Furthermore, the data may leak and can be misused without the users' knowledge. In this paper, we propose a new authentication method that preserves the privacy of individuals and is based on a generative adversarial network (GAN). Concretely, we suggest using the GAN for translating images of faces to a visually private domain (e.g., flowers or shoes). Classifiers, which are used for authentication purposes, are then trained on the images from the visually private domain. Based on our experiments, the method is robust against attacks and still provides meaningful utility.

</details>


### [12] [Unleashing the Potential of the Semantic Latent Space in Diffusion Models for Image Dehazing](https://arxiv.org/abs/2509.20091)
*Zizheng Yang,Hu Yu,Bing Li,Jinghao Zhang,Jie Huang,Feng Zhao*

Main category: cs.CV

TL;DR: 本文提出DiffLI²D方法，利用预训练扩散模型的语义潜在空间进行图像去雾，避免了重新训练扩散模型和迭代采样过程。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像去雾中表现出色，但重新训练的计算负担和推理时的多步采样限制了其广泛应用。本文探索了雾霾图像在预训练扩散模型语义潜在空间中的特性。

Method: 首先发现预训练扩散模型的语义潜在空间可以表示雾霾图像的内容和雾霾特征。基于此，将不同时间步的扩散潜在表示集成到精心设计的去雾网络中，为图像去雾提供指导。

Result: 在多个数据集上的实验表明，该方法在图像去雾性能上优于现有方法。

Conclusion: DiffLI²D通过有效利用预训练扩散模型的信息表示，为将扩散模型引入图像去雾提供了新视角，同时避免了重新训练和迭代采样。

Abstract: Diffusion models have recently been investigated as powerful generative solvers for image dehazing, owing to their remarkable capability to model the data distribution. However, the massive computational burden imposed by the retraining of diffusion models, coupled with the extensive sampling steps during the inference, limit the broader application of diffusion models in image dehazing. To address these issues, we explore the properties of hazy images in the semantic latent space of frozen pre-trained diffusion models, and propose a Diffusion Latent Inspired network for Image Dehazing, dubbed DiffLI$^2$D. Specifically, we first reveal that the semantic latent space of pre-trained diffusion models can represent the content and haze characteristics of hazy images, as the diffusion time-step changes. Building upon this insight, we integrate the diffusion latent representations at different time-steps into a delicately designed dehazing network to provide instructions for image dehazing. Our DiffLI$^2$D avoids re-training diffusion models and iterative sampling process by effectively utilizing the informative representations derived from the pre-trained diffusion models, which also offers a novel perspective for introducing diffusion models to image dehazing. Extensive experiments on multiple datasets demonstrate that the proposed method achieves superior performance to existing image dehazing methods. Code is available at https://github.com/aaaasan111/difflid.

</details>


### [13] [Optical Ocean Recipes: Creating Realistic Datasets to Facilitate Underwater Vision Research](https://arxiv.org/abs/2509.20171)
*Patricia Schöntag,David Nakath,Judith Fischer,Rüdiger Röttgers,Kevin Köser*

Main category: cs.CV

TL;DR: 本文提出了"光学海洋配方"框架，用于在受控水下环境中创建逼真数据集，解决水下机器视觉评估缺乏可控测试环境的问题。


<details>
  <summary>Details</summary>
Motivation: 水下机器视觉开发面临挑战，主要因为缺乏能够考虑光学挑战（如颜色失真、对比度降低、散射等）的受控测试环境，且现有评估方法缺乏普适性。

Method: 通过使用校准的颜色和散射添加剂创建光学海洋配方，能够在受控条件下生成逼真的水下数据集，实现可重复的水成分对图像外观影响测试。

Result: 该框架能够为多种视觉任务创建地面真实数据，包括水参数估计、图像恢复、分割、视觉SLAM和图像合成，并提供了演示数据集。

Conclusion: 光学海洋配方为水下机器视觉提供了独特且实用的测试框架，填补了现有方法的空白，有助于提高水下视觉系统的鲁棒性和通用性。

Abstract: The development and evaluation of machine vision in underwater environments remains challenging, often relying on trial-and-error-based testing tailored to specific applications. This is partly due to the lack of controlled, ground-truthed testing environments that account for the optical challenges, such as color distortion from spectrally variant light attenuation, reduced contrast and blur from backscatter and volume scattering, and dynamic light patterns from natural or artificial illumination. Additionally, the appearance of ocean water in images varies significantly across regions, depths, and seasons. However, most machine vision evaluations are conducted under specific optical water types and imaging conditions, therefore often lack generalizability. Exhaustive testing across diverse open-water scenarios is technically impractical. To address this, we introduce the \textit{Optical Ocean Recipes}, a framework for creating realistic datasets under controlled underwater conditions. Unlike synthetic or open-water data, these recipes, using calibrated color and scattering additives, enable repeatable and controlled testing of the impact of water composition on image appearance. Hence, this provides a unique framework for analyzing machine vision in realistic, yet controlled underwater scenarios. The controlled environment enables the creation of ground-truth data for a range of vision tasks, including water parameter estimation, image restoration, segmentation, visual SLAM, and underwater image synthesis. We provide a demonstration dataset generated using the Optical Ocean Recipes and briefly demonstrate the use of our system for two underwater vision tasks. The dataset and evaluation code will be made available.

</details>


### [14] [PU-Gaussian: Point Cloud Upsampling using 3D Gaussian Representation](https://arxiv.org/abs/2509.20207)
*Mahmoud Khater,Mona Strauss,Philipp von Olshausen,Alexander Reiterer*

Main category: cs.CV

TL;DR: PU-Gaussian是一种新颖的点云上采样网络，使用各向异性3D高斯分布建模局部邻域几何结构，通过直接点采样在局部几何域中显式执行上采样，并采用细化网络优化结果。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在几何可解释性和对输入稀疏性鲁棒性方面的不足，3D传感器生成的点云通常稀疏且有噪声，需要密集高保真3D表示。

Method: 使用各向异性3D高斯分布建模每个点的局部邻域，在局部几何域中通过直接点采样进行显式上采样，生成稠密但粗糙的点云，然后通过细化网络调整以获得更均匀分布和更清晰边缘。

Result: 在PU1K和PUGAN数据集上的广泛测试表明，PU-Gaussian实现了最先进的性能。

Conclusion: PU-Gaussian通过显式几何建模和局部采样方法，在点云上采样任务中取得了优异效果，代码和模型权重已公开。

Abstract: Point clouds produced by 3D sensors are often sparse and noisy, posing challenges for tasks requiring dense and high-fidelity 3D representations. Prior work has explored both implicit feature-based upsampling and distance-function learning to address this, but often at the expense of geometric interpretability or robustness to input sparsity. To overcome these limitations, we propose PU-Gaussian, a novel upsampling network that models the local neighborhood around each point using anisotropic 3D Gaussian distributions. These Gaussians capture the underlying geometric structure, allowing us to perform upsampling explicitly in the local geometric domain by direct point sampling. The sampling process generates a dense, but coarse, point cloud. A subsequent refinement network adjusts the coarse output to produce a more uniform distribution and sharper edges. We perform extensive testing on the PU1K and PUGAN datasets, demonstrating that PU-Gaussian achieves state-of-the-art performance. We make code and model weights publicly available at https://github.com/mvg-inatech/PU-Gaussian.git.

</details>


### [15] [An Anisotropic Cross-View Texture Transfer with Multi-Reference Non-Local Attention for CT Slice Interpolation](https://arxiv.org/abs/2509.20242)
*Kwang-Hyun Uhm,Hyunjun Cho,Sung-Hoo Hong,Seung-Won Jung*

Main category: cs.CV

TL;DR: 提出了一种新颖的跨视图纹理迁移方法，用于CT切片插值，通过充分利用3D CT体积的各向异性特性，将高分辨率平面内纹理细节作为参考，迁移到低分辨率平面外图像中。


<details>
  <summary>Details</summary>
Motivation: 临床实践中，CT图像通常以大切片厚度获取，导致各向异性的CT体积，平面间分辨率远低于平面内分辨率。这种不一致的分辨率可能导致疾病诊断困难，现有方法未能充分利用3D CT体积的各向异性特性。

Method: 设计了独特的框架，采用高分辨率平面内纹理细节作为参考，将其迁移到低分辨率平面外图像。引入了多参考非局部注意力模块，从多个平面内图像中提取有意义的特征，用于重建平面外高频细节。

Result: 在公共CT数据集（包括真实配对基准）上的广泛实验表明，该方法在CT切片插值方面的性能显著优于现有竞争方法。

Conclusion: 验证了所提出框架的有效性，该方法能够有效改善CT图像的平面间分辨率，为医学诊断提供更高质量的图像数据。

Abstract: Computed tomography (CT) is one of the most widely used non-invasive imaging modalities for medical diagnosis. In clinical practice, CT images are usually acquired with large slice thicknesses due to the high cost of memory storage and operation time, resulting in an anisotropic CT volume with much lower inter-slice resolution than in-plane resolution. Since such inconsistent resolution may lead to difficulties in disease diagnosis, deep learning-based volumetric super-resolution methods have been developed to improve inter-slice resolution. Most existing methods conduct single-image super-resolution on the through-plane or synthesize intermediate slices from adjacent slices; however, the anisotropic characteristic of 3D CT volume has not been well explored. In this paper, we propose a novel cross-view texture transfer approach for CT slice interpolation by fully utilizing the anisotropic nature of 3D CT volume. Specifically, we design a unique framework that takes high-resolution in-plane texture details as a reference and transfers them to low-resolution through-plane images. To this end, we introduce a multi-reference non-local attention module that extracts meaningful features for reconstructing through-plane high-frequency details from multiple in-plane images. Through extensive experiments, we demonstrate that our method performs significantly better in CT slice interpolation than existing competing methods on public CT datasets including a real-paired benchmark, verifying the effectiveness of the proposed framework. The source code of this work is available at https://github.com/khuhm/ACVTT.

</details>


### [16] [4D Driving Scene Generation With Stereo Forcing](https://arxiv.org/abs/2509.20251)
*Hao Lu,Zhuang Ma,Guangfeng Jiang,Wenhang Ge,Bohan Li,Yuzhan Cai,Wenzhao Zheng,Yunpeng Zhang,Yingcong Chen*

Main category: cs.CV

TL;DR: PhiGenesis是一个统一的4D场景生成框架，通过结合视频生成技术和几何时间一致性，能够从多视角图像序列生成动态4D高斯溅射表示，支持时间外推和空间新视角合成。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型难以合成动态4D驾驶场景，同时支持时间外推和空间新视角合成，而无需针对每个场景进行优化。弥合生成和新视角合成之间的差距是一个主要挑战。

Method: PhiGenesis采用两阶段方法：第一阶段使用预训练视频VAE和新型范围视图适配器进行前馈4D重建；第二阶段引入几何引导的视频扩散模型，使用渲染的历史4D场景作为先验，在轨迹条件下生成未来视图。为解决新视角中的几何暴露偏差，提出了Stereo Forcing条件策略。

Result: 实验结果表明，该方法在外观和几何重建、时间生成和新视角合成任务中实现了最先进的性能，同时在下游评估中表现出竞争力。

Conclusion: PhiGenesis成功解决了动态4D场景生成的挑战，提供了一个统一的框架，能够同时支持时间外推和空间新视角合成，为自动驾驶等应用提供了有力工具。

Abstract: Current generative models struggle to synthesize dynamic 4D driving scenes that simultaneously support temporal extrapolation and spatial novel view synthesis (NVS) without per-scene optimization. Bridging generation and novel view synthesis remains a major challenge. We present PhiGenesis, a unified framework for 4D scene generation that extends video generation techniques with geometric and temporal consistency. Given multi-view image sequences and camera parameters, PhiGenesis produces temporally continuous 4D Gaussian splatting representations along target 3D trajectories. In its first stage, PhiGenesis leverages a pre-trained video VAE with a novel range-view adapter to enable feed-forward 4D reconstruction from multi-view images. This architecture supports single-frame or video inputs and outputs complete 4D scenes including geometry, semantics, and motion. In the second stage, PhiGenesis introduces a geometric-guided video diffusion model, using rendered historical 4D scenes as priors to generate future views conditioned on trajectories. To address geometric exposure bias in novel views, we propose Stereo Forcing, a novel conditioning strategy that integrates geometric uncertainty during denoising. This method enhances temporal coherence by dynamically adjusting generative influence based on uncertainty-aware perturbations. Our experimental results demonstrate that our method achieves state-of-the-art performance in both appearance and geometric reconstruction, temporal generation and novel view synthesis (NVS) tasks, while simultaneously delivering competitive performance in downstream evaluations. Homepage is at \href{https://jiangxb98.github.io/PhiGensis}{PhiGensis}.

</details>


### [17] [FAST: Foreground-aware Diffusion with Accelerated Sampling Trajectory for Segmentation-oriented Anomaly Synthesis](https://arxiv.org/abs/2509.20295)
*Xichen Xu,Yanshu Wang,Jinbao Wang,Xiaoning Lei,Guoyang Xie,Guannan Jiang,Zhichao Lu*

Main category: cs.CV

TL;DR: FAST是一个面向分割任务的工业异常合成框架，通过AIAS加速采样和FARM前景感知重建模块，在10步内生成高质量的异常分割数据，显著提升下游分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有工业异常分割方法依赖像素级标注，但真实异常稀缺且标注成本高。现有合成方法难以平衡采样效率和生成质量，且忽视异常区域与背景的统计差异。

Method: 提出FAST框架：1）AIAS模块：训练自由的粗到精采样算法，10步内生成高质量异常；2）FARM模块：在掩码前景区域自适应调整异常感知噪声，保持局部异常信号。

Result: 在多个工业基准测试中，FAST在异常分割任务上持续优于现有合成方法。

Conclusion: FAST通过前景感知的扩散框架有效解决了工业异常合成的效率和质量平衡问题，为分割任务提供了可控、结构特定的异常合成方案。

Abstract: Industrial anomaly segmentation relies heavily on pixel-level annotations, yet real-world anomalies are often scarce, diverse, and costly to label. Segmentation-oriented industrial anomaly synthesis (SIAS) has emerged as a promising alternative; however, existing methods struggle to balance sampling efficiency and generation quality. Moreover, most approaches treat all spatial regions uniformly, overlooking the distinct statistical differences between anomaly and background areas. This uniform treatment hinders the synthesis of controllable, structure-specific anomalies tailored for segmentation tasks. In this paper, we propose FAST, a foreground-aware diffusion framework featuring two novel modules: the Anomaly-Informed Accelerated Sampling (AIAS) and the Foreground-Aware Reconstruction Module (FARM). AIAS is a training-free sampling algorithm specifically designed for segmentation-oriented industrial anomaly synthesis, which accelerates the reverse process through coarse-to-fine aggregation and enables the synthesis of state-of-the-art segmentation-oriented anomalies in as few as 10 steps. Meanwhile, FARM adaptively adjusts the anomaly-aware noise within the masked foreground regions at each sampling step, preserving localized anomaly signals throughout the denoising trajectory. Extensive experiments on multiple industrial benchmarks demonstrate that FAST consistently outperforms existing anomaly synthesis methods in downstream segmentation tasks. We release the code at: https://anonymous.4open.science/r/NeurIPS-938.

</details>


### [18] [PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation](https://arxiv.org/abs/2509.20358)
*Chen Wang,Chuhao Chen,Yiming Huang,Zhiyang Dou,Yuan Liu,Jiatao Gu,Lingjie Liu*

Main category: cs.CV

TL;DR: PhysCtrl是一个基于物理的图像到视频生成框架，通过物理参数和力控制实现物理上合理的视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型虽然能从文本或图像生成逼真视频，但缺乏物理合理性和3D可控性。

Method: 使用生成式物理网络学习四种材料（弹性、沙子、塑料、刚性）的物理动力学分布，通过扩散模型结合物理参数和施加力，采用时空注意力块模拟粒子相互作用。

Result: PhysCtrl能生成真实、基于物理的运动轨迹，驱动图像到视频模型产生高保真、可控的视频，在视觉质量和物理合理性上优于现有方法。

Conclusion: 该框架为视频生成引入了物理基础，提高了生成内容的物理合理性和可控性。

Abstract: Existing video generation models excel at producing photo-realistic videos from text or images, but often lack physical plausibility and 3D controllability. To overcome these limitations, we introduce PhysCtrl, a novel framework for physics-grounded image-to-video generation with physical parameters and force control. At its core is a generative physics network that learns the distribution of physical dynamics across four materials (elastic, sand, plasticine, and rigid) via a diffusion model conditioned on physics parameters and applied forces. We represent physical dynamics as 3D point trajectories and train on a large-scale synthetic dataset of 550K animations generated by physics simulators. We enhance the diffusion model with a novel spatiotemporal attention block that emulates particle interactions and incorporates physics-based constraints during training to enforce physical plausibility. Experiments show that PhysCtrl generates realistic, physics-grounded motion trajectories which, when used to drive image-to-video models, yield high-fidelity, controllable videos that outperform existing methods in both visual quality and physical plausibility. Project Page: https://cwchenwang.github.io/physctrl

</details>


### [19] [EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning](https://arxiv.org/abs/2509.20360)
*Xuan Ju,Tianyu Wang,Yuqian Zhou,He Zhang,Qing Liu,Nanxuan Zhao,Zhifei Zhang,Yijun Li,Yuanhao Cai,Shaoteng Liu,Daniil Pakhomov,Zhe Lin,Soo Ye Kim,Qiang Xu*

Main category: cs.CV

TL;DR: EditVerse是一个统一的图像和视频生成编辑框架，通过将文本、图像和视频表示为统一标记序列，实现跨模态知识迁移和任意分辨率处理。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成编辑已实现统一框架，但视频生成编辑仍因架构限制和数据稀缺而碎片化，需要解决这一差距。

Method: 使用统一标记序列表示所有模态，利用自注意力机制实现上下文学习；构建23.2万视频编辑样本的数据管道，结合大规模图像视频数据集进行联合训练。

Result: 实验和用户研究表明EditVerse达到最先进性能，超越现有开源和商业模型，展现出跨模态的涌现编辑生成能力。

Conclusion: EditVerse成功实现了图像和视频生成编辑的统一框架，解决了视频编辑数据稀缺问题，并建立了首个基于指令的视频编辑基准EditVerseBench。

Abstract: Recent advances in foundation models highlight a clear trend toward unification and scaling, showing emergent capabilities across diverse domains. While image generation and editing have rapidly transitioned from task-specific to unified frameworks, video generation and editing remain fragmented due to architectural limitations and data scarcity. In this work, we introduce EditVerse, a unified framework for image and video generation and editing within a single model. By representing all modalities, i.e., text, image, and video, as a unified token sequence, EditVerse leverages self-attention to achieve robust in-context learning, natural cross-modal knowledge transfer, and flexible handling of inputs and outputs with arbitrary resolutions and durations. To address the lack of video editing training data, we design a scalable data pipeline that curates 232K video editing samples and combines them with large-scale image and video datasets for joint training. Furthermore, we present EditVerseBench, the first benchmark for instruction-based video editing covering diverse tasks and resolutions. Extensive experiments and user studies demonstrate that EditVerse achieves state-of-the-art performance, surpassing existing open-source and commercial models, while exhibiting emergent editing and generation abilities across modalities.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [20] [TIMED: Adversarial and Autoregressive Refinement of Diffusion-Based Time Series Generation](https://arxiv.org/abs/2509.19638)
*MohammadReza EskandariNasab,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: TIMED是一个统一的时间序列生成框架，结合了扩散模型、监督网络和Wasserstein判别器，能够生成高质量且时间连贯的合成时间序列数据。


<details>
  <summary>Details</summary>
Motivation: 时间序列合成在预测和异常检测等领域至关重要，但真实数据往往稀缺、有噪声或收集成本高。时间序列生成需要同时建模观测值的边缘分布和条件时间依赖性。

Method: TIMED框架集成了去噪扩散概率模型（DDPM）捕获全局结构，监督网络学习自回归依赖关系，Wasserstein判别器提供对抗反馈确保时间平滑性，并加入最大均值差异（MMD）损失来对齐特征空间分布。

Result: 在多个多元时间序列基准测试中，TIMED生成的序列比最先进的生成模型更真实且时间连贯性更好。

Conclusion: TIMED通过统一的生成框架有效捕捉时间序列数据的无条件和条件特征，为时间序列合成任务提供了强大的解决方案。

Abstract: Generating high-quality synthetic time series is a fundamental yet challenging task across domains such as forecasting and anomaly detection, where real data can be scarce, noisy, or costly to collect. Unlike static data generation, synthesizing time series requires modeling both the marginal distribution of observations and the conditional temporal dependencies that govern sequential dynamics. We propose TIMED, a unified generative framework that integrates a denoising diffusion probabilistic model (DDPM) to capture global structure via a forward-reverse diffusion process, a supervisor network trained with teacher forcing to learn autoregressive dependencies through next-step prediction, and a Wasserstein critic that provides adversarial feedback to ensure temporal smoothness and fidelity. To further align the real and synthetic distributions in feature space, TIMED incorporates a Maximum Mean Discrepancy (MMD) loss, promoting both diversity and sample quality. All components are built using masked attention architectures optimized for sequence modeling and are trained jointly to effectively capture both unconditional and conditional aspects of time series data. Experimental results across diverse multivariate time series benchmarks demonstrate that TIMED generates more realistic and temporally coherent sequences than state-of-the-art generative models.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [21] [ROPA: Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation](https://arxiv.org/abs/2509.19454)
*Jason Chen,I-Chun Arthur Liu,Gaurav Sukhatme,Daniel Seita*

Main category: cs.RO

TL;DR: ROPA是一种离线模仿学习数据增强方法，使用Stable Diffusion合成第三人称RGB和RGB-D观察数据，同时生成对应的关节空间动作标签，通过约束优化确保双手操作场景中的物理一致性。


<details>
  <summary>Details</summary>
Motivation: 收集多样且精确的真实世界双手操作演示数据成本高且耗时，限制了可扩展性。现有数据增强方法主要针对手眼相机设置或仅生成新图像而不包含配对动作，对于第三人称RGB-D训练的数据增强研究较少。

Method: 微调Stable Diffusion来合成新的机器人姿态的第三人称RGB和RGB-D观察数据，同时生成对应的关节空间动作标签，采用约束优化来强制执行双手操作场景中适当的夹爪-物体接触约束的物理一致性。

Result: 在5个模拟任务和3个真实世界任务上进行评估，2625次模拟试验和300次真实世界试验结果表明，ROPA优于基线方法和消融实验，显示出在第三人称双手操作中进行RGB和RGB-D数据增强的可扩展潜力。

Conclusion: ROPA方法在双手操作的数据增强方面表现出色，为第三人称RGB-D观察数据的可扩展增强提供了有效解决方案。

Abstract: Training robust bimanual manipulation policies via imitation learning requires demonstration data with broad coverage over robot poses, contacts, and scene contexts. However, collecting diverse and precise real-world demonstrations is costly and time-consuming, which hinders scalability. Prior works have addressed this with data augmentation, typically for either eye-in-hand (wrist camera) setups with RGB inputs or for generating novel images without paired actions, leaving augmentation for eye-to-hand (third-person) RGB-D training with new action labels less explored. In this paper, we propose Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation (ROPA), an offline imitation learning data augmentation method that fine-tunes Stable Diffusion to synthesize third-person RGB and RGB-D observations of novel robot poses. Our approach simultaneously generates corresponding joint-space action labels while employing constrained optimization to enforce physical consistency through appropriate gripper-to-object contact constraints in bimanual scenarios. We evaluate our method on 5 simulated and 3 real-world tasks. Our results across 2625 simulation trials and 300 real-world trials demonstrate that ROPA outperforms baselines and ablations, showing its potential for scalable RGB and RGB-D data augmentation in eye-to-hand bimanual manipulation. Our project website is available at: https://ropaaug.github.io/.

</details>
