<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 28]
- [cs.LG](#cs.LG) [Total: 2]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Robust and Efficient 3D Gaussian Splatting for Urban Scene Reconstruction](https://arxiv.org/abs/2507.23006)
*Zhensheng Yuan,Haozhi Huang,Zhen Xiong,Di Wang,Guanghua Yang*

Main category: cs.CV

TL;DR: 提出了一种快速重建和实时渲染城市规模场景的框架，通过并行训练和可控细节策略优化效率，同时保持高视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 解决多视角捕获中外观变化对重建和渲染的负面影响，并提升城市规模场景的重建效率与质量。

Method: 采用场景分区并行训练、基于可见性的图像选择、可控细节策略调节高斯密度，并引入外观变换模块和增强模块（如深度正则化）。

Result: 实验表明，该方法能高效重建城市规模场景，在效率和质量上优于先前方法。

Conclusion: 该框架在保持高视觉保真度的同时，显著提升了城市规模场景的重建和渲染效率。

Abstract: We present a framework that enables fast reconstruction and real-time rendering of urban-scale scenes while maintaining robustness against appearance variations across multi-view captures. Our approach begins with scene partitioning for parallel training, employing a visibility-based image selection strategy to optimize training efficiency. A controllable level-of-detail (LOD) strategy explicitly regulates Gaussian density under a user-defined budget, enabling efficient training and rendering while maintaining high visual fidelity. The appearance transformation module mitigates the negative effects of appearance inconsistencies across images while enabling flexible adjustments. Additionally, we utilize enhancement modules, such as depth regularization, scale regularization, and antialiasing, to improve reconstruction fidelity. Experimental results demonstrate that our method effectively reconstructs urban-scale scenes and outperforms previous approaches in both efficiency and quality. The source code is available at: https://yzslab.github.io/REUrbanGS.

</details>


### [2] [Recovering Diagnostic Value: Super-Resolution-Aided Echocardiographic Classification in Resource-Constrained Imaging](https://arxiv.org/abs/2507.23027)
*Krishan Agyakari Raja Babu,Om Prabhu,Annu,Mohanasankar Sivaprakasam*

Main category: cs.CV

TL;DR: 论文探讨了深度学习超分辨率技术在低质量超声心动图中的应用，以提升分类准确性，特别适用于资源有限的环境。


<details>
  <summary>Details</summary>
Motivation: 在资源有限的环境中，低质量的超声心动图限制了诊断模型的效能，而超分辨率技术在此领域的应用尚未充分探索。

Method: 使用公开的CAMUS数据集，按图像质量分层，评估两种临床任务（2CH vs. 4CH分类和ED vs. ES分类），并应用SRGAN和SRResNet模型增强图像质量。

Result: SRResNet显著提升了分类性能，且计算效率高。

Conclusion: 超分辨率技术能有效恢复低质量超声心动图的诊断价值，适用于资源有限环境中的AI辅助诊疗。

Abstract: Automated cardiac interpretation in resource-constrained settings (RCS) is often hindered by poor-quality echocardiographic imaging, limiting the effectiveness of downstream diagnostic models. While super-resolution (SR) techniques have shown promise in enhancing magnetic resonance imaging (MRI) and computed tomography (CT) scans, their application to echocardiography-a widely accessible but noise-prone modality-remains underexplored. In this work, we investigate the potential of deep learning-based SR to improve classification accuracy on low-quality 2D echocardiograms. Using the publicly available CAMUS dataset, we stratify samples by image quality and evaluate two clinically relevant tasks of varying complexity: a relatively simple Two-Chamber vs. Four-Chamber (2CH vs. 4CH) view classification and a more complex End-Diastole vs. End-Systole (ED vs. ES) phase classification. We apply two widely used SR models-Super-Resolution Generative Adversarial Network (SRGAN) and Super-Resolution Residual Network (SRResNet), to enhance poor-quality images and observe significant gains in performance metric-particularly with SRResNet, which also offers computational efficiency. Our findings demonstrate that SR can effectively recover diagnostic value in degraded echo scans, making it a viable tool for AI-assisted care in RCS, achieving more with less.

</details>


### [3] [Adaptive Time-step Training for Enhancing Spike-Based Neural Radiance Fields](https://arxiv.org/abs/2507.23033)
*Ranxi Lin,Canming Yao,Jiayi Li,Weihang Liu,Xin Lou,Pingqiang Zhou*

Main category: cs.CV

TL;DR: 提出了一种基于脉冲神经网络的动态时间步训练策略（PATA），用于NeRF模型，以在保持渲染质量的同时减少计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: NeRF模型在训练和推理中依赖密集点采样，导致高计算成本，限制了在资源受限场景中的应用。脉冲神经网络（SNNs）因其高效能特性成为替代方案。

Method: 提出PATA框架，结合动态时间步训练策略，自动平衡渲染质量和时间步长，实现场景自适应推理。

Result: 实验表明，PATA在保持渲染质量的同时，减少了64%的推理时间步和61.55%的运行功耗。

Conclusion: PATA为NeRF模型提供了一种高效能解决方案，适用于资源受限场景。

Abstract: Neural Radiance Fields (NeRF)-based models have achieved remarkable success in 3D reconstruction and rendering tasks. However, during both training and inference, these models rely heavily on dense point sampling along rays from multiple viewpoints, resulting in a surge in floating-point operations and severely limiting their use in resource-constrained scenarios like edge computing. Spiking Neural Networks (SNNs), which communicate via binary spikes over discrete time steps, offer a promising alternative due to their energy-efficient nature. Given the inherent variability in scene scale and texture complexity in neural rendering and the prevailing practice of training separate models per scene, we propose a spike-based NeRF framework with a dynamic time step training strategy, termed Pretrain-Adaptive Time-step Adjustment (PATA). This approach automatically explores the trade-off between rendering quality and time step length during training. Consequently, it enables scene-adaptive inference with variable time steps and reduces the additional consumption of computational resources in the inference process. Anchoring to the established Instant-NGP architecture, we evaluate our method across diverse datasets. The experimental results show that PATA can preserve rendering fidelity while reducing inference time steps by 64\% and running power by 61.55\%.

</details>


### [4] [Reference-Guided Diffusion Inpainting For Multimodal Counterfactual Generation](https://arxiv.org/abs/2507.23058)
*Alexandru Buburuzan*

Main category: cs.CV

TL;DR: 论文提出了两种新型合成数据生成方法MObI和AnydoorMed，分别用于自动驾驶和医学图像分析，通过扩散模型实现高真实性和可控性。


<details>
  <summary>Details</summary>
Motivation: 由于真实数据收集成本高且复杂，合成数据方法在安全关键应用中日益重要，但需要高真实性和可控性。

Method: MObI利用扩散模型进行多模态对象修复，支持相机和激光雷达数据；AnydoorMed专注于医学图像中的参考引导修复。

Result: MObI实现了多模态场景中的无缝对象插入，AnydoorMed在乳腺扫描中保留了异常结构的完整性。

Conclusion: 研究表明，基础模型可适应多种感知模态，为构建高真实性和可控性的多模态反事实场景铺平道路。

Abstract: Safety-critical applications, such as autonomous driving and medical image analysis, require extensive multimodal data for rigorous testing. Synthetic data methods are gaining prominence due to the cost and complexity of gathering real-world data, but they demand a high degree of realism and controllability to be useful. This work introduces two novel methods for synthetic data generation in autonomous driving and medical image analysis, namely MObI and AnydoorMed, respectively. MObI is a first-of-its-kind framework for Multimodal Object Inpainting that leverages a diffusion model to produce realistic and controllable object inpaintings across perceptual modalities, demonstrated simultaneously for camera and lidar. Given a single reference RGB image, MObI enables seamless object insertion into existing multimodal scenes at a specified 3D location, guided by a bounding box, while maintaining semantic consistency and multimodal coherence. Unlike traditional inpainting methods that rely solely on edit masks, this approach uses 3D bounding box conditioning to ensure accurate spatial positioning and realistic scaling. AnydoorMed extends this paradigm to the medical imaging domain, focusing on reference-guided inpainting for mammography scans. It leverages a diffusion-based model to inpaint anomalies with impressive detail preservation, maintaining the reference anomaly's structural integrity while semantically blending it with the surrounding tissue. Together, these methods demonstrate that foundation models for reference-guided inpainting in natural images can be readily adapted to diverse perceptual modalities, paving the way for the next generation of systems capable of constructing highly realistic, controllable and multimodal counterfactual scenarios.

</details>


### [5] [X-NeMo: Expressive Neural Motion Reenactment via Disentangled Latent Attention](https://arxiv.org/abs/2507.23143)
*Xiaochen Zhao,Hongyi Xu,Guoxian Song,You Xie,Chenxu Zhang,Xiu Li,Linjie Luo,Jinli Suo,Yebin Liu*

Main category: cs.CV

TL;DR: X-NeMo是一种基于扩散的零样本肖像动画方法，通过驱动视频的面部动作生成静态肖像动画，解决了身份泄漏和表情捕捉问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法中的身份泄漏和难以捕捉细微及极端表情的问题。

Method: 提出端到端训练框架，提取1D身份无关的潜在运动描述符，通过交叉注意力控制图像生成。

Result: X-NeMo超越现有方法，生成高表现力且身份相似的动画。

Conclusion: X-NeMo通过潜在运动描述符和交叉注意力设计，有效解决了身份泄漏问题，提升了动画质量。

Abstract: We propose X-NeMo, a novel zero-shot diffusion-based portrait animation pipeline that animates a static portrait using facial movements from a driving video of a different individual. Our work first identifies the root causes of the key issues in prior approaches, such as identity leakage and difficulty in capturing subtle and extreme expressions. To address these challenges, we introduce a fully end-to-end training framework that distills a 1D identity-agnostic latent motion descriptor from driving image, effectively controlling motion through cross-attention during image generation. Our implicit motion descriptor captures expressive facial motion in fine detail, learned end-to-end from a diverse video dataset without reliance on pretrained motion detectors. We further enhance expressiveness and disentangle motion latents from identity cues by supervising their learning with a dual GAN decoder, alongside spatial and color augmentations. By embedding the driving motion into a 1D latent vector and controlling motion via cross-attention rather than additive spatial guidance, our design eliminates the transmission of spatial-aligned structural clues from the driving condition to the diffusion backbone, substantially mitigating identity leakage. Extensive experiments demonstrate that X-NeMo surpasses state-of-the-art baselines, producing highly expressive animations with superior identity resemblance. Our code and models are available for research.

</details>


### [6] [Neural Multi-View Self-Calibrated Photometric Stereo without Photometric Stereo Cues](https://arxiv.org/abs/2507.23162)
*Xu Cao,Takafumi Taketomi*

Main category: cs.CV

TL;DR: 提出了一种神经逆向渲染方法，从多视角图像中联合重建几何、空间变化反射率和光照条件，无需光照校准或中间线索。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要光照校准或中间线索，限制了灵活性和准确性。本文旨在通过单阶段优化直接从原始图像中恢复场景参数。

Method: 使用神经隐式场表示几何和反射率，结合阴影感知体积渲染。空间网络预测有符号距离和反射率潜在代码，反射率网络估计反射率值。

Result: 在形状和光照估计精度上优于现有方法，支持视角未对齐的多光源图像，并能处理复杂几何和反射率的物体。

Conclusion: 该方法在无需额外线索的情况下，实现了高精度的场景参数恢复，具有广泛的适用性。

Abstract: We propose a neural inverse rendering approach that jointly reconstructs geometry, spatially varying reflectance, and lighting conditions from multi-view images captured under varying directional lighting. Unlike prior multi-view photometric stereo methods that require light calibration or intermediate cues such as per-view normal maps, our method jointly optimizes all scene parameters from raw images in a single stage. We represent both geometry and reflectance as neural implicit fields and apply shadow-aware volume rendering. A spatial network first predicts the signed distance and a reflectance latent code for each scene point. A reflectance network then estimates reflectance values conditioned on the latent code and angularly encoded surface normal, view, and light directions. The proposed method outperforms state-of-the-art normal-guided approaches in shape and lighting estimation accuracy, generalizes to view-unaligned multi-light images, and handles objects with challenging geometry and reflectance.

</details>


### [7] [Single Image Rain Streak Removal Using Harris Corner Loss and R-CBAM Network](https://arxiv.org/abs/2507.23185)
*Jongwook Si,Sungyoung Kim*

Main category: cs.CV

TL;DR: 提出了一种新的图像去雨网络，通过引入角点损失和残差卷积块注意力模块，显著提升了去雨效果。


<details>
  <summary>Details</summary>
Motivation: 单图像去雨问题不仅需要抑制噪声，还需保留细节和视觉质量。

Method: 引入角点损失防止边界和细节丢失，并使用R-CBAM模块动态调整特征重要性。

Result: 在Rain100L和Rain100H数据集上分别达到33.29 dB和26.16 dB的PSNR。

Conclusion: 该方法在去雨任务中显著优于现有方法。

Abstract: The problem of single-image rain streak removal goes beyond simple noise suppression, requiring the simultaneous preservation of fine structural details and overall visual quality. In this study, we propose a novel image restoration network that effectively constrains the restoration process by introducing a Corner Loss, which prevents the loss of object boundaries and detailed texture information during restoration. Furthermore, we propose a Residual Convolutional Block Attention Module (R-CBAM) Block into the encoder and decoder to dynamically adjust the importance of features in both spatial and channel dimensions, enabling the network to focus more effectively on regions heavily affected by rain streaks. Quantitative evaluations conducted on the Rain100L and Rain100H datasets demonstrate that the proposed method significantly outperforms previous approaches, achieving a PSNR of 33.29 dB on Rain100L and 26.16 dB on Rain100H.

</details>


### [8] [Adversarial-Guided Diffusion for Multimodal LLM Attacks](https://arxiv.org/abs/2507.23202)
*Chengwei Xia,Fan Ma,Ruijie Quan,Kun Zhan,Yi Yang*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的对抗攻击方法（AGD），通过将目标语义注入反向扩散的噪声中，有效欺骗多模态大语言模型（MLLMs），同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 传统对抗攻击通常在高频段嵌入扰动，易被防御方法（如低通滤波）抑制。AGD旨在通过全频谱噪声嵌入对抗信号，提高攻击的鲁棒性。

Method: 采用对抗引导扩散（AGD）方法，在反向扩散过程中将目标语义注入噪声成分，利用扩散模型的全频谱特性生成对抗图像。

Result: 实验表明，AGD在攻击效果和对防御的鲁棒性上优于现有方法。

Conclusion: AGD通过噪声成分嵌入对抗信号，显著提升了攻击的隐蔽性和鲁棒性，为对抗攻击研究提供了新思路。

Abstract: This paper addresses the challenge of generating adversarial image using a diffusion model to deceive multimodal large language models (MLLMs) into generating the targeted responses, while avoiding significant distortion of the clean image. To address the above challenges, we propose an adversarial-guided diffusion (AGD) approach for adversarial attack MLLMs. We introduce adversarial-guided noise to ensure attack efficacy. A key observation in our design is that, unlike most traditional adversarial attacks which embed high-frequency perturbations directly into the clean image, AGD injects target semantics into the noise component of the reverse diffusion. Since the added noise in a diffusion model spans the entire frequency spectrum, the adversarial signal embedded within it also inherits this full-spectrum property. Importantly, during reverse diffusion, the adversarial image is formed as a linear combination of the clean image and the noise. Thus, when applying defenses such as a simple low-pass filtering, which act independently on each component, the adversarial image within the noise component is less likely to be suppressed, as it is not confined to the high-frequency band. This makes AGD inherently robust to variety defenses. Extensive experiments demonstrate that our AGD outperforms state-of-the-art methods in attack performance as well as in model robustness to some defenses.

</details>


### [9] [PixNerd: Pixel Neural Field Diffusion](https://arxiv.org/abs/2507.23268)
*Shuai Wang,Ziteng Gao,Chenhui Zhu,Weilin Huang,Limin Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为PixelNerd的单尺度、单阶段、高效的端到端解决方案，用于解决扩散变换器中因预训练VAE引入的累积误差和解码伪影问题。


<details>
  <summary>Details</summary>
Motivation: 当前扩散变换器的成功依赖于预训练VAE压缩的潜在空间，但两阶段训练范式会引入累积误差和解码伪影。

Method: 提出用神经场建模逐块解码，实现单尺度、单阶段的端到端解决方案PixelNerd。

Result: 在ImageNet 256×256和512×512上分别达到2.15和2.84的FID，并在文本到图像应用中表现优异。

Conclusion: PixelNerd提供了一种高效且无需复杂级联或VAE的解决方案，在图像生成任务中表现出色。

Abstract: The current success of diffusion transformers heavily depends on the compressed latent space shaped by the pre-trained variational autoencoder(VAE). However, this two-stage training paradigm inevitably introduces accumulated errors and decoding artifacts. To address the aforementioned problems, researchers return to pixel space at the cost of complicated cascade pipelines and increased token complexity. In contrast to their efforts, we propose to model the patch-wise decoding with neural field and present a single-scale, single-stage, efficient, end-to-end solution, coined as pixel neural field diffusion~(PixelNerd). Thanks to the efficient neural field representation in PixNerd, we directly achieved 2.15 FID on ImageNet $256\times256$ and 2.84 FID on ImageNet $512\times512$ without any complex cascade pipeline or VAE. We also extend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16 achieved a competitive 0.73 overall score on the GenEval benchmark and 80.9 overall score on the DPG benchmark.

</details>


### [10] [iLRM: An Iterative Large 3D Reconstruction Model](https://arxiv.org/abs/2507.23277)
*Gyeongjin Kang,Seungtae Nam,Xiangyu Sun,Sameh Khamis,Abdelrahman Mohamed,Eunbyung Park*

Main category: cs.CV

TL;DR: iLRM是一种迭代式大规模3D重建模型，通过解耦场景表示、分解多视图交互和注入高分辨率信息，实现了高效且高质量的3D重建。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的方法因计算成本高而难以扩展，iLRM旨在解决这一问题。

Method: 采用迭代细化机制，结合解耦、两阶段注意力方案和高分辨率信息注入。

Result: 在RE10K和DL3DV数据集上，iLRM在重建质量和速度上均优于现有方法。

Conclusion: iLRM展示了卓越的可扩展性，能够在相同计算成本下利用更多输入视图实现更高质量的重建。

Abstract: Feed-forward 3D modeling has emerged as a promising approach for rapid and high-quality 3D reconstruction. In particular, directly generating explicit 3D representations, such as 3D Gaussian splatting, has attracted significant attention due to its fast and high-quality rendering, as well as numerous applications. However, many state-of-the-art methods, primarily based on transformer architectures, suffer from severe scalability issues because they rely on full attention across image tokens from multiple input views, resulting in prohibitive computational costs as the number of views or image resolution increases. Toward a scalable and efficient feed-forward 3D reconstruction, we introduce an iterative Large 3D Reconstruction Model (iLRM) that generates 3D Gaussian representations through an iterative refinement mechanism, guided by three core principles: (1) decoupling the scene representation from input-view images to enable compact 3D representations; (2) decomposing fully-attentional multi-view interactions into a two-stage attention scheme to reduce computational costs; and (3) injecting high-resolution information at every layer to achieve high-fidelity reconstruction. Experimental results on widely used datasets, such as RE10K and DL3DV, demonstrate that iLRM outperforms existing methods in both reconstruction quality and speed. Notably, iLRM exhibits superior scalability, delivering significantly higher reconstruction quality under comparable computational cost by efficiently leveraging a larger number of input views.

</details>


### [11] [UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing](https://arxiv.org/abs/2507.23278)
*Hao Tang,Chenwei Xie,Xiaoyi Bao,Tingyu Weng,Pandeng Li,Yun Zheng,Liwei Wang*

Main category: cs.CV

TL;DR: UniLIP扩展了CLIP的功能，支持重建、生成和编辑，通过两阶段训练和自蒸馏策略保持原有理解能力，同时在生成和编辑任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的统一方法需要额外组件支持重建和生成任务，导致性能不一致或理解能力下降。UniLIP旨在解决这一问题。

Method: 采用两阶段训练和自蒸馏策略，逐步将重建能力整合到CLIP中；提出双条件架构连接MLLM和扩散变换器。

Result: 在生成任务中，UniLIP在GenEval和WISE基准上分别得分0.87和0.53；在编辑任务中，ImgEdit基准得分为3.62，均优于同类模型。

Conclusion: UniLIP成功扩展了CLIP的应用范围，使其在理解和生成/编辑任务中均表现优异。

Abstract: In this paper, we propose UniLIP, which extends CLIP to reconstruction, generation and editing, thereby building a unified tokenizer upon its exceptional comprehension capabilities. Previous CLIP-based unified methods often require additional diffusion decoders or quantization to support reconstruction and generation tasks, leading to inconsistent reconstruction or degradation of original comprehension performance.In contrast, we introduce a two-stage training scheme and a self-distillation strategy that progressively integrates reconstruction capabilities into CLIP, allowing it to maintain original comprehension performance while achieving effective image reconstruction. Furthermore, we propose a dual-condition architecture to connect the MLLM and diffusion transformer, using both learnable queries and the last layer multimodal hidden states as joint conditions. This method not only enables the utilization of the MLLM's strong reasoning capabilities in generation tasks, but also maximizes the exploitation of the rich information in UniLIP features during editing tasks. In text-to-image generation tasks, UniLIP obtains scores of 0.87 and 0.53 on GenEval and WISE benchmark respectively, surpassing all previous unified models of similar scale. In image editing, UniLIP also achieves a score of 3.62 on the ImgEdit Benchmark, surpassing recent state-of-the-art models such as BAGEL and UniWorld-V1. UniLIP effectively expand the application scope of CLIP, enabling continuous CLIP features to not only serve as the optimal choice for understanding tasks but also achieve highly competitive performance in generation and editing tasks.

</details>


### [12] [Training-free Geometric Image Editing on Diffusion Models](https://arxiv.org/abs/2507.23300)
*Hanshen Zhu,Zhen Zhu,Kaile Zhang,Yiming Gong,Yuliang Liu,Xiang Bai*

Main category: cs.CV

TL;DR: 提出了一种解耦的几何图像编辑流程，通过分离对象变换、源区域修复和目标区域细化，提升了编辑效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散基编辑方法在处理大或复杂变换时的困难。

Method: 采用训练自由的扩散方法FreeFine，分别处理对象变换、源区域修复和目标区域细化。

Result: 在GeoBench基准测试中，FreeFine在图像保真度和编辑精度上优于现有方法。

Conclusion: FreeFine在几何图像编辑任务中表现出色，尤其适用于高要求变换。

Abstract: We tackle the task of geometric image editing, where an object within an image is repositioned, reoriented, or reshaped while preserving overall scene coherence. Previous diffusion-based editing methods often attempt to handle all relevant subtasks in a single step, proving difficult when transformations become large or structurally complex. We address this by proposing a decoupled pipeline that separates object transformation, source region inpainting, and target region refinement. Both inpainting and refinement are implemented using a training-free diffusion approach, FreeFine. In experiments on our new GeoBench benchmark, which contains both 2D and 3D editing scenarios, FreeFine outperforms state-of-the-art alternatives in image fidelity, and edit precision, especially under demanding transformations. Code and benchmark are available at: https://github.com/CIawevy/FreeFine

</details>


### [13] [MagicRoad: Semantic-Aware 3D Road Surface Reconstruction via Obstacle Inpainting](https://arxiv.org/abs/2507.23340)
*Xingyue Peng,Yuandong Lyu,Lang Zhang,Jian Zhu,Songtao Wang,Jiaxin Deng,Songxin Lu,Weiliang Ma,Dangen She,Peng Jia,XianPeng Lang*

Main category: cs.CV

TL;DR: 提出了一种鲁棒的道路表面重建框架，结合了遮挡感知的2D高斯面元和语义引导的颜色增强，以恢复干净、一致的道路表面。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态遮挡、静态障碍物和光照天气变化下表现不佳，需要一种更鲁棒的重建方法。

Method: 采用平面适应的高斯表示进行大规模建模，结合分割引导的视频修复去除动态和静态前景物体，并通过语义感知的HSV空间校正增强颜色一致性。

Result: 在城市场景数据集上的实验表明，该方法在视觉一致性和几何保真度上显著优于现有方法。

Conclusion: 该框架在复杂真实条件下实现了高质量的道路表面重建。

Abstract: Road surface reconstruction is essential for autonomous driving, supporting centimeter-accurate lane perception and high-definition mapping in complex urban environments.While recent methods based on mesh rendering or 3D Gaussian splatting (3DGS) achieve promising results under clean and static conditions, they remain vulnerable to occlusions from dynamic agents, visual clutter from static obstacles, and appearance degradation caused by lighting and weather changes. We present a robust reconstruction framework that integrates occlusion-aware 2D Gaussian surfels with semantic-guided color enhancement to recover clean, consistent road surfaces. Our method leverages a planar-adapted Gaussian representation for efficient large-scale modeling, employs segmentation-guided video inpainting to remove both dynamic and static foreground objects, and enhances color coherence via semantic-aware correction in HSV space. Extensive experiments on urban-scale datasets demonstrate that our framework produces visually coherent and geometrically faithful reconstructions, significantly outperforming prior methods under real-world conditions.

</details>


### [14] [IN45023 Neural Network Design Patterns in Computer Vision Seminar Report, Summer 2025](https://arxiv.org/abs/2507.23357)
*Radu-Andrei Bourceanu,Neil De La Fuente,Jan Grimm,Andrei Jardan,Andriy Manucharyan,Cornelius Weiss,Roman Pflugfelder*

Main category: cs.CV

TL;DR: 报告分析了计算机视觉中关键设计模式的演变，通过六篇有影响力的论文，从图像识别基础架构到生成模型和自监督学习技术。


<details>
  <summary>Details</summary>
Motivation: 探索计算机视觉领域的关键设计模式如何演变，以及这些模式如何推动技术进步。

Method: 分析六篇代表性论文，包括ResNet、ViT、GANs、LDMs、DINO和MAE，总结其核心贡献和方法。

Result: ResNet解决了梯度消失问题，ViT引入注意力机制，GANs和LDMs推动了生成模型的发展，DINO和MAE提升了自监督学习的效果。

Conclusion: 计算机视觉的设计模式不断演进，从深度网络到注意力机制和自监督学习，推动了领域的高效发展。

Abstract: This report analyzes the evolution of key design patterns in computer vision by examining six influential papers. The analy- sis begins with foundational architectures for image recognition. We review ResNet, which introduced residual connections to overcome the vanishing gradient problem and enable effective training of significantly deeper convolutional networks. Subsequently, we examine the Vision Transformer (ViT), which established a new paradigm by applying the Transformer ar- chitecture to sequences of image patches, demonstrating the efficacy of attention-based models for large-scale image recogni- tion. Building on these visual representation backbones, we investigate generative models. Generative Adversarial Networks (GANs) are analyzed for their novel adversarial training process, which challenges a generator against a discriminator to learn complex data distributions. Then, Latent Diffusion Models (LDMs) are covered, which improve upon prior generative methods by performing a sequential denoising process in a perceptually compressed latent space. LDMs achieve high-fidelity synthesis with greater computational efficiency, representing the current state-of-the-art for image generation. Finally, we explore self-supervised learning techniques that reduce dependency on labeled data. DINO is a self-distillation framework in which a student network learns to match the output of a momentum-updated teacher, yielding features with strong k-NN classification performance. We conclude with Masked Autoencoders (MAE), which utilize an asymmetric encoder-decoder design to reconstruct heavily masked inputs, providing a highly scalable and effective method for pre-training large-scale vision models.

</details>


### [15] [UniEmo: Unifying Emotional Understanding and Generation with Learnable Expert Queries](https://arxiv.org/abs/2507.23372)
*Yijie Zhu,Lingsen Zhang,Zitong Yu,Rui Shao,Tao Tan,Liqiang Nie*

Main category: cs.CV

TL;DR: UniEmo是一个统一框架，将情感理解与生成任务结合，通过层次化情感理解链和扩散模型实现双向增强。


<details>
  <summary>Details</summary>
Motivation: 情感理解与生成任务虽互补但常被分开处理，UniEmo旨在统一两者以相互提升性能。

Method: 提出层次化情感理解链提取多尺度特征，融合专家查询与情感表征指导扩散模型生成图像，并引入情感相关系数与条件损失增强生成质量。

Result: UniEmo在情感理解与生成任务上显著优于现有方法。

Conclusion: 通过双向反馈机制，UniEmo成功统一并提升了情感理解与生成能力。

Abstract: Emotional understanding and generation are often treated as separate tasks, yet they are inherently complementary and can mutually enhance each other. In this paper, we propose the UniEmo, a unified framework that seamlessly integrates these two tasks. The key challenge lies in the abstract nature of emotions, necessitating the extraction of visual representations beneficial for both tasks. To address this, we propose a hierarchical emotional understanding chain with learnable expert queries that progressively extracts multi-scale emotional features, thereby serving as a foundational step for unification. Simultaneously, we fuse these expert queries and emotional representations to guide the diffusion model in generating emotion-evoking images. To enhance the diversity and fidelity of the generated emotional images, we further introduce the emotional correlation coefficient and emotional condition loss into the fusion process. This step facilitates fusion and alignment for emotional generation guided by the understanding. In turn, we demonstrate that joint training allows the generation component to provide implicit feedback to the understanding part. Furthermore, we propose a novel data filtering algorithm to select high-quality and diverse emotional images generated by the well-trained model, which explicitly feedback into the understanding part. Together, these generation-driven dual feedback processes enhance the model's understanding capacity. Extensive experiments show that UniEmo significantly outperforms state-of-the-art methods in both emotional understanding and generation tasks. The code for the proposed method is available at https://github.com/JiuTian-VL/UniEmo.

</details>


### [16] [NeRF Is a Valuable Assistant for 3D Gaussian Splatting](https://arxiv.org/abs/2507.23374)
*Shuangkang Fang,I-Chao Shen,Takeo Igarashi,Yufeng Wang,ZeSheng Wang,Yi Yang,Wenrui Ding,Shuchang Zhou*

Main category: cs.CV

TL;DR: NeRF-GS结合NeRF和3DGS的优势，通过联合优化提升3D场景表示性能。


<details>
  <summary>Details</summary>
Motivation: 解决3DGS对高斯初始化敏感、空间感知有限及高斯间相关性弱的问题。

Method: 通过共享3D空间信息对齐NeRF和3DGS的空间特征，并优化残差向量。

Result: 在基准数据集上表现优于现有方法，达到SOTA性能。

Conclusion: NeRF和3DGS是互补的，为混合方法提供了新思路。

Abstract: We introduce NeRF-GS, a novel framework that jointly optimizes Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework leverages the inherent continuous spatial representation of NeRF to mitigate several limitations of 3DGS, including sensitivity to Gaussian initialization, limited spatial awareness, and weak inter-Gaussian correlations, thereby enhancing its performance. In NeRF-GS, we revisit the design of 3DGS and progressively align its spatial features with NeRF, enabling both representations to be optimized within the same scene through shared 3D spatial information. We further address the formal distinctions between the two approaches by optimizing residual vectors for both implicit features and Gaussian positions to enhance the personalized capabilities of 3DGS. Experimental results on benchmark datasets show that NeRF-GS surpasses existing methods and achieves state-of-the-art performance. This outcome confirms that NeRF and 3DGS are complementary rather than competing, offering new insights into hybrid approaches that combine 3DGS and NeRF for efficient 3D scene representation.

</details>


### [17] [Out-of-Distribution Detection in Medical Imaging via Diffusion Trajectories](https://arxiv.org/abs/2507.23411)
*Lemar Abdi,Francisco Caetano,Amaan Valiuddin,Christiaan Viviers,Hamdi Joudeh,Fons van der Sommen*

Main category: cs.CV

TL;DR: 提出了一种基于Stein分数的去噪扩散模型（SBDDM）的无监督OOD检测方法，显著降低了计算成本并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前生成方法在计算成本高、不可靠且需要重新训练的问题，提高OOD检测的效率和鲁棒性。

Method: 利用SBDDM的前向扩散轨迹，通过Stein分数捕获轨迹曲率，仅需五步扩散即可实现异常评分。

Result: 在Near-OOD和Far-OOD检测中相对提升10.43%和18.10%，计算成本大幅降低。

Conclusion: SBDDM是一种高效、可靠的OOD检测方法，适用于实时计算机辅助诊断。

Abstract: In medical imaging, unsupervised out-of-distribution (OOD) detection offers an attractive approach for identifying pathological cases with extremely low incidence rates. In contrast to supervised methods, OOD-based approaches function without labels and are inherently robust to data imbalances. Current generative approaches often rely on likelihood estimation or reconstruction error, but these methods can be computationally expensive, unreliable, and require retraining if the inlier data changes. These limitations hinder their ability to distinguish nominal from anomalous inputs efficiently, consistently, and robustly. We propose a reconstruction-free OOD detection method that leverages the forward diffusion trajectories of a Stein score-based denoising diffusion model (SBDDM). By capturing trajectory curvature via the estimated Stein score, our approach enables accurate anomaly scoring with only five diffusion steps. A single SBDDM pre-trained on a large, semantically aligned medical dataset generalizes effectively across multiple Near-OOD and Far-OOD benchmarks, achieving state-of-the-art performance while drastically reducing computational cost during inference. Compared to existing methods, SBDDM achieves a relative improvement of up to 10.43% and 18.10% for Near-OOD and Far-OOD detection, making it a practical building block for real-time, reliable computer-aided diagnosis.

</details>


### [18] [Stable-Sim2Real: Exploring Simulation of Real-Captured 3D Data with Two-Stage Depth Diffusion](https://arxiv.org/abs/2507.23483)
*Mutian Xu,Chongjie Ye,Haolin Liu,Yushuang Wu,Jiahao Chang,Xiaoguang Han*

Main category: cs.CV

TL;DR: 提出了一种名为Stable-Sim2Real的两阶段深度扩散模型，用于数据驱动的3D数据模拟，显著提升了真实世界3D视觉任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D数据模拟方法难以完全捕捉真实数据复杂性的问题，探索数据驱动的模拟新路径。

Method: 采用两阶段深度扩散模型：第一阶段微调Stable-Diffusion生成合成与真实深度残差；第二阶段通过调整扩散损失优化局部区域。

Result: 实验表明，该方法生成的3D模拟数据显著提升了真实任务的性能，且与真实数据高度相似。

Conclusion: Stable-Sim2Real为3D数据模拟提供了高效的数据驱动解决方案，具有实际应用潜力。

Abstract: 3D data simulation aims to bridge the gap between simulated and real-captured 3D data, which is a fundamental problem for real-world 3D visual tasks. Most 3D data simulation methods inject predefined physical priors but struggle to capture the full complexity of real data. An optimal approach involves learning an implicit mapping from synthetic to realistic data in a data-driven manner, but progress in this solution has met stagnation in recent studies. This work explores a new solution path of data-driven 3D simulation, called Stable-Sim2Real, based on a novel two-stage depth diffusion model. The initial stage finetunes Stable-Diffusion to generate the residual between the real and synthetic paired depth, producing a stable but coarse depth, where some local regions may deviate from realistic patterns. To enhance this, both the synthetic and initial output depth are fed into a second-stage diffusion, where diffusion loss is adjusted to prioritize these distinct areas identified by a 3D discriminator. We provide a new benchmark scheme to evaluate 3D data simulation methods. Extensive experiments show that training the network with the 3D simulated data derived from our method significantly enhances performance in real-world 3D visual tasks. Moreover, the evaluation demonstrates the high similarity between our 3D simulated data and real-captured patterns. Project page: https://mutianxu.github.io/stable-sim2real/.

</details>


### [19] [Online Estimation of Table-Top Grown Strawberry Mass in Field Conditions with Occlusions](https://arxiv.org/abs/2507.23487)
*Jinshan Zhen,Yuanyue Ge,Tianxiao Zhu,Hui Zhao,Ya Xiong*

Main category: cs.CV

TL;DR: 该研究提出了一种基于RGB-D传感和深度学习的草莓质量估计方法，解决了遮挡和姿态变化问题，实现了实时在线估计。


<details>
  <summary>Details</summary>
Motivation: 由于草莓生长环境中的频繁遮挡和姿态变化，传统方法难以准确估计其质量。

Method: 采用YOLOv8-Seg进行实例分割，CycleGAN完成遮挡区域，倾斜角校正优化投影面积计算，多项式回归模型将几何特征映射到质量。

Result: 实验显示，孤立草莓的平均质量估计误差为8.11%，遮挡情况下为10.47%。CycleGAN在遮挡恢复上优于LaMa模型。

Conclusion: 该方法为复杂遮挡模式下的自动化收获和产量监测提供了鲁棒解决方案。

Abstract: Accurate mass estimation of table-top grown strawberries under field conditions remains challenging due to frequent occlusions and pose variations. This study proposes a vision-based pipeline integrating RGB-D sensing and deep learning to enable non-destructive, real-time and online mass estimation. The method employed YOLOv8-Seg for instance segmentation, Cycle-consistent generative adversarial network (CycleGAN) for occluded region completion, and tilt-angle correction to refine frontal projection area calculations. A polynomial regression model then mapped the geometric features to mass. Experiments demonstrated mean mass estimation errors of 8.11% for isolated strawberries and 10.47% for occluded cases. CycleGAN outperformed large mask inpainting (LaMa) model in occlusion recovery, achieving superior pixel area ratios (PAR) (mean: 0.978 vs. 1.112) and higher intersection over union (IoU) scores (92.3% vs. 47.7% in the [0.9-1] range). This approach addresses critical limitations of traditional methods, offering a robust solution for automated harvesting and yield monitoring with complex occlusion patterns.

</details>


### [20] [Gaussian Splatting Feature Fields for Privacy-Preserving Visual Localization](https://arxiv.org/abs/2507.23569)
*Maxime Pietrantoni,Gabriela Csurka,Torsten Sattler*

Main category: cs.CV

TL;DR: 论文提出了一种基于3D高斯泼溅（3DGS）的视觉定位方法，结合显式几何模型与隐式特征场，实现了高精度且保护隐私的定位。


<details>
  <summary>Details</summary>
Motivation: 解决视觉定位任务中如何结合几何信息与特征表示，同时保护隐私的问题。

Method: 提出高斯泼溅特征场（GSFFs），通过对比框架对齐3D尺度感知特征场与2D特征编码器，并利用3D结构信息聚类正则化表示学习。

Result: 在多个真实数据集上，隐私与非隐私保护的定位流程均表现出最先进的性能。

Conclusion: GSFFs为视觉定位提供了高效且隐私保护的解决方案，具有广泛的应用潜力。

Abstract: Visual localization is the task of estimating a camera pose in a known environment. In this paper, we utilize 3D Gaussian Splatting (3DGS)-based representations for accurate and privacy-preserving visual localization. We propose Gaussian Splatting Feature Fields (GSFFs), a scene representation for visual localization that combines an explicit geometry model (3DGS) with an implicit feature field. We leverage the dense geometric information and differentiable rasterization algorithm from 3DGS to learn robust feature representations grounded in 3D. In particular, we align a 3D scale-aware feature field and a 2D feature encoder in a common embedding space through a contrastive framework. Using a 3D structure-informed clustering procedure, we further regularize the representation learning and seamlessly convert the features to segmentations, which can be used for privacy-preserving visual localization. Pose refinement, which involves aligning either feature maps or segmentations from a query image with those rendered from the GSFFs scene representation, is used to achieve localization. The resulting privacy- and non-privacy-preserving localization pipelines, evaluated on multiple real-world datasets, show state-of-the-art performances.

</details>


### [21] [DivControl: Knowledge Diversion for Controllable Image Generation](https://arxiv.org/abs/2507.23620)
*Yucheng Xie,Fu Feng,Ruixiao Shi,Jing Wang,Yong Rui,Xin Geng*

Main category: cs.CV

TL;DR: DivControl提出了一种可分解的预训练框架，用于统一可控生成和高效适应，通过知识分流和动态门实现零样本泛化和参数高效适应。


<details>
  <summary>Details</summary>
Motivation: 现有方法在可控生成中要么为每个条件训练单独模型，要么依赖纠缠表示的统一架构，导致泛化能力差和适应成本高。

Method: DivControl通过SVD分解ControlNet为基本组件，利用知识分流和动态门实现条件无关的learngenes和条件特定的tailors。

Result: DivControl在可控性上达到SOTA，训练成本降低36.4倍，同时在未见条件上表现出强零样本和少样本性能。

Conclusion: DivControl展示了卓越的可扩展性、模块化和可迁移性，为可控生成提供了高效解决方案。

Abstract: Diffusion models have advanced from text-to-image (T2I) to image-to-image (I2I) generation by incorporating structured inputs such as depth maps, enabling fine-grained spatial control. However, existing methods either train separate models for each condition or rely on unified architectures with entangled representations, resulting in poor generalization and high adaptation costs for novel conditions. To this end, we propose DivControl, a decomposable pretraining framework for unified controllable generation and efficient adaptation. DivControl factorizes ControlNet via SVD into basic components-pairs of singular vectors-which are disentangled into condition-agnostic learngenes and condition-specific tailors through knowledge diversion during multi-condition training. Knowledge diversion is implemented via a dynamic gate that performs soft routing over tailors based on the semantics of condition instructions, enabling zero-shot generalization and parameter-efficient adaptation to novel conditions. To further improve condition fidelity and training efficiency, we introduce a representation alignment loss that aligns condition embeddings with early diffusion features. Extensive experiments demonstrate that DivControl achieves state-of-the-art controllability with 36.4$\times$ less training cost, while simultaneously improving average performance on basic conditions. It also delivers strong zero-shot and few-shot performance on unseen conditions, demonstrating superior scalability, modularity, and transferability.

</details>


### [22] [Adaptively Distilled ControlNet: Accelerated Training and Superior Sampling for Medical Image Synthesis](https://arxiv.org/abs/2507.23652)
*Kunpeng Qiu,Zhiying Zhou,Yongxin Guo*

Main category: cs.CV

TL;DR: 提出了一种名为Adaptively Distilled ControlNet的任务无关框架，通过双模型蒸馏加速训练和优化，解决了医学图像标注中隐私和标注效率问题。


<details>
  <summary>Details</summary>
Motivation: 医学图像标注受隐私和标注效率限制，影响分割模型的性能和泛化能力。

Method: 采用双模型蒸馏框架，教师模型基于掩码-图像对训练，学生模型仅使用掩码，并通过参数空间噪声对齐和自适应正则化优化。

Result: 在KiTS19和Polyps数据集上，TransUNet和SANet分别提升了2.4%/4.2%和2.6%/3.5%的mDice/mIoU。

Conclusion: 该框架在隐私保护和性能提升方面表现出色，代码已开源。

Abstract: Medical image annotation is constrained by privacy concerns and labor-intensive labeling, significantly limiting the performance and generalization of segmentation models. While mask-controllable diffusion models excel in synthesis, they struggle with precise lesion-mask alignment. We propose \textbf{Adaptively Distilled ControlNet}, a task-agnostic framework that accelerates training and optimization through dual-model distillation. Specifically, during training, a teacher model, conditioned on mask-image pairs, regularizes a mask-only student model via predicted noise alignment in parameter space, further enhanced by adaptive regularization based on lesion-background ratios. During sampling, only the student model is used, enabling privacy-preserving medical image generation. Comprehensive evaluations on two distinct medical datasets demonstrate state-of-the-art performance: TransUNet improves mDice/mIoU by 2.4%/4.2% on KiTS19, while SANet achieves 2.6%/3.5% gains on Polyps, highlighting its effectiveness and superiority. Code is available at GitHub.

</details>


### [23] [I2V-GS: Infrastructure-to-Vehicle View Transformation with Gaussian Splatting for Autonomous Driving Data Generation](https://arxiv.org/abs/2507.23683)
*Jialei Chen,Wuhao Xu,Sipeng He,Baoru Huang,Dongchun Ren*

Main category: cs.CV

TL;DR: I2V-GS是一种新方法，通过高斯散射将基础设施视图转换为车辆视图，用于生成自动驾驶数据，显著提升了合成质量。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶数据主要依赖车辆采集，成本高且效率低，而通过合成真实世界图像数据是一种潜在解决方案。

Method: 采用自适应深度扭曲生成密集训练视图，利用级联策略修复扭曲图像，并通过跨视图信息进行置信度优化。

Result: I2V-GS在车辆视图下的合成质量显著提升，NTA-Iou、NTL-Iou和FID分别提高了45.7%、34.2%和14.9%。

Conclusion: I2V-GS是首个实现基础设施-车辆视图转换的框架，为自动驾驶数据生成提供了高效解决方案。

Abstract: Vast and high-quality data are essential for end-to-end autonomous driving systems. However, current driving data is mainly collected by vehicles, which is expensive and inefficient. A potential solution lies in synthesizing data from real-world images. Recent advancements in 3D reconstruction demonstrate photorealistic novel view synthesis, highlighting the potential of generating driving data from images captured on the road. This paper introduces a novel method, I2V-GS, to transfer the Infrastructure view To the Vehicle view with Gaussian Splatting. Reconstruction from sparse infrastructure viewpoints and rendering under large view transformations is a challenging problem. We adopt the adaptive depth warp to generate dense training views. To further expand the range of views, we employ a cascade strategy to inpaint warped images, which also ensures inpainting content is consistent across views. To further ensure the reliability of the diffusion model, we utilize the cross-view information to perform a confidenceguided optimization. Moreover, we introduce RoadSight, a multi-modality, multi-view dataset from real scenarios in infrastructure views. To our knowledge, I2V-GS is the first framework to generate autonomous driving datasets with infrastructure-vehicle view transformation. Experimental results demonstrate that I2V-GS significantly improves synthesis quality under vehicle view, outperforming StreetGaussian in NTA-Iou, NTL-Iou, and FID by 45.7%, 34.2%, and 14.9%, respectively.

</details>


### [24] [UniLDiff: Unlocking the Power of Diffusion Priors for All-in-One Image Restoration](https://arxiv.org/abs/2507.23685)
*Zihan Cheng,Liangtai Zhou,Dian Chen,Ni Tang,Xiaotong Luo,Yanyun Qu*

Main category: cs.CV

TL;DR: 提出了一种基于潜在扩散模型（LDMs）的统一图像修复框架，通过Degradation-Aware Feature Fusion（DAFF）模块和Detail-Aware Expert Module（DAEM）模块，解决了多任务和混合退化问题。


<details>
  <summary>Details</summary>
Motivation: All-in-One Image Restoration（AiOIR）是一个有前景但具有挑战性的研究方向，需要解决多样化的退化问题。

Method: 提出了一种基于LDMs的统一框架，设计了DAFF模块自适应处理多种退化类型，并通过DAEM模块减少细节损失。

Result: 在多任务和混合退化设置下的实验表明，该方法始终达到最先进的性能。

Conclusion: 扩散先验在统一图像修复中具有实际潜力，代码将公开。

Abstract: All-in-One Image Restoration (AiOIR) has emerged as a promising yet challenging research direction. To address its core challenges, we propose a novel unified image restoration framework based on latent diffusion models (LDMs). Our approach structurally integrates low-quality visual priors into the diffusion process, unlocking the powerful generative capacity of diffusion models for diverse degradations. Specifically, we design a Degradation-Aware Feature Fusion (DAFF) module to enable adaptive handling of diverse degradation types. Furthermore, to mitigate detail loss caused by the high compression and iterative sampling of LDMs, we design a Detail-Aware Expert Module (DAEM) in the decoder to enhance texture and fine-structure recovery. Extensive experiments across multi-task and mixed degradation settings demonstrate that our method consistently achieves state-of-the-art performance, highlighting the practical potential of diffusion priors for unified image restoration. Our code will be released.

</details>


### [25] [Enhanced Velocity Field Modeling for Gaussian Video Reconstruction](https://arxiv.org/abs/2507.23704)
*Zhenyang Li,Xiaoyang Bai,Tongchen Zhang,Pengfei Shen,Weiwei Xu,Yifan Peng*

Main category: cs.CV

TL;DR: 论文提出了一种名为FlowGaussian-VR的流增强速度场建模方案，用于解决复杂运动和尺度变化视频中的3D高斯重建问题，通过光学流优化和自适应密度调整策略显著提升了视觉质量。


<details>
  <summary>Details</summary>
Motivation: 高保真3D视频重建对VR/AR中的动态场景实时渲染至关重要，但现有方法在复杂运动和尺度变化下表现不佳，导致视觉质量下降。

Method: 提出FlowGaussian-VR方案，包含速度场渲染（VFR）管道和流辅助自适应密度（FAD）策略，通过光学流优化和动态区域高斯密度调整提升重建效果。

Result: 在多视角动态重建和新视角合成任务中，模型在PSNR上提升超过2.5 dB，动态纹理模糊减少，高斯轨迹更规则且可追踪。

Conclusion: FlowGaussian-VR通过流增强的速度场建模有效解决了复杂运动场景下的3D视频重建问题，显著提升了视觉质量和动态内容处理能力。

Abstract: High-fidelity 3D video reconstruction is essential for enabling real-time rendering of dynamic scenes with realistic motion in virtual and augmented reality (VR/AR). The deformation field paradigm of 3D Gaussian splatting has achieved near-photorealistic results in video reconstruction due to the great representation capability of deep deformation networks. However, in videos with complex motion and significant scale variations, deformation networks often overfit to irregular Gaussian trajectories, leading to suboptimal visual quality. Moreover, the gradient-based densification strategy designed for static scene reconstruction proves inadequate to address the absence of dynamic content. In light of these challenges, we propose a flow-empowered velocity field modeling scheme tailored for Gaussian video reconstruction, dubbed FlowGaussian-VR. It consists of two core components: a velocity field rendering (VFR) pipeline which enables optical flow-based optimization, and a flow-assisted adaptive densification (FAD) strategy that adjusts the number and size of Gaussians in dynamic regions. We validate our model's effectiveness on multi-view dynamic reconstruction and novel view synthesis with multiple real-world datasets containing challenging motion scenarios, demonstrating not only notable visual improvements (over 2.5 dB gain in PSNR) and less blurry artifacts in dynamic textures, but also regularized and trackable per-Gaussian trajectories.

</details>


### [26] [SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D Gaussian Splatting](https://arxiv.org/abs/2507.23772)
*Di Li,Jie Feng,Jiahao Chen,Weisheng Dong,Guanbin Li,Yuhui Zheng,Mingtao Feng,Guangming Shi*

Main category: cs.CV

TL;DR: 论文提出SeqAffordSplat基准和SeqSplatNet框架，用于解决3D高斯泼溅环境中的长时程、多物体功能推理任务。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法局限于单物体、单步交互，无法满足复杂场景需求。

Method: 提出SeqSplatNet框架，结合大语言模型和条件解码器生成3D功能掩码，并引入预训练策略和特征注入机制。

Result: 在SeqAffordSplat基准上取得最优性能，实现从单步到复杂序列任务的提升。

Conclusion: SeqSplatNet有效解决了长时程3D功能推理问题，为复杂场景应用提供了新方法。

Abstract: 3D affordance reasoning, the task of associating human instructions with the functional regions of 3D objects, is a critical capability for embodied agents. Current methods based on 3D Gaussian Splatting (3DGS) are fundamentally limited to single-object, single-step interactions, a paradigm that falls short of addressing the long-horizon, multi-object tasks required for complex real-world applications. To bridge this gap, we introduce the novel task of Sequential 3D Gaussian Affordance Reasoning and establish SeqAffordSplat, a large-scale benchmark featuring 1800+ scenes to support research on long-horizon affordance understanding in complex 3DGS environments. We then propose SeqSplatNet, an end-to-end framework that directly maps an instruction to a sequence of 3D affordance masks. SeqSplatNet employs a large language model that autoregressively generates text interleaved with special segmentation tokens, guiding a conditional decoder to produce the corresponding 3D mask. To handle complex scene geometry, we introduce a pre-training strategy, Conditional Geometric Reconstruction, where the model learns to reconstruct complete affordance region masks from known geometric observations, thereby building a robust geometric prior. Furthermore, to resolve semantic ambiguities, we design a feature injection mechanism that lifts rich semantic features from 2D Vision Foundation Models (VFM) and fuses them into the 3D decoder at multiple scales. Extensive experiments demonstrate that our method sets a new state-of-the-art on our challenging benchmark, effectively advancing affordance reasoning from single-step interactions to complex, sequential tasks at the scene level.

</details>


### [27] [SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions](https://arxiv.org/abs/2507.23784)
*Jessica Bader,Leander Girrbach,Stephan Alaniz,Zeynep Akata*

Main category: cs.CV

TL;DR: 论文提出了一种新方法（TDG）和数据集（SUB），用于评估概念瓶颈模型（CBMs）在分布变化下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管CBMs在可解释性方面表现良好，但在分布变化下识别正确概念的能力不足，需要更严格的评估方法。

Method: 通过SUB数据集（基于CUB数据集生成的38,400张合成图像）和TDG方法（通过共享噪声控制生成图像），评估CBMs的鲁棒性。

Result: SUB数据集和TDG方法为CBMs的鲁棒性评估提供了新工具。

Conclusion: 该研究为开发更鲁棒的可解释模型提供了重要基准和方法支持。

Abstract: Concept Bottleneck Models (CBMs) and other concept-based interpretable models show great promise for making AI applications more transparent, which is essential in fields like medicine. Despite their success, we demonstrate that CBMs struggle to reliably identify the correct concepts under distribution shifts. To assess the robustness of CBMs to concept variations, we introduce SUB: a fine-grained image and concept benchmark containing 38,400 synthetic images based on the CUB dataset. To create SUB, we select a CUB subset of 33 bird classes and 45 concepts to generate images which substitute a specific concept, such as wing color or belly pattern. We introduce a novel Tied Diffusion Guidance (TDG) method to precisely control generated images, where noise sharing for two parallel denoising processes ensures that both the correct bird class and the correct attribute are generated. This novel benchmark enables rigorous evaluation of CBMs and similar interpretable models, contributing to the development of more robust methods. Our code is available at https://github.com/ExplainableML/sub and the dataset at http://huggingface.co/datasets/Jessica-bader/SUB.

</details>


### [28] [Gaussian Variation Field Diffusion for High-fidelity Video-to-4D Synthesis](https://arxiv.org/abs/2507.23785)
*Bowen Zhang,Sicheng Xu,Chuxin Wang,Jiaolong Yang,Feng Zhao,Dong Chen,Baining Guo*

Main category: cs.CV

TL;DR: 提出了一种从单视频输入生成高质量动态3D内容的新框架，通过直接编码高斯溅射（GS）及其时间变化，解决了4D扩散建模的高维挑战。


<details>
  <summary>Details</summary>
Motivation: 解决4D扩散建模中数据构建成本高和高维表示的挑战，实现从视频到高质量动态3D内容的生成。

Method: 引入Direct 4DMesh-to-GS Variation Field VAE，直接编码高斯溅射及其时间变化，并训练高斯变化场扩散模型。

Result: 模型在生成质量上优于现有方法，并对野外视频输入表现出良好的泛化能力。

Conclusion: 该框架为生成高质量动态3D内容提供了新途径，展示了在合成数据训练下的强大泛化能力。

Abstract: In this paper, we present a novel framework for video-to-4D generation that creates high-quality dynamic 3D content from single video inputs. Direct 4D diffusion modeling is extremely challenging due to costly data construction and the high-dimensional nature of jointly representing 3D shape, appearance, and motion. We address these challenges by introducing a Direct 4DMesh-to-GS Variation Field VAE that directly encodes canonical Gaussian Splats (GS) and their temporal variations from 3D animation data without per-instance fitting, and compresses high-dimensional animations into a compact latent space. Building upon this efficient representation, we train a Gaussian Variation Field diffusion model with temporal-aware Diffusion Transformer conditioned on input videos and canonical GS. Trained on carefully-curated animatable 3D objects from the Objaverse dataset, our model demonstrates superior generation quality compared to existing methods. It also exhibits remarkable generalization to in-the-wild video inputs despite being trained exclusively on synthetic data, paving the way for generating high-quality animated 3D content. Project page: https://gvfdiffusion.github.io/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [29] [Investigating the Invertibility of Multimodal Latent Spaces: Limitations of Optimization-Based Methods](https://arxiv.org/abs/2507.23010)
*Siwoo Park*

Main category: cs.LG

TL;DR: 本文研究了多模态潜在空间在任务特定AI模型中的逆向能力及其广泛用途，发现尽管优化可以引导模型完成逆向任务，但其逆向映射在语义和感知上缺乏一致性。


<details>
  <summary>Details</summary>
Motivation: 探索多模态潜在空间在逆向任务中的能力，填补现有研究中对模型逆向映射潜力的空白。

Method: 提出基于优化的框架，用于从期望输出推断输入特征，并在文本-图像和文本-音频模态中双向应用。

Result: 实验表明，优化虽能产生与目标对齐的输出，但逆向映射的感知质量混乱且语义不连贯。

Conclusion: 多模态潜在空间缺乏支持稳健和可解释逆向映射的结构，需进一步研究开发语义丰富且可逆的潜在空间。

Abstract: This paper investigates the inverse capabilities and broader utility of multimodal latent spaces within task-specific AI (Artificial Intelligence) models. While these models excel at their designed forward tasks (e.g., text-to-image generation, audio-to-text transcription), their potential for inverse mappings remains largely unexplored. We propose an optimization-based framework to infer input characteristics from desired outputs, applying it bidirectionally across Text-Image (BLIP, Flux.1-dev) and Text-Audio (Whisper-Large-V3, Chatterbox-TTS) modalities.   Our central hypothesis posits that while optimization can guide models towards inverse tasks, their multimodal latent spaces will not consistently support semantically meaningful and perceptually coherent inverse mappings. Experimental results consistently validate this hypothesis. We demonstrate that while optimization can force models to produce outputs that align textually with targets (e.g., a text-to-image model generating an image that an image captioning model describes correctly, or an ASR model transcribing optimized audio accurately), the perceptual quality of these inversions is chaotic and incoherent. Furthermore, when attempting to infer the original semantic input from generative models, the reconstructed latent space embeddings frequently lack semantic interpretability, aligning with nonsensical vocabulary tokens.   These findings highlight a critical limitation. multimodal latent spaces, primarily optimized for specific forward tasks, do not inherently possess the structure required for robust and interpretable inverse mappings. Our work underscores the need for further research into developing truly semantically rich and invertible multimodal latent spaces.

</details>


### [30] [DepMicroDiff: Diffusion-Based Dependency-Aware Multimodal Imputation for Microbiome Data](https://arxiv.org/abs/2507.23676)
*Rabeya Tus Sadia,Qiang Cheng*

Main category: cs.LG

TL;DR: DepMicroDiff是一种结合扩散模型和依赖感知Transformer的新框架，用于微生物组数据插补，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 微生物组数据的稀疏性和噪声对准确插补构成挑战，现有方法未能充分捕捉微生物间的复杂依赖关系和上下文元数据。

Method: DepMicroDiff结合扩散生成模型和依赖感知Transformer，利用VAE预训练和LLM编码的元数据进行条件化。

Result: 在TCGA数据集上，DepMicroDiff在Pearson相关性、余弦相似度和误差指标上显著优于基线方法。

Conclusion: DepMicroDiff在微生物组数据插补中表现出鲁棒性和泛化能力，适用于多种癌症类型。

Abstract: Microbiome data analysis is essential for understanding host health and disease, yet its inherent sparsity and noise pose major challenges for accurate imputation, hindering downstream tasks such as biomarker discovery. Existing imputation methods, including recent diffusion-based models, often fail to capture the complex interdependencies between microbial taxa and overlook contextual metadata that can inform imputation. We introduce DepMicroDiff, a novel framework that combines diffusion-based generative modeling with a Dependency-Aware Transformer (DAT) to explicitly capture both mutual pairwise dependencies and autoregressive relationships. DepMicroDiff is further enhanced by VAE-based pretraining across diverse cancer datasets and conditioning on patient metadata encoded via a large language model (LLM). Experiments on TCGA microbiome datasets show that DepMicroDiff substantially outperforms state-of-the-art baselines, achieving higher Pearson correlation (up to 0.712), cosine similarity (up to 0.812), and lower RMSE and MAE across multiple cancer types, demonstrating its robustness and generalizability for microbiome imputation.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [31] [Towards High-Resolution Alignment and Super-Resolution of Multi-Sensor Satellite Imagery](https://arxiv.org/abs/2507.23150)
*Philip Wootaek Shin,Vishal Gaur,Rahul Ramachandran,Manil Maskey,Jack Sampson,Vijaykrishnan Narayanan,Sujit Roy*

Main category: eess.IV

TL;DR: 论文提出了一种框架，用于对齐和协调Landsat Sentinel 30m和10m图像，以解决卫星传感器分辨率差异问题。


<details>
  <summary>Details</summary>
Motivation: 高分辨率卫星图像对地理空间分析至关重要，但不同传感器分辨率的差异给数据融合和下游应用带来挑战。现有超分辨率方法依赖人工降尺度图像，不适合具有不同光谱和时间特性的异构卫星传感器。

Method: 开发了一个初步框架，利用HLS10作为参考，对齐和协调HLS30图像，以弥合传感器分辨率差距并提升超分辨率Landsat图像质量。

Result: 定量和定性评估表明该方法有效，展示了其在增强卫星遥感应用中的潜力。

Conclusion: 研究为异构卫星图像超分辨率的可行性提供了见解，并指出了未来发展的关键考虑因素。

Abstract: High-resolution satellite imagery is essential for geospatial analysis, yet differences in spatial resolution across satellite sensors present challenges for data fusion and downstream applications. Super-resolution techniques can help bridge this gap, but existing methods rely on artificially downscaled images rather than real sensor data and are not well suited for heterogeneous satellite sensors with differing spectral, temporal characteristics. In this work, we develop a preliminary framework to align and Harmonized Landsat Sentinel 30m(HLS 30) imagery using Harmonized Landsat Sentinel 10m(HLS10) as a reference from the HLS dataset. Our approach aims to bridge the resolution gap between these sensors and improve the quality of super-resolved Landsat imagery. Quantitative and qualitative evaluations demonstrate the effectiveness of our method, showing its potential for enhancing satellite-based sensing applications. This study provides insights into the feasibility of heterogeneous satellite image super-resolution and highlights key considerations for future advancements in the field.

</details>


### [32] [LesionGen: A Concept-Guided Diffusion Model for Dermatology Image Synthesis](https://arxiv.org/abs/2507.23001)
*Jamil Fayyad,Nourhan Bayasi,Ziyang Yu,Homayoun Najjaran*

Main category: eess.IV

TL;DR: LesionGen是一个基于文本到图像扩散概率模型的皮肤病图像合成框架，通过高质量图像-描述对生成多样化的皮肤病图像，提升分类模型的性能。


<details>
  <summary>Details</summary>
Motivation: 皮肤病分类模型需要大量多样化的数据集，但现有数据因隐私、标注成本和人口代表性不足而受限。

Method: 利用专家标注和伪生成的概念丰富描述，训练扩散模型生成皮肤病图像。

Result: 仅使用合成数据训练的模型分类准确率接近真实数据训练的模型，且在少数群体中表现更优。

Conclusion: LesionGen为皮肤病图像合成提供了有效方法，缓解了数据稀缺问题。

Abstract: Deep learning models for skin disease classification require large, diverse, and well-annotated datasets. However, such resources are often limited due to privacy concerns, high annotation costs, and insufficient demographic representation. While text-to-image diffusion probabilistic models (T2I-DPMs) offer promise for medical data synthesis, their use in dermatology remains underexplored, largely due to the scarcity of rich textual descriptions in existing skin image datasets. In this work, we introduce LesionGen, a clinically informed T2I-DPM framework for dermatology image synthesis. Unlike prior methods that rely on simplistic disease labels, LesionGen is trained on structured, concept-rich dermatological captions derived from expert annotations and pseudo-generated, concept-guided reports. By fine-tuning a pretrained diffusion model on these high-quality image-caption pairs, we enable the generation of realistic and diverse skin lesion images conditioned on meaningful dermatological descriptions. Our results demonstrate that models trained solely on our synthetic dataset achieve classification accuracy comparable to those trained on real images, with notable gains in worst-case subgroup performance. Code and data are available here.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [33] [GSFusion:Globally Optimized LiDAR-Inertial-Visual Mapping for Gaussian Splatting](https://arxiv.org/abs/2507.23273)
*Jaeseok Park,Chanoh Park,Minsu Kim,Soohwan Kim*

Main category: cs.RO

TL;DR: GSFusion是一种结合LiDAR、惯性和视觉的在线3D高斯泼溅（3DGS）建图系统，解决了传统方法在高计算负载、低纹理或光照环境中的局限性，并通过全局姿态图优化和高效高斯初始化策略提升了渲染质量和建图效率。


<details>
  <summary>Details</summary>
Motivation: 传统基于相机传感器（包括RGB-D）的3DGS方法存在高计算负载、在低纹理或光照环境中失效以及操作范围短的问题。LiDAR虽为替代方案，但其与3DGS结合时面临全局对齐和稀疏数据导致的优化时间长等挑战。

Method: 提出GSFusion系统，通过全局姿态图优化中的surfel-to-surfel约束确保高精度地图一致性，采用像素感知的高斯初始化策略处理稀疏数据，并使用有界sigmoid约束防止高斯增长失控。

Result: 在公开和自建数据集上的实验表明，GSFusion在渲染质量和建图效率上优于现有3DGS SLAM系统。

Conclusion: GSFusion通过多传感器融合和高效优化策略，显著提升了3DGS在复杂环境中的性能和实用性。

Abstract: While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic mapping, conventional approaches based on camera sensor, even RGB-D, suffer from fundamental limitations such as high computational load, failure in environments with poor texture or illumination, and short operational ranges. LiDAR emerges as a robust alternative, but its integration with 3DGS introduces new challenges, such as the need for exceptional global alignment for photorealistic quality and prolonged optimization times caused by sparse data. To address these challenges, we propose GSFusion, an online LiDAR-Inertial-Visual mapping system that ensures high-precision map consistency through a surfel-to-surfel constraint in the global pose-graph optimization. To handle sparse data, our system employs a pixel-aware Gaussian initialization strategy for efficient representation and a bounded sigmoid constraint to prevent uncontrolled Gaussian growth. Experiments on public and our datasets demonstrate our system outperforms existing 3DGS SLAM systems in terms of rendering quality and map-building efficiency.

</details>
