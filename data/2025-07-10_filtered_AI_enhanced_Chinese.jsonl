{"id": "2507.06646", "pdf": "https://arxiv.org/pdf/2507.06646", "abs": "https://arxiv.org/abs/2507.06646", "authors": ["Zicong Peng", "Yicheng Zhan", "Josef Spjut", "Kaan Ak\u015fit"], "title": "Assessing Learned Models for Phase-only Hologram Compression", "categories": ["cs.GR"], "comment": "SIGGRAPH 2025 Poster", "summary": "We evaluate the performance of four common learned models utilizing INR and VAE structures for compressing phase-only holograms in holographic displays. The evaluated models include a vanilla MLP, SIREN, and FilmSIREN, with TAESD as the representative VAE model. Our experiments reveal that a pretrained image VAE, TAESD, with 2.2M parameters struggles with phase-only hologram compression, revealing the need for task-specific adaptations. Among the INRs, SIREN with 4.9k parameters achieves %40 compression with high quality in the reconstructed 3D images (PSNR = 34.54 dB). These results emphasize the effectiveness of INRs and identify the limitations of pretrained image compression VAEs for hologram compression task.", "AI": {"tldr": "\u8bc4\u4f30\u4e86\u56db\u79cd\u5e38\u89c1\u5b66\u4e60\u6a21\u578b\u5728\u538b\u7f29\u76f8\u4f4d\u5168\u606f\u56fe\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0INR\u6a21\u578b\uff08\u5982SIREN\uff09\u8868\u73b0\u4f18\u4e8eVAE\u6a21\u578b\uff08\u5982TAESD\uff09\u3002", "motivation": "\u7814\u7a76\u4e0d\u540c\u6a21\u578b\u5728\u76f8\u4f4d\u5168\u606f\u56fe\u538b\u7f29\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u786e\u5b9a\u6700\u9002\u5408\u7684\u6a21\u578b\u7ed3\u6784\u3002", "method": "\u6bd4\u8f83\u4e86\u56db\u79cd\u6a21\u578b\uff08MLP\u3001SIREN\u3001FilmSIREN\u548cTAESD\uff09\u5728\u538b\u7f29\u76f8\u4f4d\u5168\u606f\u56fe\u65f6\u7684\u6027\u80fd\u3002", "result": "SIREN\uff084.9k\u53c2\u6570\uff09\u5b9e\u73b0\u4e8640%\u538b\u7f29\u7387\u4e14\u91cd\u5efa\u8d28\u91cf\u9ad8\uff08PSNR=34.54 dB\uff09\uff0c\u800cTAESD\uff082.2M\u53c2\u6570\uff09\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "INR\u6a21\u578b\u5728\u76f8\u4f4d\u5168\u606f\u56fe\u538b\u7f29\u4e2d\u66f4\u6709\u6548\uff0c\u9884\u8bad\u7ec3VAE\u6a21\u578b\u9700\u4efb\u52a1\u7279\u5b9a\u9002\u914d\u3002"}}
{"id": "2507.07000", "pdf": "https://arxiv.org/pdf/2507.07000", "abs": "https://arxiv.org/abs/2507.07000", "authors": ["Wijayathunga W. M. R. D. B"], "title": "Enhancing non-Rigid 3D Model Deformations Using Mesh-based Gaussian Splatting", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We propose a novel framework that enhances non-rigid 3D model deformations by bridging mesh representations with 3D Gaussian splatting. While traditional Gaussian splatting delivers fast, real-time radiance-field rendering, its post-editing capabilities and support for large-scale, non-rigid deformations remain limited. Our method addresses these challenges by embedding Gaussian kernels directly onto explicit mesh surfaces. This allows the mesh's inherent topological and geometric priors to guide intuitive editing operations -- such as moving, scaling, and rotating individual 3D components -- and enables complex deformations like bending and stretching. This work paves the way for more flexible 3D content-creation workflows in applications spanning virtual reality, character animation, and interactive design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7f51\u683c\u8868\u793a\u4e0e3D\u9ad8\u65af\u6cfc\u6e85\u7ed3\u5408\uff0c\u589e\u5f3a\u975e\u521a\u60273D\u6a21\u578b\u53d8\u5f62\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u9ad8\u65af\u6cfc\u6e85\u867d\u80fd\u5b9e\u73b0\u5feb\u901f\u5b9e\u65f6\u8f90\u5c04\u573a\u6e32\u67d3\uff0c\u4f46\u5176\u540e\u7f16\u8f91\u529f\u80fd\u548c\u5bf9\u5927\u89c4\u6a21\u975e\u521a\u6027\u53d8\u5f62\u7684\u652f\u6301\u6709\u9650\u3002", "method": "\u5c06\u9ad8\u65af\u6838\u76f4\u63a5\u5d4c\u5165\u663e\u5f0f\u7f51\u683c\u8868\u9762\uff0c\u5229\u7528\u7f51\u683c\u7684\u62d3\u6251\u548c\u51e0\u4f55\u5148\u9a8c\u6307\u5bfc\u7f16\u8f91\u64cd\u4f5c\uff08\u5982\u79fb\u52a8\u3001\u7f29\u653e\u3001\u65cb\u8f6c\uff09\u5e76\u652f\u6301\u590d\u6742\u53d8\u5f62\uff08\u5982\u5f2f\u66f2\u548c\u62c9\u4f38\uff09\u3002", "result": "\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u76843D\u5185\u5bb9\u521b\u4f5c\u6d41\u7a0b\u3002", "conclusion": "\u4e3a\u865a\u62df\u73b0\u5b9e\u3001\u89d2\u8272\u52a8\u753b\u548c\u4ea4\u4e92\u8bbe\u8ba1\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u76843D\u5185\u5bb9\u521b\u4f5c\u5de5\u5177\u3002"}}
{"id": "2507.06234", "pdf": "https://arxiv.org/pdf/2507.06234", "abs": "https://arxiv.org/abs/2507.06234", "authors": ["Jiangzhong Cao", "Zekai Zeng", "Xu Zhang", "Huan Zhang", "Chunling Fan", "Gangyi Jiang", "Weisi Lin"], "title": "Unveiling the Underwater World: CLIP Perception Model-Guided Underwater Image Enhancement", "categories": ["cs.CV"], "comment": "10 pages, 7 figures;Accepted to PR 2025;The source code is available   at https://github.com/Ave001025/UIE_CLIP", "summary": "High-quality underwater images are essential for both machine vision tasks and viewers with their aesthetic appeal.However, the quality of underwater images is severely affected by light absorption and scattering. Deep learning-based methods for Underwater Image Enhancement (UIE) have achieved good performance. However, these methods often overlook considering human perception and lack sufficient constraints within the solution space. Consequently, the enhanced images often suffer from diminished perceptual quality or poor content restoration.To address these issues, we propose a UIE method with a Contrastive Language-Image Pre-Training (CLIP) perception loss module and curriculum contrastive regularization. Above all, to develop a perception model for underwater images that more aligns with human visual perception, the visual semantic feature extraction capability of the CLIP model is leveraged to learn an appropriate prompt pair to map and evaluate the quality of underwater images. This CLIP perception model is then incorporated as a perception loss module into the enhancement network to improve the perceptual quality of enhanced images. Furthermore, the CLIP perception model is integrated with the curriculum contrastive regularization to enhance the constraints imposed on the enhanced images within the CLIP perceptual space, mitigating the risk of both under-enhancement and over-enhancement. Specifically, the CLIP perception model is employed to assess and categorize the learning difficulty level of negatives in the regularization process, ensuring comprehensive and nuanced utilization of distorted images and negatives with varied quality levels. Extensive experiments demonstrate that our method outperforms state-of-the-art methods in terms of visual quality and generalization ability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CLIP\u611f\u77e5\u635f\u5931\u6a21\u5757\u548c\u8bfe\u7a0b\u5bf9\u6bd4\u6b63\u5219\u5316\u7684\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u611f\u77e5\u8d28\u91cf\u548c\u5185\u5bb9\u6062\u590d\u3002", "motivation": "\u6c34\u4e0b\u56fe\u50cf\u8d28\u91cf\u53d7\u5149\u5438\u6536\u548c\u6563\u5c04\u5f71\u54cd\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5ffd\u89c6\u4eba\u7c7b\u611f\u77e5\u4e14\u7ea6\u675f\u4e0d\u8db3\uff0c\u5bfc\u81f4\u589e\u5f3a\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\u6216\u5185\u5bb9\u6062\u590d\u4e0d\u4f73\u3002", "method": "\u5229\u7528CLIP\u6a21\u578b\u7684\u89c6\u89c9\u8bed\u4e49\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u8bbe\u8ba1\u611f\u77e5\u635f\u5931\u6a21\u5757\u548c\u8bfe\u7a0b\u5bf9\u6bd4\u6b63\u5219\u5316\uff0c\u4f18\u5316\u589e\u5f3a\u7f51\u7edc\u7684\u7ea6\u675f\u548c\u611f\u77e5\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408CLIP\u611f\u77e5\u548c\u8bfe\u7a0b\u5bf9\u6bd4\u6b63\u5219\u5316\u6709\u6548\u63d0\u5347\u4e86\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u7684\u611f\u77e5\u8d28\u91cf\u548c\u5185\u5bb9\u6062\u590d\u6548\u679c\u3002"}}
{"id": "2507.06523", "pdf": "https://arxiv.org/pdf/2507.06523", "abs": "https://arxiv.org/abs/2507.06523", "authors": ["Liqiang Jing", "Viet Lai", "Seunghyun Yoon", "Trung Bui", "Xinya Du"], "title": "FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and Video-to-Text Generation", "categories": ["cs.CV", "cs.CL", "cs.GR"], "comment": null, "summary": "Video Multimodal Large Language Models (VideoMLLMs) have achieved remarkable progress in both Video-to-Text and Text-to-Video tasks. However, they often suffer fro hallucinations, generating content that contradicts the visual input. Existing evaluation methods are limited to one task (e.g., V2T) and also fail to assess hallucinations in open-ended, free-form responses. To address this gap, we propose FIFA, a unified FaIthFulness evAluation framework that extracts comprehensive descriptive facts, models their semantic dependencies via a Spatio-Temporal Semantic Dependency Graph, and verifies them using VideoQA models. We further introduce Post-Correction, a tool-based correction framework that revises hallucinated content. Extensive experiments demonstrate that FIFA aligns more closely with human judgment than existing evaluation methods, and that Post-Correction effectively improves factual consistency in both text and video generation.", "AI": {"tldr": "FIFA\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08VideoMLLMs\uff09\u5fe0\u5b9e\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u63cf\u8ff0\u6027\u4e8b\u5b9e\u3001\u5efa\u6a21\u8bed\u4e49\u4f9d\u8d56\u5173\u7cfb\u5e76\u9a8c\u8bc1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5728\u5f00\u653e\u81ea\u7531\u54cd\u5e94\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u4e00\u4efb\u52a1\u4e14\u65e0\u6cd5\u8bc4\u4f30\u5f00\u653e\u81ea\u7531\u54cd\u5e94\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faFIFA\u6846\u67b6\uff0c\u63d0\u53d6\u63cf\u8ff0\u6027\u4e8b\u5b9e\uff0c\u6784\u5efa\u65f6\u7a7a\u8bed\u4e49\u4f9d\u8d56\u56fe\uff0c\u5e76\u901a\u8fc7VideoQA\u6a21\u578b\u9a8c\u8bc1\uff1b\u540c\u65f6\u5f15\u5165\u57fa\u4e8e\u5de5\u5177\u7684\u540e\u6821\u6b63\u6846\u67b6Post-Correction\u3002", "result": "FIFA\u6bd4\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u66f4\u63a5\u8fd1\u4eba\u7c7b\u5224\u65ad\uff0cPost-Correction\u6709\u6548\u63d0\u5347\u4e86\u6587\u672c\u548c\u89c6\u9891\u751f\u6210\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002", "conclusion": "FIFA\u548cPost-Correction\u4e3aVideoMLLMs\u7684\u5fe0\u5b9e\u6027\u8bc4\u4f30\u548c\u5e7b\u89c9\u4fee\u6b63\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06269", "pdf": "https://arxiv.org/pdf/2507.06269", "abs": "https://arxiv.org/abs/2507.06269", "authors": ["Rushil Desai", "Frederik Warburg", "Trevor Darrell", "Marissa Ramirez de Chanlatte"], "title": "A Probabilistic Approach to Uncertainty Quantification Leveraging 3D Geometry", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV 2025 Workshops (8 Pages, 6 Figures, 2 Tables)", "summary": "Quantifying uncertainty in neural implicit 3D representations, particularly those utilizing Signed Distance Functions (SDFs), remains a substantial challenge due to computational inefficiencies, scalability issues, and geometric inconsistencies. Existing methods typically neglect direct geometric integration, leading to poorly calibrated uncertainty maps. We introduce BayesSDF, a novel probabilistic framework for uncertainty quantification in neural implicit SDF models, motivated by scientific simulation applications with 3D environments (e.g., forests) such as modeling fluid flow through forests, where precise surface geometry and awareness of fidelity surface geometric uncertainty are essential. Unlike radiance-based models such as NeRF or 3D Gaussian splatting, which lack explicit surface formulations, SDFs define continuous and differentiable geometry, making them better suited for physical modeling and analysis. BayesSDF leverages a Laplace approximation to quantify local surface instability via Hessian-based metrics, enabling computationally efficient, surface-aware uncertainty estimation. Our method shows that uncertainty predictions correspond closely with poorly reconstructed geometry, providing actionable confidence measures for downstream use. Extensive evaluations on synthetic and real-world datasets demonstrate that BayesSDF outperforms existing methods in both calibration and geometric consistency, establishing a strong foundation for uncertainty-aware 3D scene reconstruction, simulation, and robotic decision-making.", "AI": {"tldr": "BayesSDF\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u795e\u7ecf\u9690\u5f0fSDF\u6a21\u578b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u6821\u51c6\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u79d1\u5b66\u6a21\u62df\u5e94\u7528\uff08\u5982\u68ee\u6797\u4e2d\u7684\u6d41\u4f53\u6d41\u52a8\uff09\u9700\u8981\u7cbe\u786e\u7684\u8868\u9762\u51e0\u4f55\u548c\u5bf9\u51e0\u4f55\u4e0d\u786e\u5b9a\u6027\u7684\u611f\u77e5\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u76f4\u63a5\u51e0\u4f55\u96c6\u6210\uff0c\u5bfc\u81f4\u4e0d\u786e\u5b9a\u6027\u6620\u5c04\u4e0d\u51c6\u786e\u3002", "method": "BayesSDF\u5229\u7528\u62c9\u666e\u62c9\u65af\u8fd1\u4f3c\uff0c\u901a\u8fc7\u57fa\u4e8eHessian\u7684\u6307\u6807\u91cf\u5316\u5c40\u90e8\u8868\u9762\u4e0d\u7a33\u5b9a\u6027\uff0c\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u548c\u8868\u9762\u611f\u77e5\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBayesSDF\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u4e0e\u91cd\u5efa\u51e0\u4f55\u5bc6\u5207\u76f8\u5173\u7684\u53ef\u64cd\u4f5c\u7f6e\u4fe1\u5ea6\u3002", "conclusion": "BayesSDF\u4e3a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u76843D\u573a\u666f\u91cd\u5efa\u3001\u6a21\u62df\u548c\u673a\u5668\u4eba\u51b3\u7b56\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2507.06526", "pdf": "https://arxiv.org/pdf/2507.06526", "abs": "https://arxiv.org/abs/2507.06526", "authors": ["Chaoshuo Zhang", "Chenhao Lin", "Zhengyu Zhao", "Le Yang", "Qian Wang", "Chao Shen"], "title": "Concept Unlearning by Modeling Key Steps of Diffusion Process", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image diffusion models (T2I DMs), represented by Stable Diffusion, which generate highly realistic images based on textual input, have been widely used. However, their misuse poses serious security risks. While existing concept unlearning methods aim to mitigate these risks, they struggle to balance unlearning effectiveness with generative retainability.To overcome this limitation, we innovatively propose the Key Step Concept Unlearning (KSCU) method, which ingeniously capitalizes on the unique stepwise sampling characteristic inherent in diffusion models during the image generation process. Unlike conventional approaches that treat all denoising steps equally, KSCU strategically focuses on pivotal steps with the most influence over the final outcome by dividing key steps for different concept unlearning tasks and fine-tuning the model only at those steps. This targeted approach reduces the number of parameter updates needed for effective unlearning, while maximizing the retention of the model's generative capabilities.Through extensive benchmark experiments, we demonstrate that KSCU effectively prevents T2I DMs from generating undesirable images while better retaining the model's generative capabilities.Our code will be released.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKSCU\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u9488\u5bf9\u6269\u6563\u6a21\u578b\u7684\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u6982\u5ff5\u9057\u5fd8\uff0c\u4ee5\u5e73\u8861\u9057\u5fd8\u6548\u679c\u4e0e\u751f\u6210\u80fd\u529b\u7684\u4fdd\u7559\u3002", "motivation": "\u73b0\u6709\u6982\u5ff5\u9057\u5fd8\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u9057\u5fd8\u6548\u679c\u4e0e\u751f\u6210\u80fd\u529b\u7684\u4fdd\u7559\uff0c\u5bfc\u81f4\u6269\u6563\u6a21\u578b\u7684\u5b89\u5168\u98ce\u9669\u65e0\u6cd5\u6709\u6548\u89e3\u51b3\u3002", "method": "KSCU\u65b9\u6cd5\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u9010\u6b65\u91c7\u6837\u7279\u6027\uff0c\u4e13\u6ce8\u4e8e\u5bf9\u6700\u7ec8\u7ed3\u679c\u5f71\u54cd\u6700\u5927\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u4ec5\u5728\u8fd9\u4e9b\u6b65\u9aa4\u4e0a\u5fae\u8c03\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cKSCU\u80fd\u6709\u6548\u9632\u6b62\u751f\u6210\u4e0d\u826f\u56fe\u50cf\uff0c\u540c\u65f6\u66f4\u597d\u5730\u4fdd\u7559\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u3002", "conclusion": "KSCU\u901a\u8fc7\u5173\u952e\u6b65\u9aa4\u7684\u9488\u5bf9\u6027\u9057\u5fd8\uff0c\u5b9e\u73b0\u4e86\u5b89\u5168\u6027\u4e0e\u751f\u6210\u80fd\u529b\u7684\u5e73\u8861\u3002"}}
{"id": "2507.06547", "pdf": "https://arxiv.org/pdf/2507.06547", "abs": "https://arxiv.org/abs/2507.06547", "authors": ["Yonghyun Park", "Chieh-Hsin Lai", "Satoshi Hayakawa", "Yuhta Takida", "Naoki Murata", "Wei-Hsiang Liao", "Woosung Choi", "Kin Wai Cheuk", "Junghyun Koo", "Yuki Mitsufuji"], "title": "Concept-TRAK: Understanding how diffusion models learn concepts through concept-level attribution", "categories": ["cs.CV", "cs.LG"], "comment": "Preprint", "summary": "While diffusion models excel at image generation, their growing adoption raises critical concerns around copyright issues and model transparency. Existing attribution methods identify training examples influencing an entire image, but fall short in isolating contributions to specific elements, such as styles or objects, that matter most to stakeholders. To bridge this gap, we introduce \\emph{concept-level attribution} via a novel method called \\emph{Concept-TRAK}. Concept-TRAK extends influence functions with two key innovations: (1) a reformulated diffusion training loss based on diffusion posterior sampling, enabling robust, sample-specific attribution; and (2) a concept-aware reward function that emphasizes semantic relevance. We evaluate Concept-TRAK on the AbC benchmark, showing substantial improvements over prior methods. Through diverse case studies--ranging from identifying IP-protected and unsafe content to analyzing prompt engineering and compositional learning--we demonstrate how concept-level attribution yields actionable insights for responsible generative AI development and governance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aConcept-TRAK\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u7248\u6743\u548c\u900f\u660e\u5ea6\u95ee\u9898\uff0c\u901a\u8fc7\u6982\u5ff5\u7ea7\u5f52\u56e0\u63d0\u4f9b\u66f4\u7cbe\u7ec6\u7684\u5206\u6790\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5e7f\u6cdb\u4f7f\u7528\u5f15\u53d1\u4e86\u7248\u6743\u548c\u900f\u660e\u5ea6\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u7cbe\u786e\u5f52\u56e0\u5230\u5177\u4f53\u5143\u7d20\uff08\u5982\u98ce\u683c\u6216\u5bf9\u8c61\uff09\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faConcept-TRAK\u65b9\u6cd5\uff0c\u6269\u5c55\u5f71\u54cd\u51fd\u6570\uff0c\u5305\u62ec\u57fa\u4e8e\u6269\u6563\u540e\u9a8c\u91c7\u6837\u7684\u8bad\u7ec3\u635f\u5931\u548c\u5f3a\u8c03\u8bed\u4e49\u76f8\u5173\u6027\u7684\u6982\u5ff5\u611f\u77e5\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5728AbC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cConcept-TRAK\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u5728\u7248\u6743\u4fdd\u62a4\u3001\u5185\u5bb9\u5b89\u5168\u548c\u6a21\u578b\u5206\u6790\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "Concept-TRAK\u4e3a\u751f\u6210\u5f0fAI\u7684\u8d1f\u8d23\u4efb\u5f00\u53d1\u548c\u6cbb\u7406\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u89e3\u51b3\u4e86\u6982\u5ff5\u7ea7\u5f52\u56e0\u7684\u9700\u6c42\u3002"}}
{"id": "2507.06590", "pdf": "https://arxiv.org/pdf/2507.06590", "abs": "https://arxiv.org/abs/2507.06590", "authors": ["Yin Wang", "Mu li", "Zhiying Leng", "Frederick W. B. Li", "Xiaohui Liang"], "title": "MOST: Motion Diffusion Model for Rare Text via Temporal Clip Banzhaf Interaction", "categories": ["cs.CV"], "comment": null, "summary": "We introduce MOST, a novel motion diffusion model via temporal clip Banzhaf interaction, aimed at addressing the persistent challenge of generating human motion from rare language prompts. While previous approaches struggle with coarse-grained matching and overlook important semantic cues due to motion redundancy, our key insight lies in leveraging fine-grained clip relationships to mitigate these issues. MOST's retrieval stage presents the first formulation of its kind - temporal clip Banzhaf interaction - which precisely quantifies textual-motion coherence at the clip level. This facilitates direct, fine-grained text-to-motion clip matching and eliminates prevalent redundancy. In the generation stage, a motion prompt module effectively utilizes retrieved motion clips to produce semantically consistent movements. Extensive evaluations confirm that MOST achieves state-of-the-art text-to-motion retrieval and generation performance by comprehensively addressing previous challenges, as demonstrated through quantitative and qualitative results highlighting its effectiveness, especially for rare prompts.", "AI": {"tldr": "MOST\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8fd0\u52a8\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u65f6\u95f4\u7247\u6bb5Banzhaf\u4ea4\u4e92\u89e3\u51b3\u4ece\u7f55\u89c1\u8bed\u8a00\u63d0\u793a\u751f\u6210\u4eba\u7c7b\u8fd0\u52a8\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7c97\u7c92\u5ea6\u5339\u914d\u548c\u5ffd\u7565\u8bed\u4e49\u7ebf\u7d22\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cMOST\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7247\u6bb5\u5173\u7cfb\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u65f6\u95f4\u7247\u6bb5Banzhaf\u4ea4\u4e92\u91cf\u5316\u6587\u672c-\u8fd0\u52a8\u4e00\u81f4\u6027\uff0c\u7ed3\u5408\u8fd0\u52a8\u63d0\u793a\u6a21\u5757\u751f\u6210\u8bed\u4e49\u4e00\u81f4\u7684\u8fd0\u52a8\u3002", "result": "MOST\u5728\u6587\u672c\u5230\u8fd0\u52a8\u68c0\u7d22\u548c\u751f\u6210\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5c24\u5176\u5728\u7f55\u89c1\u63d0\u793a\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "MOST\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5339\u914d\u548c\u6d88\u9664\u5197\u4f59\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u7684\u6027\u80fd\u3002"}}
{"id": "2507.06647", "pdf": "https://arxiv.org/pdf/2507.06647", "abs": "https://arxiv.org/abs/2507.06647", "authors": ["Chengkun Li", "Yuqi Tong", "Kai Chen", "Zhenya Yang", "Ruiyang Li", "Shi Qiu", "Jason Ying-Kuen Chan", "Pheng-Ann Heng", "Qi Dou"], "title": "ClipGS: Clippable Gaussian Splatting for Interactive Cinematic Visualization of Volumetric Medical Data", "categories": ["cs.CV"], "comment": "Early accepted by MICCAI 2025. Project is available at:   https://med-air.github.io/ClipGS", "summary": "The visualization of volumetric medical data is crucial for enhancing diagnostic accuracy and improving surgical planning and education. Cinematic rendering techniques significantly enrich this process by providing high-quality visualizations that convey intricate anatomical details, thereby facilitating better understanding and decision-making in medical contexts. However, the high computing cost and low rendering speed limit the requirement of interactive visualization in practical applications. In this paper, we introduce ClipGS, an innovative Gaussian splatting framework with the clipping plane supported, for interactive cinematic visualization of volumetric medical data. To address the challenges posed by dynamic interactions, we propose a learnable truncation scheme that automatically adjusts the visibility of Gaussian primitives in response to the clipping plane. Besides, we also design an adaptive adjustment model to dynamically adjust the deformation of Gaussians and refine the rendering performance. We validate our method on five volumetric medical data (including CT and anatomical slice data), and reach an average 36.635 PSNR rendering quality with 156 FPS and 16.1 MB model size, outperforming state-of-the-art methods in rendering quality and efficiency.", "AI": {"tldr": "ClipGS\u662f\u4e00\u79cd\u652f\u6301\u88c1\u526a\u5e73\u9762\u7684\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u5b66\u4f53\u79ef\u6570\u636e\u7684\u4ea4\u4e92\u5f0f\u7535\u5f71\u5316\u6e32\u67d3\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u622a\u65ad\u65b9\u6848\u548c\u81ea\u9002\u5e94\u8c03\u6574\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u533b\u5b66\u4f53\u79ef\u6570\u636e\u7684\u53ef\u89c6\u5316\u5bf9\u8bca\u65ad\u548c\u624b\u672f\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u4f4e\u6e32\u67d3\u901f\u5ea6\u96be\u4ee5\u6ee1\u8db3\u4ea4\u4e92\u9700\u6c42\u3002", "method": "\u63d0\u51faClipGS\u6846\u67b6\uff0c\u7ed3\u5408\u53ef\u5b66\u4e60\u7684\u622a\u65ad\u65b9\u6848\u548c\u81ea\u9002\u5e94\u8c03\u6574\u6a21\u578b\uff0c\u52a8\u6001\u8c03\u6574\u9ad8\u65af\u57fa\u5143\u7684\u53ef\u89c1\u6027\u548c\u53d8\u5f62\u3002", "result": "\u5728\u4e94\u79cd\u533b\u5b66\u6570\u636e\u4e0a\u6d4b\u8bd5\uff0c\u5e73\u5747PSNR\u4e3a36.635\uff0c\u5e27\u7387\u4e3a156 FPS\uff0c\u6a21\u578b\u5927\u5c0f\u4e3a16.1 MB\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ClipGS\u5728\u6e32\u67d3\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u533b\u5b66\u6570\u636e\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06651", "pdf": "https://arxiv.org/pdf/2507.06651", "abs": "https://arxiv.org/abs/2507.06651", "authors": ["Juncheng Mu", "Chengwei Ren", "Weixiang Zhang", "Liang Pan", "Xiao-Ping Zhang", "Yue Gao"], "title": "Diff$^2$I2P: Differentiable Image-to-Point Cloud Registration with Diffusion Prior", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "Learning cross-modal correspondences is essential for image-to-point cloud (I2P) registration. Existing methods achieve this mostly by utilizing metric learning to enforce feature alignment across modalities, disregarding the inherent modality gap between image and point data. Consequently, this paradigm struggles to ensure accurate cross-modal correspondences. To this end, inspired by the cross-modal generation success of recent large diffusion models, we propose Diff$^2$I2P, a fully Differentiable I2P registration framework, leveraging a novel and effective Diffusion prior for bridging the modality gap. Specifically, we propose a Control-Side Score Distillation (CSD) technique to distill knowledge from a depth-conditioned diffusion model to directly optimize the predicted transformation. However, the gradients on the transformation fail to backpropagate onto the cross-modal features due to the non-differentiability of correspondence retrieval and PnP solver. To this end, we further propose a Deformable Correspondence Tuning (DCT) module to estimate the correspondences in a differentiable way, followed by the transformation estimation using a differentiable PnP solver. With these two designs, the Diffusion model serves as a strong prior to guide the cross-modal feature learning of image and point cloud for forming robust correspondences, which significantly improves the registration. Extensive experimental results demonstrate that Diff$^2$I2P consistently outperforms SoTA I2P registration methods, achieving over 7% improvement in registration recall on the 7-Scenes benchmark.", "AI": {"tldr": "Diff$^2$I2P\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u5148\u9a8c\u7684\u5168\u5fae\u5206I2P\u914d\u51c6\u6846\u67b6\uff0c\u901a\u8fc7Control-Side Score Distillation\u548cDeformable Correspondence Tuning\u6a21\u5757\u89e3\u51b3\u6a21\u6001\u5dee\u8ddd\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u914d\u51c6\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u5ea6\u91cf\u5b66\u4e60\u5b9e\u73b0\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\uff0c\u4f46\u5ffd\u7565\u4e86\u56fe\u50cf\u4e0e\u70b9\u4e91\u6570\u636e\u4e4b\u95f4\u7684\u56fa\u6709\u6a21\u6001\u5dee\u8ddd\uff0c\u5bfc\u81f4\u8de8\u6a21\u6001\u5bf9\u5e94\u5173\u7cfb\u4e0d\u51c6\u786e\u3002", "method": "\u63d0\u51faDiff$^2$I2P\u6846\u67b6\uff0c\u5229\u7528\u6df1\u5ea6\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684Control-Side Score Distillation\u6280\u672f\u4f18\u5316\u53d8\u6362\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7Deformable Correspondence Tuning\u6a21\u5757\u5b9e\u73b0\u53ef\u5fae\u5bf9\u5e94\u5173\u7cfb\u4f30\u8ba1\u548c\u53d8\u6362\u6c42\u89e3\u3002", "result": "\u57287-Scenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDiff$^2$I2P\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u914d\u51c6\u53ec\u56de\u7387\u63d0\u5347\u8d85\u8fc77%\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u5f3a\u5148\u9a8c\uff0c\u6709\u6548\u6307\u5bfc\u8de8\u6a21\u6001\u7279\u5f81\u5b66\u4e60\uff0c\u5f62\u6210\u9c81\u68d2\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u63d0\u5347\u4e86I2P\u914d\u51c6\u6027\u80fd\u3002"}}
{"id": "2507.06654", "pdf": "https://arxiv.org/pdf/2507.06654", "abs": "https://arxiv.org/abs/2507.06654", "authors": ["Naoya Sogi", "Takashi Shibata", "Makoto Terao", "Masanori Suganuma", "Takayuki Okatani"], "title": "MS-DPPs: Multi-Source Determinantal Point Processes for Contextual Diversity Refinement of Composite Attributes in Text to Image Retrieval", "categories": ["cs.CV", "cs.AI", "cs.IR"], "comment": "IJCAI 2025. Code: https://github.com/NEC-N-SOGI/msdpp", "summary": "Result diversification (RD) is a crucial technique in Text-to-Image Retrieval for enhancing the efficiency of a practical application. Conventional methods focus solely on increasing the diversity metric of image appearances. However, the diversity metric and its desired value vary depending on the application, which limits the applications of RD. This paper proposes a novel task called CDR-CA (Contextual Diversity Refinement of Composite Attributes). CDR-CA aims to refine the diversities of multiple attributes, according to the application's context. To address this task, we propose Multi-Source DPPs, a simple yet strong baseline that extends the Determinantal Point Process (DPP) to multi-sources. We model MS-DPP as a single DPP model with a unified similarity matrix based on a manifold representation. We also introduce Tangent Normalization to reflect contexts. Extensive experiments demonstrate the effectiveness of the proposed method. Our code is publicly available at https://github.com/NEC-N-SOGI/msdpp.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCDR-CA\u7684\u65b0\u4efb\u52a1\uff0c\u901a\u8fc7\u591a\u6e90DPP\u65b9\u6cd5\u4f18\u5316\u591a\u5c5e\u6027\u591a\u6837\u6027\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u5e94\u7528\u573a\u666f\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4ec5\u5173\u6ce8\u56fe\u50cf\u5916\u89c2\u591a\u6837\u6027\uff0c\u4f46\u591a\u6837\u6027\u6307\u6807\u53ca\u5176\u671f\u671b\u503c\u56e0\u5e94\u7528\u800c\u5f02\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002", "method": "\u63d0\u51faMulti-Source DPPs\uff0c\u6269\u5c55DPP\u81f3\u591a\u6e90\uff0c\u57fa\u4e8e\u6d41\u5f62\u8868\u793a\u7684\u7edf\u4e00\u76f8\u4f3c\u77e9\u9635\u5efa\u6a21\uff0c\u5e76\u5f15\u5165\u5207\u7ebf\u5f52\u4e00\u5316\u4ee5\u53cd\u6620\u4e0a\u4e0b\u6587\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "CDR-CA\u4efb\u52a1\u53caMulti-Source DPPs\u65b9\u6cd5\u80fd\u6709\u6548\u4f18\u5316\u591a\u5c5e\u6027\u591a\u6837\u6027\uff0c\u9002\u5e94\u4e0d\u540c\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.06656", "pdf": "https://arxiv.org/pdf/2507.06656", "abs": "https://arxiv.org/abs/2507.06656", "authors": ["Hongjie Wu", "Mingqin Zhang", "Linchao He", "Ji-Zhe Zhou", "Jiancheng Lv"], "title": "Enhancing Diffusion Model Stability for Image Restoration via Gradient Management", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to ACM Multimedia 2025. Preprint version", "summary": "Diffusion models have shown remarkable promise for image restoration by leveraging powerful priors. Prominent methods typically frame the restoration problem within a Bayesian inference framework, which iteratively combines a denoising step with a likelihood guidance step. However, the interactions between these two components in the generation process remain underexplored. In this paper, we analyze the underlying gradient dynamics of these components and identify significant instabilities. Specifically, we demonstrate conflicts between the prior and likelihood gradient directions, alongside temporal fluctuations in the likelihood gradient itself. We show that these instabilities disrupt the generative process and compromise restoration performance. To address these issues, we propose Stabilized Progressive Gradient Diffusion (SPGD), a novel gradient management technique. SPGD integrates two synergistic components: (1) a progressive likelihood warm-up strategy to mitigate gradient conflicts; and (2) adaptive directional momentum (ADM) smoothing to reduce fluctuations in the likelihood gradient. Extensive experiments across diverse restoration tasks demonstrate that SPGD significantly enhances generation stability, leading to state-of-the-art performance in quantitative metrics and visually superior results. Code is available at \\href{https://github.com/74587887/SPGD}{here}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u68af\u5ea6\u7ba1\u7406\u6280\u672fSPGD\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u4f3c\u7136\u9884\u70ed\u548c\u81ea\u9002\u5e94\u65b9\u5411\u52a8\u91cf\u5e73\u6ed1\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u4e2d\u5148\u9a8c\u4e0e\u4f3c\u7136\u68af\u5ea6\u51b2\u7a81\u53ca\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u6062\u590d\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u6062\u590d\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5148\u9a8c\u4e0e\u4f3c\u7136\u68af\u5ea6\u4e4b\u95f4\u7684\u4ea4\u4e92\u53ca\u5176\u4e0d\u7a33\u5b9a\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u5f71\u54cd\u4e86\u751f\u6210\u8fc7\u7a0b\u7684\u7a33\u5b9a\u6027\u4e0e\u6062\u590d\u6548\u679c\u3002", "method": "\u63d0\u51faSPGD\u6280\u672f\uff0c\u5305\u542b\u6e10\u8fdb\u5f0f\u4f3c\u7136\u9884\u70ed\u7b56\u7565\u548c\u81ea\u9002\u5e94\u65b9\u5411\u52a8\u91cf\u5e73\u6ed1\uff0c\u5206\u522b\u7528\u4e8e\u7f13\u89e3\u68af\u5ea6\u51b2\u7a81\u548c\u51cf\u5c11\u68af\u5ea6\u6ce2\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSPGD\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u7a33\u5b9a\u6027\uff0c\u5728\u591a\u79cd\u6062\u590d\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f18\u7684\u5b9a\u91cf\u6307\u6807\u548c\u89c6\u89c9\u7ed3\u679c\u3002", "conclusion": "SPGD\u901a\u8fc7\u6709\u6548\u7ba1\u7406\u68af\u5ea6\u51b2\u7a81\u4e0e\u6ce2\u52a8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u6062\u590d\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2507.06671", "pdf": "https://arxiv.org/pdf/2507.06671", "abs": "https://arxiv.org/abs/2507.06671", "authors": ["Boyuan Tian", "Qizhe Gao", "Siran Xianyu", "Xiaotong Cui", "Minjia Zhang"], "title": "FlexGaussian: Flexible and Cost-Effective Training-Free Compression for 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "To appear at ACM MM 2025", "summary": "3D Gaussian splatting has become a prominent technique for representing and rendering complex 3D scenes, due to its high fidelity and speed advantages. However, the growing demand for large-scale models calls for effective compression to reduce memory and computation costs, especially on mobile and edge devices with limited resources. Existing compression methods effectively reduce 3D Gaussian parameters but often require extensive retraining or fine-tuning, lacking flexibility under varying compression constraints.   In this paper, we introduce FlexGaussian, a flexible and cost-effective method that combines mixed-precision quantization with attribute-discriminative pruning for training-free 3D Gaussian compression. FlexGaussian eliminates the need for retraining and adapts easily to diverse compression targets. Evaluation results show that FlexGaussian achieves up to 96.4% compression while maintaining high rendering quality (<1 dB drop in PSNR), and is deployable on mobile devices. FlexGaussian delivers high compression ratios within seconds, being 1.7-2.1x faster than state-of-the-art training-free methods and 10-100x faster than training-involved approaches. The code is being prepared and will be released soon at: https://github.com/Supercomputing-System-AI-Lab/FlexGaussian", "AI": {"tldr": "FlexGaussian\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u76843D\u9ad8\u65af\u538b\u7f29\u65b9\u6cd5\uff0c\u7ed3\u5408\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u548c\u5c5e\u6027\u5224\u522b\u526a\u679d\uff0c\u5b9e\u73b0\u9ad8\u538b\u7f29\u6bd4\uff0896.4%\uff09\u4e14\u4fdd\u6301\u9ad8\u8d28\u91cf\u6e32\u67d3\uff08PSNR\u4e0b\u964d<1 dB\uff09\uff0c\u9002\u7528\u4e8e\u79fb\u52a8\u8bbe\u5907\u3002", "motivation": "\u5927\u89c4\u6a213D\u6a21\u578b\u7684\u9700\u6c42\u589e\u957f\u9700\u8981\u9ad8\u6548\u538b\u7f29\u4ee5\u51cf\u5c11\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7075\u6d3b\u6027\u4e14\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "method": "FlexGaussian\u7ed3\u5408\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u548c\u5c5e\u6027\u5224\u522b\u526a\u679d\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u9002\u5e94\u4e0d\u540c\u538b\u7f29\u76ee\u6807\u3002", "result": "FlexGaussian\u5b9e\u73b096.4%\u538b\u7f29\u6bd4\uff0cPSNR\u4e0b\u964d<1 dB\uff0c\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb1.7-2.1\u500d\u3002", "conclusion": "FlexGaussian\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u76843D\u9ad8\u65af\u538b\u7f29\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u3002"}}
{"id": "2507.06689", "pdf": "https://arxiv.org/pdf/2507.06689", "abs": "https://arxiv.org/abs/2507.06689", "authors": ["Hao Tang", "Ling Shao", "Zhenyu Zhang", "Luc Van Gool", "Nicu Sebe"], "title": "Spatial-Temporal Graph Mamba for Music-Guided Dance Video Synthesis", "categories": ["cs.CV"], "comment": "Accepted to TPAMI 2025", "summary": "We propose a novel spatial-temporal graph Mamba (STG-Mamba) for the music-guided dance video synthesis task, i.e., to translate the input music to a dance video. STG-Mamba consists of two translation mappings: music-to-skeleton translation and skeleton-to-video translation. In the music-to-skeleton translation, we introduce a novel spatial-temporal graph Mamba (STGM) block to effectively construct skeleton sequences from the input music, capturing dependencies between joints in both the spatial and temporal dimensions. For the skeleton-to-video translation, we propose a novel self-supervised regularization network to translate the generated skeletons, along with a conditional image, into a dance video. Lastly, we collect a new skeleton-to-video translation dataset from the Internet, containing 54,944 video clips. Extensive experiments demonstrate that STG-Mamba achieves significantly better results than existing methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSTG-Mamba\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u97f3\u4e50\u5f15\u5bfc\u7684\u821e\u8e48\u89c6\u9891\u5408\u6210\u4efb\u52a1\uff0c\u901a\u8fc7\u97f3\u4e50\u5230\u9aa8\u67b6\u548c\u9aa8\u67b6\u5230\u89c6\u9891\u7684\u4e24\u6b65\u6620\u5c04\u5b9e\u73b0\u3002", "motivation": "\u89e3\u51b3\u97f3\u4e50\u5230\u821e\u8e48\u89c6\u9891\u7684\u5408\u6210\u95ee\u9898\uff0c\u6355\u6349\u7a7a\u95f4\u548c\u65f6\u95f4\u7ef4\u5ea6\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u4f7f\u7528STGM\u5757\u8fdb\u884c\u97f3\u4e50\u5230\u9aa8\u67b6\u7684\u8f6c\u6362\uff0c\u5e76\u63d0\u51fa\u81ea\u76d1\u7763\u6b63\u5219\u5316\u7f51\u7edc\u8fdb\u884c\u9aa8\u67b6\u5230\u89c6\u9891\u7684\u8f6c\u6362\u3002", "result": "\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "STG-Mamba\u5728\u97f3\u4e50\u5f15\u5bfc\u821e\u8e48\u89c6\u9891\u5408\u6210\u4efb\u52a1\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2507.06738", "pdf": "https://arxiv.org/pdf/2507.06738", "abs": "https://arxiv.org/abs/2507.06738", "authors": ["Xinyu Xie", "Weifeng Cao", "Jun Shi", "Yangyang Hu", "Hui Liang", "Wanyong Liang", "Xiaoliang Qian"], "title": "DIFFUMA: High-Fidelity Spatio-Temporal Video Prediction via Dual-Path Mamba and Diffusion Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Spatio-temporal video prediction plays a pivotal role in critical domains, ranging from weather forecasting to industrial automation. However, in high-precision industrial scenarios such as semiconductor manufacturing, the absence of specialized benchmark datasets severely hampers research on modeling and predicting complex processes. To address this challenge, we make a twofold contribution.First, we construct and release the Chip Dicing Lane Dataset (CHDL), the first public temporal image dataset dedicated to the semiconductor wafer dicing process. Captured via an industrial-grade vision system, CHDL provides a much-needed and challenging benchmark for high-fidelity process modeling, defect detection, and digital twin development.Second, we propose DIFFUMA, an innovative dual-path prediction architecture specifically designed for such fine-grained dynamics. The model captures global long-range temporal context through a parallel Mamba module, while simultaneously leveraging a diffusion module, guided by temporal features, to restore and enhance fine-grained spatial details, effectively combating feature degradation. Experiments demonstrate that on our CHDL benchmark, DIFFUMA significantly outperforms existing methods, reducing the Mean Squared Error (MSE) by 39% and improving the Structural Similarity (SSIM) from 0.926 to a near-perfect 0.988. This superior performance also generalizes to natural phenomena datasets. Our work not only delivers a new state-of-the-art (SOTA) model but, more importantly, provides the community with an invaluable data resource to drive future research in industrial AI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u534a\u5bfc\u4f53\u6676\u5706\u5207\u5272\u8fc7\u7a0b\u7684\u516c\u5f00\u6570\u636e\u96c6CHDL\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u53cc\u8def\u5f84\u9884\u6d4b\u67b6\u6784DIFFUMA\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u9ad8\u7cbe\u5ea6\u5de5\u4e1a\u573a\u666f\uff08\u5982\u534a\u5bfc\u4f53\u5236\u9020\uff09\u4e2d\u7f3a\u4e4f\u4e13\u7528\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u63a8\u52a8\u590d\u6742\u8fc7\u7a0b\u5efa\u6a21\u548c\u9884\u6d4b\u7684\u7814\u7a76\u3002", "method": "\u6784\u5efaCHDL\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51faDIFFUMA\u6a21\u578b\uff0c\u7ed3\u5408Mamba\u6a21\u5757\u548c\u6269\u6563\u6a21\u5757\uff0c\u5206\u522b\u6355\u6349\u5168\u5c40\u65f6\u5e8f\u4e0a\u4e0b\u6587\u548c\u589e\u5f3a\u7a7a\u95f4\u7ec6\u8282\u3002", "result": "DIFFUMA\u5728CHDL\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cMSE\u964d\u4f4e39%\uff0cSSIM\u63d0\u5347\u81f30.988\uff0c\u4e14\u5728\u81ea\u7136\u73b0\u8c61\u6570\u636e\u96c6\u4e0a\u4e5f\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8bba\u6587\u4e0d\u4ec5\u63d0\u51fa\u4e86\u65b0\u7684SOTA\u6a21\u578b\uff0c\u8fd8\u4e3a\u5de5\u4e1aAI\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u6570\u636e\u8d44\u6e90\u3002"}}
{"id": "2507.06739", "pdf": "https://arxiv.org/pdf/2507.06739", "abs": "https://arxiv.org/abs/2507.06739", "authors": ["Zishen Huang", "Chunyu Yang", "Mengyuan Ren"], "title": "PromptTea: Let Prompts Tell TeaCache the Optimal Threshold", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent progress in video generation, inference speed remains a major bottleneck. A common acceleration strategy involves reusing model outputs via caching mechanisms at fixed intervals. However, we find that such fixed-frequency reuse significantly degrades quality in complex scenes, while manually tuning reuse thresholds is inefficient and lacks robustness. To address this, we propose Prompt-Complexity-Aware (PCA) caching, a method that automatically adjusts reuse thresholds based on scene complexity estimated directly from the input prompt. By incorporating prompt-derived semantic cues, PCA enables more adaptive and informed reuse decisions than conventional caching methods. We also revisit the assumptions behind TeaCache and identify a key limitation: it suffers from poor input-output relationship modeling due to an oversimplified prior. To overcome this, we decouple the noisy input, enhance the contribution of meaningful textual information, and improve the model's predictive accuracy through multivariate polynomial feature expansion. To further reduce computational cost, we replace the static CFGCache with DynCFGCache, a dynamic mechanism that selectively reuses classifier-free guidance (CFG) outputs based on estimated output variations. This allows for more flexible reuse without compromising output quality. Extensive experiments demonstrate that our approach achieves significant acceleration-for example, 2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across a range of scenes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u590d\u6742\u5ea6\u7684\u81ea\u9002\u5e94\u7f13\u5b58\u65b9\u6cd5\uff08PCA\u7f13\u5b58\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u91cd\u7528\u9608\u503c\u548c\u4f18\u5316\u8f93\u5165\u8f93\u51fa\u5173\u7cfb\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u8f93\u51fa\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u4e2d\u7684\u63a8\u7406\u901f\u5ea6\u662f\u4e3b\u8981\u74f6\u9888\uff0c\u56fa\u5b9a\u9891\u7387\u7684\u7f13\u5b58\u673a\u5236\u5728\u590d\u6742\u573a\u666f\u4e2d\u8d28\u91cf\u4e0b\u964d\u660e\u663e\uff0c\u624b\u52a8\u8c03\u6574\u9608\u503c\u6548\u7387\u4f4e\u4e14\u7f3a\u4e4f\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faPCA\u7f13\u5b58\u65b9\u6cd5\uff0c\u6839\u636e\u8f93\u5165\u63d0\u793a\u4f30\u8ba1\u573a\u666f\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574\u91cd\u7528\u9608\u503c\uff1b\u6539\u8fdbTeaCache\u7684\u8f93\u5165\u8f93\u51fa\u5173\u7cfb\u5efa\u6a21\uff1b\u5f15\u5165DynCFGCache\u52a8\u6001\u9009\u62e9\u91cd\u7528CFG\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728Wan2.1\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e862.79\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "conclusion": "PCA\u7f13\u5b58\u548c\u52a8\u6001\u673a\u5236\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u7684\u6548\u7387\u548c\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u3002"}}
{"id": "2507.06812", "pdf": "https://arxiv.org/pdf/2507.06812", "abs": "https://arxiv.org/abs/2507.06812", "authors": ["Xu Yang", "Shaoli Huang", "Shenbo Xie", "Xuelin Chen", "Yifei Liu", "Changxing Ding"], "title": "Democratizing High-Fidelity Co-Speech Gesture Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV 2025", "summary": "Co-speech gesture video generation aims to synthesize realistic, audio-aligned videos of speakers, complete with synchronized facial expressions and body gestures. This task presents challenges due to the significant one-to-many mapping between audio and visual content, further complicated by the scarcity of large-scale public datasets and high computational demands. We propose a lightweight framework that utilizes 2D full-body skeletons as an efficient auxiliary condition to bridge audio signals with visual outputs. Our approach introduces a diffusion model conditioned on fine-grained audio segments and a skeleton extracted from the speaker's reference image, predicting skeletal motions through skeleton-audio feature fusion to ensure strict audio coordination and body shape consistency. The generated skeletons are then fed into an off-the-shelf human video generation model with the speaker's reference image to synthesize high-fidelity videos. To democratize research, we present CSG-405-the first public dataset with 405 hours of high-resolution videos across 71 speech types, annotated with 2D skeletons and diverse speaker demographics. Experiments show that our method exceeds state-of-the-art approaches in visual quality and synchronization while generalizing across speakers and contexts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u5229\u75282D\u5168\u8eab\u9aa8\u67b6\u4f5c\u4e3a\u8f85\u52a9\u6761\u4ef6\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u4e0e\u97f3\u9891\u540c\u6b65\u7684\u8bf4\u8bdd\u8005\u89c6\u9891\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u516c\u5f00\u6570\u636e\u96c6CSG-405\u3002", "motivation": "\u89e3\u51b3\u8bed\u97f3-\u89c6\u89c9\u5185\u5bb9\u7684\u4e00\u5bf9\u591a\u6620\u5c04\u95ee\u9898\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u5927\u89c4\u6a21\u516c\u5f00\u6570\u636e\u96c6\u548c\u9ad8\u8ba1\u7b97\u9700\u6c42\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u4f7f\u75282D\u9aa8\u67b6\u4f5c\u4e3a\u6761\u4ef6\uff0c\u7ed3\u5408\u7ec6\u7c92\u5ea6\u97f3\u9891\u7247\u6bb5\u548c\u53c2\u8003\u56fe\u50cf\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u9884\u6d4b\u9aa8\u9abc\u8fd0\u52a8\uff0c\u518d\u8f93\u5165\u73b0\u6709\u4eba\u4f53\u89c6\u9891\u751f\u6210\u6a21\u578b\u5408\u6210\u89c6\u9891\u3002", "result": "\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u540c\u6b65\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u4e0d\u540c\u8bf4\u8bdd\u8005\u548c\u573a\u666f\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u548c\u6570\u636e\u96c6\u4e3a\u8bed\u97f3-\u624b\u52bf\u89c6\u9891\u751f\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06814", "pdf": "https://arxiv.org/pdf/2507.06814", "abs": "https://arxiv.org/abs/2507.06814", "authors": ["Qingsen Yan", "Kangbiao Shi", "Yixu Feng", "Tao Hu", "Peng Wu", "Guansong Pang", "Yanning Zhang"], "title": "HVI-CIDNet+: Beyond Extreme Darkness for Low-Light Image Enhancement", "categories": ["cs.CV"], "comment": "14 pages", "summary": "Low-Light Image Enhancement (LLIE) aims to restore vivid content and details from corrupted low-light images. However, existing standard RGB (sRGB) color space-based LLIE methods often produce color bias and brightness artifacts due to the inherent high color sensitivity. While Hue, Saturation, and Value (HSV) color space can decouple brightness and color, it introduces significant red and black noise artifacts. To address this problem, we propose a new color space for LLIE, namely Horizontal/Vertical-Intensity (HVI), defined by the HV color map and learnable intensity. The HV color map enforces small distances for the red coordinates to remove red noise artifacts, while the learnable intensity compresses the low-light regions to remove black noise artifacts. Additionally, we introduce the Color and Intensity Decoupling Network+ (HVI-CIDNet+), built upon the HVI color space, to restore damaged content and mitigate color distortion in extremely dark regions. Specifically, HVI-CIDNet+ leverages abundant contextual and degraded knowledge extracted from low-light images using pre-trained vision-language models, integrated via a novel Prior-guided Attention Block (PAB). Within the PAB, latent semantic priors can promote content restoration, while degraded representations guide precise color correction, both particularly in extremely dark regions through the meticulously designed cross-attention fusion mechanism. Furthermore, we construct a Region Refinement Block that employs convolution for information-rich regions and self-attention for information-scarce regions, ensuring accurate brightness adjustments. Comprehensive results from benchmark experiments demonstrate that the proposed HVI-CIDNet+ outperforms the state-of-the-art methods on 10 datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684HVI\u989c\u8272\u7a7a\u95f4\u548cHVI-CIDNet+\u7f51\u7edc\uff0c\u7528\u4e8e\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\uff0c\u89e3\u51b3\u4e86\u989c\u8272\u504f\u5dee\u548c\u4eae\u5ea6\u4f2a\u5f71\u95ee\u9898\uff0c\u5e76\u572810\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8esRGB\u548cHSV\u989c\u8272\u7a7a\u95f4\u7684\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u5b58\u5728\u989c\u8272\u504f\u5dee\u548c\u4eae\u5ea6\u4f2a\u5f71\u95ee\u9898\uff0c\u5c24\u5176\u662f\u7ea2\u8272\u548c\u9ed1\u8272\u566a\u58f0\u4f2a\u5f71\u3002", "method": "\u63d0\u51faHVI\u989c\u8272\u7a7a\u95f4\uff0c\u7ed3\u5408HV\u989c\u8272\u56fe\u548c\u53ef\u5b66\u4e60\u5f3a\u5ea6\uff1b\u8bbe\u8ba1HVI-CIDNet+\u7f51\u7edc\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u4e0a\u4e0b\u6587\u77e5\u8bc6\uff0c\u5e76\u901a\u8fc7Prior-guided Attention Block\uff08PAB\uff09\u548cRegion Refinement Block\u5b9e\u73b0\u5185\u5bb9\u6062\u590d\u548c\u989c\u8272\u6821\u6b63\u3002", "result": "\u572810\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cHVI-CIDNet+\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "HVI\u989c\u8272\u7a7a\u95f4\u548cHVI-CIDNet+\u7f51\u7edc\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4e2d\u7684\u989c\u8272\u548c\u4eae\u5ea6\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.06830", "pdf": "https://arxiv.org/pdf/2507.06830", "abs": "https://arxiv.org/abs/2507.06830", "authors": ["Tao Feng", "Xianbing Zhao", "Zhenhua Chen", "Tien Tsin Wong", "Hamid Rezatofighi", "Gholamreza Haffari", "Lizhen Qu"], "title": "Physics-Grounded Motion Forecasting via Equation Discovery for Trajectory-Guided Image-to-Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in diffusion-based and autoregressive video generation models have achieved remarkable visual realism. However, these models typically lack accurate physical alignment, failing to replicate real-world dynamics in object motion. This limitation arises primarily from their reliance on learned statistical correlations rather than capturing mechanisms adhering to physical laws. To address this issue, we introduce a novel framework that integrates symbolic regression (SR) and trajectory-guided image-to-video (I2V) models for physics-grounded video forecasting. Our approach extracts motion trajectories from input videos, uses a retrieval-based pre-training mechanism to enhance symbolic regression, and discovers equations of motion to forecast physically accurate future trajectories. These trajectories then guide video generation without requiring fine-tuning of existing models. Evaluated on scenarios in Classical Mechanics, including spring-mass, pendulums, and projectile motions, our method successfully recovers ground-truth analytical equations and improves the physical alignment of generated videos over baseline methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7b26\u53f7\u56de\u5f52\u548c\u8f68\u8ff9\u5f15\u5bfc\u7684\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u7269\u7406\u5bf9\u9f50\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u548c\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u6a21\u578b\u867d\u89c6\u89c9\u903c\u771f\uff0c\u4f46\u7f3a\u4e4f\u7269\u7406\u51c6\u786e\u6027\uff0c\u65e0\u6cd5\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u52a8\u6001\u3002", "method": "\u901a\u8fc7\u63d0\u53d6\u8fd0\u52a8\u8f68\u8ff9\u3001\u7b26\u53f7\u56de\u5f52\u548c\u68c0\u7d22\u9884\u8bad\u7ec3\u673a\u5236\uff0c\u53d1\u73b0\u8fd0\u52a8\u65b9\u7a0b\u4ee5\u751f\u6210\u7269\u7406\u51c6\u786e\u7684\u672a\u6765\u8f68\u8ff9\uff0c\u5e76\u5f15\u5bfc\u89c6\u9891\u751f\u6210\u3002", "result": "\u5728\u7ecf\u5178\u529b\u5b66\u573a\u666f\uff08\u5982\u5f39\u7c27-\u8d28\u91cf\u3001\u6446\u9524\u548c\u629b\u4f53\u8fd0\u52a8\uff09\u4e2d\u6210\u529f\u6062\u590d\u771f\u5b9e\u8fd0\u52a8\u65b9\u7a0b\uff0c\u5e76\u63d0\u5347\u751f\u6210\u89c6\u9891\u7684\u7269\u7406\u5bf9\u9f50\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u65e0\u9700\u5fae\u8c03\u73b0\u6709\u6a21\u578b\u5373\u53ef\u663e\u8457\u63d0\u5347\u89c6\u9891\u751f\u6210\u7684\u7269\u7406\u51c6\u786e\u6027\u3002"}}
{"id": "2507.06971", "pdf": "https://arxiv.org/pdf/2507.06971", "abs": "https://arxiv.org/abs/2507.06971", "authors": ["Fei Teng", "Kai Luo", "Sheng Wu", "Siyu Li", "Pujun Guo", "Jiale Wei", "Kunyu Peng", "Jiaming Zhang", "Kailun Yang"], "title": "Hallucinating 360\u00b0: Panoramic Street-View Generation via Local Scenes Diffusion and Probabilistic Prompting", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": "The source code will be publicly available at   https://github.com/Bryant-Teng/Percep360", "summary": "Panoramic perception holds significant potential for autonomous driving, enabling vehicles to acquire a comprehensive 360{\\deg} surround view in a single shot. However, autonomous driving is a data-driven task. Complete panoramic data acquisition requires complex sampling systems and annotation pipelines, which are time-consuming and labor-intensive. Although existing street view generation models have demonstrated strong data regeneration capabilities, they can only learn from the fixed data distribution of existing datasets and cannot achieve high-quality, controllable panoramic generation. In this paper, we propose the first panoramic generation method Percep360 for autonomous driving. Percep360 enables coherent generation of panoramic data with control signals based on the stitched panoramic data. Percep360 focuses on two key aspects: coherence and controllability. Specifically, to overcome the inherent information loss caused by the pinhole sampling process, we propose the Local Scenes Diffusion Method (LSDM). LSDM reformulates the panorama generation as a spatially continuous diffusion process, bridging the gaps between different data distributions. Additionally, to achieve the controllable generation of panoramic images, we propose a Probabilistic Prompting Method (PPM). PPM dynamically selects the most relevant control cues, enabling controllable panoramic image generation. We evaluate the effectiveness of the generated images from three perspectives: image quality assessment (i.e., no-reference and with reference), controllability, and their utility in real-world Bird's Eye View (BEV) segmentation. Notably, the generated data consistently outperforms the original stitched images in no-reference quality metrics and enhances downstream perception models. The source code will be publicly available at https://github.com/Bryant-Teng/Percep360.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPercep360\u7684\u5168\u666f\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\uff0c\u901a\u8fc7\u5c40\u90e8\u573a\u666f\u6269\u6563\u65b9\u6cd5\u548c\u6982\u7387\u63d0\u793a\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u53ef\u63a7\u7684\u5168\u666f\u6570\u636e\u751f\u6210\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9700\u8981\u5168\u666f\u611f\u77e5\uff0c\u4f46\u73b0\u6709\u6570\u636e\u91c7\u96c6\u548c\u6807\u6ce8\u8fc7\u7a0b\u590d\u6742\u4e14\u8017\u65f6\uff0c\u4e14\u73b0\u6709\u751f\u6210\u6a21\u578b\u65e0\u6cd5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u53ef\u63a7\u7684\u5168\u666f\u751f\u6210\u3002", "method": "\u63d0\u51fa\u5c40\u90e8\u573a\u666f\u6269\u6563\u65b9\u6cd5\uff08LSDM\uff09\u89e3\u51b3\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u6982\u7387\u63d0\u793a\u65b9\u6cd5\uff08PPM\uff09\u5b9e\u73b0\u53ef\u63a7\u751f\u6210\u3002", "result": "\u751f\u6210\u56fe\u50cf\u5728\u65e0\u53c2\u8003\u8d28\u91cf\u6307\u6807\u4e0a\u4f18\u4e8e\u539f\u59cb\u62fc\u63a5\u56fe\u50cf\uff0c\u5e76\u63d0\u5347\u4e86\u4e0b\u6e38\u611f\u77e5\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "Percep360\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u63a7\u7684\u5168\u666f\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.06976", "pdf": "https://arxiv.org/pdf/2507.06976", "abs": "https://arxiv.org/abs/2507.06976", "authors": ["Sven Teufel", "Dominique Mayer", "J\u00f6rg Gamerdinger", "Oliver Bringmann"], "title": "DenoiseCP-Net: Efficient Collective Perception in Adverse Weather via Joint LiDAR-Based 3D Object Detection and Denoising", "categories": ["cs.CV"], "comment": null, "summary": "While automated vehicles hold the potential to significantly reduce traffic accidents, their perception systems remain vulnerable to sensor degradation caused by adverse weather and environmental occlusions. Collective perception, which enables vehicles to share information, offers a promising approach to overcoming these limitations. However, to this date collective perception in adverse weather is mostly unstudied. Therefore, we conduct the first study of LiDAR-based collective perception under diverse weather conditions and present a novel multi-task architecture for LiDAR-based collective perception under adverse weather. Adverse weather conditions can not only degrade perception capabilities, but also negatively affect bandwidth requirements and latency due to the introduced noise that is also transmitted and processed. Denoising prior to communication can effectively mitigate these issues. Therefore, we propose DenoiseCP-Net, a novel multi-task architecture for LiDAR-based collective perception under adverse weather conditions. DenoiseCP-Net integrates voxel-level noise filtering and object detection into a unified sparse convolution backbone, eliminating redundant computations associated with two-stage pipelines. This design not only reduces inference latency and computational cost but also minimizes communication overhead by removing non-informative noise. We extended the well-known OPV2V dataset by simulating rain, snow, and fog using our realistic weather simulation models. We demonstrate that DenoiseCP-Net achieves near-perfect denoising accuracy in adverse weather, reduces the bandwidth requirements by up to 23.6% while maintaining the same detection accuracy and reducing the inference latency for cooperative vehicles.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDenoiseCP-Net\u7684\u591a\u4efb\u52a1\u67b6\u6784\uff0c\u7528\u4e8e\u6076\u52a3\u5929\u6c14\u4e0b\u57fa\u4e8eLiDAR\u7684\u96c6\u4f53\u611f\u77e5\uff0c\u901a\u8fc7\u53bb\u566a\u548c\u5bf9\u8c61\u68c0\u6d4b\u7684\u96c6\u6210\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u901a\u4fe1\u5f00\u9500\u548c\u8ba1\u7b97\u5ef6\u8fdf\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u611f\u77e5\u7cfb\u7edf\u5728\u6076\u52a3\u5929\u6c14\u548c\u906e\u6321\u73af\u5883\u4e0b\u5bb9\u6613\u5931\u6548\uff0c\u96c6\u4f53\u611f\u77e5\u867d\u80fd\u5f25\u8865\u8fd9\u4e00\u7f3a\u9677\uff0c\u4f46\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u7814\u7a76\u5c1a\u4e0d\u5145\u5206\u3002", "method": "\u63d0\u51faDenoiseCP-Net\uff0c\u96c6\u6210\u4f53\u7d20\u7ea7\u53bb\u566a\u548c\u5bf9\u8c61\u68c0\u6d4b\uff0c\u91c7\u7528\u7a00\u758f\u5377\u79ef\u9aa8\u5e72\u7f51\u7edc\uff0c\u907f\u514d\u5197\u4f59\u8ba1\u7b97\u3002", "result": "\u5728\u6a21\u62df\u7684\u96e8\u96ea\u96fe\u5929\u6c14\u4e2d\uff0cDenoiseCP-Net\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u53bb\u566a\u6548\u679c\uff0c\u5e26\u5bbd\u9700\u6c42\u964d\u4f4e23.6%\uff0c\u540c\u65f6\u4fdd\u6301\u68c0\u6d4b\u7cbe\u5ea6\u548c\u51cf\u5c11\u5ef6\u8fdf\u3002", "conclusion": "DenoiseCP-Net\u4e3a\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u96c6\u4f53\u611f\u77e5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2507.07077", "pdf": "https://arxiv.org/pdf/2507.07077", "abs": "https://arxiv.org/abs/2507.07077", "authors": ["Yimu Pan", "Manas Mehta", "Gwen Sincerbeaux", "Jeffery A. Goldstein", "Alison D. Gernand", "James Z. Wang"], "title": "Reading a Ruler in the Wild", "categories": ["cs.CV"], "comment": null, "summary": "Accurately converting pixel measurements into absolute real-world dimensions remains a fundamental challenge in computer vision and limits progress in key applications such as biomedicine, forensics, nutritional analysis, and e-commerce. We introduce RulerNet, a deep learning framework that robustly infers scale \"in the wild\" by reformulating ruler reading as a unified keypoint-detection problem and by representing the ruler with geometric-progression parameters that are invariant to perspective transformations. Unlike traditional methods that rely on handcrafted thresholds or rigid, ruler-specific pipelines, RulerNet directly localizes centimeter marks using a distortion-invariant annotation and training strategy, enabling strong generalization across diverse ruler types and imaging conditions while mitigating data scarcity. We also present a scalable synthetic-data pipeline that combines graphics-based ruler generation with ControlNet to add photorealistic context, greatly increasing training diversity and improving performance. To further enhance robustness and efficiency, we propose DeepGP, a lightweight feed-forward network that regresses geometric-progression parameters from noisy marks and eliminates iterative optimization, enabling real-time scale estimation on mobile or edge devices. Experiments show that RulerNet delivers accurate, consistent, and efficient scale estimates under challenging real-world conditions. These results underscore its utility as a generalizable measurement tool and its potential for integration with other vision components for automated, scale-aware analysis in high-impact domains. A live demo is available at https://huggingface.co/spaces/ymp5078/RulerNet-Demo.", "AI": {"tldr": "RulerNet\u662f\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6807\u5c3a\u8bfb\u6570\u7edf\u4e00\u4e3a\u5173\u952e\u70b9\u68c0\u6d4b\u95ee\u9898\uff0c\u5e76\u5229\u7528\u51e0\u4f55\u7ea7\u6570\u53c2\u6570\u8868\u793a\u6807\u5c3a\uff0c\u5b9e\u73b0\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u51c6\u786e\u4f30\u8ba1\u771f\u5b9e\u4e16\u754c\u5c3a\u5bf8\u3002", "motivation": "\u89e3\u51b3\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u5c06\u50cf\u7d20\u6d4b\u91cf\u8f6c\u6362\u4e3a\u771f\u5b9e\u4e16\u754c\u5c3a\u5bf8\u7684\u6311\u6218\uff0c\u63a8\u52a8\u751f\u7269\u533b\u5b66\u3001\u6cd5\u533b\u5b66\u3001\u8425\u517b\u5206\u6790\u548c\u7535\u5b50\u5546\u52a1\u7b49\u5173\u952e\u5e94\u7528\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51faRulerNet\u6846\u67b6\uff0c\u5c06\u6807\u5c3a\u8bfb\u6570\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5173\u952e\u70b9\u68c0\u6d4b\u95ee\u9898\uff0c\u4f7f\u7528\u51e0\u4f55\u7ea7\u6570\u53c2\u6570\u8868\u793a\u6807\u5c3a\uff0c\u5e76\u91c7\u7528\u6297\u7578\u53d8\u6807\u6ce8\u548c\u8bad\u7ec3\u7b56\u7565\u3002\u8fd8\u63d0\u51fa\u5408\u6210\u6570\u636e\u7ba1\u9053\u548c\u8f7b\u91cf\u7ea7\u7f51\u7edcDeepGP\u3002", "result": "RulerNet\u5728\u591a\u6837\u5316\u6807\u5c3a\u7c7b\u578b\u548c\u6210\u50cf\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u3001\u4e00\u81f4\u4e14\u9ad8\u6548\u7684\u5c3a\u5bf8\u4f30\u8ba1\u3002", "conclusion": "RulerNet\u4f5c\u4e3a\u4e00\u79cd\u901a\u7528\u6d4b\u91cf\u5de5\u5177\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5e76\u53ef\u4e0e\u5176\u4ed6\u89c6\u89c9\u7ec4\u4ef6\u96c6\u6210\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u3001\u5c3a\u5ea6\u611f\u77e5\u7684\u5206\u6790\u3002"}}
{"id": "2507.07079", "pdf": "https://arxiv.org/pdf/2507.07079", "abs": "https://arxiv.org/abs/2507.07079", "authors": ["Ziyue Liu", "Federico Girella", "Yiming Wang", "Davide Talon"], "title": "Evaluating Attribute Confusion in Fashion Text-to-Image Generation", "categories": ["cs.CV"], "comment": "Accepted to ICIAP25. Project page: site   [https://intelligolabs.github.io/L-VQAScore/\\", "summary": "Despite the rapid advances in Text-to-Image (T2I) generation models, their evaluation remains challenging in domains like fashion, involving complex compositional generation. Recent automated T2I evaluation methods leverage pre-trained vision-language models to measure cross-modal alignment. However, our preliminary study reveals that they are still limited in assessing rich entity-attribute semantics, facing challenges in attribute confusion, i.e., when attributes are correctly depicted but associated to the wrong entities. To address this, we build on a Visual Question Answering (VQA) localization strategy targeting one single entity at a time across both visual and textual modalities. We propose a localized human evaluation protocol and introduce a novel automatic metric, Localized VQAScore (L-VQAScore), that combines visual localization with VQA probing both correct (reflection) and miss-localized (leakage) attribute generation. On a newly curated dataset featuring challenging compositional alignment scenarios, L-VQAScore outperforms state-of-the-art T2I evaluation methods in terms of correlation with human judgments, demonstrating its strength in capturing fine-grained entity-attribute associations. We believe L-VQAScore can be a reliable and scalable alternative to subjective evaluations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5L-VQAScore\uff0c\u7528\u4e8e\u89e3\u51b3T2I\u751f\u6210\u6a21\u578b\u5728\u590d\u6742\u7ec4\u5408\u751f\u6210\uff08\u5982\u65f6\u5c1a\u9886\u57df\uff09\u4e2d\u8bc4\u4f30\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u5b9e\u4f53-\u5c5e\u6027\u8bed\u4e49\u5173\u8054\u65b9\u9762\u3002", "motivation": "\u73b0\u6709\u7684T2I\u8bc4\u4f30\u65b9\u6cd5\u5728\u8bc4\u4f30\u590d\u6742\u5b9e\u4f53-\u5c5e\u6027\u8bed\u4e49\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5c5e\u6027\u6df7\u6dc6\u95ee\u9898\uff08\u5c5e\u6027\u6b63\u786e\u4f46\u5173\u8054\u5230\u9519\u8bef\u5b9e\u4f53\uff09\u3002", "method": "\u57fa\u4e8e\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u5b9a\u4f4d\u7b56\u7565\uff0c\u63d0\u51faL-VQAScore\uff0c\u7ed3\u5408\u89c6\u89c9\u5b9a\u4f4d\u548cVQA\u63a2\u6d4b\uff0c\u8bc4\u4f30\u6b63\u786e\u548c\u9519\u8bef\u5b9a\u4f4d\u7684\u5c5e\u6027\u751f\u6210\u3002", "result": "\u5728\u65b0\u6570\u636e\u96c6\u4e0a\uff0cL-VQAScore\u5728\u4e0e\u4eba\u8bc4\u4f30\u7684\u76f8\u5173\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u6355\u6349\u7ec6\u7c92\u5ea6\u5b9e\u4f53-\u5c5e\u6027\u5173\u8054\u3002", "conclusion": "L-VQAScore\u662f\u4e00\u79cd\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u4e3b\u89c2\u8bc4\u4f30\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.07104", "pdf": "https://arxiv.org/pdf/2507.07104", "abs": "https://arxiv.org/abs/2507.07104", "authors": ["Tiezheng Zhang", "Yitong Li", "Yu-cheng Chou", "Jieneng Chen", "Alan Yuille", "Chen Wei", "Junfei Xiao"], "title": "Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation from Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Building state-of-the-art Vision-Language Models (VLMs) with strong captioning capabilities typically necessitates training on billions of high-quality image-text pairs, requiring millions of GPU hours. This paper introduces the Vision-Language-Vision (VLV) auto-encoder framework, which strategically leverages key pretrained components: a vision encoder, the decoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large Language Model (LLM). Specifically, we establish an information bottleneck by regularizing the language representation space, achieved through freezing the pretrained T2I diffusion decoder. Our VLV pipeline effectively distills knowledge from the text-conditioned diffusion model using continuous embeddings, demonstrating comprehensive semantic understanding via high-quality reconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the intermediate language representations into detailed descriptions, we construct a state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o and Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and significantly reduces data requirements; by primarily utilizing single-modal images for training and maximizing the utility of existing pretrained models (image encoder, T2I diffusion model, and LLM), it circumvents the need for massive paired image-text datasets, keeping the total training expenditure under $1,000 USD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVLV\u7684\u81ea\u52a8\u7f16\u7801\u5668\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u7ec4\u4ef6\uff08\u89c6\u89c9\u7f16\u7801\u5668\u3001T2I\u6269\u6563\u6a21\u578b\u89e3\u7801\u5668\u548cLLM\uff09\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u6210\u672c\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u4e0eGPT-4o\u7b49\u9886\u5148\u6a21\u578b\u76f8\u5f53\u7684\u56fe\u50cf\u63cf\u8ff0\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9700\u8981\u5927\u91cf\u9ad8\u8d28\u91cf\u56fe\u50cf-\u6587\u672c\u5bf9\u548c\u6602\u8d35\u8ba1\u7b97\u8d44\u6e90\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u51bb\u7ed3\u9884\u8bad\u7ec3\u7684T2I\u6269\u6563\u89e3\u7801\u5668\u5efa\u7acb\u4fe1\u606f\u74f6\u9888\uff0c\u5229\u7528\u8fde\u7eed\u5d4c\u5165\u4ece\u6269\u6563\u6a21\u578b\u4e2d\u63d0\u53d6\u77e5\u8bc6\uff0c\u5e76\u5fae\u8c03LLM\u751f\u6210\u8be6\u7ec6\u63cf\u8ff0\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u6210\u672c\u6548\u76ca\u6781\u9ad8\u7684\u56fe\u50cf\u63cf\u8ff0\u6a21\u578b\uff0c\u8bad\u7ec3\u6210\u672c\u4f4e\u4e8e1000\u7f8e\u5143\uff0c\u6027\u80fd\u5ab2\u7f8eGPT-4o\u7b49\u9886\u5148\u6a21\u578b\u3002", "conclusion": "VLV\u6846\u67b6\u901a\u8fc7\u5de7\u5999\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6570\u636e\u9700\u6c42\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\u3002"}}
{"id": "2507.07105", "pdf": "https://arxiv.org/pdf/2507.07105", "abs": "https://arxiv.org/abs/2507.07105", "authors": ["Yushen Zuo", "Qi Zheng", "Mingyang Wu", "Xinrui Jiang", "Renjie Li", "Jian Wang", "Yide Zhang", "Gengchen Mai", "Lihong V. Wang", "James Zou", "Xiaoyu Wang", "Ming-Hsuan Yang", "Zhengzhong Tu"], "title": "4KAgent: Agentic Any Image to 4K Super-Resolution", "categories": ["cs.CV", "eess.IV"], "comment": "Project page: https://4kagent.github.io", "summary": "We present 4KAgent, a unified agentic super-resolution generalist system designed to universally upscale any image to 4K resolution (and even higher, if applied iteratively). Our system can transform images from extremely low resolutions with severe degradations, for example, highly distorted inputs at 256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three core components: (1) Profiling, a module that customizes the 4KAgent pipeline based on bespoke use cases; (2) A Perception Agent, which leverages vision-language models alongside image quality assessment experts to analyze the input image and make a tailored restoration plan; and (3) A Restoration Agent, which executes the plan, following a recursive execution-reflection paradigm, guided by a quality-driven mixture-of-expert policy to select the optimal output for each step. Additionally, 4KAgent embeds a specialized face restoration pipeline, significantly enhancing facial details in portrait and selfie photos. We rigorously evaluate our 4KAgent across 11 distinct task categories encompassing a total of 26 diverse benchmarks, setting new state-of-the-art on a broad spectrum of imaging domains. Our evaluations cover natural images, portrait photos, AI-generated content, satellite imagery, fluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and X-ray, demonstrating superior performance in terms of both perceptual (e.g., NIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic paradigm for low-level vision tasks, we aim to catalyze broader interest and innovation within vision-centric autonomous agents across diverse research communities. We will release all the code, models, and results at: https://4kagent.github.io.", "AI": {"tldr": "4KAgent\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u4ee3\u7406\u8d85\u5206\u8fa8\u7387\u901a\u7528\u7cfb\u7edf\uff0c\u53ef\u5c06\u4efb\u4f55\u56fe\u50cf\u63d0\u5347\u81f34K\u5206\u8fa8\u7387\uff0c\u751a\u81f3\u66f4\u9ad8\u3002\u5b83\u901a\u8fc7\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\u5b9e\u73b0\u9ad8\u8d28\u91cf\u56fe\u50cf\u6062\u590d\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u6062\u590d\u95ee\u9898\uff0c\u5c24\u5176\u662f\u4e25\u91cd\u9000\u5316\u7684\u8f93\u5165\uff0c\u901a\u8fc7\u4ee3\u7406\u8303\u5f0f\u63a8\u52a8\u4f4e\u5c42\u6b21\u89c6\u89c9\u4efb\u52a1\u7684\u521b\u65b0\u3002", "method": "\u7cfb\u7edf\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1aProfiling\uff08\u5b9a\u5236\u5316\u6d41\u7a0b\uff09\u3001Perception Agent\uff08\u5206\u6790\u8f93\u5165\u5e76\u5236\u5b9a\u6062\u590d\u8ba1\u5212\uff09\u3001Restoration Agent\uff08\u6267\u884c\u8ba1\u5212\u5e76\u4f18\u5316\u8f93\u51fa\uff09\u3002", "result": "\u572811\u4e2a\u4efb\u52a1\u7c7b\u522b\u548c26\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6db5\u76d6\u81ea\u7136\u56fe\u50cf\u3001\u8096\u50cf\u7167\u7247\u3001AI\u751f\u6210\u5185\u5bb9\u7b49\u591a\u4e2a\u9886\u57df\u3002", "conclusion": "4KAgent\u4e3a\u4f4e\u5c42\u6b21\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u4ee3\u7406\u8303\u5f0f\uff0c\u63a8\u52a8\u4e86\u89c6\u89c9\u81ea\u4e3b\u4ee3\u7406\u7684\u7814\u7a76\u548c\u521b\u65b0\u3002"}}
{"id": "2507.07106", "pdf": "https://arxiv.org/pdf/2507.07106", "abs": "https://arxiv.org/abs/2507.07106", "authors": ["Vatsal Agarwal", "Matthew Gwilliam", "Gefen Kohavi", "Eshan Verma", "Daniel Ulbricht", "Abhinav Shrivastava"], "title": "Towards Multimodal Understanding via Stable Diffusion as a Task-Aware Feature Extractor", "categories": ["cs.CV", "cs.LG"], "comment": "Website: see https://vatsalag99.github.io/mustafar/", "summary": "Recent advances in multimodal large language models (MLLMs) have enabled image-based question-answering capabilities. However, a key limitation is the use of CLIP as the visual encoder; while it can capture coarse global information, it often can miss fine-grained details that are relevant to the input query. To address these shortcomings, this work studies whether pre-trained text-to-image diffusion models can serve as instruction-aware visual encoders. Through an analysis of their internal representations, we find diffusion features are both rich in semantics and can encode strong image-text alignment. Moreover, we find that we can leverage text conditioning to focus the model on regions relevant to the input question. We then investigate how to align these features with large language models and uncover a leakage phenomenon, where the LLM can inadvertently recover information from the original diffusion prompt. We analyze the causes of this leakage and propose a mitigation strategy. Based on these insights, we explore a simple fusion strategy that utilizes both CLIP and conditional diffusion features. We evaluate our approach on both general VQA and specialized MLLM benchmarks, demonstrating the promise of diffusion models for visual understanding, particularly in vision-centric tasks that require spatial and compositional reasoning. Our project page can be found https://vatsalag99.github.io/mustafar/.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u662f\u5426\u53ef\u4ee5\u4f5c\u4e3a\u6307\u4ee4\u611f\u77e5\u7684\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u4ee5\u89e3\u51b3CLIP\u5728\u6355\u6349\u7ec6\u7c92\u5ea6\u7ec6\u8282\u4e0a\u7684\u4e0d\u8db3\u3002\u7814\u7a76\u53d1\u73b0\u6269\u6563\u7279\u5f81\u8bed\u4e49\u4e30\u5bcc\u4e14\u80fd\u7f16\u7801\u5f3a\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u7b56\u7565\u6765\u7ed3\u5408CLIP\u548c\u6269\u6563\u7279\u5f81\u3002", "motivation": "\u89e3\u51b3CLIP\u89c6\u89c9\u7f16\u7801\u5668\u5728\u6355\u6349\u7ec6\u7c92\u5ea6\u7ec6\u8282\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u63a2\u7d22\u6269\u6563\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u5206\u6790\u6269\u6563\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\uff0c\u5229\u7528\u6587\u672c\u6761\u4ef6\u805a\u7126\u76f8\u5173\u533a\u57df\uff0c\u7814\u7a76\u5982\u4f55\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\uff0c\u5e76\u63d0\u51fa\u6cc4\u6f0f\u73b0\u8c61\u7684\u7f13\u89e3\u7b56\u7565\u3002", "result": "\u5728\u901a\u7528VQA\u548c\u4e13\u7528MLLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u6269\u6563\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u4e2d\u7684\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7a7a\u95f4\u548c\u7ec4\u5408\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u9700\u8981\u7ec6\u7c92\u5ea6\u5206\u6790\u7684\u573a\u666f\u4e2d\uff0c\u878d\u5408CLIP\u548c\u6269\u6563\u7279\u5f81\u662f\u4e00\u79cd\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2507.06613", "pdf": "https://arxiv.org/pdf/2507.06613", "abs": "https://arxiv.org/abs/2507.06613", "authors": ["Anshuk Uppal", "Yuhta Takida", "Chieh-Hsin Lai", "Yuki Mitsufuji"], "title": "Denoising Multi-Beta VAE: Representation Learning for Disentanglement and Generation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "24 pages, 8 figures and 7 tables", "summary": "Disentangled and interpretable latent representations in generative models typically come at the cost of generation quality. The $\\beta$-VAE framework introduces a hyperparameter $\\beta$ to balance disentanglement and reconstruction quality, where setting $\\beta > 1$ introduces an information bottleneck that favors disentanglement over sharp, accurate reconstructions. To address this trade-off, we propose a novel generative modeling framework that leverages a range of $\\beta$ values to learn multiple corresponding latent representations. First, we obtain a slew of representations by training a single variational autoencoder (VAE), with a new loss function that controls the information retained in each latent representation such that the higher $\\beta$ value prioritize disentanglement over reconstruction fidelity. We then, introduce a non-linear diffusion model that smoothly transitions latent representations corresponding to different $\\beta$ values. This model denoises towards less disentangled and more informative representations, ultimately leading to (almost) lossless representations, enabling sharp reconstructions. Furthermore, our model supports sample generation without input images, functioning as a standalone generative model. We evaluate our framework in terms of both disentanglement and generation quality. Additionally, we observe smooth transitions in the latent spaces with respect to changes in $\\beta$, facilitating consistent manipulation of generated outputs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u4e0d\u540c\u7684\u03b2\u503c\u5b66\u4e60\u591a\u4e2a\u6f5c\u5728\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u751f\u6210\u8d28\u91cf\u548c\u89e3\u8026\u8868\u793a\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684\u03b2-VAE\u6846\u67b6\u5728\u89e3\u8026\u8868\u793a\u548c\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u9ad8\u03b2\u503c\u504f\u5411\u89e3\u8026\u4f46\u727a\u7272\u91cd\u5efa\u8d28\u91cf\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u8bad\u7ec3\u5355\u4e2aVAE\u4ee5\u5b66\u4e60\u591a\u4e2a\u6f5c\u5728\u8868\u793a\uff0c\u5e76\u7ed3\u5408\u975e\u7ebf\u6027\u6269\u6563\u6a21\u578b\u5e73\u6ed1\u8fc7\u6e21\u4e0d\u540c\u03b2\u503c\u5bf9\u5e94\u7684\u8868\u793a\u3002", "result": "\u6a21\u578b\u5728\u89e3\u8026\u548c\u751f\u6210\u8d28\u91cf\u4e0a\u5747\u8868\u73b0\u826f\u597d\uff0c\u652f\u6301\u65e0\u8f93\u5165\u56fe\u50cf\u7684\u6837\u672c\u751f\u6210\uff0c\u5e76\u5b9e\u73b0\u4e86\u6f5c\u5728\u7a7a\u95f4\u7684\u5e73\u6ed1\u8fc7\u6e21\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5e73\u8861\u4e86\u89e3\u8026\u8868\u793a\u548c\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u652f\u6301\u7075\u6d3b\u7684\u6f5c\u5728\u7a7a\u95f4\u64cd\u4f5c\u3002"}}
{"id": "2507.06828", "pdf": "https://arxiv.org/pdf/2507.06828", "abs": "https://arxiv.org/abs/2507.06828", "authors": ["Xuesong Li", "Nassir Navab", "Zhongliang Jiang"], "title": "Speckle2Self: Self-Supervised Ultrasound Speckle Reduction Without Clean Data", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Image denoising is a fundamental task in computer vision, particularly in medical ultrasound (US) imaging, where speckle noise significantly degrades image quality. Although recent advancements in deep neural networks have led to substantial improvements in denoising for natural images, these methods cannot be directly applied to US speckle noise, as it is not purely random. Instead, US speckle arises from complex wave interference within the body microstructure, making it tissue-dependent. This dependency means that obtaining two independent noisy observations of the same scene, as required by pioneering Noise2Noise, is not feasible. Additionally, blind-spot networks also cannot handle US speckle noise due to its high spatial dependency. To address this challenge, we introduce Speckle2Self, a novel self-supervised algorithm for speckle reduction using only single noisy observations. The key insight is that applying a multi-scale perturbation (MSP) operation introduces tissue-dependent variations in the speckle pattern across different scales, while preserving the shared anatomical structure. This enables effective speckle suppression by modeling the clean image as a low-rank signal and isolating the sparse noise component. To demonstrate its effectiveness, Speckle2Self is comprehensively compared with conventional filter-based denoising algorithms and SOTA learning-based methods, using both realistic simulated US images and human carotid US images. Additionally, data from multiple US machines are employed to evaluate model generalization and adaptability to images from unseen domains. \\textit{Code and datasets will be released upon acceptance.", "AI": {"tldr": "Speckle2Self\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ec5\u4f7f\u7528\u5355\u6b21\u566a\u58f0\u89c2\u6d4b\u51cf\u5c11\u533b\u5b66\u8d85\u58f0\u56fe\u50cf\u4e2d\u7684\u6591\u70b9\u566a\u58f0\u3002", "motivation": "\u533b\u5b66\u8d85\u58f0\u56fe\u50cf\u4e2d\u7684\u6591\u70b9\u566a\u58f0\u5177\u6709\u7ec4\u7ec7\u4f9d\u8d56\u6027\uff0c\u4f20\u7edf\u53bb\u566a\u65b9\u6cd5\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u591a\u5c3a\u5ea6\u6270\u52a8\uff08MSP\uff09\u64cd\u4f5c\u5f15\u5165\u7ec4\u7ec7\u4f9d\u8d56\u7684\u6591\u70b9\u566a\u58f0\u53d8\u5316\uff0c\u540c\u65f6\u4fdd\u7559\u5171\u4eab\u7684\u89e3\u5256\u7ed3\u6784\uff0c\u5c06\u5e72\u51c0\u56fe\u50cf\u5efa\u6a21\u4e3a\u4f4e\u79e9\u4fe1\u53f7\u5e76\u5206\u79bb\u7a00\u758f\u566a\u58f0\u3002", "result": "Speckle2Self\u5728\u6a21\u62df\u548c\u771f\u5b9e\u8d85\u58f0\u56fe\u50cf\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6ee4\u6ce2\u5668\u548c\u6700\u5148\u8fdb\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u8bbe\u5907\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Speckle2Self\u4e3a\u533b\u5b66\u8d85\u58f0\u56fe\u50cf\u53bb\u566a\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u81ea\u76d1\u7763\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
