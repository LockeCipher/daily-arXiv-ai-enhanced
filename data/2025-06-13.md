<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 5]
- [cs.CV](#cs.CV) [Total: 26]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.MM](#cs.MM) [Total: 3]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [FastFLUX: Pruning FLUX with Block-wise Replacement and Sandwich Training](https://arxiv.org/abs/2506.10035)
*Fuhan Cai,Yong Guo,Jie Li,Wenbo Li,Xiangzhong Fang,Jian Chen*

Main category: cs.GR

TL;DR: FastFLUX是一种架构级剪枝框架，旨在提升FLUX模型的推理效率，通过BRLL方法和ST策略，在保持图像质量的同时显著加速推理。


<details>
  <summary>Details</summary>
Motivation: 现有T2I生成模型（如FLUX）参数量大，推理速度慢且部署困难，现有加速方法性能下降明显且训练成本高。

Method: 提出Block-wise Replacement with Linear Layers (BRLL)方法替换复杂残差分支，并引入Sandwich Training (ST)策略进行局部微调。

Result: 实验表明，FastFLUX在剪枝20%层级后仍保持高质量图像生成，同时显著提升推理速度。

Conclusion: FastFLUX通过架构级剪枝和局部微调，有效解决了现有T2I模型推理效率低的问题。

Abstract: Recent advancements in text-to-image (T2I) generation have led to the emergence of highly expressive models such as diffusion transformers (DiTs), exemplified by FLUX. However, their massive parameter sizes lead to slow inference, high memory usage, and poor deployability. Existing acceleration methods (e.g., single-step distillation and attention pruning) often suffer from significant performance degradation and incur substantial training costs. To address these limitations, we propose FastFLUX, an architecture-level pruning framework designed to enhance the inference efficiency of FLUX. At its core is the Block-wise Replacement with Linear Layers (BRLL) method, which replaces structurally complex residual branches in ResBlocks with lightweight linear layers while preserving the original shortcut connections for stability. Furthermore, we introduce Sandwich Training (ST), a localized fine-tuning strategy that leverages LoRA to supervise neighboring blocks, mitigating performance drops caused by structural replacement. Experiments show that our FastFLUX maintains high image quality under both qualitative and quantitative evaluations, while significantly improving inference speed, even with 20\% of the hierarchy pruned. Our code will be available soon.

</details>


### [2] [Token Perturbation Guidance for Diffusion Models](https://arxiv.org/abs/2506.10036)
*Javad Rajabi,Soroush Mehraban,Seyedmorteza Sadat,Babak Taati*

Main category: cs.GR

TL;DR: 提出了一种名为Token Perturbation Guidance (TPG)的新方法，通过扰动中间令牌表示来提升扩散模型的生成质量，无需额外训练且适用于条件与非条件生成。


<details>
  <summary>Details</summary>
Motivation: 解决Classifier-free guidance (CFG)需要特定训练且仅适用于条件生成的局限性。

Method: TPG通过规范保持的扰动操作直接作用于扩散网络中的中间令牌表示，提供稳定且有效的引导信号。

Result: 在SDXL和Stable Diffusion 2.1上，TPG在无条件生成中FID提升近2倍，同时与CFG在提示对齐上表现接近。

Conclusion: TPG是一种通用且条件无关的引导方法，为更广泛的扩散模型带来类似CFG的优势。

Abstract: Classifier-free guidance (CFG) has become an essential component of modern diffusion models to enhance both generation quality and alignment with input conditions. However, CFG requires specific training procedures and is limited to conditional generation. To address these limitations, we propose Token Perturbation Guidance (TPG), a novel method that applies perturbation matrices directly to intermediate token representations within the diffusion network. TPG employs a norm-preserving shuffling operation to provide effective and stable guidance signals that improve generation quality without architectural changes. As a result, TPG is training-free and agnostic to input conditions, making it readily applicable to both conditional and unconditional generation. We further analyze the guidance term provided by TPG and show that its effect on sampling more closely resembles CFG compared to existing training-free guidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1 show that TPG achieves nearly a 2$\times$ improvement in FID for unconditional generation over the SDXL baseline, while closely matching CFG in prompt alignment. These results establish TPG as a general, condition-agnostic guidance method that brings CFG-like benefits to a broader class of diffusion models. The code is available at https://github.com/TaatiTeam/Token-Perturbation-Guidance

</details>


### [3] [Ambient Diffusion Omni: Training Good Models with Bad Data](https://arxiv.org/abs/2506.10038)
*Giannis Daras,Adrian Rodriguez-Munoz,Adam Klivans,Antonio Torralba,Constantinos Daskalakis*

Main category: cs.GR

TL;DR: 利用低质量、合成和分布外图像提升扩散模型质量，提出Ambient Diffusion Omni框架，通过利用图像的光谱功率律衰减和局部性，显著改进图像生成质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型依赖高质量数据集，但低质量图像中仍蕴含有用信息，探索如何利用这些被丢弃的图像提升模型性能。

Method: 提出Ambient Diffusion Omni框架，通过分析图像的光谱功率律衰减和局部性，从所有可用图像中提取信号，支持合成损坏图像训练。

Result: 在ImageNet上实现最佳FID分数，文本到图像生成的质量和多样性均有显著提升。

Conclusion: 噪声可平衡高质量分布与混合分布之间的偏差，理论分析验证了方法的有效性。

Abstract: We show how to use low-quality, synthetic, and out-of-distribution images to improve the quality of a diffusion model. Typically, diffusion models are trained on curated datasets that emerge from highly filtered data pools from the Web and other sources. We show that there is immense value in the lower-quality images that are often discarded. We present Ambient Diffusion Omni, a simple, principled framework to train diffusion models that can extract signal from all available images during training. Our framework exploits two properties of natural images -- spectral power law decay and locality. We first validate our framework by successfully training diffusion models with images synthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We then use our framework to achieve state-of-the-art ImageNet FID, and we show significant improvements in both image quality and diversity for text-to-image generative modeling. The core insight is that noise dampens the initial skew between the desired high-quality distribution and the mixed distribution we actually observe. We provide rigorous theoretical justification for our approach by analyzing the trade-off between learning from biased data versus limited unbiased data across diffusion times.

</details>


### [4] [Low-Barrier Dataset Collection with Real Human Body for Interactive Per-Garment Virtual Try-On](https://arxiv.org/abs/2506.10468)
*Zaiqiang Wu,Yechen Li,Jingyuan Liu,Yuki Shibata,Takayuki Hori,I-Chao Shen,Takeo Igarashi*

Main category: cs.GR

TL;DR: 提出一种低成本、高效的方法，通过真实人体采集数据，改进虚拟试衣的准确性和实时性。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试衣方法局限于正面视角且缺乏实时性，同时依赖昂贵机器人模型，难以普及且无法准确模拟人体变形。

Method: 使用真实人体采集数据，无需定制机器人模型；引入混合人物表示，结合简化的DensePose图提升对齐精度。

Result: 在图像质量和时间一致性上优于现有方法，用户研究表明系统有助于服装购买决策。

Conclusion: 该方法降低了数据采集成本，提高了虚拟试衣的准确性和实用性。

Abstract: Existing image-based virtual try-on methods are often limited to the front view and lack real-time performance. While per-garment virtual try-on methods have tackled these issues by capturing per-garment datasets and training per-garment neural networks, they still encounter practical limitations: (1) the robotic mannequin used to capture per-garment datasets is prohibitively expensive for widespread adoption and fails to accurately replicate natural human body deformation; (2) the synthesized garments often misalign with the human body. To address these challenges, we propose a low-barrier approach for collecting per-garment datasets using real human bodies, eliminating the necessity for a customized robotic mannequin. We also introduce a hybrid person representation that enhances the existing intermediate representation with a simplified DensePose map. This ensures accurate alignment of synthesized garment images with the human body and enables human-garment interaction without the need for customized wearable devices. We performed qualitative and quantitative evaluations against other state-of-the-art image-based virtual try-on methods and conducted ablation studies to demonstrate the superiority of our method regarding image quality and temporal consistency. Finally, our user study results indicated that most participants found our virtual try-on system helpful for making garment purchasing decisions.

</details>


### [5] [Edit360: 2D Image Edits to 3D Assets from Any Angle](https://arxiv.org/abs/2506.10507)
*Junchao Huang,Xinting Hu,Zhuotao Tian,Shaoshuai Shi,Li Jiang*

Main category: cs.GR

TL;DR: Edit360是一个无需调整的框架，将2D编辑扩展到多视角一致的3D编辑，解决了现有方法在视角限制上的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D资产编辑中受限于预定的视角，缺乏灵活性和实用性。

Method: 基于视频扩散模型，Edit360通过锚点视角编辑传播机制，在潜在和注意力空间中对齐多视角信息。

Result: 实现了高质量的多视角序列编辑，支持可定制的3D内容创建。

Conclusion: Edit360为3D编辑提供了灵活且一致的多视角解决方案。

Abstract: Recent advances in diffusion models have significantly improved image generation and editing, but extending these capabilities to 3D assets remains challenging, especially for fine-grained edits that require multi-view consistency. Existing methods typically restrict editing to predetermined viewing angles, severely limiting their flexibility and practical applications. We introduce Edit360, a tuning-free framework that extends 2D modifications to multi-view consistent 3D editing. Built upon video diffusion models, Edit360 enables user-specific editing from arbitrary viewpoints while ensuring structural coherence across all views. The framework selects anchor views for 2D modifications and propagates edits across the entire 360-degree range. To achieve this, Edit360 introduces a novel Anchor-View Editing Propagation mechanism, which effectively aligns and merges multi-view information within the latent and attention spaces of diffusion models. The resulting edited multi-view sequences facilitate the reconstruction of high-quality 3D assets, enabling customizable 3D content creation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [6] [Multimodal Cinematic Video Synthesis Using Text-to-Image and Audio Generation Models](https://arxiv.org/abs/2506.10005)
*Sridhar S,Nithin A,Shakeel Rifath,Vasantha Raj*

Main category: cs.CV

TL;DR: 该论文提出了一种结合Stable Diffusion、GPT-2和混合音频管道的60秒电影自动生成方法，通过五场景框架和优化技术实现高质量输出。


<details>
  <summary>Details</summary>
Motivation: 利用生成式人工智能技术改进多媒体创作，实现从文本输入到专业级电影视频的自动合成。

Method: 结合Stable Diffusion生成高保真图像，GPT-2构建叙事结构，混合音频管道（gTTS和YouTube音乐），并采用五场景框架、线性帧插值和后期处理技术。

Result: 实验显示该方法在视觉质量、叙事连贯性和效率方面表现优异。

Conclusion: 该方法进一步推动了文本到视频合成技术在创意、教育和工业领域的应用。

Abstract: Advances in generative artificial intelligence have altered multimedia creation, allowing for automatic cinematic video synthesis from text inputs. This work describes a method for creating 60-second cinematic movies incorporating Stable Diffusion for high-fidelity image synthesis, GPT-2 for narrative structuring, and a hybrid audio pipeline using gTTS and YouTube-sourced music. It uses a five-scene framework, which is augmented by linear frame interpolation, cinematic post-processing (e.g., sharpening), and audio-video synchronization to provide professional-quality results. It was created in a GPU-accelerated Google Colab environment using Python 3.11. It has a dual-mode Gradio interface (Simple and Advanced), which supports resolutions of up to 1024x768 and frame rates of 15-30 FPS. Optimizations such as CUDA memory management and error handling ensure reliability. The experiments demonstrate outstanding visual quality, narrative coherence, and efficiency, furthering text-to-video synthesis for creative, educational, and industrial applications.

</details>


### [7] [LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware LoRA Fine-Tuning](https://arxiv.org/abs/2506.10082)
*Chenjian Gao,Lihe Ding,Xin Cai,Zhanpeng Huang,Zibin Wang,Tianfan Xue*

Main category: cs.CV

TL;DR: 提出了一种基于掩码的LoRA调优方法，用于灵活的视频编辑，无需改变模型架构。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法依赖大规模预训练，缺乏灵活性，尤其是对后续帧的控制不足。

Method: 采用掩码驱动的LoRA调优策略，结合输入视频和参考图像，动态调节模型注意力。

Result: 实验结果表明，该方法在视频编辑性能上优于现有技术。

Conclusion: 该方法提供了一种高效且灵活的视频编辑解决方案，适用于特定编辑需求。

Abstract: Video editing using diffusion models has achieved remarkable results in generating high-quality edits for videos. However, current methods often rely on large-scale pretraining, limiting flexibility for specific edits. First-frame-guided editing provides control over the first frame, but lacks flexibility over subsequent frames. To address this, we propose a mask-based LoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video (I2V) models for flexible video editing. Our approach preserves background regions while enabling controllable edits propagation. This solution offers efficient and adaptable video editing without altering the model architecture. To better steer this process, we incorporate additional references, such as alternate viewpoints or representative scene states, which serve as visual anchors for how content should unfold. We address the control challenge using a mask-driven LoRA tuning strategy that adapts a pre-trained image-to-video model to the editing context. The model must learn from two distinct sources: the input video provides spatial structure and motion cues, while reference images offer appearance guidance. A spatial mask enables region-specific learning by dynamically modulating what the model attends to, ensuring that each area draws from the appropriate source. Experimental results show our method achieves superior video editing performance compared to state-of-the-art methods.

</details>


### [8] [SPARKE: Scalable Prompt-Aware Diversity Guidance in Diffusion Models via RKE Score](https://arxiv.org/abs/2506.10173)
*Mohammad Jalali,Haoyu Lei,Amin Gohari,Farzan Farnia*

Main category: cs.CV

TL;DR: SPARKE方法通过条件熵实现提示感知的多样性控制，显著提升了生成数据的多样性，同时降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决提示引导扩散模型中生成样本多样性不足的问题，尤其是在语义广泛的提示下。

Method: 提出SPARKE方法，利用条件熵动态测量多样性，并通过优化计算复杂度从O(n^3)降至O(n)。

Result: 在多个文本到图像扩散模型上验证，SPARKE显著提升了生成数据的多样性且计算成本低。

Conclusion: SPARKE是一种高效且可扩展的提示感知多样性引导方法。

Abstract: Diffusion models have demonstrated remarkable success in high-fidelity image synthesis and prompt-guided generative modeling. However, ensuring adequate diversity in generated samples of prompt-guided diffusion models remains a challenge, particularly when the prompts span a broad semantic spectrum and the diversity of generated data needs to be evaluated in a prompt-aware fashion across semantically similar prompts. Recent methods have introduced guidance via diversity measures to encourage more varied generations. In this work, we extend the diversity measure-based approaches by proposing the Scalable Prompt-Aware R\'eny Kernel Entropy Diversity Guidance (SPARKE) method for prompt-aware diversity guidance. SPARKE utilizes conditional entropy for diversity guidance, which dynamically conditions diversity measurement on similar prompts and enables prompt-aware diversity control. While the entropy-based guidance approach enhances prompt-aware diversity, its reliance on the matrix-based entropy scores poses computational challenges in large-scale generation settings. To address this, we focus on the special case of Conditional latent RKE Score Guidance, reducing entropy computation and gradient-based optimization complexity from the $O(n^3)$ of general entropy measures to $O(n)$. The reduced computational complexity allows for diversity-guided sampling over potentially thousands of generation rounds on different prompts. We numerically test the SPARKE method on several text-to-image diffusion models, demonstrating that the proposed method improves the prompt-aware diversity of the generated data without incurring significant computational costs. We release our code on the project page: https://mjalali.github.io/SPARKE

</details>


### [9] [Improving Personalized Search with Regularized Low-Rank Parameter Updates](https://arxiv.org/abs/2506.10182)
*Fiona Ryan,Josef Sivic,Fabian Caba Heilbron,Judy Hoffman,James M. Rehg,Bryan Russell*

Main category: cs.CV

TL;DR: 本文提出了一种通过正则化低秩适应来优化视觉-语言双编码器模型的方法，用于个性化视觉-语言检索，并在两个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 个性化视觉-语言检索需要从少量示例中学习新概念，同时整合个人与通用知识，挑战在于如何在有限数据下实现高效适应。

Method: 采用正则化低秩适应方法，仅调整语言编码器最后一层的少量参数，避免破坏通用知识。同时探索了多概念参数组合策略。

Result: 在两个基准测试（DeepFashion2和ConCon-Chi）上，个性化检索准确率比现有方法提高了4%-22%。

Conclusion: 该方法在保持通用知识的同时，显著提升了个性化检索性能，为小样本学习提供了有效解决方案。

Abstract: Personalized vision-language retrieval seeks to recognize new concepts (e.g. "my dog Fido") from only a few examples. This task is challenging because it requires not only learning a new concept from a few images, but also integrating the personal and general knowledge together to recognize the concept in different contexts. In this paper, we show how to effectively adapt the internal representation of a vision-language dual encoder model for personalized vision-language retrieval. We find that regularized low-rank adaption of a small set of parameters in the language encoder's final layer serves as a highly effective alternative to textual inversion for recognizing the personal concept while preserving general knowledge. Additionally, we explore strategies for combining parameters of multiple learned personal concepts, finding that parameter addition is effective. To evaluate how well general knowledge is preserved in a finetuned representation, we introduce a metric that measures image retrieval accuracy based on captions generated by a vision language model (VLM). Our approach achieves state-of-the-art accuracy on two benchmarks for personalized image retrieval with natural language queries - DeepFashion2 and ConCon-Chi - outperforming the prior art by 4%-22% on personal retrievals.

</details>


### [10] [PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian Splatting](https://arxiv.org/abs/2506.10335)
*Lintao Xiang,Hongpei Zheng,Yating Huang,Qijun Yang,Hujun Yin*

Main category: cs.CV

TL;DR: 提出了一种基于点特征感知的高斯泼溅框架，解决了3D高斯泼溅在稀疏视图输入下过拟合的问题，实现了实时高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法需要大量校准视图，稀疏输入时易过拟合，导致渲染质量下降。

Method: 利用立体基础模型估计相机姿态和稠密点云，通过多尺度2D特征采样和自注意力机制增强点间交互，最终通过MLP解码为高斯参数。

Result: 在多种基准测试中显著优于NeRF方法，并在少样本设置下与最先进的3DGS方法竞争。

Conclusion: 该方法在稀疏视图下实现了高质量实时渲染，为3D场景表示提供了新思路。

Abstract: 3D Gaussian splatting (3DGS) is an innovative rendering technique that surpasses the neural radiance field (NeRF) in both rendering speed and visual quality by leveraging an explicit 3D scene representation. Existing 3DGS approaches require a large number of calibrated views to generate a consistent and complete scene representation. When input views are limited, 3DGS tends to overfit the training views, leading to noticeable degradation in rendering quality. To address this limitation, we propose a Point-wise Feature-Aware Gaussian Splatting framework that enables real-time, high-quality rendering from sparse training views. Specifically, we first employ the latest stereo foundation model to estimate accurate camera poses and reconstruct a dense point cloud for Gaussian initialization. We then encode the colour attributes of each 3D Gaussian by sampling and aggregating multiscale 2D appearance features from sparse inputs. To enhance point-wise appearance representation, we design a point interaction network based on a self-attention mechanism, allowing each Gaussian point to interact with its nearest neighbors. These enriched features are subsequently decoded into Gaussian parameters through two lightweight multi-layer perceptrons (MLPs) for final rendering. Extensive experiments on diverse benchmarks demonstrate that our method significantly outperforms NeRF-based approaches and achieves competitive performance under few-shot settings compared to the state-of-the-art 3DGS methods.

</details>


### [11] [Revisiting Transformers with Insights from Image Filtering](https://arxiv.org/abs/2506.10371)
*Laziz U. Abdullaev,Maksim Tkachenko,Tan M. Nguyen*

Main category: cs.CV

TL;DR: 本文通过图像处理框架解释自注意力机制及其组件（如位置编码和残差连接），并提出两种改进方法，既提升可解释性，又提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制虽成功但缺乏理论解释，现有框架未能深入解析其组件作用，本文旨在填补这一空白。

Method: 开发统一的图像处理框架，解释自注意力及其组件，并引入两种改进方法。

Result: 改进方法不仅增强可解释性，还在语言和视觉任务中提升准确性和鲁棒性。

Conclusion: 图像处理框架为自注意力提供新视角，改进方法兼具理论和实用价值。

Abstract: The self-attention mechanism, a cornerstone of Transformer-based state-of-the-art deep learning architectures, is largely heuristic-driven and fundamentally challenging to interpret. Establishing a robust theoretical foundation to explain its remarkable success and limitations has therefore become an increasingly prominent focus in recent research. Some notable directions have explored understanding self-attention through the lens of image denoising and nonparametric regression. While promising, existing frameworks still lack a deeper mechanistic interpretation of various architectural components that enhance self-attention, both in its original formulation and subsequent variants. In this work, we aim to advance this understanding by developing a unifying image processing framework, capable of explaining not only the self-attention computation itself but also the role of components such as positional encoding and residual connections, including numerous later variants. We also pinpoint potential distinctions between the two concepts building upon our framework, and make effort to close this gap. We introduce two independent architectural modifications within transformers. While our primary objective is interpretability, we empirically observe that image processing-inspired modifications can also lead to notably improved accuracy and robustness against data contamination and adversaries across language and vision tasks as well as better long sequence understanding.

</details>


### [12] [ReconMOST: Multi-Layer Sea Temperature Reconstruction with Observations-Guided Diffusion](https://arxiv.org/abs/2506.10391)
*Yuanyi Song,Pumeng Lyu,Ben Fei,Fenghua Ling,Wanli Ouyang,Lei Bai*

Main category: cs.CV

TL;DR: 提出ReconMOST框架，利用数据驱动的扩散模型进行多层海水温度重建，解决传统方法数据稀疏、计算复杂的问题，并在全球范围内实现高精度重建。


<details>
  <summary>Details</summary>
Motivation: 传统海洋温度重建方法受限于数据稀疏和计算复杂度，机器学习方法多局限于表层或局部区域。本文旨在解决这些问题，实现全球多层温度重建。

Method: 预训练无条件扩散模型学习海洋温度场的物理分布模式，利用稀疏但高精度的现场观测数据指导反向扩散过程，生成重建结果。

Result: 在CMIP6和EN4数据上的实验显示，MSE值分别为0.049（指导）、0.680（重建）和0.633（总体），验证了框架的有效性和鲁棒性。

Conclusion: ReconMOST框架在全球多层海洋温度重建中表现出色，处理了92.5%的缺失数据，同时保持高精度和空间分辨率。

Abstract: Accurate reconstruction of ocean is essential for reflecting global climate dynamics and supporting marine meteorological research. Conventional methods face challenges due to sparse data, algorithmic complexity, and high computational costs, while increasing usage of machine learning (ML) method remains limited to reconstruction problems at the sea surface and local regions, struggling with issues like cloud occlusion. To address these limitations, this paper proposes ReconMOST, a data-driven guided diffusion model framework for multi-layer sea temperature reconstruction. Specifically, we first pre-train an unconditional diffusion model using a large collection of historical numerical simulation data, enabling the model to attain physically consistent distribution patterns of ocean temperature fields. During the generation phase, sparse yet high-accuracy in-situ observational data are utilized as guidance points for the reverse diffusion process, generating accurate reconstruction results. Importantly, in regions lacking direct observational data, the physically consistent spatial distribution patterns learned during pre-training enable implicitly guided and physically plausible reconstructions. Our method extends ML-based SST reconstruction to a global, multi-layer setting, handling over 92.5% missing data while maintaining reconstruction accuracy, spatial resolution, and superior generalization capability. We pre-train our model on CMIP6 numerical simulation data and conduct guided reconstruction experiments on CMIP6 and EN4 analysis data. The results of mean squared error (MSE) values achieve 0.049 on guidance, 0.680 on reconstruction, and 0.633 on total, respectively, demonstrating the effectiveness and robustness of the proposed framework. Our source code is available at https://github.com/norsheep/ReconMOST.

</details>


### [13] [Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation](https://arxiv.org/abs/2506.10395)
*Zhiyang Xu,Jiuhai Chen,Zhaojiang Lin,Xichen Pan,Lifu Huang,Tianyi Zhou,Madian Khabsa,Qifan Wang,Di Jin,Michihiro Yasunaga,Lili Yu,Xi Victoria Lin,Shaoliang Nie*

Main category: cs.CV

TL;DR: Pisces是一种新型的多模态基础模型，通过解耦视觉编码架构和优化的训练技术，在图像理解和生成任务中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在多模态任务中取得进展，但统一模型在图像理解和生成任务中表现不如专用模型。Pisces旨在解决这一挑战。

Method: 采用解耦视觉编码架构和定制化训练技术，结合精细的数据处理、预训练和微调。

Result: 在20多个图像理解基准测试中表现优异，并在图像生成基准测试GenEval中展现出强大的生成能力。

Conclusion: Pisces通过解耦视觉编码架构和优化训练技术，证明了图像理解与生成之间的协同关系，推动了统一多模态模型的发展。

Abstract: Recent advances in large language models (LLMs) have enabled multimodal foundation models to tackle both image understanding and generation within a unified framework. Despite these gains, unified models often underperform compared to specialized models in either task. A key challenge in developing unified models lies in the inherent differences between the visual features needed for image understanding versus generation, as well as the distinct training processes required for each modality. In this work, we introduce Pisces, an auto-regressive multimodal foundation model that addresses this challenge through a novel decoupled visual encoding architecture and tailored training techniques optimized for multimodal generation. Combined with meticulous data curation, pretraining, and finetuning, Pisces achieves competitive performance in both image understanding and image generation. We evaluate Pisces on over 20 public benchmarks for image understanding, where it demonstrates strong performance across a wide range of tasks. Additionally, on GenEval, a widely adopted benchmark for image generation, Pisces exhibits robust generative capabilities. Our extensive analysis reveals the synergistic relationship between image understanding and generation, and the benefits of using separate visual encoders, advancing the field of unified multimodal models.

</details>


### [14] [DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers](https://arxiv.org/abs/2506.10568)
*Lizhen Wang,Zhurong Xia,Tianshu Hu,Pengrui Wang,Pengfei Wang,Zerong Zheng,Ming Zhou*

Main category: cs.CV

TL;DR: 提出了一种基于扩散变换器（DiT）的框架，用于生成高保真的人-产品演示视频，解决了现有方法在身份保持和空间关系理解上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有框架在生成人-产品演示视频时，难以同时保持人和产品的身份特征，且缺乏对空间关系的理解，导致不真实的表现和不自然的交互。

Method: 采用DiT框架，注入配对的人-产品参考信息，并使用掩码交叉注意力机制；利用3D身体网格模板和产品边界框提供精确运动指导；结合结构化文本编码增强3D一致性。

Result: 在混合数据集上训练，通过数据增强策略，方法在保持身份完整性和生成真实演示动作方面优于现有技术。

Conclusion: 提出的DiT框架有效解决了人-产品演示视频生成中的身份保持和空间关系问题，生成结果更真实自然。

Abstract: In e-commerce and digital marketing, generating high-fidelity human-product demonstration videos is important for effective product presentation. However, most existing frameworks either fail to preserve the identities of both humans and products or lack an understanding of human-product spatial relationships, leading to unrealistic representations and unnatural interactions. To address these challenges, we propose a Diffusion Transformer (DiT)-based framework. Our method simultaneously preserves human identities and product-specific details, such as logos and textures, by injecting paired human-product reference information and utilizing an additional masked cross-attention mechanism. We employ a 3D body mesh template and product bounding boxes to provide precise motion guidance, enabling intuitive alignment of hand gestures with product placements. Additionally, structured text encoding is used to incorporate category-level semantics, enhancing 3D consistency during small rotational changes across frames. Trained on a hybrid dataset with extensive data augmentation strategies, our approach outperforms state-of-the-art techniques in maintaining the identity integrity of both humans and products and generating realistic demonstration motions. Project page: https://submit2025-dream.github.io/DreamActor-H1/.

</details>


### [15] [Text to Image for Multi-Label Image Recognition with Joint Prompt-Adapter Learning](https://arxiv.org/abs/2506.10575)
*Chun-Mei Feng,Kai Yu,Xinxing Xu,Salman Khan,Rick Siow Mong Goh,Wangmeng Zuo,Yong Liu*

Main category: cs.CV

TL;DR: T2I-PAL利用文本生成图像减少模态差异，结合热图和原型学习提升多标签识别性能，无需全标注图像。


<details>
  <summary>Details</summary>
Motivation: 解决CLIP模型中文本与图像模态差异问题，提升仅用文本进行参数高效微调时的图像识别性能。

Method: 利用预训练文本生成模型生成图像，结合类热图和可学习原型，同时使用提示调优和适配器学习。

Result: 在多个基准测试中平均性能提升3.47%，优于现有方法。

Conclusion: T2I-PAL有效减少模态差异，提升多标签识别性能，且无需全标注图像，兼容现有CLIP框架。

Abstract: Benefited from image-text contrastive learning, pre-trained vision-language models, e.g., CLIP, allow to direct leverage texts as images (TaI) for parameter-efficient fine-tuning (PEFT). While CLIP is capable of making image features to be similar to the corresponding text features, the modality gap remains a nontrivial issue and limits image recognition performance of TaI. Using multi-label image recognition (MLR) as an example, we present a novel method, called T2I-PAL to tackle the modality gap issue when using only text captions for PEFT. The core design of T2I-PAL is to leverage pre-trained text-to-image generation models to generate photo-realistic and diverse images from text captions, thereby reducing the modality gap. To further enhance MLR, T2I-PAL incorporates a class-wise heatmap and learnable prototypes. This aggregates local similarities, making the representation of local visual features more robust and informative for multi-label recognition. For better PEFT, we further combine both prompt tuning and adapter learning to enhance classification performance. T2I-PAL offers significant advantages: it eliminates the need for fully semantically annotated training images, thereby reducing the manual annotation workload, and it preserves the intrinsic mode of the CLIP model, allowing for seamless integration with any existing CLIP framework. Extensive experiments on multiple benchmarks, including MS-COCO, VOC2007, and NUS-WIDE, show that our T2I-PAL can boost recognition performance by 3.47% in average above the top-ranked state-of-the-art methods.

</details>


### [16] [High-resolution efficient image generation from WiFi CSI using a pretrained latent diffusion model](https://arxiv.org/abs/2506.10605)
*Eshan Ramesh,Nishio Takayuki*

Main category: cs.CV

TL;DR: LatentCSI利用预训练的潜在扩散模型（LDM）从WiFi CSI测量生成物理环境图像，通过轻量级神经网络直接映射CSI幅度到LDM的潜在空间，实现高效高质量的图像合成。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法依赖复杂计算（如GANs）和显式图像编码的问题，提出一种更高效且可控的图像生成方法。

Method: 使用轻量级神经网络将CSI幅度映射到LDM潜在空间，结合文本引导的扩散模型生成高分辨率图像。

Result: 在公开数据集和自采数据集上验证，LatentCSI在计算效率和感知质量上优于基线方法，并支持文本引导控制。

Conclusion: LatentCSI提供了一种高效、高质量且可控的图像生成方案，适用于WiFi CSI到图像的转换。

Abstract: We present LatentCSI, a novel method for generating images of the physical environment from WiFi CSI measurements that leverages a pretrained latent diffusion model (LDM). Unlike prior approaches that rely on complex and computationally intensive techniques such as GANs, our method employs a lightweight neural network to map CSI amplitudes directly into the latent space of an LDM. We then apply the LDM's denoising diffusion model to the latent representation with text-based guidance before decoding using the LDM's pretrained decoder to obtain a high-resolution image. This design bypasses the challenges of pixel-space image generation and avoids the explicit image encoding stage typically required in conventional image-to-image pipelines, enabling efficient and high-quality image synthesis. We validate our approach on two datasets: a wide-band CSI dataset we collected with off-the-shelf WiFi devices and cameras; and a subset of the publicly available MM-Fi dataset. The results demonstrate that LatentCSI outperforms baselines of comparable complexity trained directly on ground-truth images in both computational efficiency and perceptual quality, while additionally providing practical advantages through its unique capacity for text-guided controllability.

</details>


### [17] [TexTailor: Customized Text-aligned Texturing via Effective Resampling](https://arxiv.org/abs/2506.10612)
*Suin Lee,Dae-Shik Kim*

Main category: cs.CV

TL;DR: TexTailor是一种新方法，通过改进扩散模型和自适应相机位置，从文本描述生成一致的对象纹理。


<details>
  <summary>Details</summary>
Motivation: 现有方法在纹理合成过程中存在视角间纹理属性逐渐偏移的问题，且相机位置固定导致纹理信息利用不足。

Method: 采用重采样方案整合已合成纹理信息，微调深度感知扩散模型，并提出性能保留损失和自适应相机位置调整。

Result: 在Objaverse和ShapeNet数据集上，TexTailor在合成视角一致纹理方面优于现有方法。

Conclusion: TexTailor通过改进纹理合成过程，显著提升了纹理一致性和质量。

Abstract: We present TexTailor, a novel method for generating consistent object textures from textual descriptions. Existing text-to-texture synthesis approaches utilize depth-aware diffusion models to progressively generate images and synthesize textures across predefined multiple viewpoints. However, these approaches lead to a gradual shift in texture properties across viewpoints due to (1) insufficient integration of previously synthesized textures at each viewpoint during the diffusion process and (2) the autoregressive nature of the texture synthesis process. Moreover, the predefined selection of camera positions, which does not account for the object's geometry, limits the effective use of texture information synthesized from different viewpoints, ultimately degrading overall texture consistency. In TexTailor, we address these issues by (1) applying a resampling scheme that repeatedly integrates information from previously synthesized textures within the diffusion process, and (2) fine-tuning a depth-aware diffusion model on these resampled textures. During this process, we observed that using only a few training images restricts the model's original ability to generate high-fidelity images aligned with the conditioning, and therefore propose an performance preservation loss to mitigate this issue. Additionally, we improve the synthesis of view-consistent textures by adaptively adjusting camera positions based on the object's geometry. Experiments on a subset of the Objaverse dataset and the ShapeNet car dataset demonstrate that TexTailor outperforms state-of-the-art methods in synthesizing view-consistent textures. The source code for TexTailor is available at https://github.com/Adios42/Textailor

</details>


### [18] [Anatomy-Grounded Weakly Supervised Prompt Tuning for Chest X-ray Latent Diffusion Models](https://arxiv.org/abs/2506.10633)
*Konstantinos Vilouras,Ilias Stogiannidis,Junyu Yan,Alison Q. O'Neil,Sotirios A. Tsaftaris*

Main category: cs.CV

TL;DR: 本文提出了一种针对医学影像的文本到图像潜在扩散模型微调框架，以改善多模态对齐，并在标准数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像领域的文本到图像潜在扩散模型因数据有限而研究不足，且现有模型未能有效对齐临床相关文本与影像区域。

Method: 通过微调预训练模型，提出了一种改进多模态对齐的框架，适用于短语定位等下游任务。

Result: 在MS-CXR数据集上达到新SOTA，并在VinDr-CXR数据上表现出鲁棒性。

Conclusion: 该方法为医学影像领域的多模态对齐提供了有效解决方案，并具有实际应用潜力。

Abstract: Latent Diffusion Models have shown remarkable results in text-guided image synthesis in recent years. In the domain of natural (RGB) images, recent works have shown that such models can be adapted to various vision-language downstream tasks with little to no supervision involved. On the contrary, text-to-image Latent Diffusion Models remain relatively underexplored in the field of medical imaging, primarily due to limited data availability (e.g., due to privacy concerns). In this work, focusing on the chest X-ray modality, we first demonstrate that a standard text-conditioned Latent Diffusion Model has not learned to align clinically relevant information in free-text radiology reports with the corresponding areas of the given scan. Then, to alleviate this issue, we propose a fine-tuning framework to improve multi-modal alignment in a pre-trained model such that it can be efficiently repurposed for downstream tasks such as phrase grounding. Our method sets a new state-of-the-art on a standard benchmark dataset (MS-CXR), while also exhibiting robust performance on out-of-distribution data (VinDr-CXR). Our code will be made publicly available.

</details>


### [19] [Symmetrical Flow Matching: Unified Image Generation, Segmentation, and Classification with Score-Based Generative Models](https://arxiv.org/abs/2506.10634)
*Francisco Caetano,Christiaan Viviers,Peter H. N. De With,Fons van der Sommen*

Main category: cs.CV

TL;DR: SymmFlow是一种对称流匹配框架，统一了语义分割、分类和图像生成，通过双向一致性保持生成多样性，并在多任务中实现高效采样。


<details>
  <summary>Details</summary>
Motivation: 现有方法在分布变换中缺乏灵活性和一致性，SymmFlow旨在通过对称学习目标统一多任务并保留语义信息。

Method: 采用对称学习目标联合建模前向和反向变换，引入新训练目标显式保留语义信息，支持像素级和图像级标签。

Result: 在CelebAMask-HQ和COCO-Stuff上分别取得FID 11.9和7.0，语义分割和分类任务表现优异。

Conclusion: SymmFlow在多任务中实现高效生成和语义保留，性能领先，代码将开源。

Abstract: Flow Matching has emerged as a powerful framework for learning continuous transformations between distributions, enabling high-fidelity generative modeling. This work introduces Symmetrical Flow Matching (SymmFlow), a new formulation that unifies semantic segmentation, classification, and image generation within a single model. Using a symmetric learning objective, SymmFlow models forward and reverse transformations jointly, ensuring bi-directional consistency, while preserving sufficient entropy for generative diversity. A new training objective is introduced to explicitly retain semantic information across flows, featuring efficient sampling while preserving semantic structure, allowing for one-step segmentation and classification without iterative refinement. Unlike previous approaches that impose strict one-to-one mapping between masks and images, SymmFlow generalizes to flexible conditioning, supporting both pixel-level and image-level class labels. Experimental results on various benchmarks demonstrate that SymmFlow achieves state-of-the-art performance on semantic image synthesis, obtaining FID scores of 11.9 on CelebAMask-HQ and 7.0 on COCO-Stuff with only 25 inference steps. Additionally, it delivers competitive results on semantic segmentation and shows promising capabilities in classification tasks. The code will be publicly available.

</details>


### [20] [GigaVideo-1: Advancing Video Generation via Automatic Feedback with 4 GPU-Hours Fine-Tuning](https://arxiv.org/abs/2506.10639)
*Xiaoyi Bao,Jindi Lv,Xiaofeng Wang,Zheng Zhu,Xinze Chen,YuKun Zhou,Jiancheng Lv,Xingang Wang,Guan Huang*

Main category: cs.CV

TL;DR: GigaVideo-1是一种无需人工标注的高效视频生成微调框架，通过自动反馈提升预训练模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在实例保留、运动合理性等方面仍需微调，但传统方法依赖人工标注和高计算资源，实用性受限。

Method: 提出基于提示的数据引擎和奖励引导的训练策略，利用预训练视觉语言模型自动优化数据与训练过程。

Result: 在VBench-2.0基准测试中，GigaVideo-1在17个维度上平均提升4%，仅需4 GPU小时。

Conclusion: GigaVideo-1无需人工标注和大量真实数据，展示了高效且有效的视频生成微调方法。

Abstract: Recent progress in diffusion models has greatly enhanced video generation quality, yet these models still require fine-tuning to improve specific dimensions like instance preservation, motion rationality, composition, and physical plausibility. Existing fine-tuning approaches often rely on human annotations and large-scale computational resources, limiting their practicality. In this work, we propose GigaVideo-1, an efficient fine-tuning framework that advances video generation without additional human supervision. Rather than injecting large volumes of high-quality data from external sources, GigaVideo-1 unlocks the latent potential of pre-trained video diffusion models through automatic feedback. Specifically, we focus on two key aspects of the fine-tuning process: data and optimization. To improve fine-tuning data, we design a prompt-driven data engine that constructs diverse, weakness-oriented training samples. On the optimization side, we introduce a reward-guided training strategy, which adaptively weights samples using feedback from pre-trained vision-language models with a realism constraint. We evaluate GigaVideo-1 on the VBench-2.0 benchmark using Wan2.1 as the baseline across 17 evaluation dimensions. Experiments show that GigaVideo-1 consistently improves performance on almost all the dimensions with an average gain of about 4% using only 4 GPU-hours. Requiring no manual annotations and minimal real data, GigaVideo-1 demonstrates both effectiveness and efficiency. Code, model, and data will be publicly available.

</details>


### [21] [Enhancing Deepfake Detection using SE Block Attention with CNN](https://arxiv.org/abs/2506.10683)
*Subhram Dasgupta,Janelle Mason,Xiaohong Yuan,Olusola Odeyomi,Kaushik Roy*

Main category: cs.CV

TL;DR: 提出一种轻量级CNN结合SE模块的深度伪造检测模型，性能优异且资源消耗低。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术威胁信息真实性，现有检测模型体积大、资源消耗高，需高效轻量解决方案。

Method: 采用轻量级CNN结合SE模块，动态调整通道特征，提升检测效率。

Result: 在Style GAN数据集上实现94.14%分类准确率和0.985 AUC-ROC分数。

Conclusion: 该模型为高效、可扩展的深度伪造检测提供了可行方案。

Abstract: In the digital age, Deepfake present a formidable challenge by using advanced artificial intelligence to create highly convincing manipulated content, undermining information authenticity and security. These sophisticated fabrications surpass traditional detection methods in complexity and realism. To address this issue, we aim to harness cutting-edge deep learning methodologies to engineer an innovative deepfake detection model. However, most of the models designed for deepfake detection are large, causing heavy storage and memory consumption. In this research, we propose a lightweight convolution neural network (CNN) with squeeze and excitation block attention (SE) for Deepfake detection. The SE block module is designed to perform dynamic channel-wise feature recalibration. The SE block allows the network to emphasize informative features and suppress less useful ones, which leads to a more efficient and effective learning module. This module is integrated with a simple sequential model to perform Deepfake detection. The model is smaller in size and it achieves competing accuracy with the existing models for deepfake detection tasks. The model achieved an overall classification accuracy of 94.14% and AUC-ROC score of 0.985 on the Style GAN dataset from the Diverse Fake Face Dataset. Our proposed approach presents a promising avenue for combating the Deepfake challenge with minimal computational resources, developing efficient and scalable solutions for digital content verification.

</details>


### [22] [Unsourced Adversarial CAPTCHA: A Bi-Phase Adversarial CAPTCHA Framework](https://arxiv.org/abs/2506.10685)
*Xia Du,Xiaoyuan Liu,Jizhe Zhou,Zheng Lin,Chi-man Pun,Zhe Chen,Wei Ni,Jun Luo*

Main category: cs.CV

TL;DR: 提出了一种名为UAC的新框架，通过文本提示生成高保真对抗样本，增强CAPTCHA多样性，支持定向和非定向攻击。


<details>
  <summary>Details</summary>
Motivation: 传统CAPTCHA方案因深度学习发展易受攻击，现有对抗攻击方法依赖原始图像特性，导致失真且适用性受限。

Method: UAC利用大语言模型生成对抗样本，定向攻击采用EDICT方法优化扩散模型，非定向攻击采用BP-UAC策略。

Result: BP-UAC在多种系统中实现高攻击成功率，生成的CAPTCHA对人类和DNN均难以区分。

Conclusion: UAC框架有效解决了传统CAPTCHA的漏洞，提升了对抗攻击的多样性和实用性。

Abstract: With the rapid advancements in deep learning, traditional CAPTCHA schemes are increasingly vulnerable to automated attacks powered by deep neural networks (DNNs). Existing adversarial attack methods often rely on original image characteristics, resulting in distortions that hinder human interpretation and limit applicability in scenarios lacking initial input images. To address these challenges, we propose the Unsourced Adversarial CAPTCHA (UAC), a novel framework generating high-fidelity adversarial examples guided by attacker-specified text prompts. Leveraging a Large Language Model (LLM), UAC enhances CAPTCHA diversity and supports both targeted and untargeted attacks. For targeted attacks, the EDICT method optimizes dual latent variables in a diffusion model for superior image quality. In untargeted attacks, especially for black-box scenarios, we introduce bi-path unsourced adversarial CAPTCHA (BP-UAC), a two-step optimization strategy employing multimodal gradients and bi-path optimization for efficient misclassification. Experiments show BP-UAC achieves high attack success rates across diverse systems, generating natural CAPTCHAs indistinguishable to humans and DNNs.

</details>


### [23] [Stroke-based Cyclic Amplifier: Image Super-Resolution at Arbitrary Ultra-Large Scales](https://arxiv.org/abs/2506.10774)
*Wenhao Guo,Peng Lu,Xujun Peng,Zhaoran Zhao,Sheng Li*

Main category: cs.CV

TL;DR: 提出了一种基于笔划向量放大器的统一模型SbCA，用于超大规模图像超分辨率任务，解决了现有方法在超出训练范围时的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有任意尺度图像超分辨率方法在超出训练范围时性能显著下降，导致图像模糊。

Method: SbCA通过笔划向量放大器将图像分解为矢量图形进行放大，并通过细节补全模块恢复缺失细节，采用循环策略迭代优化。

Result: 实验表明，SbCA在超大规模上采样任务（如×100）中显著优于现有方法，生成高质量图像。

Conclusion: SbCA有效解决了分布漂移问题，消除了伪影和模糊，实现了高保真图像重建。

Abstract: Prior Arbitrary-Scale Image Super-Resolution (ASISR) methods often experience a significant performance decline when the upsampling factor exceeds the range covered by the training data, introducing substantial blurring. To address this issue, we propose a unified model, Stroke-based Cyclic Amplifier (SbCA), for ultra-large upsampling tasks. The key of SbCA is the stroke vector amplifier, which decomposes the image into a series of strokes represented as vector graphics for magnification. Then, the detail completion module also restores missing details, ensuring high-fidelity image reconstruction. Our cyclic strategy achieves ultra-large upsampling by iteratively refining details with this unified SbCA model, trained only once for all, while keeping sub-scales within the training range. Our approach effectively addresses the distribution drift issue and eliminates artifacts, noise and blurring, producing high-quality, high-resolution super-resolved images. Experimental validations on both synthetic and real-world datasets demonstrate that our approach significantly outperforms existing methods in ultra-large upsampling tasks (e.g. $\times100$), delivering visual quality far superior to state-of-the-art techniques.

</details>


### [24] [Post-Training Quantization for Video Matting](https://arxiv.org/abs/2506.10840)
*Tianrui Zhu,Houyuan Chen,Ruihao Gong,Michele Magno,Haotong Qin,Kai Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种专为视频抠图设计的后训练量化（PTQ）框架，通过两阶段策略、全局仿射校准和光流辅助组件，显著提升了量化模型的精度和时序一致性，甚至在超低位量化下接近全精度性能。


<details>
  <summary>Details</summary>
Motivation: 视频抠图在资源受限设备上部署时面临计算量大和精度保持的挑战，现有PTQ方法在该领域尚不成熟。

Method: 提出两阶段PTQ策略（块重建优化和全局校准）、统计驱动的全局仿射校准（GAC）方法，以及光流辅助（OFA）组件。

Result: PTQ4VM在不同位宽下均达到最先进精度，4位量化下性能接近全精度模型且计算量减少8倍。

Conclusion: 该框架为视频抠图量化提供了系统解决方案，显著提升了精度和效率。

Abstract: Video matting is crucial for applications such as film production and virtual reality, yet deploying its computationally intensive models on resource-constrained devices presents challenges. Quantization is a key technique for model compression and acceleration. As an efficient approach, Post-Training Quantization (PTQ) is still in its nascent stages for video matting, facing significant hurdles in maintaining accuracy and temporal coherence. To address these challenges, this paper proposes a novel and general PTQ framework specifically designed for video matting models, marking, to the best of our knowledge, the first systematic attempt in this domain. Our contributions include: (1) A two-stage PTQ strategy that combines block-reconstruction-based optimization for fast, stable initial quantization and local dependency capture, followed by a global calibration of quantization parameters to minimize accuracy loss. (2) A Statistically-Driven Global Affine Calibration (GAC) method that enables the network to compensate for cumulative statistical distortions arising from factors such as neglected BN layer effects, even reducing the error of existing PTQ methods on video matting tasks up to 20%. (3) An Optical Flow Assistance (OFA) component that leverages temporal and semantic priors from frames to guide the PTQ process, enhancing the model's ability to distinguish moving foregrounds in complex scenes and ultimately achieving near full-precision performance even under ultra-low-bit quantization. Comprehensive quantitative and visual results show that our PTQ4VM achieves the state-of-the-art accuracy performance across different bit-widths compared to the existing quantization methods. We highlight that the 4-bit PTQ4VM even achieves performance close to the full-precision counterpart while enjoying 8x FLOP savings.

</details>


### [25] [M4V: Multi-Modal Mamba for Text-to-Video Generation](https://arxiv.org/abs/2506.10915)
*Jiancheng Huang,Gengwei Zhang,Zequn Jie,Siyu Jiao,Yinlong Qian,Ling Chen,Yunchao Wei,Lin Ma*

Main category: cs.CV

TL;DR: M4V是一个基于Mamba架构的多模态文本到视频生成框架，通过多模态扩散Mamba块和奖励学习策略，显著降低了计算成本并提升了视频质量。


<details>
  <summary>Details</summary>
Motivation: 解决传统Transformer在视频生成中计算复杂度高的问题，同时提升多模态和时空建模的效率。

Method: 提出多模态扩散Mamba块（MM-DiM），结合多模态标记重组设计和奖励学习策略。

Result: 在768×1280分辨率下，M4V的FLOPs比基于注意力的方法减少45%，并能生成高质量视频。

Conclusion: M4V在降低计算成本的同时，有效提升了文本到视频生成的质量和效率。

Abstract: Text-to-video generation has significantly enriched content creation and holds the potential to evolve into powerful world simulators. However, modeling the vast spatiotemporal space remains computationally demanding, particularly when employing Transformers, which incur quadratic complexity in sequence processing and thus limit practical applications. Recent advancements in linear-time sequence modeling, particularly the Mamba architecture, offer a more efficient alternative. Nevertheless, its plain design limits its direct applicability to multi-modal and spatiotemporal video generation tasks. To address these challenges, we introduce M4V, a Multi-Modal Mamba framework for text-to-video generation. Specifically, we propose a multi-modal diffusion Mamba (MM-DiM) block that enables seamless integration of multi-modal information and spatiotemporal modeling through a multi-modal token re-composition design. As a result, the Mamba blocks in M4V reduce FLOPs by 45% compared to the attention-based alternative when generating videos at 768$\times$1280 resolution. Additionally, to mitigate the visual quality degradation in long-context autoregressive generation processes, we introduce a reward learning strategy that further enhances per-frame visual realism. Extensive experiments on text-to-video benchmarks demonstrate M4V's ability to produce high-quality videos while significantly lowering computational costs. Code and models will be publicly available at https://huangjch526.github.io/M4V_project.

</details>


### [26] [VINCIE: Unlocking In-context Image Editing from Video](https://arxiv.org/abs/2506.10941)
*Leigang Qu,Feng Cheng,Ziyan Yang,Qi Zhao,Shanchuan Lin,Yichun Shi,Yicong Li,Wenjie Wang,Tat-Seng Chua,Lu Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种直接从视频中学习上下文图像编辑的方法，通过设计块因果扩散变换器和多任务学习，实现了强大的编辑能力，并在多个基准测试中达到最优效果。


<details>
  <summary>Details</summary>
Motivation: 探索是否可以通过视频直接学习上下文图像编辑，避免依赖任务特定管道和专家模型。

Method: 使用视频标注为多模态序列，设计块因果扩散变换器，训练三个代理任务：下一图像预测、当前分割预测和下一分割预测。

Result: 模型在上下文图像编辑中表现优异，在两个多轮图像编辑基准测试中达到最优，并展示了多概念组合、故事生成和编辑链应用的能力。

Conclusion: 直接从视频学习上下文图像编辑是可行的，且模型在多任务中表现出色，为未来研究提供了新方向。

Abstract: In-context image editing aims to modify images based on a contextual sequence comprising text and previously generated images. Existing methods typically depend on task-specific pipelines and expert models (e.g., segmentation and inpainting) to curate training data. In this work, we explore whether an in-context image editing model can be learned directly from videos. We introduce a scalable approach to annotate videos as interleaved multimodal sequences. To effectively learn from this data, we design a block-causal diffusion transformer trained on three proxy tasks: next-image prediction, current segmentation prediction, and next-segmentation prediction. Additionally, we propose a novel multi-turn image editing benchmark to advance research in this area. Extensive experiments demonstrate that our model exhibits strong in-context image editing capabilities and achieves state-of-the-art results on two multi-turn image editing benchmarks. Despite being trained exclusively on videos, our model also shows promising abilities in multi-concept composition, story generation, and chain-of-editing applications.

</details>


### [27] [MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for Text-to-Image Reasoning](https://arxiv.org/abs/2506.10963)
*Yuxuan Luo,Yuhui Yuan,Junwen Chen,Haonan Cai,Ziyi Yue,Yuwei Yang,Fatima Zohra Daha,Ji Li,Zhouhui Lian*

Main category: cs.CV

TL;DR: 论文提出了知识图像生成任务和MMMG基准，用于评估图像生成模型的推理能力，并揭示了现有模型的不足。


<details>
  <summary>Details</summary>
Motivation: 知识图像对人类文明和学习机制至关重要，但生成此类图像需要多模态推理，现有模型表现不佳。

Method: 采用统一的KG表示和MMMG-Score评估指标，对16种文本到图像生成模型进行全面评估。

Result: 现有模型在实体保真度、关系清晰度和视觉清晰度方面表现不佳，GPT-4o得分仅为50.20。

Conclusion: 提出了FLUX-Reason作为开放基线，以推动知识图像生成领域的进一步研究。

Abstract: In this paper, we introduce knowledge image generation as a new task, alongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation Benchmark (MMMG) to probe the reasoning capability of image generation models. Knowledge images have been central to human civilization and to the mechanisms of human learning--a fact underscored by dual-coding theory and the picture-superiority effect. Generating such images is challenging, demanding multimodal reasoning that fuses world knowledge with pixel-level grounding into clear explanatory visuals. To enable comprehensive evaluation, MMMG offers 4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines, 6 educational levels, and diverse knowledge formats such as charts, diagrams, and mind maps. To eliminate confounding complexity during evaluation, we adopt a unified Knowledge Graph (KG) representation. Each KG explicitly delineates a target image's core entities and their dependencies. We further introduce MMMG-Score to evaluate generated knowledge images. This metric combines factual fidelity, measured by graph-edit distance between KGs, with visual clarity assessment. Comprehensive evaluations of 16 state-of-the-art text-to-image generation models expose serious reasoning deficits--low entity fidelity, weak relations, and clutter--with GPT-4o achieving an MMMG-Score of only 50.20, underscoring the benchmark's difficulty. To spur further progress, we release FLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that combines a reasoning LLM with diffusion models and is trained on 16,000 curated knowledge image-prompt pairs.

</details>


### [28] [GenWorld: Towards Detecting AI-generated Real-world Simulation Videos](https://arxiv.org/abs/2506.10975)
*Weiliang Chen,Wenzhao Zheng,Yu Zheng,Lei Chen,Jie Zhou,Jiwen Lu,Yueqi Duan*

Main category: cs.CV

TL;DR: 本文提出了GenWorld数据集和SpannDetector模型，用于检测高质量AI生成视频，解决了现有方法在真实世界场景中的不足。


<details>
  <summary>Details</summary>
Motivation: 视频生成技术的快速发展威胁了真实世界信息的可信度，而现有高质量真实世界数据集的缺乏阻碍了可靠检测器的发展。

Method: 提出GenWorld数据集，模拟真实世界场景并提供高质量伪造视频；设计SpannDetector模型，利用多视角一致性检测AI生成视频。

Result: 实验表明SpannDetector在检测高质量AI生成视频方面表现优异，突出了基于物理合理性的可解释检测方向。

Conclusion: GenWorld数据集和SpannDetector模型为AI生成视频检测领域提供了重要进展。

Abstract: The flourishing of video generation technologies has endangered the credibility of real-world information and intensified the demand for AI-generated video detectors. Despite some progress, the lack of high-quality real-world datasets hinders the development of trustworthy detectors. In this paper, we propose GenWorld, a large-scale, high-quality, and real-world simulation dataset for AI-generated video detection. GenWorld features the following characteristics: (1) Real-world Simulation: GenWorld focuses on videos that replicate real-world scenarios, which have a significant impact due to their realism and potential influence; (2) High Quality: GenWorld employs multiple state-of-the-art video generation models to provide realistic and high-quality forged videos; (3) Cross-prompt Diversity: GenWorld includes videos generated from diverse generators and various prompt modalities (e.g., text, image, video), offering the potential to learn more generalizable forensic features. We analyze existing methods and find they fail to detect high-quality videos generated by world models (i.e., Cosmos), revealing potential drawbacks of ignoring real-world clues. To address this, we propose a simple yet effective model, SpannDetector, to leverage multi-view consistency as a strong criterion for real-world AI-generated video detection. Experiments show that our method achieves superior results, highlighting a promising direction for explainable AI-generated video detection based on physical plausibility. We believe that GenWorld will advance the field of AI-generated video detection. Project Page: https://chen-wl20.github.io/GenWorld

</details>


### [29] [Fine-Grained Perturbation Guidance via Attention Head Selection](https://arxiv.org/abs/2506.10978)
*Donghoon Ahn,Jiwon Kang,Sanghyun Lee,Minjae Kim,Jaewon Min,Wooseok Jang,Saungwu Lee,Sayak Paul,Susung Hong,Seungryong Kim*

Main category: cs.CV

TL;DR: 论文提出了一种名为HeadHunter的系统框架，通过迭代选择注意力头来实现对生成质量和视觉属性的精细控制，并引入SoftPAG方法调节扰动强度。


<details>
  <summary>Details</summary>
Motivation: 现有注意力扰动方法缺乏确定扰动应用位置的原则性方法，尤其是在DiT架构中，质量相关计算分布在多个层中。

Method: 研究注意力扰动的粒度，从层级到单个注意力头，提出HeadHunter框架和SoftPAG方法，通过线性插值调节扰动强度。

Result: 在Stable Diffusion 3和FLUX.1等模型上验证了方法的优越性，实现了生成质量的提升和视觉风格的特异性控制。

Conclusion: 首次在扩散模型中进行了头级注意力扰动分析，揭示了注意力层的可解释性，并提供了有效的扰动策略设计。

Abstract: Recent guidance methods in diffusion models steer reverse sampling by perturbing the model to construct an implicit weak model and guide generation away from it. Among these approaches, attention perturbation has demonstrated strong empirical performance in unconditional scenarios where classifier-free guidance is not applicable. However, existing attention perturbation methods lack principled approaches for determining where perturbations should be applied, particularly in Diffusion Transformer (DiT) architectures where quality-relevant computations are distributed across layers. In this paper, we investigate the granularity of attention perturbations, ranging from the layer level down to individual attention heads, and discover that specific heads govern distinct visual concepts such as structure, style, and texture quality. Building on this insight, we propose "HeadHunter", a systematic framework for iteratively selecting attention heads that align with user-centric objectives, enabling fine-grained control over generation quality and visual attributes. In addition, we introduce SoftPAG, which linearly interpolates each selected head's attention map toward an identity matrix, providing a continuous knob to tune perturbation strength and suppress artifacts. Our approach not only mitigates the oversmoothing issues of existing layer-level perturbation but also enables targeted manipulation of specific visual styles through compositional head selection. We validate our method on modern large-scale DiT-based text-to-image models including Stable Diffusion 3 and FLUX.1, demonstrating superior performance in both general quality enhancement and style-specific guidance. Our work provides the first head-level analysis of attention perturbation in diffusion models, uncovering interpretable specialization within attention layers and enabling practical design of effective perturbation strategies.

</details>


### [30] [InstaInpaint: Instant 3D-Scene Inpainting with Masked Large Reconstruction Model](https://arxiv.org/abs/2506.10980)
*Junqi You,Chieh Hubert Lin,Weijie Lyu,Zhengbo Zhang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: InstaInpaint是一个基于参考的前馈框架，能在0.4秒内完成3D场景修复，速度比现有方法快1000倍，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景修复方法依赖耗时且计算密集的优化，无法满足实时或在线应用需求。

Method: 提出InstaInpaint框架，结合自监督掩码微调策略训练大型重建模型（LRM），从2D修复提案生成3D修复结果。

Result: 在标准基准测试中表现优异，支持灵活的下游应用（如物体插入和多区域修复）。

Conclusion: InstaInpaint在速度和性能上均优于现有方法，适用于实时3D场景修复。

Abstract: Recent advances in 3D scene reconstruction enable real-time viewing in virtual and augmented reality. To support interactive operations for better immersiveness, such as moving or editing objects, 3D scene inpainting methods are proposed to repair or complete the altered geometry. However, current approaches rely on lengthy and computationally intensive optimization, making them impractical for real-time or online applications. We propose InstaInpaint, a reference-based feed-forward framework that produces 3D-scene inpainting from a 2D inpainting proposal within 0.4 seconds. We develop a self-supervised masked-finetuning strategy to enable training of our custom large reconstruction model (LRM) on the large-scale dataset. Through extensive experiments, we analyze and identify several key designs that improve generalization, textural consistency, and geometric correctness. InstaInpaint achieves a 1000x speed-up from prior methods while maintaining a state-of-the-art performance across two standard benchmarks. Moreover, we show that InstaInpaint generalizes well to flexible downstream applications such as object insertion and multi-region inpainting. More video results are available at our project page: https://dhmbb2.github.io/InstaInpaint_page/.

</details>


### [31] [SceneCompleter: Dense 3D Scene Completion for Generative Novel View Synthesis](https://arxiv.org/abs/2506.10981)
*Weiliang Chen,Jiayi Bi,Yuanhui Huang,Wenzhao Zheng,Yueqi Duan*

Main category: cs.CV

TL;DR: SceneCompleter提出了一种通过密集3D场景补全实现3D一致生成新视角合成的框架，解决了现有方法因依赖2D补全和3D恢复技术而导致的几何失真问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在新视角合成中通常先进行2D补全再进行3D恢复，导致几何失真和表面平滑。SceneCompleter旨在通过直接3D场景补全实现更一致的生成结果。

Method: 1. 几何-外观双流扩散模型联合合成RGBD空间的新视角；2. 场景编码器从参考图像中获取更全面的场景理解。

Result: 方法在多样数据集上展示了生成新视角合成的优越一致性和合理性。

Conclusion: SceneCompleter通过融合结构和纹理信息，实现了视觉一致且3D一致的生成新视角合成。

Abstract: Generative models have gained significant attention in novel view synthesis (NVS) by alleviating the reliance on dense multi-view captures. However, existing methods typically fall into a conventional paradigm, where generative models first complete missing areas in 2D, followed by 3D recovery techniques to reconstruct the scene, which often results in overly smooth surfaces and distorted geometry, as generative models struggle to infer 3D structure solely from RGB data. In this paper, we propose SceneCompleter, a novel framework that achieves 3D-consistent generative novel view synthesis through dense 3D scene completion. SceneCompleter achieves both visual coherence and 3D-consistent generative scene completion through two key components: (1) a geometry-appearance dual-stream diffusion model that jointly synthesizes novel views in RGBD space; (2) a scene embedder that encodes a more holistic scene understanding from the reference image. By effectively fusing structural and textural information, our method demonstrates superior coherence and plausibility in generative novel view synthesis across diverse datasets. Project Page: https://chen-wl20.github.io/SceneCompleter

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [32] [Prompt-Guided Latent Diffusion with Predictive Class Conditioning for 3D Prostate MRI Generation](https://arxiv.org/abs/2506.10230)
*Emerson P. Grabke,Masoom A. Haider,Babak Taati*

Main category: eess.IV

TL;DR: 论文提出了一种名为CCELLA的双头条件方法，结合非医学大语言模型编码的文本特征和病理分类，解决了医学LDM训练中的数据稀缺问题，并显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决医学LDM训练中因数据稀缺和依赖非医学模型或大量数据微调导致的性能与科学可及性限制。

Method: 提出CCELLA双头条件方法，通过交叉注意力结合文本特征和病理分类，并设计联合损失函数和数据高效训练框架。

Result: 在有限数据下，3D FID得分为0.025，优于基准模型的0.071；合成图像提升分类器准确率至74%，且仅用合成图像训练的分类器性能接近真实数据。

Conclusion: CCELLA方法有效解决了医学LDM训练中的数据稀缺问题，显著提升了图像合成质量和分类器性能，具有科学可及性和实用性。

Abstract: Latent diffusion models (LDM) could alleviate data scarcity challenges affecting machine learning development for medical imaging. However, medical LDM training typically relies on performance- or scientific accessibility-limiting strategies including a reliance on short-prompt text encoders, the reuse of non-medical LDMs, or a requirement for fine-tuning with large data volumes. We propose a Class-Conditioned Efficient Large Language model Adapter (CCELLA) to address these limitations. CCELLA is a novel dual-head conditioning approach that simultaneously conditions the LDM U-Net with non-medical large language model-encoded text features through cross-attention and with pathology classification through the timestep embedding. We also propose a joint loss function and a data-efficient LDM training framework. In combination, these strategies enable pathology-conditioned LDM training for high-quality medical image synthesis given limited data volume and human data annotation, improving LDM performance and scientific accessibility. Our method achieves a 3D FID score of 0.025 on a size-limited prostate MRI dataset, significantly outperforming a recent foundation model with FID 0.071. When training a classifier for prostate cancer prediction, adding synthetic images generated by our method to the training dataset improves classifier accuracy from 69% to 74%. Training a classifier solely on our method's synthetic images achieved comparable performance to training on real images alone.

</details>


### [33] [Conditional diffusion models for guided anomaly detection in brain images using fluid-driven anomaly randomization](https://arxiv.org/abs/2506.10233)
*Ana Lawry Aguila,Peirong Liu,Oula Puonti,Juan Eugenio Iglesias*

Main category: eess.IV

TL;DR: 提出了一种基于条件扩散模型的新框架，用于脑MRI中的异常检测和健康图像重建，通过合成伪病理图像改进重建效果，并在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 监督学习需要大量疾病数据，但某些罕见病数据难以获取。现有无监督异常检测方法假设模型无法准确重建异常，但实际效果不佳。

Method: 采用条件扩散模型框架，结合合成伪病理图像（通过流体驱动异常随机化生成）指导健康图像重建。

Result: 模型在合成和真实病理数据上表现优异，优于变分自编码器、条件/无条件潜在扩散模型，甚至部分监督修复方法。

Conclusion: 该方法为无监督异常检测提供了更优解决方案，尤其在数据稀缺情况下表现突出。

Abstract: Supervised machine learning has enabled accurate pathology detection in brain MRI, but requires training data from diseased subjects that may not be readily available in some scenarios, for example, in the case of rare diseases. Reconstruction-based unsupervised anomaly detection, in particular using diffusion models, has gained popularity in the medical field as it allows for training on healthy images alone, eliminating the need for large disease-specific cohorts. These methods assume that a model trained on normal data cannot accurately represent or reconstruct anomalies. However, this assumption often fails with models failing to reconstruct healthy tissue or accurately reconstruct abnormal regions i.e., failing to remove anomalies. In this work, we introduce a novel conditional diffusion model framework for anomaly detection and healthy image reconstruction in brain MRI. Our weakly supervised approach integrates synthetically generated pseudo-pathology images into the modeling process to better guide the reconstruction of healthy images. To generate these pseudo-pathologies, we apply fluid-driven anomaly randomization to augment real pathology segmentation maps from an auxiliary dataset, ensuring that the synthetic anomalies are both realistic and anatomically coherent. We evaluate our model's ability to detect pathology, using both synthetic anomaly datasets and real pathology from the ATLAS dataset. In our extensive experiments, our model: (i) consistently outperforms variational autoencoders, and conditional and unconditional latent diffusion; and (ii) surpasses on most datasets, the performance of supervised inpainting methods with access to paired diseased/healthy images.

</details>


### [34] [SWDL: Stratum-Wise Difference Learning with Deep Laplacian Pyramid for Semi-Supervised 3D Intracranial Hemorrhage Segmentation](https://arxiv.org/abs/2506.10325)
*Cheng Wang,Siqi Chen,Donghua Mi,Yang Chen,Yudong Zhang,Yinsheng Li*

Main category: eess.IV

TL;DR: SWDL-Net是一种新型半监督学习框架，结合拉普拉斯金字塔和深度卷积上采样，显著提升颅内出血分割性能，仅需少量标注数据即可优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于颅内出血（ICH）标注数据获取困难且成本高，传统深度学习分割方法面临挑战，半监督学习（SSL）成为解决标注数据稀缺问题的潜在方案。

Method: 提出SWDL-Net框架，整合拉普拉斯金字塔的边缘锐化优势和深度卷积的细节精确性，通过差异学习机制实现互补。

Result: 在271例ICH数据集和公开基准测试中，仅使用2%标注数据时表现优于现有方法；在BHSD数据集（5%标注数据）上进一步验证其优越性。

Conclusion: SWDL-Net通过互补方法整合，显著提升了半监督学习在医学图像分割中的性能，尤其在标注数据稀缺场景下表现突出。

Abstract: Recent advances in medical imaging have established deep learning-based segmentation as the predominant approach, though it typically requires large amounts of manually annotated data. However, obtaining annotations for intracranial hemorrhage (ICH) remains particularly challenging due to the tedious and costly labeling process. Semi-supervised learning (SSL) has emerged as a promising solution to address the scarcity of labeled data, especially in volumetric medical image segmentation. Unlike conventional SSL methods that primarily focus on high-confidence pseudo-labels or consistency regularization, we propose SWDL-Net, a novel SSL framework that exploits the complementary advantages of Laplacian pyramid and deep convolutional upsampling. The Laplacian pyramid excels at edge sharpening, while deep convolutions enhance detail precision through flexible feature mapping. Our framework achieves superior segmentation of lesion details and boundaries through a difference learning mechanism that effectively integrates these complementary approaches. Extensive experiments on a 271-case ICH dataset and public benchmarks demonstrate that SWDL-Net outperforms current state-of-the-art methods in scenarios with only 2% labeled data. Additional evaluations on the publicly available Brain Hemorrhage Segmentation Dataset (BHSD) with 5% labeled data further confirm the superiority of our approach. Code and data have been released at https://github.com/SIAT-CT-LAB/SWDL.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [35] [EQ-TAA: Equivariant Traffic Accident Anticipation via Diffusion-Based Accident Video Synthesis](https://arxiv.org/abs/2506.10002)
*Jianwu Fang,Lei-Lei Li,Zhedong Zheng,Hongkai Yu,Jianru Xue,Zhengguo Li,Tat-Seng Chua*

Main category: cs.MM

TL;DR: 论文提出了一种基于注意力视频扩散（AVD）的模型，用于生成交通事故的因果部分，并通过无监督学习解决交通场景中的背景混淆问题。


<details>
  <summary>Details</summary>
Motivation: 交通事故预测（TAA）在零伤亡目标中至关重要，但现有方法依赖标注且易受数据偏差影响。

Method: AVD模型通过生成因果视频帧，结合文本提示生成伪事故和伪正常视频对，并引入等变三重损失（EQ-TAA）。

Result: 实验表明AVD和EQ-TAA在性能上优于现有方法。

Conclusion: AVD和EQ-TAA为无监督TAA提供了有效解决方案，减少了标注需求并提升了预测效果。

Abstract: Traffic Accident Anticipation (TAA) in traffic scenes is a challenging problem for achieving zero fatalities in the future. Current approaches typically treat TAA as a supervised learning task needing the laborious annotation of accident occurrence duration. However, the inherent long-tailed, uncertain, and fast-evolving nature of traffic scenes has the problem that real causal parts of accidents are difficult to identify and are easily dominated by data bias, resulting in a background confounding issue. Thus, we propose an Attentive Video Diffusion (AVD) model that synthesizes additional accident video clips by generating the causal part in dashcam videos, i.e., from normal clips to accident clips. AVD aims to generate causal video frames based on accident or accident-free text prompts while preserving the style and content of frames for TAA after video generation. This approach can be trained using datasets collected from various driving scenes without any extra annotations. Additionally, AVD facilitates an Equivariant TAA (EQ-TAA) with an equivariant triple loss for an anchor accident-free video clip, along with the generated pair of contrastive pseudo-normal and pseudo-accident clips. Extensive experiments have been conducted to evaluate the performance of AVD and EQ-TAA, and competitive performance compared to state-of-the-art methods has been obtained.

</details>


### [36] [HER2 Expression Prediction with Flexible Multi-Modal Inputs via Dynamic Bidirectional Reconstruction](https://arxiv.org/abs/2506.10006)
*Jie Qin,Wei Yang,Yan Su,Yiran Zhu,Weizhen Li,Yunyue Pan,Chengchang Pan,Honggang Qi*

Main category: cs.MM

TL;DR: 提出了一种自适应双模态框架，通过动态分支选择、双向跨模态GAN和混合训练协议，灵活支持单/双模态HER2预测，显著提升准确性和资源效率。


<details>
  <summary>Details</summary>
Motivation: 现有HER2评估模型通常单独分析H&E或IHC图像，而临床依赖两者的协同解读。同时获取两种模态的困难促使开发灵活的单/双模态预测框架。

Method: 1) 动态分支选择器；2) 双向跨模态GAN；3) 混合训练协议。

Result: 单模态H&E预测准确率从71.44%提升至94.25%，双模态准确率达95.09%，IHC单模态可靠性为90.28%。

Conclusion: 该框架通过弹性设计，显著提升HER2评估的准确性和资源效率，适用于资源有限的环境。

Abstract: Current HER2 assessment models for breast cancer predominantly analyze H&E or IHC images in isolation,despite clinical reliance on their synergistic interpretation. However, concurrent acquisition of both modalities is often hindered by workflow complexity and cost constraints. We propose an adaptive bimodal framework enabling flexible single-/dual-modality HER2 prediction through three innovations: 1) A dynamic branch selector that activates either single-modality reconstruction or dual-modality joint inference based on input completeness; 2) A bidirectional cross-modal GAN performing context-aware feature-space reconstruction of missing modalities; 3) A hybrid training protocol integrating adversarial learning and multi-task optimization. This architecture elevates single-modality H&E prediction accuracy from 71.44% to 94.25% while achieving 95.09% dual-modality accuracy, maintaining 90.28% reliability with sole IHC inputs. The framework's "dual-preferred, single-compatible" design delivers near-bimodal performance without requiring synchronized acquisition, particularly benefiting resource-limited settings through IHC infrastructure cost reduction. Experimental validation confirms 22.81%/12.90% accuracy improvements over H&E/IHC baselines respectively, with cross-modal reconstruction enhancing F1-scores to 0.9609 (HE to IHC) and 0.9251 (IHC to HE). By dynamically routing inputs through reconstruction-enhanced or native fusion pathways, the system mitigates performance degradation from missing data while preserving computational efficiency (78.55% parameter reduction in lightweight variant). This elastic architecture demonstrates significant potential for democratizing precise HER2 assessment across diverse healthcare settings.

</details>


### [37] [Controllable Expressive 3D Facial Animation via Diffusion in a Unified Multimodal Space](https://arxiv.org/abs/2506.10007)
*Kangwei Liu,Junwu Liu,Xiaowei Yi,Jinlin Guo,Yun Cao*

Main category: cs.MM

TL;DR: 提出了一种基于扩散模型的多模态情感3D面部动画框架，解决了单模态控制和确定性映射的限制。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖单模态信号和确定性映射的问题，以提升情感动画的表达力和多样性。

Method: 采用FLAME多模态情感绑定策略和注意力潜扩散模型，结合对比学习和内容感知注意力。

Result: 在多数指标上优于现有方法，情感相似度提升21.6%，同时保持自然面部动态。

Conclusion: 该方法通过多模态融合和扩散模型显著提升了情感3D面部动画的表现力和可控性。

Abstract: Audio-driven emotional 3D facial animation encounters two significant challenges: (1) reliance on single-modal control signals (videos, text, or emotion labels) without leveraging their complementary strengths for comprehensive emotion manipulation, and (2) deterministic regression-based mapping that constrains the stochastic nature of emotional expressions and non-verbal behaviors, limiting the expressiveness of synthesized animations. To address these challenges, we present a diffusion-based framework for controllable expressive 3D facial animation. Our approach introduces two key innovations: (1) a FLAME-centered multimodal emotion binding strategy that aligns diverse modalities (text, audio, and emotion labels) through contrastive learning, enabling flexible emotion control from multiple signal sources, and (2) an attention-based latent diffusion model with content-aware attention and emotion-guided layers, which enriches motion diversity while maintaining temporal coherence and natural facial dynamics. Extensive experiments demonstrate that our method outperforms existing approaches across most metrics, achieving a 21.6\% improvement in emotion similarity while preserving physiologically plausible facial dynamics. Project Page: https://kangweiiliu.github.io/Control_3D_Animation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [38] [A Navigation Framework Utilizing Vision-Language Models](https://arxiv.org/abs/2506.10172)
*Yicheng Duan,Kaiyu tang*

Main category: cs.RO

TL;DR: 提出了一种模块化的视觉语言导航框架，通过解耦视觉语言理解和动作规划，结合轻量级规划逻辑，实现高效导航。


<details>
  <summary>Details</summary>
Motivation: 解决大型视觉语言模型在导航任务中的计算成本和实时部署挑战。

Method: 使用冻结的视觉语言模型Qwen2.5-VL-7B-Instruct，结合提示工程、结构化历史管理和双帧视觉输入策略。

Result: 在VLN-CE设置下的Room-to-Room基准测试中初步验证，尽管在未见环境中泛化能力有限，但展示了模块化方法的潜力。

Conclusion: 模块化框架为可扩展和高效的导航系统奠定了基础，未来可通过增强环境先验和多模态输入集成进一步改进。

Abstract: Vision-and-Language Navigation (VLN) presents a complex challenge in embodied AI, requiring agents to interpret natural language instructions and navigate through visually rich, unfamiliar environments. Recent advances in large vision-language models (LVLMs), such as CLIP and Flamingo, have significantly improved multimodal understanding but introduced new challenges related to computational cost and real-time deployment. In this project, we propose a modular, plug-and-play navigation framework that decouples vision-language understanding from action planning. By integrating a frozen vision-language model, Qwen2.5-VL-7B-Instruct, with lightweight planning logic, we aim to achieve flexible, fast, and adaptable navigation without extensive model fine-tuning. Our framework leverages prompt engineering, structured history management, and a two-frame visual input strategy to enhance decision-making continuity across navigation steps. We evaluate our system on the Room-to-Room benchmark within the VLN-CE setting using the Matterport3D dataset and Habitat-Lab simulation environment. Although our initial results reveal challenges in generalizing to unseen environments under strict evaluation settings, our modular approach lays a foundation for scalable and efficient navigation systems, highlighting promising directions for future improvement through enhanced environmental priors and expanded multimodal input integration.

</details>


### [39] [Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop](https://arxiv.org/abs/2506.10968)
*Justin Kerr,Kush Hari,Ethan Weber,Chung Min Kim,Brent Yi,Tyler Bonnen,Ken Goldberg,Angjoo Kanazawa*

Main category: cs.RO

TL;DR: EyeRobot是一个具有凝视行为的机器人系统，其凝视行为源于完成实际任务的需求。通过强化学习训练凝视策略，结合手眼协调，实现了在大工作空间中的有效操作。


<details>
  <summary>Details</summary>
Motivation: 人类通过主动观察来行动，受此启发，开发了具有类似凝视行为的机器人系统EyeRobot，以提升机器人在大工作空间中的操作能力。

Method: 开发了可自由旋转的机械眼球，通过强化学习训练凝视策略。结合手眼协调训练（BC-RL循环），手部动作基于眼球观察，眼球凝视则通过手部动作的正确性获得奖励。

Result: EyeRobot在五个全景工作空间操作任务中表现出有效的手眼协调行为，能够稳定注视并忽略干扰物，提升了操作能力。

Conclusion: EyeRobot通过手眼协调和凝视策略，实现了在大工作空间中的高效操作，为机器人视觉与动作的协同提供了新思路。

Abstract: Humans do not passively observe the visual world -- we actively look in order to act. Motivated by this principle, we introduce EyeRobot, a robotic system with gaze behavior that emerges from the need to complete real-world tasks. We develop a mechanical eyeball that can freely rotate to observe its surroundings and train a gaze policy to control it using reinforcement learning. We accomplish this by first collecting teleoperated demonstrations paired with a 360 camera. This data is imported into a simulation environment that supports rendering arbitrary eyeball viewpoints, allowing episode rollouts of eye gaze on top of robot demonstrations. We then introduce a BC-RL loop to train the hand and eye jointly: the hand (BC) agent is trained from rendered eye observations, and the eye (RL) agent is rewarded when the hand produces correct action predictions. In this way, hand-eye coordination emerges as the eye looks towards regions which allow the hand to complete the task. EyeRobot implements a foveal-inspired policy architecture allowing high resolution with a small compute budget, which we find also leads to the emergence of more stable fixation as well as improved ability to track objects and ignore distractors. We evaluate EyeRobot on five panoramic workspace manipulation tasks requiring manipulation in an arc surrounding the robot arm. Our experiments suggest EyeRobot exhibits hand-eye coordination behaviors which effectively facilitate manipulation over large workspaces with a single camera. See project site for videos: https://www.eyerobot.net/

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [40] [AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation](https://arxiv.org/abs/2506.10540)
*Haoyuan Shi,Yunxin Li,Xinyu Chen,Longyue Wang,Baotian Hu,Min Zhang*

Main category: cs.MA

TL;DR: AniMaker是一个多智能体框架，通过多候选片段生成和故事感知片段选择，从文本输入生成全局一致且故事连贯的动画。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在多场景和角色连贯叙事方面存在挑战，导致叙事脱节和节奏问题。AniMaker旨在解决这些问题。

Method: AniMaker采用多智能体框架，包括导演、摄影、评审和后期制作代理，结合MCTS-Gen和AniEval技术优化片段生成与评估。

Result: 实验表明，AniMaker在VBench和AniEval等指标上表现优异，显著提升了多候选生成效率。

Conclusion: AniMaker推动了AI生成叙事动画向生产标准迈进。

Abstract: Despite rapid advancements in video generation models, generating coherent storytelling videos that span multiple scenes and characters remains challenging. Current methods often rigidly convert pre-generated keyframes into fixed-length clips, resulting in disjointed narratives and pacing issues. Furthermore, the inherent instability of video generation models means that even a single low-quality clip can significantly degrade the entire output animation's logical coherence and visual continuity. To overcome these obstacles, we introduce AniMaker, a multi-agent framework enabling efficient multi-candidate clip generation and storytelling-aware clip selection, thus creating globally consistent and story-coherent animation solely from text input. The framework is structured around specialized agents, including the Director Agent for storyboard generation, the Photography Agent for video clip generation, the Reviewer Agent for evaluation, and the Post-Production Agent for editing and voiceover. Central to AniMaker's approach are two key technical components: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search (MCTS)-inspired strategy that intelligently navigates the candidate space to generate high-potential clips while optimizing resource usage; and AniEval in Reviewer Agent, the first framework specifically designed for multi-shot animation evaluation, which assesses critical aspects such as story-level consistency, action completion, and animation-specific features by considering each clip in the context of its preceding and succeeding clips. Experiments demonstrate that AniMaker achieves superior quality as measured by popular metrics including VBench and our proposed AniEval framework, while significantly improving the efficiency of multi-candidate generation, pushing AI-generated storytelling animation closer to production standards.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [41] [Geometric Regularity in Deterministic Sampling of Diffusion-based Generative Models](https://arxiv.org/abs/2506.10177)
*Defang Chen,Zhenyu Zhou,Can Wang,Siwei Lyu*

Main category: cs.LG

TL;DR: 论文揭示了扩散生成模型中确定性采样轨迹的低维几何规律，并提出了一种动态规划方法优化采样时间表，提升图像生成性能。


<details>
  <summary>Details</summary>
Motivation: 探索扩散生成模型中确定性采样轨迹的几何规律，以优化采样过程并提升生成效率。

Method: 通过分析采样轨迹的低维子空间和“回旋镖”形状，提出基于动态规划的采样时间表优化方法。

Result: 发现采样轨迹具有低维性和形状一致性，优化后的方法在少量函数评估下显著提升图像生成质量。

Conclusion: 扩散生成模型的采样轨迹具有规律性，动态规划方法能有效优化采样过程，提升生成性能。

Abstract: Diffusion-based generative models employ stochastic differential equations (SDEs) and their equivalent probability flow ordinary differential equations (ODEs) to establish a smooth transformation between complex high-dimensional data distributions and tractable prior distributions. In this paper, we reveal a striking geometric regularity in the deterministic sampling dynamics: each simulated sampling trajectory lies within an extremely low-dimensional subspace, and all trajectories exhibit an almost identical ''boomerang'' shape, regardless of the model architecture, applied conditions, or generated content. We characterize several intriguing properties of these trajectories, particularly under closed-form solutions based on kernel-estimated data modeling. We also demonstrate a practical application of the discovered trajectory regularity by proposing a dynamic programming-based scheme to better align the sampling time schedule with the underlying trajectory structure. This simple strategy requires minimal modification to existing ODE-based numerical solvers, incurs negligible computational overhead, and achieves superior image generation performance, especially in regions with only $5 \sim 10$ function evaluations.

</details>


### [42] [Hessian Geometry of Latent Space in Generative Models](https://arxiv.org/abs/2506.10632)
*Alexander Lobashev,Dmitry Guskov,Maria Larchenko,Mikhail Tamm*

Main category: cs.LG

TL;DR: 本文提出了一种通过重构Fisher信息度量来分析生成模型潜在空间几何结构的新方法，适用于统计物理模型和扩散模型。


<details>
  <summary>Details</summary>
Motivation: 研究生成模型潜在空间的几何结构，特别是扩散模型中相变现象的复杂性。

Method: 通过近似潜在变量的后验分布学习对数配分函数，定义Fisher度量，提供理论收敛保证，并在Ising和TASEP模型上验证。

Result: 方法在重构热力学量上优于基线，揭示了扩散模型潜在空间中的分形相变结构。

Conclusion: 研究为扩散模型潜在空间的复杂结构及其与相变现象的联系提供了新见解。

Abstract: This paper presents a novel method for analyzing the latent space geometry of generative models, including statistical physics models and diffusion models, by reconstructing the Fisher information metric. The method approximates the posterior distribution of latent variables given generated samples and uses this to learn the log-partition function, which defines the Fisher metric for exponential families. Theoretical convergence guarantees are provided, and the method is validated on the Ising and TASEP models, outperforming existing baselines in reconstructing thermodynamic quantities. Applied to diffusion models, the method reveals a fractal structure of phase transitions in the latent space, characterized by abrupt changes in the Fisher metric. We demonstrate that while geodesic interpolations are approximately linear within individual phases, this linearity breaks down at phase boundaries, where the diffusion model exhibits a divergent Lipschitz constant with respect to the latent space. These findings provide new insights into the complex structure of diffusion model latent spaces and their connection to phenomena like phase transitions. Our source code is available at https://github.com/alobashev/hessian-geometry-of-diffusion-models.

</details>


### [43] [ReGuidance: A Simple Diffusion Wrapper for Boosting Sample Quality on Hard Inverse Problems](https://arxiv.org/abs/2506.10955)
*Aayush Karan,Kulin Shah,Sitan Chen*

Main category: cs.LG

TL;DR: ReGuidance是一种简单的方法，用于提升基于扩散模型的逆问题求解算法的样本真实性和奖励效果。


<details>
  <summary>Details</summary>
Motivation: 现有的训练自由方法（如DPS）在低信噪比的困难逆问题中容易偏离数据流形，导致输出不真实。

Method: 提出ReGuidance方法，通过反转候选解并重新初始化DPS来提升样本质量。

Result: 在困难逆问题（如大区域修复和高倍超分辨率）中，ReGuidance显著提升了样本质量和测量一致性。

Conclusion: ReGuidance首次为DPS提供了严格的算法保证，并在多模态数据分布上证明了其有效性。

Abstract: There has been a flurry of activity around using pretrained diffusion models as informed data priors for solving inverse problems, and more generally around steering these models using reward models. Training-free methods like diffusion posterior sampling (DPS) and its many variants have offered flexible heuristic algorithms for these tasks, but when the reward is not informative enough, e.g., in hard inverse problems with low signal-to-noise ratio, these techniques veer off the data manifold, failing to produce realistic outputs. In this work, we devise a simple wrapper, ReGuidance, for boosting both the sample realism and reward achieved by these methods. Given a candidate solution $\hat{x}$ produced by an algorithm of the user's choice, we propose inverting the solution by running the unconditional probability flow ODE in reverse starting from $\hat{x}$, and then using the resulting latent as an initialization for DPS. We evaluate our wrapper on hard inverse problems like large box in-painting and super-resolution with high upscaling. Whereas state-of-the-art baselines visibly fail, we find that applying our wrapper on top of these baselines significantly boosts sample quality and measurement consistency. We complement these findings with theory proving that on certain multimodal data distributions, ReGuidance simultaneously boosts the reward and brings the candidate solution closer to the data manifold. To our knowledge, this constitutes the first rigorous algorithmic guarantee for DPS.

</details>
