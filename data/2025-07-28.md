<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 20]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [eess.IV](#eess.IV) [Total: 7]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [ShrinkBox: Backdoor Attack on Object Detection to Disrupt Collision Avoidance in Machine Learning-based Advanced Driver Assistance Systems](https://arxiv.org/abs/2507.18656)
*Muhammad Zaeem Shahzad,Muhammad Abdullah Hanif,Bassem Ouni,Muhammad Shafique*

Main category: cs.CV

TL;DR: 论文提出了一种名为ShrinkBox的新型后门攻击方法，针对基于机器学习的ADAS系统中的物体检测功能，通过缩小边界框来干扰距离估计，攻击成功率高且隐蔽性强。


<details>
  <summary>Details</summary>
Motivation: 传统ADAS依赖昂贵传感器，而基于机器学习的ADAS（ML-ADAS）使用低成本摄像头，但其物体检测功能存在安全漏洞，需要研究新型攻击方法以提高系统安全性。

Method: 提出ShrinkBox攻击，通过轻微缩小物体检测的边界框（而非直接修改标签或物体存在性），在训练数据中植入后门，影响距离估计功能。

Result: 在YOLOv9m检测器和KITTI数据集上，攻击成功率达96%，仅需4%的污染比例，且显著增加距离估计的误差（MAE增加3倍以上）。

Conclusion: ShrinkBox攻击揭示了ML-ADAS系统中物体检测的潜在安全风险，强调了在设计和评估此类系统时需考虑对抗性攻击的防御措施。

Abstract: Advanced Driver Assistance Systems (ADAS) significantly enhance road safety by detecting potential collisions and alerting drivers. However, their reliance on expensive sensor technologies such as LiDAR and radar limits accessibility, particularly in low- and middle-income countries. Machine learning-based ADAS (ML-ADAS), leveraging deep neural networks (DNNs) with only standard camera input, offers a cost-effective alternative. Critical to ML-ADAS is the collision avoidance feature, which requires the ability to detect objects and estimate their distances accurately. This is achieved with specialized DNNs like YOLO, which provides real-time object detection, and a lightweight, detection-wise distance estimation approach that relies on key features extracted from the detections like bounding box dimensions and size. However, the robustness of these systems is undermined by security vulnerabilities in object detectors. In this paper, we introduce ShrinkBox, a novel backdoor attack targeting object detection in collision avoidance ML-ADAS. Unlike existing attacks that manipulate object class labels or presence, ShrinkBox subtly shrinks ground truth bounding boxes. This attack remains undetected in dataset inspections and standard benchmarks while severely disrupting downstream distance estimation. We demonstrate that ShrinkBox can be realized in the YOLOv9m object detector at an Attack Success Rate (ASR) of 96%, with only a 4% poisoning ratio in the training instances of the KITTI dataset. Furthermore, given the low error targets introduced in our relaxed poisoning strategy, we find that ShrinkBox increases the Mean Absolute Error (MAE) in downstream distance estimation by more than 3x on poisoned samples, potentially resulting in delays or prevention of collision warnings altogether.

</details>


### [2] [Gen-AI Police Sketches with Stable Diffusion](https://arxiv.org/abs/2507.18667)
*Nicholas Fidalgo,Aaron Contreras,Katherine Harvey,Johnny Ni*

Main category: cs.CV

TL;DR: 研究通过多模态AI方法自动化并优化嫌疑人素描，测试了三种模型，发现基线模型在结构和感知相似性上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用多模态AI技术提升嫌疑人素描的自动化水平和质量。

Method: 开发并评估了三种模型：基线Stable Diffusion模型、结合CLIP的模型，以及引入LoRA微调的新方法。

Result: 基线模型在SSIM和PSNR上表现最优，而新方法在LPIPS上有提升但仍不及基线。

Conclusion: 基线模型虽简单但在结构和清晰度上表现最佳，新方法需进一步优化。

Abstract: This project investigates the use of multimodal AI-driven approaches to automate and enhance suspect sketching. Three pipelines were developed and evaluated: (1) baseline image-to-image Stable Diffusion model, (2) same model integrated with a pre-trained CLIP model for text-image alignment, and (3) novel approach incorporating LoRA fine-tuning of the CLIP model, applied to self-attention and cross-attention layers, and integrated with Stable Diffusion. An ablation study confirmed that fine-tuning both self- and cross-attention layers yielded the best alignment between text descriptions and sketches. Performance testing revealed that Model 1 achieved the highest structural similarity (SSIM) of 0.72 and a peak signal-to-noise ratio (PSNR) of 25 dB, outperforming Model 2 and Model 3. Iterative refinement enhanced perceptual similarity (LPIPS), with Model 3 showing improvement over Model 2 but still trailing Model 1. Qualitatively, sketches generated by Model 1 demonstrated the clearest facial features, highlighting its robustness as a baseline despite its simplicity.

</details>


### [3] [SaLF: Sparse Local Fields for Multi-Sensor Rendering in Real-Time](https://arxiv.org/abs/2507.18713)
*Yun Chen,Matthew Haines,Jingkang Wang,Krzysztof Baron-Lis,Sivabalan Manivasagam,Ze Yang,Raquel Urtasun*

Main category: cs.CV

TL;DR: SaLF是一种新型体积表示方法，支持光栅化和光线追踪，用于高效模拟多传感器自动驾驶场景。


<details>
  <summary>Details</summary>
Motivation: 解决现有NeRF和3DGS方法在训练和渲染速度、传感器兼容性及通用性方面的不足。

Method: 使用稀疏3D体素基元表示体积，每个体素为局部隐式场，支持快速训练和渲染。

Result: SaLF训练时间短（<30分钟），渲染速度快（相机50+ FPS，LiDAR 600+ FPS），支持非针孔相机和旋转LiDAR。

Conclusion: SaLF在保持真实感的同时，提高了效率和功能，适用于更可扩展的自动驾驶传感器模拟。

Abstract: High-fidelity sensor simulation of light-based sensors such as cameras and LiDARs is critical for safe and accurate autonomy testing. Neural radiance field (NeRF)-based methods that reconstruct sensor observations via ray-casting of implicit representations have demonstrated accurate simulation of driving scenes, but are slow to train and render, hampering scale. 3D Gaussian Splatting (3DGS) has demonstrated faster training and rendering times through rasterization, but is primarily restricted to pinhole camera sensors, preventing usage for realistic multi-sensor autonomy evaluation. Moreover, both NeRF and 3DGS couple the representation with the rendering procedure (implicit networks for ray-based evaluation, particles for rasterization), preventing interoperability, which is key for general usage. In this work, we present Sparse Local Fields (SaLF), a novel volumetric representation that supports rasterization and raytracing. SaLF represents volumes as a sparse set of 3D voxel primitives, where each voxel is a local implicit field. SaLF has fast training (<30 min) and rendering capabilities (50+ FPS for camera and 600+ FPS LiDAR), has adaptive pruning and densification to easily handle large scenes, and can support non-pinhole cameras and spinning LiDARs. We demonstrate that SaLF has similar realism as existing self-driving sensor simulation methods while improving efficiency and enhancing capabilities, enabling more scalable simulation. https://waabi.ai/salf/

</details>


### [4] [Phoneme-Level Visual Speech Recognition via Point-Visual Fusion and Language Model Reconstruction](https://arxiv.org/abs/2507.18863)
*Matthew Kit Khinn Teng,Haibo Zhang,Takeshi Saitoh*

Main category: cs.CV

TL;DR: 提出了一种基于音素的视觉自动语音识别（V-ASR）两阶段框架，结合视觉和面部特征，并通过LLM模型重构单词，显著降低了错误率。


<details>
  <summary>Details</summary>
Motivation: 视觉自动语音识别任务因缺乏听觉线索和音素的视觉模糊性（即相似的口型对应不同的音素）而极具挑战性。现有方法通常直接从视觉线索预测单词或字符，但错误率高且需要大量预训练数据。

Method: 提出两阶段框架：第一阶段通过V-ASR预测音素，减少训练复杂度；第二阶段使用LLM模型（NLLB）将音素重构为单词。结合视觉和面部特征以提高准确性。

Result: 在LRS2和LRS3数据集上分别实现了17.4%和21.0%的词错误率（WER），表现优于现有方法。

Conclusion: 提出的PV-ASR方法通过音素预测和LLM重构，有效解决了视觉模糊性问题，显著提升了性能。

Abstract: Visual Automatic Speech Recognition (V-ASR) is a challenging task that involves interpreting spoken language solely from visual information, such as lip movements and facial expressions. This task is notably challenging due to the absence of auditory cues and the visual ambiguity of phonemes that exhibit similar visemes-distinct sounds that appear identical in lip motions. Existing methods often aim to predict words or characters directly from visual cues, but they commonly suffer from high error rates due to viseme ambiguity and require large amounts of pre-training data. We propose a novel phoneme-based two-stage framework that fuses visual and landmark motion features, followed by an LLM model for word reconstruction to address these challenges. Stage 1 consists of V-ASR, which outputs the predicted phonemes, thereby reducing training complexity. Meanwhile, the facial landmark features address speaker-specific facial characteristics. Stage 2 comprises an encoder-decoder LLM model, NLLB, that reconstructs the output phonemes back to words. Besides using a large visual dataset for deep learning fine-tuning, our PV-ASR method demonstrates superior performance by achieving 17.4% WER on the LRS2 and 21.0% WER on the LRS3 dataset.

</details>


### [5] [Gaussian Set Surface Reconstruction through Per-Gaussian Optimization](https://arxiv.org/abs/2507.18923)
*Zhentao Huang,Di Wu,Zhenbang He,Minglun Gong*

Main category: cs.CV

TL;DR: GSSR提出了一种新方法，通过均匀分布高斯并优化其法线对齐，显著提升了3D高斯重建的几何精度。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯重建方法（如3DGS和PGSR）在几何重建上存在不足，高斯分布不均匀且偏离潜在表面，影响了重建和编辑效果。

Method: GSSR结合单视角法线一致性和多视角光度一致性，优化高斯分布和法线对齐，并通过不透明度正则化和周期性重初始化进一步优化。

Result: 实验表明GSSR显著提升了几何精度，同时保持了高质量的渲染性能。

Conclusion: GSSR为高斯重建提供了更精确的几何表示，支持高效的场景编辑和新环境生成。

Abstract: 3D Gaussian Splatting (3DGS) effectively synthesizes novel views through its flexible representation, yet fails to accurately reconstruct scene geometry. While modern variants like PGSR introduce additional losses to ensure proper depth and normal maps through Gaussian fusion, they still neglect individual placement optimization. This results in unevenly distributed Gaussians that deviate from the latent surface, complicating both reconstruction refinement and scene editing. Motivated by pioneering work on Point Set Surfaces, we propose Gaussian Set Surface Reconstruction (GSSR), a method designed to distribute Gaussians evenly along the latent surface while aligning their dominant normals with the surface normal. GSSR enforces fine-grained geometric alignment through a combination of pixel-level and Gaussian-level single-view normal consistency and multi-view photometric consistency, optimizing both local and global perspectives. To further refine the representation, we introduce an opacity regularization loss to eliminate redundant Gaussians and apply periodic depth- and normal-guided Gaussian reinitialization for a cleaner, more uniform spatial distribution. Our reconstruction results demonstrate significantly improved geometric precision in Gaussian placement, enabling intuitive scene editing and efficient generation of novel Gaussian-based 3D environments. Extensive experiments validate GSSR's effectiveness, showing enhanced geometric accuracy while preserving high-quality rendering performance.

</details>


### [6] [PDT: Point Distribution Transformation with Diffusion Models](https://arxiv.org/abs/2507.18939)
*Jionghao Wang,Cheng Lin,Yuan Liu,Rui Xu,Zhiyang Dou,Xiao-Xiao Long,Hao-Xiang Guo,Taku Komura,Wenping Wang,Xin Li*

Main category: cs.CV

TL;DR: PDT是一种基于扩散模型的新型框架，用于将无序点云转换为语义上有意义的结构化分布。


<details>
  <summary>Details</summary>
Motivation: 解决如何从无序点云中提取有意义的结构信息并转换为语义分布的问题。

Method: 利用扩散模型的新架构和学习策略，通过去噪过程关联源分布和目标分布。

Result: 成功将点云转换为多种结构化输出，如表面关键点、内部稀疏关节和连续特征线。

Conclusion: PDT能够捕捉几何和语义特征，为3D几何处理任务提供强大工具。

Abstract: Point-based representations have consistently played a vital role in geometric data structures. Most point cloud learning and processing methods typically leverage the unordered and unconstrained nature to represent the underlying geometry of 3D shapes. However, how to extract meaningful structural information from unstructured point cloud distributions and transform them into semantically meaningful point distributions remains an under-explored problem. We present PDT, a novel framework for point distribution transformation with diffusion models. Given a set of input points, PDT learns to transform the point set from its original geometric distribution into a target distribution that is semantically meaningful. Our method utilizes diffusion models with novel architecture and learning strategy, which effectively correlates the source and the target distribution through a denoising process. Through extensive experiments, we show that our method successfully transforms input point clouds into various forms of structured outputs - ranging from surface-aligned keypoints, and inner sparse joints to continuous feature lines. The results showcase our framework's ability to capture both geometric and semantic features, offering a powerful tool for various 3D geometry processing tasks where structured point distributions are desired. Code will be available at this link: https://github.com/shanemankiw/PDT.

</details>


### [7] [PerioDet: Large-Scale Panoramic Radiograph Benchmark for Clinical-Oriented Apical Periodontitis Detection](https://arxiv.org/abs/2507.18958)
*Xiaocheng Fang,Jieyi Cai,Huanyu Liu,Chengju Zhou,Minhua Lu,Bingzhi Chen*

Main category: cs.CV

TL;DR: 论文提出了首个大规模根尖周炎数据集PerioXrays，并开发了一种结合背景去噪和小目标检测的自动化诊断方法PerioDet，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决根尖周炎自动化诊断中缺乏高质量标注数据集的问题。

Method: 提出PerioDet方法，结合背景去噪注意力机制（BDA）和IoU动态校准（IDC）机制。

Result: 在PerioXrays数据集上验证了PerioDet的优越性，并通过人机协作实验展示了临床适用性。

Conclusion: PerioXrays数据集和PerioDet方法为根尖周炎的自动化诊断提供了有效工具。

Abstract: Apical periodontitis is a prevalent oral pathology that presents significant public health challenges. Despite advances in automated diagnostic systems across various medical fields, the development of Computer-Aided Diagnosis (CAD) applications for apical periodontitis is still constrained by the lack of a large-scale, high-quality annotated dataset. To address this issue, we release a large-scale panoramic radiograph benchmark called "PerioXrays", comprising 3,673 images and 5,662 meticulously annotated instances of apical periodontitis. To the best of our knowledge, this is the first benchmark dataset for automated apical periodontitis diagnosis. This paper further proposes a clinical-oriented apical periodontitis detection (PerioDet) paradigm, which jointly incorporates Background-Denoising Attention (BDA) and IoU-Dynamic Calibration (IDC) mechanisms to address the challenges posed by background noise and small targets in automated detection. Extensive experiments on the PerioXrays dataset demonstrate the superiority of PerioDet in advancing automated apical periodontitis detection. Additionally, a well-designed human-computer collaborative experiment underscores the clinical applicability of our method as an auxiliary diagnostic tool for professional dentists.

</details>


### [8] [UPP: Unified Point-Level Prompting for Robust Point Cloud Analysis](https://arxiv.org/abs/2507.18997)
*Zixiang Ai,Zhenyu Cui,Yuxin Peng,Jiahuan Zhou*

Main category: cs.CV

TL;DR: 提出了一种统一的点级提示方法，通过提示机制解决点云去噪和补全问题，提升下游任务的鲁棒性和参数效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在点云增强和下游任务之间存在隔离，且去噪和补全任务的目标冲突限制了性能。

Method: 引入Rectification Prompter和Completion Prompter，结合Shape-Aware Unit模块，统一处理点云去噪和补全。

Result: 在四个数据集上的实验表明，该方法在处理噪声和不完整点云数据时优于现有方法。

Conclusion: 提出的方法通过提示机制有效解决了点云质量问题，提升了分析的鲁棒性和适应性。

Abstract: Pre-trained point cloud analysis models have shown promising advancements in various downstream tasks, yet their effectiveness is typically suffering from low-quality point cloud (i.e., noise and incompleteness), which is a common issue in real scenarios due to casual object occlusions and unsatisfactory data collected by 3D sensors. To this end, existing methods focus on enhancing point cloud quality by developing dedicated denoising and completion models. However, due to the isolation between the point cloud enhancement and downstream tasks, these methods fail to work in various real-world domains. In addition, the conflicting objectives between denoising and completing tasks further limit the ensemble paradigm to preserve critical geometric features. To tackle the above challenges, we propose a unified point-level prompting method that reformulates point cloud denoising and completion as a prompting mechanism, enabling robust analysis in a parameter-efficient manner. We start by introducing a Rectification Prompter to adapt to noisy points through the predicted rectification vector prompts, effectively filtering noise while preserving intricate geometric features essential for accurate analysis. Sequentially, we further incorporate a Completion Prompter to generate auxiliary point prompts based on the rectified point clouds, facilitating their robustness and adaptability. Finally, a Shape-Aware Unit module is exploited to efficiently unify and capture the filtered geometric features for the downstream point cloud analysis.Extensive experiments on four datasets demonstrate the superiority and robustness of our method when handling noisy and incomplete point cloud data against existing state-of-the-art methods. Our code is released at https://github.com/zhoujiahuan1991/ICCV2025-UPP.

</details>


### [9] [GPSMamba: A Global Phase and Spectral Prompt-guided Mamba for Infrared Image Super-Resolution](https://arxiv.org/abs/2507.18998)
*Yongsong Huang,Tomo Miyazaki,Xiaofeng Liu,Shinichiro Omachi*

Main category: cs.CV

TL;DR: GPSMamba提出了一种结合全局相位和频谱提示的Mamba框架，通过非因果监督和语义-频率状态空间模块，解决了红外图像超分辨率中因果建模的局限性。


<details>
  <summary>Details</summary>
Motivation: 红外图像超分辨率（IRSR）因低对比度和稀疏纹理而具有挑战性，需要强大的长程建模能力。现有方法如Mamba虽擅长长程依赖建模，但其1D因果扫描机制会破坏2D图像的全局上下文，影响细节恢复。

Method: 提出GPSMamba框架，包括自适应语义-频率状态空间模块（ASF-SSM）和热谱注意力与相位一致性损失，通过非因果监督和全局结构指导提升性能。

Result: 实验表明GPSMamba在红外图像恢复任务中达到最先进性能。

Conclusion: GPSMamba通过系统性策略解决了因果建模的局限性，为红外图像恢复提供了新范式。

Abstract: Infrared Image Super-Resolution (IRSR) is challenged by the low contrast and sparse textures of infrared data, requiring robust long-range modeling to maintain global coherence. While State-Space Models like Mamba offer proficiency in modeling long-range dependencies for this task, their inherent 1D causal scanning mechanism fragments the global context of 2D images, hindering fine-detail restoration. To address this, we propose Global Phase and Spectral Prompt-guided Mamba (GPSMamba), a framework that synergizes architectural guidance with non-causal supervision. First, our Adaptive Semantic-Frequency State Space Module (ASF-SSM) injects a fused semantic-frequency prompt directly into the Mamba block, integrating non-local context to guide reconstruction. Then, a novel Thermal-Spectral Attention and Phase Consistency Loss provides explicit, non-causal supervision to enforce global structural and spectral fidelity. By combining these two innovations, our work presents a systematic strategy to mitigate the limitations of causal modeling. Extensive experiments demonstrate that GPSMamba achieves state-of-the-art performance, validating our approach as a powerful new paradigm for infrared image restoration. Code is available at https://github.com/yongsongH/GPSMamba.

</details>


### [10] [Enhancing Reward Models for High-quality Image Generation: Beyond Text-Image Alignment](https://arxiv.org/abs/2507.19002)
*Ying Ba,Tianyu Zhang,Yalong Bai,Wenyi Mo,Tao Liang,Bing Su,Ji-Rong Wen*

Main category: cs.CV

TL;DR: 本文提出了一种新的图像评估方法ICT和HP分数，解决了现有评估模型在细节和美学评分上的不足，显著提升了评分准确性。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成系统的评估框架未能跟上其高保真和美学质量的进步，导致评分与人类偏好不一致。

Method: 设计了ICT分数以改进文本-图像对齐评估，并训练HP分数模型以提升图像美学和细节质量。

Result: 实验表明，新模型评分准确性提升超过10%，并在优化文本到图像模型中取得显著成果。

Conclusion: 研究为图像生成技术向更高阶人类美学偏好发展提供了理论和实证支持。

Abstract: Contemporary image generation systems have achieved high fidelity and superior aesthetic quality beyond basic text-image alignment. However, existing evaluation frameworks have failed to evolve in parallel. This study reveals that human preference reward models fine-tuned based on CLIP and BLIP architectures have inherent flaws: they inappropriately assign low scores to images with rich details and high aesthetic value, creating a significant discrepancy with actual human aesthetic preferences. To address this issue, we design a novel evaluation score, ICT (Image-Contained-Text) score, that achieves and surpasses the objectives of text-image alignment by assessing the degree to which images represent textual content. Building upon this foundation, we further train an HP (High-Preference) score model using solely the image modality to enhance image aesthetics and detail quality while maintaining text-image alignment. Experiments demonstrate that the proposed evaluation model improves scoring accuracy by over 10\% compared to existing methods, and achieves significant results in optimizing state-of-the-art text-to-image models. This research provides theoretical and empirical support for evolving image generation technology toward higher-order human aesthetic preferences. Code is available at https://github.com/BarretBa/ICTHP.

</details>


### [11] [ScenePainter: Semantically Consistent Perpetual 3D Scene Generation with Concept Relation Alignment](https://arxiv.org/abs/2507.19058)
*Chong Xia,Shengjun Zhang,Fangfu Liu,Chang Liu,Khodchaphun Hirunyaratsameewong,Yueqi Duan*

Main category: cs.CV

TL;DR: 论文提出ScenePainter框架，通过SceneConceptGraph解决3D场景生成中的语义漂移问题，生成更一致和沉浸的视图序列。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外推模块导致语义漂移，需要一种新框架保持语义一致性。

Method: 引入SceneConceptGraph层次图结构，动态优化外推模块的场景特定先验。

Result: 实验证明框架克服语义漂移，生成更一致和沉浸的3D视图序列。

Conclusion: ScenePainter通过场景概念图提升3D场景生成的语义一致性和多样性。

Abstract: Perpetual 3D scene generation aims to produce long-range and coherent 3D view sequences, which is applicable for long-term video synthesis and 3D scene reconstruction. Existing methods follow a "navigate-and-imagine" fashion and rely on outpainting for successive view expansion. However, the generated view sequences suffer from semantic drift issue derived from the accumulated deviation of the outpainting module. To tackle this challenge, we propose ScenePainter, a new framework for semantically consistent 3D scene generation, which aligns the outpainter's scene-specific prior with the comprehension of the current scene. To be specific, we introduce a hierarchical graph structure dubbed SceneConceptGraph to construct relations among multi-level scene concepts, which directs the outpainter for consistent novel views and can be dynamically refined to enhance diversity. Extensive experiments demonstrate that our framework overcomes the semantic drift issue and generates more consistent and immersive 3D view sequences. Project Page: https://xiac20.github.io/ScenePainter/.

</details>


### [12] [Cross-Subject Mind Decoding from Inaccurate Representations](https://arxiv.org/abs/2507.19071)
*Yangyang Xu,Bangzhen Liu,Wenqi Shao,Yong Du,Shengfeng He,Tingting Zhu*

Main category: cs.CV

TL;DR: 提出了一种双向自编码器框架（Bidirectional Autoencoder Intertwining），通过双向映射和模块化设计解决跨被试fMRI解码中的误差累积问题，显著提升了重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在跨被试fMRI解码中因认知差异和单向映射导致误差累积，重建效果不佳。

Method: 结合双向映射、Subject Bias Modulation模块、Semantic Refinement模块和Visual Coherence模块，集成ControlNet和Stable Diffusion。

Result: 在基准数据集上定性和定量评估均优于现有方法，且对新被试适应性强。

Conclusion: 该框架有效解决了跨被试解码的误差问题，提升了重建质量，具有广泛适用性。

Abstract: Decoding stimulus images from fMRI signals has advanced with pre-trained generative models. However, existing methods struggle with cross-subject mappings due to cognitive variability and subject-specific differences. This challenge arises from sequential errors, where unidirectional mappings generate partially inaccurate representations that, when fed into diffusion models, accumulate errors and degrade reconstruction fidelity. To address this, we propose the Bidirectional Autoencoder Intertwining framework for accurate decoded representation prediction. Our approach unifies multiple subjects through a Subject Bias Modulation Module while leveraging bidirectional mapping to better capture data distributions for precise representation prediction. To further enhance fidelity when decoding representations into stimulus images, we introduce a Semantic Refinement Module to improve semantic representations and a Visual Coherence Module to mitigate the effects of inaccurate visual representations. Integrated with ControlNet and Stable Diffusion, our method outperforms state-of-the-art approaches on benchmark datasets in both qualitative and quantitative evaluations. Moreover, our framework exhibits strong adaptability to new subjects with minimal training samples.

</details>


### [13] [Preserving Topological and Geometric Embeddings for Point Cloud Recovery](https://arxiv.org/abs/2507.19121)
*Kaiyue Zhou,Zelong Tan,Hongxiao Wang,Ya-li Li,Shengjin Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为TopGeoFormer的端到端架构，通过结合拓扑和几何属性，改进了点云采样和恢复过程。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效利用点云的拓扑和几何属性，导致采样和恢复效果不佳。

Method: 1. 使用连续映射提取拓扑嵌入；2. 提出InterTwining Attention融合拓扑和几何嵌入；3. 引入几何损失和拓扑约束损失优化嵌入。

Result: 实验表明，该方法在定量和定性上均优于现有方法。

Conclusion: TopGeoFormer通过结合拓扑和几何特征，显著提升了点云采样和恢复的效果。

Abstract: Recovering point clouds involves the sequential process of sampling and restoration, yet existing methods struggle to effectively leverage both topological and geometric attributes. To address this, we propose an end-to-end architecture named \textbf{TopGeoFormer}, which maintains these critical features throughout the sampling and restoration phases. First, we revisit traditional feature extraction techniques to yield topological embedding using a continuous mapping of relative relationships between neighboring points, and integrate it in both phases for preserving the structure of the original space. Second, we propose the \textbf{InterTwining Attention} to fully merge topological and geometric embeddings, which queries shape with local awareness in both phases to form a learnable shape context facilitated with point-wise, point-shape-wise, and intra-shape features. Third, we introduce a full geometry loss and a topological constraint loss to optimize the embeddings in both Euclidean and topological spaces. The geometry loss uses inconsistent matching between coarse-to-fine generations and targets for reconstructing better geometric details, and the constraint loss limits embedding variances for better approximation of the topological space. In experiments, we comprehensively analyze the circumstances using the conventional and learning-based sampling/upsampling algorithms. The quantitative and qualitative results demonstrate that our method significantly outperforms existing sampling and recovery methods.

</details>


### [14] [DASH: 4D Hash Encoding with Self-Supervised Decomposition for Real-Time Dynamic Scene Rendering](https://arxiv.org/abs/2507.19141)
*Jie Chen,Zhangchi Hu,Peixi Wu,Huyue Zhu,Hebei Li,Xiaoyan Sun*

Main category: cs.CV

TL;DR: DASH框架通过4D哈希编码与自监督分解，实现了动态场景的实时高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 现有动态高斯点云方法因低秩假设导致特征重叠和渲染质量差，而4D哈希编码直接应用于整个动态场景则会导致哈希冲突和冗余。

Method: 1. 自监督分解机制分离动态与静态成分；2. 动态部分采用多分辨率4D哈希编码；3. 时空平滑正则化策略减少变形伪影。

Result: 在真实数据集上，DASH实现了264 FPS的实时渲染，视觉质量优于现有方法。

Conclusion: DASH通过创新方法解决了动态场景渲染中的关键问题，实现了高效且高质量的实时渲染。

Abstract: Dynamic scene reconstruction is a long-term challenge in 3D vision. Existing plane-based methods in dynamic Gaussian splatting suffer from an unsuitable low-rank assumption, causing feature overlap and poor rendering quality. Although 4D hash encoding provides an explicit representation without low-rank constraints, directly applying it to the entire dynamic scene leads to substantial hash collisions and redundancy. To address these challenges, we present DASH, a real-time dynamic scene rendering framework that employs 4D hash encoding coupled with self-supervised decomposition. Our approach begins with a self-supervised decomposition mechanism that separates dynamic and static components without manual annotations or precomputed masks. Next, we introduce a multiresolution 4D hash encoder for dynamic elements, providing an explicit representation that avoids the low-rank assumption. Finally, we present a spatio-temporal smoothness regularization strategy to mitigate unstable deformation artifacts. Experiments on real-world datasets demonstrate that DASH achieves state-of-the-art dynamic rendering performance, exhibiting enhanced visual quality at real-time speeds of 264 FPS on a single 4090 GPU. Code: https://github.com/chenj02/DASH.

</details>


### [15] [Continual Learning-Based Unified Model for Unpaired Image Restoration Tasks](https://arxiv.org/abs/2507.19184)
*Kotha Kartheek,Lingamaneni Gnanesh Chowdary,Snehasis Mukherjee*

Main category: cs.CV

TL;DR: 提出了一种基于持续学习的统一图像修复框架，适用于多种天气条件下的图像恢复，通过选择性核融合层、弹性权重巩固和循环对比损失实现高效修复。


<details>
  <summary>Details</summary>
Motivation: 现有方法多针对单一天气条件，而自动驾驶等应用需要统一模型处理多种天气条件下的图像修复。

Method: 结合选择性核融合层、弹性权重巩固（EWC）和循环对比损失，提出了一种无配对的图像修复方法。

Result: 在去雾、去雪和去雨任务的标准数据集上，PSNR、SSIM和感知质量均显著优于现有方法。

Conclusion: 该框架为多天气条件下的图像修复提供了一种高效且统一的解决方案。

Abstract: Restoration of images contaminated by different adverse weather conditions such as fog, snow, and rain is a challenging task due to the varying nature of the weather conditions. Most of the existing methods focus on any one particular weather conditions. However, for applications such as autonomous driving, a unified model is necessary to perform restoration of corrupted images due to different weather conditions. We propose a continual learning approach to propose a unified framework for image restoration. The proposed framework integrates three key innovations: (1) Selective Kernel Fusion layers that dynamically combine global and local features for robust adaptive feature selection; (2) Elastic Weight Consolidation (EWC) to enable continual learning and mitigate catastrophic forgetting across multiple restoration tasks; and (3) a novel Cycle-Contrastive Loss that enhances feature discrimination while preserving semantic consistency during domain translation. Further, we propose an unpaired image restoration approach to reduce the dependance of the proposed approach on the training data. Extensive experiments on standard benchmark datasets for dehazing, desnowing and deraining tasks demonstrate significant improvements in PSNR, SSIM, and perceptual quality over the state-of-the-art.

</details>


### [16] [SemGes: Semantics-aware Co-Speech Gesture Generation using Semantic Coherence and Relevance Learning](https://arxiv.org/abs/2507.19359)
*Lanmiao Liu,Esam Ghaleb,Aslı Özyürek,Zerrin Yumak*

Main category: cs.CV

TL;DR: 提出了一种结合细粒度和全局语义信息的语音手势生成方法，通过两阶段模型提升手势的语义一致性和真实性。


<details>
  <summary>Details</summary>
Motivation: 现有手势生成研究主要关注节奏性手势，忽略了语义上下文，因此需要一种能够生成与语音语义一致的手势的方法。

Method: 首先通过向量量化变分自编码器学习运动先验，再通过第二阶段模块结合语音、文本语义和说话者身份生成手势，确保语义一致性。

Result: 实验结果表明，该方法在客观和主观指标上均优于现有方法，提升了手势的真实性和语义一致性。

Conclusion: 该方法在语音手势生成中实现了语义一致性，并通过实验验证了其优越性。

Abstract: Creating a virtual avatar with semantically coherent gestures that are aligned with speech is a challenging task. Existing gesture generation research mainly focused on generating rhythmic beat gestures, neglecting the semantic context of the gestures. In this paper, we propose a novel approach for semantic grounding in co-speech gesture generation that integrates semantic information at both fine-grained and global levels. Our approach starts with learning the motion prior through a vector-quantized variational autoencoder. Built on this model, a second-stage module is applied to automatically generate gestures from speech, text-based semantics and speaker identity that ensures consistency between the semantic relevance of generated gestures and co-occurring speech semantics through semantic coherence and relevance modules. Experimental results demonstrate that our approach enhances the realism and coherence of semantic gestures. Extensive experiments and user studies show that our method outperforms state-of-the-art approaches across two benchmarks in co-speech gesture generation in both objective and subjective metrics. The qualitative results of our model, code, dataset and pre-trained models can be viewed at https://semgesture.github.io/.

</details>


### [17] [GS-Occ3D: Scaling Vision-only Occupancy Reconstruction for Autonomous Driving with Gaussian Splatting](https://arxiv.org/abs/2507.19451)
*Baijun Ye,Minghui Qin,Saining Zhang,Moonjun Gong,Shaoting Zhu,Zebang Shen,Luan Zhang,Lu Zhang,Hao Zhao,Hang Zhao*

Main category: cs.CV

TL;DR: GS-Occ3D是一个基于视觉的占用重建框架，通过Octree-based Gaussian Surfel优化显式占用表示，解决了动态场景和遮挡问题，并在Waymo数据集上实现了最先进的几何重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖LiDAR标注，限制了可扩展性和利用众包数据的能力。视觉占用重建面临稀疏视角、动态元素和遮挡等挑战。

Method: 采用Octree-based Gaussian Surfel优化显式占用表示，将场景分解为静态背景、地面和动态物体，分别建模。

Result: 在Waymo数据集上表现优异，生成的视觉占用标签在下游任务中有效，并在Occ3D-nuScenes上展示了零样本泛化能力。

Conclusion: GS-Occ3D展示了大规模视觉占用重建在自动驾驶感知中的潜力，为未来研究提供了新范式。

Abstract: Occupancy is crucial for autonomous driving, providing essential geometric priors for perception and planning. However, existing methods predominantly rely on LiDAR-based occupancy annotations, which limits scalability and prevents leveraging vast amounts of potential crowdsourced data for auto-labeling. To address this, we propose GS-Occ3D, a scalable vision-only framework that directly reconstructs occupancy. Vision-only occupancy reconstruction poses significant challenges due to sparse viewpoints, dynamic scene elements, severe occlusions, and long-horizon motion. Existing vision-based methods primarily rely on mesh representation, which suffer from incomplete geometry and additional post-processing, limiting scalability. To overcome these issues, GS-Occ3D optimizes an explicit occupancy representation using an Octree-based Gaussian Surfel formulation, ensuring efficiency and scalability. Additionally, we decompose scenes into static background, ground, and dynamic objects, enabling tailored modeling strategies: (1) Ground is explicitly reconstructed as a dominant structural element, significantly improving large-area consistency; (2) Dynamic vehicles are separately modeled to better capture motion-related occupancy patterns. Extensive experiments on the Waymo dataset demonstrate that GS-Occ3D achieves state-of-the-art geometry reconstruction results. By curating vision-only binary occupancy labels from diverse urban scenes, we show their effectiveness for downstream occupancy models on Occ3D-Waymo and superior zero-shot generalization on Occ3D-nuScenes. It highlights the potential of large-scale vision-based occupancy reconstruction as a new paradigm for autonomous driving perception. Project Page: https://gs-occ3d.github.io/

</details>


### [18] [Fast Learning of Non-Cooperative Spacecraft 3D Models through Primitive Initialization](https://arxiv.org/abs/2507.19459)
*Pol Francesch Huc,Emily Bates,Simone D'Amico*

Main category: cs.CV

TL;DR: 提出了一种基于CNN的3D高斯泼溅（3DGS）初始化方法，解决了传统方法需要精确姿态和高计算成本的问题，适用于空间应用。


<details>
  <summary>Details</summary>
Motivation: 传统方法如NeRF和3DGS需要精确的姿态估计且计算成本高，限制了其在空间应用中的使用。

Method: 使用CNN从单张图像生成粗略3D模型和姿态估计，用于初始化3DGS，减少训练迭代和输入图像需求。

Result: 即使在不完美的姿态监督下，也能学习高保真3D表示，训练成本和输入需求显著降低。

Conclusion: 该方法为空间应用中的新视角合成提供了可行解决方案。

Abstract: The advent of novel view synthesis techniques such as NeRF and 3D Gaussian Splatting (3DGS) has enabled learning precise 3D models only from posed monocular images. Although these methods are attractive, they hold two major limitations that prevent their use in space applications: they require poses during training, and have high computational cost at training and inference. To address these limitations, this work contributes: (1) a Convolutional Neural Network (CNN) based primitive initializer for 3DGS using monocular images; (2) a pipeline capable of training with noisy or implicit pose estimates; and (3) and analysis of initialization variants that reduce the training cost of precise 3D models. A CNN takes a single image as input and outputs a coarse 3D model represented as an assembly of primitives, along with the target's pose relative to the camera. This assembly of primitives is then used to initialize 3DGS, significantly reducing the number of training iterations and input images needed -- by at least an order of magnitude. For additional flexibility, the CNN component has multiple variants with different pose estimation techniques. This work performs a comparison between these variants, evaluating their effectiveness for downstream 3DGS training under noisy or implicit pose estimates. The results demonstrate that even with imperfect pose supervision, the pipeline is able to learn high-fidelity 3D representations, opening the door for the use of novel view synthesis in space applications.

</details>


### [19] [Back to the Features: DINO as a Foundation for Video World Models](https://arxiv.org/abs/2507.19468)
*Federico Baldassarre,Marc Szafraniec,Basile Terver,Vasil Khalidov,Francisco Massa,Yann LeCun,Patrick Labatut,Maximilian Seitzer,Piotr Bojanowski*

Main category: cs.CV

TL;DR: DINO-world是一个基于DINOv2潜在空间的通用视频世界模型，通过预训练图像编码器和未来预测器在大规模无标注视频数据集上训练，学习多样场景的时间动态。


<details>
  <summary>Details</summary>
Motivation: 旨在通过预训练图像编码器和未来预测器，学习多样场景的时间动态，提升视频预测性能。

Method: 利用DINOv2预训练图像编码器，在大规模无标注视频数据集上训练未来预测器，学习时间动态。

Result: 在多种视频预测任务（如分割和深度预测）上优于先前模型，并展示了对直观物理的深刻理解。

Conclusion: DINO-world是一个强大的通用视频世界模型，可通过潜在空间模拟候选轨迹用于规划。

Abstract: We present DINO-world, a powerful generalist video world model trained to predict future frames in the latent space of DINOv2. By leveraging a pre-trained image encoder and training a future predictor on a large-scale uncurated video dataset, DINO-world learns the temporal dynamics of diverse scenes, from driving and indoor scenes to simulated environments. We show that DINO-world outperforms previous models on a variety of video prediction benchmarks, e.g. segmentation and depth forecasting, and demonstrates strong understanding of intuitive physics. Furthermore, we show that it is possible to fine-tune the predictor on observation-action trajectories. The resulting action-conditioned world model can be used for planning by simulating candidate trajectories in latent space.

</details>


### [20] [DINO-SLAM: DINO-informed RGB-D SLAM for Neural Implicit and Explicit Representations](https://arxiv.org/abs/2507.19474)
*Ziren Gong,Xiaohan Li,Fabio Tosi,Youmin Zhang,Stefano Mattoccia,Jun Wu,Matteo Poggi*

Main category: cs.CV

TL;DR: DINO-SLAM通过DINO特征增强NeRF和3DGS在SLAM系统中的场景表示，提出两种新范式，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提升SLAM系统中神经隐式和显式场景表示的全面性。

Method: 使用Scene Structure Encoder (SSE)将DINO特征增强为EDINO，并基于此提出两种NeRF和3DGS SLAM范式。

Result: 在Replica、ScanNet和TUM数据集上性能优于现有方法。

Conclusion: DINO-SLAM通过增强特征表示，显著提升了SLAM系统的性能。

Abstract: This paper presents DINO-SLAM, a DINO-informed design strategy to enhance neural implicit (Neural Radiance Field -- NeRF) and explicit representations (3D Gaussian Splatting -- 3DGS) in SLAM systems through more comprehensive scene representations. Purposely, we rely on a Scene Structure Encoder (SSE) that enriches DINO features into Enhanced DINO ones (EDINO) to capture hierarchical scene elements and their structural relationships. Building upon it, we propose two foundational paradigms for NeRF and 3DGS SLAM systems integrating EDINO features. Our DINO-informed pipelines achieve superior performance on the Replica, ScanNet, and TUM compared to state-of-the-art methods.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [21] [How good are humans at detecting AI-generated images? Learnings from an experiment](https://arxiv.org/abs/2507.18640)
*Thomas Roca,Anthony Cintron Roman,Jehú Torres Vega,Marcelo Duarte,Pengce Wang,Kevin White,Amit Misra,Juan Lavista Ferres*

Main category: cs.HC

TL;DR: 人类区分真实与AI生成图像的能力有限，整体成功率仅62%，尤其在自然和城市景观中表现较差。


<details>
  <summary>Details</summary>
Motivation: 研究人类是否能有效区分真实与AI生成图像，以应对AI生成内容可能带来的信息误导风险。

Method: 通过在线游戏收集数据，参与者对随机展示的真实与AI生成图像进行真实性判断。

Result: 整体成功率62%，人像识别较准，但自然和城市景观识别困难。

Conclusion: 人类在区分AI生成图像方面存在挑战，需透明工具（如水印和检测工具）以减少误导风险。

Abstract: As AI-powered image generation improves, a key question is how well human beings can differentiate between "real" and AI-generated or modified images. Using data collected from the online game "Real or Not Quiz.", this study investigates how effectively people can distinguish AI-generated images from real ones. Participants viewed a randomized set of real and AI-generated images, aiming to identify their authenticity. Analysis of approximately 287,000 image evaluations by over 12,500 global participants revealed an overall success rate of only 62\%, indicating a modest ability, slightly above chance. Participants were most accurate with human portraits but struggled significantly with natural and urban landscapes. These results highlight the inherent challenge humans face in distinguishing AI-generated visual content, particularly images without obvious artifacts or stylistic cues. This study stresses the need for transparency tools, such as watermarks and robust AI detection tools to mitigate the risks of misinformation arising from AI-generated content

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [22] [Face2VoiceSync: Lightweight Face-Voice Consistency for Text-Driven Talking Face Generation](https://arxiv.org/abs/2507.19225)
*Fang Kang,Yin Cao,Haoyu Chen*

Main category: cs.SD

TL;DR: 论文提出Face2VoiceSync框架，解决语音驱动面部生成中的固定语音限制问题，支持从文本和面部图像生成同步的语音和面部动画。


<details>
  <summary>Details</summary>
Motivation: 现有语音驱动面部生成方法依赖固定语音，导致应用受限（如面部与语音不匹配），因此扩展任务至更具挑战性的设置。

Method: 提出Face2VoiceSync框架，包括语音-面部对齐、多样性控制、高效训练（轻量VAE桥接视觉与音频预训练模型）和新评估指标。

Result: 实验显示Face2VoiceSync在单40GB GPU上实现视觉和音频的先进性能。

Conclusion: Face2VoiceSync成功解决了语音与面部同步生成的挑战，并在性能和效率上优于现有方法。

Abstract: Recent studies in speech-driven talking face generation achieve promising results, but their reliance on fixed-driven speech limits further applications (e.g., face-voice mismatch). Thus, we extend the task to a more challenging setting: given a face image and text to speak, generating both talking face animation and its corresponding speeches. Accordingly, we propose a novel framework, Face2VoiceSync, with several novel contributions: 1) Voice-Face Alignment, ensuring generated voices match facial appearance; 2) Diversity \& Manipulation, enabling generated voice control over paralinguistic features space; 3) Efficient Training, using a lightweight VAE to bridge visual and audio large-pretrained models, with significantly fewer trainable parameters than existing methods; 4) New Evaluation Metric, fairly assessing the diversity and identity consistency. Experiments show Face2VoiceSync achieves both visual and audio state-of-the-art performances on a single 40GB GPU.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [23] [RealDeal: Enhancing Realism and Details in Brain Image Generation via Image-to-Image Diffusion Models](https://arxiv.org/abs/2507.18830)
*Shen Zhu,Yinzhu Jin,Tyler Spears,Ifrah Zawar,P. Thomas Fletcher*

Main category: eess.IV

TL;DR: 提出了一种图像到图像的扩散模型，用于增强生成脑部图像的细节和真实感，包括锐利边缘、精细纹理、细微解剖特征和成像噪声。


<details>
  <summary>Details</summary>
Motivation: 现有的潜在扩散模型在生成脑部MRI时因潜在压缩导致图像过于平滑，缺乏真实图像中的精细结构和噪声。

Method: 通过图像到图像的扩散模型细化LDM生成的图像质量，并引入新指标评估噪声分布、锐度和纹理的真实性。

Result: 实验表明，该方法在FID和LPIPS等常用指标上表现优异，新指标也验证了生成图像的真实性。

Conclusion: RealDeal模型显著提升了生成脑部图像的细节和真实感，为生物医学图像生成提供了新思路。

Abstract: We propose image-to-image diffusion models that are designed to enhance the realism and details of generated brain images by introducing sharp edges, fine textures, subtle anatomical features, and imaging noise. Generative models have been widely adopted in the biomedical domain, especially in image generation applications. Latent diffusion models achieve state-of-the-art results in generating brain MRIs. However, due to latent compression, generated images from these models are overly smooth, lacking fine anatomical structures and scan acquisition noise that are typically seen in real images. This work formulates the realism enhancing and detail adding process as image-to-image diffusion models, which refines the quality of LDM-generated images. We employ commonly used metrics like FID and LPIPS for image realism assessment. Furthermore, we introduce new metrics to demonstrate the realism of images generated by RealDeal in terms of image noise distribution, sharpness, and texture.

</details>


### [24] [Dual Path Learning -- learning from noise and context for medical image denoising](https://arxiv.org/abs/2507.19035)
*Jitindra Fartiyal,Pedro Freire,Yasmeen Whayeb,James S. Wolffsohn,Sergei K. Turitsyn,Sergei G. Sokolov*

Main category: eess.IV

TL;DR: 论文提出了一种双路径学习（DPL）模型，通过结合噪声特征和上下文信息来去噪医学图像，并在多种模态和噪声类型中验证了其鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医学图像中的噪声会降低图像质量，影响诊断和治疗效果。现有方法通常仅依赖噪声特征或上下文信息，且局限于单一模态和噪声类型。

Method: 提出DPL模型，整合噪声和上下文信息，通过双路径学习生成最终去噪结果。

Result: DPL在多种模态和噪声类型中表现优异，PSNR比基线UNet提高了3.35%（高斯噪声）。

Conclusion: DPL是一种通用且鲁棒的医学图像去噪方法，优于现有技术。

Abstract: Medical imaging plays a critical role in modern healthcare, enabling clinicians to accurately diagnose diseases and develop effective treatment plans. However, noise, often introduced by imaging devices, can degrade image quality, leading to misinterpretation and compromised clinical outcomes. Existing denoising approaches typically rely either on noise characteristics or on contextual information from the image. Moreover, they are commonly developed and evaluated for a single imaging modality and noise type. Motivated by Geng et.al CNCL, which integrates both noise and context, this study introduces a Dual-Pathway Learning (DPL) model architecture that effectively denoises medical images by leveraging both sources of information and fusing them to generate the final output. DPL is evaluated across multiple imaging modalities and various types of noise, demonstrating its robustness and generalizability. DPL improves PSNR by 3.35% compared to the baseline UNet when evaluated on Gaussian noise and trained across all modalities. The code is available at 10.5281/zenodo.15836053.

</details>


### [25] [Reconstruct or Generate: Exploring the Spectrum of Generative Modeling for Cardiac MRI](https://arxiv.org/abs/2507.19186)
*Niklas Bubeck,Yundi Zhang,Suprosanna Shit,Daniel Rueckert,Jiazhen Pan*

Main category: eess.IV

TL;DR: 论文分析了生成模型在医学影像中的两种任务：重建与生成，比较了扩散模型和自回归模型的性能。


<details>
  <summary>Details</summary>
Motivation: 研究生成模型在医学影像中如何平衡重建（数据保真）与生成（感知质量）的不同需求。

Method: 引入“生成模型动物园”，系统评估扩散模型和自回归模型在不同任务（如修复和生成）中的表现。

Result: 扩散模型在无条件生成中表现更优，但修复任务中易产生幻觉；自回归模型修复稳定性高，但保真度较低。

Conclusion: 不同生成模型各有优劣，需根据任务需求选择合适模型。

Abstract: In medical imaging, generative models are increasingly relied upon for two distinct but equally critical tasks: reconstruction, where the goal is to restore medical imaging (usually inverse problems like inpainting or superresolution), and generation, where synthetic data is created to augment datasets or carry out counterfactual analysis. Despite shared architecture and learning frameworks, they prioritize different goals: generation seeks high perceptual quality and diversity, while reconstruction focuses on data fidelity and faithfulness. In this work, we introduce a "generative model zoo" and systematically analyze how modern latent diffusion models and autoregressive models navigate the reconstruction-generation spectrum. We benchmark a suite of generative models across representative cardiac medical imaging tasks, focusing on image inpainting with varying masking ratios and sampling strategies, as well as unconditional image generation. Our findings show that diffusion models offer superior perceptual quality for unconditional generation but tend to hallucinate as masking ratios increase, whereas autoregressive models maintain stable perceptual performance across masking levels, albeit with generally lower fidelity.

</details>


### [26] [Joint Holistic and Lesion Controllable Mammogram Synthesis via Gated Conditional Diffusion Model](https://arxiv.org/abs/2507.19201)
*Xin Li,Kaixiang Yang,Qiang Li,Zhiwei Wang*

Main category: eess.IV

TL;DR: 论文提出了一种新型的Gated Conditional Diffusion Model (GCDM)，用于联合合成完整的乳腺X光图像和局部病变，解决了现有生成模型在病变特征和周围组织关系上的不足。


<details>
  <summary>Details</summary>
Motivation: 乳腺X光检查是乳腺癌筛查的主要方法，但深度学习技术因数据不足和病变特征多样性缺乏而受限。现有生成模型未能充分强调病变特异性特征及其与周围组织的关系。

Method: GCDM基于潜在去噪扩散框架，通过软掩码嵌入表示乳腺、病变及其过渡区域，确保解剖一致性。此外，引入门控条件分支动态选择和融合病变的放射组学和几何特征。

Result: 实验表明，GCDM能精确控制小病变区域，同时提升合成乳腺X光图像的真实性和多样性。

Conclusion: GCDM在乳腺X光图像合成中表现出色，有望成为临床应用的实用工具。

Abstract: Mammography is the most commonly used imaging modality for breast cancer screening, driving an increasing demand for deep-learning techniques to support large-scale analysis. However, the development of accurate and robust methods is often limited by insufficient data availability and a lack of diversity in lesion characteristics. While generative models offer a promising solution for data synthesis, current approaches often fail to adequately emphasize lesion-specific features and their relationships with surrounding tissues. In this paper, we propose Gated Conditional Diffusion Model (GCDM), a novel framework designed to jointly synthesize holistic mammogram images and localized lesions. GCDM is built upon a latent denoising diffusion framework, where the noised latent image is concatenated with a soft mask embedding that represents breast, lesion, and their transitional regions, ensuring anatomical coherence between them during the denoising process. To further emphasize lesion-specific features, GCDM incorporates a gated conditioning branch that guides the denoising process by dynamically selecting and fusing the most relevant radiomic and geometric properties of lesions, effectively capturing their interplay. Experimental results demonstrate that GCDM achieves precise control over small lesion areas while enhancing the realism and diversity of synthesized mammograms. These advancements position GCDM as a promising tool for clinical applications in mammogram synthesis. Our code is available at https://github.com/lixinHUST/Gated-Conditional-Diffusion-Model/

</details>


### [27] [Unstable Prompts, Unreliable Segmentations: A Challenge for Longitudinal Lesion Analysis](https://arxiv.org/abs/2507.19230)
*Niels Rocholl,Ewoud Smit,Mathias Prokop,Alessa Hering*

Main category: eess.IV

TL;DR: ULS23模型在纵向病灶分析中表现不佳，主要因扫描配准错误和病灶对应失效。


<details>
  <summary>Details</summary>
Motivation: 研究ULS23模型在纵向CT扫描中的病灶分割和追踪性能，揭示单时间点模型的局限性。

Method: 使用公开临床数据集，评估模型在基线及随访CT中的表现，并通过人为位移实验验证模型脆弱性。

Result: 模型性能高度依赖病灶居中假设，位移后分割准确性显著下降。

Conclusion: 需开发端到端的纵向分析模型，而非依赖单时间点工具。

Abstract: Longitudinal lesion analysis is crucial for oncological care, yet automated tools often struggle with temporal consistency. While universal lesion segmentation models have advanced, they are typically designed for single time points. This paper investigates the performance of the ULS23 segmentation model in a longitudinal context. Using a public clinical dataset of baseline and follow-up CT scans, we evaluated the model's ability to segment and track lesions over time. We identified two critical, interconnected failure modes: a sharp degradation in segmentation quality in follow-up cases due to inter-scan registration errors, and a subsequent breakdown of the lesion correspondence process. To systematically probe this vulnerability, we conducted a controlled experiment where we artificially displaced the input volume relative to the true lesion center. Our results demonstrate that the model's performance is highly dependent on its assumption of a centered lesion; segmentation accuracy collapses when the lesion is sufficiently displaced. These findings reveal a fundamental limitation of applying single-timepoint models to longitudinal data. We conclude that robust oncological tracking requires a paradigm shift away from cascading single-purpose tools towards integrated, end-to-end models inherently designed for temporal analysis.

</details>


### [28] [NerT-CA: Efficient Dynamic Reconstruction from Sparse-view X-ray Coronary Angiography](https://arxiv.org/abs/2507.19328)
*Kirsten W. H. Maas,Danny Ruijters,Nicola Pezzotti,Anna Vilanova*

Main category: eess.IV

TL;DR: NerT-CA提出了一种结合神经和张量表示的方法，用于从稀疏视角X射线冠状动脉造影中加速4D重建，解决了现有方法训练时间长和精度不足的问题。


<details>
  <summary>Details</summary>
Motivation: 从X射线冠状动脉造影中实现3D和4D重建具有临床潜力，但现有方法依赖耗时的手动分割或易出错的自动分割，且稀疏视角和运动问题增加了难度。

Method: NerT-CA结合了神经场和张量场，将场景分解为低秩静态部分（张量场）和动态稀疏部分（神经场），从而加速重建并提高精度。

Result: 该方法在训练时间和重建精度上优于现有方法，仅需三个视角即可获得合理重建。

Conclusion: NerT-CA为稀疏视角冠状动脉造影的4D重建提供了一种高效且准确的解决方案。

Abstract: Three-dimensional (3D) and dynamic 3D+time (4D) reconstruction of coronary arteries from X-ray coronary angiography (CA) has the potential to improve clinical procedures. However, there are multiple challenges to be addressed, most notably, blood-vessel structure sparsity, poor background and blood vessel distinction, sparse-views, and intra-scan motion. State-of-the-art reconstruction approaches rely on time-consuming manual or error-prone automatic segmentations, limiting clinical usability. Recently, approaches based on Neural Radiance Fields (NeRF) have shown promise for automatic reconstructions in the sparse-view setting. However, they suffer from long training times due to their dependence on MLP-based representations. We propose NerT-CA, a hybrid approach of Neural and Tensorial representations for accelerated 4D reconstructions with sparse-view CA. Building on top of the previous NeRF-based work, we model the CA scene as a decomposition of low-rank and sparse components, utilizing fast tensorial fields for low-rank static reconstruction and neural fields for dynamic sparse reconstruction. Our approach outperforms previous works in both training time and reconstruction accuracy, yielding reasonable reconstructions from as few as three angiogram views. We validate our approach quantitatively and qualitatively on representative 4D phantom datasets.

</details>


### [29] [RealisVSR: Detail-enhanced Diffusion for Real-World 4K Video Super-Resolution](https://arxiv.org/abs/2507.19138)
*Weisong Zhao,Jingkai Zhou,Xiangyu Zhu,Weihua Chen,Xiao-Yu Zhang,Zhen Lei,Fan Wang*

Main category: eess.IV

TL;DR: RealisVSR提出了一种基于扩散模型的高频细节增强视频超分辨率方法，解决了现有方法在时间动态建模、高频细节恢复和4K超分辨率评估方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前视频超分辨率（VSR）方法在时间动态建模、高频细节恢复和4K超分辨率评估方面存在不足，RealisVSR旨在解决这些问题。

Method: 1）提出Consistency Preserved ControlNet（CPC）架构；2）引入High-Frequency Rectified Diffusion Loss（HR-Loss）；3）构建首个公开的4K VSR基准数据集RealisVideo-4K。

Result: 实验表明，RealisVSR在多个VSR基准测试中表现优异，尤其在超高分辨率场景下，且训练数据量仅为现有方法的5-25%。

Conclusion: RealisVSR通过创新的架构和损失函数，显著提升了视频超分辨率的质量，特别是在高频细节和4K分辨率场景下。

Abstract: Video Super-Resolution (VSR) has achieved significant progress through diffusion models, effectively addressing the over-smoothing issues inherent in GAN-based methods. Despite recent advances, three critical challenges persist in VSR community: 1) Inconsistent modeling of temporal dynamics in foundational models; 2) limited high-frequency detail recovery under complex real-world degradations; and 3) insufficient evaluation of detail enhancement and 4K super-resolution, as current methods primarily rely on 720P datasets with inadequate details. To address these challenges, we propose RealisVSR, a high-frequency detail-enhanced video diffusion model with three core innovations: 1) Consistency Preserved ControlNet (CPC) architecture integrated with the Wan2.1 video diffusion to model the smooth and complex motions and suppress artifacts; 2) High-Frequency Rectified Diffusion Loss (HR-Loss) combining wavelet decomposition and HOG feature constraints for texture restoration; 3) RealisVideo-4K, the first public 4K VSR benchmark containing 1,000 high-definition video-text pairs. Leveraging the advanced spatio-temporal guidance of Wan2.1, our method requires only 5-25% of the training data volume compared to existing approaches. Extensive experiments on VSR benchmarks (REDS, SPMCS, UDM10, YouTube-HQ, VideoLQ, RealisVideo-720P) demonstrate our superiority, particularly in ultra-high-resolution scenarios.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [30] [Diffusion Models for Solving Inverse Problems via Posterior Sampling with Piecewise Guidance](https://arxiv.org/abs/2507.18654)
*Saeed Mohseni-Sehdeh,Walid Saad,Kei Sakaguchi,Tao Yu*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型的分段引导框架，用于解决逆问题，平衡计算效率与精度，适用于多种任务且无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在高维分布采样中表现优异，但现有方法在解决逆问题时存在计算效率与精度难以平衡的问题。

Method: 引入分段引导机制，根据扩散时间步长定义引导项，区分高噪声和低噪声阶段的不同近似方法。

Result: 在图像修复任务中，推理时间显著减少（25%至24%），且PSNR和SSIM损失可忽略。

Conclusion: 该框架在多种逆问题中表现出高效性和适应性，同时显式纳入测量噪声，提升了重建质量。

Abstract: Diffusion models are powerful tools for sampling from high-dimensional distributions by progressively transforming pure noise into structured data through a denoising process. When equipped with a guidance mechanism, these models can also generate samples from conditional distributions. In this paper, a novel diffusion-based framework is introduced for solving inverse problems using a piecewise guidance scheme. The guidance term is defined as a piecewise function of the diffusion timestep, facilitating the use of different approximations during high-noise and low-noise phases. This design is shown to effectively balance computational efficiency with the accuracy of the guidance term. Unlike task-specific approaches that require retraining for each problem, the proposed method is problem-agnostic and readily adaptable to a variety of inverse problems. Additionally, it explicitly incorporates measurement noise into the reconstruction process. The effectiveness of the proposed framework is demonstrated through extensive experiments on image restoration tasks, specifically image inpainting and super-resolution. Using a class conditional diffusion model for recovery, compared to the \pgdm baseline, the proposed framework achieves a reduction in inference time of \(25\%\) for inpainting with both random and center masks, and \(23\%\) and \(24\%\) for \(4\times\) and \(8\times\) super-resolution tasks, respectively, while incurring only negligible loss in PSNR and SSIM.

</details>
