<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 9]
- [eess.IV](#eess.IV) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers](https://arxiv.org/abs/2509.03006)
*Tzuhsuan Huang,Cheng Yu Yeo,Tsai-Ling Huang,Hong-Han Shuai,Wen-Huang Cheng,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 通过组合CNN和Transformer攻击网络在空间域和频域的优势，提出了一种基于集成攻击网络的后处理深度水印方法，显著提升了水印模型的稳健性。


<details>
  <summary>Details</summary>
Motivation: 后处理水印技术比较灵活，可以应用于任何生成模型的输出，并允许用户为单个图像嵌入独特水印，但需要提升其稳健性。

Method: 在训练过程中添加集成攻击网络，构建了CNN和Transformer在空间域和频域的各种组合版本攻击网络，研究不同组合对水印模型稳健性的影响。

Result: 组合空间域CNN基攻击网络和频域Transformer基攻击网络获得最高稳健性。在WAVES基准测试中，该方法显著提升了基线水印方法的稳健性，在重生攻击中StegaStamp提升18.743%。

Conclusion: 通过集成多种攻击网络在训练中可有效提升后处理水印模型的稳健性，空间域CNN和频域Transformer的组合表现最优。

Abstract: Recent studies on deep watermarking have predominantly focused on in-processing watermarking, which integrates the watermarking process into image generation. However, post-processing watermarking, which embeds watermarks after image generation, offers more flexibility. It can be applied to outputs from any generative model (e.g. GANs, diffusion models) without needing access to the model's internal structure. It also allows users to embed unique watermarks into individual images. Therefore, this study focuses on post-processing watermarking and enhances its robustness by incorporating an ensemble attack network during training. We construct various versions of attack networks using CNN and Transformer in both spatial and frequency domains to investigate how each combination influences the robustness of the watermarking model. Our results demonstrate that combining a CNN-based attack network in the spatial domain with a Transformer-based attack network in the frequency domain yields the highest robustness in watermarking models. Extensive evaluation on the WAVES benchmark, using average bit accuracy as the metric, demonstrates that our ensemble attack network significantly enhances the robustness of baseline watermarking methods under various stress tests. In particular, for the Regeneration Attack defined in WAVES, our method improves StegaStamp by 18.743%. The code is released at:https://github.com/aiiu-lab/DeepRobustWatermark.

</details>


### [2] [DCDB: Dynamic Conditional Dual Diffusion Bridge for Ill-posed Multi-Tasks](https://arxiv.org/abs/2509.03044)
*Chengjie Huang,Jiafeng Yan,Jing Li,Lu Bai*

Main category: cs.CV

TL;DR: 这篇论文提出了一种动态条件双液化桥训练范式，用于解决条件液化模型在多任务场景中的挑战，特别是在训练数据缺乏的不良问题中。通过解耦液化和条件生成过程，以及使用动态条件来渐进调整统计特征，该方法在去霜和可见-红外融合任务中达到了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 条件液化模型在多任务场景中难以利用任务间的内在关联性，尤其是在训练数据缺乏的不良问题中。传统的静态条件控制方式难以适应多任务场景的动态演化特性。

Method: 提出动态条件双液化桥训练范式：1) 解耦液化和条件生成过程，避免液化模型对监督数据的依赖；2) 使用相同的噪声调度生成动态条件，渐进调整统计特征并嵌入时间相关信息，降低网络学习难度。

Result: 在去霜和可见-红外融合两个典型的不良多任务场景中，在公开数据集上的多个指标上达到了最佳性能。

Conclusion: 该方法通过动态条件控制和液化过程解耦，有效解决了多任务条件液化模型的挑战，特别是在训练数据缺乏的不良问题中显示出优勃的性能。

Abstract: Conditional diffusion models have made impressive progress in the field of image processing, but the characteristics of constructing data distribution pathways make it difficult to exploit the intrinsic correlation between tasks in multi-task scenarios, which is even worse in ill-posed tasks with a lack of training data. In addition, traditional static condition control makes it difficult for networks to learn in multi-task scenarios with its dynamically evolving characteristics. To address these challenges, we propose a dynamic conditional double diffusion bridge training paradigm to build a general framework for ill-posed multi-tasks. Firstly, this paradigm decouples the diffusion and condition generation processes, avoiding the dependence of the diffusion model on supervised data in ill-posed tasks. Secondly, generated by the same noise schedule, dynamic conditions are used to gradually adjust their statistical characteristics, naturally embed time-related information, and reduce the difficulty of network learning. We analyze the learning objectives of the network under different conditional forms in the single-step denoising process and compare the changes in its attention weights in the network, demonstrating the superiority of our dynamic conditions. Taking dehazing and visible-infrared fusion as typical ill-posed multi-task scenarios, we achieve the best performance in multiple indicators on public datasets. The code has been publicly released at: https://anonymous.4open.science/r/DCDB-D3C2.

</details>


### [3] [High Cursive Complex Character Recognition using GAN External Classifier](https://arxiv.org/abs/2509.03062)
*S M Rafiuddin*

Main category: cs.CV

TL;DR: 通过GAN生成假手写字符图像并添加对抗性噪声来扩充训练数据，ADA-GAN模型在复杂和连续字符分类中显示更好的稳健性和效果。


<details>
  <summary>Details</summary>
Motivation: 手写字符因其复杂和连续性质，比简单非连续字符更难分类，需要更有效的分类方法。

Method: 使用生成对抗网络(GAN)，生成器网络生产假手写字符图像，经过添加对抗性扰动噪声并达到一定信心阈值后，用于扩充训练数据集。

Result: 卷积神经网络的准确性随字符复杂度增加而下降，但ADA-GAN模型在复杂和连续字符分类中显示出更好的稳健性和效果。

Conclusion: ADA-GAN模型通过生成对抗网络和数据增强技术，能够有效提高复杂手写字符分类的性能。

Abstract: Handwritten characters can be trickier to classify due to their complex and cursive nature compared to simple and non-cursive characters. We present an external classifier along with a Generative Adversarial Network that can classify highly cursive and complex characters. The generator network produces fake handwritten character images, which are then used to augment the training data after adding adversarially perturbed noise and achieving a confidence score above a threshold with the discriminator network. The results show that the accuracy of convolutional neural networks decreases as character complexity increases, but our proposed model, ADA-GAN, remains more robust and effective for both cursive and complex characters.

</details>


### [4] [Temporally-Aware Diffusion Model for Brain Progression Modelling with Bidirectional Temporal Regularisation](https://arxiv.org/abs/2509.03141)
*Mattia Litrico,Francesco Guarnera,Mario Valerio Giuffrida,Daniele Ravì,Sebastiano Battiato*

Main category: cs.CV

TL;DR: 提出TADM-3D模型，使用3D扩散模型和脑龄估计器来准确预测MRI脑部结构随时间的变化，解决了现有方法在时间关系建模和3D上下文利用方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有MRI预测方法存在三个主要问题：(1)无法明确捕捉结构变化与时间间隔的关系；(2)仅依赖扫描插值，缺乏临床实用性；(3)大多基于2D架构，忽略了完整的3D解剖上下文。需要开发能够准确预测脑部病理进展的3D时序感知模型。

Method: 提出TADM-3D（3D时序感知扩散模型），使用预训练的脑龄估计器(BAE)指导扩散模型生成准确反映年龄差异的MRI。同时提出回溯时间正则化(BITR)，通过双向训练（从基线到随访以及从随访到基线）提高时序准确性。

Result: 在OASIS-3数据集上训练和评估，并在NACC数据集的外部测试集上验证了泛化性能。代码将在接受后提供。

Conclusion: TADM-3D通过结合脑龄估计和双向训练正则化，能够生成更准确的时序MRI预测，为临床评估疾病进展提供了有价值的工具。

Abstract: Generating realistic MRIs to accurately predict future changes in the structure of brain is an invaluable tool for clinicians in assessing clinical outcomes and analysing the disease progression at the patient level. However, current existing methods present some limitations: (i) some approaches fail to explicitly capture the relationship between structural changes and time intervals, especially when trained on age-imbalanced datasets; (ii) others rely only on scan interpolation, which lack clinical utility, as they generate intermediate images between timepoints rather than future pathological progression; and (iii) most approaches rely on 2D slice-based architectures, thereby disregarding full 3D anatomical context, which is essential for accurate longitudinal predictions. We propose a 3D Temporally-Aware Diffusion Model (TADM-3D), which accurately predicts brain progression on MRI volumes. To better model the relationship between time interval and brain changes, TADM-3D uses a pre-trained Brain-Age Estimator (BAE) that guides the diffusion model in the generation of MRIs that accurately reflect the expected age difference between baseline and generated follow-up scans. Additionally, to further improve the temporal awareness of TADM-3D, we propose the Back-In-Time Regularisation (BITR), by training TADM-3D to predict bidirectionally from the baseline to follow-up (forward), as well as from the follow-up to baseline (backward). Although predicting past scans has limited clinical applications, this regularisation helps the model generate temporally more accurate scans. We train and evaluate TADM-3D on the OASIS-3 dataset, and we validate the generalisation performance on an external test set from the NACC dataset. The code will be available upon acceptance.

</details>


### [5] [PPORLD-EDNetLDCT: A Proximal Policy Optimization-Based Reinforcement Learning Framework for Adaptive Low-Dose CT Denoising](https://arxiv.org/abs/2509.03185)
*Debopom Sutradhar,Ripon Kumar Debnath,Mohaimenul Azam Khan Raiaan,Yan Zhang,Reem E. Mohamed,Sami Azam*

Main category: cs.CV

TL;DR: 使用强化学习算法PPO的编码器-解码器网络(PPORLD-EDNetLDCT)，通过实时优化去噪策略，在LDCT图像去噪中获得了更高的PSNR、SSIM和更低的RMSE，并在新冠病分类任务中提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 低强度CT扩散能够最小化放射换受，但会导致噪声增加和图像质量下降。传统的迭代优化或监督学习方法往往无法保持图像质量。

Method: 提出了一种基于强化学习的编码器-解码器网络(PPORLD-EDNetLDCT)，利用进阶的后置策略优化(PPO)算法，基于图像质量反馈实时优化去噪策略，通过自定义gym环境进行训练。

Result: 在低强度CT图像和投影数据集上，方法达到了PSNR 41.87、SSIM 0.9814和RMSE 0.00236。在NIH-AAPM-Mayo Clinic数据集上获得PSNR 41.52、SSIM 0.9723和RMSE 0.0051。在新冠病LDCT数据集的分类任务中，去噪后图像的分类准确率提升到94%，比无RL去噪方法提高4%。

Conclusion: 该方法为更安全和准确的低强度CT成像提供了有前景的解决方案。

Abstract: Low-dose computed tomography (LDCT) is critical for minimizing radiation exposure, but it often leads to increased noise and reduced image quality. Traditional denoising methods, such as iterative optimization or supervised learning, often fail to preserve image quality. To address these challenges, we introduce PPORLD-EDNetLDCT, a reinforcement learning-based (RL) approach with Encoder-Decoder for LDCT. Our method utilizes a dynamic RL-based approach in which an advanced posterior policy optimization (PPO) algorithm is used to optimize denoising policies in real time, based on image quality feedback, trained via a custom gym environment. The experimental results on the low dose CT image and projection dataset demonstrate that the proposed PPORLD-EDNetLDCT model outperforms traditional denoising techniques and other DL-based methods, achieving a peak signal-to-noise ratio of 41.87, a structural similarity index measure of 0.9814 and a root mean squared error of 0.00236. Moreover, in NIH-AAPM-Mayo Clinic Low Dose CT Challenge dataset our method achived a PSNR of 41.52, SSIM of 0.9723 and RMSE of 0.0051. Furthermore, we validated the quality of denoising using a classification task in the COVID-19 LDCT dataset, where the images processed by our method improved the classification accuracy to 94\%, achieving 4\% higher accuracy compared to denoising without RL-based denoising. This method offers a promising solution for safer and more accurate LDCT imaging.

</details>


### [6] [AIVA: An AI-based Virtual Companion for Emotion-aware Interaction](https://arxiv.org/abs/2509.03212)
*Chenxi Li*

Main category: cs.CV

TL;DR: 通过多模态情感感知网络和情感敏感提示工程，将LLM升级为能够理解和响应非语言情绪信号的情感敏感代理


<details>
  <summary>Details</summary>
Motivation: 解决LLM仅能处理单一文本模态、缺乏情感感知能力的限制，提升HCI的沉浸式和共情性交互

Method: 设计多模态情感感知网络(MSPN)，采用跨模态融合transformer和监督对比学习＋情感敏感提示工程策略＋TTS和动态头像模块

Result: 开发出能够捕捉多模态情感索引、生成情感对齐响应的虚拟伴侣，实现了表情丰富的交互

Conclusion: 提供了情感敏感代理的框架，在伴侣机器人、社会养老、心理健康等领域具有应用潜力

Abstract: Recent advances in Large Language Models (LLMs) have significantly improved natural language understanding and generation, enhancing Human-Computer Interaction (HCI). However, LLMs are limited to unimodal text processing and lack the ability to interpret emotional cues from non-verbal signals, hindering more immersive and empathetic interactions. This work explores integrating multimodal sentiment perception into LLMs to create emotion-aware agents. We propose \ours, an AI-based virtual companion that captures multimodal sentiment cues, enabling emotionally aligned and animated HCI. \ours introduces a Multimodal Sentiment Perception Network (MSPN) using a cross-modal fusion transformer and supervised contrastive learning to provide emotional cues. Additionally, we develop an emotion-aware prompt engineering strategy for generating empathetic responses and integrate a Text-to-Speech (TTS) system and animated avatar module for expressive interactions. \ours provides a framework for emotion-aware agents with applications in companion robotics, social care, mental health, and human-centered AI.

</details>


### [7] [LGBP-OrgaNet: Learnable Gaussian Band Pass Fusion of CNN and Transformer Features for Robust Organoid Segmentation and Tracking](https://arxiv.org/abs/2509.03221)
*Jing Zhang,Siying Tao,Jiao Li,Tianhe Wang,Junchen Wu,Ruqian Hao,Xiaohui Du,Ruirong Tan,Rui Li*

Main category: cs.CV

TL;DR: 基于深度学习的LGBP-OrgaNet系统，通过CNN和Transformer模块提取补充信息，采用创新的可学习高斯带通融合模块和双向交叉融合块，实现了高精度的组织团分割、跟踪和定量分析


<details>
  <summary>Details</summary>
Motivation: 组织团能够复制器官结构和功能，其形状和大小可以反映发育状态，但传统的药物标记方法容易破坏组织团结构，需要一种自动化、无损坏的分割和跟踪方法

Method: 提出LGBP-OrgaNet深度学习系统：1）利用CNN和Transformer模块提取补充信息 2）创新的可学习高斯带通融合模块（Learnable Gaussian Band Pass Fusion）融合两个分支数据 3）在解码器中使用双向交叉融合块（Bidirectional Cross Fusion Block）融合多尺度特征 4）通过渐进式连接和上采样完成解码

Result: SROrga在组织团分割数据集上展现了满意的分割精度和稳健性

Conclusion: 该方法为组织团研究提供了一种强大的工具，能够在不破坏组织团结构的前提下实现高精度的自动化分割和跟踪

Abstract: Organoids replicate organ structure and function, playing a crucial role in fields such as tumor treatment and drug screening. Their shape and size can indicate their developmental status, but traditional fluorescence labeling methods risk compromising their structure. Therefore, this paper proposes an automated, non-destructive approach to organoid segmentation and tracking. We introduced the LGBP-OrgaNet, a deep learning-based system proficient in accurately segmenting, tracking, and quantifying organoids. The model leverages complementary information extracted from CNN and Transformer modules and introduces the innovative feature fusion module, Learnable Gaussian Band Pass Fusion, to merge data from two branches. Additionally, in the decoder, the model proposes a Bidirectional Cross Fusion Block to fuse multi-scale features, and finally completes the decoding through progressive concatenation and upsampling. SROrga demonstrates satisfactory segmentation accuracy and robustness on organoids segmentation datasets, providing a potent tool for organoid research.

</details>


### [8] [SynBT: High-quality Tumor Synthesis for Breast Tumor Segmentation by 3D Diffusion Model](https://arxiv.org/abs/2509.03267)
*Hongxu Yang,Edina Timko,Levente Lippenszky,Vanda Czipczer,Lehel Ferenczi*

Main category: cs.CV

TL;DR: 提出SynBT模型，使用3D扩散模型生成高质量乳腺肿瘤MRI图像，通过patch-to-volume自编码器和mask条件扩散模型，在大FOV图像中实现真实肿瘤合成，提升分割模型性能2-3% Dice分数


<details>
  <summary>Details</summary>
Motivation: 现有肿瘤合成方法在大空间体积肿瘤（如乳腺MRI大视场）上表现不佳，常用方法基于小patch，需要高质量大FOV肿瘤合成技术

Method: 提出SynBT 3D医学扩散模型，包含patch-to-volume自编码器压缩高分辨率MRI到紧凑潜在空间，保持大FOV体积分辨率；使用mask条件扩散模型在选定乳腺组织区域合成乳腺肿瘤

Result: 在肿瘤分割任务中评估，高质量肿瘤合成方法使常见分割模型性能提升2-3% Dice分数（大型公共数据集）

Conclusion: SynBT方法能为MRI图像中的肿瘤分割提供益处，生成高质量合成肿瘤改善机器学习模型训练和分割性能

Abstract: Synthetic tumors in medical images offer controllable characteristics that facilitate the training of machine learning models, leading to an improved segmentation performance. However, the existing methods of tumor synthesis yield suboptimal performances when tumor occupies a large spatial volume, such as breast tumor segmentation in MRI with a large field-of-view (FOV), while commonly used tumor generation methods are based on small patches. In this paper, we propose a 3D medical diffusion model, called SynBT, to generate high-quality breast tumor (BT) in contrast-enhanced MRI images. The proposed model consists of a patch-to-volume autoencoder, which is able to compress the high-resolution MRIs into compact latent space, while preserving the resolution of volumes with large FOV. Using the obtained latent space feature vector, a mask-conditioned diffusion model is used to synthesize breast tumors within selected regions of breast tissue, resulting in realistic tumor appearances. We evaluated the proposed method for a tumor segmentation task, which demonstrated the proposed high-quality tumor synthesis method can facilitate the common segmentation models with performance improvement of 2-3% Dice Score on a large public dataset, and therefore provides benefits for tumor segmentation in MRI images.

</details>


### [9] [InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds](https://arxiv.org/abs/2509.03324)
*Yixiong Jing,Cheng Zhang,Haibing Wu,Guangming Wang,Olaf Wysocki,Brian Sheil*

Main category: cs.CV

TL;DR: InfraDiffusion是一个零样本框架，通过虚拟相机将砖石点云转换为深度图，并使用DDNM进行恢复，无需特定任务训练即可显著提升砖块级分割效果。


<details>
  <summary>Details</summary>
Motivation: 在低光照环境下（如砖石隧道），获取高分辨率RGB图像不切实际，而点云虽然对暗光环境鲁棒，但通常是非结构化、稀疏且嘈杂的，限制了细粒度分割。

Method: 将砖石点云通过虚拟相机投影为深度图，采用去噪扩散零空间模型（DDNM）进行恢复，增强深度图的视觉清晰度和几何一致性，然后使用Segment Anything Model（SAM）进行砖块级分割。

Result: 在砖石桥梁和隧道点云数据集上的实验显示，该方法在砖块级分割方面有显著改进。

Conclusion: InfraDiffusion展示了在砖石资产自动化检测方面的潜力，特别是在低光照环境下点云数据的细粒度分割应用。

Abstract: Point clouds are widely used for infrastructure monitoring by providing geometric information, where segmentation is required for downstream tasks such as defect detection. Existing research has automated semantic segmentation of structural components, while brick-level segmentation (identifying defects such as spalling and mortar loss) has been primarily conducted from RGB images. However, acquiring high-resolution images is impractical in low-light environments like masonry tunnels. Point clouds, though robust to dim lighting, are typically unstructured, sparse, and noisy, limiting fine-grained segmentation. We present InfraDiffusion, a zero-shot framework that projects masonry point clouds into depth maps using virtual cameras and restores them by adapting the Denoising Diffusion Null-space Model (DDNM). Without task-specific training, InfraDiffusion enhances visual clarity and geometric consistency of depth maps. Experiments on masonry bridge and tunnel point cloud datasets show significant improvements in brick-level segmentation using the Segment Anything Model (SAM), underscoring its potential for automated inspection of masonry assets. Our code and data is available at https://github.com/Jingyixiong/InfraDiffusion-official-implement.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [10] [MitoDetect++: A Domain-Robust Pipeline for Mitosis Detection and Atypical Subtyping](https://arxiv.org/abs/2509.02586)
*Esha Sadia Nasir,Jiaqi Lv,Mostafa Jahanifer,Shan E Ahmed Raza*

Main category: eess.IV

TL;DR: MitoDetect++是一个用于MIDOG 2025挑战赛的深度学习管道，统一处理有丝分裂检测和非典型有丝分裂分类，在验证域上达到0.892的平衡准确率


<details>
  <summary>Details</summary>
Motivation: 自动检测和分类有丝分裂图像，特别是区分非典型与正常有丝分裂，是计算病理学中的关键挑战

Method: 使用U-Net编码器-解码器架构（EfficientNetV2-L主干）进行检测，Virchow2视觉变换器（LoRA微调）进行分类，结合注意力模块、强数据增强、焦点损失和分层交叉验证

Result: 在验证域上获得0.892的平衡准确率，显示出良好的临床适用性和跨任务可扩展性

Conclusion: 该方法通过统一的深度学习管道有效解决了有丝分裂检测和分类问题，具有强大的泛化能力和临床实用性

Abstract: Automated detection and classification of mitotic figures especially distinguishing atypical from normal remain critical challenges in computational pathology. We present MitoDetect++, a unified deep learning pipeline designed for the MIDOG 2025 challenge, addressing both mitosis detection and atypical mitosis classification. For detection (Track 1), we employ a U-Net-based encoder-decoder architecture with EfficientNetV2-L as the backbone, enhanced with attention modules, and trained via combined segmentation losses. For classification (Track 2), we leverage the Virchow2 vision transformer, fine-tuned efficiently using Low-Rank Adaptation (LoRA) to minimize resource consumption. To improve generalization and mitigate domain shifts, we integrate strong augmentations, focal loss, and group-aware stratified 5-fold cross-validation. At inference, we deploy test-time augmentation (TTA) to boost robustness. Our method achieves a balanced accuracy of 0.892 across validation domains, highlighting its clinical applicability and scalability across tasks.

</details>


### [11] [Foundation Model-Driven Classification of Atypical Mitotic Figures with Domain-Aware Training Strategies](https://arxiv.org/abs/2509.02601)
*Piotr Giedziun,Jan Sołtysik,Mateusz Górczany,Norbert Ropiak,Marcin Przymus,Piotr Krajewski,Jarosław Kwiecień,Artur Bartczak,Izabela Wasiak,Mateusz Maniewski*

Main category: eess.IV

TL;DR: 基于H-optimus-0病理基础模型，通过LoRA微调、MixUp增帽等技术，实现了正常和非式有困有困分裂相的二元分类任务


<details>
  <summary>Details</summary>
Motivation: 解决MIDOG 2025挑战赛身份识别正常有困分裂相(NMFs)与非式有困分裂相(AMFs)的复杂分类问题，探索基础模型在该领域的应用潜力与挑战

Method: 选择H-optimus-0病理基础模型，采用LoRA低质量调整、MixUp数据增帽，结合软标签、难例挖掘、适应性焦损失、距离学习和域适应技术

Result: 在预测评估阶段取得了合理的性能表现，证明了基础模型在该复杂分类任务中的应用潜力

Conclusion: 该方法成功展示了基础模型在病理形态学分析中的价值，同时也揭示了在复杂分类任务中的挑战与限制

Abstract: We present a solution for the MIDOG 2025 Challenge Track~2, addressing binary classification of normal mitotic figures (NMFs) versus atypical mitotic figures (AMFs). The approach leverages pathology-specific foundation model H-optimus-0, selected based on recent cross-domain generalization benchmarks and our empirical testing, with Low-Rank Adaptation (LoRA) fine-tuning and MixUp augmentation. Implementation includes soft labels based on multi-expert consensus, hard negative mining, and adaptive focal loss, metric learning and domain adaptation. The method demonstrates both the promise and challenges of applying foundation models to this complex classification task, achieving reasonable performance in the preliminary evaluation phase.

</details>


### [12] [Adaptive Learning Strategies for Mitotic Figure Classification in MIDOG2025 Challenge](https://arxiv.org/abs/2509.02640)
*Biwen Meng,Xi Long,Jingxin Liu*

Main category: eess.IV

TL;DR: 使用UNI2-h病理基础模型进行三种变体适配，通过视觉提示调优(VPT)和测试时增强(TTA)结合染色标准化，在MIDOG2025挑战赛中实现了88.37%的平衡准确率和95.13%的ROC-AUC


<details>
  <summary>Details</summary>
Motivation: 非典型有丝分裂图形(AMFs)是异常细胞分裂的重要临床指标，但由于形态学模糊性和扫描仪变异性，其可靠检测仍然具有挑战性

Method: 研究了UNI2-h病理基础模型的三种适配变体：从LoRA基线开始，发现视觉提示调优(VPT)显著改善泛化能力，进一步整合测试时增强(TTA)与Vahadane和Macenko染色标准化

Result: 最终提交在初步排行榜上获得0.8837的平衡准确率和0.9513的ROC-AUC，排名前十

Conclusion: 基于提示的适配结合染色标准化TTA为不同成像条件下的非典型有丝分裂分类提供了有效策略

Abstract: Atypical mitotic figures (AMFs) are clinically relevant indicators of abnormal cell division, yet their reliable detection remains challenging due to morphological ambiguity and scanner variability. In this work, we investigated three variants of adapting the pathology foundation model UNI2-h for the MIDOG2025 Track 2 challenge. Starting from a LoRA-based baseline, we found that visual prompt tuning (VPT) substantially improved generalization, and that further integrating test-time augmentation (TTA) with Vahadane and Macenko stain normalization provided the best robustness. Our final submission achieved a balanced accuracy of 0.8837 and an ROC-AUC of 0.9513 on the preliminary leaderboard, ranking within the top 10 teams. These results demonstrate that prompt-based adaptation combined with stain-normalization TTA offers an effective strategy for atypical mitosis classification under diverse imaging conditions.

</details>


### [13] [Prompt-Guided Patch UNet-VAE with Adversarial Supervision for Adrenal Gland Segmentation in Computed Tomography Medical Images](https://arxiv.org/abs/2509.03188)
*Hania Ghouse,Muzammil Behzad*

Main category: eess.IV

TL;DR: 通过结合变分重建、监督分割和对抗性补丁反馈的统一框架，解决CT图像中小器官分割的挑战，提高了边界敏感区域的分割准确性


<details>
  <summary>Details</summary>
Motivation: 解决腹部小器官（如副腰腺）在CT成像中的分割挑战，包括严重类不平衡、空间上下文信息较少和标注数据有限等问题

Method: 基于VAE-UNet的统一框架，结合变分重建、监督分割和对抗性补丁反馈；使用VGG特征的感知重建损失和PatchGAN判别器；通过学习的潜空间生成合成补丁来扩充训练数据

Result: 在BTCV数据集上的完整实验表明，该方法提高了分割准确性，尤其是在边界敏感区域，同时保持了强大的重建质量

Conclusion: 混合生成式-判别式训练方案对小器官分割有效，为在数据稀缺场景中平衡真实性、多样性和解剖一致性提供了新的见解

Abstract: Segmentation of small and irregularly shaped abdominal organs, such as the adrenal glands in CT imaging, remains a persistent challenge due to severe class imbalance, poor spatial context, and limited annotated data. In this work, we propose a unified framework that combines variational reconstruction, supervised segmentation, and adversarial patch-based feedback to address these limitations in a principled and scalable manner. Our architecture is built upon a VAE-UNet backbone that jointly reconstructs input patches and generates voxel-level segmentation masks, allowing the model to learn disentangled representations of anatomical structure and appearance. We introduce a patch-based training pipeline that selectively injects synthetic patches generated from the learned latent space, and systematically study the effects of varying synthetic-to-real patch ratios during training. To further enhance output fidelity, the framework incorporates perceptual reconstruction loss using VGG features, as well as a PatchGAN-style discriminator for adversarial supervision over spatial realism. Comprehensive experiments on the BTCV dataset demonstrate that our approach improves segmentation accuracy, particularly in boundary-sensitive regions, while maintaining strong reconstruction quality. Our findings highlight the effectiveness of hybrid generative-discriminative training regimes for small-organ segmentation and provide new insights into balancing realism, diversity, and anatomical consistency in data-scarce scenarios.

</details>
