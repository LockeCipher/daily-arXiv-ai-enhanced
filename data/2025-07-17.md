<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 26]
- [cs.LG](#cs.LG) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [HPR3D: Hierarchical Proxy Representation for High-Fidelity 3D Reconstruction and Controllable Editing](https://arxiv.org/abs/2507.11971)
*Tielong Wang,Yuxuan Xiong,Jinfan Liu,Zhifan Zhang,Ye Chen,Yue Shi,Bingbing Ni*

Main category: cs.GR

TL;DR: 提出了一种新型的3D层次代理节点表示方法，解决了现有3D表示在通用性、编辑性和复杂性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D表示（如网格、体素、点云和NeRF）存在任务特定性、编辑困难和结构模糊等问题，缺乏通用性。

Method: 通过稀疏的层次化代理节点表示物体的形状和纹理，每个节点存储局部信息，并通过轻量级解码和神经插值实现高效查询。

Result: 实验表明，该方法在3D重建和编辑中表现出高效表达、高保真渲染和优越的编辑性。

Conclusion: 提出的3D层次代理节点表示方法在通用性、编辑性和复杂性方面优于现有技术。

Abstract: Current 3D representations like meshes, voxels, point clouds, and NeRF-based neural implicit fields exhibit significant limitations: they are often task-specific, lacking universal applicability across reconstruction, generation, editing, and driving. While meshes offer high precision, their dense vertex data complicates editing; NeRFs deliver excellent rendering but suffer from structural ambiguity, hindering animation and manipulation; all representations inherently struggle with the trade-off between data complexity and fidelity. To overcome these issues, we introduce a novel 3D Hierarchical Proxy Node representation. Its core innovation lies in representing an object's shape and texture via a sparse set of hierarchically organized (tree-structured) proxy nodes distributed on its surface and interior. Each node stores local shape and texture information (implicitly encoded by a small MLP) within its neighborhood. Querying any 3D coordinate's properties involves efficient neural interpolation and lightweight decoding from relevant nearby and parent nodes. This framework yields a highly compact representation where nodes align with local semantics, enabling direct drag-and-edit manipulation, and offers scalable quality-complexity control. Extensive experiments across 3D reconstruction and editing demonstrate our method's expressive efficiency, high-fidelity rendering quality, and superior editability.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models](https://arxiv.org/abs/2507.11554)
*Zejian Li,Yize Li,Chenye Meng,Zhongni Liu,Yang Ling,Shengyuan Zhang,Guang Yang,Changyuan Yang,Zhiyuan Yang,Lingyun Sun*

Main category: cs.CV

TL;DR: Inversion-DPO是一种新的对齐框架，通过DDIM反演改进DPO，避免了奖励建模，提升了扩散模型的训练精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法需要计算密集型训练，可能导致模型准确性和效率下降，Inversion-DPO旨在解决这些问题。

Method: 利用DDIM反演将DPO重新表述，通过从样本到噪声的确定性反演进行后验采样，无需辅助奖励模型。

Result: 在文本到图像生成和组合图像生成任务中，Inversion-DPO显著优于现有方法，生成高保真且组合一致的图像。

Conclusion: Inversion-DPO为扩散模型提供了一种高效、高精度的对齐新方法，适用于复杂现实生成任务。

Abstract: Recent advancements in diffusion models (DMs) have been propelled by alignment methods that post-train models to better conform to human preferences. However, these approaches typically require computation-intensive training of a base model and a reward model, which not only incurs substantial computational overhead but may also compromise model accuracy and training efficiency. To address these limitations, we propose Inversion-DPO, a novel alignment framework that circumvents reward modeling by reformulating Direct Preference Optimization (DPO) with DDIM inversion for DMs. Our method conducts intractable posterior sampling in Diffusion-DPO with the deterministic inversion from winning and losing samples to noise and thus derive a new post-training paradigm. This paradigm eliminates the need for auxiliary reward models or inaccurate appromixation, significantly enhancing both precision and efficiency of training. We apply Inversion-DPO to a basic task of text-to-image generation and a challenging task of compositional image generation. Extensive experiments show substantial performance improvements achieved by Inversion-DPO compared to existing post-training methods and highlight the ability of the trained generative models to generate high-fidelity compositionally coherent images. For the post-training of compostitional image geneation, we curate a paired dataset consisting of 11,140 images with complex structural annotations and comprehensive scores, designed to enhance the compositional capabilities of generative models. Inversion-DPO explores a new avenue for efficient, high-precision alignment in diffusion models, advancing their applicability to complex realistic generation tasks. Our code is available at https://github.com/MIGHTYEZ/Inversion-DPO

</details>


### [3] [Expert Operational GANS: Towards Real-Color Underwater Image Restoration](https://arxiv.org/abs/2507.11562)
*Ozer Can Devecioglu,Serkan Kiranyaz,Mehmet Yamac,Moncef Gabbouj*

Main category: cs.CV

TL;DR: xOp-GAN是一种新型GAN模型，通过多个专家生成器网络分别处理不同质量范围的图像，解决了水下图像恢复中的异质性问题。


<details>
  <summary>Details</summary>
Motivation: 传统单生成器GAN难以应对水下图像恢复中的复杂退化问题，需要一种能处理异质性域的方法。

Method: 提出xOp-GAN，包含多个专家生成器，每个生成器专注于特定质量范围的图像恢复，并通过判别器选择最佳结果。

Result: 在LSUI数据集上，xOp-GAN的PSNR达到25.16 dB，显著优于单回归器模型。

Conclusion: xOp-GAN通过多生成器和判别器协同工作，显著提升了水下图像恢复的性能。

Abstract: The wide range of deformation artifacts that arise from complex light propagation, scattering, and depth-dependent attenuation makes the underwater image restoration to remain a challenging problem. Like other single deep regressor networks, conventional GAN-based restoration methods struggle to perform well across this heterogeneous domain, since a single generator network is typically insufficient to capture the full range of visual degradations. In order to overcome this limitation, we propose xOp-GAN, a novel GAN model with several expert generator networks, each trained solely on a particular subset with a certain image quality. Thus, each generator can learn to maximize its restoration performance for a particular quality range. Once a xOp-GAN is trained, each generator can restore the input image and the best restored image can then be selected by the discriminator based on its perceptual confidence score. As a result, xOP-GAN is the first GAN model with multiple generators where the discriminator is being used during the inference of the regression task. Experimental results on benchmark Large Scale Underwater Image (LSUI) dataset demonstrates that xOp-GAN achieves PSNR levels up to 25.16 dB, surpassing all single-regressor models by a large margin even, with reduced complexity.

</details>


### [4] [Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders](https://arxiv.org/abs/2507.11638)
*Benjamin Keel,Aaron Quyn,David Jayne,Maryam Mohsin,Samuel D. Relton*

Main category: cs.CV

TL;DR: 该论文提出了一种基于变分自编码器（VAE）的特征编码模型，用于提高直肠癌淋巴结转移（LNM）分期的准确性，替代了传统的大型预训练卷积神经网络（CNN）。


<details>
  <summary>Details</summary>
Motivation: 现有基于淋巴结大小、形状和纹理形态的放射学标准诊断准确性有限，而VAE通过重建图像直接编码视觉特征和有意义的数据模式，生成解耦且结构化的潜在空间，更具可解释性。

Method: 使用VAE作为特征编码器，结合多层感知机（MLP），在168名未接受新辅助治疗的患者的MRI数据集上进行训练和评估，术后病理N分期作为真实标签。

Result: 提出的'VAE-MLP'模型在MRI数据集上实现了最先进的性能，交叉验证指标为AUC 0.86 +/- 0.05，敏感性0.79 +/- 0.06，特异性0.85 +/- 0.05。

Conclusion: VAE-MLP模型在直肠癌淋巴结转移分期中表现出色，提供了一种更高效且可解释的替代方案。

Abstract: Effective treatment for rectal cancer relies on accurate lymph node metastasis (LNM) staging. However, radiological criteria based on lymph node (LN) size, shape and texture morphology have limited diagnostic accuracy. In this work, we investigate applying a Variational Autoencoder (VAE) as a feature encoder model to replace the large pre-trained Convolutional Neural Network (CNN) used in existing approaches. The motivation for using a VAE is that the generative model aims to reconstruct the images, so it directly encodes visual features and meaningful patterns across the data. This leads to a disentangled and structured latent space which can be more interpretable than a CNN. Models are deployed on an in-house MRI dataset with 168 patients who did not undergo neo-adjuvant treatment. The post-operative pathological N stage was used as the ground truth to evaluate model predictions. Our proposed model 'VAE-MLP' achieved state-of-the-art performance on the MRI dataset, with cross-validated metrics of AUC 0.86 +/- 0.05, Sensitivity 0.79 +/- 0.06, and Specificity 0.85 +/- 0.05. Code is available at: https://github.com/benkeel/Lymph_Node_Classification_MIUA.

</details>


### [5] [CorrMoE: Mixture of Experts with De-stylization Learning for Cross-Scene and Cross-Domain Correspondence Pruning](https://arxiv.org/abs/2507.11834)
*Peiwen Xia,Tangfei Liao,Wei Zhu,Danhuai Zhao,Jianjun Ke,Kaihao Zhang,Tong Lu,Tao Wang*

Main category: cs.CV

TL;DR: CorrMoE是一种新的对应关系修剪框架，通过去风格化双分支和双融合专家混合模块提升跨域和跨场景的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理密集对应关系时假设视觉域一致，忽视了多样场景结构的挑战。

Method: 提出De-stylization Dual Branch和Bi-Fusion Mixture of Experts模块，分别解决域偏移和场景多样性问题。

Result: 在基准数据集上表现优于现有方法，具有更高的准确性和泛化能力。

Conclusion: CorrMoE通过创新设计显著提升了跨域和跨场景的对应关系修剪性能。

Abstract: Establishing reliable correspondences between image pairs is a fundamental task in computer vision, underpinning applications such as 3D reconstruction and visual localization. Although recent methods have made progress in pruning outliers from dense correspondence sets, they often hypothesize consistent visual domains and overlook the challenges posed by diverse scene structures. In this paper, we propose CorrMoE, a novel correspondence pruning framework that enhances robustness under cross-domain and cross-scene variations. To address domain shift, we introduce a De-stylization Dual Branch, performing style mixing on both implicit and explicit graph features to mitigate the adverse influence of domain-specific representations. For scene diversity, we design a Bi-Fusion Mixture of Experts module that adaptively integrates multi-perspective features through linear-complexity attention and dynamic expert routing. Extensive experiments on benchmark datasets demonstrate that CorrMoE achieves superior accuracy and generalization compared to state-of-the-art methods. The code and pre-trained models are available at https://github.com/peiwenxia/CorrMoE.

</details>


### [6] [Spatial Frequency Modulation for Semantic Segmentation](https://arxiv.org/abs/2507.11893)
*Linwei Chen,Ying Fu,Lin Gu,Dezhi Zheng,Jifeng Dai*

Main category: cs.CV

TL;DR: 提出了一种空间频率调制（SFM）方法，通过调制高频特征以降低频率，再通过解调恢复高频信息，有效减轻混叠并保留细节。


<details>
  <summary>Details</summary>
Motivation: 高频信息对语义分割精度至关重要，但在下采样过程中易受混叠或失真影响。

Method: 采用自适应重采样（ARS）调制高频特征，设计多尺度自适应上采样（MSAU）解调，并结合多尺度信息交互。

Result: SFM有效减轻混叠并保留细节，适用于多种任务和架构。

Conclusion: SFM是一种通用且有效的方法，适用于多种视觉任务。

Abstract: High spatial frequency information, including fine details like textures, significantly contributes to the accuracy of semantic segmentation. However, according to the Nyquist-Shannon Sampling Theorem, high-frequency components are vulnerable to aliasing or distortion when propagating through downsampling layers such as strided-convolution. Here, we propose a novel Spatial Frequency Modulation (SFM) that modulates high-frequency features to a lower frequency before downsampling and then demodulates them back during upsampling. Specifically, we implement modulation through adaptive resampling (ARS) and design a lightweight add-on that can densely sample the high-frequency areas to scale up the signal, thereby lowering its frequency in accordance with the Frequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling (MSAU) to demodulate the modulated feature and recover high-frequency information through non-uniform upsampling This module further improves segmentation by explicitly exploiting information interaction between densely and sparsely resampled areas at multiple scales. Both modules can seamlessly integrate with various architectures, extending from convolutional neural networks to transformers. Feature visualization and analysis confirm that our method effectively alleviates aliasing while successfully retaining details after demodulation. Finally, we validate the broad applicability and effectiveness of SFM by extending it to image classification, adversarial robustness, instance segmentation, and panoptic segmentation tasks. The code is available at \href{https://github.com/Linwei-Chen/SFM}{https://github.com/Linwei-Chen/SFM}.

</details>


### [7] [Dark-EvGS: Event Camera as an Eye for Radiance Field in the Dark](https://arxiv.org/abs/2507.11931)
*Jingqian Wu,Peiqi Duan,Zongqiang Wang,Changwei Wang,Boxin Shi,Edmund Y. Lam*

Main category: cs.CV

TL;DR: Dark-EvGS框架通过事件相机和3D高斯泼溅技术，在低光环境下重建明亮的多视角图像，解决了噪声、低质量帧和色调不一致问题。


<details>
  <summary>Details</summary>
Motivation: 低光环境下传统相机难以捕捉清晰多视角图像，事件相机和高斯泼溅技术有潜力解决这一问题，但现有方法仍面临噪声和色调问题。

Method: 提出Dark-EvGS框架，采用三重监督学习、色调匹配模块，并构建首个真实数据集。

Result: 实验表明，该方法在低光条件下优于现有方法，成功重建辐射场。

Conclusion: Dark-EvGS为低光环境下的多视角图像重建提供了有效解决方案。

Abstract: In low-light environments, conventional cameras often struggle to capture clear multi-view images of objects due to dynamic range limitations and motion blur caused by long exposure. Event cameras, with their high-dynamic range and high-speed properties, have the potential to mitigate these issues. Additionally, 3D Gaussian Splatting (GS) enables radiance field reconstruction, facilitating bright frame synthesis from multiple viewpoints in low-light conditions. However, naively using an event-assisted 3D GS approach still faced challenges because, in low light, events are noisy, frames lack quality, and the color tone may be inconsistent. To address these issues, we propose Dark-EvGS, the first event-assisted 3D GS framework that enables the reconstruction of bright frames from arbitrary viewpoints along the camera trajectory. Triplet-level supervision is proposed to gain holistic knowledge, granular details, and sharp scene rendering. The color tone matching block is proposed to guarantee the color consistency of the rendered frames. Furthermore, we introduce the first real-captured dataset for the event-guided bright frame synthesis task via 3D GS-based radiance field reconstruction. Experiments demonstrate that our method achieves better results than existing methods, conquering radiance field reconstruction under challenging low-light conditions. The code and sample data are included in the supplementary material.

</details>


### [8] [RaDL: Relation-aware Disentangled Learning for Multi-Instance Text-to-Image Generation](https://arxiv.org/abs/2507.11947)
*Geon Park,Seon Bin Kim,Gunho Jung,Seong-Whan Lee*

Main category: cs.CV

TL;DR: RaDL框架通过关系感知解耦学习，解决了多实例图像生成中的关系差异和属性泄漏问题，显著提升了位置准确性和实例间关系表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多实例图像生成中难以处理关系差异和多重属性泄漏，RaDL旨在解决这些问题。

Method: RaDL利用可学习参数增强实例特定属性，并通过关系注意力生成关系感知图像特征，结合全局提示中的动作动词。

Result: 在COCO-Position、COCO-MIG和DrawBench等基准测试中，RaDL表现优于现有方法，显著提升了位置准确性和实例间关系。

Conclusion: RaDL是多实例图像生成的解决方案，能同时考虑实例间关系和多重属性。

Abstract: With recent advancements in text-to-image (T2I) models, effectively generating multiple instances within a single image prompt has become a crucial challenge. Existing methods, while successful in generating positions of individual instances, often struggle to account for relationship discrepancy and multiple attributes leakage. To address these limitations, this paper proposes the relation-aware disentangled learning (RaDL) framework. RaDL enhances instance-specific attributes through learnable parameters and generates relation-aware image features via Relation Attention, utilizing action verbs extracted from the global prompt. Through extensive evaluations on benchmarks such as COCO-Position, COCO-MIG, and DrawBench, we demonstrate that RaDL outperforms existing methods, showing significant improvements in positional accuracy, multiple attributes consideration, and the relationships between instances. Our results present RaDL as the solution for generating images that consider both the relationships and multiple attributes of each instance within the multi-instance image.

</details>


### [9] [Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on Short Videos for Content Appropriateness Evaluation](https://arxiv.org/abs/2507.11968)
*Sahid Hossain Mustakim,S M Jishanul Islam,Ummay Maria Muna,Montasir Chowdhury,Mohammed Jawwadul Islam,Sadia Ahmmed,Tashfia Sikder,Syed Tasdid Azam Dhrubo,Swakkhar Shatabda*

Main category: cs.CV

TL;DR: 该论文提出了一个评估多模态大语言模型（MLLMs）在短视频内容审核中安全性的框架，包括SVMA数据集和ChimeraBreak攻击策略，揭示了模型在视觉、听觉和语义推理路径上的显著漏洞。


<details>
  <summary>Details</summary>
Motivation: 当前的安全评估主要针对单模态攻击，未能全面评估多模态攻击的脆弱性，尤其是在短视频内容审核中。

Method: 提出了SVMA数据集和ChimeraBreak三模态攻击策略，通过实验评估MLLMs的安全性。

Result: 实验显示MLLMs在视觉、听觉和语义推理路径上存在显著漏洞，攻击成功率（ASR）较高。

Conclusion: 研究结果为开发更鲁棒和安全的MLLMs提供了重要见解。

Abstract: Multimodal Large Language Models (MLLMs) are increasingly used for content moderation, yet their robustness in short-form video contexts remains underexplored. Current safety evaluations often rely on unimodal attacks, failing to address combined attack vulnerabilities. In this paper, we introduce a comprehensive framework for evaluating the tri-modal safety of MLLMs. First, we present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising diverse short-form videos with human-guided synthetic adversarial attacks. Second, we propose ChimeraBreak, a novel tri-modal attack strategy that simultaneously challenges visual, auditory, and semantic reasoning pathways. Extensive experiments on state-of-the-art MLLMs reveal significant vulnerabilities with high Attack Success Rates (ASR). Our findings uncover distinct failure modes, showing model biases toward misclassifying benign or policy-violating content. We assess results using LLM-as-a-judge, demonstrating attack reasoning efficacy. Our dataset and findings provide crucial insights for developing more robust and safe MLLMs.

</details>


### [10] [EC-Diff: Fast and High-Quality Edge-Cloud Collaborative Inference for Diffusion Models](https://arxiv.org/abs/2507.11980)
*Jiajian Xie,Shengyu Zhang,Zhou Zhao,Fan Wu,Fei Wu*

Main category: cs.CV

TL;DR: EC-Diff是一种混合边缘-云协作框架，通过梯度噪声估计加速云推理，并通过K步噪声近似策略和两阶段贪心搜索算法优化云-边缘切换点，显著提升生成质量和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有混合边缘-云协作框架中，过多的云去噪步骤延长推理时间，而步骤不足则导致语义模糊和边缘模型输出不一致。

Method: 提出K步噪声近似策略减少云推理频率，设计两阶段贪心搜索算法优化噪声近似和边缘模型切换参数。

Result: 实验表明，EC-Diff在生成质量上优于边缘推理，推理速度比云推理平均提升2倍。

Conclusion: EC-Diff通过优化云-边缘协作，实现了高效且高质量的图像和视频生成。

Abstract: Diffusion Models have shown remarkable proficiency in image and video synthesis. As model size and latency increase limit user experience, hybrid edge-cloud collaborative framework was recently proposed to realize fast inference and high-quality generation, where the cloud model initiates high-quality semantic planning and the edge model expedites later-stage refinement. However, excessive cloud denoising prolongs inference time, while insufficient steps cause semantic ambiguity, leading to inconsistency in edge model output. To address these challenges, we propose EC-Diff that accelerates cloud inference through gradient-based noise estimation while identifying the optimal point for cloud-edge handoff to maintain generation quality. Specifically, we design a K-step noise approximation strategy to reduce cloud inference frequency by using noise gradients between steps and applying cloud inference periodically to adjust errors. Then we design a two-stage greedy search algorithm to efficiently find the optimal parameters for noise approximation and edge model switching. Extensive experiments demonstrate that our method significantly enhances generation quality compared to edge inference, while achieving up to an average $2\times$ speedup in inference compared to cloud inference. Video samples and source code are available at https://ec-diff.github.io/.

</details>


### [11] [Unsupervised Part Discovery via Descriptor-Based Masked Image Restoration with Optimized Constraints](https://arxiv.org/abs/2507.11985)
*Jiahao Xia,Yike Wu,Wenjian Huang,Jianguo Zhang,Jian Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为Masked Part Autoencoder (MPAE)的无监督部件发现方法，通过学习部件描述符和特征图，利用掩码和恢复机制，实现跨类别和场景的鲁棒性部件发现。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏细粒度标签，部件级特征的研究较少，现有无监督方法在跨类别和场景的鲁棒性不足，限制了应用范围。

Method: MPAE通过学习部件描述符和特征图，利用掩码图像生成局部特征，并通过相似性恢复掩码区域，使部件描述符与形状对齐。

Result: MPAE在复杂场景中鲁棒地发现与实际物体形状匹配的部件，并通过实验验证了其跨类别和场景的有效性。

Conclusion: MPAE为无监督部件发现提供了新范式，解决了遮挡问题，并支持跨类别部件相似性研究。

Abstract: Part-level features are crucial for image understanding, but few studies focus on them because of the lack of fine-grained labels. Although unsupervised part discovery can eliminate the reliance on labels, most of them cannot maintain robustness across various categories and scenarios, which restricts their application range. To overcome this limitation, we present a more effective paradigm for unsupervised part discovery, named Masked Part Autoencoder (MPAE). It first learns part descriptors as well as a feature map from the inputs and produces patch features from a masked version of the original images. Then, the masked regions are filled with the learned part descriptors based on the similarity between the local features and descriptors. By restoring these masked patches using the part descriptors, they become better aligned with their part shapes, guided by appearance features from unmasked patches. Finally, MPAE robustly discovers meaningful parts that closely match the actual object shapes, even in complex scenarios. Moreover, several looser yet more effective constraints are proposed to enable MPAE to identify the presence of parts across various scenarios and categories in an unsupervised manner. This provides the foundation for addressing challenges posed by occlusion and for exploring part similarity across multiple categories. Extensive experiments demonstrate that our method robustly discovers meaningful parts across various categories and scenarios. The code is available at the project https://github.com/Jiahao-UTS/MPAE.

</details>


### [12] [Style Composition within Distinct LoRA modules for Traditional Art](https://arxiv.org/abs/2507.11986)
*Jaehyun Lee,Wonhark Park,Wonsik Shin,Hyunho Lee,Hyoung Min Na,Nojun Kwak*

Main category: cs.CV

TL;DR: 提出了一种零样本扩散管道，通过融合不同风格模型的去噪潜在空间，实现区域特定的风格混合。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到图像模型中风格纠缠和插值不平滑的问题，实现可控的区域风格混合。

Method: 在去噪过程中融合不同风格模型的潜在空间，利用空间掩码和深度图条件控制风格混合。

Result: 成功实现了根据掩码进行区域特定风格混合，保持了各风格的保真度。

Conclusion: 该方法为多风格混合提供了可控且高质量的解决方案。

Abstract: Diffusion-based text-to-image models have achieved remarkable results in synthesizing diverse images from text prompts and can capture specific artistic styles via style personalization. However, their entangled latent space and lack of smooth interpolation make it difficult to apply distinct painting techniques in a controlled, regional manner, often causing one style to dominate. To overcome this, we propose a zero-shot diffusion pipeline that naturally blends multiple styles by performing style composition on the denoised latents predicted during the flow-matching denoising process of separately trained, style-specialized models. We leverage the fact that lower-noise latents carry stronger stylistic information and fuse them across heterogeneous diffusion pipelines using spatial masks, enabling precise, region-specific style control. This mechanism preserves the fidelity of each individual style while allowing user-guided mixing. Furthermore, to ensure structural coherence across different models, we incorporate depth-map conditioning via ControlNet into the diffusion framework. Qualitative and quantitative experiments demonstrate that our method successfully achieves region-specific style mixing according to the given masks.

</details>


### [13] [ID-EA: Identity-driven Text Enhancement and Adaptation with Textual Inversion for Personalized Text-to-Image Generation](https://arxiv.org/abs/2507.11990)
*Hyun-Jun Jin,Young-Eun Kim,Seong-Whan Lee*

Main category: cs.CV

TL;DR: ID-EA框架通过ID-Enhancer和ID-Adapter改进文本嵌入与视觉身份嵌入的对齐，显著提升个性化肖像生成中的身份一致性。


<details>
  <summary>Details</summary>
Motivation: 当前Textual Inversion方法在个性化肖像生成中难以保持面部身份一致性，主要由于文本与视觉嵌入空间在身份语义上的不匹配。

Method: ID-EA包含ID-Enhancer和ID-Adapter：前者通过文本ID锚点优化视觉身份嵌入，后者调整预训练UNet模型的交叉注意力模块以确保身份保留。

Result: ID-EA在身份保留指标上显著优于现有方法，且计算效率高，生成速度比现有方法快约15倍。

Conclusion: ID-EA通过改进文本与视觉嵌入的对齐，有效解决了身份一致性问题，同时提升了生成效率。

Abstract: Recently, personalized portrait generation with a text-to-image diffusion model has significantly advanced with Textual Inversion, emerging as a promising approach for creating high-fidelity personalized images. Despite its potential, current Textual Inversion methods struggle to maintain consistent facial identity due to semantic misalignments between textual and visual embedding spaces regarding identity. We introduce ID-EA, a novel framework that guides text embeddings to align with visual identity embeddings, thereby improving identity preservation in a personalized generation. ID-EA comprises two key components: the ID-driven Enhancer (ID-Enhancer) and the ID-conditioned Adapter (ID-Adapter). First, the ID-Enhancer integrates identity embeddings with a textual ID anchor, refining visual identity embeddings derived from a face recognition model using representative text embeddings. Then, the ID-Adapter leverages the identity-enhanced embedding to adapt the text condition, ensuring identity preservation by adjusting the cross-attention module in the pre-trained UNet model. This process encourages the text features to find the most related visual clues across the foreground snippets. Extensive quantitative and qualitative evaluations demonstrate that ID-EA substantially outperforms state-of-the-art methods in identity preservation metrics while achieving remarkable computational efficiency, generating personalized portraits approximately 15 times faster than existing approaches.

</details>


### [14] [SGLoc: Semantic Localization System for Camera Pose Estimation from 3D Gaussian Splatting Representation](https://arxiv.org/abs/2507.12027)
*Beining Xu,Siting Zhu,Hesheng Wang*

Main category: cs.CV

TL;DR: SGLoc是一种新颖的定位系统，通过利用语义信息直接从3D高斯泼溅（3DGS）表示中回归相机姿态。


<details>
  <summary>Details</summary>
Motivation: 解决无需初始姿态先验的6自由度（6DoF）相机姿态估计问题，利用语义信息提升定位精度。

Method: 采用多级姿态回归策略，结合语义全局检索算法，逐步估计和优化查询图像的姿态。

Result: 在12scenes和7scenes数据集上表现优于基线方法，展示了无需初始姿态先验的全局定位能力。

Conclusion: SGLoc通过语义信息和3DGS表示实现了高效的相机姿态估计，适用于无先验姿态的场景定位。

Abstract: We propose SGLoc, a novel localization system that directly regresses camera poses from 3D Gaussian Splatting (3DGS) representation by leveraging semantic information. Our method utilizes the semantic relationship between 2D image and 3D scene representation to estimate the 6DoF pose without prior pose information. In this system, we introduce a multi-level pose regression strategy that progressively estimates and refines the pose of query image from the global 3DGS map, without requiring initial pose priors. Moreover, we introduce a semantic-based global retrieval algorithm that establishes correspondences between 2D (image) and 3D (3DGS map). By matching the extracted scene semantic descriptors of 2D query image and 3DGS semantic representation, we align the image with the local region of the global 3DGS map, thereby obtaining a coarse pose estimation. Subsequently, we refine the coarse pose by iteratively optimizing the difference between the query image and the rendered image from 3DGS. Our SGLoc demonstrates superior performance over baselines on 12scenes and 7scenes datasets, showing excellent capabilities in global localization without initial pose prior. Code will be available at https://github.com/IRMVLab/SGLoc.

</details>


### [15] [MS-DETR: Towards Effective Video Moment Retrieval and Highlight Detection by Joint Motion-Semantic Learning](https://arxiv.org/abs/2507.12062)
*Hongxu Ma,Guanshuo Wang,Fufu Yu,Qiong Jia,Shouhong Ding*

Main category: cs.CV

TL;DR: MS-DETR框架通过统一学习捕获运动-语义特征，提升视频时刻检索和高光检测性能，并通过生成策略和对比去噪学习解决数据稀疏问题。


<details>
  <summary>Details</summary>
Motivation: 现有DETR框架未充分利用视频中时间运动和空间语义的复杂关系，限制了性能提升。

Method: 提出MS-DETR，编码器显式建模运动与语义的模态内相关性，解码器利用任务间相关性进行精确定位；通过生成策略和对比去噪学习解决数据稀疏问题。

Result: 在四个基准测试中，MS-DETR显著优于现有最优模型。

Conclusion: MS-DETR通过统一学习运动-语义特征和解决数据稀疏问题，显著提升了视频时刻检索和高光检测的性能。

Abstract: Video Moment Retrieval (MR) and Highlight Detection (HD) aim to pinpoint specific moments and assess clip-wise relevance based on the text query. While DETR-based joint frameworks have made significant strides, there remains untapped potential in harnessing the intricate relationships between temporal motion and spatial semantics within video content. In this paper, we propose the Motion-Semantics DETR (MS-DETR), a framework that captures rich motion-semantics features through unified learning for MR/HD tasks. The encoder first explicitly models disentangled intra-modal correlations within motion and semantics dimensions, guided by the given text queries. Subsequently, the decoder utilizes the task-wise correlation across temporal motion and spatial semantics dimensions to enable precise query-guided localization for MR and refined highlight boundary delineation for HD. Furthermore, we observe the inherent sparsity dilemma within the motion and semantics dimensions of MR/HD datasets. To address this issue, we enrich the corpus from both dimensions by generation strategies and propose contrastive denoising learning to ensure the above components learn robustly and effectively. Extensive experiments on four MR/HD benchmarks demonstrate that our method outperforms existing state-of-the-art models by a margin. Our code is available at https://github.com/snailma0229/MS-DETR.git.

</details>


### [16] [BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images](https://arxiv.org/abs/2507.12095)
*Davide Di Nucci,Matteo Tomei,Guido Borghi,Luca Ciuffreda,Roberto Vezzani,Rita Cucchiara*

Main category: cs.CV

TL;DR: 提出了一种基于稀疏视图输入的车辆3D重建方法，结合深度图和鲁棒的姿态估计架构，改进了高斯泼溅技术，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法如NeRF和高斯泼溅依赖密集输入视图，限制了实际应用，因此需解决稀疏视图下的高质量重建问题。

Method: 整合选择性光度损失（仅用于高置信像素）和DUSt3R架构改进姿态估计，增强高斯泼溅技术，并构建新数据集。

Result: 在多个基准测试中达到领先水平，即使在输入受限条件下也能实现高质量重建。

Conclusion: 该方法显著提升了稀疏视图下的车辆3D重建效果，具有实际应用潜力。

Abstract: Accurate 3D reconstruction of vehicles is vital for applications such as vehicle inspection, predictive maintenance, and urban planning. Existing methods like Neural Radiance Fields and Gaussian Splatting have shown impressive results but remain limited by their reliance on dense input views, which hinders real-world applicability. This paper addresses the challenge of reconstructing vehicles from sparse-view inputs, leveraging depth maps and a robust pose estimation architecture to synthesize novel views and augment training data. Specifically, we enhance Gaussian Splatting by integrating a selective photometric loss, applied only to high-confidence pixels, and replacing standard Structure-from-Motion pipelines with the DUSt3R architecture to improve camera pose estimation. Furthermore, we present a novel dataset featuring both synthetic and real-world public transportation vehicles, enabling extensive evaluation of our approach. Experimental results demonstrate state-of-the-art performance across multiple benchmarks, showcasing the method's ability to achieve high-quality reconstructions even under constrained input conditions.

</details>


### [17] [DeepShade: Enable Shade Simulation by Text-conditioned Image Generation](https://arxiv.org/abs/2507.12103)
*Longchao Da,Xiangrui Liu,Mithun Shivakoti,Thirulogasankar Pranav Kutralingam,Yezhou Yang,Hua Wei*

Main category: cs.CV

TL;DR: 论文提出了一种名为DeepShade的扩散模型，用于从卫星图像中学习并合成阴影变化，以改进热浪天气下的路径规划。


<details>
  <summary>Details</summary>
Motivation: 热浪对公共健康构成威胁，但现有路径规划系统缺乏阴影信息，主要因为卫星图像噪声大且训练数据有限。

Method: 1. 构建包含多样城市布局的阴影数据集；2. 提出DeepShade模型，结合RGB和边缘特征，利用对比学习捕捉阴影时间变化。

Result: 模型能根据已知条件（如时间、太阳角度）生成更准确的阴影图像，并在亚利桑那州Tempe的实际路径规划中验证了实用性。

Conclusion: 该研究为极端热浪天气下的城市规划提供了参考，具有潜在的环境应用价值。

Abstract: Heatwaves pose a significant threat to public health, especially as global warming intensifies. However, current routing systems (e.g., online maps) fail to incorporate shade information due to the difficulty of estimating shades directly from noisy satellite imagery and the limited availability of training data for generative models. In this paper, we address these challenges through two main contributions. First, we build an extensive dataset covering diverse longitude-latitude regions, varying levels of building density, and different urban layouts. Leveraging Blender-based 3D simulations alongside building outlines, we capture building shadows under various solar zenith angles throughout the year and at different times of day. These simulated shadows are aligned with satellite images, providing a rich resource for learning shade patterns. Second, we propose the DeepShade, a diffusion-based model designed to learn and synthesize shade variations over time. It emphasizes the nuance of edge features by jointly considering RGB with the Canny edge layer, and incorporates contrastive learning to capture the temporal change rules of shade. Then, by conditioning on textual descriptions of known conditions (e.g., time of day, solar angles), our framework provides improved performance in generating shade images. We demonstrate the utility of our approach by using our shade predictions to calculate shade ratios for real-world route planning in Tempe, Arizona. We believe this work will benefit society by providing a reference for urban planning in extreme heat weather and its potential practical applications in the environment.

</details>


### [18] [LidarPainter: One-Step Away From Any Lidar View To Novel Guidance](https://arxiv.org/abs/2507.12114)
*Yuzhou Ji,Ke Ma,Hong Cai,Anchun Zhang,Lizhuang Ma,Xin Tan*

Main category: cs.CV

TL;DR: LidarPainter是一种基于扩散模型的实时方法，用于从稀疏LiDAR数据和损坏的渲染中恢复一致的驾驶场景视图，支持高保真车道变换和风格化生成。


<details>
  <summary>Details</summary>
Motivation: 动态驾驶场景重建在数字孪生系统和自动驾驶模拟中非常重要，但现有方法在偏离输入轨迹时会出现质量下降问题。

Method: 提出LidarPainter，一种一步扩散模型，从稀疏LiDAR条件和损坏的渲染中实时恢复一致的驾驶视图。

Result: 实验表明，LidarPainter在速度、质量和资源效率上优于现有方法，速度快7倍且仅需五分之一的GPU内存。

Conclusion: LidarPainter不仅提升了重建质量，还支持通过文本提示生成多样化风格，扩展了现有资源库。

Abstract: Dynamic driving scene reconstruction is of great importance in fields like digital twin system and autonomous driving simulation. However, unacceptable degradation occurs when the view deviates from the input trajectory, leading to corrupted background and vehicle models. To improve reconstruction quality on novel trajectory, existing methods are subject to various limitations including inconsistency, deformation, and time consumption. This paper proposes LidarPainter, a one-step diffusion model that recovers consistent driving views from sparse LiDAR condition and artifact-corrupted renderings in real-time, enabling high-fidelity lane shifts in driving scene reconstruction. Extensive experiments show that LidarPainter outperforms state-of-the-art methods in speed, quality and resource efficiency, specifically 7 x faster than StreetCrafter with only one fifth of GPU memory required. LidarPainter also supports stylized generation using text prompts such as "foggy" and "night", allowing for a diverse expansion of the existing asset library.

</details>


### [19] [Learning Pixel-adaptive Multi-layer Perceptrons for Real-time Image Enhancement](https://arxiv.org/abs/2507.12135)
*Junyu Lou,Xiaorui Zhao,Kexuan Shi,Shuhang Gu*

Main category: cs.CV

TL;DR: 提出了一种结合双边网格和MLP的BPAM框架，用于图像增强，解决了现有方法的线性和全局参数限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有双边网格方法仅支持线性变换，而传统MLP方法难以处理局部变化，需结合两者优势。

Method: 通过双边网格存储MLP参数，动态生成像素级MLP，并引入网格分解策略和多通道引导图优化参数提取。

Result: 在公开数据集上表现优于现有方法，且保持实时处理能力。

Conclusion: BPAM框架成功结合了双边网格的空间建模和MLP的非线性能力，提升了图像增强效果。

Abstract: Deep learning-based bilateral grid processing has emerged as a promising solution for image enhancement, inherently encoding spatial and intensity information while enabling efficient full-resolution processing through slicing operations. However, existing approaches are limited to linear affine transformations, hindering their ability to model complex color relationships. Meanwhile, while multi-layer perceptrons (MLPs) excel at non-linear mappings, traditional MLP-based methods employ globally shared parameters, which is hard to deal with localized variations. To overcome these dual challenges, we propose a Bilateral Grid-based Pixel-Adaptive Multi-layer Perceptron (BPAM) framework. Our approach synergizes the spatial modeling of bilateral grids with the non-linear capabilities of MLPs. Specifically, we generate bilateral grids containing MLP parameters, where each pixel dynamically retrieves its unique transformation parameters and obtain a distinct MLP for color mapping based on spatial coordinates and intensity values. In addition, we propose a novel grid decomposition strategy that categorizes MLP parameters into distinct types stored in separate subgrids. Multi-channel guidance maps are used to extract category-specific parameters from corresponding subgrids, ensuring effective utilization of color information during slicing while guiding precise parameter generation. Extensive experiments on public datasets demonstrate that our method outperforms state-of-the-art methods in performance while maintaining real-time processing capabilities.

</details>


### [20] [AD-GS: Object-Aware B-Spline Gaussian Splatting for Self-Supervised Autonomous Driving](https://arxiv.org/abs/2507.12137)
*Jiawei Xu,Kai Deng,Zexin Fan,Shenlong Wang,Jin Xie,Jian Yang*

Main category: cs.CV

TL;DR: AD-GS是一种自监督框架，用于从单一日志中高质量渲染驾驶场景，无需手动标注，通过动态高斯和双向时间可见性掩模实现场景分解。


<details>
  <summary>Details</summary>
Motivation: 解决现有自监督方法在动态物体运动和场景分解上的不足，减少对昂贵手动标注的依赖。

Method: 结合局部感知B样条曲线和全局感知三角函数的学习运动模型，动态高斯表示物体，双向时间可见性掩模和物理刚性正则化增强鲁棒性。

Result: 在无标注方法中表现显著优于现有技术，与依赖标注的方法竞争。

Conclusion: AD-GS为高质量驾驶场景渲染提供了一种高效且无需标注的解决方案。

Abstract: Modeling and rendering dynamic urban driving scenes is crucial for self-driving simulation. Current high-quality methods typically rely on costly manual object tracklet annotations, while self-supervised approaches fail to capture dynamic object motions accurately and decompose scenes properly, resulting in rendering artifacts. We introduce AD-GS, a novel self-supervised framework for high-quality free-viewpoint rendering of driving scenes from a single log. At its core is a novel learnable motion model that integrates locality-aware B-spline curves with global-aware trigonometric functions, enabling flexible yet precise dynamic object modeling. Rather than requiring comprehensive semantic labeling, AD-GS automatically segments scenes into objects and background with the simplified pseudo 2D segmentation, representing objects using dynamic Gaussians and bidirectional temporal visibility masks. Further, our model incorporates visibility reasoning and physically rigid regularization to enhance robustness. Extensive evaluations demonstrate that our annotation-free model significantly outperforms current state-of-the-art annotation-free methods and is competitive with annotation-dependent approaches.

</details>


### [21] [Neural Human Pose Prior](https://arxiv.org/abs/2507.12138)
*Michal Heker,Sefy Kararlitsky,David Tolpin*

Main category: cs.CV

TL;DR: 提出了一种基于归一化流的数据驱动方法，用于建模人体姿态的神经先验，通过RealNVP学习6D旋转姿态的灵活密度分布。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在建模6D旋转姿态分布时的启发式或低表达能力问题，提供稳定的学习框架。

Method: 利用RealNVP学习6D旋转姿态的密度分布，通过反转Gram-Schmidt过程解决流形分布建模问题。

Result: 通过定性和定量评估验证了方法的有效性，并通过消融研究分析了其影响。

Conclusion: 为人体运动捕捉和重建中的姿态先验提供了概率基础，具有框架无关性和易复现性。

Abstract: We introduce a principled, data-driven approach for modeling a neural prior over human body poses using normalizing flows. Unlike heuristic or low-expressivity alternatives, our method leverages RealNVP to learn a flexible density over poses represented in the 6D rotation format. We address the challenge of modeling distributions on the manifold of valid 6D rotations by inverting the Gram-Schmidt process during training, enabling stable learning while preserving downstream compatibility with rotation-based frameworks. Our architecture and training pipeline are framework-agnostic and easily reproducible. We demonstrate the effectiveness of the learned prior through both qualitative and quantitative evaluations, and we analyze its impact via ablation studies. This work provides a sound probabilistic foundation for integrating pose priors into human motion capture and reconstruction pipelines.

</details>


### [22] [Wavelet-based Decoupling Framework for low-light Stereo Image Enhancement](https://arxiv.org/abs/2507.12188)
*Shuangli Du,Siming Yan,Zhenghao Shi,Zhenzhen You,Lu Sun*

Main category: cs.CV

TL;DR: 本文提出了一种基于小波变换的低光立体图像增强方法，通过特征空间解耦解决现有方法中特征纠缠和黑盒特性问题。


<details>
  <summary>Details</summary>
Motivation: 低光图像存在复杂的退化问题，现有方法将所有退化因素编码在单一潜在空间中，导致特征高度纠缠和模型易受捷径学习影响。

Method: 利用小波变换将特征空间分解为低频分支（用于光照调整）和多个高频分支（用于纹理增强），并提出了高频引导的跨视图交互模块（HF-CIM）和基于交叉注意力的细节纹理增强模块（DTEM）。

Result: 实验结果表明，该方法在光照调整和高频信息恢复方面具有显著优势。

Conclusion: 通过小波变换和特征空间解耦，该方法有效提升了低光立体图像的增强效果，代码和数据集已公开。

Abstract: Low-light images suffer from complex degradation, and existing enhancement methods often encode all degradation factors within a single latent space. This leads to highly entangled features and strong black-box characteristics, making the model prone to shortcut learning. To mitigate the above issues, this paper proposes a wavelet-based low-light stereo image enhancement method with feature space decoupling. Our insight comes from the following findings: (1) Wavelet transform enables the independent processing of low-frequency and high-frequency information. (2) Illumination adjustment can be achieved by adjusting the low-frequency component of a low-light image, extracted through multi-level wavelet decomposition. Thus, by using wavelet transform the feature space is decomposed into a low-frequency branch for illumination adjustment and multiple high-frequency branches for texture enhancement. Additionally, stereo low-light image enhancement can extract useful cues from another view to improve enhancement. To this end, we propose a novel high-frequency guided cross-view interaction module (HF-CIM) that operates within high-frequency branches rather than across the entire feature space, effectively extracting valuable image details from the other view. Furthermore, to enhance the high-frequency information, a detail and texture enhancement module (DTEM) is proposed based on cross-attention mechanism. The model is trained on a dataset consisting of images with uniform illumination and images with non-uniform illumination. Experimental results on both real and synthetic images indicate that our algorithm offers significant advantages in light adjustment while effectively recovering high-frequency information. The code and dataset are publicly available at: https://github.com/Cherisherr/WDCI-Net.git.

</details>


### [23] [Revealing the Ancient Beauty: Digital Reconstruction of Temple Tiles using Computer Vision](https://arxiv.org/abs/2507.12195)
*Arkaprabha Basu*

Main category: cs.CV

TL;DR: 论文提出了三种先进技术（Fractal Convolution、SSTF和Super Resolution）用于印度文化遗产的保护与修复，结合计算机视觉和机器学习，实现了高效且低成本的自动化处理。


<details>
  <summary>Details</summary>
Motivation: 现代数字化方法为文化遗产保护带来了革命性变化，但印度古迹的特殊性需要更精细的技术。本文旨在通过创新方法解决这一问题，平衡传统与创新。

Method: 1. Fractal Convolution用于图像分割；2. SSTF结合MosaicSlice数据增强方法；3. Super Resolution提升图像质量。

Result: 提出的方法实现了高精度修复和区域填充，同时保持了文化遗产的真实性，且成本可控。

Conclusion: 研究为文化遗产保护提供了高效且美观的解决方案，推动了该领域的进步。

Abstract: Modern digitised approaches have dramatically changed the preservation and restoration of cultural treasures, integrating computer scientists into multidisciplinary projects with ease. Machine learning, deep learning, and computer vision techniques have revolutionised developing sectors like 3D reconstruction, picture inpainting,IoT-based methods, genetic algorithms, and image processing with the integration of computer scientists into multidisciplinary initiatives. We suggest three cutting-edge techniques in recognition of the special qualities of Indian monuments, which are famous for their architectural skill and aesthetic appeal. First is the Fractal Convolution methodology, a segmentation method based on image processing that successfully reveals subtle architectural patterns within these irreplaceable cultural buildings. The second is a revolutionary Self-Sensitive Tile Filling (SSTF) method created especially for West Bengal's mesmerising Bankura Terracotta Temples with a brand-new data augmentation method called MosaicSlice on the third. Furthermore, we delve deeper into the Super Resolution strategy to upscale the images without losing significant amount of quality. Our methods allow for the development of seamless region-filling and highly detailed tiles while maintaining authenticity using a novel data augmentation strategy within affordable costs introducing automation. By providing effective solutions that preserve the delicate balance between tradition and innovation, this study improves the subject and eventually ensures unrivalled efficiency and aesthetic excellence in cultural heritage protection. The suggested approaches advance the field into an era of unmatched efficiency and aesthetic quality while carefully upholding the delicate equilibrium between tradition and innovation.

</details>


### [24] [MGFFD-VLM: Multi-Granularity Prompt Learning for Face Forgery Detection with VLM](https://arxiv.org/abs/2507.12232)
*Tao Chen,Jingyi Zhang,Decheng Liu,Chunlei Peng*

Main category: cs.CV

TL;DR: 论文提出了一种新的深度伪造检测框架MGFFD-VLM，通过扩展数据集和引入多粒度提示学习等方法，显著提升了检测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用人脸质量相关属性，且缺乏有效的训练策略，导致检测效果受限。

Method: 扩展了VQA数据集为DD-VQA+，引入Attribute-Driven Hybrid LoRA Strategy、多粒度提示学习和伪造感知训练策略。

Result: 实验表明，该方法在文本伪造判断和分析方面优于现有方法，准确率更高。

Conclusion: MGFFD-VLM框架通过多策略结合，显著提升了深度伪造检测的性能和可解释性。

Abstract: Recent studies have utilized visual large language models (VLMs) to answer not only "Is this face a forgery?" but also "Why is the face a forgery?" These studies introduced forgery-related attributes, such as forgery location and type, to construct deepfake VQA datasets and train VLMs, achieving high accuracy while providing human-understandable explanatory text descriptions. However, these methods still have limitations. For example, they do not fully leverage face quality-related attributes, which are often abnormal in forged faces, and they lack effective training strategies for forgery-aware VLMs. In this paper, we extend the VQA dataset to create DD-VQA+, which features a richer set of attributes and a more diverse range of samples. Furthermore, we introduce a novel forgery detection framework, MGFFD-VLM, which integrates an Attribute-Driven Hybrid LoRA Strategy to enhance the capabilities of Visual Large Language Models (VLMs). Additionally, our framework incorporates Multi-Granularity Prompt Learning and a Forgery-Aware Training Strategy. By transforming classification and forgery segmentation results into prompts, our method not only improves forgery classification but also enhances interpretability. To further boost detection performance, we design multiple forgery-related auxiliary losses. Experimental results demonstrate that our approach surpasses existing methods in both text-based forgery judgment and analysis, achieving superior accuracy.

</details>


### [25] [FADE: Adversarial Concept Erasure in Flow Models](https://arxiv.org/abs/2507.12283)
*Zixuan Fu,Yan Ren,Finn Carter,Chenyue Wang,Ze Niu,Dacheng Yu,Emily Davis,Bo Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为FADE的新方法，用于从文本到图像扩散模型中删除指定概念，确保隐私和公平性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成方面表现出色，但也存在隐私泄露和偏见传播的风险，需要一种方法可靠地删除敏感或有害概念。

Method: FADE结合轨迹感知微调策略和对抗目标，最小化被删除概念与模型输出之间的互信息。

Result: FADE在多个基准测试中表现最佳，概念删除效果和图像质量均优于现有方法。

Conclusion: FADE为安全公平的生成建模设定了新标准，无需从头训练即可删除指定概念。

Abstract: Diffusion models have demonstrated remarkable image generation capabilities, but also pose risks in privacy and fairness by memorizing sensitive concepts or perpetuating biases. We propose a novel \textbf{concept erasure} method for text-to-image diffusion models, designed to remove specified concepts (e.g., a private individual or a harmful stereotype) from the model's generative repertoire. Our method, termed \textbf{FADE} (Fair Adversarial Diffusion Erasure), combines a trajectory-aware fine-tuning strategy with an adversarial objective to ensure the concept is reliably removed while preserving overall model fidelity. Theoretically, we prove a formal guarantee that our approach minimizes the mutual information between the erased concept and the model's outputs, ensuring privacy and fairness. Empirically, we evaluate FADE on Stable Diffusion and FLUX, using benchmarks from prior work (e.g., object, celebrity, explicit content, and style erasure tasks from MACE). FADE achieves state-of-the-art concept removal performance, surpassing recent baselines like ESD, UCE, MACE, and ANT in terms of removal efficacy and image quality. Notably, FADE improves the harmonic mean of concept removal and fidelity by 5--10\% over the best prior method. We also conduct an ablation study to validate each component of FADE, confirming that our adversarial and trajectory-preserving objectives each contribute to its superior performance. Our work sets a new standard for safe and fair generative modeling by unlearning specified concepts without retraining from scratch.

</details>


### [26] [Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models](https://arxiv.org/abs/2507.12318)
*Samuel Lavoie,Michael Noukhovitch,Aaron Courville*

Main category: cs.CV

TL;DR: 论文提出离散潜在码（DLC）作为扩散模型的输入条件表示，提升生成质量和组合性，实现超出训练分布的样本生成。


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型成功的关键在于输入条件表示，理想表示应提升样本保真度、易于生成且具有组合性。

Method: 引入基于自监督学习的离散潜在码（DLC），作为离散令牌序列替代连续嵌入，用于条件扩散模型。

Result: DLC显著提升无条件图像生成质量（ImageNet上SOTA），并支持组合生成超出训练分布的新样本。

Conclusion: DLC为扩散模型提供高效、组合性强的条件表示，支持文本到图像生成等应用。

Abstract: We argue that diffusion models' success in modeling complex distributions is, for the most part, coming from their input conditioning. This paper investigates the representation used to condition diffusion models from the perspective that ideal representations should improve sample fidelity, be easy to generate, and be compositional to allow out-of-training samples generation. We introduce Discrete Latent Code (DLC), an image representation derived from Simplicial Embeddings trained with a self-supervised learning objective. DLCs are sequences of discrete tokens, as opposed to the standard continuous image embeddings. They are easy to generate and their compositionality enables sampling of novel images beyond the training distribution. Diffusion models trained with DLCs have improved generation fidelity, establishing a new state-of-the-art for unconditional image generation on ImageNet. Additionally, we show that composing DLCs allows the image generator to produce out-of-distribution samples that coherently combine the semantics of images in diverse ways. Finally, we showcase how DLCs can enable text-to-image generation by leveraging large-scale pretrained language models. We efficiently finetune a text diffusion language model to generate DLCs that produce novel samples outside of the image generator training distribution.

</details>


### [27] [Unsupervised Monocular 3D Keypoint Discovery from Multi-View Diffusion Priors](https://arxiv.org/abs/2507.12336)
*Subin Jeon,In Cho,Junyoung Hong,Seon Joo Kim*

Main category: cs.CV

TL;DR: KeyDiff3D是一个无监督的单目3D关键点估计框架，通过预训练的多视角扩散模型生成多视角图像作为监督信号，实现从单张图像预测3D关键点。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵的手动标注或多视角图像，而KeyDiff3D仅需单视角图像即可实现3D关键点估计。

Method: 利用预训练的多视角扩散模型生成多视角图像作为监督信号，并从中提取2D多视角特征构建3D特征体积。

Result: 在Human3.6M、Stanford Dogs等数据集上验证了方法的准确性、泛化能力及对3D对象生成与操纵的能力。

Conclusion: KeyDiff3D无需昂贵标注，有效实现了单目3D关键点估计，并支持对扩散模型生成的3D对象进行操纵。

Abstract: This paper introduces KeyDiff3D, a framework for unsupervised monocular 3D keypoints estimation that accurately predicts 3D keypoints from a single image. While previous methods rely on manual annotations or calibrated multi-view images, both of which are expensive to collect, our method enables monocular 3D keypoints estimation using only a collection of single-view images. To achieve this, we leverage powerful geometric priors embedded in a pretrained multi-view diffusion model. In our framework, this model generates multi-view images from a single image, serving as a supervision signal to provide 3D geometric cues to our model. We also use the diffusion model as a powerful 2D multi-view feature extractor and construct 3D feature volumes from its intermediate representations. This transforms implicit 3D priors learned by the diffusion model into explicit 3D features. Beyond accurate keypoints estimation, we further introduce a pipeline that enables manipulation of 3D objects generated by the diffusion model. Experimental results on diverse aspects and datasets, including Human3.6M, Stanford Dogs, and several in-the-wild and out-of-domain datasets, highlight the effectiveness of our method in terms of accuracy, generalization, and its ability to enable manipulation of 3D objects generated by the diffusion model from a single image.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [28] [RegCL: Continual Adaptation of Segment Anything Model via Model Merging](https://arxiv.org/abs/2507.12297)
*Yuan-Chen Shu,Zhiwei Lin,Yongtao Wang*

Main category: cs.LG

TL;DR: 本文提出RegCL，一种新型非回放持续学习框架，通过模型合并实现多领域知识高效整合，解决SAM在特定领域性能受限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为特定领域设计，跨领域使用时性能下降，限制了模型的可扩展性。

Method: RegCL将模型合并算法引入持续学习，通过优化权重合并不同领域的适配模块参数。

Result: 实验表明，RegCL在多领域下游任务中表现优异，验证了其动态场景下的有效性。

Conclusion: RegCL通过参数高效合并，实现了多领域知识的整合，解决了灾难性遗忘问题。

Abstract: To address the performance limitations of the Segment Anything Model (SAM) in specific domains, existing works primarily adopt adapter-based one-step adaptation paradigms. However, some of these methods are specific developed for specific domains. If used on other domains may lead to performance degradation. This issue of catastrophic forgetting severely limits the model's scalability. To address this issue, this paper proposes RegCL, a novel non-replay continual learning (CL) framework designed for efficient multi-domain knowledge integration through model merging. Specifically, RegCL incorporates the model merging algorithm into the continual learning paradigm by merging the parameters of SAM's adaptation modules (e.g., LoRA modules) trained on different domains. The merging process is guided by weight optimization, which minimizes prediction discrepancies between the merged model and each of the domain-specific models. RegCL effectively consolidates multi-domain knowledge while maintaining parameter efficiency, i.e., the model size remains constant regardless of the number of tasks, and no historical data storage is required. Experimental results demonstrate that RegCL achieves favorable continual learning performance across multiple downstream datasets, validating its effectiveness in dynamic scenarios.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [29] [3D Wavelet Latent Diffusion Model for Whole-Body MR-to-CT Modality Translation](https://arxiv.org/abs/2507.11557)
*Jiaxu Zheng,Meiman He,Xuhui Tang,Xiong Wang,Tuoyu Cao,Tianyi Zeng,Lichi Zhang,Chenyu You*

Main category: eess.IV

TL;DR: 本文提出了一种新型3D小波潜在扩散模型（3D-WLDM），用于解决MR到CT图像合成中的空间对齐和图像质量问题。


<details>
  <summary>Details</summary>
Motivation: MR成像在临床诊断和治疗中日益重要，但现有的MR到CT合成方法存在空间对齐差和图像质量不足的问题，影响下游临床应用。

Method: 通过在小波潜在空间中进行模态转换，结合小波残差模块增强特征捕获，并利用双跳跃连接注意力机制生成高分辨率CT图像。

Result: 模型能够生成具有更好骨骼结构和软组织对比的高分辨率CT图像。

Conclusion: 3D-WLDM显著提升了MR到CT合成的准确性和图像质量，适用于临床任务。

Abstract: Magnetic Resonance (MR) imaging plays an essential role in contemporary clinical diagnostics. It is increasingly integrated into advanced therapeutic workflows, such as hybrid Positron Emission Tomography/Magnetic Resonance (PET/MR) imaging and MR-only radiation therapy. These integrated approaches are critically dependent on accurate estimation of radiation attenuation, which is typically facilitated by synthesizing Computed Tomography (CT) images from MR scans to generate attenuation maps. However, existing MR-to-CT synthesis methods for whole-body imaging often suffer from poor spatial alignment between the generated CT and input MR images, and insufficient image quality for reliable use in downstream clinical tasks. In this paper, we present a novel 3D Wavelet Latent Diffusion Model (3D-WLDM) that addresses these limitations by performing modality translation in a learned latent space. By incorporating a Wavelet Residual Module into the encoder-decoder architecture, we enhance the capture and reconstruction of fine-scale features across image and latent spaces. To preserve anatomical integrity during the diffusion process, we disentangle structural and modality-specific characteristics and anchor the structural component to prevent warping. We also introduce a Dual Skip Connection Attention mechanism within the diffusion model, enabling the generation of high-resolution CT images with improved representation of bony structures and soft-tissue contrast.

</details>


### [30] [Predicting Pulmonary Hypertension in Newborns: A Multi-view VAE Approach](https://arxiv.org/abs/2507.11561)
*Lucas Erlacher,Samuel Ruipérez-Campillo,Holger Michel,Sven Wellmann,Thomas M. Sutter,Ece Ozkan,Julia E. Vogt*

Main category: eess.IV

TL;DR: 该论文提出了一种基于多视角变分自编码器（VAE）的方法，用于新生儿肺动脉高压（PH）的预测，通过多视角超声心动图视频提高特征提取和鲁棒性，优于单视角和监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 新生儿肺动脉高压（PH）的诊断依赖超声心动图，但其准确性受操作者主观影响，且现有自动化方法多针对成人，单视角模型性能有限。多视角方法虽有潜力，但现有模型泛化能力不足。

Method: 采用多视角变分自编码器（VAE）框架，利用超声心动图视频提取复杂潜在表征，提升特征提取和模型鲁棒性，并与单视角和监督学习方法进行对比。

Result: 实验结果表明，多视角学习方法在泛化能力和分类准确性上优于单视角和监督学习方法。

Conclusion: 多视角学习方法能有效提升新生儿PH评估的鲁棒性和准确性。

Abstract: Pulmonary hypertension (PH) in newborns is a critical condition characterized by elevated pressure in the pulmonary arteries, leading to right ventricular strain and heart failure. While right heart catheterization (RHC) is the diagnostic gold standard, echocardiography is preferred due to its non-invasive nature, safety, and accessibility. However, its accuracy highly depends on the operator, making PH assessment subjective. While automated detection methods have been explored, most models focus on adults and rely on single-view echocardiographic frames, limiting their performance in diagnosing PH in newborns. While multi-view echocardiography has shown promise in improving PH assessment, existing models struggle with generalizability. In this work, we employ a multi-view variational autoencoder (VAE) for PH prediction using echocardiographic videos. By leveraging the VAE framework, our model captures complex latent representations, improving feature extraction and robustness. We compare its performance against single-view and supervised learning approaches. Our results show improved generalization and classification accuracy, highlighting the effectiveness of multi-view learning for robust PH assessment in newborns.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [31] [DoRF: Doppler Radiance Fields for Robust Human Activity Recognition Using Wi-Fi](https://arxiv.org/abs/2507.12132)
*Navid Hasanzadeh,Shahrokh Valaee*

Main category: eess.SP

TL;DR: 本文提出了一种基于Wi-Fi CSI的3D潜在运动表示方法，通过构建统一的Doppler辐射场（DoRF）提升人类活动识别的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管Wi-Fi CSI的多普勒速度投影在人类活动识别（HAR）中表现出一定的鲁棒性，但其泛化能力仍不足以实际应用。

Method: 受神经辐射场（NeRF）启发，从Wi-Fi CSI的一维多普勒速度投影中重建3D潜在运动表示，并构建统一的Doppler辐射场（DoRF）。

Result: 实验结果表明，该方法显著提升了Wi-Fi HAR的泛化准确性。

Conclusion: DoRF在实用传感应用中具有巨大潜力。

Abstract: Wi-Fi Channel State Information (CSI) has gained increasing interest for remote sensing applications. Recent studies show that Doppler velocity projections extracted from CSI can enable human activity recognition (HAR) that is robust to environmental changes and generalizes to new users. However, despite these advances, generalizability still remains insufficient for practical deployment. Inspired by neural radiance fields (NeRF), which learn a volumetric representation of a 3D scene from 2D images, this work proposes a novel approach to reconstruct an informative 3D latent motion representation from one-dimensional Doppler velocity projections extracted from Wi-Fi CSI. The resulting latent representation is then used to construct a uniform Doppler radiance field (DoRF) of the motion, providing a comprehensive view of the performed activity and improving the robustness to environmental variability. The results show that the proposed approach noticeably enhances the generalization accuracy of Wi-Fi-based HAR, highlighting the strong potential of DoRFs for practical sensing applications.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [32] [A Spatial-Physics Informed Model for 3D Spiral Sample Scanned by SQUID Microscopy](https://arxiv.org/abs/2507.11853)
*J. Senthilnath,Jayasanker Jayabalan,Zhuoyi Lin,Aye Phyu Phyu Aung,Chen Hao,Kaixin Xu,Yeow Kheng Lim,F. C. Wellstood*

Main category: physics.ins-det

TL;DR: 论文提出了一种空间物理信息模型（SPIM），用于改进半导体先进封装中的非破坏性测试（NDT），通过磁成像（MFI）解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 由于先进封装层深度和复杂性增加，现有非破坏性测试方法（如FFT）未考虑涡流效应和图像错位问题，需要更有效的解决方案。

Method: SPIM包含三个关键部分：磁图像增强（对齐信号以减少涡流效应）、磁图像对齐（解决扫描显微镜的错位问题）、以及结合毕奥-萨伐尔定律和FFT的磁场反演方法。

Result: SPIM提高了I通道锐度0.3%，减少Q通道锐度25%，并成功校正了0.30的旋转和错位偏差。

Conclusion: SPIM展示了空间分析与物理驱动模型结合在实际应用中的潜力，为半导体封装测试提供了更精确的工具。

Abstract: The development of advanced packaging is essential in the semiconductor manufacturing industry. However, non-destructive testing (NDT) of advanced packaging becomes increasingly challenging due to the depth and complexity of the layers involved. In such a scenario, Magnetic field imaging (MFI) enables the imaging of magnetic fields generated by currents. For MFI to be effective in NDT, the magnetic fields must be converted into current density. This conversion has typically relied solely on a Fast Fourier Transform (FFT) for magnetic field inversion; however, the existing approach does not consider eddy current effects or image misalignment in the test setup. In this paper, we present a spatial-physics informed model (SPIM) designed for a 3D spiral sample scanned using Superconducting QUantum Interference Device (SQUID) microscopy. The SPIM encompasses three key components: i) magnetic image enhancement by aligning all the "sharp" wire field signals to mitigate the eddy current effect using both in-phase (I-channel) and quadrature-phase (Q-channel) images; (ii) magnetic image alignment that addresses skew effects caused by any misalignment of the scanning SQUID microscope relative to the wire segments; and (iii) an inversion method for converting magnetic fields to magnetic currents by integrating the Biot-Savart Law with FFT. The results show that the SPIM improves I-channel sharpness by 0.3% and reduces Q-channel sharpness by 25%. Also, we were able to remove rotational and skew misalignments of 0.30 in a real image. Overall, SPIM highlights the potential of combining spatial analysis with physics-driven models in practical applications.

</details>
