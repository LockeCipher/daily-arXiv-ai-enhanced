{"id": "2506.17450", "pdf": "https://arxiv.org/pdf/2506.17450", "abs": "https://arxiv.org/abs/2506.17450", "authors": ["Jiacheng Chen", "Ramin Mehran", "Xuhui Jia", "Saining Xie", "Sanghyun Woo"], "title": "BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://blenderfusion.github.io", "summary": "We present BlenderFusion, a generative visual compositing framework that synthesizes new scenes by recomposing objects, camera, and background. It follows a layering-editing-compositing pipeline: (i) segmenting and converting visual inputs into editable 3D entities (layering), (ii) editing them in Blender with 3D-grounded control (editing), and (iii) fusing them into a coherent scene using a generative compositor (compositing). Our generative compositor extends a pre-trained diffusion model to process both the original (source) and edited (target) scenes in parallel. It is fine-tuned on video frames with two key training strategies: (i) source masking, enabling flexible modifications like background replacement; (ii) simulated object jittering, facilitating disentangled control over objects and camera. BlenderFusion significantly outperforms prior methods in complex compositional scene editing tasks.", "AI": {"tldr": "BlenderFusion\u662f\u4e00\u4e2a\u751f\u6210\u5f0f\u89c6\u89c9\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u65b0\u7ec4\u5408\u7269\u4f53\u3001\u76f8\u673a\u548c\u80cc\u666f\u6765\u5408\u6210\u65b0\u573a\u666f\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u590d\u6742\u573a\u666f\u7f16\u8f91\u4efb\u52a1\u4e2d\u7684\u7075\u6d3b\u6027\u548c\u63a7\u5236\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42-\u7f16\u8f91-\u5408\u6210\u6d41\u7a0b\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u548c\u4e24\u79cd\u5173\u952e\u8bad\u7ec3\u7b56\u7565\uff08\u6e90\u63a9\u7801\u548c\u6a21\u62df\u7269\u4f53\u6296\u52a8\uff09\u3002", "result": "\u5728\u590d\u6742\u573a\u666f\u7f16\u8f91\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "BlenderFusion\u4e3a\u751f\u6210\u5f0f\u573a\u666f\u7f16\u8f91\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u5de5\u5177\u3002"}}
{"id": "2506.17636", "pdf": "https://arxiv.org/pdf/2506.17636", "abs": "https://arxiv.org/abs/2506.17636", "authors": ["Shihan Chen", "Zhaojin Li", "Zeyu Chen", "Qingsong Yan", "Gaoyang Shen", "Ran Duan"], "title": "3D Gaussian Splatting for Fine-Detailed Surface Reconstruction in Large-Scale Scene", "categories": ["cs.GR", "cs.CV", "eess.IV"], "comment": "IROS 2025", "summary": "Recent developments in 3D Gaussian Splatting have made significant advances in surface reconstruction. However, scaling these methods to large-scale scenes remains challenging due to high computational demands and the complex dynamic appearances typical of outdoor environments. These challenges hinder the application in aerial surveying and autonomous driving. This paper proposes a novel solution to reconstruct large-scale surfaces with fine details, supervised by full-sized images. Firstly, we introduce a coarse-to-fine strategy to reconstruct a coarse model efficiently, followed by adaptive scene partitioning and sub-scene refining from image segments. Additionally, we integrate a decoupling appearance model to capture global appearance variations and a transient mask model to mitigate interference from moving objects. Finally, we expand the multi-view constraint and introduce a single-view regularization for texture-less areas. Our experiments were conducted on the publicly available dataset GauU-Scene V2, which was captured using unmanned aerial vehicles. To the best of our knowledge, our method outperforms existing NeRF-based and Gaussian-based methods, achieving high-fidelity visual results and accurate surface from full-size image optimization. Open-source code will be available on GitHub.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5927\u89c4\u6a21\u573a\u666f\u8868\u9762\u91cd\u5efa\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u7b56\u7565\u3001\u81ea\u9002\u5e94\u5206\u533a\u548c\u5916\u89c2\u89e3\u8026\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u8ba1\u7b97\u590d\u6742\u6027\u548c\u52a8\u6001\u5916\u89c2\u95ee\u9898\u3002", "motivation": "\u73b0\u67093D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u9762\u4e34\u8ba1\u7b97\u590d\u6742\u6027\u548c\u52a8\u6001\u5916\u89c2\u7684\u6311\u6218\uff0c\u9650\u5236\u4e86\u5176\u5728\u822a\u6d4b\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u7c97\u5230\u7ec6\u7b56\u7565\u3001\u81ea\u9002\u5e94\u573a\u666f\u5206\u533a\u3001\u5916\u89c2\u89e3\u8026\u6a21\u578b\u548c\u77ac\u6001\u63a9\u6a21\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u89c6\u89d2\u7ea6\u675f\u548c\u5355\u89c6\u89d2\u6b63\u5219\u5316\u3002", "result": "\u5728GauU-Scene V2\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709NeRF\u548c\u9ad8\u65af\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u89c6\u89c9\u6548\u679c\u548c\u7cbe\u786e\u8868\u9762\u91cd\u5efa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u7684\u6311\u6218\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2506.17237", "pdf": "https://arxiv.org/pdf/2506.17237", "abs": "https://arxiv.org/abs/2506.17237", "authors": ["Dip Roy"], "title": "Mechanistic Interpretability of Diffusion Models: Circuit-Level Analysis and Causal Validation", "categories": ["cs.CV"], "comment": null, "summary": "We present a quantitative circuit-level analysis of diffusion models, establishing computational pathways and mechanistic principles underlying image generation processes. Through systematic intervention experiments across 2,000 synthetic and 2,000 CelebA facial images, we discover fundamental algorithmic differences in how diffusion architectures process synthetic versus naturalistic data distributions. Our investigation reveals that real-world face processing requires circuits with measurably higher computational complexity (complexity ratio = 1.084 plus/minus 0.008, p < 0.001), exhibiting distinct attention specialization patterns with entropy divergence ranging from 0.015 to 0.166 across denoising timesteps. We identify eight functionally distinct attention mechanisms showing specialized computational roles: edge detection (entropy = 3.18 plus/minus 0.12), texture analysis (entropy = 4.16 plus/minus 0.08), and semantic understanding (entropy = 2.67 plus/minus 0.15). Intervention analysis demonstrates critical computational bottlenecks where targeted ablations produce 25.6% to 128.3% performance degradation, providing causal evidence for identified circuit functions. These findings establish quantitative foundations for algorithmic understanding and control of generative model behavior through mechanistic intervention strategies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u91cf\u5316\u7535\u8def\u5206\u6790\u6269\u6563\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u5408\u6210\u4e0e\u81ea\u7136\u6570\u636e\u5904\u7406\u7684\u7b97\u6cd5\u5dee\u5f02\uff0c\u5e76\u8bc6\u522b\u4e86\u516b\u79cd\u529f\u80fd\u4e0d\u540c\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002", "motivation": "\u7814\u7a76\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u8ba1\u7b97\u8def\u5f84\u548c\u673a\u5236\u539f\u7406\uff0c\u4ee5\u7406\u89e3\u5176\u5bf9\u5408\u6210\u4e0e\u81ea\u7136\u6570\u636e\u7684\u4e0d\u540c\u5904\u7406\u65b9\u5f0f\u3002", "method": "\u901a\u8fc7\u5bf92,000\u5f20\u5408\u6210\u56fe\u50cf\u548c2,000\u5f20CelebA\u4eba\u8138\u56fe\u50cf\u8fdb\u884c\u7cfb\u7edf\u6027\u5e72\u9884\u5b9e\u9a8c\uff0c\u5206\u6790\u8ba1\u7b97\u590d\u6742\u6027\u548c\u6ce8\u610f\u529b\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0\u771f\u5b9e\u4eba\u8138\u5904\u7406\u9700\u8981\u66f4\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\uff081.084\u00b10.008\uff09\uff0c\u5e76\u8bc6\u522b\u4e86\u516b\u79cd\u529f\u80fd\u4e0d\u540c\u7684\u6ce8\u610f\u529b\u673a\u5236\uff08\u5982\u8fb9\u7f18\u68c0\u6d4b\u3001\u7eb9\u7406\u5206\u6790\u7b49\uff09\u3002", "conclusion": "\u7814\u7a76\u4e3a\u751f\u6210\u6a21\u578b\u884c\u4e3a\u7684\u7b97\u6cd5\u7406\u89e3\u548c\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9a\u91cf\u57fa\u7840\uff0c\u652f\u6301\u901a\u8fc7\u673a\u5236\u5e72\u9884\u7b56\u7565\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2506.18251", "pdf": "https://arxiv.org/pdf/2506.18251", "abs": "https://arxiv.org/abs/2506.18251", "authors": ["Chao Li", "Jiawei Fan", "Anbang Yao"], "title": "Morse: Dual-Sampling for Lossless Acceleration of Diffusion Models", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "This work is accepted to ICML 2025. The project page:   https://github.com/deep-optimization/Morse", "summary": "In this paper, we present Morse, a simple dual-sampling framework for accelerating diffusion models losslessly. The key insight of Morse is to reformulate the iterative generation (from noise to data) process via taking advantage of fast jump sampling and adaptive residual feedback strategies. Specifically, Morse involves two models called Dash and Dot that interact with each other. The Dash model is just the pre-trained diffusion model of any type, but operates in a jump sampling regime, creating sufficient space for sampling efficiency improvement. The Dot model is significantly faster than the Dash model, which is learnt to generate residual feedback conditioned on the observations at the current jump sampling point on the trajectory of the Dash model, lifting the noise estimate to easily match the next-step estimate of the Dash model without jump sampling. By chaining the outputs of the Dash and Dot models run in a time-interleaved fashion, Morse exhibits the merit of flexibly attaining desired image generation performance while improving overall runtime efficiency. With our proposed weight sharing strategy between the Dash and Dot models, Morse is efficient for training and inference. Our method shows a lossless speedup of 1.78X to 3.31X on average over a wide range of sampling step budgets relative to 9 baseline diffusion models on 6 image generation tasks. Furthermore, we show that our method can be also generalized to improve the Latent Consistency Model (LCM-SDXL, which is already accelerated with consistency distillation technique) tailored for few-step text-to-image synthesis. The code and models are available at https://github.com/deep-optimization/Morse.", "AI": {"tldr": "Morse\u662f\u4e00\u4e2a\u65e0\u635f\u52a0\u901f\u6269\u6563\u6a21\u578b\u7684\u53cc\u91c7\u6837\u6846\u67b6\uff0c\u901a\u8fc7\u5feb\u901f\u8df3\u8dc3\u91c7\u6837\u548c\u81ea\u9002\u5e94\u6b8b\u5dee\u53cd\u9988\u7b56\u7565\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u901a\u5e38\u8017\u65f6\u8f83\u957f\uff0cMorse\u65e8\u5728\u65e0\u635f\u52a0\u901f\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "method": "Morse\u5305\u542bDash\u548cDot\u4e24\u4e2a\u6a21\u578b\uff1aDash\u4e3a\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u91c7\u7528\u8df3\u8dc3\u91c7\u6837\uff1bDot\u751f\u6210\u6b8b\u5dee\u53cd\u9988\uff0c\u4e0eDash\u4ea4\u66ff\u8fd0\u884c\u4ee5\u63d0\u5347\u6548\u7387\u3002", "result": "\u57286\u4e2a\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0cMorse\u5b9e\u73b0\u4e861.78X\u52303.31X\u7684\u65e0\u635f\u52a0\u901f\uff0c\u4e14\u9002\u7528\u4e8eLatent Consistency Model\u3002", "conclusion": "Morse\u901a\u8fc7\u53cc\u91c7\u6837\u6846\u67b6\u548c\u6743\u91cd\u5171\u4eab\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u8fd0\u884c\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2506.18407", "pdf": "https://arxiv.org/pdf/2506.18407", "abs": "https://arxiv.org/abs/2506.18407", "authors": ["Yiyao Wang", "Bo Pan", "Ke Wang", "Han Liu", "Jinyuan Mao", "Yuxin Liu", "Minfeng Zhu", "Bo Zhang", "Weifeng Chen", "Xiuqi Huang", "Wei Chen"], "title": "What You Think Is What You Get: Bridge User Intent and Transfer Function Design through Multimodal Large Language Models", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Direct volume rendering (DVR) is a fundamental technique for visualizing volumetric data, with transfer functions (TFs) playing a crucial role in extracting meaningful structures. However, designing effective TFs remains unintuitive due to the semantic gap between user intent and TF parameter space. Researchers have developed numerous TF optimization methods to bridge this gap. However, existing methods still face two challenges: large exploration space and weak generalizability. To address these issues, we propose What You Think is What You Get (WYTWYG) framework, which leveraging Multi-model Large Language Models (MLLMs) to guide the TF optimization based on user intent. Specifically, we first introduce a novel TF optimization approach comprising two core components: (1) an evolution-based explorer for effective exploration of the TF space, and (2) a volume rendering quality evaluator based on MLLMs to provide generalizable visual guidance. We further propose a TF interactive design system based on this approach. We demonstrate the general applicability of our framework through three case studies, and validate the effectiveness of each component through extensive experiments. Our code is available at: https://github.com/wyysteelhead/TFevolve.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWYTWYG\u7684\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u57fa\u4e8e\u7528\u6237\u610f\u56fe\u6307\u5bfc\u4f20\u9012\u51fd\u6570\uff08TF\uff09\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u63a2\u7d22\u7a7a\u95f4\u5927\u548c\u6cdb\u5316\u80fd\u529b\u5f31\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u9012\u51fd\u6570\uff08TFs\uff09\u5728\u76f4\u63a5\u4f53\u79ef\u6e32\u67d3\uff08DVR\uff09\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8bbe\u8ba1\u6709\u6548\u7684TFs\u4ecd\u4e0d\u76f4\u89c2\uff0c\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u9762\u4e34\u63a2\u7d22\u7a7a\u95f4\u5927\u548c\u6cdb\u5316\u80fd\u529b\u5f31\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faWYTWYG\u6846\u67b6\uff0c\u5305\u542b\u57fa\u4e8e\u8fdb\u5316\u7684\u63a2\u7d22\u5668\u548c\u57fa\u4e8eMLLMs\u7684\u4f53\u79ef\u6e32\u67d3\u8d28\u91cf\u8bc4\u4f30\u5668\uff0c\u5e76\u5f00\u53d1\u4e86\u4ea4\u4e92\u5f0fTF\u8bbe\u8ba1\u7cfb\u7edf\u3002", "result": "\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u6846\u67b6\u7684\u901a\u7528\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u6709\u6548\u6027\u3002", "conclusion": "WYTWYG\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86TF\u4f18\u5316\u4e2d\u7684\u63a2\u7d22\u7a7a\u95f4\u548c\u6cdb\u5316\u95ee\u9898\uff0c\u4e3aDVR\u63d0\u4f9b\u4e86\u66f4\u76f4\u89c2\u7684\u8bbe\u8ba1\u5de5\u5177\u3002"}}
{"id": "2506.18601", "pdf": "https://arxiv.org/pdf/2506.18601", "abs": "https://arxiv.org/abs/2506.18601", "authors": ["Denys Rozumnyi", "Jonathon Luiten", "Numair Khan", "Johannes Sch\u00f6nberger", "Peter Kontschieder"], "title": "BulletGen: Improving 4D Reconstruction with Bullet-Time Generation", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Transforming casually captured, monocular videos into fully immersive dynamic experiences is a highly ill-posed task, and comes with significant challenges, e.g., reconstructing unseen regions, and dealing with the ambiguity in monocular depth estimation. In this work we introduce BulletGen, an approach that takes advantage of generative models to correct errors and complete missing information in a Gaussian-based dynamic scene representation. This is done by aligning the output of a diffusion-based video generation model with the 4D reconstruction at a single frozen \"bullet-time\" step. The generated frames are then used to supervise the optimization of the 4D Gaussian model. Our method seamlessly blends generative content with both static and dynamic scene components, achieving state-of-the-art results on both novel-view synthesis, and 2D/3D tracking tasks.", "AI": {"tldr": "BulletGen\u5229\u7528\u751f\u6210\u6a21\u578b\u7ea0\u6b63\u9ad8\u65af\u52a8\u6001\u573a\u666f\u8868\u793a\u4e2d\u7684\u9519\u8bef\u5e76\u8865\u5168\u7f3a\u5931\u4fe1\u606f\uff0c\u901a\u8fc7\u6269\u6563\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e0e4D\u91cd\u5efa\u5bf9\u9f50\uff0c\u5b9e\u73b0\u6c89\u6d78\u5f0f\u52a8\u6001\u4f53\u9a8c\u3002", "motivation": "\u89e3\u51b3\u4ece\u5355\u76ee\u89c6\u9891\u91cd\u5efa\u52a8\u6001\u573a\u666f\u65f6\u7684\u672a\u89c2\u6d4b\u533a\u57df\u91cd\u5efa\u548c\u6df1\u5ea6\u4f30\u8ba1\u6a21\u7cca\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u751f\u6210\u6a21\u578b\u4e0e\u9ad8\u65af\u52a8\u6001\u573a\u666f\u8868\u793a\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u5e27\u76d1\u77634D\u9ad8\u65af\u6a21\u578b\u4f18\u5316\u3002", "result": "\u5728\u65b0\u89c6\u89d2\u5408\u6210\u548c2D/3D\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "BulletGen\u6210\u529f\u878d\u5408\u751f\u6210\u5185\u5bb9\u4e0e\u9759\u6001/\u52a8\u6001\u573a\u666f\u7ec4\u4ef6\uff0c\u63d0\u5347\u52a8\u6001\u573a\u666f\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2506.18680", "pdf": "https://arxiv.org/pdf/2506.18680", "abs": "https://arxiv.org/abs/2506.18680", "authors": ["Anindita Ghosh", "Bing Zhou", "Rishabh Dabral", "Jian Wang", "Vladislav Golyanik", "Christian Theobalt", "Philipp Slusallek", "Chuan Guo"], "title": "DuetGen: Music Driven Two-Person Dance Generation via Hierarchical Masked Modeling", "categories": ["cs.GR", "cs.CV", "cs.SD", "eess.AS"], "comment": "11 pages, 7 figures, 2 tables, accepted in ACM Siggraph 2025   conference track", "summary": "We present DuetGen, a novel framework for generating interactive two-person dances from music. The key challenge of this task lies in the inherent complexities of two-person dance interactions, where the partners need to synchronize both with each other and with the music. Inspired by the recent advances in motion synthesis, we propose a two-stage solution: encoding two-person motions into discrete tokens and then generating these tokens from music. To effectively capture intricate interactions, we represent both dancers' motions as a unified whole to learn the necessary motion tokens, and adopt a coarse-to-fine learning strategy in both the stages. Our first stage utilizes a VQ-VAE that hierarchically separates high-level semantic features at a coarse temporal resolution from low-level details at a finer resolution, producing two discrete token sequences at different abstraction levels. Subsequently, in the second stage, two generative masked transformers learn to map music signals to these dance tokens: the first producing high-level semantic tokens, and the second, conditioned on music and these semantic tokens, producing the low-level tokens. We train both transformers to learn to predict randomly masked tokens within the sequence, enabling them to iteratively generate motion tokens by filling an empty token sequence during inference. Through the hierarchical masked modeling and dedicated interaction representation, DuetGen achieves the generation of synchronized and interactive two-person dances across various genres. Extensive experiments and user studies on a benchmark duet dance dataset demonstrate state-of-the-art performance of DuetGen in motion realism, music-dance alignment, and partner coordination.", "AI": {"tldr": "DuetGen\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u97f3\u4e50\u751f\u6210\u4ea4\u4e92\u5f0f\u53cc\u4eba\u821e\u8e48\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\uff08\u7f16\u7801\u548c\u751f\u6210\uff09\u548c\u5206\u5c42\u63a9\u7801\u5efa\u6a21\u5b9e\u73b0\u540c\u6b65\u548c\u4e92\u52a8\u3002", "motivation": "\u53cc\u4eba\u821e\u8e48\u7684\u540c\u6b65\u548c\u4e92\u52a8\u590d\u6742\u6027\u662f\u4e3b\u8981\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u821e\u4f34\u4e4b\u95f4\u4ee5\u53ca\u4e0e\u97f3\u4e50\u7684\u534f\u8c03\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u4f7f\u7528VQ-VAE\u5c06\u53cc\u4eba\u52a8\u4f5c\u7f16\u7801\u4e3a\u79bb\u6563\u6807\u8bb0\uff1b2) \u4f7f\u7528\u63a9\u7801\u53d8\u6362\u5668\u4ece\u97f3\u4e50\u751f\u6210\u8fd9\u4e9b\u6807\u8bb0\uff0c\u5206\u5c42\u5904\u7406\u9ad8\u3001\u4f4e\u7ea7\u522b\u7279\u5f81\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cDuetGen\u5728\u52a8\u4f5c\u771f\u5b9e\u6027\u3001\u97f3\u4e50\u821e\u8e48\u5bf9\u9f50\u548c\u821e\u4f34\u534f\u8c03\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DuetGen\u901a\u8fc7\u5206\u5c42\u5efa\u6a21\u548c\u4ea4\u4e92\u8868\u793a\uff0c\u6210\u529f\u751f\u6210\u4e86\u540c\u6b65\u4e14\u4e92\u52a8\u7684\u53cc\u4eba\u821e\u8e48\uff0c\u6027\u80fd\u9886\u5148\u3002"}}
{"id": "2506.17361", "pdf": "https://arxiv.org/pdf/2506.17361", "abs": "https://arxiv.org/abs/2506.17361", "authors": ["Xufei Wang", "Mingjian Zhang", "Fei Ge", "Jinchen Zhu", "Wen Sha", "Jifen Ren", "Zhimeng Hou", "Shouguo Zheng", "ling Zheng", "Shizhuang Weng"], "title": "Efficient Feedback Gate Network for Hyperspectral Image Super-Resolution", "categories": ["cs.CV", "cs.LG"], "comment": "20 pages,17 figures", "summary": "Even without auxiliary images, single hyperspectral image super-resolution (SHSR) methods can be designed to improve the spatial resolution of hyperspectral images. However, failing to explore coherence thoroughly along bands and spatial-spectral information leads to the limited performance of the SHSR. In this study, we propose a novel group-based SHSR method termed the efficient feedback gate network, which uses various feedbacks and gate operations involving large kernel convolutions and spectral interactions. In particular, by providing different guidance for neighboring groups, we can learn rich band information and hierarchical hyperspectral spatial information using channel shuffling and dilatation convolution in shuffled and progressive dilated fusion module(SPDFM). Moreover, we develop a wide-bound perception gate block and a spectrum enhancement gate block to construct the spatial-spectral reinforcement gate module (SSRGM) and obtain highly representative spatial-spectral features efficiently. Additionally, we apply a three-dimensional SSRGM to enhance holistic information and coherence for hyperspectral data. The experimental results on three hyperspectral datasets demonstrate the superior performance of the proposed network over the state-of-the-art methods in terms of spectral fidelity and spatial content reconstruction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u7ec4\u7684\u9ad8\u6548\u53cd\u9988\u95e8\u7f51\u7edc\uff08EFGN\uff09\uff0c\u7528\u4e8e\u5355\u5e45\u9ad8\u5149\u8c31\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08SHSR\uff09\uff0c\u901a\u8fc7\u53cd\u9988\u95e8\u64cd\u4f5c\u548c\u5927\u6838\u5377\u79ef\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709SHSR\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u6ce2\u6bb5\u95f4\u548c\u7a7a\u95f4-\u5149\u8c31\u4fe1\u606f\u7684\u8fde\u8d2f\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002", "method": "\u63d0\u51faEFGN\uff0c\u7ed3\u5408\u53cd\u9988\u95e8\u64cd\u4f5c\u3001\u5927\u6838\u5377\u79ef\u548c\u5149\u8c31\u4ea4\u4e92\uff1b\u5f15\u5165SPDFM\u6a21\u5757\u5b66\u4e60\u6ce2\u6bb5\u4fe1\u606f\u548c\u5c42\u6b21\u7a7a\u95f4\u4fe1\u606f\uff1b\u5f00\u53d1SSRGM\u6a21\u5757\u589e\u5f3a\u7a7a\u95f4-\u5149\u8c31\u7279\u5f81\u3002", "result": "\u5728\u4e09\u4e2a\u9ad8\u5149\u8c31\u6570\u636e\u96c6\u4e0a\uff0cEFGN\u5728\u5149\u8c31\u4fdd\u771f\u5ea6\u548c\u7a7a\u95f4\u5185\u5bb9\u91cd\u5efa\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "EFGN\u901a\u8fc7\u9ad8\u6548\u5229\u7528\u6ce2\u6bb5\u548c\u7a7a\u95f4-\u5149\u8c31\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86SHSR\u7684\u6027\u80fd\u3002"}}
{"id": "2506.18671", "pdf": "https://arxiv.org/pdf/2506.18671", "abs": "https://arxiv.org/abs/2506.18671", "authors": ["Yuqin Dai", "Wanlu Zhu", "Ronghui Li", "Xiu Li", "Zhenyu Zhang", "Jun Li", "Jian Yang"], "title": "TCDiff++: An End-to-end Trajectory-Controllable Diffusion Model for Harmonious Music-Driven Group Choreography", "categories": ["cs.SD", "cs.CV", "cs.GR", "eess.AS"], "comment": null, "summary": "Music-driven dance generation has garnered significant attention due to its wide range of industrial applications, particularly in the creation of group choreography. During the group dance generation process, however, most existing methods still face three primary issues: multi-dancer collisions, single-dancer foot sliding and abrupt swapping in the generation of long group dance. In this paper, we propose TCDiff++, a music-driven end-to-end framework designed to generate harmonious group dance. Specifically, to mitigate multi-dancer collisions, we utilize a dancer positioning embedding to better maintain the relative positioning among dancers. Additionally, we incorporate a distance-consistency loss to ensure that inter-dancer distances remain within plausible ranges. To address the issue of single-dancer foot sliding, we introduce a swap mode embedding to indicate dancer swapping patterns and design a Footwork Adaptor to refine raw motion, thereby minimizing foot sliding. For long group dance generation, we present a long group diffusion sampling strategy that reduces abrupt position shifts by injecting positional information into the noisy input. Furthermore, we integrate a Sequence Decoder layer to enhance the model's ability to selectively process long sequences. Extensive experiments demonstrate that our TCDiff++ achieves state-of-the-art performance, particularly in long-duration scenarios, ensuring high-quality and coherent group dance generation.", "AI": {"tldr": "TCDiff++\u662f\u4e00\u4e2a\u97f3\u4e50\u9a71\u52a8\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u7fa4\u821e\u751f\u6210\u4e2d\u7684\u78b0\u649e\u3001\u811a\u6ed1\u548c\u4f4d\u7f6e\u7a81\u53d8\u95ee\u9898\uff0c\u901a\u8fc7\u5d4c\u5165\u548c\u635f\u5931\u51fd\u6570\u4f18\u5316\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7fa4\u821e\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7fa4\u821e\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u591a\u821e\u8005\u78b0\u649e\u3001\u5355\u821e\u8005\u811a\u6ed1\u548c\u957f\u5e8f\u5217\u4f4d\u7f6e\u7a81\u53d8\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "method": "\u63d0\u51faTCDiff++\u6846\u67b6\uff0c\u5305\u62ec\u821e\u8005\u5b9a\u4f4d\u5d4c\u5165\u3001\u8ddd\u79bb\u4e00\u81f4\u6027\u635f\u5931\u3001\u4ea4\u6362\u6a21\u5f0f\u5d4c\u5165\u3001\u6b65\u6cd5\u9002\u914d\u5668\u548c\u957f\u5e8f\u5217\u6269\u6563\u91c7\u6837\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTCDiff++\u5728\u957f\u5e8f\u5217\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u8fde\u8d2f\u7684\u7fa4\u821e\u3002", "conclusion": "TCDiff++\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7fa4\u821e\u751f\u6210\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u751f\u6210\u6548\u679c\u3002"}}
{"id": "2506.17608", "pdf": "https://arxiv.org/pdf/2506.17608", "abs": "https://arxiv.org/abs/2506.17608", "authors": ["Nikitha SR", "Aradhya Neeraj Mathur", "Tarun Ram Menta", "Rishabh Jain", "Mausoom Sarkar"], "title": "HIRE: Lightweight High-Resolution Image Feature Enrichment for Multimodal LLMs", "categories": ["cs.CV"], "comment": "Accepted in CVPR 2025 Workshop on What's Next in Multimodal   Foundational Models", "summary": "The integration of high-resolution image features in modern multimodal large language models has demonstrated significant improvements in fine-grained visual understanding tasks, achieving high performance across multiple benchmarks. Since these features are obtained from large image encoders like ViT, they come with a significant increase in computational costs due to multiple calls to these encoders. In this work, we first develop an intuition for feature upsampling as a natural extension of high-resolution feature generation. Through extensive experiments and ablations, we demonstrate how a shallow feature enricher can achieve competitive results with tremendous reductions in training and inference time as well as computational cost, with upto 1.5x saving in FLOPs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6d45\u5c42\u7279\u5f81\u589e\u5f3a\u5668\uff0c\u901a\u8fc7\u7279\u5f81\u4e0a\u91c7\u6837\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\u751f\u6210\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7279\u5f81\u5728\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9700\u591a\u6b21\u8c03\u7528\u5927\u578b\u56fe\u50cf\u7f16\u7801\u5668\u3002", "method": "\u63d0\u51fa\u6d45\u5c42\u7279\u5f81\u589e\u5f3a\u5668\uff0c\u901a\u8fc7\u7279\u5f81\u4e0a\u91c7\u6837\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\uff0c\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u4e0a\u5927\u5e45\u51cf\u5c11\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e1.5\u500dFLOPs\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u6d45\u5c42\u7279\u5f81\u589e\u5f3a\u5668\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\u751f\u6210\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2506.17705", "pdf": "https://arxiv.org/pdf/2506.17705", "abs": "https://arxiv.org/abs/2506.17705", "authors": ["Bo Pan", "Yang Chen", "Yingwei Pan", "Ting Yao", "Wei Chen", "Tao Mei"], "title": "DreamJourney: Perpetual View Generation with Video Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Perpetual view generation aims to synthesize a long-term video corresponding to an arbitrary camera trajectory solely from a single input image. Recent methods commonly utilize a pre-trained text-to-image diffusion model to synthesize new content of previously unseen regions along camera movement. However, the underlying 2D diffusion model lacks 3D awareness and results in distorted artifacts. Moreover, they are limited to generating views of static 3D scenes, neglecting to capture object movements within the dynamic 4D world. To alleviate these issues, we present DreamJourney, a two-stage framework that leverages the world simulation capacity of video diffusion models to trigger a new perpetual scene view generation task with both camera movements and object dynamics. Specifically, in stage I, DreamJourney first lifts the input image to 3D point cloud and renders a sequence of partial images from a specific camera trajectory. A video diffusion model is then utilized as generative prior to complete the missing regions and enhance visual coherence across the sequence, producing a cross-view consistent video adheres to the 3D scene and camera trajectory. Meanwhile, we introduce two simple yet effective strategies (early stopping and view padding) to further stabilize the generation process and improve visual quality. Next, in stage II, DreamJourney leverages a multimodal large language model to produce a text prompt describing object movements in current view, and uses video diffusion model to animate current view with object movements. Stage I and II are repeated recurrently, enabling perpetual dynamic scene view generation. Extensive experiments demonstrate the superiority of our DreamJourney over state-of-the-art methods both quantitatively and qualitatively. Our project page: https://dream-journey.vercel.app.", "AI": {"tldr": "DreamJourney\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5229\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u52a8\u6001\u573a\u666f\u7684\u957f\u671f\u89c6\u56fe\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u57283D\u611f\u77e5\u548c\u52a8\u6001\u5bf9\u8c61\u8fd0\u52a8\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u57fa\u4e8e2D\u6269\u6563\u6a21\u578b\uff0c\u7f3a\u4e4f3D\u611f\u77e5\u80fd\u529b\uff0c\u4e14\u65e0\u6cd5\u6355\u6349\u52a8\u60014D\u4e16\u754c\u4e2d\u7684\u5bf9\u8c61\u8fd0\u52a8\uff0c\u5bfc\u81f4\u751f\u6210\u89c6\u56fe\u5931\u771f\u4e14\u9759\u6001\u3002", "method": "DreamJourney\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc73D\u70b9\u4e91\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u4e00\u81f4\u7684\u8de8\u89c6\u56fe\u89c6\u9891\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63cf\u8ff0\u5bf9\u8c61\u8fd0\u52a8\u5e76\u52a8\u6001\u751f\u6210\u89c6\u56fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDreamJourney\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DreamJourney\u901a\u8fc7\u7ed3\u54083D\u611f\u77e5\u548c\u52a8\u6001\u5bf9\u8c61\u8fd0\u52a8\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u957f\u671f\u52a8\u6001\u573a\u666f\u89c6\u56fe\u751f\u6210\u3002"}}
{"id": "2506.17707", "pdf": "https://arxiv.org/pdf/2506.17707", "abs": "https://arxiv.org/abs/2506.17707", "authors": ["Jihyun Kim", "Junho Park", "Kyeongbo Kong", "Suk-Ju Kang"], "title": "Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered by Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Accepted by IEEE Transactions on Multimedia", "summary": "We present Programmable-Room, a framework which interactively generates and edits a 3D room mesh, given natural language instructions. For precise control of a room's each attribute, we decompose the challenging task into simpler steps such as creating plausible 3D coordinates for room meshes, generating panorama images for the texture, constructing 3D meshes by integrating the coordinates and panorama texture images, and arranging furniture. To support the various decomposed tasks with a unified framework, we incorporate visual programming (VP). VP is a method that utilizes a large language model (LLM) to write a Python-like program which is an ordered list of necessary modules for the various tasks given in natural language. We develop most of the modules. Especially, for the texture generating module, we utilize a pretrained large-scale diffusion model to generate panorama images conditioned on text and visual prompts (i.e., layout, depth, and semantic map) simultaneously. Specifically, we enhance the panorama image generation quality by optimizing the training objective with a 1D representation of a panorama scene obtained from bidirectional LSTM. We demonstrate Programmable-Room's flexibility in generating and editing 3D room meshes, and prove our framework's superiority to an existing model quantitatively and qualitatively. Project page is available in https://jihyun0510.github.io/Programmable_Room_Page/.", "AI": {"tldr": "Programmable-Room\u662f\u4e00\u4e2a\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4ea4\u4e92\u751f\u6210\u548c\u7f16\u8f913D\u623f\u95f4\u7f51\u683c\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u7f16\u7a0b\u548c\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u623f\u95f4\u751f\u6210\u548c\u7f16\u8f91\u3002", "motivation": "\u89e3\u51b3\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u7cbe\u786e\u63a7\u52363D\u623f\u95f4\u7f51\u683c\u751f\u6210\u548c\u7f16\u8f91\u7684\u6311\u6218\uff0c\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u591a\u4e2a\u7b80\u5355\u6b65\u9aa4\uff0c\u5e76\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u652f\u6301\u8fd9\u4e9b\u4efb\u52a1\u3002", "method": "\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u751f\u62103D\u5750\u6807\u3001\u5168\u666f\u56fe\u50cf\u7eb9\u7406\u3001\u6784\u5efa3D\u7f51\u683c\u548c\u5bb6\u5177\u5e03\u7f6e\uff0c\u5229\u7528\u89c6\u89c9\u7f16\u7a0b\uff08VP\uff09\u548c\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff08\u4f18\u5316\u8bad\u7ec3\u76ee\u6807\uff09\u5b9e\u73b0\u3002", "result": "\u5c55\u793a\u4e86\u6846\u67b6\u5728\u751f\u6210\u548c\u7f16\u8f913D\u623f\u95f4\u7f51\u683c\u65b9\u9762\u7684\u7075\u6d3b\u6027\uff0c\u5e76\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "Programmable-Room\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u4e3a3D\u623f\u95f4\u751f\u6210\u548c\u7f16\u8f91\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.17746", "pdf": "https://arxiv.org/pdf/2506.17746", "abs": "https://arxiv.org/abs/2506.17746", "authors": ["Sourabh Vasant Gothe", "Ayon Chattopadhyay", "Gunturi Venkata Sai Phani Kiran", "Pratik", "Vibhav Agarwal", "Jayesh Rajkumar Vachhani", "Sourav Ghosh", "Parameswaranath VM", "Barath Raj KR"], "title": "PhysID: Physics-based Interactive Dynamics from a Single-view Image", "categories": ["cs.CV"], "comment": "Published in 2025 IEEE International Conference on Acoustics, Speech   and Signal Processing (ICASSP). Project page: https://physid.github.io/", "summary": "Transforming static images into interactive experiences remains a challenging task in computer vision. Tackling this challenge holds the potential to elevate mobile user experiences, notably through interactive and AR/VR applications. Current approaches aim to achieve this either using pre-recorded video responses or requiring multi-view images as input. In this paper, we present PhysID, that streamlines the creation of physics-based interactive dynamics from a single-view image by leveraging large generative models for 3D mesh generation and physical property prediction. This significantly reduces the expertise required for engineering-intensive tasks like 3D modeling and intrinsic property calibration, enabling the process to be scaled with minimal manual intervention. We integrate an on-device physics-based engine for physically plausible real-time rendering with user interactions. PhysID represents a leap forward in mobile-based interactive dynamics, offering real-time, non-deterministic interactions and user-personalization with efficient on-device memory consumption. Experiments evaluate the zero-shot capabilities of various Multimodal Large Language Models (MLLMs) on diverse tasks and the performance of 3D reconstruction models. These results demonstrate the cohesive functioning of all modules within the end-to-end framework, contributing to its effectiveness.", "AI": {"tldr": "PhysID\u5229\u7528\u751f\u6210\u6a21\u578b\u4ece\u5355\u89c6\u89d2\u56fe\u50cf\u521b\u5efa\u7269\u7406\u4ea4\u4e92\u52a8\u6001\uff0c\u7b80\u53163D\u5efa\u6a21\u548c\u7269\u7406\u5c5e\u6027\u6821\u51c6\uff0c\u5b9e\u73b0\u5b9e\u65f6\u4ea4\u4e92\u548c\u4e2a\u6027\u5316\u3002", "motivation": "\u63d0\u5347\u79fb\u52a8\u7528\u6237\u4f53\u9a8c\uff0c\u901a\u8fc7\u4ea4\u4e92\u548cAR/VR\u5e94\u7528\uff0c\u89e3\u51b3\u9759\u6001\u56fe\u50cf\u8f6c\u5316\u4e3a\u4ea4\u4e92\u4f53\u9a8c\u7684\u6311\u6218\u3002", "method": "\u5229\u7528\u5927\u578b\u751f\u6210\u6a21\u578b\u8fdb\u884c3D\u7f51\u683c\u751f\u6210\u548c\u7269\u7406\u5c5e\u6027\u9884\u6d4b\uff0c\u7ed3\u5408\u8bbe\u5907\u7aef\u7269\u7406\u5f15\u64ce\u5b9e\u73b0\u5b9e\u65f6\u6e32\u67d3\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u80fd\u529b\u548c3D\u91cd\u5efa\u6a21\u578b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u7aef\u5230\u7aef\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "PhysID\u5728\u79fb\u52a8\u7aef\u4ea4\u4e92\u52a8\u6001\u9886\u57df\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u652f\u6301\u5b9e\u65f6\u3001\u975e\u786e\u5b9a\u6027\u4ea4\u4e92\u548c\u4f4e\u5185\u5b58\u6d88\u8017\u3002"}}
{"id": "2506.17759", "pdf": "https://arxiv.org/pdf/2506.17759", "abs": "https://arxiv.org/abs/2506.17759", "authors": ["Fadi Abdeladhim Zidi", "Djamel Eddine Boukhari", "Abdellah Zakaria Sellam", "Abdelkrim Ouafi", "Cosimo Distante", "Salah Eddine Bekhouche", "Abdelmalik Taleb-Ahmed"], "title": "LoLA-SpecViT: Local Attention SwiGLU Vision Transformer with LoRA for Hyperspectral Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral image classification remains a challenging task due to the high dimensionality of spectral data, significant inter-band redundancy, and the limited availability of annotated samples. While recent transformer-based models have improved the global modeling of spectral-spatial dependencies, their scalability and adaptability under label-scarce conditions remain limited. In this work, we propose \\textbf{LoLA-SpecViT}(Low-rank adaptation Local Attention Spectral Vision Transformer), a lightweight spectral vision transformer that addresses these limitations through a parameter-efficient architecture tailored to the unique characteristics of hyperspectral imagery. Our model combines a 3D convolutional spectral front-end with local window-based self-attention, enhancing both spectral feature extraction and spatial consistency while reducing computational complexity. To further improve adaptability, we integrate low-rank adaptation (LoRA) into attention and projection layers, enabling fine-tuning with over 80\\% fewer trainable parameters. A novel cyclical learning rate scheduler modulates LoRA adaptation strength during training, improving convergence and generalisation. Extensive experiments on three benchmark datasets WHU-Hi LongKou, WHU-Hi HongHu, and Salinas demonstrate that LoLA-SpecViT consistently outperforms state-of-the-art baselines, achieving up to 99.91\\% accuracy with substantially fewer parameters and enhanced robustness under low-label regimes. The proposed framework provides a scalable and generalizable solution for real-world HSI applications in agriculture, environmental monitoring, and remote sensing analytics. Our code is available in the following \\href{https://github.com/FadiZidiDz/LoLA-SpecViT}{GitHub Repository}.", "AI": {"tldr": "LoLA-SpecViT\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5149\u8c31\u89c6\u89c9\u53d8\u6362\u5668\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u67b6\u6784\u89e3\u51b3\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6311\u6218\uff0c\u7ed3\u54083D\u5377\u79ef\u548c\u5c40\u90e8\u81ea\u6ce8\u610f\u529b\uff0c\u663e\u8457\u51cf\u5c11\u53c2\u6570\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u9762\u4e34\u9ad8\u7ef4\u6570\u636e\u3001\u5197\u4f59\u548c\u6807\u6ce8\u6837\u672c\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u73b0\u6709\u53d8\u6362\u5668\u6a21\u578b\u5728\u6807\u7b7e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u7684\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51faLoLA-SpecViT\uff0c\u7ed3\u54083D\u5377\u79ef\u524d\u7aef\u548c\u5c40\u90e8\u81ea\u6ce8\u610f\u529b\uff0c\u96c6\u6210\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u4ee5\u51cf\u5c11\u53c2\u6570\uff0c\u5e76\u91c7\u7528\u5faa\u73af\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u4f18\u5316\u8bad\u7ec3\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6700\u9ad8\u8fbe99.91%\u51c6\u786e\u7387\uff0c\u53c2\u6570\u5927\u5e45\u51cf\u5c11\uff0c\u4e14\u5728\u4f4e\u6807\u7b7e\u6761\u4ef6\u4e0b\u9c81\u68d2\u6027\u66f4\u5f3a\u3002", "conclusion": "LoLA-SpecViT\u4e3a\u9ad8\u5149\u8c31\u56fe\u50cf\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u519c\u4e1a\u3001\u73af\u5883\u76d1\u6d4b\u7b49\u9886\u57df\u3002"}}
{"id": "2506.17896", "pdf": "https://arxiv.org/pdf/2506.17896", "abs": "https://arxiv.org/abs/2506.17896", "authors": ["Junho Park", "Andrew Sangwoo Ye", "Taein Kwon"], "title": "EgoWorld: Translating Exocentric View to Egocentric View using Rich Exocentric Observations", "categories": ["cs.CV", "cs.AI"], "comment": "Project Page: https://redorangeyellowy.github.io/EgoWorld/", "summary": "Egocentric vision is essential for both human and machine visual understanding, particularly in capturing the detailed hand-object interactions needed for manipulation tasks. Translating third-person views into first-person views significantly benefits augmented reality (AR), virtual reality (VR) and robotics applications. However, current exocentric-to-egocentric translation methods are limited by their dependence on 2D cues, synchronized multi-view settings, and unrealistic assumptions such as necessity of initial egocentric frame and relative camera poses during inference. To overcome these challenges, we introduce EgoWorld, a novel two-stage framework that reconstructs an egocentric view from rich exocentric observations, including projected point clouds, 3D hand poses, and textual descriptions. Our approach reconstructs a point cloud from estimated exocentric depth maps, reprojects it into the egocentric perspective, and then applies diffusion-based inpainting to produce dense, semantically coherent egocentric images. Evaluated on the H2O and TACO datasets, EgoWorld achieves state-of-the-art performance and demonstrates robust generalization to new objects, actions, scenes, and subjects. Moreover, EgoWorld shows promising results even on unlabeled real-world examples.", "AI": {"tldr": "EgoWorld\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u5916\u4e2d\u5fc3\u89c2\u6d4b\u91cd\u5efa\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d562D\u7ebf\u7d22\u548c\u591a\u89c6\u56fe\u540c\u6b65\u7684\u9650\u5236\u3002", "motivation": "\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u5bf9\u4e8eAR\u3001VR\u548c\u673a\u5668\u4eba\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d562D\u7ebf\u7d22\u548c\u540c\u6b65\u591a\u89c6\u56fe\u8bbe\u7f6e\uff0c\u4e14\u5047\u8bbe\u4e0d\u5207\u5b9e\u9645\u3002", "method": "EgoWorld\u901a\u8fc7\u5916\u4e2d\u5fc3\u6df1\u5ea6\u56fe\u91cd\u5efa\u70b9\u4e91\uff0c\u5c06\u5176\u91cd\u6295\u5f71\u5230\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\uff0c\u5e76\u5e94\u7528\u6269\u6563\u4fee\u590d\u751f\u6210\u5bc6\u96c6\u4e14\u8bed\u4e49\u4e00\u81f4\u7684\u56fe\u50cf\u3002", "result": "\u5728H2O\u548cTACO\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u9002\u7528\u4e8e\u65b0\u5bf9\u8c61\u3001\u52a8\u4f5c\u3001\u573a\u666f\u548c\u4e3b\u4f53\uff0c\u751a\u81f3\u5728\u672a\u6807\u8bb0\u7684\u771f\u5b9e\u4e16\u754c\u793a\u4f8b\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "EgoWorld\u901a\u8fc7\u521b\u65b0\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5916\u4e2d\u5fc3\u5230\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u8f6c\u6362\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.17975", "pdf": "https://arxiv.org/pdf/2506.17975", "abs": "https://arxiv.org/abs/2506.17975", "authors": ["Mischa Dombrowski", "Bernhard Kainz"], "title": "Enabling PSO-Secure Synthetic Data Sharing Using Diversity-Aware Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Synthetic data has recently reached a level of visual fidelity that makes it nearly indistinguishable from real data, offering great promise for privacy-preserving data sharing in medical imaging. However, fully synthetic datasets still suffer from significant limitations: First and foremost, the legal aspect of sharing synthetic data is often neglected and data regulations, such as the GDPR, are largley ignored. Secondly, synthetic models fall short of matching the performance of real data, even for in-domain downstream applications. Recent methods for image generation have focused on maximising image diversity instead of fidelity solely to improve the mode coverage and therefore the downstream performance of synthetic data. In this work, we shift perspective and highlight how maximizing diversity can also be interpreted as protecting natural persons from being singled out, which leads to predicate singling-out (PSO) secure synthetic datasets. Specifically, we propose a generalisable framework for training diffusion models on personal data which leads to unpersonal synthetic datasets achieving performance within one percentage point of real-data models while significantly outperforming state-of-the-art methods that do not ensure privacy. Our code is available at https://github.com/MischaD/Trichotomy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6700\u5927\u5316\u5408\u6210\u6570\u636e\u591a\u6837\u6027\u6765\u4fdd\u62a4\u9690\u79c1\u7684\u65b9\u6cd5\uff0c\u786e\u4fdd\u6570\u636e\u65e0\u6cd5\u88ab\u7528\u4e8e\u8bc6\u522b\u4e2a\u4eba\uff0c\u540c\u65f6\u6027\u80fd\u63a5\u8fd1\u771f\u5b9e\u6570\u636e\u3002", "motivation": "\u5f53\u524d\u5408\u6210\u6570\u636e\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u9690\u79c1\u4fdd\u62a4\u6f5c\u529b\u5927\uff0c\u4f46\u6cd5\u5f8b\u5408\u89c4\u6027\u548c\u6027\u80fd\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u901a\u7528\u6846\u67b6\uff0c\u751f\u6210\u975e\u4e2a\u6027\u5316\u5408\u6210\u6570\u636e\uff0c\u786e\u4fdd\u9690\u79c1\u3002", "result": "\u5408\u6210\u6570\u636e\u6027\u80fd\u63a5\u8fd1\u771f\u5b9e\u6570\u636e\uff08\u5dee\u8ddd1%\u4ee5\u5185\uff09\uff0c\u4f18\u4e8e\u73b0\u6709\u975e\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u3002", "conclusion": "\u6700\u5927\u5316\u591a\u6837\u6027\u4e0d\u4ec5\u63d0\u5347\u6027\u80fd\uff0c\u8fd8\u80fd\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\uff0c\u4e3a\u5408\u6210\u6570\u636e\u5171\u4eab\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2506.18095", "pdf": "https://arxiv.org/pdf/2506.18095", "abs": "https://arxiv.org/abs/2506.18095", "authors": ["Junying Chen", "Zhenyang Cai", "Pengcheng Chen", "Shunian Chen", "Ke Ji", "Xidong Wang", "Yunjin Yang", "Benyou Wang"], "title": "ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in multimodal generative models have unlocked photorealistic, instruction-aligned image generation, yet leading systems like GPT-4o-Image remain proprietary and inaccessible. To democratize these capabilities, we present ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and 46K text-and-image-to-image data, all synthesized using GPT-4o's image generation capabilities for distilling its advanced image generation abilities. Leveraging this dataset, we develop Janus-4o, a multimodal large language model capable of both text-to-image and text-and-image-to-image generation. Janus-4o not only significantly improves text-to-image generation over its predecessor, Janus-Pro, but also newly supports text-and-image-to-image generation. Notably, it achieves impressive performance in text-and-image-to-image generation from scratch, using only 91K synthetic samples and 6 hours of training on an 8 A800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will foster open research in photorealistic, instruction-aligned image generation.", "AI": {"tldr": "ShareGPT-4o-Image\u6570\u636e\u96c6\u548cJanus-4o\u6a21\u578b\u7684\u53d1\u5e03\u65e8\u5728\u63a8\u52a8\u5f00\u653e\u7814\u7a76\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u5230\u56fe\u50cf\u548c\u6587\u672c\u52a0\u56fe\u50cf\u5230\u56fe\u50cf\u7684\u751f\u6210\u3002", "motivation": "\u5f53\u524d\u9886\u5148\u7684\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\uff08\u5982GPT-4o-Image\uff09\u662f\u4e13\u6709\u7684\uff0c\u65e0\u6cd5\u516c\u5f00\u8bbf\u95ee\u3002\u4e3a\u4e86\u666e\u53ca\u8fd9\u4e9b\u80fd\u529b\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86ShareGPT-4o-Image\u6570\u636e\u96c6\u548cJanus-4o\u6a21\u578b\u3002", "method": "\u5229\u7528GPT-4o\u751f\u6210\u768445K\u6587\u672c\u5230\u56fe\u50cf\u548c46K\u6587\u672c\u52a0\u56fe\u50cf\u5230\u56fe\u50cf\u6570\u636e\uff0c\u8bad\u7ec3\u4e86Janus-4o\u6a21\u578b\uff0c\u652f\u6301\u4e24\u79cd\u751f\u6210\u4efb\u52a1\u3002", "result": "Janus-4o\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e0a\u663e\u8457\u4f18\u4e8e\u524d\u4ee3\u6a21\u578b\uff0c\u5e76\u65b0\u589e\u4e86\u6587\u672c\u52a0\u56fe\u50cf\u5230\u56fe\u50cf\u751f\u6210\u529f\u80fd\uff0c\u4ec5\u970091K\u5408\u6210\u6837\u672c\u548c6\u5c0f\u65f6\u8bad\u7ec3\u5373\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\u3002", "conclusion": "ShareGPT-4o-Image\u548cJanus-4o\u7684\u53d1\u5e03\u5c06\u4fc3\u8fdb\u5f00\u653e\u7814\u7a76\uff0c\u63a8\u52a8\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.18134", "pdf": "https://arxiv.org/pdf/2506.18134", "abs": "https://arxiv.org/abs/2506.18134", "authors": ["Quan Zhou", "Gan Luo", "Qiang Hu", "Qingyong Zhang", "Jinhua Zhang", "Yinjiao Tian", "Qiang Li", "Zhiwei Wang"], "title": "Targeted False Positive Synthesis via Detector-guided Adversarial Diffusion Attacker for Robust Polyp Detection", "categories": ["cs.CV"], "comment": "Early Accepted by MICCAI 2025", "summary": "Polyp detection is crucial for colorectal cancer screening, yet existing models are limited by the scale and diversity of available data. While generative models show promise for data augmentation, current methods mainly focus on enhancing polyp diversity, often overlooking the critical issue of false positives. In this paper, we address this gap by proposing an adversarial diffusion framework to synthesize high-value false positives. The extensive variability of negative backgrounds presents a significant challenge in false positive synthesis. To overcome this, we introduce two key innovations: First, we design a regional noise matching strategy to construct a negative synthesis space using polyp detection datasets. This strategy trains a negative-centric diffusion model by masking polyp regions, ensuring the model focuses exclusively on learning diverse background patterns. Second, we introduce the Detector-guided Adversarial Diffusion Attacker (DADA) module, which perturbs the negative synthesis process to disrupt a pre-trained detector's decision, guiding the negative-centric diffusion model to generate high-value, detector-confusing false positives instead of low-value, ordinary backgrounds. Our approach is the first to apply adversarial diffusion to lesion detection, establishing a new paradigm for targeted false positive synthesis and paving the way for more reliable clinical applications in colorectal cancer screening. Extensive results on public and in-house datasets verify the superiority of our method over the current state-of-the-arts, with our synthesized data improving the detectors by at least 2.6% and 2.7% in F1-score, respectively, over the baselines. Codes are at https://github.com/Huster-Hq/DADA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6297\u6027\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u5408\u6210\u9ad8\u4ef7\u503c\u7684\u5047\u9633\u6027\u6837\u672c\uff0c\u4ee5\u6539\u8fdb\u7ed3\u80a0\u606f\u8089\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u5047\u9633\u6027\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u751f\u6210\u6a21\u578b\u591a\u5173\u6ce8\u606f\u8089\u591a\u6837\u6027\u800c\u5ffd\u7565\u5047\u9633\u6027\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u533a\u57df\u566a\u58f0\u5339\u914d\u7b56\u7565\u548cDetector-guided Adversarial Diffusion Attacker (DADA)\u6a21\u5757\uff0c\u7528\u4e8e\u5408\u6210\u591a\u6837\u80cc\u666f\u548c\u751f\u6210\u9ad8\u4ef7\u503c\u5047\u9633\u6027\u6837\u672c\u3002", "result": "\u5728\u516c\u5f00\u548c\u5185\u90e8\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u5408\u6210\u6570\u636e\u4f7f\u68c0\u6d4b\u5668\u7684F1\u5206\u6570\u5206\u522b\u63d0\u9ad8\u4e86\u81f3\u5c112.6%\u548c2.7%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u75c5\u7076\u68c0\u6d4b\u4e2d\u7684\u5047\u9633\u6027\u5408\u6210\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u6709\u671b\u63d0\u5347\u7ed3\u80a0\u764c\u7b5b\u67e5\u7684\u4e34\u5e8a\u53ef\u9760\u6027\u3002"}}
{"id": "2506.18164", "pdf": "https://arxiv.org/pdf/2506.18164", "abs": "https://arxiv.org/abs/2506.18164", "authors": ["Varun Belagali", "Pierre Marza", "Srikar Yellapragada", "Zilinghan Li", "Tarak Nath Nandi", "Ravi K Madduri", "Joel Saltz", "Stergios Christodoulidis", "Maria Vakalopoulou", "Dimitris Samaras"], "title": "CDG-MAE: Learning Correspondences from Diffusion Generated Views", "categories": ["cs.CV"], "comment": null, "summary": "Learning dense correspondences, critical for application such as video label propagation, is hindered by tedious and unscalable manual annotation. Self-supervised methods address this by using a cross-view pretext task, often modeled with a masked autoencoder, where a masked target view is reconstructed from an anchor view. However, acquiring effective training data remains a challenge - collecting diverse video datasets is difficult and costly, while simple image crops lack necessary pose variations. This paper introduces CDG-MAE, a novel MAE-based self-supervised method that uses diverse synthetic views generated from static images via an image-conditioned diffusion model. These generated views exhibit substantial changes in pose and perspective, providing a rich training signal that overcomes the limitations of video and crop-based anchors. We present a quantitative method to evaluate local and global consistency of generated images, discussing their use for cross-view self-supervised pretraining. Furthermore, we enhance the standard single-anchor MAE setting to a multi-anchor strategy to effectively modulate the difficulty of pretext task. CDG-MAE significantly outperforms state-of-the-art MAE methods reliant only on images and substantially narrows the performance gap to video-based approaches.", "AI": {"tldr": "CDG-MAE\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMAE\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u6837\u5408\u6210\u89c6\u56fe\uff0c\u514b\u670d\u4e86\u89c6\u9891\u548c\u56fe\u50cf\u88c1\u526a\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5bc6\u96c6\u5bf9\u5e94\u5b66\u4e60\u4e2d\u7684\u8bad\u7ec3\u6570\u636e\u83b7\u53d6\u96be\u9898\uff0c\u907f\u514d\u4f9d\u8d56\u6602\u8d35\u4e14\u6709\u9650\u7684\u89c6\u9891\u6570\u636e\u6216\u7f3a\u4e4f\u591a\u6837\u6027\u7684\u56fe\u50cf\u88c1\u526a\u3002", "method": "\u4f7f\u7528\u56fe\u50cf\u6761\u4ef6\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u6837\u5408\u6210\u89c6\u56fe\uff0c\u63d0\u51fa\u591a\u951a\u70b9\u7b56\u7565\u8c03\u8282\u4efb\u52a1\u96be\u5ea6\uff0c\u5e76\u8bc4\u4f30\u751f\u6210\u56fe\u50cf\u7684\u5c40\u90e8\u548c\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "CDG-MAE\u663e\u8457\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u56fe\u50cf\u7684MAE\u65b9\u6cd5\uff0c\u5e76\u5927\u5e45\u7f29\u5c0f\u4e0e\u57fa\u4e8e\u89c6\u9891\u65b9\u6cd5\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u901a\u8fc7\u5408\u6210\u89c6\u56fe\u548c\u591a\u951a\u70b9\u7b56\u7565\uff0cCDG-MAE\u4e3a\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u6570\u636e\u548c\u66f4\u7075\u6d3b\u7684\u4efb\u52a1\u8bbe\u8ba1\u3002"}}
{"id": "2506.18208", "pdf": "https://arxiv.org/pdf/2506.18208", "abs": "https://arxiv.org/abs/2506.18208", "authors": ["Ankit Sanjyal"], "title": "Limitations of NERF with pre-trained Vision Features for Few-Shot 3D Reconstruction", "categories": ["cs.CV"], "comment": "5 pages, 1 table, 2 figures. First submission. Code available at:   \\url{https://github.com/ANKITSANJYAL/nerf-few-shot-limitations}", "summary": "Neural Radiance Fields (NeRF) have revolutionized 3D scene reconstruction from sparse image collections. Recent work has explored integrating pre-trained vision features, particularly from DINO, to enhance few-shot reconstruction capabilities. However, the effectiveness of such approaches remains unclear, especially in extreme few-shot scenarios. In this paper, we present a systematic evaluation of DINO-enhanced NeRF models, comparing baseline NeRF, frozen DINO features, LoRA fine-tuned features, and multi-scale feature fusion. Surprisingly, our experiments reveal that all DINO variants perform worse than the baseline NeRF, achieving PSNR values around 12.9 to 13.0 compared to the baseline's 14.71. This counterintuitive result suggests that pre-trained vision features may not be beneficial for few-shot 3D reconstruction and may even introduce harmful biases. We analyze potential causes including feature-task mismatch, overfitting to limited data, and integration challenges. Our findings challenge common assumptions in the field and suggest that simpler architectures focusing on geometric consistency may be more effective for few-shot scenarios.", "AI": {"tldr": "DINO-enhanced NeRF\u6a21\u578b\u5728\u6781\u7aef\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u5982\u57fa\u7ebfNeRF\uff0c\u9884\u8bad\u7ec3\u89c6\u89c9\u7279\u5f81\u53ef\u80fd\u5bf9\u5c11\u6837\u672c3D\u91cd\u5efa\u65e0\u76ca\u751a\u81f3\u6709\u5bb3\u3002", "motivation": "\u63a2\u7d22\u9884\u8bad\u7ec3\u89c6\u89c9\u7279\u5f81\uff08\u5982DINO\uff09\u5bf9\u5c11\u6837\u672c3D\u91cd\u5efa\u7684\u6548\u679c\uff0c\u9a8c\u8bc1\u5176\u5728\u6781\u7aef\u5c11\u6837\u672c\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30DINO\u589e\u5f3a\u7684NeRF\u6a21\u578b\uff0c\u5305\u62ec\u57fa\u7ebfNeRF\u3001\u51bb\u7ed3DINO\u7279\u5f81\u3001LoRA\u5fae\u8c03\u7279\u5f81\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u3002", "result": "\u6240\u6709DINO\u53d8\u4f53\u8868\u73b0\u5747\u5dee\u4e8e\u57fa\u7ebfNeRF\uff08PSNR 12.9-13.0 vs. 14.71\uff09\uff0c\u8868\u660e\u9884\u8bad\u7ec3\u7279\u5f81\u53ef\u80fd\u5f15\u5165\u6709\u5bb3\u504f\u5dee\u3002", "conclusion": "\u9884\u8bad\u7ec3\u89c6\u89c9\u7279\u5f81\u5728\u5c11\u6837\u672c3D\u91cd\u5efa\u4e2d\u53ef\u80fd\u65e0\u6548\uff0c\u5efa\u8bae\u5173\u6ce8\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u7b80\u5355\u67b6\u6784\u3002"}}
{"id": "2506.18226", "pdf": "https://arxiv.org/pdf/2506.18226", "abs": "https://arxiv.org/abs/2506.18226", "authors": ["Xunzhi Xiang", "Qi Fan"], "title": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Autoregressive conditional image generation models have emerged as a dominant paradigm in text-to-image synthesis. These methods typically convert images into one-dimensional token sequences and leverage the self-attention mechanism, which has achieved remarkable success in natural language processing, to capture long-range dependencies, model global context, and ensure semantic coherence. However, excessively long contexts during inference lead to significant memory overhead caused by KV-cache and computational delays. To alleviate these challenges, we systematically analyze how global semantics, spatial layouts, and fine-grained textures are formed during inference, and propose a novel training-free context optimization method called Adaptive Dynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies historical tokens crucial for maintaining local texture consistency and those essential for ensuring global semantic coherence, thereby efficiently streamlining attention computation. Additionally, we introduce a dynamic KV-cache update mechanism tailored for ADSA, reducing GPU memory consumption during inference by approximately $50\\%$. Extensive qualitative and quantitative experiments demonstrate the effectiveness and superiority of our approach in terms of both generation quality and resource efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u4e0a\u4e0b\u6587\u4f18\u5316\u65b9\u6cd5ADSA\uff0c\u901a\u8fc7\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u51cf\u5c11\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u63a8\u7406\u65f6\u56e0\u957f\u4e0a\u4e0b\u6587\u5bfc\u81f4\u5185\u5b58\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faADSA\u65b9\u6cd5\uff0c\u52a8\u6001\u8bc6\u522b\u5173\u952e\u5386\u53f2token\uff0c\u4f18\u5316\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u5e76\u5f15\u5165\u52a8\u6001KV-cache\u66f4\u65b0\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660eADSA\u5728\u751f\u6210\u8d28\u91cf\u548c\u8d44\u6e90\u6548\u7387\u4e0a\u5747\u8868\u73b0\u4f18\u8d8a\uff0cGPU\u5185\u5b58\u6d88\u8017\u51cf\u5c11\u7ea650%\u3002", "conclusion": "ADSA\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u7684\u6027\u80fd\u3002"}}
{"id": "2506.18270", "pdf": "https://arxiv.org/pdf/2506.18270", "abs": "https://arxiv.org/abs/2506.18270", "authors": ["Qinrong Cai", "Yu Guan", "Zhibo Chen", "Dong Liang", "Qiuyun Fan", "Qiegen Liu"], "title": "Adaptive Mask-guided K-space Diffusion for Accelerated MRI Reconstruction", "categories": ["cs.CV"], "comment": "10 pages, 9 figures", "summary": "As the deep learning revolution marches on, masked modeling has emerged as a distinctive approach that involves predicting parts of the original data that are proportionally masked during training, and has demonstrated exceptional performance in multiple fields. Magnetic Resonance Imaging (MRI) reconstruction is a critical task in medical imaging that seeks to recover high-quality images from under-sampled k-space data. However, previous MRI reconstruction strategies usually optimized the entire image domain or k-space, without considering the importance of different frequency regions in the k-space This work introduces a diffusion model based on adaptive masks (AMDM), which utilizes the adaptive adjustment of frequency distribution based on k-space data to develop a hybrid masks mechanism that adapts to different k-space inputs. This enables the effective separation of high-frequency and low-frequency components, producing diverse frequency-specific representations. Additionally, the k-space frequency distribution informs the generation of adaptive masks, which, in turn, guide a closed-loop diffusion process. Experimental results verified the ability of this method to learn specific frequency information and thereby improved the quality of MRI reconstruction, providing a flexible framework for optimizing k-space data using masks in the future.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u63a9\u7801\u7684\u6269\u6563\u6a21\u578b\uff08AMDM\uff09\uff0c\u7528\u4e8eMRI\u91cd\u5efa\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574k\u7a7a\u95f4\u6570\u636e\u7684\u9891\u7387\u5206\u5e03\uff0c\u6709\u6548\u5206\u79bb\u9ad8\u4f4e\u9891\u6210\u5206\uff0c\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edfMRI\u91cd\u5efa\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651k\u7a7a\u95f4\u4e0d\u540c\u9891\u7387\u533a\u57df\u7684\u91cd\u8981\u6027\uff0c\u5bfc\u81f4\u91cd\u5efa\u6548\u679c\u53d7\u9650\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u63a9\u7801\u673a\u5236\uff0c\u6839\u636ek\u7a7a\u95f4\u6570\u636e\u7684\u9891\u7387\u5206\u5e03\u751f\u6210\u52a8\u6001\u63a9\u7801\uff0c\u6307\u5bfc\u6269\u6563\u8fc7\u7a0b\uff0c\u5206\u79bb\u9ad8\u4f4e\u9891\u6210\u5206\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5b66\u4e60\u7279\u5b9a\u9891\u7387\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347MRI\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "AMDM\u4e3a\u672a\u6765\u57fa\u4e8e\u63a9\u7801\u4f18\u5316k\u7a7a\u95f4\u6570\u636e\u63d0\u4f9b\u4e86\u7075\u6d3b\u6846\u67b6\u3002"}}
{"id": "2506.18323", "pdf": "https://arxiv.org/pdf/2506.18323", "abs": "https://arxiv.org/abs/2506.18323", "authors": ["Muhammad Azeem Aslam", "Hassan Khalid", "Nisar Ahmed"], "title": "A Multi-Scale Spatial Attention-Based Zero-Shot Learning Framework for Low-Light Image Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Low-light image enhancement remains a challenging task, particularly in the absence of paired training data. In this study, we present LucentVisionNet, a novel zero-shot learning framework that addresses the limitations of traditional and deep learning-based enhancement methods. The proposed approach integrates multi-scale spatial attention with a deep curve estimation network, enabling fine-grained enhancement while preserving semantic and perceptual fidelity. To further improve generalization, we adopt a recurrent enhancement strategy and optimize the model using a composite loss function comprising six tailored components, including a novel no-reference image quality loss inspired by human visual perception. Extensive experiments on both paired and unpaired benchmark datasets demonstrate that LucentVisionNet consistently outperforms state-of-the-art supervised, unsupervised, and zero-shot methods across multiple full-reference and no-reference image quality metrics. Our framework achieves high visual quality, structural consistency, and computational efficiency, making it well-suited for deployment in real-world applications such as mobile photography, surveillance, and autonomous navigation.", "AI": {"tldr": "LucentVisionNet\u662f\u4e00\u79cd\u96f6\u6837\u672c\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u7a7a\u95f4\u6ce8\u610f\u529b\u548c\u6df1\u5ea6\u66f2\u7ebf\u4f30\u8ba1\u7f51\u7edc\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u6311\u6218\uff0c\u8d85\u8d8a\u4f20\u7edf\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u96c6\u6210\u591a\u5c3a\u5ea6\u7a7a\u95f4\u6ce8\u610f\u529b\u4e0e\u6df1\u5ea6\u66f2\u7ebf\u4f30\u8ba1\u7f51\u7edc\uff0c\u91c7\u7528\u5faa\u73af\u589e\u5f3a\u7b56\u7565\u548c\u590d\u5408\u635f\u5931\u51fd\u6570\u4f18\u5316\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u76d1\u7763\u3001\u65e0\u76d1\u7763\u548c\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u63d0\u5347\u89c6\u89c9\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "LucentVisionNet\u9002\u7528\u4e8e\u79fb\u52a8\u6444\u5f71\u3001\u76d1\u63a7\u548c\u81ea\u4e3b\u5bfc\u822a\u7b49\u5b9e\u9645\u5e94\u7528\uff0c\u5177\u6709\u9ad8\u8d28\u91cf\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2506.18325", "pdf": "https://arxiv.org/pdf/2506.18325", "abs": "https://arxiv.org/abs/2506.18325", "authors": ["Yu Xie", "Chengjie Zeng", "Lingyun Zhang", "Yanwei Fu"], "title": "NSFW-Classifier Guided Prompt Sanitization for Safe Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of text-to-image (T2I) models, such as Stable Diffusion, has enhanced their capability to synthesize images from textual prompts. However, this progress also raises significant risks of misuse, including the generation of harmful content (e.g., pornography, violence, discrimination), which contradicts the ethical goals of T2I technology and hinders its sustainable development. Inspired by \"jailbreak\" attacks in large language models, which bypass restrictions through subtle prompt modifications, this paper proposes NSFW-Classifier Guided Prompt Sanitization (PromptSan), a novel approach to detoxify harmful prompts without altering model architecture or degrading generation capability. PromptSan includes two variants: PromptSan-Modify, which iteratively identifies and replaces harmful tokens in input prompts using text NSFW classifiers during inference, and PromptSan-Suffix, which trains an optimized suffix token sequence to neutralize harmful intent while passing both text and image NSFW classifier checks. Extensive experiments demonstrate that PromptSan achieves state-of-the-art performance in reducing harmful content generation across multiple metrics, effectively balancing safety and usability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPromptSan\u65b9\u6cd5\uff0c\u901a\u8fc7\u6587\u672cNSFW\u5206\u7c7b\u5668\u6307\u5bfc\u7684\u63d0\u793a\u51c0\u5316\u6280\u672f\uff0c\u51cf\u5c11T2I\u6a21\u578b\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u7684\u98ce\u9669\u3002", "motivation": "T2I\u6a21\u578b\uff08\u5982Stable Diffusion\uff09\u7684\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u4e86\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u7684\u98ce\u9669\uff0c\u5982\u8272\u60c5\u3001\u66b4\u529b\u7b49\uff0c\u8fdd\u80cc\u4f26\u7406\u76ee\u6807\u5e76\u963b\u788d\u53ef\u6301\u7eed\u53d1\u5c55\u3002", "method": "\u63d0\u51faPromptSan\u65b9\u6cd5\uff0c\u5305\u62ecPromptSan-Modify\uff08\u8fed\u4ee3\u8bc6\u522b\u5e76\u66ff\u6362\u6709\u5bb3\u6807\u8bb0\uff09\u548cPromptSan-Suffix\uff08\u8bad\u7ec3\u4f18\u5316\u540e\u7f00\u6807\u8bb0\u5e8f\u5217\u4e2d\u548c\u6709\u5bb3\u610f\u56fe\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPromptSan\u5728\u591a\u6307\u6807\u4e0a\u663e\u8457\u51cf\u5c11\u6709\u5bb3\u5185\u5bb9\u751f\u6210\uff0c\u5e73\u8861\u5b89\u5168\u6027\u4e0e\u53ef\u7528\u6027\u3002", "conclusion": "PromptSan\u5728\u4e0d\u6539\u53d8\u6a21\u578b\u67b6\u6784\u6216\u964d\u4f4e\u751f\u6210\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u51c0\u5316\u6709\u5bb3\u63d0\u793a\uff0c\u4e3aT2I\u6280\u672f\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2506.18346", "pdf": "https://arxiv.org/pdf/2506.18346", "abs": "https://arxiv.org/abs/2506.18346", "authors": ["Tongshun Zhang", "Pingping Liu", "Mengen Cai", "Zijian Zhang", "Yubing Lu", "Qiuzhan Zhou"], "title": "BSMamba: Brightness and Semantic Modeling for Long-Range Interaction in Low-Light Image Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Current low-light image enhancement (LLIE) methods face significant limitations in simultaneously improving brightness while preserving semantic consistency, fine details, and computational efficiency. With the emergence of state-space models, particularly Mamba, image restoration has achieved remarkable performance, yet existing visual Mamba approaches flatten 2D images into 1D token sequences using fixed scanning rules, critically limiting interactions between distant tokens with causal relationships and constraining their ability to capture meaningful long-range dependencies. To address these fundamental limitations, we propose BSMamba, a novel visual Mamba architecture comprising two specially designed components: Brightness Mamba and Semantic Mamba. The Brightness Mamba revolutionizes token interaction patterns by prioritizing connections between distant tokens with similar brightness levels, effectively addressing the challenge of brightness restoration in LLIE tasks through brightness-guided selective attention. Complementing this, the Semantic Mamba establishes priority interactions between tokens sharing similar semantic meanings, allowing the model to maintain contextual consistency by connecting semantically related regions across the image, thus preserving the hierarchical nature of image semantics during enhancement. By intelligently modeling tokens based on brightness and semantic similarity rather than arbitrary scanning patterns, BSMamba transcends the constraints of conventional token sequencing while adhering to the principles of causal modeling. Extensive experiments demonstrate that BSMamba achieves state-of-the-art performance in LLIE while preserving semantic consistency.", "AI": {"tldr": "BSMamba\u662f\u4e00\u79cd\u65b0\u578b\u89c6\u89c9Mamba\u67b6\u6784\uff0c\u901a\u8fc7\u4eae\u5ea6Mamba\u548c\u8bed\u4e49Mamba\u7ec4\u4ef6\uff0c\u89e3\u51b3\u4e86\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4e2d\u4eae\u5ea6\u6062\u590d\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u5728\u63d0\u5347\u4eae\u5ea6\u7684\u540c\u65f6\u96be\u4ee5\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u7ec6\u8282\uff0c\u4e14\u4f20\u7edf\u89c6\u89c9Mamba\u65b9\u6cd5\u56e0\u56fa\u5b9a\u626b\u63cf\u89c4\u5219\u9650\u5236\u4e86\u8fdc\u8ddd\u79bb\u6807\u8bb0\u7684\u4ea4\u4e92\u3002", "method": "BSMamba\u5305\u542b\u4eae\u5ea6Mamba\u548c\u8bed\u4e49Mamba\uff0c\u5206\u522b\u57fa\u4e8e\u4eae\u5ea6\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u5efa\u6a21\u6807\u8bb0\u4ea4\u4e92\uff0c\u907f\u514d\u56fa\u5b9a\u626b\u63cf\u89c4\u5219\u7684\u5c40\u9650\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eBSMamba\u5728\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "conclusion": "BSMamba\u901a\u8fc7\u4eae\u5ea6\u548c\u8bed\u4e49\u5f15\u5bfc\u7684\u6807\u8bb0\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u7684\u6548\u679c\u3002"}}
{"id": "2506.18414", "pdf": "https://arxiv.org/pdf/2506.18414", "abs": "https://arxiv.org/abs/2506.18414", "authors": ["Ciro Listone", "Aniello Murano"], "title": "Latent Space Analysis for Melanoma Prevention", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 4 figures, under review", "summary": "Melanoma represents a critical health risk due to its aggressive progression and high mortality, underscoring the need for early, interpretable diagnostic tools. While deep learning has advanced in skin lesion classification, most existing models provide only binary outputs, offering limited clinical insight. This work introduces a novel approach that extends beyond classification, enabling interpretable risk modelling through a Conditional Variational Autoencoder. The proposed method learns a structured latent space that captures semantic relationships among lesions, allowing for a nuanced, continuous assessment of morphological differences. An SVM is also trained on this representation effectively differentiating between benign nevi and melanomas, demonstrating strong and consistent performance. More importantly, the learned latent space supports visual and geometric interpretation of malignancy, with the spatial proximity of a lesion to known melanomas serving as a meaningful indicator of risk. This approach bridges predictive performance with clinical applicability, fostering early detection, highlighting ambiguous cases, and enhancing trust in AI-assisted diagnosis through transparent and interpretable decision-making.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u53ef\u89e3\u91ca\u6027\u98ce\u9669\u5efa\u6a21\u65b9\u6cd5\uff0c\u7528\u4e8e\u76ae\u80a4\u75c5\u53d8\u5206\u7c7b\uff0c\u652f\u6301\u8fde\u7eed\u8bc4\u4f30\u548c\u53ef\u89c6\u5316\u89e3\u91ca\u3002", "motivation": "\u9ed1\u8272\u7d20\u7624\u7684\u9ad8\u81f4\u6b7b\u7387\u9700\u8981\u65e9\u671f\u53ef\u89e3\u91ca\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4ec5\u63d0\u4f9b\u4e8c\u5143\u8f93\u51fa\uff0c\u4e34\u5e8a\u4ef7\u503c\u6709\u9650\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\uff0c\u7ed3\u5408SVM\u5206\u7c7b\u5668\uff0c\u5b9e\u73b0\u75c5\u53d8\u7684\u8fde\u7eed\u8bc4\u4f30\u548c\u98ce\u9669\u6307\u793a\u3002", "result": "\u65b9\u6cd5\u5728\u533a\u5206\u826f\u6027\u75e3\u548c\u9ed1\u8272\u7d20\u7624\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6f5c\u5728\u7a7a\u95f4\u652f\u6301\u53ef\u89c6\u5316\u548c\u51e0\u4f55\u89e3\u91ca\uff0c\u589e\u5f3a\u4e34\u5e8a\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u9884\u6d4b\u6027\u80fd\u4e0e\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\uff0c\u4fc3\u8fdb\u65e9\u671f\u68c0\u6d4b\u548cAI\u8bca\u65ad\u7684\u4fe1\u4efb\u3002"}}
{"id": "2506.18437", "pdf": "https://arxiv.org/pdf/2506.18437", "abs": "https://arxiv.org/abs/2506.18437", "authors": ["Sijin He", "Guangfeng Lin", "Tao Li", "Yajun Chen"], "title": "Frequency-Domain Fusion Transformer for Image Inpainting", "categories": ["cs.CV"], "comment": null, "summary": "Image inpainting plays a vital role in restoring missing image regions and supporting high-level vision tasks, but traditional methods struggle with complex textures and large occlusions. Although Transformer-based approaches have demonstrated strong global modeling capabilities, they often fail to preserve high-frequency details due to the low-pass nature of self-attention and suffer from high computational costs. To address these challenges, this paper proposes a Transformer-based image inpainting method incorporating frequency-domain fusion. Specifically, an attention mechanism combining wavelet transform and Gabor filtering is introduced to enhance multi-scale structural modeling and detail preservation. Additionally, a learnable frequency-domain filter based on the fast Fourier transform is designed to replace the feedforward network, enabling adaptive noise suppression and detail retention. The model adopts a four-level encoder-decoder structure and is guided by a novel loss strategy to balance global semantics and fine details. Experimental results demonstrate that the proposed method effectively improves the quality of image inpainting by preserving more high-frequency information.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9891\u57df\u878d\u5408\u7684Transformer\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c0f\u6ce2\u53d8\u6362\u548cGabor\u6ee4\u6ce2\u589e\u5f3a\u7ec6\u8282\u4fdd\u7559\uff0c\u5e76\u8bbe\u8ba1\u4e86\u53ef\u5b66\u4e60\u7684\u9891\u57df\u6ee4\u6ce2\u5668\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u7eb9\u7406\u548c\u5927\u9762\u79ef\u906e\u6321\uff0c\u800cTransformer\u65b9\u6cd5\u56e0\u81ea\u6ce8\u610f\u529b\u7684\u4f4e\u901a\u7279\u6027\u65e0\u6cd5\u4fdd\u7559\u9ad8\u9891\u7ec6\u8282\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u7ed3\u5408\u5c0f\u6ce2\u53d8\u6362\u548cGabor\u6ee4\u6ce2\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8bbe\u8ba1\u53ef\u5b66\u4e60\u7684\u9891\u57df\u6ee4\u6ce2\u5668\u66ff\u4ee3\u524d\u9988\u7f51\u7edc\uff0c\u91c7\u7528\u56db\u5c42\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u548c\u65b0\u635f\u5931\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u4fdd\u7559\u66f4\u591a\u9ad8\u9891\u4fe1\u606f\uff0c\u63d0\u5347\u56fe\u50cf\u4fee\u590d\u8d28\u91cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4fdd\u7559\u9ad8\u9891\u7ec6\u8282\u548c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.18463", "pdf": "https://arxiv.org/pdf/2506.18463", "abs": "https://arxiv.org/abs/2506.18463", "authors": ["Sophia Sirko-Galouchenko", "Spyros Gidaris", "Antonin Vobecky", "Andrei Bursuc", "Nicolas Thome"], "title": "DIP: Unsupervised Dense In-Context Post-training of Visual Representations", "categories": ["cs.CV"], "comment": null, "summary": "We introduce DIP, a novel unsupervised post-training method designed to enhance dense image representations in large-scale pretrained vision encoders for in-context scene understanding. Unlike prior approaches that rely on complex self-distillation architectures, our method trains the vision encoder using pseudo-tasks that explicitly simulate downstream in-context scenarios, inspired by meta-learning principles. To enable post-training on unlabeled data, we propose an automatic mechanism for generating in-context tasks that combines a pretrained diffusion model and the vision encoder itself. DIP is simple, unsupervised, and computationally efficient, requiring less than 9 hours on a single A100 GPU. By learning dense representations through pseudo in-context tasks, it achieves strong performance across a wide variety of downstream real-world in-context scene understanding tasks. It outperforms both the initial vision encoder and prior methods, offering a practical and effective solution for improving dense representations. Code available here: https://github.com/sirkosophia/DIP", "AI": {"tldr": "DIP\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u4e0b\u6e38\u573a\u666f\u4efb\u52a1\u63d0\u5347\u5bc6\u96c6\u56fe\u50cf\u8868\u793a\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63d0\u5347\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u5728\u4e0a\u4e0b\u6587\u573a\u666f\u7406\u89e3\u4e2d\u7684\u5bc6\u96c6\u8868\u793a\u80fd\u529b\uff0c\u907f\u514d\u590d\u6742\u81ea\u84b8\u998f\u67b6\u6784\u3002", "method": "\u5229\u7528\u4f2a\u4efb\u52a1\u6a21\u62df\u4e0b\u6e38\u573a\u666f\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u89c6\u89c9\u7f16\u7801\u5668\u81ea\u52a8\u751f\u6210\u4efb\u52a1\uff0c\u57fa\u4e8e\u5143\u5b66\u4e60\u539f\u7406\u3002", "result": "\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u9ad8\u6548\uff08\u5355A100 GPU\u8017\u65f6<9\u5c0f\u65f6\uff09\u3002", "conclusion": "DIP\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u9ad8\u6548\u4e14\u6709\u6548\u7684\u5bc6\u96c6\u8868\u793a\u6539\u8fdb\u65b9\u6848\u3002"}}
{"id": "2506.18484", "pdf": "https://arxiv.org/pdf/2506.18484", "abs": "https://arxiv.org/abs/2506.18484", "authors": ["Pascal Kl\u00f6ckner", "Jos\u00e9 Teixeira", "Diana Montezuma", "Jaime S. Cardoso", "Hugo M. Horlings", "Sara P. Oliveira"], "title": "GANs vs. Diffusion Models for virtual staining with the HER2match dataset", "categories": ["cs.CV"], "comment": null, "summary": "Virtual staining is a promising technique that uses deep generative models to recreate histological stains, providing a faster and more cost-effective alternative to traditional tissue chemical staining. Specifically for H&E-HER2 staining transfer, despite a rising trend in publications, the lack of sufficient public datasets has hindered progress in the topic. Additionally, it is currently unclear which model frameworks perform best for this particular task. In this paper, we introduce the HER2match dataset, the first publicly available dataset with the same breast cancer tissue sections stained with both H&E and HER2. Furthermore, we compare the performance of several Generative Adversarial Networks (GANs) and Diffusion Models (DMs), and implement a novel Brownian Bridge Diffusion Model for H&E-HER2 translation. Our findings indicate that, overall, GANs perform better than DMs, with only the BBDM achieving comparable results. Furthermore, we emphasize the importance of data alignment, as all models trained on HER2match produced vastly improved visuals compared to the widely used consecutive-slide BCI dataset. This research provides a new high-quality dataset ([available upon publication acceptance]), improving both model training and evaluation. In addition, our comparison of frameworks offers valuable guidance for researchers working on the topic.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u516c\u5f00\u7684H&E-HER2\u67d3\u8272\u914d\u5bf9\u6570\u636e\u96c6HER2match\uff0c\u5e76\u6bd4\u8f83\u4e86GANs\u548cDMs\u5728H&E-HER2\u67d3\u8272\u8f6c\u6362\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0GANs\u6574\u4f53\u4f18\u4e8eDMs\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u6570\u636e\u5bf9\u9f50\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u89e3\u51b3H&E-HER2\u67d3\u8272\u8f6c\u6362\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u516c\u5f00\u6570\u636e\u96c6\u548c\u6a21\u578b\u6027\u80fd\u4e0d\u660e\u786e\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165HER2match\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u591a\u79cdGANs\u548cDMs\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684Brownian Bridge Diffusion Model\uff08BBDM\uff09\u3002", "result": "GANs\u6574\u4f53\u8868\u73b0\u4f18\u4e8eDMs\uff0c\u4ec5BBDM\u4e0e\u4e4b\u76f8\u5f53\uff1b\u6570\u636e\u5bf9\u9f50\u663e\u8457\u63d0\u5347\u6a21\u578b\u6548\u679c\u3002", "conclusion": "HER2match\u6570\u636e\u96c6\u548c\u6a21\u578b\u6bd4\u8f83\u4e3aH&E-HER2\u67d3\u8272\u8f6c\u6362\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u8d44\u6e90\u548c\u6307\u5bfc\u3002"}}
{"id": "2506.18493", "pdf": "https://arxiv.org/pdf/2506.18493", "abs": "https://arxiv.org/abs/2506.18493", "authors": ["Trong-Vu Hoang", "Quang-Binh Nguyen", "Thanh-Toan Do", "Tam V. Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "title": "ShowFlow: From Robust Single Concept to Condition-Free Multi-Concept Generation", "categories": ["cs.CV"], "comment": null, "summary": "Customizing image generation remains a core challenge in controllable image synthesis. For single-concept generation, maintaining both identity preservation and prompt alignment is challenging. In multi-concept scenarios, relying solely on a prompt without additional conditions like layout boxes or semantic masks, often leads to identity loss and concept omission. In this paper, we introduce ShowFlow, a comprehensive framework designed to tackle these challenges. We propose ShowFlow-S for single-concept image generation, and ShowFlow-M for handling multiple concepts. ShowFlow-S introduces a KronA-WED adapter, which integrates a Kronecker adapter with weight and embedding decomposition, and employs a disentangled learning approach with a novel attention regularization objective to enhance single-concept generation. Building on this foundation, ShowFlow-M directly reuses the learned models from ShowFlow-S to support multi-concept generation without extra conditions, incorporating a Subject-Adaptive Matching Attention (SAMA) and a layout consistency strategy as the plug-and-play module. Extensive experiments and user studies validate ShowFlow's effectiveness, highlighting its potential in real-world applications like advertising and virtual dressing.", "AI": {"tldr": "ShowFlow\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5355\u6982\u5ff5\u548c\u591a\u6982\u5ff5\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u8eab\u4efd\u4fdd\u6301\u548c\u63d0\u793a\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u53ef\u63a7\u56fe\u50cf\u5408\u6210\u4e2d\u5355\u6982\u5ff5\u548c\u591a\u6982\u5ff5\u751f\u6210\u7684\u6311\u6218\uff0c\u5982\u8eab\u4efd\u4e22\u5931\u548c\u6982\u5ff5\u9057\u6f0f\u3002", "method": "ShowFlow-S\u4f7f\u7528KronA-WED\u9002\u914d\u5668\u548c\u89e3\u8026\u5b66\u4e60\u65b9\u6cd5\uff1bShowFlow-M\u91cd\u7528ShowFlow-S\u6a21\u578b\uff0c\u52a0\u5165SAMA\u548c\u5e03\u5c40\u4e00\u81f4\u6027\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86ShowFlow\u5728\u5e7f\u544a\u548c\u865a\u62df\u8bd5\u8863\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "ShowFlow\u5728\u5355\u6982\u5ff5\u548c\u591a\u6982\u5ff5\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.18520", "pdf": "https://arxiv.org/pdf/2506.18520", "abs": "https://arxiv.org/abs/2506.18520", "authors": ["JiaKui Hu", "Zhengjian Yao", "Lujia Jin", "Hangzhou He", "Yanye Lu"], "title": "Enhancing Image Restoration Transformer via Adaptive Translation Equivariance", "categories": ["cs.CV"], "comment": null, "summary": "Translation equivariance is a fundamental inductive bias in image restoration, ensuring that translated inputs produce translated outputs. Attention mechanisms in modern restoration transformers undermine this property, adversely impacting both training convergence and generalization. To alleviate this issue, we propose two key strategies for incorporating translation equivariance: slide indexing and component stacking. Slide indexing maintains operator responses at fixed positions, with sliding window attention being a notable example, while component stacking enables the arrangement of translation-equivariant operators in parallel or sequentially, thereby building complex architectures while preserving translation equivariance. However, these strategies still create a dilemma in model design between the high computational cost of self-attention and the fixed receptive field associated with sliding window attention. To address this, we develop an adaptive sliding indexing mechanism to efficiently select key-value pairs for each query, which are then concatenated in parallel with globally aggregated key-value pairs. The designed network, called the Translation Equivariance Adaptive Transformer (TEAFormer), is assessed across a variety of image restoration tasks. The results highlight its superiority in terms of effectiveness, training convergence, and generalization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTEAFormer\u7684\u7f51\u7edc\uff0c\u901a\u8fc7\u6ed1\u52a8\u7d22\u5f15\u548c\u7ec4\u4ef6\u5806\u53e0\u7b56\u7565\u89e3\u51b3\u56fe\u50cf\u4fee\u590d\u4e2d\u6ce8\u610f\u529b\u673a\u5236\u7834\u574f\u5e73\u79fb\u7b49\u53d8\u6027\u7684\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u6ed1\u52a8\u7d22\u5f15\u673a\u5236\u4ee5\u5e73\u8861\u8ba1\u7b97\u6210\u672c\u548c\u611f\u53d7\u91ce\u3002", "motivation": "\u73b0\u4ee3\u4fee\u590d\u53d8\u6362\u5668\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\u7834\u574f\u4e86\u5e73\u79fb\u7b49\u53d8\u6027\uff0c\u5f71\u54cd\u4e86\u8bad\u7ec3\u6536\u655b\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6ed1\u52a8\u7d22\u5f15\u548c\u7ec4\u4ef6\u5806\u53e0\u7b56\u7565\uff0c\u5e76\u8bbe\u8ba1\u81ea\u9002\u5e94\u6ed1\u52a8\u7d22\u5f15\u673a\u5236\uff0c\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\u3002", "result": "TEAFormer\u5728\u591a\u79cd\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6548\u679c\u3001\u8bad\u7ec3\u6536\u655b\u548c\u6cdb\u5316\u80fd\u529b\u5747\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "TEAFormer\u901a\u8fc7\u81ea\u9002\u5e94\u6ed1\u52a8\u7d22\u5f15\u673a\u5236\u6709\u6548\u5e73\u8861\u4e86\u8ba1\u7b97\u6210\u672c\u548c\u611f\u53d7\u91ce\uff0c\u63d0\u5347\u4e86\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2506.18527", "pdf": "https://arxiv.org/pdf/2506.18527", "abs": "https://arxiv.org/abs/2506.18527", "authors": ["JiaKui Hu", "Yuxiao Yang", "Jialun Liu", "Jinbo Wu", "Chen Zhao", "Yanye Lu"], "title": "Auto-Regressively Generating Multi-View Consistent Images", "categories": ["cs.CV"], "comment": null, "summary": "Generating multi-view images from human instructions is crucial for 3D content creation. The primary challenges involve maintaining consistency across multiple views and effectively synthesizing shapes and textures under diverse conditions. In this paper, we propose the Multi-View Auto-Regressive (MV-AR) method, which leverages an auto-regressive model to progressively generate consistent multi-view images from arbitrary prompts. Firstly, the next-token-prediction capability of the AR model significantly enhances its effectiveness in facilitating progressive multi-view synthesis. When generating widely-separated views, MV-AR can utilize all its preceding views to extract effective reference information. Subsequently, we propose a unified model that accommodates various prompts via architecture designing and training strategies. To address multiple conditions, we introduce condition injection modules for text, camera pose, image, and shape. To manage multi-modal conditions simultaneously, a progressive training strategy is employed. This strategy initially adopts the text-to-multi-view (t2mv) model as a baseline to enhance the development of a comprehensive X-to-multi-view (X2mv) model through the randomly dropping and combining conditions. Finally, to alleviate the overfitting problem caused by limited high-quality data, we propose the \"Shuffle View\" data augmentation technique, thus significantly expanding the training data by several magnitudes. Experiments demonstrate the performance and versatility of our MV-AR, which consistently generates consistent multi-view images across a range of conditions and performs on par with leading diffusion-based multi-view image generation models. Code and models will be released at https://github.com/MILab-PKU/MVAR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u89c6\u56fe\u81ea\u56de\u5f52\uff08MV-AR\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u6a21\u578b\u9010\u6b65\u751f\u6210\u4e00\u81f4\u7684\u591a\u89c6\u56fe\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u548c\u591a\u6837\u5316\u6761\u4ef6\u4e0b\u7684\u5f62\u72b6\u4e0e\u7eb9\u7406\u5408\u6210\u95ee\u9898\u3002", "motivation": "\u591a\u89c6\u56fe\u56fe\u50cf\u751f\u6210\u57283D\u5185\u5bb9\u521b\u4f5c\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u548c\u591a\u6837\u5316\u6761\u4ef6\u4e0b\u5408\u6210\u7684\u6311\u6218\u3002", "method": "\u5229\u7528\u81ea\u56de\u5f52\u6a21\u578b\u7684\u9010\u4ee4\u724c\u9884\u6d4b\u80fd\u529b\uff0c\u8bbe\u8ba1\u7edf\u4e00\u6a21\u578b\u5904\u7406\u591a\u6837\u5316\u63d0\u793a\uff0c\u5e76\u5f15\u5165\u6761\u4ef6\u6ce8\u5165\u6a21\u5757\u548c\u6e10\u8fdb\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMV-AR\u80fd\u751f\u6210\u4e00\u81f4\u7684\u591a\u89c6\u56fe\u56fe\u50cf\uff0c\u6027\u80fd\u4e0e\u9886\u5148\u7684\u6269\u6563\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "MV-AR\u5728\u591a\u89c6\u56fe\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5f00\u6e90\u3002"}}
{"id": "2506.18564", "pdf": "https://arxiv.org/pdf/2506.18564", "abs": "https://arxiv.org/abs/2506.18564", "authors": ["Xuanyu Zhang", "Weiqi Li", "Shijie Zhao", "Junlin Li", "Li Zhang", "Jian Zhang"], "title": "VQ-Insight: Teaching VLMs for AI-Generated Video Quality Understanding via Progressive Visual Reinforcement Learning", "categories": ["cs.CV"], "comment": "Technical Report", "summary": "Recent advances in AI-generated content (AIGC) have led to the emergence of powerful text-to-video generation models. Despite these successes, evaluating the quality of AIGC-generated videos remains challenging due to limited generalization, lack of temporal awareness, heavy reliance on large-scale annotated datasets, and the lack of effective interaction with generation models. Most current approaches rely on supervised finetuning of vision-language models (VLMs), which often require large-scale annotated datasets and tend to decouple understanding and generation. To address these shortcomings, we propose VQ-Insight, a novel reasoning-style VLM framework for AIGC video quality assessment. Our approach features: (1) a progressive video quality learning scheme that combines image quality warm-up, general task-specific temporal learning, and joint optimization with the video generation model; (2) the design of multi-dimension scoring rewards, preference comparison rewards, and temporal modeling rewards to enhance both generalization and specialization in video quality evaluation. Extensive experiments demonstrate that VQ-Insight consistently outperforms state-of-the-art baselines in preference comparison, multi-dimension scoring, and natural video scoring, bringing significant improvements for video generation tasks.", "AI": {"tldr": "VQ-Insight\u662f\u4e00\u4e2a\u65b0\u578b\u7684\u63a8\u7406\u5f0f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u751f\u6210\u89c6\u9891\u7684\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524dAI\u751f\u6210\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3001\u7f3a\u4e4f\u65f6\u95f4\u611f\u77e5\u3001\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faVQ-Insight\u6846\u67b6\uff0c\u5305\u62ec\u6e10\u8fdb\u5f0f\u89c6\u9891\u8d28\u91cf\u5b66\u4e60\u65b9\u6848\u548c\u591a\u7ef4\u5ea6\u8bc4\u5206\u5956\u52b1\u8bbe\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660eVQ-Insight\u5728\u591a\u7ef4\u5ea6\u8bc4\u5206\u548c\u81ea\u7136\u89c6\u9891\u8bc4\u5206\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "VQ-Insight\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u4efb\u52a1\u7684\u8d28\u91cf\u8bc4\u4f30\u6548\u679c\u3002"}}
{"id": "2506.18569", "pdf": "https://arxiv.org/pdf/2506.18569", "abs": "https://arxiv.org/abs/2506.18569", "authors": ["Oleh Kuzyk", "Zuoyue Li", "Marc Pollefeys", "Xi Wang"], "title": "VisualChef: Generating Visual Aids in Cooking via Mask Inpainting", "categories": ["cs.CV"], "comment": null, "summary": "Cooking requires not only following instructions but also understanding, executing, and monitoring each step - a process that can be challenging without visual guidance. Although recipe images and videos offer helpful cues, they often lack consistency in focus, tools, and setup. To better support the cooking process, we introduce VisualChef, a method for generating contextual visual aids tailored to cooking scenarios. Given an initial frame and a specified action, VisualChef generates images depicting both the action's execution and the resulting appearance of the object, while preserving the initial frame's environment. Previous work aims to integrate knowledge extracted from large language models by generating detailed textual descriptions to guide image generation, which requires fine-grained visual-textual alignment and involves additional annotations. In contrast, VisualChef simplifies alignment through mask-based visual grounding. Our key insight is identifying action-relevant objects and classifying them to enable targeted modifications that reflect the intended action and outcome while maintaining a consistent environment. In addition, we propose an automated pipeline to extract high-quality initial, action, and final state frames. We evaluate VisualChef quantitatively and qualitatively on three egocentric video datasets and show its improvements over state-of-the-art methods.", "AI": {"tldr": "VisualChef\u662f\u4e00\u79cd\u751f\u6210\u70f9\u996a\u573a\u666f\u4e0a\u4e0b\u6587\u89c6\u89c9\u8f85\u52a9\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u63a9\u7801\u7684\u89c6\u89c9\u5b9a\u4f4d\u7b80\u5316\u5bf9\u9f50\uff0c\u751f\u6210\u6267\u884c\u52a8\u4f5c\u548c\u7ed3\u679c\u7684\u56fe\u50cf\u3002", "motivation": "\u70f9\u996a\u8fc7\u7a0b\u4e2d\u7f3a\u4e4f\u4e00\u81f4\u7684\u89c6\u89c9\u6307\u5bfc\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u7684\u6587\u672c-\u89c6\u89c9\u5bf9\u9f50\uff0cVisualChef\u65e8\u5728\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u52a8\u4f5c\u76f8\u5173\u5bf9\u8c61\u5e76\u5206\u7c7b\uff0c\u8fdb\u884c\u9488\u5bf9\u6027\u4fee\u6539\uff0c\u540c\u65f6\u4fdd\u6301\u73af\u5883\u4e00\u81f4\uff1b\u63d0\u51fa\u81ea\u52a8\u5316\u6d41\u7a0b\u63d0\u53d6\u9ad8\u8d28\u91cf\u5e27\u3002", "result": "\u5728\u4e09\u4e2a\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "VisualChef\u901a\u8fc7\u7b80\u5316\u89c6\u89c9-\u6587\u672c\u5bf9\u9f50\uff0c\u6709\u6548\u751f\u6210\u70f9\u996a\u573a\u666f\u7684\u89c6\u89c9\u8f85\u52a9\u3002"}}
{"id": "2506.18575", "pdf": "https://arxiv.org/pdf/2506.18575", "abs": "https://arxiv.org/abs/2506.18575", "authors": ["Kaifeng Sheng", "Zheng Zhou", "Yingliang Peng", "Qianwei Wang"], "title": "2D Triangle Splatting for Direct Differentiable Mesh Training", "categories": ["cs.CV"], "comment": "13 pages, 8 figures", "summary": "Differentiable rendering with 3D Gaussian primitives has emerged as a powerful method for reconstructing high-fidelity 3D scenes from multi-view images. While it offers improvements over NeRF-based methods, this representation still encounters challenges with rendering speed and advanced rendering effects, such as relighting and shadow rendering, compared to mesh-based models. In this paper, we propose 2D Triangle Splatting (2DTS), a novel method that replaces 3D Gaussian primitives with 2D triangle facelets. This representation naturally forms a discrete mesh-like structure while retaining the benefits of continuous volumetric modeling. By incorporating a compactness parameter into the triangle primitives, we enable direct training of photorealistic meshes. Our experimental results demonstrate that our triangle-based method, in its vanilla version (without compactness tuning), achieves higher fidelity compared to state-of-the-art Gaussian-based methods. Furthermore, our approach produces reconstructed meshes with superior visual quality compared to existing mesh reconstruction methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a2D Triangle Splatting\uff082DTS\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u75282D\u4e09\u89d2\u5f62\u9762\u7247\u66ff\u4ee33D\u9ad8\u65af\u57fa\u5143\uff0c\u4ee5\u63d0\u5347\u6e32\u67d3\u901f\u5ea6\u548c\u9ad8\u7ea7\u6e32\u67d3\u6548\u679c\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e3D\u9ad8\u65af\u57fa\u5143\u7684\u53ef\u5fae\u5206\u6e32\u67d3\u5728\u591a\u89c6\u89d2\u56fe\u50cf\u91cd\u5efa\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6e32\u67d3\u901f\u5ea6\u548c\u9ad8\u7ea7\u6548\u679c\uff08\u5982\u91cd\u5149\u7167\u548c\u9634\u5f71\u6e32\u67d3\uff09\u4e0a\u4ecd\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u75282D\u4e09\u89d2\u5f62\u9762\u7247\u4f5c\u4e3a\u57fa\u5143\uff0c\u7ed3\u5408\u7d27\u51d1\u6027\u53c2\u6570\uff0c\u76f4\u63a5\u8bad\u7ec3\u51fa\u903c\u771f\u7684\u7f51\u683c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u672a\u4f18\u5316\u7d27\u51d1\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u6bd4\u73b0\u6709\u9ad8\u65af\u57fa\u5143\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u4fdd\u771f\u5ea6\uff0c\u4e14\u91cd\u5efa\u7f51\u683c\u7684\u89c6\u89c9\u8d28\u91cf\u66f4\u4f18\u3002", "conclusion": "2DTS\u65b9\u6cd5\u5728\u4fdd\u7559\u8fde\u7eed\u4f53\u79ef\u5efa\u6a21\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u901f\u5ea6\u548c\u6548\u679c\uff0c\u4e3a\u9ad8\u8d28\u91cf3D\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.18587", "pdf": "https://arxiv.org/pdf/2506.18587", "abs": "https://arxiv.org/abs/2506.18587", "authors": ["Antoine Saget", "Baptiste Lafabregue", "Antoine Cornu\u00e9jols", "Pierre Gan\u00e7arski"], "title": "Resampling Augmentation for Time Series Contrastive Learning: Application to Remote Sensing", "categories": ["cs.CV"], "comment": "10 pages, 2 figures, accepted at 42nd International Conference on   Machine Learning (ICML 2025) Terrabytes workshop", "summary": "Given the abundance of unlabeled Satellite Image Time Series (SITS) and the scarcity of labeled data, contrastive self-supervised pretraining emerges as a natural tool to leverage this vast quantity of unlabeled data. However, designing effective data augmentations for contrastive learning remains challenging for time series. We introduce a novel resampling-based augmentation strategy that generates positive pairs by upsampling time series and extracting disjoint subsequences while preserving temporal coverage. We validate our approach on multiple agricultural classification benchmarks using Sentinel-2 imagery, showing that it outperforms common alternatives such as jittering, resizing, and masking. Further, we achieve state-of-the-art performance on the S2-Agri100 dataset without employing spatial information or temporal encodings, surpassing more complex masked-based SSL frameworks. Our method offers a simple, yet effective, contrastive learning augmentation for remote sensing time series.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cd\u91c7\u6837\u7684\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u7b56\u7565\uff0c\u7528\u4e8e\u536b\u661f\u56fe\u50cf\u65f6\u95f4\u5e8f\u5217\uff08SITS\uff09\uff0c\u5728\u519c\u4e1a\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5e38\u89c1\u65b9\u6cd5\u3002", "motivation": "\u5229\u7528\u5927\u91cf\u672a\u6807\u8bb0\u7684\u536b\u661f\u56fe\u50cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u89e3\u51b3\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u540c\u65f6\u514b\u670d\u65f6\u95f4\u5e8f\u5217\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u6570\u636e\u589e\u5f3a\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u91cd\u91c7\u6837\u7b56\u7565\u751f\u6210\u6b63\u6837\u672c\u5bf9\uff0c\u901a\u8fc7\u4e0a\u91c7\u6837\u65f6\u95f4\u5e8f\u5217\u5e76\u63d0\u53d6\u4e0d\u91cd\u53e0\u7684\u5b50\u5e8f\u5217\uff0c\u4fdd\u6301\u65f6\u95f4\u8986\u76d6\u8303\u56f4\u3002", "result": "\u5728\u591a\u4e2a\u519c\u4e1a\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u5e38\u89c1\u65b9\u6cd5\uff08\u5982\u6296\u52a8\u3001\u8c03\u6574\u5927\u5c0f\u548c\u63a9\u7801\uff09\uff0c\u5e76\u5728S2-Agri100\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9065\u611f\u65f6\u95f4\u5e8f\u5217\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u7b56\u7565\u3002"}}
{"id": "2506.18655", "pdf": "https://arxiv.org/pdf/2506.18655", "abs": "https://arxiv.org/abs/2506.18655", "authors": ["Wenxu Qian", "Chaoyue Wang", "Hou Peng", "Zhiyu Tan", "Hao Li", "Anxiang Zeng"], "title": "RDPO: Real Data Preference Optimization for Physics Consistency Video Generation", "categories": ["cs.CV", "I.2.6; I.2.10"], "comment": "16 pages, 10 figures", "summary": "Video generation techniques have achieved remarkable advancements in visual quality, yet faithfully reproducing real-world physics remains elusive. Preference-based model post-training may improve physical consistency, but requires costly human-annotated datasets or reward models that are not yet feasible. To address these challenges, we present Real Data Preference Optimisation (RDPO), an annotation-free framework that distills physical priors directly from real-world videos. Specifically, the proposed RDPO reverse-samples real video sequences with a pre-trained generator to automatically build preference pairs that are statistically distinguishable in terms of physical correctness. A multi-stage iterative training schedule then guides the generator to obey physical laws increasingly well. Benefiting from the dynamic information explored from real videos, our proposed RDPO significantly improves the action coherence and physical realism of the generated videos. Evaluations on multiple benchmarks and human evaluations have demonstrated that RDPO achieves improvements across multiple dimensions. The source code and demonstration of this paper are available at: https://wwenxu.github.io/RDPO/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u7684\u6846\u67b6RDPO\uff0c\u901a\u8fc7\u4ece\u771f\u5b9e\u89c6\u9891\u4e2d\u63d0\u53d6\u7269\u7406\u5148\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u89c6\u9891\u7684\u52a8\u4f5c\u8fde\u8d2f\u6027\u548c\u7269\u7406\u771f\u5b9e\u611f\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u751f\u6210\u6280\u672f\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u771f\u5b9e\u7269\u7406\u6a21\u62df\u65b9\u9762\u4ecd\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u57fa\u4e8e\u504f\u597d\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u6216\u5956\u52b1\u6a21\u578b\u3002", "method": "RDPO\u901a\u8fc7\u53cd\u5411\u91c7\u6837\u771f\u5b9e\u89c6\u9891\u5e8f\u5217\u81ea\u52a8\u6784\u5efa\u504f\u597d\u5bf9\uff0c\u5e76\u91c7\u7528\u591a\u9636\u6bb5\u8fed\u4ee3\u8bad\u7ec3\u8ba1\u5212\u9010\u6b65\u63d0\u5347\u751f\u6210\u5668\u7684\u7269\u7406\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u4eba\u5de5\u8bc4\u4f30\u4e2d\uff0cRDPO\u5728\u52a8\u4f5c\u8fde\u8d2f\u6027\u548c\u7269\u7406\u771f\u5b9e\u611f\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "RDPO\u901a\u8fc7\u76f4\u63a5\u4ece\u771f\u5b9e\u89c6\u9891\u4e2d\u63d0\u53d6\u7269\u7406\u5148\u9a8c\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u89c6\u9891\u7684\u7269\u7406\u4e00\u81f4\u6027\u3002"}}
{"id": "2506.18677", "pdf": "https://arxiv.org/pdf/2506.18677", "abs": "https://arxiv.org/abs/2506.18677", "authors": ["Adam Yang", "Nadula Kadawedduwa", "Tianfu Wang", "Maria Molina", "Christopher Metzler"], "title": "Reconstructing Tornadoes in 3D with Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Accurately reconstructing the 3D structure of tornadoes is critically important for understanding and preparing for this highly destructive weather phenomenon. While modern 3D scene reconstruction techniques, such as 3D Gaussian splatting (3DGS), could provide a valuable tool for reconstructing the 3D structure of tornados, at present we are critically lacking a controlled tornado dataset with which to develop and validate these tools. In this work we capture and release a novel multiview dataset of a small lab-based tornado. We demonstrate one can effectively reconstruct and visualize the 3D structure of this tornado using 3DGS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9e\u9a8c\u5ba4\u7684\u5c0f\u578b\u9f99\u5377\u98ce\u591a\u89c6\u89d2\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u4f7f\u75283D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u6280\u672f\u6709\u6548\u91cd\u5efa\u51763D\u7ed3\u6784\u7684\u65b9\u6cd5\u3002", "motivation": "\u51c6\u786e\u91cd\u5efa\u9f99\u5377\u98ce\u76843D\u7ed3\u6784\u5bf9\u7406\u89e3\u548c\u5e94\u5bf9\u8fd9\u4e00\u7834\u574f\u6027\u5929\u6c14\u73b0\u8c61\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7528\u4e8e\u5f00\u53d1\u548c\u9a8c\u8bc1\u76f8\u5173\u5de5\u5177\u7684\u53d7\u63a7\u6570\u636e\u96c6\u3002", "method": "\u6355\u83b7\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u5b9e\u9a8c\u5ba4\u5c0f\u578b\u9f99\u5377\u98ce\u7684\u591a\u89c6\u89d2\u6570\u636e\u96c6\uff0c\u5e76\u5e94\u75283D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u6280\u672f\u8fdb\u884c3D\u91cd\u5efa\u3002", "result": "\u6210\u529f\u91cd\u5efa\u5e76\u53ef\u89c6\u5316\u4e86\u5b9e\u9a8c\u5ba4\u9f99\u5377\u98ce\u76843D\u7ed3\u6784\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u4e3a\u672a\u6765\u7814\u7a76\u9f99\u5377\u98ce\u76843D\u7ed3\u6784\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u548c\u57fa\u51c6\u3002"}}
{"id": "2506.18678", "pdf": "https://arxiv.org/pdf/2506.18678", "abs": "https://arxiv.org/abs/2506.18678", "authors": ["Tianchen Deng", "Guole Shen", "Xun Chen", "Shenghai Yuan", "Hongming Shen", "Guohao Peng", "Zhenyu Wu", "Jingchuan Wang", "Lihua Xie", "Danwei Wang", "Hesheng Wang", "Weidong Chen"], "title": "MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Neural implicit scene representations have recently shown promising results in dense visual SLAM. However, existing implicit SLAM algorithms are constrained to single-agent scenarios, and fall difficulties in large-scale scenes and long sequences. Existing NeRF-based multi-agent SLAM frameworks cannot meet the constraints of communication bandwidth. To this end, we propose the first distributed multi-agent collaborative neural SLAM framework with hybrid scene representation, distributed camera tracking, intra-to-inter loop closure, and online distillation for multiple submap fusion. A novel triplane-grid joint scene representation method is proposed to improve scene reconstruction. A novel intra-to-inter loop closure method is designed to achieve local (single-agent) and global (multi-agent) consistency. We also design a novel online distillation method to fuse the information of different submaps to achieve global consistency. Furthermore, to the best of our knowledge, there is no real-world dataset for NeRF-based/GS-based SLAM that provides both continuous-time trajectories groundtruth and high-accuracy 3D meshes groundtruth. To this end, we propose the first real-world Dense slam (DES) dataset covering both single-agent and multi-agent scenarios, ranging from small rooms to large-scale outdoor scenes, with high-accuracy ground truth for both 3D mesh and continuous-time camera trajectory. This dataset can advance the development of the research in both SLAM, 3D reconstruction, and visual foundation model. Experiments on various datasets demonstrate the superiority of the proposed method in both mapping, tracking, and communication. The dataset and code will open-source on https://github.com/dtc111111/mcnslam.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u795e\u7ecfSLAM\u6846\u67b6\uff0c\u7ed3\u5408\u6df7\u5408\u573a\u666f\u8868\u793a\u3001\u5206\u5e03\u5f0f\u76f8\u673a\u8ddf\u8e2a\u3001\u5c40\u90e8\u5230\u5168\u5c40\u95ed\u73af\u4ee5\u53ca\u5728\u7ebf\u84b8\u998f\u6280\u672f\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u573a\u666f\u548c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u9690\u5f0fSLAM\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u667a\u80fd\u4f53\u573a\u666f\uff0c\u4e14\u5728\u5927\u89c4\u6a21\u573a\u666f\u548c\u957f\u5e8f\u5217\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u57fa\u4e8eNeRF\u7684\u591a\u667a\u80fd\u4f53SLAM\u6846\u67b6\u65e0\u6cd5\u6ee1\u8db3\u901a\u4fe1\u5e26\u5bbd\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e09\u5e73\u9762\u7f51\u683c\u8054\u5408\u573a\u666f\u8868\u793a\u65b9\u6cd5\u3001\u5c40\u90e8\u5230\u5168\u5c40\u95ed\u73af\u65b9\u6cd5\u4ee5\u53ca\u5728\u7ebf\u84b8\u998f\u6280\u672f\uff0c\u7528\u4e8e\u591a\u5b50\u5730\u56fe\u878d\u5408\u548c\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u6620\u5c04\u3001\u8ddf\u8e2a\u548c\u901a\u4fe1\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u5305\u542b\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u573a\u666f\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u591a\u667a\u80fd\u4f53\u534f\u4f5cSLAM\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u65b0\u6570\u636e\u96c6\u5c06\u63a8\u52a8SLAM\u30013D\u91cd\u5efa\u548c\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u7814\u7a76\u3002"}}
{"id": "2506.18701", "pdf": "https://arxiv.org/pdf/2506.18701", "abs": "https://arxiv.org/abs/2506.18701", "authors": ["Yifan Zhang", "Chunli Peng", "Boyang Wang", "Puyi Wang", "Qingcheng Zhu", "Fei Kang", "Biao Jiang", "Zedong Gao", "Eric Li", "Yang Liu", "Yahui Zhou"], "title": "Matrix-Game: Interactive World Foundation Model", "categories": ["cs.CV", "cs.AI"], "comment": "Technical Report", "summary": "We introduce Matrix-Game, an interactive world foundation model for controllable game world generation. Matrix-Game is trained using a two-stage pipeline that first performs large-scale unlabeled pretraining for environment understanding, followed by action-labeled training for interactive video generation. To support this, we curate Matrix-Game-MC, a comprehensive Minecraft dataset comprising over 2,700 hours of unlabeled gameplay video clips and over 1,000 hours of high-quality labeled clips with fine-grained keyboard and mouse action annotations. Our model adopts a controllable image-to-world generation paradigm, conditioned on a reference image, motion context, and user actions. With over 17 billion parameters, Matrix-Game enables precise control over character actions and camera movements, while maintaining high visual quality and temporal coherence. To evaluate performance, we develop GameWorld Score, a unified benchmark measuring visual quality, temporal quality, action controllability, and physical rule understanding for Minecraft world generation. Extensive experiments show that Matrix-Game consistently outperforms prior open-source Minecraft world models (including Oasis and MineWorld) across all metrics, with particularly strong gains in controllability and physical consistency. Double-blind human evaluations further confirm the superiority of Matrix-Game, highlighting its ability to generate perceptually realistic and precisely controllable videos across diverse game scenarios. To facilitate future research on interactive image-to-world generation, we will open-source the Matrix-Game model weights and the GameWorld Score benchmark at https://github.com/SkyworkAI/Matrix-Game.", "AI": {"tldr": "Matrix-Game\u662f\u4e00\u4e2a\u7528\u4e8e\u53ef\u63a7\u6e38\u620f\u4e16\u754c\u751f\u6210\u7684\u4ea4\u4e92\u5f0f\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5b9e\u73b0\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\uff0c\u5e76\u5728Minecraft\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u7cbe\u786e\u63a7\u5236\u89d2\u8272\u52a8\u4f5c\u548c\u76f8\u673a\u79fb\u52a8\u7684\u6e38\u620f\u4e16\u754c\u751f\u6210\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u89c6\u89c9\u8d28\u91cf\u548c\u65f6\u95f4\u8fde\u8d2f\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u9884\u8bad\u7ec3\u548c\u73af\u5883\u7406\u89e3\uff0c\u968f\u540e\u8fdb\u884c\u5e26\u6807\u7b7e\u7684\u4ea4\u4e92\u5f0f\u89c6\u9891\u751f\u6210\u8bad\u7ec3\u3002", "result": "Matrix-Game\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u65f6\u95f4\u8d28\u91cf\u3001\u52a8\u4f5c\u53ef\u63a7\u6027\u548c\u7269\u7406\u89c4\u5219\u7406\u89e3\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u53ef\u63a7\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "Matrix-Game\u4e3a\u4ea4\u4e92\u5f0f\u56fe\u50cf\u5230\u4e16\u754c\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5f00\u6e90\u6a21\u578b\u548c\u8bc4\u6d4b\u57fa\u51c6\u4ee5\u63a8\u52a8\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2506.18792", "pdf": "https://arxiv.org/pdf/2506.18792", "abs": "https://arxiv.org/abs/2506.18792", "authors": ["Michal Nazarczuk", "Sibi Catley-Chandar", "Thomas Tanay", "Zhensong Zhang", "Gregory Slabaugh", "Eduardo P\u00e9rez-Pellitero"], "title": "ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs", "categories": ["cs.CV"], "comment": null, "summary": "Dynamic Novel View Synthesis aims to generate photorealistic views of moving subjects from arbitrary viewpoints. This task is particularly challenging when relying on monocular video, where disentangling structure from motion is ill-posed and supervision is scarce. We introduce Video Diffusion-Aware Reconstruction (ViDAR), a novel 4D reconstruction framework that leverages personalised diffusion models to synthesise a pseudo multi-view supervision signal for training a Gaussian splatting representation. By conditioning on scene-specific features, ViDAR recovers fine-grained appearance details while mitigating artefacts introduced by monocular ambiguity. To address the spatio-temporal inconsistency of diffusion-based supervision, we propose a diffusion-aware loss function and a camera pose optimisation strategy that aligns synthetic views with the underlying scene geometry. Experiments on DyCheck, a challenging benchmark with extreme viewpoint variation, show that ViDAR outperforms all state-of-the-art baselines in visual quality and geometric consistency. We further highlight ViDAR's strong improvement over baselines on dynamic regions and provide a new benchmark to compare performance in reconstructing motion-rich parts of the scene. Project page: https://vidar-4d.github.io", "AI": {"tldr": "ViDAR\u5229\u7528\u4e2a\u6027\u5316\u6269\u6563\u6a21\u578b\u751f\u6210\u4f2a\u591a\u89c6\u89d2\u76d1\u7763\u4fe1\u53f7\uff0c\u901a\u8fc7\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u8bad\u7ec3\uff0c\u63d0\u5347\u52a8\u6001\u573a\u666f\u4e0b\u7684\u65b0\u89c6\u89d2\u5408\u6210\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u5355\u76ee\u89c6\u9891\u4e2d\u52a8\u6001\u573a\u666f\u65b0\u89c6\u89d2\u5408\u6210\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u7ed3\u6784-\u8fd0\u52a8\u89e3\u8026\u7684\u6a21\u7cca\u6027\u548c\u76d1\u7763\u4fe1\u53f7\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faViDAR\u6846\u67b6\uff0c\u7ed3\u5408\u4e2a\u6027\u5316\u6269\u6563\u6a21\u578b\u751f\u6210\u4f2a\u591a\u89c6\u89d2\u76d1\u7763\u4fe1\u53f7\uff0c\u5e76\u5f15\u5165\u6269\u6563\u611f\u77e5\u635f\u5931\u51fd\u6570\u548c\u76f8\u673a\u4f4d\u59ff\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728DyCheck\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cViDAR\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u52a8\u6001\u533a\u57df\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "ViDAR\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u573a\u666f\u65b0\u89c6\u89d2\u5408\u6210\u7684\u6027\u80fd\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2506.18851", "pdf": "https://arxiv.org/pdf/2506.18851", "abs": "https://arxiv.org/abs/2506.18851", "authors": ["Zhuowei Chen", "Bingchuan Li", "Tianxiang Ma", "Lijie Liu", "Mingcong Liu", "Yi Zhang", "Gen Li", "Xinghui Li", "Siyu Zhou", "Qian He", "Xinglong Wu"], "title": "Phantom-Data : Towards a General Subject-Consistent Video Generation Dataset", "categories": ["cs.CV"], "comment": "Project page:https://phantom-video.github.io/Phantom-Data/", "summary": "Subject-to-video generation has witnessed substantial progress in recent years. However, existing models still face significant challenges in faithfully following textual instructions. This limitation, commonly known as the copy-paste problem, arises from the widely used in-pair training paradigm. This approach inherently entangles subject identity with background and contextual attributes by sampling reference images from the same scene as the target video. To address this issue, we introduce \\textbf{Phantom-Data, the first general-purpose cross-pair subject-to-video consistency dataset}, containing approximately one million identity-consistent pairs across diverse categories. Our dataset is constructed via a three-stage pipeline: (1) a general and input-aligned subject detection module, (2) large-scale cross-context subject retrieval from more than 53 million videos and 3 billion images, and (3) prior-guided identity verification to ensure visual consistency under contextual variation. Comprehensive experiments show that training with Phantom-Data significantly improves prompt alignment and visual quality while preserving identity consistency on par with in-pair baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Phantom-Data\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4e3b\u9898\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\u7684\u590d\u5236\u7c98\u8d34\u95ee\u9898\uff0c\u901a\u8fc7\u8de8\u573a\u666f\u6570\u636e\u63d0\u5347\u6587\u672c\u6307\u4ee4\u7684\u5fe0\u5b9e\u5ea6\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u4e3b\u9898\u5230\u89c6\u9891\u751f\u6210\u4e2d\u5b58\u5728\u590d\u5236\u7c98\u8d34\u95ee\u9898\uff0c\u539f\u56e0\u662f\u8bad\u7ec3\u65f6\u4f7f\u7528\u540c\u573a\u666f\u53c2\u8003\u56fe\u50cf\uff0c\u5bfc\u81f4\u4e3b\u9898\u8eab\u4efd\u4e0e\u80cc\u666f\u5c5e\u6027\u7ea0\u7f20\u3002", "method": "\u6784\u5efaPhantom-Data\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a\u4e3b\u9898\u68c0\u6d4b\u3001\u8de8\u4e0a\u4e0b\u6587\u4e3b\u9898\u68c0\u7d22\u548c\u8eab\u4efd\u9a8c\u8bc1\uff0c\u786e\u4fdd\u89c6\u89c9\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528Phantom-Data\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u63d0\u793a\u5bf9\u9f50\u548c\u89c6\u89c9\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u3002", "conclusion": "Phantom-Data\u662f\u9996\u4e2a\u901a\u7528\u8de8\u573a\u666f\u4e3b\u9898\u5230\u89c6\u9891\u4e00\u81f4\u6027\u6570\u636e\u96c6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2506.18862", "pdf": "https://arxiv.org/pdf/2506.18862", "abs": "https://arxiv.org/abs/2506.18862", "authors": ["Zhongbin Guo", "Yuhao Wang", "Ping Jian", "Xinyue Chen", "Wei Peng", "Ertai E"], "title": "TAMMs: Temporal-Aware Multimodal Model for Satellite Image Change Understanding and Forecasting", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to the 33rd ACM International Conference on Multimedia. Our   dataset can be found at https://huggingface.co/datasets/IceInPot/TAMMs", "summary": "Satellite image time-series analysis demands fine-grained spatial-temporal reasoning, which remains a challenge for existing multimodal large language models (MLLMs). In this work, we study the capabilities of MLLMs on a novel task that jointly targets temporal change understanding and future scene generation, aiming to assess their potential for modeling complex multimodal dynamics over time. We propose TAMMs, a Temporal-Aware Multimodal Model for satellite image change understanding and forecasting, which enhances frozen MLLMs with lightweight temporal modules for structured sequence encoding and contextual prompting. To guide future image generation, TAMMs introduces a Semantic-Fused Control Injection (SFCI) mechanism that adaptively combines high-level semantic reasoning and structural priors within an enhanced ControlNet. This dual-path conditioning enables temporally consistent and semantically grounded image synthesis. Experiments demonstrate that TAMMs outperforms strong MLLM baselines in both temporal change understanding and future image forecasting tasks, highlighting how carefully designed temporal reasoning and semantic fusion can unlock the full potential of MLLMs for spatio-temporal understanding.", "AI": {"tldr": "TAMMs\u6a21\u578b\u901a\u8fc7\u8f7b\u91cf\u7ea7\u65f6\u5e8f\u6a21\u5757\u548c\u8bed\u4e49\u878d\u5408\u673a\u5236\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u536b\u661f\u56fe\u50cf\u65f6\u5e8f\u5206\u6790\u548c\u672a\u6765\u573a\u666f\u751f\u6210\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u536b\u661f\u56fe\u50cf\u65f6\u5e8f\u5206\u6790\u4e2d\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\uff0c\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30MLLMs\u5728\u590d\u6742\u591a\u6a21\u6001\u52a8\u6001\u5efa\u6a21\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faTAMMs\u6a21\u578b\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u65f6\u5e8f\u6a21\u5757\u548c\u8bed\u4e49\u878d\u5408\u63a7\u5236\u6ce8\u5165\u673a\u5236\uff08SFCI\uff09\uff0c\u589e\u5f3aMLLMs\u7684\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTAMMs\u5728\u65f6\u5e8f\u53d8\u5316\u7406\u89e3\u548c\u672a\u6765\u56fe\u50cf\u9884\u6d4b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u65f6\u5e8f\u63a8\u7406\u548c\u8bed\u4e49\u878d\u5408\u80fd\u5145\u5206\u53d1\u6325MLLMs\u5728\u65f6\u7a7a\u7406\u89e3\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.18866", "pdf": "https://arxiv.org/pdf/2506.18866", "abs": "https://arxiv.org/abs/2506.18866", "authors": ["Qijun Gan", "Ruizi Yang", "Jianke Zhu", "Shaofei Xue", "Steven Hoi"], "title": "OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Project page: https://omni-avatar.github.io/", "summary": "Significant progress has been made in audio-driven human animation, while most existing methods focus mainly on facial movements, limiting their ability to create full-body animations with natural synchronization and fluidity. They also struggle with precise prompt control for fine-grained generation. To tackle these challenges, we introduce OmniAvatar, an innovative audio-driven full-body video generation model that enhances human animation with improved lip-sync accuracy and natural movements. OmniAvatar introduces a pixel-wise multi-hierarchical audio embedding strategy to better capture audio features in the latent space, enhancing lip-syncing across diverse scenes. To preserve the capability for prompt-driven control of foundation models while effectively incorporating audio features, we employ a LoRA-based training approach. Extensive experiments show that OmniAvatar surpasses existing models in both facial and semi-body video generation, offering precise text-based control for creating videos in various domains, such as podcasts, human interactions, dynamic scenes, and singing. Our project page is https://omni-avatar.github.io/.", "AI": {"tldr": "OmniAvatar\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u97f3\u9891\u9a71\u52a8\u5168\u8eab\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u5c42\u7ea7\u97f3\u9891\u5d4c\u5165\u7b56\u7565\u548cLoRA\u8bad\u7ec3\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u5507\u540c\u6b65\u548c\u81ea\u7136\u52a8\u4f5c\uff0c\u540c\u65f6\u652f\u6301\u7cbe\u786e\u7684\u6587\u672c\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9762\u90e8\u52a8\u4f5c\uff0c\u96be\u4ee5\u751f\u6210\u81ea\u7136\u540c\u6b65\u7684\u5168\u8eab\u52a8\u753b\uff0c\u4e14\u7f3a\u4e4f\u7cbe\u7ec6\u7684\u63d0\u793a\u63a7\u5236\u80fd\u529b\u3002", "method": "\u91c7\u7528\u50cf\u7d20\u7ea7\u591a\u5c42\u7ea7\u97f3\u9891\u5d4c\u5165\u7b56\u7565\u548cLoRA\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7ed3\u5408\u97f3\u9891\u7279\u5f81\u548c\u57fa\u7840\u6a21\u578b\u7684\u63d0\u793a\u63a7\u5236\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOmniAvatar\u5728\u9762\u90e8\u548c\u534a\u8eab\u89c6\u9891\u751f\u6210\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u652f\u6301\u591a\u79cd\u573a\u666f\u7684\u7cbe\u786e\u63a7\u5236\u3002", "conclusion": "OmniAvatar\u5728\u97f3\u9891\u9a71\u52a8\u52a8\u753b\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u540c\u6b65\u6027\u548c\u7075\u6d3b\u6027\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u6587\u672c\u63a7\u5236\u7684\u7cbe\u786e\u6027\u3002"}}
{"id": "2506.18871", "pdf": "https://arxiv.org/pdf/2506.18871", "abs": "https://arxiv.org/abs/2506.18871", "authors": ["Chenyuan Wu", "Pengfei Zheng", "Ruiran Yan", "Shitao Xiao", "Xin Luo", "Yueze Wang", "Wanli Li", "Xiyan Jiang", "Yexin Liu", "Junjie Zhou", "Ze Liu", "Ziyi Xia", "Chaofan Li", "Haoge Deng", "Jiahao Wang", "Kun Luo", "Bo Zhang", "Defu Lian", "Xinlong Wang", "Zhongyuan Wang", "Tiejun Huang", "Zheng Liu"], "title": "OmniGen2: Exploration to Advanced Multimodal Generation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "In this work, we introduce OmniGen2, a versatile and open-source generative model designed to provide a unified solution for diverse generation tasks, including text-to-image, image editing, and in-context generation. Unlike OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image modalities, utilizing unshared parameters and a decoupled image tokenizer. This design enables OmniGen2 to build upon existing multimodal understanding models without the need to re-adapt VAE inputs, thereby preserving the original text generation capabilities. To facilitate the training of OmniGen2, we developed comprehensive data construction pipelines, encompassing image editing and in-context generation data. Additionally, we introduce a reflection mechanism tailored for image generation tasks and curate a dedicated reflection dataset based on OmniGen2. Despite its relatively modest parameter size, OmniGen2 achieves competitive results on multiple task benchmarks, including text-to-image and image editing. To further evaluate in-context generation, also referred to as subject-driven tasks, we introduce a new benchmark named OmniContext. OmniGen2 achieves state-of-the-art performance among open-source models in terms of consistency. We will release our models, training code, datasets, and data construction pipeline to support future research in this field. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link: https://github.com/VectorSpaceLab/OmniGen2", "AI": {"tldr": "OmniGen2\u662f\u4e00\u4e2a\u591a\u529f\u80fd\u5f00\u6e90\u751f\u6210\u6a21\u578b\uff0c\u652f\u6301\u6587\u672c\u5230\u56fe\u50cf\u3001\u56fe\u50cf\u7f16\u8f91\u548c\u4e0a\u4e0b\u6587\u751f\u6210\u4efb\u52a1\uff0c\u901a\u8fc7\u53cc\u89e3\u7801\u8def\u5f84\u548c\u72ec\u7acb\u53c2\u6570\u8bbe\u8ba1\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4e3a\u89e3\u51b3\u591a\u4efb\u52a1\u751f\u6210\u6a21\u578b\u7684\u7edf\u4e00\u6027\u548c\u6027\u80fd\u95ee\u9898\uff0cOmniGen2\u5728OmniGen v1\u57fa\u7840\u4e0a\u6539\u8fdb\uff0c\u907f\u514d\u91cd\u65b0\u9002\u5e94VAE\u8f93\u5165\u5e76\u4fdd\u7559\u6587\u672c\u751f\u6210\u80fd\u529b\u3002", "method": "\u91c7\u7528\u53cc\u89e3\u7801\u8def\u5f84\uff08\u6587\u672c\u548c\u56fe\u50cf\u6a21\u6001\uff09\u3001\u72ec\u7acb\u53c2\u6570\u548c\u5206\u79bb\u7684\u56fe\u50cf\u6807\u8bb0\u5668\uff0c\u7ed3\u5408\u53cd\u5c04\u673a\u5236\u548c\u4e13\u7528\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u6587\u672c\u5230\u56fe\u50cf\u548c\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u65b0\u57fa\u51c6OmniContext\u4e2d\u8fbe\u5230\u5f00\u6e90\u6a21\u578b\u7684\u6700\u5148\u8fdb\u4e00\u81f4\u6027\u3002", "conclusion": "OmniGen2\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u548c\u6570\u636e\u652f\u6301\uff0c\u4e3a\u591a\u4efb\u52a1\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c06\u5f00\u6e90\u6a21\u578b\u548c\u5de5\u5177\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2506.18881", "pdf": "https://arxiv.org/pdf/2506.18881", "abs": "https://arxiv.org/abs/2506.18881", "authors": ["Xinyu Zhang", "Dong Gong", "Zicheng Duan", "Anton van den Hengel", "Lingqiao Liu"], "title": "Let Your Video Listen to Your Music!", "categories": ["cs.CV", "cs.MM"], "comment": "project page: https://zhangxinyu-xyz.github.io/MVAA/", "summary": "Aligning the rhythm of visual motion in a video with a given music track is a practical need in multimedia production, yet remains an underexplored task in autonomous video editing. Effective alignment between motion and musical beats enhances viewer engagement and visual appeal, particularly in music videos, promotional content, and cinematic editing. Existing methods typically depend on labor-intensive manual cutting, speed adjustments, or heuristic-based editing techniques to achieve synchronization. While some generative models handle joint video and music generation, they often entangle the two modalities, limiting flexibility in aligning video to music beats while preserving the full visual content. In this paper, we propose a novel and efficient framework, termed MVAA (Music-Video Auto-Alignment), that automatically edits video to align with the rhythm of a given music track while preserving the original visual content. To enhance flexibility, we modularize the task into a two-step process in our MVAA: aligning motion keyframes with audio beats, followed by rhythm-aware video inpainting. Specifically, we first insert keyframes at timestamps aligned with musical beats, then use a frame-conditioned diffusion model to generate coherent intermediate frames, preserving the original video's semantic content. Since comprehensive test-time training can be time-consuming, we adopt a two-stage strategy: pretraining the inpainting module on a small video set to learn general motion priors, followed by rapid inference-time fine-tuning for video-specific adaptation. This hybrid approach enables adaptation within 10 minutes with one epoch on a single NVIDIA 4090 GPU using CogVideoX-5b-I2V as the backbone. Extensive experiments show that our approach can achieve high-quality beat alignment and visual smoothness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMVAA\u7684\u6846\u67b6\uff0c\u81ea\u52a8\u5c06\u89c6\u9891\u4e0e\u97f3\u4e50\u8282\u62cd\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u7559\u539f\u59cb\u89c6\u89c9\u5185\u5bb9\u3002", "motivation": "\u591a\u5a92\u4f53\u5236\u4f5c\u4e2d\u9700\u8981\u5c06\u89c6\u9891\u8fd0\u52a8\u8282\u594f\u4e0e\u97f3\u4e50\u5bf9\u9f50\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6216\u542f\u53d1\u5f0f\u7f16\u8f91\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u6cd5\uff1a1) \u5c06\u8fd0\u52a8\u5173\u952e\u5e27\u4e0e\u97f3\u9891\u8282\u62cd\u5bf9\u9f50\uff1b2) \u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u8fde\u8d2f\u4e2d\u95f4\u5e27\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u9ad8\u8d28\u91cf\u5b9e\u73b0\u8282\u62cd\u5bf9\u9f50\u548c\u89c6\u89c9\u6d41\u7545\u6027\u3002", "conclusion": "MVAA\u6846\u67b6\u9ad8\u6548\u7075\u6d3b\uff0c\u9002\u7528\u4e8e\u89c6\u9891\u4e0e\u97f3\u4e50\u7684\u81ea\u52a8\u5bf9\u9f50\u3002"}}
{"id": "2506.18899", "pdf": "https://arxiv.org/pdf/2506.18899", "abs": "https://arxiv.org/abs/2506.18899", "authors": ["Kaiyi Huang", "Yukun Huang", "Xintao Wang", "Zinan Lin", "Xuefei Ning", "Pengfei Wan", "Di Zhang", "Yu Wang", "Xihui Liu"], "title": "FilMaster: Bridging Cinematic Principles and Generative AI for Automated Film Generation", "categories": ["cs.CV"], "comment": "Project Page: https://filmaster-ai.github.io/", "summary": "AI-driven content creation has shown potential in film production. However, existing film generation systems struggle to implement cinematic principles and thus fail to generate professional-quality films, particularly lacking diverse camera language and cinematic rhythm. This results in templated visuals and unengaging narratives. To address this, we introduce FilMaster, an end-to-end AI system that integrates real-world cinematic principles for professional-grade film generation, yielding editable, industry-standard outputs. FilMaster is built on two key principles: (1) learning cinematography from extensive real-world film data and (2) emulating professional, audience-centric post-production workflows. Inspired by these principles, FilMaster incorporates two stages: a Reference-Guided Generation Stage which transforms user input to video clips, and a Generative Post-Production Stage which transforms raw footage into audiovisual outputs by orchestrating visual and auditory elements for cinematic rhythm. Our generation stage highlights a Multi-shot Synergized RAG Camera Language Design module to guide the AI in generating professional camera language by retrieving reference clips from a vast corpus of 440,000 film clips. Our post-production stage emulates professional workflows by designing an Audience-Centric Cinematic Rhythm Control module, including Rough Cut and Fine Cut processes informed by simulated audience feedback, for effective integration of audiovisual elements to achieve engaging content. The system is empowered by generative AI models like (M)LLMs and video generation models. Furthermore, we introduce FilmEval, a comprehensive benchmark for evaluating AI-generated films. Extensive experiments show FilMaster's superior performance in camera language design and cinematic rhythm control, advancing generative AI in professional filmmaking.", "AI": {"tldr": "FilMaster\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684AI\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u771f\u5b9e\u4e16\u754c\u7684\u7535\u5f71\u539f\u5219\u751f\u6210\u4e13\u4e1a\u7ea7\u7535\u5f71\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u5728\u955c\u5934\u8bed\u8a00\u548c\u8282\u594f\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709AI\u7535\u5f71\u751f\u6210\u7cfb\u7edf\u7f3a\u4e4f\u591a\u6837\u5316\u7684\u955c\u5934\u8bed\u8a00\u548c\u7535\u5f71\u8282\u594f\uff0c\u5bfc\u81f4\u89c6\u89c9\u6548\u679c\u6a21\u677f\u5316\u548c\u53d9\u4e8b\u4e0d\u5438\u5f15\u4eba\u3002", "method": "FilMaster\u57fa\u4e8e\u4e24\u4e2a\u5173\u952e\u539f\u5219\uff1a\u4ece\u5927\u91cf\u7535\u5f71\u6570\u636e\u4e2d\u5b66\u4e60\u6444\u5f71\u6280\u672f\uff0c\u5e76\u6a21\u62df\u4ee5\u89c2\u4f17\u4e3a\u4e2d\u5fc3\u7684\u540e\u671f\u5236\u4f5c\u6d41\u7a0b\u3002\u7cfb\u7edf\u5206\u4e3a\u53c2\u8003\u5f15\u5bfc\u751f\u6210\u9636\u6bb5\u548c\u751f\u6210\u540e\u671f\u5236\u4f5c\u9636\u6bb5\u3002", "result": "FilMaster\u5728\u955c\u5934\u8bed\u8a00\u8bbe\u8ba1\u548c\u7535\u5f71\u8282\u594f\u63a7\u5236\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u901a\u8fc7FilmEval\u57fa\u51c6\u9a8c\u8bc1\u3002", "conclusion": "FilMaster\u63a8\u52a8\u4e86\u751f\u6210\u5f0fAI\u5728\u4e13\u4e1a\u7535\u5f71\u5236\u4f5c\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u4f9b\u4e86\u53ef\u7f16\u8f91\u3001\u884c\u4e1a\u6807\u51c6\u7684\u8f93\u51fa\u3002"}}
{"id": "2506.18900", "pdf": "https://arxiv.org/pdf/2506.18900", "abs": "https://arxiv.org/abs/2506.18900", "authors": ["Kiymet Akdemir", "Tahira Kazimi", "Pinar Yanardag"], "title": "Audit & Repair: An Agentic Framework for Consistent Story Visualization in Text-to-Image Diffusion Models", "categories": ["cs.CV"], "comment": "Project webpage: https://auditandrepair.github.io/", "summary": "Story visualization has become a popular task where visual scenes are generated to depict a narrative across multiple panels. A central challenge in this setting is maintaining visual consistency, particularly in how characters and objects persist and evolve throughout the story. Despite recent advances in diffusion models, current approaches often fail to preserve key character attributes, leading to incoherent narratives. In this work, we propose a collaborative multi-agent framework that autonomously identifies, corrects, and refines inconsistencies across multi-panel story visualizations. The agents operate in an iterative loop, enabling fine-grained, panel-level updates without re-generating entire sequences. Our framework is model-agnostic and flexibly integrates with a variety of diffusion models, including rectified flow transformers such as Flux and latent diffusion models such as Stable Diffusion. Quantitative and qualitative experiments show that our method outperforms prior approaches in terms of multi-panel consistency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6545\u4e8b\u53ef\u89c6\u5316\u4e2d\u89d2\u8272\u548c\u5bf9\u8c61\u7684\u4e00\u81f4\u6027\u4fdd\u6301\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u6a21\u578b\u5728\u6545\u4e8b\u53ef\u89c6\u5316\u4e2d\u96be\u4ee5\u4fdd\u6301\u89d2\u8272\u548c\u5bf9\u8c61\u7684\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u53d9\u4e8b\u4e0d\u8fde\u8d2f\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u5faa\u73af\u81ea\u4e3b\u8bc6\u522b\u3001\u4fee\u6b63\u548c\u4f18\u5316\u4e0d\u4e00\u81f4\u6027\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u9762\u677f\u7ea7\u66f4\u65b0\u3002", "result": "\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u9762\u677f\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6a21\u578b\u65e0\u5173\uff0c\u53ef\u7075\u6d3b\u96c6\u6210\u591a\u79cd\u6269\u6563\u6a21\u578b\uff0c\u6709\u6548\u63d0\u5347\u6545\u4e8b\u53ef\u89c6\u5316\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2506.18901", "pdf": "https://arxiv.org/pdf/2506.18901", "abs": "https://arxiv.org/abs/2506.18901", "authors": ["Wenqiang Sun", "Fangyun Wei", "Jinjing Zhao", "Xi Chen", "Zilong Chen", "Hongyang Zhang", "Jun Zhang", "Yan Lu"], "title": "From Virtual Games to Real-World Play", "categories": ["cs.CV"], "comment": "Project page: https://wenqsun.github.io/RealPlay/", "summary": "We introduce RealPlay, a neural network-based real-world game engine that enables interactive video generation from user control signals. Unlike prior works focused on game-style visuals, RealPlay aims to produce photorealistic, temporally consistent video sequences that resemble real-world footage. It operates in an interactive loop: users observe a generated scene, issue a control command, and receive a short video chunk in response. To enable such realistic and responsive generation, we address key challenges including iterative chunk-wise prediction for low-latency feedback, temporal consistency across iterations, and accurate control response. RealPlay is trained on a combination of labeled game data and unlabeled real-world videos, without requiring real-world action annotations. Notably, we observe two forms of generalization: (1) control transfer-RealPlay effectively maps control signals from virtual to real-world scenarios; and (2) entity transfer-although training labels originate solely from a car racing game, RealPlay generalizes to control diverse real-world entities, including bicycles and pedestrians, beyond vehicles. Project page can be found: https://wenqsun.github.io/RealPlay/", "AI": {"tldr": "RealPlay\u662f\u4e00\u4e2a\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u5b9e\u65f6\u6e38\u620f\u5f15\u64ce\uff0c\u901a\u8fc7\u7528\u6237\u63a7\u5236\u4fe1\u53f7\u751f\u6210\u4ea4\u4e92\u5f0f\u89c6\u9891\uff0c\u4e13\u6ce8\u4e8e\u903c\u771f\u4e14\u65f6\u95f4\u4e00\u81f4\u7684\u89c6\u9891\u5e8f\u5217\u3002", "motivation": "\u65e8\u5728\u751f\u6210\u903c\u771f\u7684\u89c6\u9891\u5e8f\u5217\uff0c\u800c\u975e\u4f20\u7edf\u6e38\u620f\u98ce\u683c\u7684\u89c6\u89c9\u6548\u679c\uff0c\u540c\u65f6\u89e3\u51b3\u4f4e\u5ef6\u8fdf\u53cd\u9988\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u51c6\u786e\u63a7\u5236\u54cd\u5e94\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u5206\u5757\u9884\u6d4b\u6280\u672f\uff0c\u7ed3\u5408\u6807\u8bb0\u7684\u6e38\u620f\u6570\u636e\u548c\u65e0\u6807\u8bb0\u7684\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e0\u9700\u771f\u5b9e\u4e16\u754c\u52a8\u4f5c\u6807\u6ce8\u3002", "result": "\u5b9e\u73b0\u4e86\u63a7\u5236\u4fe1\u53f7\u4ece\u865a\u62df\u5230\u771f\u5b9e\u573a\u666f\u7684\u6620\u5c04\uff0c\u5e76\u80fd\u591f\u6cdb\u5316\u63a7\u5236\u591a\u79cd\u771f\u5b9e\u4e16\u754c\u5b9e\u4f53\uff08\u5982\u81ea\u884c\u8f66\u548c\u884c\u4eba\uff09\u3002", "conclusion": "RealPlay\u5c55\u793a\u4e86\u5728\u903c\u771f\u89c6\u9891\u751f\u6210\u548c\u4ea4\u4e92\u63a7\u5236\u65b9\u9762\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.18904", "pdf": "https://arxiv.org/pdf/2506.18904", "abs": "https://arxiv.org/abs/2506.18904", "authors": ["Yang Liu", "Chuanchen Luo", "Zimo Tang", "Yingyan Li", "Yuran Yang", "Yuanyong Ning", "Lue Fan", "Junran Peng", "Zhaoxiang Zhang"], "title": "TC-Light: Temporally Consistent Relighting for Dynamic Long Videos", "categories": ["cs.CV"], "comment": "Project Page: https://dekuliutesla.github.io/tclight/ Code:   https://github.com/Linketic/TC-Light", "summary": "Editing illumination in long videos with complex dynamics has significant value in various downstream tasks, including visual content creation and manipulation, as well as data scaling up for embodied AI through sim2real and real2real transfer. Nevertheless, existing video relighting techniques are predominantly limited to portrait videos or fall into the bottleneck of temporal consistency and computation efficiency. In this paper, we propose TC-Light, a novel paradigm characterized by the proposed two-stage post optimization mechanism. Starting from the video preliminarily relighted by an inflated video relighting model, it optimizes appearance embedding in the first stage to align global illumination. Then it optimizes the proposed canonical video representation, i.e., Unique Video Tensor (UVT), to align fine-grained texture and lighting in the second stage. To comprehensively evaluate performance, we also establish a long and highly dynamic video benchmark. Extensive experiments show that our method enables physically plausible relighting results with superior temporal coherence and low computation cost. The code and video demos are available at https://dekuliutesla.github.io/tclight/.", "AI": {"tldr": "TC-Light\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u4f18\u5316\u673a\u5236\uff0c\u7528\u4e8e\u957f\u89c6\u9891\u4e2d\u7684\u5149\u7167\u7f16\u8f91\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u7684\u74f6\u9888\u3002", "motivation": "\u957f\u89c6\u9891\u4e2d\u590d\u6742\u52a8\u6001\u7684\u5149\u7167\u7f16\u8f91\u5728\u89c6\u89c9\u5185\u5bb9\u521b\u4f5c\u548cAI\u6570\u636e\u6269\u5c55\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u73b0\u6709\u6280\u672f\u5c40\u9650\u4e8e\u8096\u50cf\u89c6\u9891\u6216\u9762\u4e34\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002", "method": "TC-Light\u91c7\u7528\u4e24\u9636\u6bb5\u540e\u4f18\u5316\u673a\u5236\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f18\u5316\u5916\u89c2\u5d4c\u5165\u4ee5\u5bf9\u9f50\u5168\u5c40\u5149\u7167\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f18\u5316\u63d0\u51fa\u7684\u552f\u4e00\u89c6\u9891\u5f20\u91cf\uff08UVT\uff09\u4ee5\u5bf9\u9f50\u7ec6\u7c92\u5ea6\u7eb9\u7406\u548c\u5149\u7167\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u7269\u7406\u4e0a\u5408\u7406\u7684\u5149\u7167\u7f16\u8f91\u7ed3\u679c\uff0c\u5177\u6709\u4f18\u5f02\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "TC-Light\u4e3a\u957f\u89c6\u9891\u5149\u7167\u7f16\u8f91\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u4e00\u81f4\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2506.17324", "pdf": "https://arxiv.org/pdf/2506.17324", "abs": "https://arxiv.org/abs/2506.17324", "authors": ["Emma Finn", "T. Anderson Keller", "Manos Theodosis", "Demba E. Ba"], "title": "Origins of Creativity in Attention-Based Diffusion Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "As diffusion models have become the tool of choice for image generation and as the quality of the images continues to improve, the question of how `creativity' originates in diffusion has become increasingly important. The score matching perspective on diffusion has proven particularly fruitful for understanding how and why diffusion models generate images that remain plausible while differing significantly from their training images. In particular, as explained in (Kamb \\& Ganguli, 2024) and others, e.g., (Ambrogioni, 2023), theory suggests that if our score matching were optimal, we would only be able to recover training samples through our diffusion process. However, as shown by Kamb \\& Ganguli, (2024), in diffusion models where the score is parametrized by a simple CNN, the inductive biases of the CNN itself (translation equivariance and locality) allow the model to generate samples that globally do not match any training samples, but are rather patch-wise `mosaics'. Notably, however, this theory does not extend to describe the role of self-attention in this process. In this work, we take a preliminary step in this direction to extend this theory to the case of diffusion models whose score is parametrized by a CNN with a final self-attention layer. We show that our theory suggests that self-attention will induce a globally image-consistent arrangement of local features beyond the patch-level in generated samples, and we verify this behavior empirically on a carefully crafted dataset.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u6269\u6563\u6a21\u578b\u4e2d\u521b\u9020\u6027\u7684\u8d77\u6e90\uff0c\u7279\u522b\u662f\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5982\u4f55\u5f71\u54cd\u751f\u6210\u56fe\u50cf\u7684\u5168\u5c40\u4e00\u81f4\u6027\u3002", "motivation": "\u968f\u7740\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7406\u89e3\u5176\u521b\u9020\u6027\u6765\u6e90\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\u3002\u73b0\u6709\u7406\u8bba\u672a\u80fd\u89e3\u91ca\u81ea\u6ce8\u610f\u529b\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u6269\u5c55\u4e86\u73b0\u6709\u7406\u8bba\uff0c\u7814\u7a76\u4e86\u7531CNN\u548c\u81ea\u6ce8\u610f\u529b\u5c42\u53c2\u6570\u5316\u7684\u6269\u6563\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u81ea\u6ce8\u610f\u529b\u7684\u5f71\u54cd\u3002", "result": "\u7406\u8bba\u8868\u660e\u81ea\u6ce8\u610f\u529b\u80fd\u4fc3\u8fdb\u751f\u6210\u56fe\u50cf\u4e2d\u5c40\u90e8\u7279\u5f81\u7684\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u884c\u4e3a\u3002", "conclusion": "\u81ea\u6ce8\u610f\u529b\u5728\u6269\u6563\u6a21\u578b\u4e2d\u8d77\u5230\u4e86\u5173\u952e\u4f5c\u7528\uff0c\u4f7f\u751f\u6210\u7684\u56fe\u50cf\u5728\u5168\u5c40\u4e0a\u66f4\u4e00\u81f4\u3002"}}
{"id": "2506.17337", "pdf": "https://arxiv.org/pdf/2506.17337", "abs": "https://arxiv.org/abs/2506.17337", "authors": ["Yuan Zhong", "Ruinan Jin", "Xiaoxiao Li", "Qi Dou"], "title": "Can Common VLMs Rival Medical VLMs? Evaluation and Strategic Insights", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Medical vision-language models (VLMs) leverage large-scale pretraining for diverse imaging tasks but require substantial computational and data resources. Meanwhile, common or general-purpose VLMs (e.g., CLIP, LLaVA), though not trained for medical use, show promise with fine-tuning. This raises a key question: Can efficient fine-tuned common VLMs rival generalist medical VLMs for solving specific medical imaging tasks? This study systematically evaluates common and medical VLMs across disease diagnosis and visual question answering (VQA). Using CLIP-based and LLaVA-based models, we examine (1) off-the-shelf performance gaps in in-domain (ID) settings, (2) whether fine-tuning bridges these gaps, and (3) generalization to out-of-domain (OOD) tasks on unseen medical modalities. While medical-specific pretraining provides advantages in ID settings, common VLMs match or surpass medical-specific models after lightweight fine-tuning, with LoRA-based adaptation proving highly effective among different tasks. In OOD tasks, common VLMs demonstrate strong adaptability in some tasks, challenging the assumption that medical-specific pre-training is essential. These findings suggest that leveraging common VLMs with fine-tuning offers a scalable and cost-effective alternative to developing large-scale medical VLMs, providing crucial insights for future research in the medical imaging field.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u7ecf\u8fc7\u8f7b\u91cf\u7ea7\u5fae\u8c03\u7684\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u7279\u5b9a\u533b\u5b66\u6210\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u4e0e\u4e13\u7528\u533b\u5b66VLMs\u76f8\u5f53\uff0c\u6311\u6218\u4e86\u533b\u5b66\u4e13\u7528\u9884\u8bad\u7ec3\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u63a2\u8ba8\u901a\u7528VLMs\uff08\u5982CLIP\u3001LLaVA\uff09\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5fae\u8c03\u5728\u533b\u5b66\u6210\u50cf\u4efb\u52a1\u4e2d\u4e0e\u4e13\u7528\u533b\u5b66VLMs\u7ade\u4e89\uff0c\u4ee5\u8282\u7701\u8ba1\u7b97\u548c\u6570\u636e\u8d44\u6e90\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u901a\u7528\u548c\u533b\u5b66VLMs\u5728\u75be\u75c5\u8bca\u65ad\u548c\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u62ec\u57df\u5185\uff08ID\uff09\u548c\u57df\u5916\uff08OOD\uff09\u4efb\u52a1\uff0c\u5e76\u91c7\u7528LoRA\u5fae\u8c03\u65b9\u6cd5\u3002", "result": "\u901a\u7528VLMs\u5728\u5fae\u8c03\u540e\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u57df\u5185\u4efb\u52a1\u4e2d\uff0c\u751a\u81f3\u8d85\u8d8a\u4e13\u7528\u533b\u5b66VLMs\uff1b\u5728\u57df\u5916\u4efb\u52a1\u4e2d\u4e5f\u5c55\u73b0\u51fa\u8f83\u5f3a\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u901a\u7528VLMs\u7ed3\u5408\u5fae\u8c03\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4e3a\u533b\u5b66\u6210\u50cf\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2506.17540", "pdf": "https://arxiv.org/pdf/2506.17540", "abs": "https://arxiv.org/abs/2506.17540", "authors": ["Tingting Liu", "Yuan Liu", "Jinhui Tang", "Liyin Yuan", "Chengyu Liu", "Chunlai Li", "Xiubao Sui", "Qian Chen"], "title": "MTSIC: Multi-stage Transformer-based GAN for Spectral Infrared Image Colorization", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Thermal infrared (TIR) images, acquired through thermal radiation imaging, are unaffected by variations in lighting conditions and atmospheric haze. However, TIR images inherently lack color and texture information, limiting downstream tasks and potentially causing visual fatigue. Existing colorization methods primarily rely on single-band images with limited spectral information and insufficient feature extraction capabilities, which often result in image distortion and semantic ambiguity. In contrast, multiband infrared imagery provides richer spectral data, facilitating the preservation of finer details and enhancing semantic accuracy. In this paper, we propose a generative adversarial network (GAN)-based framework designed to integrate spectral information to enhance the colorization of infrared images. The framework employs a multi-stage spectral self-attention Transformer network (MTSIC) as the generator. Each spectral feature is treated as a token for self-attention computation, and a multi-head self-attention mechanism forms a spatial-spectral attention residual block (SARB), achieving multi-band feature mapping and reducing semantic confusion. Multiple SARB units are integrated into a Transformer-based single-stage network (STformer), which uses a U-shaped architecture to extract contextual information, combined with multi-scale wavelet blocks (MSWB) to align semantic information in the spatial-frequency dual domain. Multiple STformer modules are cascaded to form MTSIC, progressively optimizing the reconstruction quality. Experimental results demonstrate that the proposed method significantly outperforms traditional techniques and effectively enhances the visual quality of infrared images.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGAN\u7684\u591a\u6ce2\u6bb5\u7ea2\u5916\u56fe\u50cf\u5f69\u8272\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u5149\u8c31\u81ea\u6ce8\u610f\u529bTransformer\u7f51\u7edc\uff08MTSIC\uff09\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u8bed\u4e49\u51c6\u786e\u6027\u3002", "motivation": "\u70ed\u7ea2\u5916\uff08TIR\uff09\u56fe\u50cf\u7f3a\u4e4f\u989c\u8272\u548c\u7eb9\u7406\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u5e76\u53ef\u80fd\u5bfc\u81f4\u89c6\u89c9\u75b2\u52b3\u3002\u73b0\u6709\u65b9\u6cd5\u56e0\u5355\u6ce2\u6bb5\u4fe1\u606f\u4e0d\u8db3\u5bfc\u81f4\u56fe\u50cf\u5931\u771f\u548c\u8bed\u4e49\u6a21\u7cca\u3002", "method": "\u91c7\u7528GAN\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u6ce2\u6bb5\u5149\u8c31\u4fe1\u606f\uff0c\u8bbe\u8ba1\u4e86MTSIC\u751f\u6210\u5668\uff0c\u5229\u7528\u7a7a\u95f4-\u5149\u8c31\u6ce8\u610f\u529b\u6b8b\u5dee\u5757\uff08SARB\uff09\u548cU\u5f62\u67b6\u6784\u63d0\u53d6\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u591a\u5c3a\u5ea6\u5c0f\u6ce2\u5757\uff08MSWB\uff09\u5bf9\u9f50\u8bed\u4e49\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6280\u672f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7ea2\u5916\u56fe\u50cf\u7684\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "\u63d0\u51fa\u7684MTSIC\u6846\u67b6\u901a\u8fc7\u591a\u6ce2\u6bb5\u5149\u8c31\u4fe1\u606f\u878d\u5408\uff0c\u663e\u8457\u6539\u5584\u4e86\u7ea2\u5916\u56fe\u50cf\u5f69\u8272\u5316\u7684\u6548\u679c\u3002"}}
{"id": "2506.17623", "pdf": "https://arxiv.org/pdf/2506.17623", "abs": "https://arxiv.org/abs/2506.17623", "authors": ["Yuesheng Huang", "Peng Zhang", "Riliang Liu", "Jiaqi Liang"], "title": "Can Generated Images Serve as a Viable Modality for Text-Centric Multimodal Learning?", "categories": ["cs.MM", "cs.CV"], "comment": "4 figures,7 tables", "summary": "A significant ``modality gap\" exists between the abundance of text-only data and the increasing power of multimodal models. This work systematically investigates whether images generated on-the-fly by Text-to-Image (T2I) models can serve as a valuable complementary modality for text-centric tasks. Through a comprehensive evaluation framework on text classification, we analyze the impact of critical variables, including T2I model quality, prompt engineering strategies, and multimodal fusion architectures. Our findings demonstrate that this``synthetic perception\" can yield significant performance gains, even when augmenting strong large language model baselines. However, we find the effectiveness of this approach is highly conditional, depending critically on the semantic alignment between text and the generated image, the inherent ``visual groundability\" of the task, and the generative fidelity of the T2I model. Our work establishes the first rigorous benchmark for this paradigm, providing a clear analysis of its potential and current limitations, and demonstrating its viability as a pathway to enrich language understanding in traditionally unimodal scenarios.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u901a\u8fc7\u6587\u672c\u751f\u6210\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u4e3a\u6587\u672c\u4efb\u52a1\u63d0\u4f9b\u8865\u5145\u6a21\u6001\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0\u5176\u6027\u80fd\u63d0\u5347\u663e\u8457\u4f46\u6761\u4ef6\u82db\u523b\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u6570\u636e\u4e0e\u591a\u6a21\u6001\u6a21\u578b\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u8ddd\uff0c\u63a2\u7d22T2I\u6a21\u578b\u5728\u6587\u672c\u4efb\u52a1\u4e2d\u7684\u6f5c\u5728\u4ef7\u503c\u3002", "method": "\u901a\u8fc7\u6587\u672c\u5206\u7c7b\u4efb\u52a1\uff0c\u8bc4\u4f30T2I\u6a21\u578b\u8d28\u91cf\u3001\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u548c\u591a\u6a21\u6001\u878d\u5408\u67b6\u6784\u7684\u5f71\u54cd\u3002", "result": "\u5408\u6210\u611f\u77e5\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u6548\u679c\u9ad8\u5ea6\u4f9d\u8d56\u6587\u672c\u4e0e\u56fe\u50cf\u7684\u8bed\u4e49\u5bf9\u9f50\u3001\u4efb\u52a1\u7684\u53ef\u89c6\u5316\u57fa\u7840\u53caT2I\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8fd9\u4e00\u8303\u5f0f\u5efa\u7acb\u4e86\u9996\u4e2a\u4e25\u683c\u57fa\u51c6\uff0c\u660e\u786e\u4e86\u5176\u6f5c\u529b\u4e0e\u5c40\u9650\uff0c\u5c55\u793a\u4e86\u5728\u4f20\u7edf\u5355\u6a21\u6001\u573a\u666f\u4e2d\u4e30\u5bcc\u8bed\u8a00\u7406\u89e3\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2506.18371", "pdf": "https://arxiv.org/pdf/2506.18371", "abs": "https://arxiv.org/abs/2506.18371", "authors": ["Sara Rehmat", "Hafeez Ur Rehman"], "title": "Transforming H&E images into IHC: A Variance-Penalized GAN for Precision Oncology", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The overexpression of the human epidermal growth factor receptor 2 (HER2) in breast cells is a key driver of HER2-positive breast cancer, a highly aggressive subtype requiring precise diagnosis and targeted therapy. Immunohistochemistry (IHC) is the standard technique for HER2 assessment but is costly, labor-intensive, and highly dependent on antibody selection. In contrast, hematoxylin and eosin (H&E) staining, a routine histopathological procedure, offers broader accessibility but lacks HER2 specificity. This study proposes an advanced deep learning-based image translation framework to generate highfidelity IHC images from H&E-stained tissue samples, enabling cost-effective and scalable HER2 assessment. By modifying the loss function of pyramid pix2pix, we mitigate mode collapse, a fundamental limitation in generative adversarial networks (GANs), and introduce a novel variance-based penalty that enforces structural diversity in generated images. Our model particularly excels in translating HER2-positive (IHC 3+) images, which have remained challenging for existing methods due to their complex morphological variations. Extensive evaluations on the BCI histopathological dataset demonstrate that our model surpasses state-of-the-art methods in terms of peak signal-tonoise ratio (PSNR), structural similarity index (SSIM), and Frechet Inception Distance (FID), particularly in accurately translating HER2-positive (IHC 3+) images. Beyond medical imaging, our model exhibits superior performance in general image-to-image translation tasks, showcasing its potential across multiple domains. This work marks a significant step toward AI-driven precision oncology, offering a reliable and efficient alternative to traditional HER2 diagnostics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u56fe\u50cf\u7ffb\u8bd1\u6846\u67b6\uff0c\u4eceH&E\u67d3\u8272\u6837\u672c\u751f\u6210\u9ad8\u8d28\u91cfIHC\u56fe\u50cf\uff0c\u7528\u4e8e\u4f4e\u6210\u672c\u3001\u53ef\u6269\u5c55\u7684HER2\u8bc4\u4f30\u3002", "motivation": "HER2\u9633\u6027\u4e73\u817a\u764c\u7684\u8bca\u65ad\u4f9d\u8d56\u6602\u8d35\u7684IHC\u6280\u672f\uff0c\u800cH&E\u67d3\u8272\u867d\u666e\u53ca\u4f46\u7f3a\u4e4f\u7279\u5f02\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7AI\u6280\u672f\u89e3\u51b3\u8fd9\u4e00\u77db\u76fe\u3002", "method": "\u6539\u8fdb\u91d1\u5b57\u5854pix2pix\u7684\u635f\u5931\u51fd\u6570\uff0c\u5f15\u5165\u57fa\u4e8e\u65b9\u5dee\u7684\u60e9\u7f5a\u9879\uff0c\u63d0\u5347\u751f\u6210\u56fe\u50cf\u7684\u591a\u6837\u6027\u548c\u8d28\u91cf\uff0c\u7279\u522b\u9488\u5bf9HER2\u9633\u6027\uff08IHC 3+\uff09\u56fe\u50cf\u3002", "result": "\u5728BCI\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5728PSNR\u3001SSIM\u548cFID\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728HER2\u9633\u6027\u56fe\u50cf\u7ffb\u8bd1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3aHER2\u8bca\u65ad\u63d0\u4f9b\u4e86\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u901a\u7528\u56fe\u50cf\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u63a8\u52a8\u4e86AI\u9a71\u52a8\u7684\u7cbe\u51c6\u80bf\u7624\u5b66\u53d1\u5c55\u3002"}}
{"id": "2506.18885", "pdf": "https://arxiv.org/pdf/2506.18885", "abs": "https://arxiv.org/abs/2506.18885", "authors": ["Annika Thomas", "Aneesa Sonawalla", "Alex Rose", "Jonathan P. How"], "title": "GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale Multi-Agent Gaussian SLAM", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "3D Gaussian splatting has emerged as an expressive scene representation for RGB-D visual SLAM, but its application to large-scale, multi-agent outdoor environments remains unexplored. Multi-agent Gaussian SLAM is a promising approach to rapid exploration and reconstruction of environments, offering scalable environment representations, but existing approaches are limited to small-scale, indoor environments. To that end, we propose Gaussian Reconstruction via Multi-Agent Dense SLAM, or GRAND-SLAM, a collaborative Gaussian splatting SLAM method that integrates i) an implicit tracking module based on local optimization over submaps and ii) an approach to inter- and intra-robot loop closure integrated into a pose-graph optimization framework. Experiments show that GRAND-SLAM provides state-of-the-art tracking performance and 28% higher PSNR than existing methods on the Replica indoor dataset, as well as 91% lower multi-agent tracking error and improved rendering over existing multi-agent methods on the large-scale, outdoor Kimera-Multi dataset.", "AI": {"tldr": "GRAND-SLAM\u662f\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u9ad8\u65af\u6cfc\u6e85SLAM\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u5c3a\u5ea6\u6237\u5916\u73af\u5883\uff0c\u7ed3\u5408\u4e86\u5c40\u90e8\u4f18\u5316\u548c\u95ed\u73af\u68c0\u6d4b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u9ad8\u65af\u6cfc\u6e85SLAM\u65b9\u6cd5\u4ec5\u9002\u7528\u4e8e\u5c0f\u89c4\u6a21\u5ba4\u5185\u73af\u5883\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5927\u5c3a\u5ea6\u591a\u667a\u80fd\u4f53\u6237\u5916\u573a\u666f\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faGRAND-SLAM\u65b9\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8e\u5c40\u90e8\u4f18\u5316\u7684\u9690\u5f0f\u8ddf\u8e2a\u6a21\u5757\u548c\u591a\u667a\u80fd\u4f53\u95ed\u73af\u68c0\u6d4b\u7684\u4f4d\u59ff\u56fe\u4f18\u5316\u6846\u67b6\u3002", "result": "\u5728Replica\u5ba4\u5185\u6570\u636e\u96c6\u4e0a\uff0cGRAND-SLAM\u7684\u8ddf\u8e2a\u6027\u80fd\u548cPSNR\u5206\u522b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd528%\uff1b\u5728Kimera-Multi\u6237\u5916\u6570\u636e\u96c6\u4e0a\uff0c\u591a\u667a\u80fd\u4f53\u8ddf\u8e2a\u8bef\u5dee\u964d\u4f4e91%\u3002", "conclusion": "GRAND-SLAM\u5728\u5927\u5c3a\u5ea6\u591a\u667a\u80fd\u4f53\u6237\u5916\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u573a\u666f\u91cd\u5efa\u548c\u63a2\u7d22\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
