<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 11]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [ARGS: Advanced Regularization on Aligning Gaussians over the Surface](https://arxiv.org/abs/2508.21344)
*Jeong Uk Lee,Sung Hee Choi*

Main category: cs.GR

TL;DR: 该论文在SuGaR基础上提出两种正则化策略：有效秩正则化防止高斯形状过度各向异性，以及神经SDF正则化提供全局表面先验，从而提升3D高斯泼溅的网格重建质量和视觉一致性。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅(3DGS)重建方法在视觉保真度和场景一致性方面仍有改进空间，特别是单个高斯形状的优化和整体表面连贯性存在局限性。

Method: 1. 有效秩正则化：鼓励高斯采用更平衡的"圆盘状"形状而非"针状"形状；2. 神经SDF正则化：通过Eikonal损失保持距离属性，提供连续全局表面先验。

Result: 提出的两种互补正则化策略能够同时改善单个高斯基元的保真度和整体表面行为，实现更准确和连贯的视觉重建。

Conclusion: 通过结合形状正则化和全局表面先验，该方法显著提升了从3D高斯泼溅数据重建高质量3D网格和视觉效果的能力。

Abstract: Reconstructing high-quality 3D meshes and visuals from 3D Gaussian Splatting(3DGS) still remains a central challenge in computer graphics. Although existing models such as SuGaR offer effective solutions for rendering, there is is still room to improve improve both visual fidelity and scene consistency. This work builds upon SuGaR by introducing two complementary regularization strategies that address common limitations in both the shape of individual Gaussians and the coherence of the overall surface. The first strategy introduces an effective rank regularization, motivated by recent studies on Gaussian primitive structures. This regularization discourages extreme anisotropy-specifically, "needle-like" shapes-by favoring more balanced, "disk-like" forms that are better suited for stable surface reconstruction. The second strategy integrates a neural Signed Distance Function (SDF) into the optimization process. The SDF is regularized with an Eikonal loss to maintain proper distance properties and provides a continuous global surface prior, guiding Gaussians toward better alignment with the underlying geometry. These two regularizations aim to improve both the fidelity of individual Gaussian primitives and their collective surface behavior. The final model can make more accurate and coherent visuals from 3DGS data.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [Q-Align: Alleviating Attention Leakage in Zero-Shot Appearance Transfer via Query-Query Alignment](https://arxiv.org/abs/2508.21090)
*Namu Kim,Wonbin Kweon,Minsoo Kim,Hwanjo Yu*

Main category: cs.CV

TL;DR: Q-Align方法通过Query-Query对齐解决大模型零样本外观迁移中的注意力泄漏问题，在保持结构的同时提升外观保真度


<details>
  <summary>Details</summary>
Motivation: 解决大规模图像生成模型在零样本外观迁移中出现的注意力泄漏问题，该问题源于Query-Key对齐导致的语义映射错误

Method: 提出Q-Align方法，包含三个核心贡献：1) Query-Query对齐实现精细空间语义映射；2) Key-Value重排增强特征对应；3) 使用重排后的键值进行注意力精化保持语义一致性

Result: 通过大量实验验证，Q-Align在外观保真度方面优于最先进方法，同时保持竞争力的结构保持能力

Conclusion: Q-Align有效解决了注意力泄漏问题，为零样本外观迁移提供了更准确的语义对齐方法

Abstract: We observe that zero-shot appearance transfer with large-scale image generation models faces a significant challenge: Attention Leakage. This challenge arises when the semantic mapping between two images is captured by the Query-Key alignment. To tackle this issue, we introduce Q-Align, utilizing Query-Query alignment to mitigate attention leakage and improve the semantic alignment in zero-shot appearance transfer. Q-Align incorporates three core contributions: (1) Query-Query alignment, facilitating the sophisticated spatial semantic mapping between two images; (2) Key-Value rearrangement, enhancing feature correspondence through realignment; and (3) Attention refinement using rearranged keys and values to maintain semantic consistency. We validate the effectiveness of Q-Align through extensive experiments and analysis, and Q-Align outperforms state-of-the-art methods in appearance fidelity while maintaining competitive structure preservation.

</details>


### [3] [ERTACache: Error Rectification and Timesteps Adjustment for Efficient Diffusion](https://arxiv.org/abs/2508.21091)
*Xurui Peng,Hong Liu,Chenqian Yan,Rui Ma,Fangmin Chen,Xing Wang,Zhihua Wu,Songwei Liu,Mingbao Lin*

Main category: cs.CV

TL;DR: ERTACache是一个扩散模型加速框架，通过分析缓存误差的两个主要来源（特征偏移误差和步长放大误差），采用离线残差分析和轨迹感知校正系数，实现2倍推理加速同时保持甚至提升视觉质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型由于迭代推理过程导致计算开销巨大。虽然特征缓存是一种有前景的加速策略，但简单的重用会导致明显的质量下降。需要解决缓存引入的累积误差问题。

Method: 提出ERTACache框架：1）离线残差分析识别可重用步骤；2）通过轨迹感知校正系数动态调整积分间隔；3）使用闭式残差线性化模型近似缓存误差。联合校正特征偏移误差和步长放大误差。

Result: 在标准图像和视频生成基准测试中，ERTACache实现高达2倍的推理加速，同时一致保持甚至改善视觉质量。在Wan2.1视频扩散模型上，2倍加速下VBench指标下降最小，有效保持基线保真度。

Conclusion: ERTACache通过系统分析缓存误差机制并提出联合校正方案，成功解决了扩散模型缓存加速中的质量退化问题，实现了高效且准确的采样，为扩散模型的实际部署提供了有效的加速解决方案。

Abstract: Diffusion models suffer from substantial computational overhead due to their inherently iterative inference process. While feature caching offers a promising acceleration strategy by reusing intermediate outputs across timesteps, naive reuse often incurs noticeable quality degradation. In this work, we formally analyze the cumulative error introduced by caching and decompose it into two principal components: feature shift error, caused by inaccuracies in cached outputs, and step amplification error, which arises from error propagation under fixed timestep schedules. To address these issues, we propose ERTACache, a principled caching framework that jointly rectifies both error types. Our method employs an offline residual profiling stage to identify reusable steps, dynamically adjusts integration intervals via a trajectory-aware correction coefficient, and analytically approximates cache-induced errors through a closed-form residual linearization model. Together, these components enable accurate and efficient sampling under aggressive cache reuse. Extensive experiments across standard image and video generation benchmarks show that ERTACache achieves up to 2x inference speedup while consistently preserving or even improving visual quality. Notably, on the state-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x acceleration with minimal VBench degradation, effectively maintaining baseline fidelity while significantly improving efficiency. The code is available at https://github.com/bytedance/ERTACache.

</details>


### [4] [Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text-to-Image Generation Models](https://arxiv.org/abs/2508.21099)
*Xiangtao Meng,Yingkai Dong,Ning Yu,Li Wang,Zheng Li,Shanqing Guo*

Main category: cs.CV

TL;DR: Safe-Control是一个即插即用的安全补丁，用于减少文本到图像生成模型中的不安全内容生成，通过数据驱动策略和安全感知条件向锁定模型注入安全控制信号。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成模型安全机制容易受到分布偏移的规避，或需要大量模型特定调整，需要一种更有效和灵活的解决方案。

Method: 使用数据驱动策略和安全感知条件，以补丁形式向锁定的T2I模型注入安全控制信号，支持构建多种安全补丁并灵活合并为统一补丁。

Result: 在六个不同的T2I模型上评估显示，Safe-Control将不安全内容生成概率降至7%，显著优于基线方法的约20%，同时保持良性图像质量和文本对齐。

Conclusion: Safe-Control是一种有效的即插即用安全解决方案，能够显著减少T2I模型的不安全内容生成，同时保持生成质量，优于现有最先进的安全机制。

Abstract: Despite the advancements in Text-to-Image (T2I) generation models, their potential for misuse or even abuse raises serious safety concerns. Model developers have made tremendous efforts to introduce safety mechanisms that can address these concerns in T2I models. However, the existing safety mechanisms, whether external or internal, either remain susceptible to evasion under distribution shifts or require extensive model-specific adjustments. To address these limitations, we introduce Safe-Control, an innovative plug-and-play safety patch designed to mitigate unsafe content generation in T2I models. Using data-driven strategies and safety-aware conditions, Safe-Control injects safety control signals into the locked T2I model, acting as an update in a patch-like manner. Model developers can also construct various safety patches to meet the evolving safety requirements, which can be flexibly merged into a single, unified patch. Its plug-and-play design further ensures adaptability, making it compatible with other T2I models of similar denoising architecture. We conduct extensive evaluations on six diverse and public T2I models. Empirical results highlight that Safe-Control is effective in reducing unsafe content generation across six diverse T2I models with similar generative architectures, yet it successfully maintains the quality and text alignment of benign images. Compared to seven state-of-the-art safety mechanisms, including both external and internal defenses, Safe-Control significantly outperforms all baselines in reducing unsafe content generation. For example, it reduces the probability of unsafe content generation to 7%, compared to approximately 20% for most baseline methods, under both unsafe prompts and the latest adversarial attacks.

</details>


### [5] [Reverse Imaging for Wide-spectrum Generalization of Cardiac MRI Segmentation](https://arxiv.org/abs/2508.21254)
*Yidong Zhao,Peter Kellman,Hui Xue,Tongyun Yang,Yi Zhang,Yuchi Han,Orlando Simonetti,Qian Tao*

Main category: cs.CV

TL;DR: Reverse Imaging是一种基于物理原理的心脏MRI数据增强方法，通过逆向推断底层自旋属性来解决不同成像序列间的泛化问题，显著提升分割模型的跨域性能


<details>
  <summary>Details</summary>
Motivation: 预训练的心脏MRI分割模型难以泛化到不同对比度的成像序列，因为成像协议变化导致图像对比度差异巨大，但所有图像都由相同的自旋属性（质子密度、T1、T2值）控制

Method: 提出Reverse Imaging方法：1）从观测的MRI图像逆向求解非线性逆问题推断底层自旋属性；2）使用扩散模型学习mSASHA数据集中的自旋属性先验分布；3）基于推断的自旋属性合成任意新序列的图像

Result: 该方法能够从MR图像获得有意义的自旋属性估计，实现高度灵活的图像合成，使分割模型在完全不同图像对比度和成像协议下都能实现高精度分割

Conclusion: Reverse Imaging通过物理驱动的数据增强和域适应方法，从根本上解决了心脏MRI分割的泛化问题，实现了宽谱泛化能力

Abstract: Pretrained segmentation models for cardiac magnetic resonance imaging (MRI) struggle to generalize across different imaging sequences due to significant variations in image contrast. These variations arise from changes in imaging protocols, yet the same fundamental spin properties, including proton density, T1, and T2 values, govern all acquired images. With this core principle, we introduce Reverse Imaging, a novel physics-driven method for cardiac MRI data augmentation and domain adaptation to fundamentally solve the generalization problem. Our method reversely infers the underlying spin properties from observed cardiac MRI images, by solving ill-posed nonlinear inverse problems regularized by the prior distribution of spin properties. We acquire this "spin prior" by learning a generative diffusion model from the multiparametric SAturation-recovery single-SHot acquisition sequence (mSASHA) dataset, which offers joint cardiac T1 and T2 maps. Our method enables approximate but meaningful spin-property estimates from MR images, which provide an interpretable "latent variable" that lead to highly flexible image synthesis of arbitrary novel sequences. We show that Reverse Imaging enables highly accurate segmentation across vastly different image contrasts and imaging protocols, realizing wide-spectrum generalization of cardiac MRI segmentation.

</details>


### [6] [Print2Volume: Generating Synthetic OCT-based 3D Fingerprint Volume from 2D Fingerprint Image](https://arxiv.org/abs/2508.21371)
*Qingran Miao,Haixia Wang,Haohao Sun,Yilong Zhang*

Main category: cs.CV

TL;DR: 提出Print2Volume框架，从2D指纹图像生成逼真的合成OCT 3D指纹数据，解决OCT数据稀缺问题，显著提升识别性能


<details>
  <summary>Details</summary>
Motivation: OCT技术能获取高分辨率3D指纹数据，但数据采集成本高、耗时长，导致大规模数据集稀缺，阻碍了深度学习算法的发展

Method: 三阶段框架：1）2D风格迁移模块将二值指纹转为灰度图像；2）3D结构扩展网络将2D图像外推为3D解剖体积；3）基于3D GAN的OCT真实性细化器添加纹理和噪声

Result: 生成42万个合成样本，预训练模型后在真实小数据集上微调，将等错误率从15.62%降至2.50%

Conclusion: Print2Volume能有效生成高质量合成OCT指纹数据，显著缓解数据稀缺问题，大幅提升生物识别性能

Abstract: Optical Coherence Tomography (OCT) enables the acquisition of high-resolution, three-dimensional fingerprint data, capturing rich subsurface structures for robust biometric recognition. However, the high cost and time-consuming nature of OCT data acquisition have led to a scarcity of large-scale public datasets, significantly hindering the development of advanced algorithms, particularly data-hungry deep learning models. To address this critical bottleneck, this paper introduces Print2Volume, a novel framework for generating realistic, synthetic OCT-based 3D fingerprints from 2D fingerprint image. Our framework operates in three sequential stages: (1) a 2D style transfer module that converts a binary fingerprint into a grayscale images mimicking the style of a Z-direction mean-projected OCT scan; (2) a 3D Structure Expansion Network that extrapolates the 2D im-age into a plausible 3D anatomical volume; and (3) an OCT Realism Refiner, based on a 3D GAN, that renders the structural volume with authentic textures, speckle noise, and other imaging characteristics. Using Print2Volume, we generated a large-scale synthetic dataset of 420,000 samples. Quantitative experiments demonstrate the high quality of our synthetic data and its significant impact on recognition performance. By pre-training a recognition model on our synthetic data and fine-tuning it on a small real-world dataset, we achieved a remarkable reduction in the Equal Error Rate (EER) from 15.62% to 2.50% on the ZJUT-EIFD benchmark, proving the effectiveness of our approach in overcoming data scarcity.

</details>


### [7] [Scale-GS: Efficient Scalable Gaussian Splatting via Redundancy-filtering Training on Streaming Content](https://arxiv.org/abs/2508.21444)
*Jiayu Yang,Weijian Su,Songqian Zhang,Yuqi Han,Jinli Suo,Qiang Zhang*

Main category: cs.CV

TL;DR: M框架通过分层高斯球结构和混合变形生成策略，在动态场景中实现了高效训练和高质量渲染，显著减少了训练时间和计算开销


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅在动态场景中存在数据量大和训练时间长的问题，需要一种可扩展的高效训练框架

Method: 使用基于锚点的分层高斯球结构，粗粒度高斯表示场景结构，细粒度高斯负责细节渲染；采用混合变形和生成策略建模运动；双向自适应掩码机制提升训练效率

Result: 在广泛实验中，M框架实现了卓越的视觉质量，同时相比最先进方法显著减少了训练时间

Conclusion: 该框架为动态场景的高斯泼溅提供了一种高效可扩展的解决方案，在保持高质量渲染的同时大幅提升训练效率

Abstract: 3D Gaussian Splatting (3DGS) enables high-fidelity real-time rendering, a key requirement for immersive applications. However, the extension of 3DGS to dynamic scenes remains limitations on the substantial data volume of dense Gaussians and the prolonged training time required for each frame. This paper presents \M, a scalable Gaussian Splatting framework designed for efficient training in streaming tasks. Specifically, Gaussian spheres are hierarchically organized by scale within an anchor-based structure. Coarser-level Gaussians represent the low-resolution structure of the scene, while finer-level Gaussians, responsible for detailed high-fidelity rendering, are selectively activated by the coarser-level Gaussians. To further reduce computational overhead, we introduce a hybrid deformation and spawning strategy that models motion of inter-frame through Gaussian deformation and triggers Gaussian spawning to characterize wide-range motion. Additionally, a bidirectional adaptive masking mechanism enhances training efficiency by removing static regions and prioritizing informative viewpoints. Extensive experiments demonstrate that \M~ achieves superior visual quality while significantly reducing training time compared to state-of-the-art methods.

</details>


### [8] [Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation](https://arxiv.org/abs/2508.21529)
*Ronan Docherty,Antonis Vamvakeros,Samuel J. Cooper*

Main category: cs.CV

TL;DR: 提出一种卷积神经网络上采样器，将低分辨率基础模型特征与输入图像结合，高效处理显微镜图像分割，显著减少标注需求并提高分割质量


<details>
  <summary>Details</summary>
Motivation: 现有基于patch的基础模型特征在处理显微镜图像时面临两个问题：1）无法捕捉微结构中的精细特征；2）难以处理材料科学和生物图像分析中的大尺寸图像

Method: 训练卷积神经网络，参考输入图像对低分辨率（大patch尺寸）基础模型特征进行上采样，无需额外训练即可应用于各种显微镜图像的特征提取和分割

Result: 上采样后的丰富特征能够分离难以分割的相（如发丝裂纹），交互式分割使用这些深度特征能够以更少的标签和更快的速度产生高质量分割结果

Conclusion: 该方法为显微镜图像分析提供了一种高效的特征提取和分割解决方案，显著优于传统的卷积网络训练或微调方法

Abstract: Feature foundation models - usually vision transformers - offer rich semantic descriptors of images, useful for downstream tasks such as (interactive) segmentation and object detection. For computational efficiency these descriptors are often patch-based, and so struggle to represent the fine features often present in micrographs; they also struggle with the large image sizes present in materials and biological image analysis. In this work, we train a convolutional neural network to upsample low-resolution (i.e, large patch size) foundation model features with reference to the input image. We apply this upsampler network (without any further training) to efficiently featurise and then segment a variety of microscopy images, including plant cells, a lithium-ion battery cathode and organic crystals. The richness of these upsampled features admits separation of hard to segment phases, like hairline cracks. We demonstrate that interactive segmentation with these deep features produces high-quality segmentations far faster and with far fewer labels than training or finetuning a more traditional convolutional network.

</details>


### [9] [Complete Gaussian Splats from a Single Image with Denoising Diffusion Models](https://arxiv.org/abs/2508.21542)
*Ziwei Liao,Mohamed Sayed,Steven L. Waslander,Sara Vicente,Daniyar Turmukhambetov,Michael Firman*

Main category: cs.CV

TL;DR: 提出了一种基于潜在扩散模型的单图像3D场景重建方法，使用高斯泼溅技术来补全遮挡和未观测区域，通过变分自重构器在无监督条件下学习潜在空间，支持高质量360度渲染。


<details>
  <summary>Details</summary>
Motivation: 传统高斯泼溅方法需要密集观测，无法重建遮挡和未观测区域；回归方法只能预测单一模式，导致模糊和不真实的结果，无法处理多可能性解释。

Method: 使用潜在扩散模型学习基于单张输入图像的条件化3D高斯泼溅表示分布；提出变分自重构器从2D图像无监督学习潜在空间，然后在其上训练扩散模型。

Result: 方法能够生成忠实重建结果和多样化样本，成功补全遮挡表面，实现高质量的360度渲染。

Conclusion: 该方法通过生成式建模解决了单图像3D重建中的遮挡补全问题，相比传统回归方法能够产生更真实和多样化的结果。

Abstract: Gaussian splatting typically requires dense observations of the scene and can fail to reconstruct occluded and unobserved areas. We propose a latent diffusion model to reconstruct a complete 3D scene with Gaussian splats, including the occluded parts, from only a single image during inference. Completing the unobserved surfaces of a scene is challenging due to the ambiguity of the plausible surfaces. Conventional methods use a regression-based formulation to predict a single "mode" for occluded and out-of-frustum surfaces, leading to blurriness, implausibility, and failure to capture multiple possible explanations. Thus, they often address this problem partially, focusing either on objects isolated from the background, reconstructing only visible surfaces, or failing to extrapolate far from the input views. In contrast, we propose a generative formulation to learn a distribution of 3D representations of Gaussian splats conditioned on a single input image. To address the lack of ground-truth training data, we propose a Variational AutoReconstructor to learn a latent space only from 2D images in a self-supervised manner, over which a diffusion model is trained. Our method generates faithful reconstructions and diverse samples with the ability to complete the occluded surfaces for high-quality 360-degree renderings.

</details>


### [10] [FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA](https://arxiv.org/abs/2508.21712)
*Alvaro Patricio,Atabak Dehban,Rodrigo Ventura*

Main category: cs.CV

TL;DR: FLORA是一个轻量级的合成数据生成管道，使用LoRA微调Flux 1.1扩散模型，大幅降低计算需求，仅需消费级GPU即可生成高质量合成图像用于目标检测数据增强。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的合成数据生成方法需要企业级GPU和大量合成图像，计算资源需求过高，限制了实际应用。

Method: 使用Flux 1.1 Dev扩散模型，通过低秩适应(LoRA)进行微调，构建轻量级合成数据生成管道。

Result: 在7个目标检测数据集上验证，仅用500张合成图像就能超越ODGEN基线使用5000张图像的性能，mAP@.50:.95提升高达21.3%。

Conclusion: FLORA证明了质量优先、效率导向的方法比暴力生成更有效，仅用10%的数据和少量计算成本就能达到最先进性能，使合成数据生成更加实用和可及。

Abstract: Recent advances in diffusion-based generative models have demonstrated significant potential in augmenting scarce datasets for object detection tasks. Nevertheless, most recent models rely on resource-intensive full fine-tuning of large-scale diffusion models, requiring enterprise-grade GPUs (e.g., NVIDIA V100) and thousands of synthetic images. To address these limitations, we propose Flux LoRA Augmentation (FLORA), a lightweight synthetic data generation pipeline. Our approach uses the Flux 1.1 Dev diffusion model, fine-tuned exclusively through Low-Rank Adaptation (LoRA). This dramatically reduces computational requirements, enabling synthetic dataset generation with a consumer-grade GPU (e.g., NVIDIA RTX 4090). We empirically evaluate our approach on seven diverse object detection datasets. Our results demonstrate that training object detectors with just 500 synthetic images generated by our approach yields superior detection performance compared to models trained on 5000 synthetic images from the ODGEN baseline, achieving improvements of up to 21.3% in mAP@.50:.95. This work demonstrates that it is possible to surpass state-of-the-art performance with far greater efficiency, as FLORA achieves superior results using only 10% of the data and a fraction of the computational cost. This work demonstrates that a quality and efficiency-focused approach is more effective than brute-force generation, making advanced synthetic data creation more practical and accessible for real-world scenarios.

</details>


### [11] [CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models](https://arxiv.org/abs/2508.21732)
*João Valente,Atabak Dehban,Rodrigo Ventura*

Main category: cs.CV

TL;DR: CAD2DMD-SET是一个合成数据生成工具，通过3D CAD模型和高级渲染技术生成数字测量设备(DMD)的视觉问答数据集，显著提升大型视觉语言模型在复杂真实场景下的读数能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在数字测量设备读数任务中表现不佳，特别是在真实世界的杂乱、遮挡、极端视角和运动模糊等挑战性条件下，这限制了其在头戴式摄像头和增强现实应用中的实用性。

Method: 利用3D CAD模型、高级渲染和高保真图像合成技术开发CAD2DMD-SET工具，生成多样化的VQA标注合成DMD数据集。同时创建DMDBench验证集（1000张标注真实图像）用于评估模型性能。

Result: 使用平均归一化Levenshtein相似度(ANLS)对三个最先进LVLM进行基准测试，通过CAD2DMD-SET生成的数据集微调LoRA后，InternVL模型得分提升200%，且在其他任务上性能未下降。

Conclusion: CAD2DMD-SET训练数据集显著提高了LVLM在挑战性条件下的鲁棒性和性能，该工具将作为开源发布，允许社区添加不同测量设备并生成自己的数据集。

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities across various multimodal tasks. They continue, however, to struggle with trivial scenarios such as reading values from Digital Measurement Devices (DMDs), particularly in real-world conditions involving clutter, occlusions, extreme viewpoints, and motion blur; common in head-mounted cameras and Augmented Reality (AR) applications. Motivated by these limitations, this work introduces CAD2DMD-SET, a synthetic data generation tool designed to support visual question answering (VQA) tasks involving DMDs. By leveraging 3D CAD models, advanced rendering, and high-fidelity image composition, our tool produces diverse, VQA-labelled synthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present DMDBench, a curated validation set of 1,000 annotated real-world images designed to evaluate model performance under practical constraints. Benchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein Similarity (ANLS) and further fine-tuning LoRA's of these models with CAD2DMD-SET's generated dataset yielded substantial improvements, with InternVL showcasing a score increase of 200% without degrading on other tasks. This demonstrates that the CAD2DMD-SET training dataset substantially improves the robustness and performance of LVLMs when operating under the previously stated challenging conditions. The CAD2DMD-SET tool is expected to be released as open-source once the final version of this manuscript is prepared, allowing the community to add different measurement devices and generate their own datasets.

</details>


### [12] [The Demon is in Ambiguity: Revisiting Situation Recognition with Single Positive Multi-Label Learning](https://arxiv.org/abs/2508.21816)
*Yiming Lin,Yuchen Niu,Shang Wang,Kaizhu Huang,Qiufeng Wang,Xiao-Bo Jin*

Main category: cs.CV

TL;DR: 本文揭示了场景识别中动词分类本质上是多标签问题，提出了单正例多标签学习框架和GE-VerbMLP模型，在保持传统指标竞争力的同时实现了3%以上的MAP提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法将动词分类视为单标签问题，但视觉事件识别存在固有歧义性，同一图像可能被多个动词类别合理描述，这种单标签设定无法处理动词类别间的语义重叠问题。

Method: 将动词分类重新定义为单正例多标签学习(SPMLL)问题，提出图增强动词多层感知机(GE-VerbMLP)，结合图神经网络捕捉标签相关性，使用对抗训练优化决策边界。

Result: 在真实数据集上的大量实验表明，该方法在保持传统top-1和top-5准确率指标竞争力的同时，实现了超过3%的平均精度均值(MAP)提升。

Conclusion: 动词分类本质上是多标签问题，提出的SPMLL框架和GE-VerbMLP模型有效解决了视觉事件识别中的语义模糊性，为场景识别任务提供了更全面的评估基准和解决方案。

Abstract: Context recognition (SR) is a fundamental task in computer vision that aims to extract structured semantic summaries from images by identifying key events and their associated entities. Specifically, given an input image, the model must first classify the main visual events (verb classification), then identify the participating entities and their semantic roles (semantic role labeling), and finally localize these entities in the image (semantic role localization). Existing methods treat verb classification as a single-label problem, but we show through a comprehensive analysis that this formulation fails to address the inherent ambiguity in visual event recognition, as multiple verb categories may reasonably describe the same image. This paper makes three key contributions: First, we reveal through empirical analysis that verb classification is inherently a multi-label problem due to the ubiquitous semantic overlap between verb categories. Second, given the impracticality of fully annotating large-scale datasets with multiple labels, we propose to reformulate verb classification as a single positive multi-label learning (SPMLL) problem - a novel perspective in SR research. Third, we design a comprehensive multi-label evaluation benchmark for SR that is carefully designed to fairly evaluate model performance in a multi-label setting. To address the challenges of SPMLL, we futher develop the Graph Enhanced Verb Multilayer Perceptron (GE-VerbMLP), which combines graph neural networks to capture label correlations and adversarial training to optimize decision boundaries. Extensive experiments on real-world datasets show that our approach achieves more than 3\% MAP improvement while remaining competitive on traditional top-1 and top-5 accuracy metrics.

</details>
