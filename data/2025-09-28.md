<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 18]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [SeHDR: Single-Exposure HDR Novel View Synthesis via 3D Gaussian Bracketing](https://arxiv.org/abs/2509.20400)
*Yiyu Li,Haoyuan Wang,Ke Xu,Gerhard Petrus Hancke,Rynson W. H. Lau*

Main category: cs.GR

TL;DR: SeHDR是一种从单曝光多视角LDR图像生成HDR新视角的3D高斯溅射方法，无需多曝光输入。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要从不同曝光的多视角LDR图像中学习HDR场景表示，这既繁琐又容易产生误差（如物体运动模糊和校准不准确）。

Method: 首先从单曝光LDR输入学习基础3D高斯，然后估计具有相同几何但不同线性颜色的多个3D高斯，最后通过可微分神经曝光融合将基础和高斯估计整合为HDR高斯进行新视角渲染。

Result: 大量实验表明SeHDR优于现有方法和精心设计的基线。

Conclusion: SeHDR能够从单曝光多视角LDR图像成功学习HDR场景表示，解决了传统方法需要多曝光输入的局限性。

Abstract: This paper presents SeHDR, a novel high dynamic range 3D Gaussian Splatting (HDR-3DGS) approach for generating HDR novel views given multi-view LDR images. Unlike existing methods that typically require the multi-view LDR input images to be captured from different exposures, which are tedious to capture and more likely to suffer from errors (e.g., object motion blurs and calibration/alignment inaccuracies), our approach learns the HDR scene representation from multi-view LDR images of a single exposure. Our key insight to this ill-posed problem is that by first estimating Bracketed 3D Gaussians (i.e., with different exposures) from single-exposure multi-view LDR images, we may then be able to merge these bracketed 3D Gaussians into an HDR scene representation. Specifically, SeHDR first learns base 3D Gaussians from single-exposure LDR inputs, where the spherical harmonics parameterize colors in a linear color space. We then estimate multiple 3D Gaussians with identical geometry but varying linear colors conditioned on exposure manipulations. Finally, we propose the Differentiable Neural Exposure Fusion (NeEF) to integrate the base and estimated 3D Gaussians into HDR Gaussians for novel view rendering. Extensive experiments demonstrate that SeHDR outperforms existing methods as well as carefully designed baselines.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [Seedream 4.0: Toward Next-generation Multimodal Image Generation](https://arxiv.org/abs/2509.20427)
*Team Seedream,Yunpeng Chen,Yu Gao,Lixue Gong,Meng Guo,Qiushan Guo,Zhiyao Guo,Xiaoxia Hou,Weilin Huang,Yixuan Huang,Xiaowen Jian,Huafeng Kuang,Zhichao Lai,Fanshi Li,Liang Li,Xiaochen Lian,Chao Liao,Liyang Liu,Wei Liu,Yanzuo Lu,Zhengxiong Luo,Tongtong Ou,Guang Shi,Yichun Shi,Shiqi Sun,Yu Tian,Zhi Tian,Peng Wang,Rui Wang,Xun Wang,Ye Wang,Guofeng Wu,Jie Wu,Wenxu Wu,Yonghui Wu,Xin Xia,Xuefeng Xiao,Shuang Xu,Xin Yan,Ceyuan Yang,Jianchao Yang,Zhonghua Zhai,Chenlin Zhang,Heng Zhang,Qi Zhang,Xinyu Zhang,Yuwei Zhang,Shijia Zhao,Wenliang Zhao,Wenjia Zhu*

Main category: cs.CV

TL;DR: Seedream 4.0是一个高效的多模态图像生成系统，统一了文本到图像合成、图像编辑和多图像组合功能，采用高效扩散变换器和强大VAE，支持原生高分辨率图像生成，在T2I和多模态图像编辑任务上达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 开发一个统一的框架来处理文本到图像合成、图像编辑和多图像组合任务，扩展传统T2I系统为更交互和多维的创意工具，推动生成AI在创意和专业应用中的边界。

Method: 使用高效扩散变换器和强大VAE减少图像token数量，在数十亿文本-图像对上进行预训练，采用多模态后训练联合训练T2I和图像编辑任务，集成对抗蒸馏、分布匹配、量化和推测解码进行推理加速。

Result: 系统能在1.8秒内生成2K图像（无LLM/VLM作为PE模型），在T2I和多模态图像编辑任务上达到最先进结果，在复杂任务中展示出色的多模态能力，包括精确图像编辑和上下文推理。

Conclusion: Seedream 4.0将传统T2I系统扩展为更交互和多维的创意工具，在创意和专业应用中推动了生成AI的边界，现已可在指定网址访问。

Abstract: We introduce Seedream 4.0, an efficient and high-performance multimodal image generation system that unifies text-to-image (T2I) synthesis, image editing, and multi-image composition within a single framework. We develop a highly efficient diffusion transformer with a powerful VAE which also can reduce the number of image tokens considerably. This allows for efficient training of our model, and enables it to fast generate native high-resolution images (e.g., 1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning diverse taxonomies and knowledge-centric concepts. Comprehensive data collection across hundreds of vertical scenarios, coupled with optimized strategies, ensures stable and large-scale training, with strong generalization. By incorporating a carefully fine-tuned VLM model, we perform multi-modal post-training for training both T2I and image editing tasks jointly. For inference acceleration, we integrate adversarial distillation, distribution matching, and quantization, as well as speculative decoding. It achieves an inference time of up to 1.8 seconds for generating a 2K image (without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream 4.0 can achieve state-of-the-art results on both T2I and multimodal image editing. In particular, it demonstrates exceptional multimodal capabilities in complex tasks, including precise image editing and in-context reasoning, and also allows for multi-image reference, and can generate multiple output images. This extends traditional T2I systems into an more interactive and multidimensional creative tool, pushing the boundary of generative AI for both creativity and professional applications. Seedream 4.0 is now accessible on https://www.volcengine.com/experience/ark?launch=seedream.

</details>


### [3] [Shared Neural Space: Unified Precomputed Feature Encoding for Multi-Task and Cross Domain Vision](https://arxiv.org/abs/2509.20481)
*Jing Li,Oskar Bartosz,Chengyu Wang,Michal Wnuczynski,Dilshan Godaliyadda,Michael Polley*

Main category: cs.CV

TL;DR: 提出通用神经空间（NS），通过编码器-解码器框架预计算跨视觉和成像任务的特征，使多个下游AI模块共享同一特征空间，减少冗余并提高跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 针对当前AI模型在成像和视觉应用中为特定高精度任务定制化设计，导致在模块化任务序列中效率低下的问题，需要一种更高效的解决方案。

Method: 采用轻量级CNN骨干网络构建编码器-解码器框架，学习变换感知、可泛化的表示，在通用神经空间中预计算特征，支持多种下游任务。

Result: 实验证明该方法能够高效执行去马赛克、去噪、深度估计和语义分割等多种成像和视觉任务，同时具有更好的跨域泛化能力。

Conclusion: 通用神经空间架构为高效多任务视觉流水线奠定了基础，相比大型Transformer骨干网络更轻量，具有更广泛的硬件适用性。

Abstract: The majority of AI models in imaging and vision are customized to perform on specific high-precision task. However, this strategy is inefficient for applications with a series of modular tasks, since each requires a mapping into a disparate latent domain. To address this inefficiency, we proposed a universal Neural Space (NS), where an encoder-decoder framework pre-computes features across vision and imaging tasks. Our encoder learns transformation aware, generalizable representations, which enable multiple downstream AI modules to share the same feature space. This architecture reduces redundancy, improves generalization across domain shift, and establishes a foundation for effecient multi-task vision pipelines. Furthermore, as opposed to larger transformer backbones, our backbone is lightweight and CNN-based, allowing for wider across hardware. We furthur demonstrate that imaging and vision modules, such as demosaicing, denoising, depth estimation and semantic segmentation can be performed efficiently in the NS.

</details>


### [4] [InstructVTON: Optimal Auto-Masking and Natural-Language-Guided Interactive Style Control for Inpainting-Based Virtual Try-On](https://arxiv.org/abs/2509.20524)
*Julien Han,Shuwen Qiu,Qi Li,Xingzi Xu,Mehmet Saygin Seyfioglu,Kavosh Asadi,Karim Bouyarmane*

Main category: cs.CV

TL;DR: InstructVTON是一个基于指令的交互式虚拟试穿系统，通过自然语言指导实现细粒度和复杂的风格控制，支持单件或多件服装试穿。


<details>
  <summary>Details</summary>
Motivation: 传统基于掩码的虚拟试穿方法需要精确绘制二进制掩码，这需要专业知识且在某些场景下无法实现（如将长袖卷起），限制了用户体验和功能。

Method: 利用视觉语言模型和图像分割模型根据用户提供的图像和自由文本风格指令自动生成二进制掩码，简化用户操作并支持传统掩码方法无法实现的试穿场景。

Result: InstructVTON能够与现有虚拟试穿模型互操作，实现最先进的结果和风格控制。

Conclusion: 该系统通过自动化掩码生成和自然语言交互，显著提升了虚拟试穿的易用性和功能性，支持更复杂的试穿场景。

Abstract: We present InstructVTON, an instruction-following interactive virtual try-on system that allows fine-grained and complex styling control of the resulting generation, guided by natural language, on single or multiple garments. A computationally efficient and scalable formulation of virtual try-on formulates the problem as an image-guided or image-conditioned inpainting task. These inpainting-based virtual try-on models commonly use a binary mask to control the generation layout. Producing a mask that yields desirable result is difficult, requires background knowledge, might be model dependent, and in some cases impossible with the masking-based approach (e.g. trying on a long-sleeve shirt with "sleeves rolled up" styling on a person wearing long-sleeve shirt with sleeves down, where the mask will necessarily cover the entire sleeve). InstructVTON leverages Vision Language Models (VLMs) and image segmentation models for automated binary mask generation. These masks are generated based on user-provided images and free-text style instructions. InstructVTON simplifies the end-user experience by removing the necessity of a precisely drawn mask, and by automating execution of multiple rounds of image generation for try-on scenarios that cannot be achieved with masking-based virtual try-on models alone. We show that InstructVTON is interoperable with existing virtual try-on models to achieve state-of-the-art results with styling control.

</details>


### [5] [FreeInsert: Personalized Object Insertion with Geometric and Style Control](https://arxiv.org/abs/2509.20756)
*Yuhong Zhang,Han Wang,Yiwen Wang,Rong Xie,Li Song*

Main category: cs.CV

TL;DR: FreeInsert是一个无需训练的图像编辑框架，通过利用3D几何信息将对象插入任意场景，实现几何控制和风格一致性。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法在个性化图像合成任务中存在几何控制不足、风格不一致以及需要大量训练的问题。

Method: 首先将2D对象转换为3D，在3D层面进行交互编辑，然后从指定视角重新渲染为2D图像，结合扩散适配器实现几何、风格和内容控制。

Result: 能够生成几何控制精确、风格一致的编辑图像，无需额外训练。

Conclusion: FreeInsert通过3D几何信息有效解决了图像编辑中的几何控制和风格一致性问题，提供了一种无需训练的高质量图像编辑方案。

Abstract: Text-to-image diffusion models have made significant progress in image generation, allowing for effortless customized generation. However, existing image editing methods still face certain limitations when dealing with personalized image composition tasks. First, there is the issue of lack of geometric control over the inserted objects. Current methods are confined to 2D space and typically rely on textual instructions, making it challenging to maintain precise geometric control over the objects. Second, there is the challenge of style consistency. Existing methods often overlook the style consistency between the inserted object and the background, resulting in a lack of realism. In addition, the challenge of inserting objects into images without extensive training remains significant. To address these issues, we propose \textit{FreeInsert}, a novel training-free framework that customizes object insertion into arbitrary scenes by leveraging 3D geometric information. Benefiting from the advances in existing 3D generation models, we first convert the 2D object into 3D, perform interactive editing at the 3D level, and then re-render it into a 2D image from a specified view. This process introduces geometric controls such as shape or view. The rendered image, serving as geometric control, is combined with style and content control achieved through diffusion adapters, ultimately producing geometrically controlled, style-consistent edited images via the diffusion model.

</details>


### [6] [CusEnhancer: A Zero-Shot Scene and Controllability Enhancement Method for Photo Customization via ResInversion](https://arxiv.org/abs/2509.20775)
*Maoye Ren,Praneetha Vaddamanu,Jianjin Xu,Fernando De la Torre Frade*

Main category: cs.CV

TL;DR: CustomEnhancer是一个零样本增强框架，通过三重流融合生成方法增强身份定制模型，实现高质量的人像生成和精确控制，同时提出ResInversion方法将反演时间减少129倍。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像扩散模型在合成真实人像时面临场景退化、控制不足和感知身份不优的问题，需要改进身份定制模型的能力。

Method: 提出CustomEnhancer框架，利用人脸交换技术和预训练扩散模型获取额外表示，通过三重流融合生成方法结合两个兼容的反向潜在空间来操纵个性化模型的关键空间，统一生成和重建过程。

Result: 实验表明CustomEnhancer在场景多样性、身份保真度和免训练控制方面达到最先进水平，ResInversion方法比NTI效率显著提升。

Conclusion: CustomEnhancer框架有效解决了现有身份定制模型的局限性，实现了高质量、可控的人像生成，同时大幅提升了计算效率。

Abstract: Recently remarkable progress has been made in synthesizing realistic human photos using text-to-image diffusion models. However, current approaches face degraded scenes, insufficient control, and suboptimal perceptual identity. We introduce CustomEnhancer, a novel framework to augment existing identity customization models. CustomEnhancer is a zero-shot enhancement pipeline that leverages face swapping techniques, pretrained diffusion model, to obtain additional representations in a zeroshot manner for encoding into personalized models. Through our proposed triple-flow fused PerGeneration approach, which identifies and combines two compatible counter-directional latent spaces to manipulate a pivotal space of personalized model, we unify the generation and reconstruction processes, realizing generation from three flows. Our pipeline also enables comprehensive training-free control over the generation process of personalized models, offering precise controlled personalization for them and eliminating the need for controller retraining for per-model. Besides, to address the high time complexity of null-text inversion (NTI), we introduce ResInversion, a novel inversion method that performs noise rectification via a pre-diffusion mechanism, reducing the inversion time by 129 times. Experiments demonstrate that CustomEnhancer reach SOTA results at scene diversity, identity fidelity, training-free controls, while also showing the efficiency of our ResInversion over NTI. The code will be made publicly available upon paper acceptance.

</details>


### [7] [DAC-LoRA: Dynamic Adversarial Curriculum for Efficient and Robust Few-Shot Adaptation](https://arxiv.org/abs/2509.20792)
*Ved Umrajkar*

Main category: cs.CV

TL;DR: DAC-LoRA是一种将对抗训练集成到参数高效微调(PEFT)中的新框架，通过动态对抗课程逐步增强Vision-Language Models的鲁棒性，在不显著影响清洁准确率的情况下显著提升对抗攻击抵抗力。


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models在关键应用中广泛使用，但即使使用LoRA等PEFT方法进行高效适配，这些模型仍然容易受到对抗攻击，可能危及安全关键决策。CLIP作为众多下游VLM的骨干网络，其漏洞可能在整个多模态AI生态系统中传播。

Method: 提出Dynamic Adversarial Curriculum (DAC-LoRA)框架，核心原理是智能的渐进式挑战攻击课程。该方法基于一阶平稳条件(FOSC)和TRADES启发式损失函数，可应用于任何迭代攻击方法。

Result: DAC-LoRA在对抗鲁棒性方面取得了实质性改进，同时没有显著损害清洁准确率。该方法轻量级且广泛适用。

Conclusion: DAC-LoRA框架可以轻松集成到标准PEFT流程中，显著增强模型的鲁棒性，为关键应用中的Vision-Language Models提供了有效的安全保护方案。

Abstract: Vision-Language Models (VLMs) are foundational to critical applications like autonomous driving, medical diagnosis, and content moderation. While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA enable their efficient adaptation to specialized tasks, these models remain vulnerable to adversarial attacks that can compromise safety-critical decisions. CLIP, the backbone for numerous downstream VLMs, is a high-value target whose vulnerabilities can cascade across the multimodal AI ecosystem. We propose Dynamic Adversarial Curriculum DAC-LoRA, a novel framework that integrates adversarial training into PEFT. The core principle of our method i.e. an intelligent curriculum of progressively challenging attack, is general and can potentially be applied to any iterative attack method. Guided by the First-Order Stationary Condition (FOSC) and a TRADES-inspired loss, DAC-LoRA achieves substantial improvements in adversarial robustness without significantly compromising clean accuracy. Our work presents an effective, lightweight, and broadly applicable method to demonstrate that the DAC-LoRA framework can be easily integrated into a standard PEFT pipeline to significantly enhance robustness.

</details>


### [8] [Integrating Object Interaction Self-Attention and GAN-Based Debiasing for Visual Question Answering](https://arxiv.org/abs/2509.20884)
*Zhifei Li,Feng Qiu,Yiran Wang,Yujing Xia,Kui Xiao,Miao Zhang,Yan Zhang*

Main category: cs.CV

TL;DR: IOG-VQA模型通过对象交互自注意力和GAN去偏技术提升VQA性能，有效解决数据集偏见问题。


<details>
  <summary>Details</summary>
Motivation: 现有VQA模型存在训练数据偏见问题，过度依赖表面模式，对多样化问题和图像泛化能力不足。

Method: 结合对象交互自注意力机制捕捉图像内对象复杂交互，以及GAN去偏框架生成无偏数据分布。

Result: 在VQA-CP v1和VQA-CP v2数据集上表现优异，特别在处理有偏和不平衡数据分布方面。

Conclusion: 解决对象交互和数据集偏见对推进VQA任务至关重要。

Abstract: Visual Question Answering (VQA) presents a unique challenge by requiring models to understand and reason about visual content to answer questions accurately. Existing VQA models often struggle with biases introduced by the training data, leading to over-reliance on superficial patterns and inadequate generalization to diverse questions and images. This paper presents a novel model, IOG-VQA, which integrates Object Interaction Self-Attention and GAN-Based Debiasing to enhance VQA model performance. The self-attention mechanism allows our model to capture complex interactions between objects within an image, providing a more comprehensive understanding of the visual context. Meanwhile, the GAN-based debiasing framework generates unbiased data distributions, helping the model to learn more robust and generalizable features. By leveraging these two components, IOG-VQA effectively combines visual and textual information to address the inherent biases in VQA datasets. Extensive experiments on the VQA-CP v1 and VQA-CP v2 datasets demonstrate that our model shows excellent performance compared with the existing methods, particularly in handling biased and imbalanced data distributions highlighting the importance of addressing both object interactions and dataset biases in advancing VQA tasks. Our code is available at https://github.com/HubuKG/IOG-VQA.

</details>


### [9] [Nuclear Diffusion Models for Low-Rank Background Suppression in Videos](https://arxiv.org/abs/2509.20886)
*Tristan S. W. Stevens,Oisín Nolan,Jean-Luc Robert,Ruud J. G. van Sloun*

Main category: cs.CV

TL;DR: 提出了一种结合低秩时间建模和扩散后验采样的混合框架Nuclear Diffusion，用于视频去雾，在心脏超声去雾任务中表现优于传统RPCA方法。


<details>
  <summary>Details</summary>
Motivation: 视频序列常包含结构化噪声和背景伪影，传统RPCA方法的稀疏性假设难以捕捉真实视频数据的丰富变异性。

Method: 集成低秩时间建模与扩散后验采样的混合框架，利用模型基础的时间模型和深度生成先验。

Result: 在心脏超声去雾任务中，相比传统RPCA，在对比度增强(gCNR)和信号保持(KS统计量)方面表现更优。

Conclusion: 结合模型基础的时间模型与深度生成先验在高质量视频恢复方面具有潜力。

Abstract: Video sequences often contain structured noise and background artifacts that obscure dynamic content, posing challenges for accurate analysis and restoration. Robust principal component methods address this by decomposing data into low-rank and sparse components. Still, the sparsity assumption often fails to capture the rich variability present in real video data. To overcome this limitation, a hybrid framework that integrates low-rank temporal modeling with diffusion posterior sampling is proposed. The proposed method, Nuclear Diffusion, is evaluated on a real-world medical imaging problem, namely cardiac ultrasound dehazing, and demonstrates improved dehazing performance compared to traditional RPCA concerning contrast enhancement (gCNR) and signal preservation (KS statistic). These results highlight the potential of combining model-based temporal models with deep generative priors for high-fidelity video restoration.

</details>


### [10] [SimDiff: Simulator-constrained Diffusion Model for Physically Plausible Motion Generation](https://arxiv.org/abs/2509.20927)
*Akihisa Watanabe,Jiawei Ren,Li Siyao,Yichen Peng,Erwin Wu,Edgar Simo-Serra*

Main category: cs.CV

TL;DR: SimDiff：一种模拟器约束的扩散模型，通过将环境参数直接集成到去噪过程中，无需推理时的重复模拟器调用即可高效生成物理合理的人体运动。


<details>
  <summary>Details</summary>
Motivation: 现有方法在扩散过程中加入基于模拟器的运动投影层来确保物理合理性，但由于模拟器的顺序性质导致计算成本高昂且无法并行化。

Method: 将模拟器运动投影解释为扩散过程中的引导形式，提出SimDiff模型，通过将环境参数（如重力、风力）直接条件化到去噪过程中来生成物理合理的运动。

Result: SimDiff能够高效生成物理合理的运动，无需推理时的重复模拟器调用，并提供对不同物理系数的细粒度控制，同时成功泛化到未见过的环境参数组合。

Conclusion: SimDiff通过将环境参数直接集成到扩散过程中，实现了高效的物理合理运动生成，并展示了组合泛化能力。

Abstract: Generating physically plausible human motion is crucial for applications such as character animation and virtual reality. Existing approaches often incorporate a simulator-based motion projection layer to the diffusion process to enforce physical plausibility. However, such methods are computationally expensive due to the sequential nature of the simulator, which prevents parallelization. We show that simulator-based motion projection can be interpreted as a form of guidance, either classifier-based or classifier-free, within the diffusion process. Building on this insight, we propose SimDiff, a Simulator-constrained Diffusion Model that integrates environment parameters (e.g., gravity, wind) directly into the denoising process. By conditioning on these parameters, SimDiff generates physically plausible motions efficiently, without repeated simulator calls at inference, and also provides fine-grained control over different physical coefficients. Moreover, SimDiff successfully generalizes to unseen combinations of environmental parameters, demonstrating compositional generalization.

</details>


### [11] [SiNGER: A Clearer Voice Distills Vision Transformers Further](https://arxiv.org/abs/2509.20986)
*Geunhyeok Yu,Sunjae Jeong,Yoonyoung Choi,Jaeseung Kim,Hyoseok Hwang*

Main category: cs.CV

TL;DR: 提出SiNGER蒸馏框架，通过零空间引导的能量重分配来抑制Vision Transformers中的高范数伪影，同时保留信息信号，提升学生模型性能。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers会产生高范数伪影，在知识蒸馏过程中这些伪影会主导目标函数，导致学生模型过度拟合伪影而忽视信息信号，限制了大规模模型的增益。

Method: 使用零空间引导的扰动方法进行教师特征精炼，在精炼过程中保留信息同时抑制伪影，然后通过LoRA适配器将精炼后的教师特征蒸馏给学生模型。

Result: 在多个下游任务中实现了最先进的性能，生成了更清晰和可解释的表征。

Conclusion: SiNGER框架能够有效解决教师特征中伪影和信息信号之间的权衡问题，显著提升学生模型的表现。

Abstract: Vision Transformers are widely adopted as the backbone of vision foundation models, but they are known to produce high-norm artifacts that degrade representation quality. When knowledge distillation transfers these features to students, high-norm artifacts dominate the objective, so students overfit to artifacts and underweight informative signals, diminishing the gains from larger models. Prior work attempted to remove artifacts but encountered an inherent trade-off between artifact suppression and preserving informative signals from teachers. To address this, we introduce Singular Nullspace-Guided Energy Reallocation (SiNGER), a novel distillation framework that suppresses artifacts while preserving informative signals. The key idea is principled teacher feature refinement: during refinement, we leverage the nullspace-guided perturbation to preserve information while suppressing artifacts. Then, the refined teacher's features are distilled to a student. We implement this perturbation efficiently with a LoRA-based adapter that requires minimal structural modification. Extensive experiments show that \oursname consistently improves student models, achieving state-of-the-art performance in multiple downstream tasks and producing clearer and more interpretable representations.

</details>


### [12] [A Single Neuron Works: Precise Concept Erasure in Text-to-Image Diffusion Models](https://arxiv.org/abs/2509.21008)
*Qinqin He,Jiaqi Weng,Jialing Tao,Hui Xue*

Main category: cs.CV

TL;DR: SNCE是一种通过操纵单个神经元来精确防止有害内容生成的新方法，使用稀疏自编码器将文本嵌入映射到解耦的潜在空间，通过调制频率评分识别有害概念相关神经元，实现精准概念擦除。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型在图像生成方面表现出色，但也存在生成有害内容的安全风险。现有概念擦除方法的关键挑战是在精确移除目标概念的同时最小化图像质量下降。

Method: 训练稀疏自编码器将文本嵌入映射到稀疏解耦的潜在空间，设计基于调制频率评分的神经元识别方法准确定位有害概念相关神经元，通过抑制有害概念特定神经元的激活实现概念擦除。

Result: 在各种基准测试中，SNCE在目标概念擦除方面达到了最先进的结果，同时保持了模型对非目标概念的生成能力。此外，该方法对对抗攻击表现出强大的鲁棒性，显著优于现有方法。

Conclusion: SNCE通过单神经元操作实现了精确的概念擦除，在保持图像质量的同时有效防止有害内容生成，具有优异的鲁棒性和实用性。

Abstract: Text-to-image models exhibit remarkable capabilities in image generation. However, they also pose safety risks of generating harmful content. A key challenge of existing concept erasure methods is the precise removal of target concepts while minimizing degradation of image quality. In this paper, we propose Single Neuron-based Concept Erasure (SNCE), a novel approach that can precisely prevent harmful content generation by manipulating only a single neuron. Specifically, we train a Sparse Autoencoder (SAE) to map text embeddings into a sparse, disentangled latent space, where individual neurons align tightly with atomic semantic concepts. To accurately locate neurons responsible for harmful concepts, we design a novel neuron identification method based on the modulated frequency scoring of activation patterns. By suppressing activations of the harmful concept-specific neuron, SNCE achieves surgical precision in concept erasure with minimal disruption to image quality. Experiments on various benchmarks demonstrate that SNCE achieves state-of-the-art results in target concept erasure, while preserving the model's generation capabilities for non-target concepts. Additionally, our method exhibits strong robustness against adversarial attacks, significantly outperforming existing methods.

</details>


### [13] [UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep Decomposition](https://arxiv.org/abs/2509.21086)
*Guojun Lei,Rong Zhang,Chi Wang,Tianhang Liu,Hong Li,Zhiyuan Ma,Weiwei Xu*

Main category: cs.CV

TL;DR: UniTransfer是一个新颖的视频概念迁移架构，通过空间和扩散时间步分解实现精确可控的视频概念迁移，在视觉保真度和可编辑性方面超越现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视频概念迁移方法在精确控制和编辑性方面存在局限，需要一种能够对视频不同组件进行细粒度控制的新架构。

Method: 提出空间分解（前景主体、背景、运动流）和扩散时间步分解的双重分解策略，采用双到单流DiT架构，结合自监督预训练和Chain-of-Prompt机制，利用LLM进行渐进式生成指导。

Result: 在OpenAnimal数据集上的广泛实验表明，该方法能够实现高质量和可控的视频概念迁移，在视觉保真度和可编辑性方面超越现有基线。

Conclusion: UniTransfer通过空间和时间步分解的渐进式范式，为视频概念迁移提供了精确可控的解决方案，展现了优越的性能和编辑能力。

Abstract: We propose a novel architecture UniTransfer, which introduces both spatial and diffusion timestep decomposition in a progressive paradigm, achieving precise and controllable video concept transfer. Specifically, in terms of spatial decomposition, we decouple videos into three key components: the foreground subject, the background, and the motion flow. Building upon this decomposed formulation, we further introduce a dual-to-single-stream DiT-based architecture for supporting fine-grained control over different components in the videos. We also introduce a self-supervised pretraining strategy based on random masking to enhance the decomposed representation learning from large-scale unlabeled video data. Inspired by the Chain-of-Thought reasoning paradigm, we further revisit the denoising diffusion process and propose a Chain-of-Prompt (CoP) mechanism to achieve the timestep decomposition. We decompose the denoising process into three stages of different granularity and leverage large language models (LLMs) for stage-specific instructions to guide the generation progressively. We also curate an animal-centric video dataset called OpenAnimal to facilitate the advancement and benchmarking of research in video concept transfer. Extensive experiments demonstrate that our method achieves high-quality and controllable video concept transfer across diverse reference images and scenes, surpassing existing baselines in both visual fidelity and editability. Web Page: https://yu-shaonian.github.io/UniTransfer-Web/

</details>


### [14] [MotionFlow:Learning Implicit Motion Flow for Complex Camera Trajectory Control in Video Generation](https://arxiv.org/abs/2509.21119)
*Guojun Lei,Chi Wang,Yikai Wang,Hong Li,Ying Song,Weiwei Xu*

Main category: cs.CV

TL;DR: 提出了一种将相机和物体运动整合为像素运动的新方法，通过稳定扩散网络学习参考运动图，结合语义对象先验生成符合指定相机轨迹的视频。


<details>
  <summary>Details</summary>
Motivation: 现有方法分别学习相机和物体运动可能导致相对运动混淆，难以在相机和物体运动同时存在时保持视频一致性和泛化性。

Method: 将相机和物体运动转换为像素运动，使用稳定扩散网络学习参考运动图，结合语义对象先验输入图像到视频网络生成视频。

Result: 大量实验验证本模型大幅优于现有最先进方法。

Conclusion: 提出的方法能有效生成符合指定相机轨迹的视频，同时保持物体运动的一致性。

Abstract: Generating videos guided by camera trajectories poses significant challenges in achieving consistency and generalizability, particularly when both camera and object motions are present. Existing approaches often attempt to learn these motions separately, which may lead to confusion regarding the relative motion between the camera and the objects. To address this challenge, we propose a novel approach that integrates both camera and object motions by converting them into the motion of corresponding pixels. Utilizing a stable diffusion network, we effectively learn reference motion maps in relation to the specified camera trajectory. These maps, along with an extracted semantic object prior, are then fed into an image-to-video network to generate the desired video that can accurately follow the designated camera trajectory while maintaining consistent object motions. Extensive experiments verify that our model outperforms SOTA methods by a large margin.

</details>


### [15] [Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation](https://arxiv.org/abs/2509.21227)
*Seyed Amir Kasaei,Ali Aghayari,Arash Marioriyad,Niki Sepasian,MohammadAmin Fazli,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: 本文对文本-图像生成评估指标进行了广泛研究，发现没有单一指标在所有任务中表现一致，不同指标在不同类型的组合问题上表现各异，VQA类指标并非总是最优，而某些基于嵌入的指标在特定情况下表现更好。


<details>
  <summary>Details</summary>
Motivation: 文本-图像生成发展迅速，但评估输出是否真正捕捉提示中的对象、属性和关系仍是核心挑战。当前评估严重依赖自动化指标，但这些指标往往基于惯例或流行度采用，而非经过人类判断验证。

Method: 对广泛使用的组合文本-图像评估指标进行广泛研究，超越简单相关性分析，检查它们在不同组合挑战中的行为，并比较不同指标家族与人类判断的一致性。

Result: 结果显示没有单一指标在所有任务中表现一致：性能随组合问题类型而变化。VQA类指标虽然流行但并非总是最优，而某些基于嵌入的指标在特定情况下表现更强。仅图像指标对组合评估贡献很小。

Conclusion: 这些发现强调了仔细和透明选择指标的重要性，既是为了可信的评估，也是为了它们在生成中作为奖励模型的使用。

Abstract: Text-image generation has advanced rapidly, but assessing whether outputs truly capture the objects, attributes, and relations described in prompts remains a central challenge. Evaluation in this space relies heavily on automated metrics, yet these are often adopted by convention or popularity rather than validated against human judgment. Because evaluation and reported progress in the field depend directly on these metrics, it is critical to understand how well they reflect human preferences. To address this, we present a broad study of widely used metrics for compositional text-image evaluation. Our analysis goes beyond simple correlation, examining their behavior across diverse compositional challenges and comparing how different metric families align with human judgments. The results show that no single metric performs consistently across tasks: performance varies with the type of compositional problem. Notably, VQA-based metrics, though popular, are not uniformly superior, while certain embedding-based metrics prove stronger in specific cases. Image-only metrics, as expected, contribute little to compositional evaluation, as they are designed for perceptual quality rather than alignment. These findings underscore the importance of careful and transparent metric selection, both for trustworthy evaluation and for their use as reward models in generation. Project page is available at \href{https://amirkasaei.com/eval-the-evals/}{this URL}.

</details>


### [16] [Dense Semantic Matching with VGGT Prior](https://arxiv.org/abs/2509.21263)
*Songlin Yang,Tianyi Wei,Yushi Lan,Zeqi Xiao,Anyi Rao,Xingang Pan*

Main category: cs.CV

TL;DR: 提出了一种基于3D几何基础模型VGGT的语义匹配方法，通过保留VGGT早期特征、微调后期层、添加语义头，并结合循环一致性训练、合成数据增强等技术，解决了现有方法的几何模糊性和最近邻规则限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有语义匹配方法存在两个主要问题：几何模糊性（依赖2D基础模型特征难以区分对称结构）和最近邻规则限制（像素级匹配忽略跨图像不可见性和流形保持）。需要几何感知的像素描述符和整体密集对应机制。

Method: 利用VGGT的几何基础特征和整体密集匹配能力，保留其早期特征阶段，微调后期层，添加语义头实现双向对应。在数据稀缺情况下采用循环一致性训练策略、合成数据增强和渐进式训练方法。

Result: 实验表明该方法在几何感知、匹配可靠性和流形保持方面表现优异，超越了之前的基线方法。

Conclusion: 通过有效利用3D几何基础模型VGGT并结合适当的适应策略，成功解决了语义匹配中的几何模糊性和最近邻规则限制问题，实现了更可靠的像素级对应关系。

Abstract: Semantic matching aims to establish pixel-level correspondences between instances of the same category and represents a fundamental task in computer vision. Existing approaches suffer from two limitations: (i) Geometric Ambiguity: Their reliance on 2D foundation model features (e.g., Stable Diffusion, DINO) often fails to disambiguate symmetric structures, requiring extra fine-tuning yet lacking generalization; (ii) Nearest-Neighbor Rule: Their pixel-wise matching ignores cross-image invisibility and neglects manifold preservation. These challenges call for geometry-aware pixel descriptors and holistic dense correspondence mechanisms. Inspired by recent advances in 3D geometric foundation models, we turn to VGGT, which provides geometry-grounded features and holistic dense matching capabilities well aligned with these needs. However, directly transferring VGGT is challenging, as it was originally designed for geometry matching within cross views of a single instance, misaligned with cross-instance semantic matching, and further hindered by the scarcity of dense semantic annotations. To address this, we propose an approach that (i) retains VGGT's intrinsic strengths by reusing early feature stages, fine-tuning later ones, and adding a semantic head for bidirectional correspondences; and (ii) adapts VGGT to the semantic matching scenario under data scarcity through cycle-consistent training strategy, synthetic data augmentation, and progressive training recipe with aliasing artifact mitigation. Extensive experiments demonstrate that our approach achieves superior geometry awareness, matching reliability, and manifold preservation, outperforming previous baselines.

</details>


### [17] [MedVSR: Medical Video Super-Resolution with Cross State-Space Propagation](https://arxiv.org/abs/2509.21265)
*Xinyu Liu,Guolei Sun,Cheng Wang,Yixuan Yuan,Ender Konukoglu*

Main category: cs.CV

TL;DR: 提出MedVSR框架，专门用于医学视频超分辨率，通过跨状态空间传播解决对齐问题，内部状态空间重建模块增强组织结构并减少伪影，在多个医学场景数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高分辨率医学视频对准确诊断至关重要，但硬件限制和生理约束使其难以获取。现有VSR模型在处理医学视频时面临相机抖动、噪声、帧间突变等独特挑战，导致光流误差和对齐困难，且容易引入伪影和扭曲特征误导医生诊断。

Method: 提出MedVSR框架：1）跨状态空间传播(CSSP)通过将远距离帧投影为状态空间模型中的控制矩阵，选择性地传播一致且信息丰富的特征到相邻帧实现有效对齐；2）内部状态空间重建(ISSR)模块通过联合长距离空间特征学习和大核短距离信息聚合来增强组织结构并减少伪影。

Result: 在包括内窥镜和白内障手术在内的多个医学场景的四个数据集上进行实验，MedVSR在重建性能和效率方面显著优于现有的VSR模型。

Conclusion: MedVSR是针对医学视频超分辨率定制的有效框架，成功解决了医学视频特有的对齐问题和伪影问题，在多种医学场景下表现出优越性能。

Abstract: High-resolution (HR) medical videos are vital for accurate diagnosis, yet are hard to acquire due to hardware limitations and physiological constraints. Clinically, the collected low-resolution (LR) medical videos present unique challenges for video super-resolution (VSR) models, including camera shake, noise, and abrupt frame transitions, which result in significant optical flow errors and alignment difficulties. Additionally, tissues and organs exhibit continuous and nuanced structures, but current VSR models are prone to introducing artifacts and distorted features that can mislead doctors. To this end, we propose MedVSR, a tailored framework for medical VSR. It first employs Cross State-Space Propagation (CSSP) to address the imprecise alignment by projecting distant frames as control matrices within state-space models, enabling the selective propagation of consistent and informative features to neighboring frames for effective alignment. Moreover, we design an Inner State-Space Reconstruction (ISSR) module that enhances tissue structures and reduces artifacts with joint long-range spatial feature learning and large-kernel short-range information aggregation. Experiments across four datasets in diverse medical scenarios, including endoscopy and cataract surgeries, show that MedVSR significantly outperforms existing VSR models in reconstruction performance and efficiency. Code released at https://github.com/CUHK-AIM-Group/MedVSR.

</details>


### [18] [NewtonGen: Physics-Consistent and Controllable Text-to-Video Generation via Neural Newtonian Dynamics](https://arxiv.org/abs/2509.21309)
*Yu Yuan,Xijun Wang,Tharindu Wickremasinghe,Zeeshan Nadir,Bole Ma,Stanley H. Chan*

Main category: cs.CV

TL;DR: 提出了NewtonGen框架，通过可训练的神经牛顿动力学将物理约束注入视频生成过程，解决现有文本到视频生成模型在物理一致性和可控性方面的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的文本到视频生成模型存在物理一致性不足的问题，如物体向上坠落、速度和方向突变等，且缺乏精确的参数控制能力。这些限制源于模型仅从外观学习运动分布，而缺乏对底层动力学的理解。

Method: 提出NewtonGen框架，整合数据驱动合成与可学习物理原理。核心是可训练的神经牛顿动力学(NND)，能够建模和预测各种牛顿运动，从而将潜在动力学约束注入视频生成过程。

Result: 通过联合利用数据先验和动力学指导，NewtonGen实现了物理一致的视频合成，并具备精确的参数控制能力。

Conclusion: NewtonGen框架通过引入可学习的物理原理，显著提升了文本到视频生成的物理一致性和可控性，为解决当前模型在动力学理解方面的局限性提供了有效方案。

Abstract: A primary bottleneck in large-scale text-to-video generation today is physical consistency and controllability. Despite recent advances, state-of-the-art models often produce unrealistic motions, such as objects falling upward, or abrupt changes in velocity and direction. Moreover, these models lack precise parameter control, struggling to generate physically consistent dynamics under different initial conditions. We argue that this fundamental limitation stems from current models learning motion distributions solely from appearance, while lacking an understanding of the underlying dynamics. In this work, we propose NewtonGen, a framework that integrates data-driven synthesis with learnable physical principles. At its core lies trainable Neural Newtonian Dynamics (NND), which can model and predict a variety of Newtonian motions, thereby injecting latent dynamical constraints into the video generation process. By jointly leveraging data priors and dynamical guidance, NewtonGen enables physically consistent video synthesis with precise parameter control.

</details>


### [19] [SD3.5-Flash: Distribution-Guided Distillation of Generative Flows](https://arxiv.org/abs/2509.21318)
*Hmrishav Bandyopadhyay,Rahim Entezari,Jim Scott,Reshinth Adithyan,Yi-Zhe Song,Varun Jampani*

Main category: cs.CV

TL;DR: SD3.5-Flash是一个高效的少步蒸馏框架，可将高质量图像生成带到消费级设备。通过重新制定的分布匹配目标、时间步共享和分时步微调等创新，结合管道优化，实现在不同硬件上的快速生成和内存高效部署。


<details>
  <summary>Details</summary>
Motivation: 将计算密集的整流流模型蒸馏到消费级设备，使先进的生成AI真正实用化部署，从手机到桌面电脑都能使用。

Method: 采用少步蒸馏框架，引入时间步共享减少梯度噪声，分时步微调改进提示对齐，结合文本编码器重构和专用量化等管道优化。

Result: 通过大规模用户研究评估，SD3.5-Flash在少步方法中表现一致优于现有方法。

Conclusion: 该框架实现了生成AI的民主化访问，使高质量图像生成真正实用化部署到各类设备。

Abstract: We present SD3.5-Flash, an efficient few-step distillation framework that brings high-quality image generation to accessible consumer devices. Our approach distills computationally prohibitive rectified flow models through a reformulated distribution matching objective tailored specifically for few-step generation. We introduce two key innovations: "timestep sharing" to reduce gradient noise and "split-timestep fine-tuning" to improve prompt alignment. Combined with comprehensive pipeline optimizations like text encoder restructuring and specialized quantization, our system enables both rapid generation and memory-efficient deployment across different hardware configurations. This democratizes access across the full spectrum of devices, from mobile phones to desktop computers. Through extensive evaluation including large-scale user studies, we demonstrate that SD3.5-Flash consistently outperforms existing few-step methods, making advanced generative AI truly accessible for practical deployment.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [20] [KeyWorld: Key Frame Reasoning Enables Effective and Efficient World Models](https://arxiv.org/abs/2509.21027)
*Sibo Li,Qianyue Hao,Yu Shang,Yong Li*

Main category: cs.RO

TL;DR: KeyWorld是一个改进文本条件机器人世界模型的框架，通过将Transformer计算集中在少量语义关键帧上，同时使用轻量级卷积模型填充中间帧，实现了5.68倍的加速，并提高了生成视频的物理有效性。


<details>
  <summary>Details</summary>
Motivation: 当前机器人世界模型的推理速度和生成轨迹的物理合理性是限制其实际应用的关键瓶颈，主要源于逐帧生成方法的冗余计算和对关键转换语义重要性的忽视。

Method: KeyWorld首先通过迭代简化机器人运动轨迹识别重要转换，获取真实关键帧；然后训练DiT模型从文本任务描述推理生成这些物理意义关键帧；最后使用轻量级插值器高效重建完整视频。

Result: 在LIBERO基准测试中，KeyWorld相比逐帧生成基线实现了5.68倍加速，关注运动感知关键帧进一步提高了生成视频的物理有效性，特别是在复杂任务上。

Conclusion: 该方法为在实时机器人控制和其他需要高效有效世界模型的领域中部署世界模型提供了一条实用路径。

Abstract: Robotic world models are a promising paradigm for forecasting future environment states, yet their inference speed and the physical plausibility of generated trajectories remain critical bottlenecks, limiting their real-world applications. This stems from the redundancy of the prevailing frame-to-frame generation approach, where the model conducts costly computation on similar frames, as well as neglecting the semantic importance of key transitions. To address this inefficiency, we propose KeyWorld, a framework that improves text-conditioned robotic world models by concentrating transformers computation on a few semantic key frames while employing a lightweight convolutional model to fill the intermediate frames. Specifically, KeyWorld first identifies significant transitions by iteratively simplifying the robot's motion trajectories, obtaining the ground truth key frames. Then, a DiT model is trained to reason and generate these physically meaningful key frames from textual task descriptions. Finally, a lightweight interpolator efficiently reconstructs the full video by inpainting all intermediate frames. Evaluations on the LIBERO benchmark demonstrate that KeyWorld achieves a 5.68$\times$ acceleration compared to the frame-to-frame generation baseline, and focusing on the motion-aware key frames further contributes to the physical validity of the generated videos, especially on complex tasks. Our approach highlights a practical path toward deploying world models in real-time robotic control and other domains requiring both efficient and effective world models. Code is released at https://anonymous.4open.science/r/Keyworld-E43D.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [21] [Beyond Visual Similarity: Rule-Guided Multimodal Clustering with explicit domain rules](https://arxiv.org/abs/2509.20501)
*Kishor Datta Gupta,Mohd Ariful Haque,Marufa Kamal,Ahmed Rafi Hasan,Md. Mahfuzur Rahman,Roy George*

Main category: cs.LG

TL;DR: DARTVAE是一个基于规则引导的多模态聚类框架，通过将领域特定约束直接嵌入表示学习过程，结合变分自编码器和LLM生成的规则，实现更符合领域语义的聚类。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法仅依赖输入数据的相似性，难以捕捉许多领域中关键的结构或语义约束。需要开发能够直接整合领域知识的聚类方法。

Method: 扩展VAE架构，将显式规则、语义表示和数据驱动特征嵌入统一潜在空间，通过规则一致性和违反惩罚在损失函数中强制约束合规。规则由LLM生成并构建为知识图谱。

Result: 在飞机和汽车数据集上的实验表明，规则引导的聚类产生了更具操作意义和可解释性的聚类结果（如分离无人机、统一隐形飞机、区分SUV和轿车），同时改进了传统聚类指标。

Conclusion: DARTVAE通过将规则编码与学习表示相结合，比纯数据驱动模型实现了更有意义和一致的聚类结果，突显了约束引导多模态聚类在复杂知识密集型场景中的实用性。

Abstract: Traditional clustering techniques often rely solely on similarity in the input data, limiting their ability to capture structural or semantic constraints that are critical in many domains. We introduce the Domain Aware Rule Triggered Variational Autoencoder (DARTVAE), a rule guided multimodal clustering framework that incorporates domain specific constraints directly into the representation learning process. DARTVAE extends the VAE architecture by embedding explicit rules, semantic representations, and data driven features into a unified latent space, while enforcing constraint compliance through rule consistency and violation penalties in the loss function. Unlike conventional clustering methods that rely only on visual similarity or apply rules as post hoc filters, DARTVAE treats rules as first class learning signals. The rules are generated by LLMs, structured into knowledge graphs, and enforced through a loss function combining reconstruction, KL divergence, consistency, and violation penalties. Experiments on aircraft and automotive datasets demonstrate that rule guided clustering produces more operationally meaningful and interpretable clusters for example, isolating UAVs, unifying stealth aircraft, or separating SUVs from sedans while improving traditional clustering metrics. However, the framework faces challenges: LLM generated rules may hallucinate or conflict, excessive rules risk overfitting, and scaling to complex domains increases computational and consistency difficulties. By combining rule encodings with learned representations, DARTVAE achieves more meaningful and consistent clustering outcomes than purely data driven models, highlighting the utility of constraint guided multimodal clustering for complex, knowledge intensive settings.

</details>


### [22] [FHRFormer: A Self-supervised Transformer Approach for Fetal Heart Rate Inpainting and Forecasting](https://arxiv.org/abs/2509.20852)
*Kjersti Engan,Neel Kanwal,Anita Yeconia,Ladislaus Blacy,Yuda Munyaw,Estomih Mduma,Hege Ersdal*

Main category: cs.LG

TL;DR: 提出基于掩码变换器自编码器的方法来重建缺失的胎儿心率信号，该方法能捕捉数据的空间和频率特征，适用于信号修复和预测。


<details>
  <summary>Details</summary>
Motivation: 约10%的新生儿需要呼吸辅助，5%需要通气支持。胎儿心率监测在产前护理中至关重要，但传感器位移和位置变化常导致信号丢失，限制AI分析的准确性。传统插值方法无法保持信号的频谱特性。

Method: 使用掩码变换器自编码器方法重建缺失的胎儿心率信号，该方法能同时捕捉数据的空间和频率成分，对不同程度的缺失数据具有鲁棒性。

Result: 该方法在不同缺失持续时间下表现出鲁棒性，可用于信号修复和预测，支持AI风险算法开发。

Conclusion: 该方法可应用于回顾性研究数据集，未来可集成到可穿戴胎儿心率监测设备中，实现更早、更稳健的风险检测。

Abstract: Approximately 10\% of newborns require assistance to initiate breathing at birth, and around 5\% need ventilation support. Fetal heart rate (FHR) monitoring plays a crucial role in assessing fetal well-being during prenatal care, enabling the detection of abnormal patterns and supporting timely obstetric interventions to mitigate fetal risks during labor. Applying artificial intelligence (AI) methods to analyze large datasets of continuous FHR monitoring episodes with diverse outcomes may offer novel insights into predicting the risk of needing breathing assistance or interventions. Recent advances in wearable FHR monitors have enabled continuous fetal monitoring without compromising maternal mobility. However, sensor displacement during maternal movement, as well as changes in fetal or maternal position, often lead to signal dropouts, resulting in gaps in the recorded FHR data. Such missing data limits the extraction of meaningful insights and complicates automated (AI-based) analysis. Traditional approaches to handle missing data, such as simple interpolation techniques, often fail to preserve the spectral characteristics of the signals. In this paper, we propose a masked transformer-based autoencoder approach to reconstruct missing FHR signals by capturing both spatial and frequency components of the data. The proposed method demonstrates robustness across varying durations of missing data and can be used for signal inpainting and forecasting. The proposed approach can be applied retrospectively to research datasets to support the development of AI-based risk algorithms. In the future, the proposed method could be integrated into wearable FHR monitoring devices to achieve earlier and more robust risk detection.

</details>


### [23] [A Unified Framework for Diffusion Model Unlearning with f-Divergence](https://arxiv.org/abs/2509.21167)
*Nicola Novello,Federico Fontana,Luigi Cinque,Deniz Gunduz,Andrea M. Tonello*

Main category: cs.LG

TL;DR: 提出了一个基于f-散度的统一框架用于扩散模型遗忘，证明传统MSE方法是该框架的特例，分析了不同f-散度对遗忘质量和收敛性的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型遗忘方法通常依赖最小化目标概念和锚概念输出分布之间的均方误差，但这种方法存在局限性。

Method: 提出了基于f-散度的统一遗忘框架，任何f-散度都可以被利用，提供了选择最优散度的灵活性。

Result: 分析表明不同f-散度主要影响算法的收敛性和遗忘质量，在激进遗忘和概念保留之间提供不同权衡。

Conclusion: 该统一框架为特定应用选择最优散度提供了灵活范式，平衡了不同遗忘需求。

Abstract: Machine unlearning aims to remove specific knowledge from a trained model. While diffusion models (DMs) have shown remarkable generative capabilities, existing unlearning methods for text-to-image (T2I) models often rely on minimizing the mean squared error (MSE) between the output distribution of a target and an anchor concept. We show that this MSE-based approach is a special case of a unified $f$-divergence-based framework, in which any $f$-divergence can be utilized. We analyze the benefits of using different $f$-divergences, that mainly impact the convergence properties of the algorithm and the quality of unlearning. The proposed unified framework offers a flexible paradigm that allows to select the optimal divergence for a specific application, balancing different trade-offs between aggressive unlearning and concept preservation.

</details>
