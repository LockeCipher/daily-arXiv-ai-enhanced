<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 4]
- [cs.CV](#cs.CV) [Total: 37]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.LG](#cs.LG) [Total: 2]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Interactive Drawing Guidance for Anime Illustrations with Diffusion Model](https://arxiv.org/abs/2507.09140)
*Chuang Chen,Xiaoxuan Xie,Yongming Zhang,Tianyu Zhang,Haoran Xie*

Main category: cs.GR

TL;DR: 提出了一种基于StreamDiffusion和LoRA的交互式动漫绘图引导系统，通过实时指导和优化草图提升绘图效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 解决动漫绘图对初学者难度高的问题，提供实时指导以简化创作流程。

Method: 结合StreamDiffusion和LoRA微调Stable Diffusion，将手绘草图转化为动漫风格RGB图像，再通过Informative Drawings模型和优化器生成结构化引导草图。

Result: 用户研究表明系统显著提升了绘图效率和准确性。

Conclusion: 该系统为动漫绘图提供了有效的实时指导工具，适合初学者使用。

Abstract: Creating high-quality anime illustrations presents notable challenges, particularly for beginners, due to the intricate styles and fine details inherent in anime art. We present an interactive drawing guidance system specifically designed for anime illustrations to address this issue. It offers real-time guidance to help users refine their work and streamline the creative process. Our system is built upon the StreamDiffusion pipeline to deliver real-time drawing assistance. We fine-tune Stable Diffusion with LoRA to synthesize anime style RGB images from user-provided hand-drawn sketches and prompts. Leveraging the Informative Drawings model, we transform these RGB images into rough sketches, which are further refined into structured guidance sketches using a custom-designed optimizer. The proposed system offers precise, real-time guidance aligned with the creative intent of the user, significantly enhancing both the efficiency and accuracy of the drawing process. To assess the effectiveness of our approach, we conducted a user study, gathering empirical feedback on both system performance and interface usability.

</details>


### [2] [Physics-Aware Fluid Field Generation from User Sketches Using Helmholtz-Hodge Decomposition](https://arxiv.org/abs/2507.09146)
*Ryuichi Miyauchi,Hengyuan Chang,Tsukasa Fukusato,Kazunori Miyata,Haoran Xie*

Main category: cs.GR

TL;DR: 本文提出了一种交互式设计2D矢量场的方法，通过结合生成模型和物理属性编辑，解决了传统方法难以保持物理特性的问题。


<details>
  <summary>Details</summary>
Motivation: 控制复杂流体行为的挑战性，尤其是生成模型在保持物理属性（如不可压缩性）方面的不足。

Method: 使用潜在扩散模型（LDM）从用户草图中生成初始矢量场，再通过Helmholtz-Hodge分解局部提取物理属性并重新组合。

Result: 实验证明了该方法在保持物理属性的同时，能够满足用户意图。

Conclusion: 提出的方法有效解决了生成模型在物理属性保持上的问题，为流体模拟提供了更直观的设计工具。

Abstract: Fluid simulation techniques are widely used in various fields such as film production, but controlling complex fluid behaviors remains challenging. While recent generative models enable intuitive generation of vector fields from user sketches, they struggle to maintain physical properties such as incompressibility. To address these issues, this paper proposes a method for interactively designing 2D vector fields. Conventional generative models can intuitively generate vector fields from user sketches, but remain difficult to consider physical properties. Therefore, we add a simple editing process after generating the vector field. In the first stage, we use a latent diffusion model~(LDM) to automatically generate initial 2D vector fields from user sketches. In the second stage, we apply the Helmholtz-Hodge decomposition to locally extract physical properties such as incompressibility from the results generated by LDM and recompose them according to user intentions. Through multiple experiments, we demonstrate the effectiveness of our proposed method.

</details>


### [3] [RectifiedHR: High-Resolution Diffusion via Energy Profiling and Adaptive Guidance Scheduling](https://arxiv.org/abs/2507.09441)
*Ankit Sanjyal*

Main category: cs.GR

TL;DR: 提出自适应分类器自由引导（CFG）调度策略，解决扩散模型高分辨率图像合成中的能量不稳定和引导伪影问题。


<details>
  <summary>Details</summary>
Motivation: 高分辨率图像合成中扩散模型的能量不稳定和引导伪影问题影响视觉质量。

Method: 分析采样过程中的潜在能量景观，提出能量感知调度策略，动态调整引导强度。

Result: 采用线性递减CFG调度的DPM++ 2M模型表现最佳，稳定性得分（0.9998）和一致性指标（0.9873）优于固定引导方法。

Conclusion: 能量分析框架为理解和改进扩散模型行为提供了有力工具。

Abstract: High-resolution image synthesis with diffusion models often suffers from energy instabilities and guidance artifacts that degrade visual quality. We analyze the latent energy landscape during sampling and propose adaptive classifier-free guidance (CFG) schedules that maintain stable energy trajectories. Our approach introduces energy-aware scheduling strategies that modulate guidance strength over time, achieving superior stability scores (0.9998) and consistency metrics (0.9873) compared to fixed-guidance approaches. We demonstrate that DPM++ 2M with linear-decreasing CFG scheduling yields optimal performance, providing sharper, more faithful images while reducing artifacts. Our energy profiling framework serves as a powerful diagnostic tool for understanding and improving diffusion model behavior.

</details>


### [4] [ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions](https://arxiv.org/abs/2507.10542)
*Shivangi Aneja,Sebastian Weiss,Irene Baeza,Prashanth Chandran,Gaspard Zoss,Matthias Nießner,Derek Bradley*

Main category: cs.GR

TL;DR: 提出了一种结合局部面部表情与3D高斯泼溅的方法，用于生成高保真、实时的3D头部虚拟形象。


<details>
  <summary>Details</summary>
Motivation: 解决在近距离渲染数字虚拟形象时，捕捉面部微特征和表情的挑战，以实现更真实的表现。

Method: 使用基于局部补丁的表情特征，结合3D高斯泼溅技术，通过补丁级几何模型提取表情并动态合成3D高斯。

Result: ScaffoldAvatar在实时性、多样性和视觉自然性上达到最先进水平。

Conclusion: 通过局部补丁级表情控制，实现了高保真、实时的3D头部虚拟形象生成。

Abstract: Generating high-fidelity real-time animated sequences of photorealistic 3D head avatars is important for many graphics applications, including immersive telepresence and movies. This is a challenging problem particularly when rendering digital avatar close-ups for showing character's facial microfeatures and expressions. To capture the expressive, detailed nature of human heads, including skin furrowing and finer-scale facial movements, we propose to couple locally-defined facial expressions with 3D Gaussian splatting to enable creating ultra-high fidelity, expressive and photorealistic 3D head avatars. In contrast to previous works that operate on a global expression space, we condition our avatar's dynamics on patch-based local expression features and synthesize 3D Gaussians at a patch level. In particular, we leverage a patch-based geometric 3D face model to extract patch expressions and learn how to translate these into local dynamic skin appearance and motion by coupling the patches with anchor points of Scaffold-GS, a recent hierarchical scene representation. These anchors are then used to synthesize 3D Gaussians on-the-fly, conditioned by patch-expressions and viewing direction. We employ color-based densification and progressive training to obtain high-quality results and faster convergence for high resolution 3K training images. By leveraging patch-level expressions, ScaffoldAvatar consistently achieves state-of-the-art performance with visually natural motion, while encompassing diverse facial expressions and styles in real time.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [5] [Detecting Deepfake Talking Heads from Facial Biometric Anomalies](https://arxiv.org/abs/2507.08917)
*Justin D. Norman,Hany Farid*

Main category: cs.CV

TL;DR: 提出了一种基于面部生物特征异常模式的深度学习检测方法，用于识别深度伪造视频。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术容易被用于欺诈、诈骗和政治虚假信息，亟需有效的检测手段。

Method: 利用面部生物特征中的非自然模式，开发了一种新型法医机器学习技术。

Result: 在大规模深度伪造数据集上评估了该技术的有效性，并测试了其对视频篡改的鲁棒性和泛化能力。

Conclusion: 该方法能有效检测深度伪造视频，并对未知生成器具有一定的泛化能力。

Abstract: The combination of highly realistic voice cloning, along with visually compelling avatar, face-swap, or lip-sync deepfake video generation, makes it relatively easy to create a video of anyone saying anything. Today, such deepfake impersonations are often used to power frauds, scams, and political disinformation. We propose a novel forensic machine learning technique for the detection of deepfake video impersonations that leverages unnatural patterns in facial biometrics. We evaluate this technique across a large dataset of deepfake techniques and impersonations, as well as assess its reliability to video laundering and its generalization to previously unseen video deepfake generators.

</details>


### [6] [From images to properties: a NeRF-driven framework for granular material parameter inversion](https://arxiv.org/abs/2507.09005)
*Cheng-Hsi Hsiao,Krishna Kumar*

Main category: cs.CV

TL;DR: 提出了一种结合NeRF和MPM的新框架，通过视觉观测推断颗粒材料特性，摩擦角估计误差在2度以内。


<details>
  <summary>Details</summary>
Motivation: 在无法直接测量的情况下，通过视觉观测表征颗粒材料的特性。

Method: 生成合成实验数据，用NeRF重建3D几何，MPM模拟材料行为，通过贝叶斯优化估计摩擦角。

Result: 摩擦角估计误差在2度以内，验证了方法的有效性。

Conclusion: 该方法为颗粒材料的视觉表征提供了实用解决方案。

Abstract: We introduce a novel framework that integrates Neural Radiance Fields (NeRF) with Material Point Method (MPM) simulation to infer granular material properties from visual observations. Our approach begins by generating synthetic experimental data, simulating an plow interacting with sand. The experiment is rendered into realistic images as the photographic observations. These observations include multi-view images of the experiment's initial state and time-sequenced images from two fixed cameras. Using NeRF, we reconstruct the 3D geometry from the initial multi-view images, leveraging its capability to synthesize novel viewpoints and capture intricate surface details. The reconstructed geometry is then used to initialize material point positions for the MPM simulation, where the friction angle remains unknown. We render images of the simulation under the same camera setup and compare them to the observed images. By employing Bayesian optimization, we minimize the image loss to estimate the best-fitting friction angle. Our results demonstrate that friction angle can be estimated with an error within 2 degrees, highlighting the effectiveness of inverse analysis through purely visual observations. This approach offers a promising solution for characterizing granular materials in real-world scenarios where direct measurement is impractical or impossible.

</details>


### [7] [Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?](https://arxiv.org/abs/2507.09052)
*Fang Chen,Alex Villa,Gongbo Liang,Xiaoyi Lu,Meng Tang*

Main category: cs.CV

TL;DR: 论文提出两种对比损失函数，用于解决类别不平衡扩散模型中尾部类别图像多样性不足的问题，同时保持头部类别的保真度和多样性。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡数据导致尾部类别图像合成多样性不足，需要在不影响头部类别的情况下提升尾部类别的多样性。

Method: 引入两种对比损失函数：无监督InfoNCE损失和MSE损失，通过对比条件与无条件生成增强尾部类别多样性。

Result: 方法在多个数据集（如CIFAR10/100-LT等）上优于标准DDPM和其他替代方法。

Conclusion: 对比学习框架简单有效，成功提升了类别不平衡扩散模型的性能。

Abstract: Training data for class-conditional image synthesis often exhibit a long-tailed distribution with limited images for tail classes. Such an imbalance causes mode collapse and reduces the diversity of synthesized images for tail classes. For class-conditional diffusion models trained on imbalanced data, we aim to improve the diversity of tail class images without compromising the fidelity and diversity of head class images. We achieve this by introducing two deceptively simple but highly effective contrastive loss functions. Firstly, we employ an unsupervised InfoNCE loss utilizing negative samples to increase the distance/dissimilarity among synthetic images, particularly for tail classes. To further enhance the diversity of tail classes, our second loss is an MSE loss that contrasts class-conditional generation with unconditional generation at large timesteps. This second loss makes the denoising process insensitive to class conditions for the initial steps, which enriches tail classes through knowledge sharing from head classes. Conditional-unconditional alignment has been shown to enhance the performance of long-tailed GAN. We are the first to adapt such alignment to diffusion models. We successfully leveraged contrastive learning for class-imbalanced diffusion models. Our contrastive learning framework is easy to implement and outperforms standard DDPM and alternative methods for class-imbalanced diffusion models across various datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and ImageNetLT.

</details>


### [8] [Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning](https://arxiv.org/abs/2507.09102)
*Yiyang Chen,Shanshan Zhao,Lunhao Duan,Changxing Ding,Dacheng Tao*

Main category: cs.CV

TL;DR: PointSD利用文本到图像扩散模型（如Stable Diffusion）的强大能力，通过3D编码器替换文本编码器，构建点云到图像的扩散模型，以增强3D自监督学习。


<details>
  <summary>Details</summary>
Motivation: 现有3D扩散模型受限于小规模数据集，而文本到图像扩散模型（如Stable Diffusion）在大规模数据集上训练，可能弥补这一不足。

Method: 提出PointSD框架，通过点云引导去噪渲染图像，提取SD特征，并训练3D骨干网络对齐这些特征以实现语义学习。

Result: 实验证明PointSD能有效提升点云自监督学习性能。

Conclusion: PointSD通过结合SD模型的能力，成功克服了3D数据集的限制，提升了3D表示学习效果。

Abstract: Diffusion-based models, widely used in text-to-image generation, have proven effective in 2D representation learning. Recently, this framework has been extended to 3D self-supervised learning by constructing a conditional point generator for enhancing 3D representations. However, its performance remains constrained by the 3D diffusion model, which is trained on the available 3D datasets with limited size. We hypothesize that the robust capabilities of text-to-image diffusion models, particularly Stable Diffusion (SD), which is trained on large-scale datasets, can help overcome these limitations. To investigate this hypothesis, we propose PointSD, a framework that leverages the SD model for 3D self-supervised learning. By replacing the SD model's text encoder with a 3D encoder, we train a point-to-image diffusion model that allows point clouds to guide the denoising of rendered noisy images. With the trained point-to-image diffusion model, we use noise-free images as the input and point clouds as the condition to extract SD features. Next, we train a 3D backbone by aligning its features with these SD features, thereby facilitating direct semantic learning. Comprehensive experiments on downstream point cloud tasks and ablation studies demonstrate that the SD model can enhance point cloud self-supervised learning. Code is publicly available at https://github.com/wdttt/PointSD.

</details>


### [9] [Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production](https://arxiv.org/abs/2507.09105)
*Maoxiao Ye,Xinfeng Ye,Mano Manoharan*

Main category: cs.CV

TL;DR: 本文提出了一种结合自回归和扩散模型的混合方法，用于手语生成（SLP），解决了传统方法在推理阶段的错误累积问题，同时提升了生成质量和实时性。


<details>
  <summary>Details</summary>
Motivation: 传统自回归方法在推理时无法避免错误累积，而扩散模型虽能生成高质量结果，但迭代特性限制了实时应用。本文旨在结合两者的优势，提升SLP的性能。

Method: 采用混合自回归和扩散模型的方法，设计了多尺度姿态表示模块和基于置信度的因果注意力机制，以捕捉精细动作并动态引导生成过程。

Result: 在PHOENIX14T和How2Sign数据集上的实验表明，该方法在生成质量和实时流式效率上均表现优异。

Conclusion: 混合方法有效结合了自回归和扩散模型的优势，显著提升了手语生成的准确性和实时性。

Abstract: Earlier Sign Language Production (SLP) models typically relied on autoregressive methods that generate output tokens one by one, which inherently provide temporal alignment. Although techniques like Teacher Forcing can prevent model collapse during training, they still cannot solve the problem of error accumulation during inference, since ground truth is unavailable at that stage. In contrast, more recent approaches based on diffusion models leverage step-by-step denoising to enable high-quality generation. However, the iterative nature of these models and the requirement to denoise entire sequences limit their applicability in real-time tasks like SLP. To address it, we apply a hybrid approach combining autoregressive and diffusion models to SLP for the first time, leveraging the strengths of both models in sequential dependency modeling and output refinement. To capture fine-grained body movements, we design a Multi-Scale Pose Representation module that separately extracts detailed features from distinct articulators and integrates them via a Multi-Scale Fusion module. Furthermore, we introduce a Confidence-Aware Causal Attention mechanism that utilizes joint-level confidence scores to dynamically guide the pose generation process, improving accuracy and robustness. Extensive experiments on the PHOENIX14T and How2Sign datasets demonstrate the effectiveness of our method in both generation quality and real-time streaming efficiency.

</details>


### [10] [$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting](https://arxiv.org/abs/2507.09144)
*Zhimin Liao,Ping Wei,Ruijie Zhang,Shuaijia Chen,Haoxuan Wang,Ziyang Ren*

Main category: cs.CV

TL;DR: 论文提出了一种名为$I^{2}$-World的高效4D占用预测框架，通过解耦场景标记化实现动态3D场景建模，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶系统中复杂3D场景的高效标记化问题，以预测场景演化和生成未见场景。

Method: 采用双标记器设计（场景内和场景间），结合多尺度残差量化和残差聚合时间依赖性，使用编码器-解码器架构实现高效控制和时间一致性。

Result: 在4D占用预测任务中，mIoU和IoU分别提升25.1%和36.9%，训练内存仅需2.9GB，实时推理速度为37.0FPS。

Conclusion: $I^{2}$-World框架在性能和效率上均表现优异，为3D场景建模提供了高效解决方案。

Abstract: Forecasting the evolution of 3D scenes and generating unseen scenarios via occupancy-based world models offers substantial potential for addressing corner cases in autonomous driving systems. While tokenization has revolutionized image and video generation, efficiently tokenizing complex 3D scenes remains a critical challenge for 3D world models. To address this, we propose $I^{2}$-World, an efficient framework for 4D occupancy forecasting. Our method decouples scene tokenization into intra-scene and inter-scene tokenizers. The intra-scene tokenizer employs a multi-scale residual quantization strategy to hierarchically compress 3D scenes while preserving spatial details. The inter-scene tokenizer residually aggregates temporal dependencies across timesteps. This dual design preserves the compactness of 3D tokenizers while retaining the dynamic expressiveness of 4D tokenizers. Unlike decoder-only GPT-style autoregressive models, $I^{2}$-World adopts an encoder-decoder architecture. The encoder aggregates spatial context from the current scene and predicts a transformation matrix to enable high-level control over scene generation. The decoder, conditioned on this matrix and historical tokens, ensures temporal consistency during generation. Experiments demonstrate that $I^{2}$-World achieves state-of-the-art performance, outperforming existing methods by 25.1\% in mIoU and 36.9\% in IoU for 4D occupancy forecasting while exhibiting exceptional computational efficiency: it requires merely 2.9 GB of training memory and achieves real-time inference at 37.0 FPS. Our code is available on https://github.com/lzzzzzm/II-World.

</details>


### [11] [Stable Score Distillation](https://arxiv.org/abs/2507.09168)
*Haiming Zhu,Yangyang Xu,Chenshu Xu,Tingrui Shen,Wenxi Liu,Yong Du,Jun Yu,Shengfeng He*

Main category: cs.CV

TL;DR: 论文提出了一种名为稳定分数蒸馏（SSD）的新方法，用于改进文本引导的图像和3D编辑，解决了现有方法在稳定性、空间控制和编辑强度上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Delta Denoising Score依赖复杂的辅助结构，导致优化信号冲突和局部编辑不精确，因此需要一种更稳定、更高效的方法。

Method: SSD通过锚定单一分类器到源提示，利用Classifier-Free Guidance方程实现跨提示对齐，并引入常数项空文本分支以稳定优化过程。此外，还加入提示增强分支以提高编辑强度。

Result: SSD在2D和3D编辑任务（如NeRF和文本驱动风格编辑）中取得了最先进的结果，收敛更快且复杂度更低。

Conclusion: SSD为文本引导编辑提供了一种稳健高效的解决方案，能够保持原始内容结构并实现平滑、提示特定的修改。

Abstract: Text-guided image and 3D editing have advanced with diffusion-based models, yet methods like Delta Denoising Score often struggle with stability, spatial control, and editing strength. These limitations stem from reliance on complex auxiliary structures, which introduce conflicting optimization signals and restrict precise, localized edits. We introduce Stable Score Distillation (SSD), a streamlined framework that enhances stability and alignment in the editing process by anchoring a single classifier to the source prompt. Specifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves cross-prompt alignment, and introduces a constant term null-text branch to stabilize the optimization process. This approach preserves the original content's structure and ensures that editing trajectories are closely aligned with the source prompt, enabling smooth, prompt-specific modifications while maintaining coherence in surrounding regions. Additionally, SSD incorporates a prompt enhancement branch to boost editing strength, particularly for style transformations. Our method achieves state-of-the-art results in 2D and 3D editing tasks, including NeRF and text-driven style edits, with faster convergence and reduced complexity, providing a robust and efficient solution for text-guided editing.

</details>


### [12] [THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage](https://arxiv.org/abs/2507.09200)
*Trong-Thuan Nguyen,Pha Nguyen,Jackson Cothren,Alper Yilmaz,Minh-Triet Tran,Khoa Luu*

Main category: cs.CV

TL;DR: 论文提出了一种名为THYME的方法，通过层次特征聚合和循环时间细化，解决了动态场景图生成中的空间细节和时间依赖问题，并在新数据集AeroEye-v1.0上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 动态场景理解在自动驾驶、监控和体育分析等应用中需求迫切，但现有方法在空间细节和时间一致性上表现不足。

Method: 提出THYME方法，结合层次特征聚合和循环时间细化，同时建模多尺度空间上下文和时间一致性。

Result: 在ASPIRe和AeroEye-v1.0数据集上，THYME优于现有方法，提升了场景理解的准确性。

Conclusion: THYME方法有效解决了动态场景图生成中的关键问题，为复杂场景理解提供了新思路。

Abstract: The rapid proliferation of video in applications such as autonomous driving, surveillance, and sports analytics necessitates robust methods for dynamic scene understanding. Despite advances in static scene graph generation and early attempts at video scene graph generation, previous methods often suffer from fragmented representations, failing to capture fine-grained spatial details and long-range temporal dependencies simultaneously. To address these limitations, we introduce the Temporal Hierarchical Cyclic Scene Graph (THYME) approach, which synergistically integrates hierarchical feature aggregation with cyclic temporal refinement to address these limitations. In particular, THYME effectively models multi-scale spatial context and enforces temporal consistency across frames, yielding more accurate and coherent scene graphs. In addition, we present AeroEye-v1.0, a novel aerial video dataset enriched with five types of interactivity that overcome the constraints of existing datasets and provide a comprehensive benchmark for dynamic scene graph generation. Empirically, extensive experiments on ASPIRe and AeroEye-v1.0 demonstrate that the proposed THYME approach outperforms state-of-the-art methods, offering improved scene understanding in ground-view and aerial scenarios.

</details>


### [13] [EgoAnimate: Generating Human Animations from Egocentric top-down Views](https://arxiv.org/abs/2507.09230)
*G. Kutay Türkoglu,Julian Tanke,Iheb Belgacem,Lev Markhasin*

Main category: cs.CV

TL;DR: 本文提出了一种基于生成先验的方法，从第一人称视角重建可动画化虚拟形象，利用Stable Diffusion减少训练负担并提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 理想数字远程呈现需要准确复制人的身体、服装和动作，但第一人称视角存在遮挡和身体比例失真问题，现有方法依赖多视角数据集训练。

Method: 基于Stable Diffusion和ControlNet，提出从遮挡的俯视图像生成真实正面视图的流程，并将其输入图像到动作模型。

Result: 实现了从单张俯视图像生成可动画化虚拟形象，减少了训练依赖并提高了泛化能力。

Conclusion: 该方法为更易用和通用的远程呈现系统奠定了基础。

Abstract: An ideal digital telepresence experience requires accurate replication of a person's body, clothing, and movements. To capture and transfer these movements into virtual reality, the egocentric (first-person) perspective can be adopted, which enables the use of a portable and cost-effective device without front-view cameras. However, this viewpoint introduces challenges such as occlusions and distorted body proportions.   There are few works reconstructing human appearance from egocentric views, and none use a generative prior-based approach. Some methods create avatars from a single egocentric image during inference, but still rely on multi-view datasets during training. To our knowledge, this is the first study using a generative backbone to reconstruct animatable avatars from egocentric inputs. Based on Stable Diffusion, our method reduces training burden and improves generalizability.   Inspired by methods such as SiTH and MagicMan, which perform 360-degree reconstruction from a frontal image, we introduce a pipeline that generates realistic frontal views from occluded top-down images using ControlNet and a Stable Diffusion backbone.   Our goal is to convert a single top-down egocentric image into a realistic frontal representation and feed it into an image-to-motion model. This enables generation of avatar motions from minimal input, paving the way for more accessible and generalizable telepresence systems.

</details>


### [14] [Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models](https://arxiv.org/abs/2507.09279)
*Anita Kriz,Elizabeth Laura Janes,Xing Shen,Tal Arbel*

Main category: cs.CV

TL;DR: Prompt4Trust是一个强化学习框架，用于提升多模态大语言模型（MLLMs）在医疗领域的置信度校准，同时提高任务准确性。


<details>
  <summary>Details</summary>
Motivation: MLLMs在医疗应用中存在对提示设计敏感和错误高置信度响应的问题，影响临床决策的可靠性。

Method: 通过轻量级LLM生成上下文感知的辅助提示，指导下游MLLM生成置信度更准确的响应。

Result: 在PMC-VQA基准测试中取得最先进的医学视觉问答性能，并展示了对更大MLLMs的零样本泛化能力。

Conclusion: Prompt4Trust展示了自动化提示工程在提升MLLMs在安全关键场景中可信度的潜力。

Abstract: Multimodal large language models (MLLMs) hold considerable promise for applications in healthcare. However, their deployment in safety-critical settings is hindered by two key limitations: (i) sensitivity to prompt design, and (ii) a tendency to generate incorrect responses with high confidence. As clinicians may rely on a model's stated confidence to gauge the reliability of its predictions, it is especially important that when a model expresses high confidence, it is also highly accurate. We introduce Prompt4Trust, the first reinforcement learning (RL) framework for prompt augmentation targeting confidence calibration in MLLMs. A lightweight LLM is trained to produce context-aware auxiliary prompts that guide a downstream task MLLM to generate responses in which the expressed confidence more accurately reflects predictive accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically prioritizes aspects of calibration most critical for safe and trustworthy clinical decision-making. Beyond improvements driven by this clinically motivated calibration objective, our proposed method also improves task accuracy, achieving state-of-the-art medical visual question answering (VQA) performance on the PMC-VQA benchmark, which is composed of multiple-choice questions spanning diverse medical imaging modalities. Moreover, our framework trained with a small downstream task MLLM showed promising zero-shot generalization to larger MLLMs in our experiments, suggesting the potential for scalable calibration without the associated computational costs. This work demonstrates the potential of automated yet human-aligned prompt engineering for improving the the trustworthiness of MLLMs in safety critical settings. Our codebase can be found at https://github.com/xingbpshen/vccrl-llm.

</details>


### [15] [Generative Latent Kernel Modeling for Blind Motion Deblurring](https://arxiv.org/abs/2507.09285)
*Chenhao Ding,Jiangtao Zhang,Zongsheng Yue,Hui Wang,Qian Zhao,Deyu Meng*

Main category: cs.CV

TL;DR: 提出了一种基于深度生成模型的盲运动去模糊框架，通过预训练的GAN生成模糊核先验分布和初始化器，解决了传统方法对初始模糊核高度敏感的问题。


<details>
  <summary>Details</summary>
Motivation: 传统深度先验方法在盲运动去模糊中因优化过程高度非凸而对初始模糊核极度敏感，限制了性能。

Method: 预训练基于GAN的模糊核生成器和初始化器，约束解在紧凑的潜在核流形中，提升现有方法的性能。

Result: 在挑战性基准数据集上实现了最先进的性能，且无需额外先验即可处理非均匀运动模糊。

Conclusion: 提出的框架有效解决了模糊核初始化敏感问题，提升了盲运动去模糊的整体性能。

Abstract: Deep prior-based approaches have demonstrated remarkable success in blind motion deblurring (BMD) recently. These methods, however, are often limited by the high non-convexity of the underlying optimization process in BMD, which leads to extreme sensitivity to the initial blur kernel. To address this issue, we propose a novel framework for BMD that leverages a deep generative model to encode the kernel prior and induce a better initialization for the blur kernel. Specifically, we pre-train a kernel generator based on a generative adversarial network (GAN) to aptly characterize the kernel's prior distribution, as well as a kernel initializer to provide a well-informed and high-quality starting point for kernel estimation. By combining these two components, we constrain the BMD solution within a compact latent kernel manifold, thus alleviating the aforementioned sensitivity for kernel initialization. Notably, the kernel generator and initializer are designed to be easily integrated with existing BMD methods in a plug-and-play manner, enhancing their overall performance. Furthermore, we extend our approach to tackle blind non-uniform motion deblurring without the need for additional priors, achieving state-of-the-art performance on challenging benchmark datasets. The source code is available at https://github.com/dch0319/GLKM-Deblur.

</details>


### [16] [AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning](https://arxiv.org/abs/2507.09308)
*Zile Wang,Hao Yu,Jiabo Zhan,Chun Yuan*

Main category: cs.CV

TL;DR: 论文提出了ALPHA基准和ALPHAVAE模型，用于生成透明或分层的RGBA图像，显著提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 当前潜在扩散模型在RGB图像合成上表现优异，但RGBA图像生成因缺乏大规模基准而未被充分探索。

Method: 提出ALPHA基准，并开发ALPHAVAE模型，通过扩展预训练的RGB VAE，结合多种目标训练。

Result: ALPHAVAE在仅8K图像上训练，PSNR提升4.9 dB，SSIM提升3.2%，且透明图像生成效果更优。

Conclusion: ALPHAVAE为RGBA图像生成提供了高效解决方案，代码和数据已开源。

Abstract: Recent advances in latent diffusion models have achieved remarkable results in high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress and reconstruct pixel data at low computational cost. However, the generation of transparent or layered content (RGBA image) remains largely unexplored, due to the lack of large-scale benchmarks. In this work, we propose ALPHA, the first comprehensive RGBA benchmark that adapts standard RGB metrics to four-channel images via alpha blending over canonical backgrounds. We further introduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB VAE by incorporating a dedicated alpha channel. The model is trained with a composite objective that combines alpha-blended pixel reconstruction, patch-level fidelity, perceptual consistency, and dual KL divergence constraints to ensure latent fidelity across both RGB and alpha representations. Our RGBA VAE, trained on only 8K images in contrast to 1M used by prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase in SSIM over LayerDiffuse in reconstruction. It also enables superior transparent image generation when fine-tuned within a latent diffusion framework. Our code, data, and models are released on https://github.com/o0o0o00o0/AlphaVAE for reproducibility.

</details>


### [17] [QuarterMap: Efficient Post-Training Token Pruning for Visual State Space Models](https://arxiv.org/abs/2507.09514)
*Tien-Yu Chi,Hung-Yueh Chiang,Diana Marculescu,Kai-Chiang Wu*

Main category: cs.CV

TL;DR: QuarterMap是一种后训练激活剪枝方法，通过移除冗余空间激活并恢复维度，提升VMamba等SSM模型的效率，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 解决VMamba等基于状态空间模型（SSM）的视觉骨干网络在空间扫描中的冗余问题，提升运行效率。

Method: 提出QuarterMap方法，在扫描前剪枝冗余空间激活，并通过最近邻上采样恢复维度。

Result: 在ImageNet-1K上实现11%的速度提升，精度下降小于0.9%；在ADE20K和MedMamba上同样有效。

Conclusion: QuarterMap是一种即插即用的部署效率工具，适用于SSM模型，且不影响迁移性。

Abstract: State space models (SSMs) reduce the quadratic complexity of transformers by leveraging linear recurrence. Recently, VMamba has emerged as a strong SSM-based vision backbone, yet remains bottlenecked by spatial redundancy in its four-directional scan. We propose QuarterMap, a post-training activation pruning method that removes redundant spatial activations before scanning and restores dimensions via nearest-neighbor upsampling. Our method improves throughput without retraining. On ImageNet-1K, QuarterMap achieves up to 11% speedup on VMamba with less than 0.9% accuracy drop, and yields similar gains on ADE20K segmentation. Beyond VMamba, we validate QuarterMap on MedMamba, a domain-specific model that shares the same four-directional scanning structure, where it consistently improves throughput while preserving accuracy across multiple medical imaging tasks. Compared to token merging methods like ToMe, QuarterMap is tailored for SSMs and avoids costly merge-unmerge operations. Our method offers a plug-and-play tool for deployment-time efficiency without compromising transferability.

</details>


### [18] [When Schrödinger Bridge Meets Real-World Image Dehazing with Unpaired Training](https://arxiv.org/abs/2507.09524)
*Yunwei Lan,Zhigao Cui,Xin Luo,Chang Liu,Nian Wang,Menglin Zhang,Yanzhao Su,Dong Liu*

Main category: cs.CV

TL;DR: DehazeSB是一种基于Schrödinger Bridge的新型无配对去雾框架，通过最优传输理论直接连接雾图和清晰图的分布，生成高质量结果。


<details>
  <summary>Details</summary>
Motivation: 现有基于GAN的无配对去雾方法因生成器的传输映射能力有限而效果受限，需要改进。

Method: 利用最优传输理论，结合细节保留正则化和基于CLIP的提示学习，实现高效去雾。

Result: 在多个真实数据集上表现出优越性能。

Conclusion: DehazeSB通过Schrödinger Bridge和细节保留技术，显著提升了无配对去雾的效果。

Abstract: Recent advancements in unpaired dehazing, particularly those using GANs, show promising performance in processing real-world hazy images. However, these methods tend to face limitations due to the generator's limited transport mapping capability, which hinders the full exploitation of their effectiveness in unpaired training paradigms. To address these challenges, we propose DehazeSB, a novel unpaired dehazing framework based on the Schr\"odinger Bridge. By leveraging optimal transport (OT) theory, DehazeSB directly bridges the distributions between hazy and clear images. This enables optimal transport mappings from hazy to clear images in fewer steps, thereby generating high-quality results. To ensure the consistency of structural information and details in the restored images, we introduce detail-preserving regularization, which enforces pixel-level alignment between hazy inputs and dehazed outputs. Furthermore, we propose a novel prompt learning to leverage pre-trained CLIP models in distinguishing hazy images and clear ones, by learning a haze-aware vision-language alignment. Extensive experiments on multiple real-world datasets demonstrate our method's superiority. Code: https://github.com/ywxjm/DehazeSB.

</details>


### [19] [Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges](https://arxiv.org/abs/2507.09562)
*Yidong Jiang*

Main category: cs.CV

TL;DR: 本文首次全面调查了SAM及其变体的提示工程技术，系统整理了该领域的研究进展，揭示了提示工程从简单几何输入到多模态方法的发展，并讨论了优化挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 尽管SAM通过提示方法革新了图像分割，但提示工程的关键作用尚未充分研究，本文旨在填补这一空白。

Method: 系统整理和分析SAM及其变体的提示工程技术，包括方法、应用和挑战。

Result: 研究发现提示工程已从简单几何输入发展为多模态方法，支持SAM在医疗影像和遥感等领域的应用。

Conclusion: 本文为分割基础模型中的提示工程提供了结构化框架，并指出了未来研究方向。

Abstract: The Segment Anything Model (SAM) has revolutionized image segmentation through its innovative prompt-based approach, yet the critical role of prompt engineering in its success remains underexplored. This paper presents the first comprehensive survey focusing specifically on prompt engineering techniques for SAM and its variants. We systematically organize and analyze the rapidly growing body of work in this emerging field, covering fundamental methodologies, practical applications, and key challenges. Our review reveals how prompt engineering has evolved from simple geometric inputs to sophisticated multimodal approaches, enabling SAM's adaptation across diverse domains including medical imaging and remote sensing. We identify unique challenges in prompt optimization and discuss promising research directions. This survey fills an important gap in the literature by providing a structured framework for understanding and advancing prompt engineering in foundation models for segmentation.

</details>


### [20] [MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models](https://arxiv.org/abs/2507.09574)
*Haozhe Zhao,Zefan Cai,Shuzheng Si,Liang Chen,Jiuxiang Gu,Wen Xiao,Junjie Hu*

Main category: cs.CV

TL;DR: MENTOR是一个新型自回归框架，通过两阶段训练实现多模态输入与图像输出的细粒度对齐，提升了生成控制性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到图像模型在精确视觉控制、多模态输入平衡和复杂多模态图像生成训练需求方面的局限性。

Method: 结合自回归图像生成器和两阶段训练范式：多模态对齐阶段和指令调优阶段。

Result: 在DreamBench++基准测试中表现优异，优于基线方法，同时具有更高的图像重建保真度和训练效率。

Conclusion: MENTOR通过高效的多模态条件调优，显著提升了图像生成的精确性和控制性。

Abstract: Recent text-to-image models produce high-quality results but still struggle with precise visual control, balancing multimodal inputs, and requiring extensive training for complex multimodal image generation. To address these limitations, we propose MENTOR, a novel autoregressive (AR) framework for efficient Multimodal-conditioned Tuning for Autoregressive multimodal image generation. MENTOR combines an AR image generator with a two-stage training paradigm, enabling fine-grained, token-level alignment between multimodal inputs and image outputs without relying on auxiliary adapters or cross-attention modules. The two-stage training consists of: (1) a multimodal alignment stage that establishes robust pixel- and semantic-level alignment, followed by (2) a multimodal instruction tuning stage that balances the integration of multimodal inputs and enhances generation controllability. Despite modest model size, suboptimal base components, and limited training resources, MENTOR achieves strong performance on the DreamBench++ benchmark, outperforming competitive baselines in concept preservation and prompt following. Additionally, our method delivers superior image reconstruction fidelity, broad task adaptability, and improved training efficiency compared to diffusion-based methods. Dataset, code, and models are available at: https://github.com/HaozheZhao/MENTOR

</details>


### [21] [Demystifying Flux Architecture](https://arxiv.org/abs/2507.09595)
*Or Greenberg*

Main category: cs.CV

TL;DR: FLUX.1是一个基于扩散的文本到图像生成模型，旨在实现高保真文本-图像对齐，同时保持高质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 尽管FLUX.1是开源的，但缺乏官方技术文档，因此需要逆向工程以支持其作为未来研究和开发的基础。

Method: 通过逆向工程从源代码中解析FLUX.1的架构和训练设置。

Result: 成功解析了FLUX.1的架构，为后续研究提供了支持。

Conclusion: 本报告为FLUX.1的逆向工程提供了技术细节，有助于其在未来研究中的应用。

Abstract: FLUX.1 is a diffusion-based text-to-image generation model developed by Black Forest Labs, designed to achieve faithful text-image alignment while maintaining high image quality and diversity. FLUX is considered state-of-the-art in text-to-image generation, outperforming popular models such as Midjourney, DALL-E 3, Stable Diffusion 3 (SD3), and SDXL. Although publicly available as open source, the authors have not released official technical documentation detailing the model's architecture or training setup. This report summarizes an extensive reverse-engineering effort aimed at demystifying FLUX's architecture directly from its source code, to support its adoption as a backbone for future research and development. This document is an unofficial technical report and is not published or endorsed by the original developers or their affiliated institutions.

</details>


### [22] [Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision Interactive](https://arxiv.org/abs/2507.09612)
*You Huang,Lichao Chen,Jiayi Ji,Liujuan Cao,Shengchuan Zhang,Rongrong Ji*

Main category: cs.CV

TL;DR: Inter2Former通过优化密集令牌处理的计算分配，提出四种关键改进，解决了交互式分割中速度与质量的权衡问题，实现了CPU设备上的高效SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前交互式分割方法在密集令牌处理上存在速度与质量的矛盾：密集令牌方法精度高但速度慢，SAM速度快但质量低。Inter2Former旨在解决这一挑战。

Method: 提出四种改进：1) 动态提示嵌入(DPE)，自适应处理感兴趣区域；2) 动态混合注意力(DHA)，根据区域类型选择注意力机制；3) 混合专家系统(HMoE)，在FFN模块中优化计算；4) 动态局部上采样(DLU)，轻量级MLP定位对象并精细上采样。

Result: 在高精度交互式分割基准测试中，Inter2Former在CPU设备上实现了SOTA性能与高效率。

Conclusion: Inter2Former通过计算优化，成功平衡了交互式分割的速度与质量，为实际应用提供了高效解决方案。

Abstract: Interactive segmentation (IS) improves annotation efficiency by segmenting target regions from user prompts, with widespread applications in real-world scenarios. Current approaches face a critical trade-off: dense-token methods achieve superior accuracy and detail preservation but suffer from prohibitively slow processing on CPU devices, while the Segment Anything Model (SAM) advances the field with sparse prompt tokens for fast inference but compromises segmentation quality. In this paper, we propose Inter2Former to address this challenge by optimizing computation allocation in dense-token processing, which introduces four key enhancements. First, we propose Dynamic Prompt Embedding (DPE) that adaptively processes only regions of interest while avoiding additional overhead from background tokens. Second, we introduce Dynamic Hybrid Attention (DHA), which leverages previous segmentation masks to route tokens through either full attention (O(N2)) for boundary regions or our proposed efficient BSQ attention (O(N)) for non-boundary regions. Third, we develop Hybrid Mixture of Experts (HMoE), which applies similar adaptive computation strategies in FFN modules with CPU-optimized parallel processing. Finally, we present Dynamic Local Upsampling (DLU), a reverse operation of DPE, which localizes objects with a lightweight MLP and performs fine-grained upsampling only in detected regions. Experimental results on high-precision IS benchmarks demonstrate that Inter2Former achieves SOTA performance with high efficiency on CPU devices.

</details>


### [23] [Generate Aligned Anomaly: Region-Guided Few-Shot Anomaly Image-Mask Pair Synthesis for Industrial Inspection](https://arxiv.org/abs/2507.09619)
*Yilin Lu,Jianghang Lin,Linhuang Xie,Kai Zhao,Yansong Qu,Shengchuan Zhang,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: GAA是一个基于区域引导的少样本异常图像-掩码对生成框架，通过预训练的潜在扩散模型生成真实、多样且语义对齐的异常样本，解决了现有方法在异常合成中的低真实性和对齐问题。


<details>
  <summary>Details</summary>
Motivation: 工业制造中异常样本稀缺，现有异常合成方法存在低真实性、掩码对齐不准确和泛化性差的问题。

Method: GAA采用局部概念分解建模异常语义和空间信息，自适应多轮异常聚类增强表示一致性，区域引导掩码生成确保对齐，并引入低质量样本过滤模块。

Result: 在MVTec AD和LOCO数据集上的实验表明，GAA在异常合成质量和下游任务（如定位和分类）中表现优异。

Conclusion: GAA通过创新的区域引导和少样本生成策略，显著提升了异常合成的真实性和下游任务性能。

Abstract: Anomaly inspection plays a vital role in industrial manufacturing, but the scarcity of anomaly samples significantly limits the effectiveness of existing methods in tasks such as localization and classification. While several anomaly synthesis approaches have been introduced for data augmentation, they often struggle with low realism, inaccurate mask alignment, and poor generalization. To overcome these limitations, we propose Generate Aligned Anomaly (GAA), a region-guided, few-shot anomaly image-mask pair generation framework. GAA leverages the strong priors of a pretrained latent diffusion model to generate realistic, diverse, and semantically aligned anomalies using only a small number of samples. The framework first employs Localized Concept Decomposition to jointly model the semantic features and spatial information of anomalies, enabling flexible control over the type and location of anomalies. It then utilizes Adaptive Multi-Round Anomaly Clustering to perform fine-grained semantic clustering of anomaly concepts, thereby enhancing the consistency of anomaly representations. Subsequently, a region-guided mask generation strategy ensures precise alignment between anomalies and their corresponding masks, while a low-quality sample filtering module is introduced to further improve the overall quality of the generated samples. Extensive experiments on the MVTec AD and LOCO datasets demonstrate that GAA achieves superior performance in both anomaly synthesis quality and downstream tasks such as localization and classification.

</details>


### [24] [Brain Stroke Detection and Classification Using CT Imaging with Transformer Models and Explainable AI](https://arxiv.org/abs/2507.09630)
*Shomukh Qari,Maha A. Thafar*

Main category: cs.CV

TL;DR: 提出了一种基于MaxViT的多类别中风分类AI框架，结合数据增强和XAI技术，实现了98%的准确率和F1分数，旨在提升中风早期诊断的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 中风是全球主要死因之一，早期准确诊断对改善患者预后至关重要，尤其在急诊环境中。CT扫描因其快速、可及性和成本效益成为关键影像学手段。

Method: 采用MaxViT作为主要深度学习模型，结合其他Transformer变体（如Vision Transformer和ConvNext），并应用数据增强技术（包括合成图像生成）以提升模型泛化能力。

Result: MaxViT模型在增强数据训练下表现最佳，准确率和F1分数达98.00%，优于其他模型和基线方法。

Conclusion: 研究开发了一种可信赖的AI辅助诊断工具，结合XAI技术（如Grad-CAM++），为临床实践提供了高准确性和可解释性的中风早期检测方案。

Abstract: Stroke is one of the leading causes of death globally, making early and accurate diagnosis essential for improving patient outcomes, particularly in emergency settings where timely intervention is critical. CT scans are the key imaging modality because of their speed, accessibility, and cost-effectiveness. This study proposed an artificial intelligence framework for multiclass stroke classification (ischemic, hemorrhagic, and no stroke) using CT scan images from a dataset provided by the Republic of Turkey's Ministry of Health. The proposed method adopted MaxViT, a state-of-the-art Vision Transformer, as the primary deep learning model for image-based stroke classification, with additional transformer variants (vision transformer, transformer-in-transformer, and ConvNext). To enhance model generalization and address class imbalance, we applied data augmentation techniques, including synthetic image generation. The MaxViT model trained with augmentation achieved the best performance, reaching an accuracy and F1-score of 98.00%, outperforming all other evaluated models and the baseline methods. The primary goal of this study was to distinguish between stroke types with high accuracy while addressing crucial issues of transparency and trust in artificial intelligence models. To achieve this, Explainable Artificial Intelligence (XAI) was integrated into the framework, particularly Grad-CAM++. It provides visual explanations of the model's decisions by highlighting relevant stroke regions in the CT scans and establishing an accurate, interpretable, and clinically applicable solution for early stroke detection. This research contributed to the development of a trustworthy AI-assisted diagnostic tool for stroke, facilitating its integration into clinical practice and enhancing access to timely and optimal stroke diagnosis in emergency departments, thereby saving more lives.

</details>


### [25] [Prompt2DEM: High-Resolution DEMs for Urban and Open Environments from Global Prompts Using a Monocular Foundation Model](https://arxiv.org/abs/2507.09681)
*Osher Rafaeli,Tal Svoray,Ariel Nahlieli*

Main category: cs.CV

TL;DR: 提出了一种基于提示的单目深度估计框架，用于生成高分辨率数字高程模型（DEM），显著提升了分辨率（100倍），并在多样化的景观中表现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 高分辨率高程估计对水文、城市形态和生态系统研究至关重要，但现有方法（如超分辨率技术和单目深度估计）存在局限性。

Method: 使用低分辨率SRTM高程数据作为提示，结合高分辨率RGB图像，通过微调视觉变换器编码器实现DEM估计、填补和更新。

Result: 框架实现了从30米到30厘米的100倍分辨率提升，MAE低于5米，优于SRTM 18%，适用于水文和环境研究。

Conclusion: 该框架为全球高程测绘提供了新范式，具有可扩展性和实用性，代码和模型已公开。

Abstract: High-resolution elevation estimations are essential to understand catchment and hillslope hydrology, study urban morphology and dynamics, and monitor the growth, decline, and mortality of terrestrial ecosystems. Various deep learning approaches (e.g., super-resolution techniques, monocular depth estimation) have been developed to create high-resolution Digital Elevation Models (DEMs). However, super-resolution techniques are limited by the upscaling factor, and monocular depth estimation lacks global elevation context, making its conversion to a seamless DEM restricted. The recently introduced technique of prompt-based monocular depth estimation has opened new opportunities to extract estimates of absolute elevation in a global context. We present here a framework for the estimation of high-resolution DEMs as a new paradigm for absolute global elevation mapping. It is exemplified using low-resolution Shuttle Radar Topography Mission (SRTM) elevation data as prompts and high-resolution RGB imagery from the National Agriculture Imagery Program (NAIP). The approach fine-tunes a vision transformer encoder with LiDAR-derived DEMs and employs a versatile prompting strategy, enabling tasks such as DEM estimation, void filling, and updating. Our framework achieves a 100x resolution gain (from 30-m to 30-cm), surpassing prior methods by an order of magnitude. Evaluations across three diverse U.S. landscapes show robust generalization, capturing urban structures and fine-scale terrain features with < 5 m MAE relative to LiDAR, improving over SRTM by up to 18%. Hydrological analysis confirms suitability for hazard and environmental studies. We demonstrate scalability by applying the framework to large regions in the U.S. and Israel. All code and pretrained models are publicly available at: https://osherr1996.github.io/prompt2dem_propage/.

</details>


### [26] [Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation](https://arxiv.org/abs/2507.09748)
*Yu Lei,Bingde Liu,Qingsong Xie,Haonan Lu,Zhijie Deng*

Main category: cs.CV

TL;DR: 本文提出了一种改进的变分分数蒸馏方法（$L^2$-VSD），通过调整优化顺序和线性化模型，解决了传统VSD方法的收敛慢和不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 传统VSD方法在实践中存在收敛慢和优化不稳定的问题，研究发现这是由于LoRA与3D分布不匹配导致的。

Method: 提出线性化前瞻变分分数蒸馏（$L^2$-VSD），通过调整优化顺序和线性化模型，提高了生成质量和训练稳定性。

Result: 实验证明$L^2$-VSD在生成质量和效率上优于现有方法，并能无缝集成到其他VSD框架中。

Conclusion: $L^2$-VSD通过简单而有效的改进，显著提升了文本到3D生成的性能。

Abstract: Text-to-3D generation based on score distillation of pre-trained 2D diffusion models has gained increasing interest, with variational score distillation (VSD) as a remarkable example. VSD proves that vanilla score distillation can be improved by introducing an extra score-based model, which characterizes the distribution of images rendered from 3D models, to correct the distillation gradient. Despite the theoretical foundations, VSD, in practice, is likely to suffer from slow and sometimes ill-posed convergence. In this paper, we perform an in-depth investigation of the interplay between the introduced score model and the 3D model, and find that there exists a mismatching problem between LoRA and 3D distributions in practical implementation. We can simply adjust their optimization order to improve the generation quality. By doing so, the score model looks ahead to the current 3D state and hence yields more reasonable corrections. Nevertheless, naive lookahead VSD may suffer from unstable training in practice due to the potential over-fitting. To address this, we propose to use a linearized variant of the model for score distillation, giving rise to the Linearized Lookahead Variational Score Distillation ($L^2$-VSD). $L^2$-VSD can be realized efficiently with forward-mode autodiff functionalities of existing deep learning libraries. Extensive experiments validate the efficacy of $L^2$-VSD, revealing its clear superiority over prior score distillation-based methods. We also show that our method can be seamlessly incorporated into any other VSD-based text-to-3D framework.

</details>


### [27] [Counterfactual Visual Explanation via Causally-Guided Adversarial Steering](https://arxiv.org/abs/2507.09881)
*Yiran Qiao,Disheng Liu,Yiren Lu,Yu Yin,Mengnan Du,Jing Ma*

Main category: cs.CV

TL;DR: CECAS框架通过因果引导的对抗方法生成反事实解释，避免虚假因素干扰，提升解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有反事实视觉解释方法忽视因果关系和虚假相关性，导致解释质量受限。

Method: 提出CECAS框架，结合因果视角和对抗方法生成反事实解释。

Result: 在多个基准数据集上表现优于现有方法，平衡了有效性、稀疏性、接近性和真实性。

Conclusion: CECAS通过因果整合显著提升反事实解释的质量和实用性。

Abstract: Recent work on counterfactual visual explanations has contributed to making artificial intelligence models more explainable by providing visual perturbation to flip the prediction. However, these approaches neglect the causal relationships and the spurious correlations behind the image generation process, which often leads to unintended alterations in the counterfactual images and renders the explanations with limited quality. To address this challenge, we introduce a novel framework CECAS, which first leverages a causally-guided adversarial method to generate counterfactual explanations. It innovatively integrates a causal perspective to avoid unwanted perturbations on spurious factors in the counterfactuals. Extensive experiments demonstrate that our method outperforms existing state-of-the-art approaches across multiple benchmark datasets and ultimately achieves a balanced trade-off among various aspects of validity, sparsity, proximity, and realism.

</details>


### [28] [MCGA: Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention](https://arxiv.org/abs/2507.09885)
*Zhanjiang Yang,Lijun Sun,Jiawei Dong,Xiaoxin An,Yang Liu,Meng Li*

Main category: cs.CV

TL;DR: MCGA提出了一种两阶段方法，通过先学习光谱模式再估计映射，解决了RGB到HSI重建的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接学习RGB到HSI的映射，忽略了从低维到高维信息转换的固有挑战。

Method: MCGA采用两阶段方法：1）多尺度VQ-VAE学习光谱模式，提取混合码本（MoC）；2）通过查询MoC特征优化RGB到HSI映射。引入灰度感知注意力和量化自注意力以提升重建质量。

Result: 实验表明MCGA在HSI重建中达到最先进性能。

Conclusion: MCGA通过结合先验知识和物理驱动的注意力机制，实现了轻量高效的HSI重建。

Abstract: Reconstructing hyperspectral images (HSI) from RGB images is a cost-effective solution for various vision-based applications. However, most existing learning-based hyperspectral reconstruction methods directly learn the RGB-to-HSI mapping using complex attention mechanisms, neglecting the inherent challenge of transitioning from low-dimensional to high-dimensional information. To address this limitation, we propose a two-stage approach, MCGA, which first learns spectral patterns before estimating the mapping. In the first stage, a multi-scale VQ-VAE learns representations from heterogeneous HSI datasets, extracting a Mixture of Codebooks (MoC). In the second stage, the RGB-to-HSI mapping is refined by querying features from the MoC to replace latent HSI representations, incorporating prior knowledge rather than forcing a direct high-dimensional transformation. To further enhance reconstruction quality, we introduce Grayscale-Aware Attention and Quantized Self-Attention, which adaptively adjust feature map intensities to meet hyperspectral reconstruction requirements. This physically motivated attention mechanism ensures lightweight and efficient HSI recovery. Moreover, we propose an entropy-based Test-Time Adaptation strategy to improve robustness in real-world scenarios. Extensive experiments demonstrate that our method, MCGA, achieves state-of-the-art performance. The code and models will be released at https://github.com/Fibonaccirabbit/MCGA

</details>


### [29] [IGD: Instructional Graphic Design with Multimodal Layer Generation](https://arxiv.org/abs/2507.09910)
*Yadong Qu,Shancheng Fang,Yuxin Wang,Xiaorui Wang,Zhineng Chen,Hongtao Xie,Yongdong Zhang*

Main category: cs.CV

TL;DR: IGD是一种基于自然语言指令快速生成可编辑多模态层的图形设计方法，解决了现有方法缺乏创造力和非可编辑的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图形设计方法依赖布局生成或生成不可编辑的图像文件，缺乏智能化和实用性。

Method: IGD结合参数化渲染和图像资产生成，利用MLLM的多模态理解能力预测属性、排序和布局，并通过扩散模型生成图像内容。

Result: 实验结果表明，IGD在复杂图形设计任务中具有优越性。

Conclusion: IGD为图形设计提供了新的解决方案，支持可扩展性和可编辑性。

Abstract: Graphic design visually conveys information and data by creating and combining text, images and graphics. Two-stage methods that rely primarily on layout generation lack creativity and intelligence, making graphic design still labor-intensive. Existing diffusion-based methods generate non-editable graphic design files at image level with poor legibility in visual text rendering, which prevents them from achieving satisfactory and practical automated graphic design. In this paper, we propose Instructional Graphic Designer (IGD) to swiftly generate multimodal layers with editable flexibility with only natural language instructions. IGD adopts a new paradigm that leverages parametric rendering and image asset generation. First, we develop a design platform and establish a standardized format for multi-scenario design files, thus laying the foundation for scaling up data. Second, IGD utilizes the multimodal understanding and reasoning capabilities of MLLM to accomplish attribute prediction, sequencing and layout of layers. It also employs a diffusion model to generate image content for assets. By enabling end-to-end training, IGD architecturally supports scalability and extensibility in complex graphic design tasks. The superior experimental results demonstrate that IGD offers a new solution for graphic design.

</details>


### [30] [Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios](https://arxiv.org/abs/2507.09915)
*Siyue Yao,Mingjie Sun,Eng Gee Lim,Ran Yi,Baojiang Zhong,Moncef Gabbouj*

Main category: cs.CV

TL;DR: Crucial-Diff是一个领域无关的框架，通过生成关键样本解决数据稀缺问题，提升检测和分割性能。


<details>
  <summary>Details</summary>
Motivation: 数据稀缺导致模型过拟合和数据集不平衡，现有生成方法生成的样本重复或简单，无法针对下游模型的弱点提供关键信息。

Method: 提出Crucial-Diff框架，包含SAFE模块（统一特征提取）和WASM模块（基于下游模型反馈生成难检测样本）。

Result: 在MVTec上达到83.63%的AP和78.12%的F1-MAX；在息肉数据集上达到81.64%的mIoU和87.69%的mDice。

Conclusion: Crucial-Diff能生成多样且高质量的样本，显著提升下游任务性能。

Abstract: The scarcity of data in various scenarios, such as medical, industry and autonomous driving, leads to model overfitting and dataset imbalance, thus hindering effective detection and segmentation performance. Existing studies employ the generative models to synthesize more training samples to mitigate data scarcity. However, these synthetic samples are repetitive or simplistic and fail to provide "crucial information" that targets the downstream model's weaknesses. Additionally, these methods typically require separate training for different objects, leading to computational inefficiencies. To address these issues, we propose Crucial-Diff, a domain-agnostic framework designed to synthesize crucial samples. Our method integrates two key modules. The Scene Agnostic Feature Extractor (SAFE) utilizes a unified feature extractor to capture target information. The Weakness Aware Sample Miner (WASM) generates hard-to-detect samples using feedback from the detection results of downstream model, which is then fused with the output of SAFE module. Together, our Crucial-Diff framework generates diverse, high-quality training data, achieving a pixel-level AP of 83.63% and an F1-MAX of 78.12% on MVTec. On polyp dataset, Crucial-Diff reaches an mIoU of 81.64% and an mDice of 87.69%. Code will be released after acceptance.

</details>


### [31] [4D-MISR: A unified model for low-dose super-resolution imaging via feature fusion](https://arxiv.org/abs/2507.09953)
*Zifei Wang,Zian Mao,Xiaoya He,Xi Huang,Haoran Zhang,Chun Cheng,Shufen Chu,Tingzheng Hou,Xiaoqin Zeng,Yujun Xie*

Main category: cs.CV

TL;DR: 提出一种基于多图像超分辨率（MISR）和卷积神经网络（CNN）的方法，用于在超低剂量条件下实现原子级分辨率，适用于辐射敏感材料的电子显微镜成像。


<details>
  <summary>Details</summary>
Motivation: 电子显微镜在辐射敏感材料（如蛋白质和二维材料）中的应用受到辐射损伤的限制，需要突破传统电子显微镜的剂量限制。

Method: 结合多图像超分辨率（MISR）和卷积神经网络（CNN），开发了一种双路径、注意力引导的网络，用于4D-STEM，实现超低剂量下的原子级超分辨率成像。

Result: 在超低剂量条件下，该方法的空间分辨率与传统ptychography相当，适用于非晶、半晶和晶态辐射敏感材料。

Conclusion: 该方法扩展了4D-STEM的能力，为辐射敏感材料的结构分析提供了一种通用且高效的新方法。

Abstract: While electron microscopy offers crucial atomic-resolution insights into structure-property relationships, radiation damage severely limits its use on beam-sensitive materials like proteins and 2D materials. To overcome this challenge, we push beyond the electron dose limits of conventional electron microscopy by adapting principles from multi-image super-resolution (MISR) that have been widely used in remote sensing. Our method fuses multiple low-resolution, sub-pixel-shifted views and enhances the reconstruction with a convolutional neural network (CNN) that integrates features from synthetic, multi-angle observations. We developed a dual-path, attention-guided network for 4D-STEM that achieves atomic-scale super-resolution from ultra-low-dose data. This provides robust atomic-scale visualization across amorphous, semi-crystalline, and crystalline beam-sensitive specimens. Systematic evaluations on representative materials demonstrate comparable spatial resolution to conventional ptychography under ultra-low-dose conditions. Our work expands the capabilities of 4D-STEM, offering a new and generalizable method for the structural analysis of radiation-vulnerable materials.

</details>


### [32] [Latent Diffusion Models with Masked AutoEncoders](https://arxiv.org/abs/2507.09984)
*Junho Lee,Jeongwoo Shin,Hyungwook Choi,Joonseok Lee*

Main category: cs.CV

TL;DR: 论文分析了潜在扩散模型（LDMs）中自编码器的关键属性，提出了一种新的变分掩码自编码器（VMAEs），并展示了其在图像生成质量和计算效率上的显著提升。


<details>
  <summary>Details</summary>
Motivation: 尽管潜在扩散模型在图像生成方面具有巨大潜力，但自编码器的理想属性和最优设计尚未充分探索。

Method: 通过分析自编码器在LDMs中的作用，识别了三个关键属性（潜在平滑性、感知压缩质量和重建质量），并提出VMAEs以满足这些属性。将VMAEs集成到LDM框架中，形成LDMAEs。

Result: 实验表明，LDMAEs显著提升了图像生成质量和计算效率。

Conclusion: VMAEs是一种有效的自编码器设计，能够同时满足潜在扩散模型的关键需求。

Abstract: In spite of remarkable potential of the Latent Diffusion Models (LDMs) in image generation, the desired properties and optimal design of the autoencoders have been underexplored. In this work, we analyze the role of autoencoders in LDMs and identify three key properties: latent smoothness, perceptual compression quality, and reconstruction quality. We demonstrate that existing autoencoders fail to simultaneously satisfy all three properties, and propose Variational Masked AutoEncoders (VMAEs), taking advantage of the hierarchical features maintained by Masked AutoEncoder. We integrate VMAEs into the LDM framework, introducing Latent Diffusion Models with Masked AutoEncoders (LDMAEs). Through comprehensive experiments, we demonstrate significantly enhanced image generation quality and computational efficiency.

</details>


### [33] [3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving](https://arxiv.org/abs/2507.09993)
*Yixun Zhang,Lizhi Wang,Junjun Zhao,Wending Zhao,Feng Zhou,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: 3DGAA是一种基于3D高斯分布的对抗攻击框架，通过联合优化几何和外观属性，生成物理上可实现且鲁棒的对抗对象，显著降低目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有2D和3D物理攻击方法在平衡物理真实性和攻击鲁棒性方面存在不足，需要一种更全面的对抗攻击框架。

Method: 利用3D高斯分布的14维参数化，联合优化几何（形状、尺度、旋转）和外观（颜色、不透明度）属性，并引入物理过滤和增强模块以提高泛化能力。

Result: 在虚拟和物理实验中，3DGAA将检测mAP从87.21%降至7.38%，显著优于现有方法，并展示了高迁移性。

Conclusion: 3DGAA为评估自动驾驶感知系统的安全性提供了一种实用的对抗攻击框架。

Abstract: Camera-based object detection systems play a vital role in autonomous driving, yet they remain vulnerable to adversarial threats in real-world environments. While existing 2D and 3D physical attacks typically optimize texture, they often struggle to balance physical realism and attack robustness. In this work, we propose 3D Gaussian-based Adversarial Attack (3DGAA), a novel adversarial object generation framework that leverages the full 14-dimensional parameterization of 3D Gaussian Splatting (3DGS) to jointly optimize geometry and appearance in physically realizable ways. Unlike prior works that rely on patches or texture, 3DGAA jointly perturbs both geometric attributes (shape, scale, rotation) and appearance attributes (color, opacity) to produce physically realistic and transferable adversarial objects. We further introduce a physical filtering module to preserve geometric fidelity, and a physical augmentation module to simulate complex physical scenarios, thus enhancing attack generalization under real-world conditions. We evaluate 3DGAA on both virtual benchmarks and physical-world setups using miniature vehicle models. Experimental results show that 3DGAA achieves to reduce the detection mAP from 87.21% to 7.38%, significantly outperforming existing 3D physical attacks. Moreover, our method maintains high transferability across different physical conditions, demonstrating a new state-of-the-art in physically realizable adversarial attacks. These results validate 3DGAA as a practical attack framework for evaluating the safety of perception systems in autonomous driving.

</details>


### [34] [From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level Controllable Human Image Generation](https://arxiv.org/abs/2507.10217)
*Jeongho Kim,Sunghyun Park,Hyoungwoo Park,Sungrack Yun,Jaegul Choo,Seokeon Cho*

Main category: cs.CV

TL;DR: 提出了一种名为Wardrobe Polyptych LoRA的新方法，通过部分级可控模型实现个性化人类图像生成，减少了计算负担并提高了生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在个性化人类图像生成中面临计算成本高和实时性差的问题，需要解决精确属性保留的挑战。

Method: 通过训练LoRA层，结合空间参考和选择性主题区域损失，减少信息丢失并提高生成一致性。

Result: 实验表明，该方法在保真度和一致性上显著优于现有技术，支持少样本训练和单模型推理。

Conclusion: Wardrobe Polyptych LoRA为个性化人类图像生成提供了一种高效且高质量的解决方案。

Abstract: Recent diffusion models achieve personalization by learning specific subjects, allowing learned attributes to be integrated into generated images. However, personalized human image generation remains challenging due to the need for precise and consistent attribute preservation (e.g., identity, clothing details). Existing subject-driven image generation methods often require either (1) inference-time fine-tuning with few images for each new subject or (2) large-scale dataset training for generalization. Both approaches are computationally expensive and impractical for real-time applications. To address these limitations, we present Wardrobe Polyptych LoRA, a novel part-level controllable model for personalized human image generation. By training only LoRA layers, our method removes the computational burden at inference while ensuring high-fidelity synthesis of unseen subjects. Our key idea is to condition the generation on the subject's wardrobe and leverage spatial references to reduce information loss, thereby improving fidelity and consistency. Additionally, we introduce a selective subject region loss, which encourages the model to disregard some of reference images during training. Our loss ensures that generated images better align with text prompts while maintaining subject integrity. Notably, our Wardrobe Polyptych LoRA requires no additional parameters at the inference stage and performs generation using a single model trained on a few training samples. We construct a new dataset and benchmark tailored for personalized human image generation. Extensive experiments show that our approach significantly outperforms existing techniques in fidelity and consistency, enabling realistic and identity-preserving full-body synthesis.

</details>


### [35] [Transferring Styles for Reduced Texture Bias and Improved Robustness in Semantic Segmentation Networks](https://arxiv.org/abs/2507.10239)
*Ben Hamscher,Edgar Heinert,Annika Mütze,Kira Maag,Matthias Rottmann*

Main category: cs.CV

TL;DR: 通过风格迁移增强语义分割任务，减少纹理偏见并提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究风格迁移是否能减少语义分割中的纹理偏见并增强模型鲁棒性。

Method: 使用基于Voronoi细胞的随机区域风格迁移数据训练语义分割模型。

Result: 风格迁移减少了纹理偏见，显著提升了模型对图像损坏和对抗攻击的鲁棒性。

Conclusion: 风格迁移在语义分割中有效减少纹理偏见并提升鲁棒性，适用于多种架构和数据集。

Abstract: Recent research has investigated the shape and texture biases of deep neural networks (DNNs) in image classification which influence their generalization capabilities and robustness. It has been shown that, in comparison to regular DNN training, training with stylized images reduces texture biases in image classification and improves robustness with respect to image corruptions. In an effort to advance this line of research, we examine whether style transfer can likewise deliver these two effects in semantic segmentation. To this end, we perform style transfer with style varying across artificial image areas. Those random areas are formed by a chosen number of Voronoi cells. The resulting style-transferred data is then used to train semantic segmentation DNNs with the objective of reducing their dependence on texture cues while enhancing their reliance on shape-based features. In our experiments, it turns out that in semantic segmentation, style transfer augmentation reduces texture bias and strongly increases robustness with respect to common image corruptions as well as adversarial attacks. These observations hold for convolutional neural networks and transformer architectures on the Cityscapes dataset as well as on PASCAL Context, showing the generality of the proposed method.

</details>


### [36] [Show and Polish: Reference-Guided Identity Preservation in Face Video Restoration](https://arxiv.org/abs/2507.10293)
*Wenkang Han,Wang Lin,Yiyun Zhou,Qi Liu,Shulei Wang,Chang Yao,Jingyuan Chen*

Main category: cs.CV

TL;DR: IP-FVR是一种新颖的人脸视频修复方法，通过引入高质量参考图像和身份保持机制，解决了传统方法在严重退化情况下难以保留身份特征的问题。


<details>
  <summary>Details</summary>
Motivation: 传统人脸视频修复方法在严重退化时难以保留细粒度的身份特征，导致结果缺乏个体特性。

Method: IP-FVR利用参考图像提供身份条件，采用解耦交叉注意力机制和反馈学习方法，结合余弦相似度奖励信号和时间聚合，减少帧内和帧间身份漂移。

Result: 实验表明，IP-FVR在合成和真实数据集上均优于现有方法，显著提升了修复质量和身份一致性。

Conclusion: IP-FVR在身份保持和视频修复质量方面表现出色，具有实际应用的潜力。

Abstract: Face Video Restoration (FVR) aims to recover high-quality face videos from degraded versions. Traditional methods struggle to preserve fine-grained, identity-specific features when degradation is severe, often producing average-looking faces that lack individual characteristics. To address these challenges, we introduce IP-FVR, a novel method that leverages a high-quality reference face image as a visual prompt to provide identity conditioning during the denoising process. IP-FVR incorporates semantically rich identity information from the reference image using decoupled cross-attention mechanisms, ensuring detailed and identity consistent results. For intra-clip identity drift (within 24 frames), we introduce an identity-preserving feedback learning method that combines cosine similarity-based reward signals with suffix-weighted temporal aggregation. This approach effectively minimizes drift within sequences of frames. For inter-clip identity drift, we develop an exponential blending strategy that aligns identities across clips by iteratively blending frames from previous clips during the denoising process. This method ensures consistent identity representation across different clips. Additionally, we enhance the restoration process with a multi-stream negative prompt, guiding the model's attention to relevant facial attributes and minimizing the generation of low-quality or incorrect features. Extensive experiments on both synthetic and real-world datasets demonstrate that IP-FVR outperforms existing methods in both quality and identity preservation, showcasing its substantial potential for practical applications in face video restoration.

</details>


### [37] [Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching](https://arxiv.org/abs/2507.10318)
*Yuhan Liu,Jingwen Fu,Yang Wu,Kangyi Wu,Pengna Li,Jiayi Wu,Sanping Zhou,Jingmin Xin*

Main category: cs.CV

TL;DR: 论文提出了一种新框架IMD，利用预训练扩散模型解决图像特征匹配中的对齐问题，显著提升了多实例场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在引入基础模型时忽视了单图像理解与跨图像理解需求之间的对齐问题，导致多实例特征匹配效果不佳。

Method: IMD框架包含两部分：1) 使用生成式扩散模型捕捉实例级细节；2) 提出跨图像交互提示模块促进图像对间的双向信息交互。

Result: IMD在常用基准测试中达到新SOTA，并在多实例基准IMIM上提升12%。

Conclusion: IMD有效解决了基础模型在特征匹配中的对齐问题，显著提升了多实例场景下的性能。

Abstract: Leveraging the vision foundation models has emerged as a mainstream paradigm that improves the performance of image feature matching. However, previous works have ignored the misalignment when introducing the foundation models into feature matching. The misalignment arises from the discrepancy between the foundation models focusing on single-image understanding and the cross-image understanding requirement of feature matching. Specifically, 1) the embeddings derived from commonly used foundation models exhibit discrepancies with the optimal embeddings required for feature matching; 2) lacking an effective mechanism to leverage the single-image understanding ability into cross-image understanding. A significant consequence of the misalignment is they struggle when addressing multi-instance feature matching problems. To address this, we introduce a simple but effective framework, called IMD (Image feature Matching with a pre-trained Diffusion model) with two parts: 1) Unlike the dominant solutions employing contrastive-learning based foundation models that emphasize global semantics, we integrate the generative-based diffusion models to effectively capture instance-level details. 2) We leverage the prompt mechanism in generative model as a natural tunnel, propose a novel cross-image interaction prompting module to facilitate bidirectional information interaction between image pairs. To more accurately measure the misalignment, we propose a new benchmark called IMIM, which focuses on multi-instance scenarios. Our proposed IMD establishes a new state-of-the-art in commonly evaluated benchmarks, and the superior improvement 12% in IMIM indicates our method efficiently mitigates the misalignment.

</details>


### [38] [Text Embedding Knows How to Quantize Text-Guided Diffusion Models](https://arxiv.org/abs/2507.10340)
*Hongjae Lee,Myungjun Son,Dongjea Kang,Seung-Won Jung*

Main category: cs.CV

TL;DR: QLIP是一种新的量化方法，利用文本提示指导扩散模型的逐层比特精度选择，降低计算复杂度并提升生成图像质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成任务中表现出色，但计算复杂度高，限制了其在资源受限环境中的应用。现有量化方法未充分利用输入条件（如文本提示）的信息。

Method: 提出QLIP方法，通过文本提示动态选择每层和每个时间步的比特精度，并可无缝集成到现有量化方法中。

Result: 实验表明，QLIP能有效降低计算复杂度，并在多个数据集上提升生成图像质量。

Conclusion: QLIP为扩散模型量化提供了高效且灵活的解决方案，尤其适用于资源受限环境。

Abstract: Despite the success of diffusion models in image generation tasks such as text-to-image, the enormous computational complexity of diffusion models limits their use in resource-constrained environments. To address this, network quantization has emerged as a promising solution for designing efficient diffusion models. However, existing diffusion model quantization methods do not consider input conditions, such as text prompts, as an essential source of information for quantization. In this paper, we propose a novel quantization method dubbed Quantization of Language-to-Image diffusion models using text Prompts (QLIP). QLIP leverages text prompts to guide the selection of bit precision for every layer at each time step. In addition, QLIP can be seamlessly integrated into existing quantization methods to enhance quantization efficiency. Our extensive experiments demonstrate the effectiveness of QLIP in reducing computational complexity and improving the quality of the generated images across various datasets.

</details>


### [39] [RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening](https://arxiv.org/abs/2507.10461)
*Tao Tang,Chengxu Yang*

Main category: cs.CV

TL;DR: RAPNet是一种新的遥感图像融合架构，通过内容自适应卷积和动态特征融合模块提升空间细节提取和光谱保真度。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在遥感图像融合中因卷积核的均匀应用而忽略局部内容变化，限制了性能。

Method: 提出RAPNet，采用Receptive-field Adaptive Pansharpening Convolution (RAPConv)和Pansharpening Dynamic Feature Fusion (PAN-DFF)模块，实现空间自适应卷积和特征融合。

Result: 在公开数据集上，RAPNet在定量和定性评估中均优于现有方法。

Conclusion: RAPNet通过自适应组件显著提升了遥感图像融合的性能。

Abstract: Pansharpening refers to the process of integrating a high resolution panchromatic (PAN) image with a lower resolution multispectral (MS) image to generate a fused product, which is pivotal in remote sensing. Despite the effectiveness of CNNs in addressing this challenge, they are inherently constrained by the uniform application of convolutional kernels across all spatial positions, overlooking local content variations. To overcome this issue, we introduce RAPNet, a new architecture that leverages content-adaptive convolution. At its core, RAPNet employs the Receptive-field Adaptive Pansharpening Convolution (RAPConv), designed to produce spatially adaptive kernels responsive to local feature context, thereby enhancing the precision of spatial detail extraction. Additionally, the network integrates the Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an attention mechanism to achieve an optimal balance between spatial detail enhancement and spectral fidelity. Comprehensive evaluations on publicly available datasets confirm that RAPNet delivers superior performance compared to existing approaches, as demonstrated by both quantitative metrics and qualitative assessments. Ablation analyses further substantiate the effectiveness of the proposed adaptive components.

</details>


### [40] [RefSTAR: Blind Facial Image Restoration with Reference Selection, Transfer, and Reconstruction](https://arxiv.org/abs/2507.10470)
*Zhicun Yin,Junjie Chen,Ming Liu,Zhixin Wang,Fan Li,Renjing Pei,Xiaoming Li,Rynson W. H. Lau,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 本文提出了一种名为RefSTAR的盲人脸图像修复方法，通过参考图像的选择、特征转移和重建，解决了身份保留问题。


<details>
  <summary>Details</summary>
Motivation: 盲人脸图像修复因未知的复杂退化和人类对脸部的敏感性而极具挑战性，现有方法在身份保留上存在不足。

Method: 提出RefSTAR方法，包括参考选择模块（RefSel）、特征融合范式及重建机制，并结合循环一致性损失。

Result: 实验表明，RefSTAR在身份保留和参考特征转移质量上表现优异。

Conclusion: RefSTAR通过有效引入参考图像特征，显著提升了盲人脸图像修复的效果。

Abstract: Blind facial image restoration is highly challenging due to unknown complex degradations and the sensitivity of humans to faces. Although existing methods introduce auxiliary information from generative priors or high-quality reference images, they still struggle with identity preservation problems, mainly due to improper feature introduction on detailed textures. In this paper, we focus on effectively incorporating appropriate features from high-quality reference images, presenting a novel blind facial image restoration method that considers reference selection, transfer, and reconstruction (RefSTAR). In terms of selection, we construct a reference selection (RefSel) module. For training the RefSel module, we construct a RefSel-HQ dataset through a mask generation pipeline, which contains annotating masks for 10,000 ground truth-reference pairs. As for the transfer, due to the trivial solution in vanilla cross-attention operations, a feature fusion paradigm is designed to force the features from the reference to be integrated. Finally, we propose a reference image reconstruction mechanism that further ensures the presence of reference image features in the output image. The cycle consistency loss is also redesigned in conjunction with the mask. Extensive experiments on various backbone models demonstrate superior performance, showing better identity preservation ability and reference feature transfer quality. Source code, dataset, and pre-trained models are available at https://github.com/yinzhicun/RefSTAR.

</details>


### [41] [Quantize-then-Rectify: Efficient VQ-VAE Training](https://arxiv.org/abs/2507.10547)
*Borui Zhang,Qihang Rao,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: ReVQ框架通过预训练的VAE快速训练VQ-VAE，显著降低计算成本，同时保持高质量重建。


<details>
  <summary>Details</summary>
Motivation: 解决高压缩率VQ-VAE训练计算成本高的问题。

Method: 提出Quantize-then-Rectify (ReVQ)框架，结合通道多组量化和后矫正器。

Result: 在ImageNet上压缩至512个token，rFID=1.06，训练成本降低两个数量级。

Conclusion: ReVQ在效率和重建质量间取得优越平衡。

Abstract: Visual tokenizers are pivotal in multimodal large models, acting as bridges between continuous inputs and discrete tokens. Nevertheless, training high-compression-rate VQ-VAEs remains computationally demanding, often necessitating thousands of GPU hours. This work demonstrates that a pre-trained VAE can be efficiently transformed into a VQ-VAE by controlling quantization noise within the VAE's tolerance threshold. We present \textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs to enable rapid VQ-VAE training with minimal computational overhead. By integrating \textbf{channel multi-group quantization} to enlarge codebook capacity and a \textbf{post rectifier} to mitigate quantization errors, ReVQ compresses ImageNet images into at most 512 tokens while sustaining competitive reconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training costs by over two orders of magnitude relative to state-of-the-art approaches: ReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours, whereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental results show that ReVQ achieves superior efficiency-reconstruction trade-offs.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [42] [PanoDiff-SR: Synthesizing Dental Panoramic Radiographs using Diffusion and Super-resolution](https://arxiv.org/abs/2507.09227)
*Sanyam Jain,Bruna Neves de Freitas,Andreas Basse-OConnor,Alexandros Iosifidis,Ruben Pauwels*

Main category: eess.IV

TL;DR: 本文提出了一种结合扩散生成（PanoDiff）和超分辨率（SR）的方法，用于生成高质量合成牙科全景X光片（PRs），以解决医学图像数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 近年来，对高质量合成医学图像的需求增加，以缓解人工智能研究中公开数据集的稀缺问题，并用于教育目的。

Method: 采用扩散生成模型生成低分辨率（256 X 128）PRs种子，再通过超分辨率模型提升至高分辨率（1024 X 512）。SR模型采用先进的transformer，学习局部-全局关系，以生成更清晰的边缘和纹理。

Result: 实验结果显示，高分辨率合成图像与真实图像的Frechet inception距离为40.69，Inception分数分别为2.55（真实HR）、2.30（合成HR）、2.90（真实LR）和2.98（合成LR）。临床专家区分真实与合成图像的平均准确率为68.5%。

Conclusion: 该方法成功生成了高质量的合成牙科全景X光片，且在专家评估中表现出接近真实图像的质量。

Abstract: There has been increasing interest in the generation of high-quality, realistic synthetic medical images in recent years. Such synthetic datasets can mitigate the scarcity of public datasets for artificial intelligence research, and can also be used for educational purposes. In this paper, we propose a combination of diffusion-based generation (PanoDiff) and Super-Resolution (SR) for generating synthetic dental panoramic radiographs (PRs). The former generates a low-resolution (LR) seed of a PR (256 X 128) which is then processed by the SR model to yield a high-resolution (HR) PR of size 1024 X 512. For SR, we propose a state-of-the-art transformer that learns local-global relationships, resulting in sharper edges and textures. Experimental results demonstrate a Frechet inception distance score of 40.69 between 7243 real and synthetic images (in HR). Inception scores were 2.55, 2.30, 2.90 and 2.98 for real HR, synthetic HR, real LR and synthetic LR images, respectively. Among a diverse group of six clinical experts, all evaluating a mixture of 100 synthetic and 100 real PRs in a time-limited observation, the average accuracy in distinguishing real from synthetic images was 68.5% (with 50% corresponding to random guessing).

</details>


### [43] [prNet: Data-Driven Phase Retrieval via Stochastic Refinement](https://arxiv.org/abs/2507.09608)
*Mehmet Onurcan Kaya,Figen S. Oktem*

Main category: eess.IV

TL;DR: 提出了一种基于Langevin动力学的新型相位检索框架，通过高效的后验采样平衡失真与感知质量。


<details>
  <summary>Details</summary>
Motivation: 传统方法过于关注像素级精度，而忽视了感知质量与失真的权衡。

Method: 结合随机采样、学习去噪和基于模型的更新，设计了三种复杂度递增的变体，包括Langevin推断、自适应噪声调度学习和并行采样。

Result: 在多个基准测试中实现了最先进的性能，兼顾了保真度和感知质量。

Conclusion: 该框架为相位检索提供了一种新的高效方法，显著提升了感知质量与失真的平衡。

Abstract: We propose a novel framework for phase retrieval that leverages Langevin dynamics to enable efficient posterior sampling, yielding reconstructions that explicitly balance distortion and perceptual quality. Unlike conventional approaches that prioritize pixel-wise accuracy, our method navigates the perception-distortion tradeoff through a principled combination of stochastic sampling, learned denoising, and model-based updates. The framework comprises three variants of increasing complexity, integrating theoretically grounded Langevin inference, adaptive noise schedule learning, parallel reconstruction sampling, and warm-start initialization from classical solvers. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple benchmarks, both in terms of fidelity and perceptual quality.

</details>


### [44] [AI-Enhanced Pediatric Pneumonia Detection: A CNN-Based Approach Using Data Augmentation and Generative Adversarial Networks (GANs)](https://arxiv.org/abs/2507.09759)
*Abdul Manaf,Nimra Mughal*

Main category: eess.IV

TL;DR: 该研究提出了一种基于机器学习的儿科胸部肺炎分类系统，利用CNN模型和GAN生成合成图像，解决了数据不足和类别不平衡问题，最终通过Flask应用实现实时分类。


<details>
  <summary>Details</summary>
Motivation: 肺炎是五岁以下儿童死亡的主要原因，需要准确的胸部X光诊断。研究旨在通过深度学习提高诊断效率和准确性，特别是在资源有限的临床环境中。

Method: 使用5,863张标记的儿童胸部X光图像训练CNN模型，采用数据增强（旋转、缩放、剪切、水平翻转）和GAN生成合成图像解决数据不足和类别不平衡问题。

Result: 结合原始、增强和GAN生成数据，系统在准确率和F1分数上表现最佳，并通过Flask应用实现实时分类。

Conclusion: 研究表明深度学习和GAN在儿科肺炎分类中具有潜力，可显著提高诊断效率和准确性，适用于资源有限的临床环境。

Abstract: Pneumonia is a leading cause of mortality in children under five, requiring accurate chest X-ray diagnosis. This study presents a machine learning-based Pediatric Chest Pneumonia Classification System to assist healthcare professionals in diagnosing pneumonia from chest X-ray images. The CNN-based model was trained on 5,863 labeled chest X-ray images from children aged 0-5 years from the Guangzhou Women and Children's Medical Center. To address limited data, we applied augmentation techniques (rotation, zooming, shear, horizontal flipping) and employed GANs to generate synthetic images, addressing class imbalance. The system achieved optimal performance using combined original, augmented, and GAN-generated data, evaluated through accuracy and F1 score metrics. The final model was deployed via a Flask web application, enabling real-time classification with probability estimates. Results demonstrate the potential of deep learning and GANs in improving diagnostic accuracy and efficiency for pediatric pneumonia classification, particularly valuable in resource-limited clinical settings https://github.com/AbdulManaf12/Pediatric-Chest-Pneumonia-Classification

</details>


### [45] [IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution](https://arxiv.org/abs/2507.09923)
*Sejin Park,Sangmin Lee,Kyong Hwan Jin,Seung-Won Jung*

Main category: eess.IV

TL;DR: 提出了一种基于插值混合查找表（IM-LUT）的新框架，用于任意尺度超分辨率（ASISR），通过混合多个插值函数提升效率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于查找表（LUT）的方法仅适用于固定尺度，而现有ASISR方法计算成本高且内存需求大。

Method: 提出IM-LUT框架，通过IM-Net预测插值函数的混合权重，并利用LUT替换高计算量操作。

Result: 在多个基准数据集上，IM-LUT在图像质量和效率之间取得了最佳平衡。

Conclusion: IM-LUT是一种适用于资源受限应用的高效解决方案。

Abstract: Super-resolution (SR) has been a pivotal task in image processing, aimed at enhancing image resolution across various applications. Recently, look-up table (LUT)-based approaches have attracted interest due to their efficiency and performance. However, these methods are typically designed for fixed scale factors, making them unsuitable for arbitrary-scale image SR (ASISR). Existing ASISR techniques often employ implicit neural representations, which come with considerable computational cost and memory demands. To address these limitations, we propose Interpolation Mixing LUT (IM-LUT), a novel framework that operates ASISR by learning to blend multiple interpolation functions to maximize their representational capacity. Specifically, we introduce IM-Net, a network trained to predict mixing weights for interpolation functions based on local image patterns and the target scale factor. To enhance efficiency of interpolation-based methods, IM-Net is transformed into IM-LUT, where LUTs are employed to replace computationally expensive operations, enabling lightweight and fast inference on CPUs while preserving reconstruction quality. Experimental results on several benchmark datasets demonstrate that IM-LUT consistently achieves a superior balance between image quality and efficiency compared to existing methods, highlighting its potential as a promising solution for resource-constrained applications.

</details>


### [46] [Graph-based Multi-Modal Interaction Lightweight Network for Brain Tumor Segmentation (GMLN-BTS) in Edge Iterative MRI Lesion Localization System (EdgeIMLocSys)](https://arxiv.org/abs/2507.09995)
*Guohao Huo,Ruiting Dai,Hao Tang*

Main category: eess.IV

TL;DR: 提出EdgeIMLocSys系统，结合GMLN-BTS网络和VRUM模块，通过人类反馈持续学习，提升脑肿瘤分割的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决MRI扫描仪成像质量差异导致的模型泛化问题，提升脑肿瘤分割的临床适用性。

Method: 采用GMLN-BTS网络，包含M2AE编码器和G2MCIM模块，结合VRUM模块优化分割边界。

Result: 在BraTS2017数据集上Dice得分为85.1%，参数量仅4.58M，显著优于现有轻量方法。

Conclusion: 实现了高精度、低资源消耗的脑肿瘤分割，适合资源受限的临床环境。

Abstract: Brain tumor segmentation plays a critical role in clinical diagnosis and treatment planning, yet the variability in imaging quality across different MRI scanners presents significant challenges to model generalization. To address this, we propose the Edge Iterative MRI Lesion Localization System (EdgeIMLocSys), which integrates Continuous Learning from Human Feedback to adaptively fine-tune segmentation models based on clinician feedback, thereby enhancing robustness to scanner-specific imaging characteristics. Central to this system is the Graph-based Multi-Modal Interaction Lightweight Network for Brain Tumor Segmentation (GMLN-BTS), which employs a Modality-Aware Adaptive Encoder (M2AE) to extract multi-scale semantic features efficiently, and a Graph-based Multi-Modal Collaborative Interaction Module (G2MCIM) to model complementary cross-modal relationships via graph structures. Additionally, we introduce a novel Voxel Refinement UpSampling Module (VRUM) that synergistically combines linear interpolation and multi-scale transposed convolutions to suppress artifacts while preserving high-frequency details, improving segmentation boundary accuracy. Our proposed GMLN-BTS model achieves a Dice score of 85.1% on the BraTS2017 dataset with only 4.58 million parameters, representing a 98% reduction compared to mainstream 3D Transformer models, and significantly outperforms existing lightweight approaches. This work demonstrates a synergistic breakthrough in achieving high-accuracy, resource-efficient brain tumor segmentation suitable for deployment in resource-constrained clinical environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [47] [Learning Diffusion Models with Flexible Representation Guidance](https://arxiv.org/abs/2507.08980)
*Chenyu Wang,Cai Zhou,Sharut Gupta,Zongyu Lin,Stefanie Jegelka,Stephen Bates,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 本文提出了一种系统框架，通过表示引导改进扩散模型，引入两种新策略并展示了在多个任务中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型可以通过额外的表示引导提升生成质量，但现有方法缺乏系统性框架。本文旨在填补这一空白。

Method: 提出了两种策略：1）将目标表示与样本配对学习联合模型；2）设计平衡表示学习和数据生成的最优训练课程。

Result: 在图像、蛋白质序列和分子生成任务中表现优越，ImageNet 256×256任务上训练速度提升23.3倍。

Conclusion: 表示引导显著提升扩散模型的训练效率和生成质量，具有广泛应用潜力。

Abstract: Diffusion models can be improved with additional guidance towards more effective representations of input. Indeed, prior empirical work has already shown that aligning internal representations of the diffusion model with those of pre-trained models improves generation quality. In this paper, we present a systematic framework for incorporating representation guidance into diffusion models. We provide alternative decompositions of denoising models along with their associated training criteria, where the decompositions determine when and how the auxiliary representations are incorporated. Guided by our theoretical insights, we introduce two new strategies for enhancing representation alignment in diffusion models. First, we pair examples with target representations either derived from themselves or arisen from different synthetic modalities, and subsequently learn a joint model over the multimodal pairs. Second, we design an optimal training curriculum that balances representation learning and data generation. Our experiments across image, protein sequence, and molecule generation tasks demonstrate superior performance as well as accelerated training. In particular, on the class-conditional ImageNet $256\times 256$ benchmark, our guidance results in $23.3$ times faster training than the original SiT-XL as well as four times speedup over the state-of-the-art method REPA. The code is available at https://github.com/ChenyuWang-Monica/REED.

</details>


### [48] [Warm Starts Accelerate Generative Modelling](https://arxiv.org/abs/2507.09212)
*Jonas Scholz,Richard E. Turner*

Main category: cs.LG

TL;DR: 提出了一种名为“warm-start model”的确定性模型，通过提供更好的起始点加速条件生成，显著减少生成过程所需的步骤。


<details>
  <summary>Details</summary>
Motivation: 传统迭代生成模型（如扩散模型和流匹配）生成高保真样本需要数百次函数评估，速度慢。

Method: 引入warm-start模型，预测一个条件化的先验分布N(mu, sigma)，替代传统的N(0, I)先验，减少生成过程的距离。

Result: 在图像修复等任务中，仅需11次函数评估即可达到与1000步DDPM基线竞争的结果。

Conclusion: warm-start模型简单高效，可与任何标准生成模型结合，无需修改，进一步加速生成过程。

Abstract: Iterative generative models, like diffusion and flow-matching, create high-fidelity samples by progressively refining a noise vector into data. However, this process is notoriously slow, often requiring hundreds of function evaluations. We introduce the warm-start model, a simple, deterministic model that dramatically accelerates conditional generation by providing a better starting point. Instead of starting generation from an uninformed N(0, I) prior, our warm-start model predicts an informed prior N(mu, sigma), whose moments are conditioned on the input context. This "warm start" substantially reduces the distance the generative process must traverse, particularly when the conditioning information is strongly informative. On tasks like image inpainting, our method achieves results competitive with a 1000-step DDPM baseline using only 11 total function evaluations (1 for the warm start, 10 for generation). A simple conditional normalization trick makes our method compatible with any standard generative model and sampler without modification, allowing it to be combined with other efficient sampling techniques for further acceleration. Our implementation is available at https://github.com/jonas-scholz123/warm-start-model.

</details>
