<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 27]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [eess.IV](#eess.IV) [Total: 4]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Causal Reasoning Elicits Controllable 3D Scene Generation](https://arxiv.org/abs/2509.15249)
*Shen Chen,Ruiyu Zhao,Jiale Zhou,Zongkai Wu,Jenq-Neng Hwang,Lei Li*

Main category: cs.GR

TL;DR: CausalStruct是一个将因果推理嵌入3D场景生成的新框架，通过构建因果图来建模对象间的逻辑依赖和物理约束，提升场景的逻辑一致性和真实感。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景生成方法难以建模对象间复杂的逻辑依赖和物理约束，限制了在动态和真实环境中的适应能力。

Method: 利用大语言模型构建因果图，节点表示对象和属性，边编码因果依赖和物理约束；通过因果顺序确定对象放置顺序，因果干预调整空间配置；使用PID控制器优化对象尺度和位置，结合3D高斯泼溅和分数蒸馏采样提升形状精度和渲染稳定性。

Result: 实验表明CausalStruct生成的3D场景具有增强的逻辑一致性、真实的空间交互和鲁棒的适应性。

Conclusion: 该框架成功将因果推理融入3D场景生成，显著提升了生成场景的逻辑连贯性和物理合理性。

Abstract: Existing 3D scene generation methods often struggle to model the complex logical dependencies and physical constraints between objects, limiting their ability to adapt to dynamic and realistic environments. We propose CausalStruct, a novel framework that embeds causal reasoning into 3D scene generation. Utilizing large language models (LLMs), We construct causal graphs where nodes represent objects and attributes, while edges encode causal dependencies and physical constraints. CausalStruct iteratively refines the scene layout by enforcing causal order to determine the placement order of objects and applies causal intervention to adjust the spatial configuration according to physics-driven constraints, ensuring consistency with textual descriptions and real-world dynamics. The refined scene causal graph informs subsequent optimization steps, employing a Proportional-Integral-Derivative(PID) controller to iteratively tune object scales and positions. Our method uses text or images to guide object placement and layout in 3D scenes, with 3D Gaussian Splatting and Score Distillation Sampling improving shape accuracy and rendering stability. Extensive experiments show that CausalStruct generates 3D scenes with enhanced logical coherence, realistic spatial interactions, and robust adaptability.

</details>


### [2] [Generating Detailed Character Motion from Blocking Poses](https://arxiv.org/abs/2509.16064)
*Purvi Goel,Guy Tevet,C. K. Liu,Kayvon Fatahalian*

Main category: cs.GR

TL;DR: 本文提出了一种使用生成扩散模型进行运动细节化的方法，能够将粗略的动画姿势转换为详细自然的角色动画。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型可以解决不精确时间姿势的修正问题，但缺乏有效方法利用扩散先验来增强稀疏的阻塞姿势细节。

Method: 采用推理时技巧：在某些扩散步骤中，将无条件扩散模型输出与输入阻塞姿势约束按每个姿势的容忍权重混合，然后将结果作为输入条件传递给现有的运动重定时模型。

Result: 该方法显著优于现有通过混合模型输出或通过表达阻塞姿势约束作为指导的方法，能够稳健地将阻塞级姿势转换为合理的详细角色动画。

Conclusion: 这是第一个能够稳健地将阻塞级姿势转换为合理详细角色动画的扩散模型，为动画制作提供了有效的自动化工具。

Abstract: We focus on the problem of using generative diffusion models for the task of motion detailing: converting a rough version of a character animation, represented by a sparse set of coarsely posed, and imprecisely timed blocking poses, into a detailed, natural looking character animation. Current diffusion models can address the problem of correcting the timing of imprecisely timed poses, but we find that no good solution exists for leveraging the diffusion prior to enhance a sparse set of blocking poses with additional pose detail. We overcome this challenge using a simple inference-time trick. At certain diffusion steps, we blend the outputs of an unconditioned diffusion model with input blocking pose constraints using per-blocking-pose tolerance weights, and pass this result in as the input condition to an pre-existing motion retiming model. We find this approach works significantly better than existing approaches that attempt to add detail by blending model outputs or via expressing blocking pose constraints as guidance. The result is the first diffusion model that can robustly convert blocking-level poses into plausible detailed character animations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [3] [ProFusion: 3D Reconstruction of Protein Complex Structures from Multi-view AFM Images](https://arxiv.org/abs/2509.15242)
*Jaydeep Rade,Md Hasibul Hasan Hasib,Meric Ozturk,Baboucarr Faal,Sheng Yang,Dipali G. Sashital,Vincenzo Venditti,Baoyu Chen,Soumik Sarkar,Adarsh Krishnamurthy,Anwesha Sarkar*

Main category: cs.CV

TL;DR: ProFusion是一个结合深度学习和原子力显微镜的混合框架，用于预测大型蛋白质复合物的3D结构。它通过虚拟AFM生成合成数据集，训练条件扩散模型和NeRF模型进行3D重建。


<details>
  <summary>Details</summary>
Motivation: AI方法在预测大型蛋白质复合物结构时因缺乏3D空间线索而困难，实验方法如Cryo-EM准确但成本高、耗时。需要开发准确且成本效益高的蛋白质复合物结构预测方法。

Method: 开发虚拟AFM框架生成合成数据集，训练条件扩散模型合成新视角图像，使用实例特定NeRF模型进行3D重建。

Result: 重建的3D蛋白质结构在AFM成像分辨率内达到平均Chamfer距离，显示高结构保真度。在实验AFM图像上广泛验证，证明准确预测蛋白质复合物结构的潜力。

Conclusion: ProFusion展示了使用AFM实验进行准确、成本效益高的蛋白质复合物结构预测和快速迭代验证的强大潜力。

Abstract: AI-based in silico methods have improved protein structure prediction but often struggle with large protein complexes (PCs) involving multiple interacting proteins due to missing 3D spatial cues. Experimental techniques like Cryo-EM are accurate but costly and time-consuming. We present ProFusion, a hybrid framework that integrates a deep learning model with Atomic Force Microscopy (AFM), which provides high-resolution height maps from random orientations, naturally yielding multi-view data for 3D reconstruction. However, generating a large-scale AFM imaging data set sufficient to train deep learning models is impractical. Therefore, we developed a virtual AFM framework that simulates the imaging process and generated a dataset of ~542,000 proteins with multi-view synthetic AFM images. We train a conditional diffusion model to synthesize novel views from unposed inputs and an instance-specific Neural Radiance Field (NeRF) model to reconstruct 3D structures. Our reconstructed 3D protein structures achieve an average Chamfer Distance within the AFM imaging resolution, reflecting high structural fidelity. Our method is extensively validated on experimental AFM images of various PCs, demonstrating strong potential for accurate, cost-effective protein complex structure prediction and rapid iterative validation using AFM experiments.

</details>


### [4] [RespoDiff: Dual-Module Bottleneck Transformation for Responsible & Faithful T2I Generation](https://arxiv.org/abs/2509.15257)
*Silpa Vadakkeeveetil Sreelatha,Sauradip Nag,Muhammad Awais,Serge Belongie,Anjan Dutta*

Main category: cs.CV

TL;DR: RespoDiff是一个新颖的文本到图像生成框架，通过在扩散模型的中间瓶颈表示上引入双模块转换，同时确保公平性、安全性和语义保真度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在提高公平性和安全性时往往牺牲语义保真度和图像质量，需要一种能够同时优化这两个目标的方法。

Method: 提出双模块学习方法：一个模块专注于捕获和执行负责任概念（公平性和安全性），另一个模块致力于保持与中性提示的语义对齐。引入新的分数匹配目标来协调两个模块。

Result: 在负责任生成方面优于现有方法，确保语义对齐的同时不牺牲图像保真度。在多样化未见提示上，负责任和语义一致的生成提高了20%。可无缝集成到SDXL等大规模模型中。

Conclusion: RespoDiff框架有效解决了文本到图像生成中的公平性和安全性问题，同时保持了高质量的语义对齐和图像保真度。

Abstract: The rapid advancement of diffusion models has enabled high-fidelity and semantically rich text-to-image generation; however, ensuring fairness and safety remains an open challenge. Existing methods typically improve fairness and safety at the expense of semantic fidelity and image quality. In this work, we propose RespoDiff, a novel framework for responsible text-to-image generation that incorporates a dual-module transformation on the intermediate bottleneck representations of diffusion models. Our approach introduces two distinct learnable modules: one focused on capturing and enforcing responsible concepts, such as fairness and safety, and the other dedicated to maintaining semantic alignment with neutral prompts. To facilitate the dual learning process, we introduce a novel score-matching objective that enables effective coordination between the modules. Our method outperforms state-of-the-art methods in responsible generation by ensuring semantic alignment while optimizing both objectives without compromising image fidelity. Our approach improves responsible and semantically coherent generation by 20% across diverse, unseen prompts. Moreover, it integrates seamlessly into large-scale models like SDXL, enhancing fairness and safety. Code will be released upon acceptance.

</details>


### [5] [Autoguided Online Data Curation for Diffusion Model Training](https://arxiv.org/abs/2509.15267)
*Valeria Pais,Luis Oala,Daniele Faccio,Marco Aversa*

Main category: cs.CV

TL;DR: 本文研究了自动引导和在线数据选择方法是否能提高生成扩散模型的训练效率，通过整合JEST和自动引导方法进行对比实验，发现自动引导能持续提升样本质量和多样性，而早期数据选择仅在训练初期有优势。


<details>
  <summary>Details</summary>
Motivation: 生成模型的计算成本重新激发了高效数据筛选的潜力，本研究旨在探索最新的自动引导和在线数据选择方法是否能改善生成扩散模型的时间和样本效率。

Method: 将联合示例选择（JEST）和自动引导整合到统一代码库中，在2D合成数据生成和3x64x64图像生成任务上进行对比实验，考虑选择开销的影响。

Result: 自动引导在所有实验中都能持续提升样本质量和多样性。早期AJEST在数据效率上能匹配或略微超过单独使用自动引导，但其时间开销和复杂性使得自动引导或均匀随机数据选择在多数情况下更优。

Conclusion: 虽然有针对性的在线选择在早期训练中能带来效率提升，但稳健的样本质量改进主要由自动引导驱动。数据选择仅在特定情况下有益。

Abstract: The costs of generative model compute rekindled promises and hopes for efficient data curation. In this work, we investigate whether recently developed autoguidance and online data selection methods can improve the time and sample efficiency of training generative diffusion models. We integrate joint example selection (JEST) and autoguidance into a unified code base for fast ablation and benchmarking. We evaluate combinations of data curation on a controlled 2-D synthetic data generation task as well as (3x64x64)-D image generation. Our comparisons are made at equal wall-clock time and equal number of samples, explicitly accounting for the overhead of selection. Across experiments, autoguidance consistently improves sample quality and diversity. Early AJEST (applying selection only at the beginning of training) can match or modestly exceed autoguidance alone in data efficiency on both tasks. However, its time overhead and added complexity make autoguidance or uniform random data selection preferable in most situations. These findings suggest that while targeted online selection can yield efficiency gains in early training, robust sample quality improvements are primarily driven by autoguidance. We discuss limitations and scope, and outline when data selection may be beneficial.

</details>


### [6] [PRISM: Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images](https://arxiv.org/abs/2509.15270)
*Emanuele Ricco,Elia Onofri,Lorenzo Cima,Stefano Cresci,Roberto Di Pietro*

Main category: cs.CV

TL;DR: PRISM是一个基于傅里叶变换的图像指纹框架，用于识别AI生成图像的来源模型，在商业环境中确保内容来源的可追溯性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的普及，商业环境中需要能够识别AI生成内容来源的方法，特别是付费用户期望获得内容来源的保证。

Method: PRISM采用相位增强的径向图像签名映射框架，通过离散傅里叶变换的径向缩减，利用振幅和相位信息捕捉模型特定签名，然后通过线性判别分析进行聚类。

Result: 在自建数据集PRISM-36K上达到92.04%的归因准确率，在四个基准测试中平均准确率为81.60%，在真假图像检测任务中平均准确率为88.41%，在GenImage上达到95.06%。

Conclusion: 频域指纹识别在跨架构和跨数据集模型归因中表现出有效性，为生成式AI系统的问责和信任提供了可行解决方案。

Abstract: A critical need has emerged for generative AI: attribution methods. That is, solutions that can identify the model originating AI-generated content. This feature, generally relevant in multimodal applications, is especially sensitive in commercial settings where users subscribe to paid proprietary services and expect guarantees about the source of the content they receive. To address these issues, we introduce PRISM, a scalable Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images. PRISM is based on a radial reduction of the discrete Fourier transform that leverages amplitude and phase information to capture model-specific signatures. The output of the above process is subsequently clustered via linear discriminant analysis to achieve reliable model attribution in diverse settings, even if the model's internal details are inaccessible. To support our work, we construct PRISM-36K, a novel dataset of 36,000 images generated by six text-to-image GAN- and diffusion-based models. On this dataset, PRISM achieves an attribution accuracy of 92.04%. We additionally evaluate our method on four benchmarks from the literature, reaching an average accuracy of 81.60%. Finally, we evaluate our methodology also in the binary task of detecting real vs fake images, achieving an average accuracy of 88.41%. We obtain our best result on GenImage with an accuracy of 95.06%, whereas the original benchmark achieved 82.20%. Our results demonstrate the effectiveness of frequency-domain fingerprinting for cross-architecture and cross-dataset model attribution, offering a viable solution for enforcing accountability and trust in generative AI systems.

</details>


### [7] [LowDiff: Efficient Diffusion Sampling with Low-Resolution Condition](https://arxiv.org/abs/2509.15342)
*Jiuyi Xu,Qing Jin,Meida Chen,Andrew Feng,Yang Sui,Yangming Shi*

Main category: cs.CV

TL;DR: LowDiff是一种基于级联方法的扩散模型框架，通过从低分辨率到高分辨率的渐进式生成来提高采样效率，在保持图像质量的同时显著减少高分辨率采样步骤。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成方面取得了显著成功，但实际应用受到采样速度慢的限制。现有方法主要关注模型压缩或减少去噪步骤，而忽略了在生成过程中利用多分辨率输入的可能性。

Method: 提出LowDiff框架，采用级联方法逐步生成更高分辨率的输出，使用统一模型从低分辨率逐步细化到目标分辨率，适用于像素空间和潜在空间的扩散模型。

Result: 在CIFAR-10、FFHQ和ImageNet上的实验表明，该方法在所有数据集和设置上实现了超过50%的吞吐量提升，同时保持相当或更好的质量。具体指标包括：无条件CIFAR-10 FID 2.11，IS 9.87；条件CIFAR-10 FID 1.94，IS 10.03；FFHQ 64x64 FID 2.43；ImageNet 256x256 FID 4.00，IS 195.06。

Conclusion: LowDiff通过创新的架构设计和生成技术，在保持高质量的同时显著提高了扩散模型的采样效率，证明了利用多分辨率生成策略的有效性和通用性。

Abstract: Diffusion models have achieved remarkable success in image generation but their practical application is often hindered by the slow sampling speed. Prior efforts of improving efficiency primarily focus on compressing models or reducing the total number of denoising steps, largely neglecting the possibility to leverage multiple input resolutions in the generation process. In this work, we propose LowDiff, a novel and efficient diffusion framework based on a cascaded approach by generating increasingly higher resolution outputs. Besides, LowDiff employs a unified model to progressively refine images from low resolution to the desired resolution. With the proposed architecture design and generation techniques, we achieve comparable or even superior performance with much fewer high-resolution sampling steps. LowDiff is applicable to diffusion models in both pixel space and latent space. Extensive experiments on both conditional and unconditional generation tasks across CIFAR-10, FFHQ and ImageNet demonstrate the effectiveness and generality of our method. Results show over 50% throughput improvement across all datasets and settings while maintaining comparable or better quality. On unconditional CIFAR-10, LowDiff achieves an FID of 2.11 and IS of 9.87, while on conditional CIFAR-10, an FID of 1.94 and IS of 10.03. On FFHQ 64x64, LowDiff achieves an FID of 2.43, and on ImageNet 256x256, LowDiff built on LightningDiT-B/1 produces high-quality samples with a FID of 4.00 and an IS of 195.06, together with substantial efficiency gains.

</details>


### [8] [MaskAttn-SDXL: Controllable Region-Level Text-To-Image Generation](https://arxiv.org/abs/2509.15357)
*Yu Chang,Jiahao Chen,Anzhe Cheng,Paul Bogdan*

Main category: cs.CV

TL;DR: 提出MaskAttn-SDXL方法，通过在SDXL的交叉注意力logits中注入二进制掩码来稀疏化token与潜在空间交互，解决多对象提示中的组合失败问题


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型在多对象、属性和空间关系的提示上经常出现组合失败，导致实体纠缠、属性混淆和空间线索违反

Method: 在SDXL UNet的交叉注意力logits上应用区域级门控机制，为每层学习二进制掩码，在softmax前注入到交叉注意力logit图中，仅保留语义相关的连接

Result: 模型提高了多对象提示中的空间合规性和属性绑定，同时保持整体图像质量和多样性

Conclusion: logit级别的掩码交叉注意力是实施组合控制的数据高效原语，该方法可作为文本到图像生成中空间控制的实用扩展

Abstract: Text-to-image diffusion models achieve impressive realism but often suffer from compositional failures on prompts with multiple objects, attributes, and spatial relations, resulting in cross-token interference where entities entangle, attributes mix across objects, and spatial cues are violated. To address these failures, we propose MaskAttn-SDXL,a region-level gating mechanism applied to the cross-attention logits of Stable Diffusion XL(SDXL)'s UNet. MaskAttn-SDXL learns a binary mask per layer, injecting it into each cross-attention logit map before softmax to sparsify token-to-latent interactions so that only semantically relevant connections remain active. The method requires no positional encodings, auxiliary tokens, or external region masks, and preserves the original inference path with negligible overhead. In practice, our model improves spatial compliance and attribute binding in multi-object prompts while preserving overall image quality and diversity. These findings demonstrate that logit-level maksed cross-attention is an data-efficient primitve for enforcing compositional control, and our method thus serves as a practical extension for spatial control in text-to-image generation.

</details>


### [9] [RaceGAN: A Framework for Preserving Individuality while Converting Racial Information for Image-to-Image Translation](https://arxiv.org/abs/2509.15391)
*Mst Tasnim Pervin,George Bebis,Fang Jiang,Alireza Tavakkoli*

Main category: cs.CV

TL;DR: RaceGAN是一个新颖的多域图像到图像转换框架，专门用于种族特征转换，能够在保持个体特征和高层语义的同时映射多个域的样式代码，无需参考图像。


<details>
  <summary>Details</summary>
Motivation: 现有的GAN模型如CycleGAN、StarGAN等在种族特征转换方面存在局限性，要么无法处理多域转换，要么无法保持个体特征，或者需要额外的参考图像。

Method: RaceGAN通过多域图像到图像转换技术，在种族属性转换过程中映射样式代码，同时保持个体特征和高层语义，无需依赖参考图像。

Result: 在芝加哥人脸数据集上的测试表明，RaceGAN在种族特征转换（亚洲人、白人和黑人）方面优于其他模型，并通过InceptionReNetv2分类验证了其有效性。

Conclusion: RaceGAN成功实现了种族特征的准确转换，同时保持了个体特征，并且能够将潜在空间划分为不同种族群体的清晰聚类。

Abstract: Generative adversarial networks (GANs) have demonstrated significant progress in unpaired image-to-image translation in recent years for several applications. CycleGAN was the first to lead the way, although it was restricted to a pair of domains. StarGAN overcame this constraint by tackling image-to-image translation across various domains, although it was not able to map in-depth low-level style changes for these domains. Style mapping via reference-guided image synthesis has been made possible by the innovations of StarGANv2 and StyleGAN. However, these models do not maintain individuality and need an extra reference image in addition to the input. Our study aims to translate racial traits by means of multi-domain image-to-image translation. We present RaceGAN, a novel framework capable of mapping style codes over several domains during racial attribute translation while maintaining individuality and high level semantics without relying on a reference image. RaceGAN outperforms other models in translating racial features (i.e., Asian, White, and Black) when tested on Chicago Face Dataset. We also give quantitative findings utilizing InceptionReNetv2-based classification to demonstrate the effectiveness of our racial translation. Moreover, we investigate how well the model partitions the latent space into distinct clusters of faces for each ethnic group.

</details>


### [10] [CAGE: Continuity-Aware edGE Network Unlocks Robust Floorplan Reconstruction](https://arxiv.org/abs/2509.15459)
*Yiyi Liu,Chunyang Liu,Weiqin Jiao,Bojian Wu,Fashuai Li,Biao Xiong*

Main category: cs.CV

TL;DR: CAGE是一种从点云密度图直接重建矢量平面图的鲁棒框架，通过边缘中心表示和双查询变换器解码器实现连续性和拓扑有效性


<details>
  <summary>Details</summary>
Motivation: 传统基于角点的多边形表示对噪声和观测不完整高度敏感，导致布局碎片化；现有线分组方法难以恢复精细几何细节

Method: 提出原生边缘中心表示法，将墙段建模为定向几何连续边；开发双查询变换器解码器，在去噪框架中集成扰动和潜在查询

Result: 在Structured3D和SceneCAD数据集上达到SOTA性能：房间F1 99.1%，角点F1 91.7%，角度F1 89.3%；展示强跨数据集泛化能力

Conclusion: CAGE框架通过边缘中心表示和双查询解码器设计，显著提高了平面图重建的鲁棒性和几何连续性

Abstract: We present \textbf{CAGE} (\textit{Continuity-Aware edGE}) network, a \textcolor{red}{robust} framework for reconstructing vector floorplans directly from point-cloud density maps. Traditional corner-based polygon representations are highly sensitive to noise and incomplete observations, often resulting in fragmented or implausible layouts. Recent line grouping methods leverage structural cues to improve robustness but still struggle to recover fine geometric details. To address these limitations, we propose a \textit{native} edge-centric formulation, modeling each wall segment as a directed, geometrically continuous edge. This representation enables inference of coherent floorplan structures, ensuring watertight, topologically valid room boundaries while improving robustness and reducing artifacts. Towards this design, we develop a dual-query transformer decoder that integrates perturbed and latent queries within a denoising framework, which not only stabilizes optimization but also accelerates convergence. Extensive experiments on Structured3D and SceneCAD show that \textbf{CAGE} achieves state-of-the-art performance, with F1 scores of 99.1\% (rooms), 91.7\% (corners), and 89.3\% (angles). The method also demonstrates strong cross-dataset generalization, underscoring the efficacy of our architectural innovations. Code and pretrained models will be released upon acceptance.

</details>


### [11] [OpenViGA: Video Generation for Automotive Driving Scenes by Streamlining and Fine-Tuning Open Source Models with Public Data](https://arxiv.org/abs/2509.15479)
*Björn Möller,Zhengyang Li,Malte Stelzer,Thomas Graave,Fabian Bettels,Muaaz Ataya,Tim Fingscheidt*

Main category: cs.CV

TL;DR: OpenViGA是一个开源的自动驾驶场景视频生成系统，通过分析图像分词器、世界模型和视频解码器三个组件，使用预训练开源模型和公开数据集构建，实现了可复现的高质量视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成系统通常使用大型模型，需要大量训练资源，设计选择缺乏透明度，且缺乏公开的代码和数据集。OpenViGA旨在解决这些问题。

Method: 基于预训练开源模型，使用BDD100K数据集在学术规模GPU硬件上进行微调，通过图像分词器、世界模型和视频解码器三个组件构建连贯的视频生成系统。

Result: 在256x256分辨率、4fps条件下，仅使用一帧算法延迟就能逐帧预测真实的驾驶场景视频。

Conclusion: OpenViGA提供了一个完全可复现的开源视频生成系统，解决了现有系统在资源需求、透明度和可用性方面的不足。

Abstract: Recent successful video generation systems that predict and create realistic automotive driving scenes from short video inputs assign tokenization, future state prediction (world model), and video decoding to dedicated models. These approaches often utilize large models that require significant training resources, offer limited insight into design choices, and lack publicly available code and datasets. In this work, we address these deficiencies and present OpenViGA, an open video generation system for automotive driving scenes. Our contributions are: Unlike several earlier works for video generation, such as GAIA-1, we provide a deep analysis of the three components of our system by separate quantitative and qualitative evaluation: Image tokenizer, world model, video decoder. Second, we purely build upon powerful pre-trained open source models from various domains, which we fine-tune by publicly available automotive data (BDD100K) on GPU hardware at academic scale. Third, we build a coherent video generation system by streamlining interfaces of our components. Fourth, due to public availability of the underlying models and data, we allow full reproducibility. Finally, we also publish our code and models on Github. For an image size of 256x256 at 4 fps we are able to predict realistic driving scene videos frame-by-frame with only one frame of algorithmic latency.

</details>


### [12] [Lynx: Towards High-Fidelity Personalized Video Generation](https://arxiv.org/abs/2509.15496)
*Shen Sang,Tiancheng Zhi,Tianpei Gu,Jing Liu,Linjie Luo*

Main category: cs.CV

TL;DR: Lynx是一个基于DiT的高保真个性化视频合成模型，通过两个轻量级适配器确保身份保真度，在身份保持和视频质量方面表现优异


<details>
  <summary>Details</summary>
Motivation: 解决个性化视频生成中的身份保真度问题，确保从单张输入图像生成的视频能够准确保持人物身份特征

Method: 基于开源DiT基础模型，引入ID-adapter使用Perceiver Resampler将ArcFace面部嵌入转换为紧凑身份令牌，Ref-adapter通过交叉注意力在所有transformer层注入冻结参考路径的密集VAE特征

Result: 在包含40个主体和20个无偏提示的800个测试案例评估中，Lynx表现出卓越的面部相似度、竞争力的提示跟随能力和强大的视频质量

Conclusion: Lynx通过创新的适配器设计在个性化视频生成领域取得了显著进展，实现了身份保真度和视觉真实性的平衡

Abstract: We present Lynx, a high-fidelity model for personalized video synthesis from a single input image. Built on an open-source Diffusion Transformer (DiT) foundation model, Lynx introduces two lightweight adapters to ensure identity fidelity. The ID-adapter employs a Perceiver Resampler to convert ArcFace-derived facial embeddings into compact identity tokens for conditioning, while the Ref-adapter integrates dense VAE features from a frozen reference pathway, injecting fine-grained details across all transformer layers through cross-attention. These modules collectively enable robust identity preservation while maintaining temporal coherence and visual realism. Through evaluation on a curated benchmark of 40 subjects and 20 unbiased prompts, which yielded 800 test cases, Lynx has demonstrated superior face resemblance, competitive prompt following, and strong video quality, thereby advancing the state of personalized video generation.

</details>


### [13] [SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models](https://arxiv.org/abs/2509.15536)
*Sen Wang,Jingyi Tian,Le Wang,Zhimin Liao,Jiayi Li,Huaiyi Dong,Kun Xia,Sanping Zhou,Wei Tang,Hua Gang*

Main category: cs.CV

TL;DR: SAMPO是一个混合世界模型框架，结合了视觉自回归建模和因果建模，通过尺度级自回归和运动提示来解决现有模型在空间结构、解码效率和运动建模方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归世界模型在视觉一致性预测方面存在困难，主要问题包括空间结构破坏、解码效率低下和运动建模不足。

Method: SAMPO整合了时间因果解码和双向空间注意力，采用非对称多尺度分词器保留空间细节，并引入轨迹感知运动提示模块注入时空线索。

Result: 实验表明SAMPO在动作条件视频预测和基于模型的控制中表现优异，生成质量提升且推理速度快4.4倍，同时展现出零样本泛化能力和良好的扩展性。

Conclusion: SAMPO通过创新的混合框架设计，有效解决了世界模型中的视觉一致性和效率问题，为长期决策规划提供了更可靠的模拟环境。

Abstract: World models allow agents to simulate the consequences of actions in imagined environments for planning, control, and long-horizon decision-making. However, existing autoregressive world models struggle with visually coherent predictions due to disrupted spatial structure, inefficient decoding, and inadequate motion modeling. In response, we propose \textbf{S}cale-wise \textbf{A}utoregression with \textbf{M}otion \textbf{P}r\textbf{O}mpt (\textbf{SAMPO}), a hybrid framework that combines visual autoregressive modeling for intra-frame generation with causal modeling for next-frame generation. Specifically, SAMPO integrates temporal causal decoding with bidirectional spatial attention, which preserves spatial locality and supports parallel decoding within each scale. This design significantly enhances both temporal consistency and rollout efficiency. To further improve dynamic scene understanding, we devise an asymmetric multi-scale tokenizer that preserves spatial details in observed frames and extracts compact dynamic representations for future frames, optimizing both memory usage and model performance. Additionally, we introduce a trajectory-aware motion prompt module that injects spatiotemporal cues about object and robot trajectories, focusing attention on dynamic regions and improving temporal consistency and physical realism. Extensive experiments show that SAMPO achieves competitive performance in action-conditioned video prediction and model-based control, improving generation quality with 4.4$\times$ faster inference. We also evaluate SAMPO's zero-shot generalization and scaling behavior, demonstrating its ability to generalize to unseen tasks and benefit from larger model sizes.

</details>


### [14] [Beyond Words: Enhancing Desire, Emotion, and Sentiment Recognition with Non-Verbal Cues](https://arxiv.org/abs/2509.15540)
*Wei Chen,Tongguan Wang,Feiyue Xue,Junkai Li,Hui Liu,Ying Sha*

Main category: cs.CV

TL;DR: 本文提出了一种对称双向多模态学习框架，用于欲望、情感和情绪识别，通过文本和图像模态的相互引导来捕捉意图相关表示，在MSED数据集上取得了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注言语线索而忽视图像作为补充的非言语线索，多模态方法在人类欲望理解方面仍未被充分探索。

Method: 采用对称双向多模态学习框架，使用低分辨率图像获取全局视觉表示进行跨模态对齐，高分辨率图像分割为子图像并通过掩码图像建模来增强细粒度局部特征捕捉能力。引入文本引导的图像解码器和图像引导的文本解码器促进深度跨模态交互。

Result: 在MSED数据集上，该方法在欲望理解方面F1分数提升1.1%，情感识别提升0.6%，情绪分析提升0.9%，均优于现有最先进方法。

Conclusion: 提出的对称双向多模态学习框架有效提升了欲望、情感和情绪识别的性能，验证了方法的有效性。

Abstract: Desire, as an intention that drives human behavior, is closely related to both emotion and sentiment. Multimodal learning has advanced sentiment and emotion recognition, but multimodal approaches specially targeting human desire understanding remain underexplored. And existing methods in sentiment analysis predominantly emphasize verbal cues and overlook images as complementary non-verbal cues. To address these gaps, we propose a Symmetrical Bidirectional Multimodal Learning Framework for Desire, Emotion, and Sentiment Recognition, which enforces mutual guidance between text and image modalities to effectively capture intention-related representations in the image. Specifically, low-resolution images are used to obtain global visual representations for cross-modal alignment, while high resolution images are partitioned into sub-images and modeled with masked image modeling to enhance the ability to capture fine-grained local features. A text-guided image decoder and an image-guided text decoder are introduced to facilitate deep cross-modal interaction at both local and global representations of image information. Additionally, to balance perceptual gains with computation cost, a mixed-scale image strategy is adopted, where high-resolution images are cropped into sub-images for masked modeling. The proposed approach is evaluated on MSED, a multimodal dataset that includes a desire understanding benchmark, as well as emotion and sentiment recognition. Experimental results indicate consistent improvements over other state-of-the-art methods, validating the effectiveness of our proposed method. Specifically, our method outperforms existing approaches, achieving F1-score improvements of 1.1% in desire understanding, 0.6% in emotion recognition, and 0.9% in sentiment analysis. Our code is available at: https://github.com/especiallyW/SyDES.

</details>


### [15] [MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild](https://arxiv.org/abs/2509.15548)
*Deming Li,Kaiwen Jiang,Yutao Tang,Ravi Ramamoorthi,Rama Chellappa,Cheng Peng*

Main category: cs.CV

TL;DR: MS-GS是一个针对稀疏视图和多外观场景的3D高斯泼溅框架，利用单目深度估计的几何先验和SfM点锚定算法，通过几何引导的虚拟视图监督实现多视图约束，在稀疏视图和多外观条件下实现逼真渲染。


<details>
  <summary>Details</summary>
Motivation: 解决野外照片集合中图像数量有限且存在多种外观（如不同时间、季节）时，场景重建和新视图合成的挑战。现有NeRF和3DGS方法容易过度平滑和过拟合。

Method: 基于单目深度估计提取几何先验，使用SfM点锚定算法提取局部语义区域进行可靠对齐和几何提示，提出细粒度和粗粒度方案的几何引导虚拟视图监督来鼓励3D一致性和减少过拟合。

Result: MS-GS在各种挑战性的稀疏视图和多外观条件下实现了逼真渲染，在不同数据集上显著优于现有方法。

Conclusion: MS-GS框架通过几何先验和虚拟视图监督有效解决了稀疏视图多外观场景的重建问题，为更现实的基准测试提供了数据集和实验设置。

Abstract: In-the-wild photo collections often contain limited volumes of imagery and exhibit multiple appearances, e.g., taken at different times of day or seasons, posing significant challenges to scene reconstruction and novel view synthesis. Although recent adaptations of Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) have improved in these areas, they tend to oversmooth and are prone to overfitting. In this paper, we present MS-GS, a novel framework designed with Multi-appearance capabilities in Sparse-view scenarios using 3DGS. To address the lack of support due to sparse initializations, our approach is built on the geometric priors elicited from monocular depth estimations. The key lies in extracting and utilizing local semantic regions with a Structure-from-Motion (SfM) points anchored algorithm for reliable alignment and geometry cues. Then, to introduce multi-view constraints, we propose a series of geometry-guided supervision at virtual views in a fine-grained and coarse scheme to encourage 3D consistency and reduce overfitting. We also introduce a dataset and an in-the-wild experiment setting to set up more realistic benchmarks. We demonstrate that MS-GS achieves photorealistic renderings under various challenging sparse-view and multi-appearance conditions and outperforms existing approaches significantly across different datasets.

</details>


### [16] [UNIV: Unified Foundation Model for Infrared and Visible Modalities](https://arxiv.org/abs/2509.15642)
*Fangyuan Mao,Shuo Wang,Jilin Mei,Chen Min,Shun Lu,Fuyang Liu,Yu Hu*

Main category: cs.CV

TL;DR: UNIV是一个受生物学启发的统一基础模型，用于红外和可见光模态，通过注意力引导的对比学习和双知识保留机制，在跨模态任务中实现卓越性能。


<details>
  <summary>Details</summary>
Motivation: 解决RGB-可见光和红外数据预训练模型在自动驾驶等多模态场景中表现不佳的问题，实现全天候鲁棒感知。

Method: 提出Patch-wise Cross-modality Contrastive Learning（PCCL）模拟视网膜水平细胞的侧向抑制，以及双知识保留机制结合LoRA适配器和同步蒸馏来防止灾难性遗忘。

Result: 在红外任务上显著提升性能（语义分割+1.7 mIoU，目标检测+0.7 mAP），同时在可见光RGB任务上保持99%以上的基线性能。

Conclusion: UNIV模型成功实现了跨模态特征对齐和知识保留，为全天候感知系统提供了有效的解决方案。

Abstract: The demand for joint RGB-visible and infrared perception is growing rapidly, particularly to achieve robust performance under diverse weather conditions. Although pre-trained models for RGB-visible and infrared data excel in their respective domains, they often underperform in multimodal scenarios, such as autonomous vehicles equipped with both sensors. To address this challenge, we propose a biologically inspired UNified foundation model for Infrared and Visible modalities (UNIV), featuring two key innovations. First, we introduce Patch-wise Cross-modality Contrastive Learning (PCCL), an attention-guided distillation framework that mimics retinal horizontal cells' lateral inhibition, which enables effective cross-modal feature alignment while remaining compatible with any transformer-based architecture. Second, our dual-knowledge preservation mechanism emulates the retina's bipolar cell signal routing - combining LoRA adapters (2% added parameters) with synchronous distillation to prevent catastrophic forgetting, thereby replicating the retina's photopic (cone-driven) and scotopic (rod-driven) functionality. To support cross-modal learning, we introduce the MVIP dataset, the most comprehensive visible-infrared benchmark to date. It contains 98,992 precisely aligned image pairs spanning diverse scenarios. Extensive experiments demonstrate UNIV's superior performance on infrared tasks (+1.7 mIoU in semantic segmentation and +0.7 mAP in object detection) while maintaining 99%+ of the baseline performance on visible RGB tasks. Our code is available at https://github.com/fangyuanmao/UNIV.

</details>


### [17] [GS-Scale: Unlocking Large-Scale 3D Gaussian Splatting Training via Host Offloading](https://arxiv.org/abs/2509.15645)
*Donghyun Lee,Dawoon Jeong,Jae W. Lee,Hongil Yoon*

Main category: cs.CV

TL;DR: GS-Scale是一个针对3D高斯泼溅技术的内存高效训练系统，通过将高斯数据存储在主机内存中，仅按需传输子集到GPU，显著降低GPU内存需求3.3-5.6倍，同时保持训练速度。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅技术虽然提供高质量渲染和快速渲染速度，但在大规模场景训练时面临GPU内存不足的挑战，因为需要存储参数、梯度和优化器状态。

Method: GS-Scale采用三种系统级优化：(1)选择性卸载几何参数以加速视锥体剔除；(2)参数转发将CPU优化器更新与GPU计算流水线化；(3)延迟优化器更新以减少对零梯度高斯的不必要内存访问。

Result: 在大型数据集上的评估显示，GS-Scale将GPU内存需求降低3.3-5.6倍，在RTX 4070移动GPU上可将高斯数量从400万扩展到1800万，LPIPS指标提升23-35%。

Conclusion: GS-Scale使大规模3D高斯泼溅训练能够在消费级GPU上实现，解决了内存瓶颈问题，同时保持训练效率。

Abstract: The advent of 3D Gaussian Splatting has revolutionized graphics rendering by delivering high visual quality and fast rendering speeds. However, training large-scale scenes at high quality remains challenging due to the substantial memory demands required to store parameters, gradients, and optimizer states, which can quickly overwhelm GPU memory. To address these limitations, we propose GS-Scale, a fast and memory-efficient training system for 3D Gaussian Splatting. GS-Scale stores all Gaussians in host memory, transferring only a subset to the GPU on demand for each forward and backward pass. While this dramatically reduces GPU memory usage, it requires frustum culling and optimizer updates to be executed on the CPU, introducing slowdowns due to CPU's limited compute and memory bandwidth. To mitigate this, GS-Scale employs three system-level optimizations: (1) selective offloading of geometric parameters for fast frustum culling, (2) parameter forwarding to pipeline CPU optimizer updates with GPU computation, and (3) deferred optimizer update to minimize unnecessary memory accesses for Gaussians with zero gradients. Our extensive evaluations on large-scale datasets demonstrate that GS-Scale significantly lowers GPU memory demands by 3.3-5.6x, while achieving training speeds comparable to GPU without host offloading. This enables large-scale 3D Gaussian Splatting training on consumer-grade GPUs; for instance, GS-Scale can scale the number of Gaussians from 4 million to 18 million on an RTX 4070 Mobile GPU, leading to 23-35% LPIPS (learned perceptual image patch similarity) improvement.

</details>


### [18] [FingerSplat: Contactless Fingerprint 3D Reconstruction and Generation based on 3D Gaussian Splatting](https://arxiv.org/abs/2509.15648)
*Yuwei Jia,Yutang Lu,Zhe Cui,Fei Su*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D高斯泼溅的无接触指纹3D配准、重建和生成框架，首次将该技术应用于指纹识别领域，能够在无需相机参数的情况下从稀疏的2D图像中实现有效的3D配准和完整重建。


<details>
  <summary>Details</summary>
Motivation: 当前无接触指纹识别性能落后于接触式方法，主要原因是缺乏姿态变化的无接触指纹数据以及未充分利用隐式3D指纹表示。

Method: 通过集成3D高斯泼溅技术，构建了一个新的无接触指纹识别范式，实现了3D指纹的重建和生成，无需相机参数信息即可从稀疏输入图像完成3D配准和重建。

Result: 实验证明该方法能够从2D图像准确对齐和重建3D指纹，并从3D模型生成高质量的无接触指纹，从而提升了无接触指纹识别的性能。

Conclusion: 这是首个将3D高斯泼溅应用于指纹识别的工作，为无接触指纹识别提供了新的技术路径，显著提高了识别性能。

Abstract: Researchers have conducted many pioneer researches on contactless fingerprints, yet the performance of contactless fingerprint recognition still lags behind contact-based methods primary due to the insufficient contactless fingerprint data with pose variations and lack of the usage of implicit 3D fingerprint representations. In this paper, we introduce a novel contactless fingerprint 3D registration, reconstruction and generation framework by integrating 3D Gaussian Splatting, with the goal of offering a new paradigm for contactless fingerprint recognition that integrates 3D fingerprint reconstruction and generation. To our knowledge, this is the first work to apply 3D Gaussian Splatting to the field of fingerprint recognition, and the first to achieve effective 3D registration and complete reconstruction of contactless fingerprints with sparse input images and without requiring camera parameters information. Experiments on 3D fingerprint registration, reconstruction, and generation prove that our method can accurately align and reconstruct 3D fingerprints from 2D images, and sequentially generates high-quality contactless fingerprints from 3D model, thus increasing the performances for contactless fingerprint recognition.

</details>


### [19] [Camera Splatting for Continuous View Optimization](https://arxiv.org/abs/2509.15677)
*Gahye Lee,Hyomin Kim,Gwangjin Ju,Jooeun Son,Hyejeong Yoon,Seungyong Lee*

Main category: cs.CV

TL;DR: Camera Splatting是一种新颖的视图优化框架，通过将相机建模为3D高斯分布（相机splat），在表面附近放置虚拟点相机来优化视图合成。


<details>
  <summary>Details</summary>
Motivation: 现有的Farthest View Sampling (FVS)方法在捕捉复杂视角相关现象（如强烈金属反射和复杂纹理）方面存在不足，需要更有效的视图优化方法。

Method: 将每个相机建模为3D高斯分布（相机splat），在表面附近采样3D点放置虚拟点相机，通过可微分连续优化相机splat来实现视图优化，类似于原始3D高斯splatting的方法。

Result: 与FVS方法相比，Camera Splatting在捕捉复杂视角相关现象方面表现出更优越的性能，能够更好地处理强烈金属反射和复杂纹理（如文字）。

Conclusion: Camera Splatting提供了一种有效的视图优化框架，在复杂场景的视图合成任务中优于传统的FVS方法。

Abstract: We propose Camera Splatting, a novel view optimization framework for novel view synthesis. Each camera is modeled as a 3D Gaussian, referred to as a camera splat, and virtual cameras, termed point cameras, are placed at 3D points sampled near the surface to observe the distribution of camera splats. View optimization is achieved by continuously and differentiably refining the camera splats so that desirable target distributions are observed from the point cameras, in a manner similar to the original 3D Gaussian splatting. Compared to the Farthest View Sampling (FVS) approach, our optimized views demonstrate superior performance in capturing complex view-dependent phenomena, including intense metallic reflections and intricate textures such as text.

</details>


### [20] [Layout Stroke Imitation: A Layout Guided Handwriting Stroke Generation for Style Imitation with Diffusion Model](https://arxiv.org/abs/2509.15678)
*Sidra Hanif,Longin Jan Latecki*

Main category: cs.CV

TL;DR: 本文提出了一种用于手写笔画生成的条件扩散模型，通过多尺度注意力特征和词间距布局来模仿书法风格，在笔画生成任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 手写笔画生成对于提升手写识别和书写顺序恢复等任务性能至关重要。现有研究未能将词间距作为显式的手写特征考虑，导致风格模仿时词间距不一致。

Method: 提出多尺度注意力特征用于书法风格模仿，引入词布局信息以促进词间距控制，并设计条件扩散模型进行笔画生成而非直接生成风格图像。

Result: 实验表明，提出的扩散模型在笔画生成任务上优于当前最先进方法，并与最近的图像生成网络具有竞争力。

Conclusion: 通过结合书法风格和词布局指导的条件扩散模型，能够实现更好的手写模仿和书法风格的笔画生成。

Abstract: Handwriting stroke generation is crucial for improving the performance of tasks such as handwriting recognition and writers order recovery. In handwriting stroke generation, it is significantly important to imitate the sample calligraphic style. The previous studies have suggested utilizing the calligraphic features of the handwriting. However, they had not considered word spacing (word layout) as an explicit handwriting feature, which results in inconsistent word spacing for style imitation. Firstly, this work proposes multi-scale attention features for calligraphic style imitation. These multi-scale feature embeddings highlight the local and global style features. Secondly, we propose to include the words layout, which facilitates word spacing for handwriting stroke generation. Moreover, we propose a conditional diffusion model to predict strokes in contrast to previous work, which directly generated style images. Stroke generation provides additional temporal coordinate information, which is lacking in image generation. Hence, our proposed conditional diffusion model for stroke generation is guided by calligraphic style and word layout for better handwriting imitation and stroke generation in a calligraphic style. Our experimentation shows that the proposed diffusion model outperforms the current state-of-the-art stroke generation and is competitive with recent image generation networks.

</details>


### [21] [FloorSAM: SAM-Guided Floorplan Reconstruction with Semantic-Geometric Fusion](https://arxiv.org/abs/2509.15750)
*Han Ye,Haofu Wang,Yunchi Zhang,Jiangjian Xiao,Yuqiang Jin,Jinyuan Liu,Wen-An Zhang,Uladzislau Sychou,Alexander Tuzikov,Vladislav Sobolevskii,Valerii Zakharov,Boris Sokolov,Minglei Fu*

Main category: cs.CV

TL;DR: FloorSAM是一个集成点云密度图和Segment Anything Model（SAM）的框架，用于从LiDAR数据中准确重建建筑平面图，解决了传统方法在噪声、泛化能力和几何细节损失方面的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法如几何算法和基于Mask R-CNN的深度学习在点云数据重建建筑平面图时面临噪声干扰、泛化能力有限和几何细节丢失的问题，需要更鲁棒的解决方案。

Method: 使用网格过滤、自适应分辨率投影和图像增强创建鲁棒的俯视密度图，结合SAM的零样本学习进行精确房间分割，通过自适应提示点和多阶段过滤生成房间掩码，最后进行掩码和点云联合分析以提取和正则化轮廓。

Result: 在Giblayout和ISPRS数据集上的测试显示，FloorSAM在准确性、召回率和鲁棒性方面优于传统方法，特别是在噪声和复杂环境中表现更佳。

Conclusion: FloorSAM通过结合点云密度图和SAM的零样本学习能力，能够有效重建准确的建筑平面图并恢复房间拓扑关系，为室内导航、BIM和精确测量提供了可靠工具。

Abstract: Reconstructing building floor plans from point cloud data is key for indoor navigation, BIM, and precise measurements. Traditional methods like geometric algorithms and Mask R-CNN-based deep learning often face issues with noise, limited generalization, and loss of geometric details. We propose FloorSAM, a framework that integrates point cloud density maps with the Segment Anything Model (SAM) for accurate floor plan reconstruction from LiDAR data. Using grid-based filtering, adaptive resolution projection, and image enhancement, we create robust top-down density maps. FloorSAM uses SAM's zero-shot learning for precise room segmentation, improving reconstruction across diverse layouts. Room masks are generated via adaptive prompt points and multistage filtering, followed by joint mask and point cloud analysis for contour extraction and regularization. This produces accurate floor plans and recovers room topological relationships. Tests on Giblayout and ISPRS datasets show better accuracy, recall, and robustness than traditional methods, especially in noisy and complex settings. Code and materials: github.com/Silentbarber/FloorSAM.

</details>


### [22] [Vision-Language Models as Differentiable Semantic and Spatial Rewards for Text-to-3D Generation](https://arxiv.org/abs/2509.15772)
*Weimin Bai,Yubo Li,Weijian Luo,Wenzheng Chen,He Sun*

Main category: cs.CV

TL;DR: VLM3D是一个新颖的文本到3D生成框架，通过将大型视觉语言模型集成到SDS流程中，解决了现有方法在语义对齐和3D空间一致性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有SDS方法存在两个根本限制：(1)依赖CLIP式文本编码器导致粗粒度语义对齐，难以处理细粒度提示；(2)2D扩散先验缺乏显式3D空间约束，导致多对象场景中的几何不一致和对象关系不准确。

Method: 提出VLM3D框架，将大型视觉语言模型作为可微分语义和空间先验集成到SDS流程中。基于开源Qwen2.5-VL模型实现，利用其丰富的语言接地监督和空间理解能力。

Result: 在GPTeval3D基准测试中，VLM3D在多样化对象和复杂场景上显著优于现有SDS方法，在语义保真度、几何一致性和空间正确性方面表现优异。

Conclusion: VLM3D通过集成VLM先验，有效提升了文本到3D生成的质量，特别是在细粒度语义对齐和3D空间一致性方面取得了显著改进。

Abstract: Score Distillation Sampling (SDS) enables high-quality text-to-3D generation by supervising 3D models through the denoising of multi-view 2D renderings, using a pretrained text-to-image diffusion model to align with the input prompt and ensure 3D consistency. However, existing SDS-based methods face two fundamental limitations: (1) their reliance on CLIP-style text encoders leads to coarse semantic alignment and struggles with fine-grained prompts; and (2) 2D diffusion priors lack explicit 3D spatial constraints, resulting in geometric inconsistencies and inaccurate object relationships in multi-object scenes. To address these challenges, we propose VLM3D, a novel text-to-3D generation framework that integrates large vision-language models (VLMs) into the SDS pipeline as differentiable semantic and spatial priors. Unlike standard text-to-image diffusion priors, VLMs leverage rich language-grounded supervision that enables fine-grained prompt alignment. Moreover, their inherent vision language modeling provides strong spatial understanding, which significantly enhances 3D consistency for single-object generation and improves relational reasoning in multi-object scenes. We instantiate VLM3D based on the open-source Qwen2.5-VL model and evaluate it on the GPTeval3D benchmark. Experiments across diverse objects and complex scenes show that VLM3D significantly outperforms prior SDS-based methods in semantic fidelity, geometric coherence, and spatial correctness.

</details>


### [23] [Zero-Shot Visual Grounding in 3D Gaussians via View Retrieval](https://arxiv.org/abs/2509.15871)
*Liwei Liao,Xufeng Li,Xiaoyun Zheng,Boning Liu,Feng Gao,Ronggang Wang*

Main category: cs.CV

TL;DR: GVR是一个新颖的零样本3D视觉定位框架，通过将3D视觉定位转化为2D检索任务，避免了昂贵的3D标注和逐场景训练的需求。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉定位方法面临两个主要挑战：难以处理3D高斯溅射中的隐式空间纹理表示，需要逐场景训练；通常需要大量标注数据进行有效训练。

Method: 提出基于视图检索的定位方法，利用对象级视图检索从多个视角收集定位线索，将3D视觉定位转化为2D检索任务。

Result: 广泛实验表明，该方法在避免逐场景训练的同时实现了最先进的视觉定位性能。

Conclusion: GVR为零样本3D视觉定位研究提供了坚实基础，消除了对3D标注和逐场景训练的依赖。

Abstract: 3D Visual Grounding (3DVG) aims to locate objects in 3D scenes based on text prompts, which is essential for applications such as robotics. However, existing 3DVG methods encounter two main challenges: first, they struggle to handle the implicit representation of spatial textures in 3D Gaussian Splatting (3DGS), making per-scene training indispensable; second, they typically require larges amounts of labeled data for effective training. To this end, we propose \underline{G}rounding via \underline{V}iew \underline{R}etrieval (GVR), a novel zero-shot visual grounding framework for 3DGS to transform 3DVG as a 2D retrieval task that leverages object-level view retrieval to collect grounding clues from multiple views, which not only avoids the costly process of 3D annotation, but also eliminates the need for per-scene training. Extensive experiments demonstrate that our method achieves state-of-the-art visual grounding performance while avoiding per-scene training, providing a solid foundation for zero-shot 3DVG research. Video demos can be found in https://github.com/leviome/GVR_demos.

</details>


### [24] [DistillMatch: Leveraging Knowledge Distillation from Vision Foundation Model for Multimodal Image Matching](https://arxiv.org/abs/2509.16017)
*Meng Yang,Fan Fan,Zizhuo Li,Songchu Deng,Yong Ma,Jiayi Ma*

Main category: cs.CV

TL;DR: DistillMatch是一种多模态图像匹配方法，通过从视觉基础模型进行知识蒸馏来提取高层次语义特征，并注入模态类别信息来增强跨模态相关性理解。


<details>
  <summary>Details</summary>
Motivation: 多模态图像匹配面临显著的外观差异挑战，现有深度学习方法由于高质量标注数据稀缺而性能不佳，缺乏对多样化场景的适应性。视觉基础模型具有泛化性和鲁棒性特征表示，可用于多模态匹配。

Method: 采用知识蒸馏构建轻量级学生模型，从DINOv2和DINOv3等VFM提取高层次语义特征；注入模态类别信息保留模态特定信息；设计V2I-GAN进行可见光到伪红外图像转换以增强数据增强。

Result: 实验表明DistillMatch在公共数据集上优于现有算法。

Conclusion: 该方法通过知识蒸馏和模态信息注入有效解决了多模态图像匹配的挑战，展示了VFM在多模态任务中的潜力。

Abstract: Multimodal image matching seeks pixel-level correspondences between images of different modalities, crucial for cross-modal perception, fusion and analysis. However, the significant appearance differences between modalities make this task challenging. Due to the scarcity of high-quality annotated datasets, existing deep learning methods that extract modality-common features for matching perform poorly and lack adaptability to diverse scenarios. Vision Foundation Model (VFM), trained on large-scale data, yields generalizable and robust feature representations adapted to data and tasks of various modalities, including multimodal matching. Thus, we propose DistillMatch, a multimodal image matching method using knowledge distillation from VFM. DistillMatch employs knowledge distillation to build a lightweight student model that extracts high-level semantic features from VFM (including DINOv2 and DINOv3) to assist matching across modalities. To retain modality-specific information, it extracts and injects modality category information into the other modality's features, which enhances the model's understanding of cross-modal correlations. Furthermore, we design V2I-GAN to boost the model's generalization by translating visible to pseudo-infrared images for data augmentation. Experiments show that DistillMatch outperforms existing algorithms on public datasets.

</details>


### [25] [GLip: A Global-Local Integrated Progressive Framework for Robust Visual Speech Recognition](https://arxiv.org/abs/2509.16031)
*Tianyue Wang,Shuang Yang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: GLip是一个用于视觉语音识别的全局-局部集成渐进框架，通过双路径特征提取和两阶段学习策略，有效应对光照变化、遮挡等现实视觉挑战。


<details>
  <summary>Details</summary>
Motivation: 现有VSR方法对现实世界中的视觉挑战（如光照变化、遮挡、模糊和姿态变化）关注有限，需要更鲁棒的解决方案。

Method: 提出GLip框架，采用双路径特征提取架构，集成全局和局部特征，通过两阶段渐进学习：第一阶段学习视觉特征与语音单元的粗略对齐，第二阶段通过上下文增强模块动态整合局部特征与全局上下文。

Result: 在LRS2和LRS3基准测试中一致优于现有方法，并在新引入的挑战性中文数据集上验证了有效性。

Conclusion: GLip通过渐进式学习策略有效利用判别性局部区域，在各种视觉挑战下表现出增强的鲁棒性。

Abstract: Visual speech recognition (VSR), also known as lip reading, is the task of recognizing speech from silent video. Despite significant advancements in VSR over recent decades, most existing methods pay limited attention to real-world visual challenges such as illumination variations, occlusions, blurring, and pose changes. To address these challenges, we propose GLip, a Global-Local Integrated Progressive framework designed for robust VSR. GLip is built upon two key insights: (i) learning an initial \textit{coarse} alignment between visual features across varying conditions and corresponding speech content facilitates the subsequent learning of \textit{precise} visual-to-speech mappings in challenging environments; (ii) under adverse conditions, certain local regions (e.g., non-occluded areas) often exhibit more discriminative cues for lip reading than global features. To this end, GLip introduces a dual-path feature extraction architecture that integrates both global and local features within a two-stage progressive learning framework. In the first stage, the model learns to align both global and local visual features with corresponding acoustic speech units using easily accessible audio-visual data, establishing a coarse yet semantically robust foundation. In the second stage, we introduce a Contextual Enhancement Module (CEM) to dynamically integrate local features with relevant global context across both spatial and temporal dimensions, refining the coarse representations into precise visual-speech mappings. Our framework uniquely exploits discriminative local regions through a progressive learning strategy, demonstrating enhanced robustness against various visual challenges and consistently outperforming existing methods on the LRS2 and LRS3 benchmarks. We further validate its effectiveness on a newly introduced challenging Mandarin dataset.

</details>


### [26] [Blind-Spot Guided Diffusion for Self-supervised Real-World Denoising](https://arxiv.org/abs/2509.16091)
*Shen Cheng,Haipeng Li,Haibin Huang,Xiaohong Liu,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 提出Blind-Spot Guided Diffusion框架，通过双分支扩散模型解决盲点网络在自监督图像去噪中的局限性，在SIDD和DND数据集上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决盲点网络在自监督去噪中牺牲局部细节和引入像素不连续性的问题，以及扩散模型在自监督去噪中难以适应的问题。

Method: 采用双分支扩散框架：BSN扩散分支生成半清洁图像，常规扩散分支捕捉噪声分布；使用BSN分支指导采样过程，在无配对数据下有效训练。

Result: 在SIDD和DND数据集上的大量实验证明了该方法的卓越性能，达到了最先进的水平。

Conclusion: 该方法为真实世界图像去噪提供了一个高效的自监督解决方案，代码和预训练模型已开源。

Abstract: In this work, we present Blind-Spot Guided Diffusion, a novel self-supervised framework for real-world image denoising. Our approach addresses two major challenges: the limitations of blind-spot networks (BSNs), which often sacrifice local detail and introduce pixel discontinuities due to spatial independence assumptions, and the difficulty of adapting diffusion models to self-supervised denoising. We propose a dual-branch diffusion framework that combines a BSN-based diffusion branch, generating semi-clean images, with a conventional diffusion branch that captures underlying noise distributions. To enable effective training without paired data, we use the BSN-based branch to guide the sampling process, capturing noise structure while preserving local details. Extensive experiments on the SIDD and DND datasets demonstrate state-of-the-art performance, establishing our method as a highly effective self-supervised solution for real-world denoising. Code and pre-trained models are released at: https://github.com/Sumching/BSGD.

</details>


### [27] [RadarGaussianDet3D: An Efficient and Effective Gaussian-based 3D Detector with 4D Automotive Radars](https://arxiv.org/abs/2509.16119)
*Weiyi Xiong,Bing Zhu,Tao Huang,Zewei Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种基于高斯分布的3D雷达检测器RadarGaussianDet3D，通过高斯原语和分布作为中间表示，解决了现有4D雷达检测器特征稀疏、检测精度不足和推理速度慢的问题。


<details>
  <summary>Details</summary>
Motivation: 现有4D雷达3D检测器依赖pillar编码器进行BEV特征提取，导致特征图稀疏、表示质量下降；边界框属性独立优化导致检测精度不足；在高性能GPU上推理速度尚可，但在车载嵌入式设备上难以满足实时性要求。

Method: 设计了Point Gaussian Encoder (PGE)将每个点转换为高斯原语，采用3D Gaussian Splatting技术进行BEV栅格化，生成更密集的特征图；提出Box Gaussian Loss (BGL)将边界框转换为3D高斯分布并测量距离，实现更全面一致的优化。

Result: 在TJ4DRadSet和View-of-Delft数据集上的实验表明，RadarGaussianDet3D实现了最先进的检测精度，同时显著提升了推理速度。

Conclusion: RadarGaussianDet3D在检测精度和推理速度方面均表现出色，具有在自动驾驶中实时部署的潜力。

Abstract: 4D automotive radars have gained increasing attention for autonomous driving due to their low cost, robustness, and inherent velocity measurement capability. However, existing 4D radar-based 3D detectors rely heavily on pillar encoders for BEV feature extraction, where each point contributes to only a single BEV grid, resulting in sparse feature maps and degraded representation quality. In addition, they also optimize bounding box attributes independently, leading to sub-optimal detection accuracy. Moreover, their inference speed, while sufficient for high-end GPUs, may fail to meet the real-time requirement on vehicle-mounted embedded devices. To overcome these limitations, an efficient and effective Gaussian-based 3D detector, namely RadarGaussianDet3D is introduced, leveraging Gaussian primitives and distributions as intermediate representations for radar points and bounding boxes. In RadarGaussianDet3D, a novel Point Gaussian Encoder (PGE) is designed to transform each point into a Gaussian primitive after feature aggregation and employs the 3D Gaussian Splatting (3DGS) technique for BEV rasterization, yielding denser feature maps. PGE exhibits exceptionally low latency, owing to the optimized algorithm for point feature aggregation and fast rendering of 3DGS. In addition, a new Box Gaussian Loss (BGL) is proposed, which converts bounding boxes into 3D Gaussian distributions and measures their distance to enable more comprehensive and consistent optimization. Extensive experiments on TJ4DRadSet and View-of-Delft demonstrate that RadarGaussianDet3D achieves state-of-the-art detection accuracy while delivering substantially faster inference, highlighting its potential for real-time deployment in autonomous driving.

</details>


### [28] [AcT2I: Evaluating and Improving Action Depiction in Text-to-Image Models](https://arxiv.org/abs/2509.16141)
*Vatsal Malaviya,Agneet Chatterjee,Maitreya Patel,Yezhou Yang,Chitta Baral*

Main category: cs.CV

TL;DR: 本文介绍了AcT2I基准，用于评估文本到图像模型在生成动作中心提示图像时的性能，并提出了一种利用大型语言模型的知识蒸馏技术来增强提示信息，显著提高了图像生成准确性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型在处理复杂动作和交互场景时存在困难，无法准确捕捉动作描绘中的细微和隐含属性，导致生成的图像缺乏关键上下文细节。

Method: 开发了训练免费的知识蒸馏技术，利用大型语言模型在三个维度上增强提示信息，特别是注入时间细节，以改善图像生成准确性。

Result: 实验验证显示，领先的文本到图像模型在AcT2I基准上表现不佳，但通过提出的方法，最佳模型实现了72%的准确性提升。

Conclusion: 研究揭示了当前文本到图像方法在需要复杂推理的图像生成中的局限性，并证明系统整合语言知识可以显著提升生成细微和上下文准确图像的能力。

Abstract: Text-to-Image (T2I) models have recently achieved remarkable success in generating images from textual descriptions. However, challenges still persist in accurately rendering complex scenes where actions and interactions form the primary semantic focus. Our key observation in this work is that T2I models frequently struggle to capture nuanced and often implicit attributes inherent in action depiction, leading to generating images that lack key contextual details. To enable systematic evaluation, we introduce AcT2I, a benchmark designed to evaluate the performance of T2I models in generating images from action-centric prompts. We experimentally validate that leading T2I models do not fare well on AcT2I. We further hypothesize that this shortcoming arises from the incomplete representation of the inherent attributes and contextual dependencies in the training corpora of existing T2I models. We build upon this by developing a training-free, knowledge distillation technique utilizing Large Language Models to address this limitation. Specifically, we enhance prompts by incorporating dense information across three dimensions, observing that injecting prompts with temporal details significantly improves image generation accuracy, with our best model achieving an increase of 72%. Our findings highlight the limitations of current T2I methods in generating images that require complex reasoning and demonstrate that integrating linguistic knowledge in a systematic way can notably advance the generation of nuanced and contextually accurate images.

</details>


### [29] [MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer](https://arxiv.org/abs/2509.16197)
*Yanghao Li,Rui Qian,Bowen Pan,Haotian Zhang,Haoshuo Huang,Bowen Zhang,Jialing Tong,Haoxuan You,Xianzhi Du,Zhe Gan,Hyunjik Kim,Chao Jia,Zhenbang Wang,Yinfei Yang,Mingfei Gao,Zi-Yi Dou,Wenze Hu,Chang Gao,Dongxu Li,Philipp Dufter,Zirui Wang,Guoli Yin,Zhengdong Zhang,Chen Chen,Yang Zhao,Ruoming Pang,Zhifeng Chen*

Main category: cs.CV

TL;DR: Manzano是一个统一的多模态大语言模型框架，通过混合图像分词器和精心设计的训练方法，在图像理解和图像生成能力之间取得了良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的开源多模态模型在理解能力和生成能力之间存在性能权衡，需要一个统一的框架来同时优化这两种能力。

Method: 使用共享的视觉编码器配合两个轻量级适配器，分别生成用于图像理解的连续嵌入和用于图像生成的离散标记。采用统一的自动回归LLM预测文本和图像标记，配合辅助扩散解码器将图像标记转换为像素。

Result: Manzano在统一模型中达到了最先进的性能，在文本丰富的评估中与专业模型竞争力相当。研究表明任务冲突最小，模型规模扩展带来一致收益。

Conclusion: 混合分词器的设计选择得到验证，该框架为统一的多模态理解与生成提供了可扩展的解决方案。

Abstract: Unified multimodal Large Language Models (LLMs) that can both understand and generate visual content hold immense potential. However, existing open-source models often suffer from a performance trade-off between these capabilities. We present Manzano, a simple and scalable unified framework that substantially reduces this tension by coupling a hybrid image tokenizer with a well-curated training recipe. A single shared vision encoder feeds two lightweight adapters that produce continuous embeddings for image-to-text understanding and discrete tokens for text-to-image generation within a common semantic space. A unified autoregressive LLM predicts high-level semantics in the form of text and image tokens, with an auxiliary diffusion decoder subsequently translating the image tokens into pixels. The architecture, together with a unified training recipe over understanding and generation data, enables scalable joint learning of both capabilities. Manzano achieves state-of-the-art results among unified models, and is competitive with specialist models, particularly on text-rich evaluation. Our studies show minimal task conflicts and consistent gains from scaling model size, validating our design choice of a hybrid tokenizer.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [30] [CoReVLA: A Dual-Stage End-to-End Autonomous Driving Framework for Long-Tail Scenarios via Collect-and-Refine](https://arxiv.org/abs/2509.15968)
*Shiyu Fang,Yiming Cui,Haoyang Liang,Chen Lv,Peng Hang,Jian Sun*

Main category: cs.RO

TL;DR: CoReVLA是一个持续学习的端到端自动驾驶框架，通过数据收集和行为精炼的双阶段过程，提升在长尾安全关键场景中的性能表现。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统在长尾安全关键场景中表现有限，这些罕见情况导致了不成比例的事故数量。视觉语言动作模型具有强大的推理能力，但受限于高质量数据的缺乏和低效学习。

Method: 采用双阶段方法：1）在开源驾驶QA数据集上进行联合微调，获得基础驾驶场景理解；2）在CAVE仿真平台中收集驾驶员接管数据，通过直接偏好优化进行模型精炼，避免人工设计奖励导致的奖励黑客问题。

Result: 在Bench2Drive基准测试中，CoReVLA获得72.18的驾驶分数和50%的成功率，在长尾安全关键场景下分别比最先进方法高出7.96分和15%。案例研究显示模型能够利用过去的接管经验持续改进性能。

Conclusion: CoReVLA框架能够准确感知驾驶场景并做出适当决策，通过持续学习机制有效提升自动驾驶系统在长尾安全关键场景中的表现。

Abstract: Autonomous Driving (AD) systems have made notable progress, but their performance in long-tail, safety-critical scenarios remains limited. These rare cases contribute a disproportionate number of accidents. Vision-Language Action (VLA) models have strong reasoning abilities and offer a potential solution, but their effectiveness is limited by the lack of high-quality data and inefficient learning in such conditions. To address these challenges, we propose CoReVLA, a continual learning end-to-end autonomous driving framework that improves the performance in long-tail scenarios through a dual-stage process of data Collection and behavior Refinement. First, the model is jointly fine-tuned on a mixture of open-source driving QA datasets, allowing it to acquire a foundational understanding of driving scenarios. Next, CoReVLA is deployed within the Cave Automatic Virtual Environment (CAVE) simulation platform, where driver takeover data is collected from real-time interactions. Each takeover indicates a long-tail scenario that CoReVLA fails to handle reliably. Finally, the model is refined via Direct Preference Optimization (DPO), allowing it to learn directly from human preferences and thereby avoid reward hacking caused by manually designed rewards. Extensive open-loop and closed-loop experiments demonstrate that the proposed CoReVLA model can accurately perceive driving scenarios and make appropriate decisions. On the Bench2Drive benchmark, CoReVLA achieves a Driving Score (DS) of 72.18 and a Success Rate (SR) of 50%, outperforming state-of-the-art methods by 7.96 DS and 15% SR under long-tail, safety-critical scenarios. Furthermore, case studies demonstrate the model's ability to continually improve its performance in similar failure-prone scenarios by leveraging past takeover experiences. All codea and preprocessed datasets are available at: https://github.com/FanGShiYuu/CoReVLA

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [31] [Kuramoto Orientation Diffusion Models](https://arxiv.org/abs/2509.15328)
*Yue Song,T. Anderson Keller,Sevan Brodjian,Takeru Miyato,Yisong Yue,Pietro Perona,Max Welling*

Main category: cs.LG

TL;DR: 提出基于Kuramoto同步动力学的生成模型，用于处理具有方向性模式的图像（如指纹和纹理），通过同步-去同步过程实现结构化生成。


<details>
  <summary>Details</summary>
Motivation: 传统各向同性欧几里得扩散难以有效建模具有相干角度方向模式的图像，受生物系统中相位同步现象启发，利用Kuramoto动力学作为结构化先验。

Method: 使用随机Kuramoto动力学构建分数生成模型，前向过程通过全局/局部耦合振荡器相互作用实现相位同步，反向过程通过学习的分数函数进行去同步生成。

Result: 在通用图像基准上取得竞争性结果，在指纹和纹理等方向密集数据集上显著提升生成质量。

Conclusion: 生物启发的同步动力学作为结构化先验在生成建模中具有广阔前景。

Abstract: Orientation-rich images, such as fingerprints and textures, often exhibit coherent angular directional patterns that are challenging to model using standard generative approaches based on isotropic Euclidean diffusion. Motivated by the role of phase synchronization in biological systems, we propose a score-based generative model built on periodic domains by leveraging stochastic Kuramoto dynamics in the diffusion process. In neural and physical systems, Kuramoto models capture synchronization phenomena across coupled oscillators -- a behavior that we re-purpose here as an inductive bias for structured image generation. In our framework, the forward process performs \textit{synchronization} among phase variables through globally or locally coupled oscillator interactions and attraction to a global reference phase, gradually collapsing the data into a low-entropy von Mises distribution. The reverse process then performs \textit{desynchronization}, generating diverse patterns by reversing the dynamics with a learned score function. This approach enables structured destruction during forward diffusion and a hierarchical generation process that progressively refines global coherence into fine-scale details. We implement wrapped Gaussian transition kernels and periodicity-aware networks to account for the circular geometry. Our method achieves competitive results on general image benchmarks and significantly improves generation quality on orientation-dense datasets like fingerprints and textures. Ultimately, this work demonstrates the promise of biologically inspired synchronization dynamics as structured priors in generative modeling.

</details>


### [32] [Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification](https://arxiv.org/abs/2509.15591)
*Zinan Lin,Enshu Liu,Xuefei Ning,Junyi Zhu,Wenyu Wang,Sergey Yekhanin*

Main category: cs.LG

TL;DR: 本文提出Latent Zoning Network (LZN)，通过创建共享高斯潜空间来统一生成建模、表示学习和分类三大机器学习任务，简化ML流程并促进任务间协同。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习中生成建模、表示学习和分类的最先进解决方案相互独立，缺乏统一原则。本文旨在探索能否用一个统一框架同时解决这三个核心问题。

Method: LZN构建共享高斯潜空间，为每种数据类型（如图像、文本、标签）配备编码器和解码器。ML任务通过编码器和解码器的组合实现：条件生成使用标签编码器和图像解码器，表示学习使用图像编码器，分类使用图像编码器和标签解码器。

Result: 在三个场景中验证LZN：1）与Rectified Flow结合将CIFAR10的FID从2.76提升到2.59；2）无监督表示学习在线性分类上优于MoCo和SimCLR；3）在CIFAR10上同时实现生成和分类任务，获得SOTA分类精度。

Conclusion: LZN展示了统一框架解决多任务ML问题的潜力，为简化机器学习流程和促进任务协同提供了有前景的方向。

Abstract: Generative modeling, representation learning, and classification are three core problems in machine learning (ML), yet their state-of-the-art (SoTA) solutions remain largely disjoint. In this paper, we ask: Can a unified principle address all three? Such unification could simplify ML pipelines and foster greater synergy across tasks. We introduce Latent Zoning Network (LZN) as a step toward this goal. At its core, LZN creates a shared Gaussian latent space that encodes information across all tasks. Each data type (e.g., images, text, labels) is equipped with an encoder that maps samples to disjoint latent zones, and a decoder that maps latents back to data. ML tasks are expressed as compositions of these encoders and decoders: for example, label-conditional image generation uses a label encoder and image decoder; image embedding uses an image encoder; classification uses an image encoder and label decoder. We demonstrate the promise of LZN in three increasingly complex scenarios: (1) LZN can enhance existing models (image generation): When combined with the SoTA Rectified Flow model, LZN improves FID on CIFAR10 from 2.76 to 2.59-without modifying the training objective. (2) LZN can solve tasks independently (representation learning): LZN can implement unsupervised representation learning without auxiliary loss functions, outperforming the seminal MoCo and SimCLR methods by 9.3% and 0.2%, respectively, on downstream linear classification on ImageNet. (3) LZN can solve multiple tasks simultaneously (joint generation and classification): With image and label encoders/decoders, LZN performs both tasks jointly by design, improving FID and achieving SoTA classification accuracy on CIFAR10. The code and trained models are available at https://github.com/microsoft/latent-zoning-networks. The project website is at https://zinanlin.me/blogs/latent_zoning_networks.html.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [33] [Recent Advancements in Microscopy Image Enhancement using Deep Learning: A Survey](https://arxiv.org/abs/2509.15363)
*Debasish Dutta,Neeharika Sonowal,Risheraj Barauh,Deepjyoti Chetia,Sanjib Kr Kalita*

Main category: eess.IV

TL;DR: 这篇综述论文概述了深度学习在显微镜图像增强领域的最新进展，重点关注超分辨率、重建和去噪三个关键领域的发展、应用、挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 显微镜图像增强对于理解生物细胞和材料的微观细节至关重要。近年来，深度学习方法的快速发展显著推动了该领域的进步，需要对这些最新技术进行系统性总结。

Method: 采用综述研究方法，对显微镜图像增强领域（特别是超分辨率、重建和去噪三个核心领域）的深度学习技术进行系统性梳理和分析。

Result: 提供了该领域快速发展现状的快照，总结了当前趋势和深度学习的实际应用价值。

Conclusion: 深度学习在显微镜图像增强领域展现出巨大潜力，但仍面临挑战，需要进一步研究和发展。

Abstract: Microscopy image enhancement plays a pivotal role in understanding the details of biological cells and materials at microscopic scales. In recent years, there has been a significant rise in the advancement of microscopy image enhancement, specifically with the help of deep learning methods. This survey paper aims to provide a snapshot of this rapidly growing state-of-the-art method, focusing on its evolution, applications, challenges, and future directions. The core discussions take place around the key domains of microscopy image enhancement of super-resolution, reconstruction, and denoising, with each domain explored in terms of its current trends and their practical utility of deep learning.

</details>


### [34] [Analysis Plug-and-Play Methods for Imaging Inverse Problems](https://arxiv.org/abs/2509.15422)
*Edward P. Chandler,Shirin Shoushtari,Brendt Wohlberg,Ulugbek S. Kamilov*

Main category: eess.IV

TL;DR: 本文提出了一种基于梯度域的PnP（Plug-and-Play Priors）分析方法，通过在梯度域而非图像域训练去噪器，将总变差正则化扩展为学习型TV正则化，并开发了两种分析PnP算法。


<details>
  <summary>Details</summary>
Motivation: 传统的PnP方法直接在图像域应用去噪器作为隐式先验，本文探索在图像的变换表示（如梯度域）上施加先验的替代分析公式，这可以看作是总变差正则化的学习型扩展。

Method: 训练在梯度域操作的高斯去噪器，开发了基于半二次分裂（APnP-HQS）和交替方向乘子法（APnP-ADMM）的两种分析PnP算法。

Result: 在图像去模糊和超分辨率任务上的评估表明，分析公式的性能与图像域PnP算法相当。

Conclusion: 梯度域PnP方法为图像重建提供了一种有效的替代方案，将经典TV正则化成功扩展为数据驱动的学习型正则化方法。

Abstract: Plug-and-Play Priors (PnP) is a popular framework for solving imaging inverse problems by integrating learned priors in the form of denoisers trained to remove Gaussian noise from images. In standard PnP methods, the denoiser is applied directly in the image domain, serving as an implicit prior on natural images. This paper considers an alternative analysis formulation of PnP, in which the prior is imposed on a transformed representation of the image, such as its gradient. Specifically, we train a Gaussian denoiser to operate in the gradient domain, rather than on the image itself. Conceptually, this is an extension of total variation (TV) regularization to learned TV regularization. To incorporate this gradient-domain prior in image reconstruction algorithms, we develop two analysis PnP algorithms based on half-quadratic splitting (APnP-HQS) and the alternating direction method of multipliers (APnP-ADMM). We evaluate our approach on image deblurring and super-resolution, demonstrating that the analysis formulation achieves performance comparable to image-domain PnP algorithms.

</details>


### [35] [QWD-GAN: Quality-aware Wavelet-driven GAN for Unsupervised Medical Microscopy Images Denoising](https://arxiv.org/abs/2509.15814)
*Qijun Yang,Yating Huang,Lintao Xiang,Hujun Yin*

Main category: eess.IV

TL;DR: 提出了一种基于GAN的无监督图像去噪方法QWD-GAN，通过小波变换多尺度自适应生成器和双分支判别器，在生物医学显微镜图像去噪中取得优异效果。


<details>
  <summary>Details</summary>
Motivation: 生物医学显微镜图像去噪面临采集条件限制、复杂噪声类型、算法适应性和临床需求等多重挑战，现有深度学习方法在图像细节保留、算法效率和临床可解释性方面仍需改进。

Method: 采用GAN架构，引入基于小波变换的多尺度自适应生成器，以及集成差异感知特征图与原始特征的双分支判别器。

Result: 在多个生物医学显微镜图像数据集上的实验表明，该模型实现了最先进的去噪性能，特别擅长保留高频信息，且双分支判别器与多种GAN框架兼容。

Conclusion: 提出的QWD-GAN模型在生物医学图像去噪中表现出色，具有质量感知和小波驱动的特点。

Abstract: Image denoising plays a critical role in biomedical and microscopy imaging, especially when acquiring wide-field fluorescence-stained images. This task faces challenges in multiple fronts, including limitations in image acquisition conditions, complex noise types, algorithm adaptability, and clinical application demands. Although many deep learning-based denoising techniques have demonstrated promising results, further improvements are needed in preserving image details, enhancing algorithmic efficiency, and increasing clinical interpretability. We propose an unsupervised image denoising method based on a Generative Adversarial Network (GAN) architecture. The approach introduces a multi-scale adaptive generator based on the Wavelet Transform and a dual-branch discriminator that integrates difference perception feature maps with original features. Experimental results on multiple biomedical microscopy image datasets show that the proposed model achieves state-of-the-art denoising performance, particularly excelling in the preservation of high-frequency information. Furthermore, the dual-branch discriminator is seamlessly compatible with various GAN frameworks. The proposed quality-aware, wavelet-driven GAN denoising model is termed as QWD-GAN.

</details>


### [36] [PRISM: Probabilistic and Robust Inverse Solver with Measurement-Conditioned Diffusion Prior for Blind Inverse Problems](https://arxiv.org/abs/2509.16106)
*Yuanyun Hu,Evan Bell,Guijin Wang,Yu Sun*

Main category: eess.IV

TL;DR: PRISM是一种新的概率鲁棒逆求解器，使用测量条件扩散先验来解决盲逆问题，无需完整前向算子知识，在盲图像去模糊任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前大多数基于扩散模型的逆求解器需要完整的前向算子知识，这限制了它们在盲逆问题中的应用。

Method: 将强大的测量条件扩散模型融入理论上有原则的后验采样方案中，开发了PRISM方法。

Result: 在盲图像去模糊实验中，PRISM在图像和模糊核恢复方面都优于现有最先进方法。

Conclusion: PRISM为盲逆问题提供了有效的解决方案，在计算成像领域具有重要应用价值。

Abstract: Diffusion models are now commonly used to solve inverse problems in computational imaging. However, most diffusion-based inverse solvers require complete knowledge of the forward operator to be used. In this work, we introduce a novel probabilistic and robust inverse solver with measurement-conditioned diffusion prior (PRISM) to effectively address blind inverse problems. PRISM offers a technical advancement over current methods by incorporating a powerful measurement-conditioned diffusion model into a theoretically principled posterior sampling scheme. Experiments on blind image deblurring validate the effectiveness of the proposed method, demonstrating the superior performance of PRISM over state-of-the-art baselines in both image and blur kernel recovery.

</details>
