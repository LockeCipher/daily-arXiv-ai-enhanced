<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 5]
- [cs.CV](#cs.CV) [Total: 41]
- [eess.IV](#eess.IV) [Total: 6]
- [cs.CE](#cs.CE) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Real-Time Scene Reconstruction using Light Field Probes](https://arxiv.org/abs/2507.14624)
*Yaru Liu,Derek Nowrouzezahri,Morgan Mcguire*

Main category: cs.GR

TL;DR: 提出了一种基于探针数据结构的神经渲染方法，用于高效重建复杂大场景，无需显式依赖场景几何。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染方法在大规模场景中难以平衡质量、速度和复杂度，而传统几何重建方法成本高。

Method: 利用稀疏图像重建多尺度隐式几何表示，通过探针数据结构存储高精度深度信息，实现高效渲染。

Result: 方法显著降低了计算成本，渲染效率与场景复杂度无关，适用于VR/AR应用。

Conclusion: 探针数据结构结合神经表示，为大规模场景重建提供了高效解决方案。

Abstract: Reconstructing photo-realistic large-scale scenes from images, for example at city scale, is a long-standing problem in computer graphics. Neural rendering is an emerging technique that enables photo-realistic image synthesis from previously unobserved viewpoints; however, state-of-the-art neural rendering methods have difficulty efficiently rendering a high complex large-scale scene because these methods typically trade scene size, fidelity, and rendering speed for quality. The other stream of techniques utilizes scene geometries for reconstruction. But the cost of building and maintaining a large set of geometry data increases as scene size grows. Our work explores novel view synthesis methods that efficiently reconstruct complex scenes without explicit use of scene geometries. Specifically, given sparse images of the scene (captured from the real world), we reconstruct intermediate, multi-scale, implicit representations of scene geometries. In this way, our method avoids explicitly relying on scene geometry, significantly reducing the computational cost of maintaining large 3D data. Unlike current methods, we reconstruct the scene using a probe data structure. Probe data hold highly accurate depth information of dense data points, enabling the reconstruction of highly complex scenes. By reconstructing the scene using probe data, the rendering cost is independent of the complexity of the scene. As such, our approach combines geometry reconstruction and novel view synthesis. Moreover, when rendering large-scale scenes, compressing and streaming probe data is more efficient than using explicit scene geometry. Therefore, our neural representation approach can potentially be applied to virtual reality (VR) and augmented reality (AR) applications.

</details>


### [2] [Towards Geometric and Textural Consistency 3D Scene Generation via Single Image-guided Model Generation and Layout Optimization](https://arxiv.org/abs/2507.14841)
*Xiang Tang,Ruotong Li,Xiaopeng Fan*

Main category: cs.GR

TL;DR: 提出了一种三阶段框架，通过单图像引导生成和空间布局优化，解决从单RGB图像生成3D场景的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前方法在多物体场景中难以保证生成质量和场景一致性，需改进。

Method: 三阶段框架：图像实例分割与修复、伪立体视角构建与相机参数估计、模型参数化与布局优化。

Result: 在几何精度和纹理保真度上优于现有方法，且在场景布局合成中表现优异。

Conclusion: 该方法显著提升了3D场景生成的完整性和一致性。

Abstract: In recent years, 3D generation has made great strides in both academia and industry. However, generating 3D scenes from a single RGB image remains a significant challenge, as current approaches often struggle to ensure both object generation quality and scene coherence in multi-object scenarios. To overcome these limitations, we propose a novel three-stage framework for 3D scene generation with explicit geometric representations and high-quality textural details via single image-guided model generation and spatial layout optimization. Our method begins with an image instance segmentation and inpainting phase, which recovers missing details of occluded objects in the input images, thereby achieving complete generation of foreground 3D assets. Subsequently, our approach captures the spatial geometry of reference image by constructing pseudo-stereo viewpoint for camera parameter estimation and scene depth inference, while employing a model selection strategy to ensure optimal alignment between the 3D assets generated in the previous step and the input. Finally, through model parameterization and minimization of the Chamfer distance between point clouds in 3D and 2D space, our approach optimizes layout parameters to produce an explicit 3D scene representation that maintains precise alignment with input guidance image. Extensive experiments on multi-object scene image sets have demonstrated that our approach not only outperforms state-of-the-art methods in terms of geometric accuracy and texture fidelity of individual generated 3D models, but also has significant advantages in scene layout synthesis.

</details>


### [3] [Blended Point Cloud Diffusion for Localized Text-guided Shape Editing](https://arxiv.org/abs/2507.15399)
*Etai Sella,Noam Atia,Ron Mokady,Hadar Averbuch-Elor*

Main category: cs.GR

TL;DR: 提出了一种基于修复的框架，用于编辑点云表示的3D形状，通过结构引导和坐标混合算法实现局部编辑并保持全局一致性。


<details>
  <summary>Details</summary>
Motivation: 自然语言为3D形状的局部细粒度编辑提供了直观接口，但现有方法难以在局部修改时保持全局一致性。

Method: 利用基础3D扩散模型进行局部形状编辑，结合部分条件形状的结构引导，并提出了推理时坐标混合算法以平衡重建与修复。

Result: 实验表明，该方法在保真度和文本描述一致性上优于其他技术。

Conclusion: 该方法实现了3D形状的细粒度编辑，同时避免了计算成本高且不准确的逆过程。

Abstract: Natural language offers a highly intuitive interface for enabling localized fine-grained edits of 3D shapes. However, prior works face challenges in preserving global coherence while locally modifying the input 3D shape. In this work, we introduce an inpainting-based framework for editing shapes represented as point clouds. Our approach leverages foundation 3D diffusion models for achieving localized shape edits, adding structural guidance in the form of a partial conditional shape, ensuring that other regions correctly preserve the shape's identity. Furthermore, to encourage identity preservation also within the local edited region, we propose an inference-time coordinate blending algorithm which balances reconstruction of the full shape with inpainting at a progression of noise levels during the inference process. Our coordinate blending algorithm seamlessly blends the original shape with its edited version, enabling a fine-grained editing of 3D shapes, all while circumventing the need for computationally expensive and often inaccurate inversion. Extensive experiments show that our method outperforms alternative techniques across a wide range of metrics that evaluate both fidelity to the original shape and also adherence to the textual description.

</details>


### [4] [ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting](https://arxiv.org/abs/2507.15454)
*Ruijie Zhu,Mulin Yu,Linning Xu,Lihan Jiang,Yixuan Li,Tianzhu Zhang,Jiangmiao Pang,Bo Dai*

Main category: cs.GR

TL;DR: ObjectGS是一个结合3D高斯泼溅与语义理解的对象感知框架，通过局部锚点和对象ID实现精确的对象级重建。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅缺乏语义理解，限制了对象级感知能力，因此需要一种能够统一3D重建与语义理解的方法。

Method: ObjectGS将场景中的对象建模为局部锚点，生成神经高斯并共享对象ID，通过动态调整锚点和优化特征，结合分类损失实现语义约束。

Result: 实验表明，ObjectGS在开放词汇和全景分割任务中优于现有方法，并能无缝集成网格提取和场景编辑等应用。

Conclusion: ObjectGS成功实现了3D场景重建与语义理解的统一，为对象级感知提供了高效解决方案。

Abstract: 3D Gaussian Splatting is renowned for its high-fidelity reconstructions and real-time novel view synthesis, yet its lack of semantic understanding limits object-level perception. In this work, we propose ObjectGS, an object-aware framework that unifies 3D scene reconstruction with semantic understanding. Instead of treating the scene as a unified whole, ObjectGS models individual objects as local anchors that generate neural Gaussians and share object IDs, enabling precise object-level reconstruction. During training, we dynamically grow or prune these anchors and optimize their features, while a one-hot ID encoding with a classification loss enforces clear semantic constraints. We show through extensive experiments that ObjectGS not only outperforms state-of-the-art methods on open-vocabulary and panoptic segmentation tasks, but also integrates seamlessly with applications like mesh extraction and scene editing. Project page: https://ruijiezhu94.github.io/ObjectGS_page

</details>


### [5] [Gaussian Splatting with Discretized SDF for Relightable Assets](https://arxiv.org/abs/2507.15629)
*Zuo-Liang Zhu,Jian Yang,Beibei Wang*

Main category: cs.GR

TL;DR: 论文提出了一种离散化SDF方法，通过将连续SDF编码到高斯基元中，解决了3D高斯溅射在逆渲染中的几何约束问题，提高了重光照质量且无需额外内存。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射（3DGS）在逆渲染中因离散高斯基元难以应用几何约束，现有方法引入SDF但增加内存和训练复杂度。本文旨在解决这一问题。

Method: 提出离散化SDF，将其编码到高斯基元中，并通过SDF-to-opacity转换链接SDF与高斯不透明度，避免光线步进计算。使用投影一致性损失正则化离散样本。

Result: 实验表明，该方法在重光照质量上优于现有基于高斯的逆渲染方法，且无需额外内存或复杂优化设计。

Conclusion: 离散化SDF方法有效解决了3DGS在逆渲染中的几何约束问题，提升了性能且简化了实现。

Abstract: 3D Gaussian splatting (3DGS) has shown its detailed expressive ability and highly efficient rendering speed in the novel view synthesis (NVS) task. The application to inverse rendering still faces several challenges, as the discrete nature of Gaussian primitives makes it difficult to apply geometry constraints. Recent works introduce the signed distance field (SDF) as an extra continuous representation to regularize the geometry defined by Gaussian primitives. It improves the decomposition quality, at the cost of increasing memory usage and complicating training. Unlike these works, we introduce a discretized SDF to represent the continuous SDF in a discrete manner by encoding it within each Gaussian using a sampled value. This approach allows us to link the SDF with the Gaussian opacity through an SDF-to-opacity transformation, enabling rendering the SDF via splatting and avoiding the computational cost of ray marching.The key challenge is to regularize the discrete samples to be consistent with the underlying SDF, as the discrete representation can hardly apply the gradient-based constraints (\eg Eikonal loss). For this, we project Gaussians onto the zero-level set of SDF and enforce alignment with the surface from splatting, namely a projection-based consistency loss. Thanks to the discretized SDF, our method achieves higher relighting quality, while requiring no extra memory beyond GS and avoiding complex manually designed optimization. The experiments reveal that our method outperforms existing Gaussian-based inverse rendering methods. Our code is available at https://github.com/NK-CS-ZZL/DiscretizedSDF.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [6] [Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution](https://arxiv.org/abs/2507.14367)
*Weiming Ren,Raghav Goyal,Zhiming Hu,Tristan Ty Aumentado-Armstrong,Iqbal Mohomed,Alex Levinshtein*

Main category: cs.CV

TL;DR: 该论文研究了生成超分辨率（GSR）中的幻觉问题，提出了一种基于多模态大语言模型（MLLM）的幻觉评分（HS）方法，并通过特征距离优化GSR模型以减少幻觉。


<details>
  <summary>Details</summary>
Motivation: GSR模型虽然在高感知图像质量上表现优异，但其生成的细节可能与低分辨率图像（LRI）或真实图像（GTI）不匹配，这种幻觉问题限制了其实际应用。

Method: 利用MLLM构建提示，评估幻觉视觉元素并生成HS；发现某些深度特征距离与HS强相关，提出用这些特征作为可微奖励函数优化GSR模型。

Result: HS与人类评估高度一致，并为超分辨率模型提供了补充性见解；通过特征距离优化有效减少了幻觉。

Conclusion: HS是衡量和减少GSR幻觉的有效工具，特征距离优化方法为模型对齐提供了新思路。

Abstract: Generative super-resolution (GSR) currently sets the state-of-the-art in terms of perceptual image quality, overcoming the "regression-to-the-mean" blur of prior non-generative models. However, from a human perspective, such models do not fully conform to the optimal balance between quality and fidelity. Instead, a different class of artifacts, in which generated details fail to perceptually match the low resolution image (LRI) or ground-truth image (GTI), is a critical but under studied issue in GSR, limiting its practical deployments. In this work, we focus on measuring, analyzing, and mitigating these artifacts (i.e., "hallucinations"). We observe that hallucinations are not well-characterized with existing image metrics or quality models, as they are orthogonal to both exact fidelity and no-reference quality. Instead, we take advantage of a multimodal large language model (MLLM) by constructing a prompt that assesses hallucinatory visual elements and generates a "Hallucination Score" (HS). We find that our HS is closely aligned with human evaluations, and also provides complementary insights to prior image metrics used for super-resolution (SR) models. In addition, we find certain deep feature distances have strong correlations with HS. We therefore propose to align the GSR models by using such features as differentiable reward functions to mitigate hallucinations.

</details>


### [7] [Adaptive 3D Gaussian Splatting Video Streaming](https://arxiv.org/abs/2507.14432)
*Han Gong,Qiyue Li,Zhi Liu,Hao Zhou,Peng Yuan Zhou,Zhu Li,Jie Li*

Main category: cs.CV

TL;DR: 提出了一种基于高斯变形场的3DGS视频流框架，通过混合显著性分块和差异化质量建模，实现了高效压缩和带宽适应。


<details>
  <summary>Details</summary>
Motivation: 3DGS视频因其数据量大和传输复杂度高，对流媒体提出了挑战。

Method: 设计基于高斯变形场的3DGS视频构建方法，结合混合显著性分块和差异化质量建模。

Result: 实验验证表明，该方法在视频质量、压缩效果和传输速率上优于现有方法。

Conclusion: 提出的框架有效解决了3DGS视频流的高效传输问题。

Abstract: The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the quality of volumetric video representation. Meanwhile, in contrast to conventional volumetric video, 3DGS video poses significant challenges for streaming due to its substantially larger data volume and the heightened complexity involved in compression and transmission. To address these issues, we introduce an innovative framework for 3DGS volumetric video streaming. Specifically, we design a 3DGS video construction method based on the Gaussian deformation field. By employing hybrid saliency tiling and differentiated quality modeling of 3DGS video, we achieve efficient data compression and adaptation to bandwidth fluctuations while ensuring high transmission quality. Then we build a complete 3DGS video streaming system and validate the transmission performance. Through experimental evaluation, our method demonstrated superiority over existing approaches in various aspects, including video quality, compression effectiveness, and transmission rate.

</details>


### [8] [IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark](https://arxiv.org/abs/2507.14449)
*Zhe Cao,Jin Zhang,Ruiheng Zhang*

Main category: cs.CV

TL;DR: IRGPT是一种针对真实红外图像的多模态大语言模型，通过大规模红外-文本数据集（IR-TD）和双跨模态课程迁移学习策略，解决了红外图像领域数据稀缺和特性独特的问题。


<details>
  <summary>Details</summary>
Motivation: 解决真实红外图像因数据稀缺和领域特性导致的视觉-语言模型性能受限问题。

Method: 构建IR-TD数据集（26万真实红外图像-文本对），提出双跨模态课程迁移学习策略，从可见光到红外域系统迁移知识。

Result: 在9项任务（如识别、定位）中达到最先进性能。

Conclusion: IRGPT通过真实数据和迁移学习策略，显著提升了红外图像的多模态处理能力。

Abstract: Real-world infrared imagery presents unique challenges for vision-language models due to the scarcity of aligned text data and domain-specific characteristics. Although existing methods have advanced the field, their reliance on synthetic infrared images generated through style transfer from visible images, which limits their ability to capture the unique characteristics of the infrared modality. To address this, we propose IRGPT, the first multi-modal large language model for real-world infrared images, built upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K authentic image-text pairs. The proposed IR-TD dataset contains real infrared images paired with meticulously handcrafted texts, where the initial drafts originated from two complementary processes: (1) LLM-generated descriptions of visible images, and (2) rule-based descriptions of annotations. Furthermore, we introduce a bi-cross-modal curriculum transfer learning strategy that systematically transfers knowledge from visible to infrared domains by considering the difficulty scores of both infrared-visible and infrared-text. Evaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT achieves state-of-the-art performance even compared with larger-scale models.

</details>


### [9] [Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation](https://arxiv.org/abs/2507.14454)
*Han Gong,Qiyue Li,Jie Li,Zhi Liu*

Main category: cs.CV

TL;DR: 本文提出了一种针对3D高斯泼溅视频（3DGS）流媒体的全面解决方案，包括基于显著性分析的自适应分块技术、新型质量评估框架和基于元学习的自适应比特率算法，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 3DGS流媒体在提供沉浸式3D视频体验方面表现出色，但仍面临分块、质量评估和比特率适应等基础挑战，需要进一步研究。

Method: 提出了自适应3DGS分块技术（结合显著性和时空特征）、新型质量评估框架（联合评估3DGS表示和2D渲染图像质量）和基于元学习的自适应比特率算法。

Result: 实验表明，所提方法在性能上显著优于现有技术。

Conclusion: 本文的解决方案为3DGS流媒体的关键挑战提供了有效方法，推动了该领域的发展。

Abstract: 3D Gaussian splatting video (3DGS) streaming has recently emerged as a research hotspot in both academia and industry, owing to its impressive ability to deliver immersive 3D video experiences. However, research in this area is still in its early stages, and several fundamental challenges, such as tiling, quality assessment, and bitrate adaptation, require further investigation. In this paper, we tackle these challenges by proposing a comprehensive set of solutions. Specifically, we propose an adaptive 3DGS tiling technique guided by saliency analysis, which integrates both spatial and temporal features. Each tile is encoded into versions possessing dedicated deformation fields and multiple quality levels for adaptive selection. We also introduce a novel quality assessment framework for 3DGS video that jointly evaluates spatial-domain degradation in 3DGS representations during streaming and the quality of the resulting 2D rendered images. Additionally, we develop a meta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS video streaming, achieving optimal performance across varying network conditions. Extensive experiments demonstrate that our proposed approaches significantly outperform state-of-the-art methods.

</details>


### [10] [Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow](https://arxiv.org/abs/2507.14500)
*Zhiyuan Hua,Dehao Yuan,Cornelia Fermüller*

Main category: cs.CV

TL;DR: 提出了一种基于事件法向流的运动分割与自运动估计框架，利用神经形态视觉传感器数据，通过几何约束和优化流程实现高效分割与运动估计。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖光流或深度估计，而神经形态传感器的高时空分辨率数据未被充分利用。本文旨在利用事件数据特性，提出更高效的运动分割与估计方法。

Method: 采用优化流程，包括事件过分割、残差分析分离运动物体、基于运动相似性和时间一致性的层次聚类。

Result: 在EVIMO2v2数据集上验证，无需完整光流计算即可实现精确分割和运动估计，尤其在物体边界表现优越。

Conclusion: 该方法在实时机器人及导航应用中具有显著潜力，展现了事件数据的独特优势。

Abstract: This paper introduces a robust framework for motion segmentation and egomotion estimation using event-based normal flow, tailored specifically for neuromorphic vision sensors. In contrast to traditional methods that rely heavily on optical flow or explicit depth estimation, our approach exploits the sparse, high-temporal-resolution event data and incorporates geometric constraints between normal flow, scene structure, and inertial measurements. The proposed optimization-based pipeline iteratively performs event over-segmentation, isolates independently moving objects via residual analysis, and refines segmentations using hierarchical clustering informed by motion similarity and temporal consistency. Experimental results on the EVIMO2v2 dataset validate that our method achieves accurate segmentation and translational motion estimation without requiring full optical flow computation. This approach demonstrates significant advantages at object boundaries and offers considerable potential for scalable, real-time robotic and navigation applications.

</details>


### [11] [Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey](https://arxiv.org/abs/2507.14501)
*Jiahui Zhang,Yuelei Li,Anpei Chen,Muyu Xu,Kunhao Liu,Jianyuan Wang,Xiao-Xiao Long,Hanxue Liang,Zexiang Xu,Hao Su,Christian Theobalt,Christian Rupprecht,Andrea Vedaldi,Hanspeter Pfister,Shijian Lu,Fangneng Zhan*

Main category: cs.CV

TL;DR: 本文综述了基于前馈方法的3D重建和视图合成技术，分类讨论了不同表示架构，并探讨了其应用、数据集和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统3D重建和视图合成方法计算复杂，难以应用于实际场景，而基于深度学习的前馈方法提供了快速且通用的解决方案。

Method: 通过分类讨论点云、3D高斯泼溅（3DGS）、神经辐射场（NeRF）等表示架构，分析其在姿态无关重建、动态3D重建等任务中的应用。

Result: 总结了前馈方法在数字人、SLAM、机器人等领域的应用，并提供了常用数据集和评估协议的详细统计。

Conclusion: 前馈方法在3D视觉领域具有巨大潜力，但仍存在开放挑战，未来研究方向值得关注。

Abstract: 3D reconstruction and view synthesis are foundational problems in computer vision, graphics, and immersive technologies such as augmented reality (AR), virtual reality (VR), and digital twins. Traditional methods rely on computationally intensive iterative optimization in a complex chain, limiting their applicability in real-world scenarios. Recent advances in feed-forward approaches, driven by deep learning, have revolutionized this field by enabling fast and generalizable 3D reconstruction and view synthesis. This survey offers a comprehensive review of feed-forward techniques for 3D reconstruction and view synthesis, with a taxonomy according to the underlying representation architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural Radiance Fields (NeRF), etc. We examine key tasks such as pose-free reconstruction, dynamic 3D reconstruction, and 3D-aware image and video synthesis, highlighting their applications in digital humans, SLAM, robotics, and beyond. In addition, we review commonly used datasets with detailed statistics, along with evaluation protocols for various downstream tasks. We conclude by discussing open research challenges and promising directions for future work, emphasizing the potential of feed-forward approaches to advance the state of the art in 3D vision.

</details>


### [12] [DCHM: Depth-Consistent Human Modeling for Multiview Detection](https://arxiv.org/abs/2507.14505)
*Jiahao Ma,Tianyu Wang,Miaomiao Liu,David Ahmedt-Aristizabal,Chuong Nguyen*

Main category: cs.CV

TL;DR: 提出了一种名为DCHM的框架，通过深度一致性和多视图融合改进行人检测中的人体建模，显著减少噪声并提升精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在人体建模中引入噪声且精度低，依赖昂贵的人工标注，难以泛化到多样场景。

Method: 采用超像素级高斯泼溅技术，实现稀疏视图、大规模和拥挤场景下的多视图深度一致性，生成精确点云。

Result: 实验表明，DCHM显著减少噪声，性能优于现有方法，并首次在挑战性场景中重建行人和多视图分割。

Conclusion: DCHM无需依赖人工标注，能准确建模行人，适用于复杂场景，是行人检测领域的重大进步。

Abstract: Multiview pedestrian detection typically involves two stages: human modeling and pedestrian localization. Human modeling represents pedestrians in 3D space by fusing multiview information, making its quality crucial for detection accuracy. However, existing methods often introduce noise and have low precision. While some approaches reduce noise by fitting on costly multiview 3D annotations, they often struggle to generalize across diverse scenes. To eliminate reliance on human-labeled annotations and accurately model humans, we propose Depth-Consistent Human Modeling (DCHM), a framework designed for consistent depth estimation and multiview fusion in global coordinates. Specifically, our proposed pipeline with superpixel-wise Gaussian Splatting achieves multiview depth consistency in sparse-view, large-scaled, and crowded scenarios, producing precise point clouds for pedestrian localization. Extensive validations demonstrate that our method significantly reduces noise during human modeling, outperforming previous state-of-the-art baselines. Additionally, to our knowledge, DCHM is the first to reconstruct pedestrians and perform multiview segmentation in such a challenging setting. Code is available on the \href{https://jiahao-ma.github.io/DCHM/}{project page}.

</details>


### [13] [Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance](https://arxiv.org/abs/2507.14553)
*Xiaoran Wu*

Main category: cs.CV

TL;DR: 开发了一个相机引导系统，帮助用户识别和去除照片中的杂乱内容，提升照片美学质量。


<details>
  <summary>Details</summary>
Motivation: 照片中的杂乱内容会分散注意力，影响情感传达，业余摄影师常因经验不足而忽视这一问题。

Method: 结合美学评估算法和生成对抗网络的图像修复技术，提供交互式杂乱识别与去除工具。

Result: 用户研究表明，系统能帮助用户更快识别杂乱并拍摄更高质量的照片。

Conclusion: 该系统通过灵活界面和精准算法，有效提升了用户的摄影体验和照片质量。

Abstract: Clutter in photos is a distraction preventing photographers from conveying the intended emotions or stories to the audience. Photography amateurs frequently include clutter in their photos due to unconscious negligence or the lack of experience in creating a decluttered, aesthetically appealing scene for shooting. We are thus motivated to develop a camera guidance system that provides solutions and guidance for clutter identification and removal. We estimate and visualize the contribution of objects to the overall aesthetics and content of a photo, based on which users can interactively identify clutter. Suggestions on getting rid of clutter, as well as a tool that removes cluttered objects computationally, are provided to guide users to deal with different kinds of clutter and improve their photographic work. Two technical novelties underpin interactions in our system: a clutter distinguishment algorithm with aesthetics evaluations for objects and an iterative image inpainting algorithm based on generative adversarial nets that reconstructs missing regions of removed objects for high-resolution images. User studies demonstrate that our system provides flexible interfaces and accurate algorithms that allow users to better identify distractions and take higher quality images within less time.

</details>


### [14] [Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation](https://arxiv.org/abs/2507.14575)
*Andrea Moschetto,Lemuel Puglisi,Alec Sargood,Pierluigi Dell'Acqua,Francesco Guarnera,Sebastiano Battiato,Daniele Ravì*

Main category: cs.CV

TL;DR: 本文比较了GAN、扩散模型和流匹配技术在MRI图像合成中的表现，发现Pix2Pix在结构保真度和计算效率上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 减少MRI扫描时间和成本，通过计算合成缺失的模态。

Method: 使用GAN、扩散模型和流匹配技术进行T1w到T2w的2D MRI图像合成，并在三个公开数据集上评估。

Result: Pix2Pix在结构保真度、图像质量和计算效率上表现最佳。

Conclusion: GAN在小型数据集上表现更优，为实际MRI工作流程提供了实用指导。

Abstract: Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering distinct diagnostic insights. However, acquiring all desired modalities increases scan time and cost, motivating research into computational methods for cross-modal synthesis. To address this, recent approaches aim to synthesize missing MRI contrasts from those already acquired, reducing acquisition time while preserving diagnostic quality. Image-to-image (I2I) translation provides a promising framework for this task. In this paper, we present a comprehensive benchmark of generative models$\unicode{x2013}$specifically, Generative Adversarial Networks (GANs), diffusion models, and flow matching (FM) techniques$\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All frameworks are implemented with comparable settings and evaluated on three publicly available MRI datasets of healthy adults. Our quantitative and qualitative analyses show that the GAN-based Pix2Pix model outperforms diffusion and FM-based methods in terms of structural fidelity, image quality, and computational efficiency. Consistent with existing literature, these results suggest that flow-based models are prone to overfitting on small datasets and simpler tasks, and may require more data to match or surpass GAN performance. These findings offer practical guidance for deploying I2I translation techniques in real-world MRI workflows and highlight promising directions for future research in cross-modal medical image synthesis. Code and models are publicly available at https://github.com/AndreaMoschetto/medical-I2I-benchmark.

</details>


### [15] [DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF](https://arxiv.org/abs/2507.14596)
*Doriand Petit,Steve Bourgeois,Vincent Gay-Bellile,Florian Chabot,Loïc Barthe*

Main category: cs.CV

TL;DR: DiSCO-3D是一种结合无监督分割和开放词汇指导的3D开放词汇子概念发现方法，在开放词汇和无监督分割的边缘案例中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅适应特定任务目标或场景内容，无法同时满足场景和用户查询的需求。DiSCO-3D旨在填补这一空白。

Method: 基于神经场表示，结合无监督分割和弱开放词汇指导。

Result: 在开放词汇子概念发现中表现优异，并在开放词汇和无监督分割的边缘案例中达到最先进水平。

Conclusion: DiSCO-3D为3D语义分割提供了一种灵活且高效的方法，能够同时适应场景和用户查询。

Abstract: 3D semantic segmentation provides high-level scene understanding for applications in robotics, autonomous systems, \textit{etc}. Traditional methods adapt exclusively to either task-specific goals (open-vocabulary segmentation) or scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the first method addressing the broader problem of 3D Open-Vocabulary Sub-concepts Discovery, which aims to provide a 3D semantic segmentation that adapts to both the scene and user queries. We build DiSCO-3D on Neural Fields representations, combining unsupervised segmentation with weak open-vocabulary guidance. Our evaluations demonstrate that DiSCO-3D achieves effective performance in Open-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in the edge cases of both open-vocabulary and unsupervised segmentation.

</details>


### [16] [BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM](https://arxiv.org/abs/2507.14632)
*Haiquan Wen,Tianxiao Li,Zhenglin Huang,Yiwei He,Guangliang Cheng*

Main category: cs.CV

TL;DR: BusterX++是一个新型跨模态框架，用于检测和解释合成媒体，通过强化学习后训练策略和多阶段训练提升性能。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的进步增加了虚假信息的风险，现有单模态检测方法无法应对多模态合成内容。

Method: 采用强化学习后训练策略，结合多阶段训练、思维奖励和混合推理。

Result: 实验证明BusterX++在性能上显著提升，并具有泛化能力。

Conclusion: BusterX++为跨模态合成媒体检测提供了有效解决方案，并引入高质量基准GenBuster++。

Abstract: Recent advances in generative AI have dramatically improved image and video synthesis capabilities, significantly increasing the risk of misinformation through sophisticated fake content. In response, detection methods have evolved from traditional approaches to multimodal large language models (MLLMs), offering enhanced transparency and interpretability in identifying synthetic media. However, current detection systems remain fundamentally limited by their single-modality design. These approaches analyze images or videos separately, making them ineffective against synthetic content that combines multiple media formats. To address these challenges, we introduce \textbf{BusterX++}, a novel framework designed specifically for cross-modal detection and explanation of synthetic media. Our approach incorporates an advanced reinforcement learning (RL) post-training strategy that eliminates cold-start. Through Multi-stage Training, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and substantial performance improvements. To enable comprehensive evaluation, we also present \textbf{GenBuster++}, a cross-modal benchmark leveraging state-of-the-art image and video generation techniques. This benchmark comprises 4,000 images and video clips, meticulously curated by human experts using a novel filtering methodology to ensure high quality, diversity, and real-world applicability. Extensive experiments demonstrate the effectiveness and generalizability of our approach.

</details>


### [17] [Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models](https://arxiv.org/abs/2507.14797)
*Beier Zhu,Ruoyu Wang,Tong Zhao,Hanwang Zhang,Chi Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为EPD的新型ODE求解器，通过并行梯度评估减少截断误差，实现高质量低延迟采样。


<details>
  <summary>Details</summary>
Motivation: 扩散模型因顺序去噪导致采样延迟高，现有加速方法在低延迟下图像质量下降。

Method: EPD通过并行梯度评估减少截断误差，可完全并行化，且通过蒸馏方式优化少量可学习参数。

Result: 在多个图像合成基准测试中，EPD在5 NFE延迟下表现优异，FID显著优于现有方法。

Conclusion: EPD是一种高效且可扩展的ODE求解器，显著提升了扩散模型的采样效率和质量。

Abstract: Diffusion models (DMs) have achieved state-of-the-art generative performance but suffer from high sampling latency due to their sequential denoising nature. Existing solver-based acceleration methods often face image quality degradation under a low-latency budget. In this paper, we propose the Ensemble Parallel Direction solver (dubbed as \ours), a novel ODE solver that mitigates truncation errors by incorporating multiple parallel gradient evaluations in each ODE step. Importantly, since the additional gradient computations are independent, they can be fully parallelized, preserving low-latency sampling.   Our method optimizes a small set of learnable parameters in a distillation fashion, ensuring minimal training overhead.   In addition, our method can serve as a plugin to improve existing ODE samplers. Extensive experiments on various image synthesis benchmarks demonstrate the effectiveness of our \ours~in achieving high-quality and low-latency sampling. For example, at the same latency level of 5 NFE, EPD achieves an FID of 4.47 on CIFAR-10, 7.97 on FFHQ, 8.17 on ImageNet, and 8.26 on LSUN Bedroom, surpassing existing learning-based solvers by a significant margin. Codes are available in https://github.com/BeierZhu/EPD.

</details>


### [18] [Exploring Scalable Unified Modeling for General Low-Level Vision](https://arxiv.org/abs/2507.14801)
*Xiangyu Chen,Kaiwen Zhu,Yuandong Pu,Shuo Cao,Xiaohui Li,Wenlong Zhang,Yihao Liu,Yu Qiao,Jiantao Zhou,Chao Dong*

Main category: cs.CV

TL;DR: 提出了一种基于视觉提示的统一低层视觉任务处理框架VPIP，通过输入-目标图像对作为提示，支持多种任务。开发了统一模型GenLV，并在大规模任务上验证了其性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决低层视觉任务多样性带来的统一建模挑战，探索多任务联合训练的潜力。

Method: 设计了包含图像处理主干、提示编码器和提示交互模块的VPIP框架，支持灵活架构集成和任务特定表示利用。

Result: 在多任务基准上表现优异，增加训练任务数量提升了泛化能力，尤其在数据有限的任务上。

Conclusion: VPIP框架有效、可扩展，为零样本泛化、少样本迁移和任务特定微调提供了统一基础。

Abstract: Low-level vision involves a wide spectrum of tasks, including image restoration, enhancement, stylization, and feature extraction, which differ significantly in both task formulation and output domains. To address the challenge of unified modeling across such diverse tasks, we propose a Visual task Prompt-based Image Processing (VPIP) framework that leverages input-target image pairs as visual prompts to guide the model in performing a variety of low-level vision tasks. The framework comprises an end-to-end image processing backbone, a prompt encoder, and a prompt interaction module, enabling flexible integration with various architectures and effective utilization of task-specific visual representations. Based on this design, we develop a unified low-level vision model, GenLV, and evaluate its performance across multiple representative tasks. To explore the scalability of this approach, we extend the framework along two dimensions: model capacity and task diversity. We construct a large-scale benchmark consisting of over 100 low-level vision tasks and train multiple versions of the model with varying scales. Experimental results show that the proposed method achieves considerable performance across a wide range of tasks. Notably, increasing the number of training tasks enhances generalization, particularly for tasks with limited data, indicating the model's ability to learn transferable representations through joint training. Further evaluations in zero-shot generalization, few-shot transfer, and task-specific fine-tuning scenarios demonstrate the model's strong adaptability, confirming the effectiveness, scalability, and potential of the proposed framework as a unified foundation for general low-level vision modeling.

</details>


### [19] [Light Future: Multimodal Action Frame Prediction via InstructPix2Pix](https://arxiv.org/abs/2507.14809)
*Zesen Zhong,Duomin Zhang,Yijia Li*

Main category: cs.CV

TL;DR: 提出了一种轻量级的机器人动作预测方法，利用InstructPix2Pix模型进行多模态未来帧预测，显著降低了计算成本和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 预测未来运动轨迹在机器人、自主系统和人类活动预测等领域至关重要，有助于更安全和智能的决策。

Method: 通过改进InstructPix2Pix模型，使其能够接受视觉和文本输入，实现多模态未来帧预测。仅需单张图像和文本提示即可预测未来10秒的视觉帧。

Result: 在RoboTWin数据集上，该方法在SSIM和PSNR指标上优于现有基线模型，且计算成本更低。

Conclusion: 该方法在机器人动作预测任务中表现出高效性和灵活性，特别适用于对运动轨迹精度要求高的场景。

Abstract: Predicting future motion trajectories is a critical capability across domains such as robotics, autonomous systems, and human activity forecasting, enabling safer and more intelligent decision-making. This paper proposes a novel, efficient, and lightweight approach for robot action prediction, offering significantly reduced computational cost and inference latency compared to conventional video prediction models. Importantly, it pioneers the adaptation of the InstructPix2Pix model for forecasting future visual frames in robotic tasks, extending its utility beyond static image editing. We implement a deep learning-based visual prediction framework that forecasts what a robot will observe 100 frames (10 seconds) into the future, given a current image and a textual instruction. We repurpose and fine-tune the InstructPix2Pix model to accept both visual and textual inputs, enabling multimodal future frame prediction. Experiments on the RoboTWin dataset (generated based on real-world scenarios) demonstrate that our method achieves superior SSIM and PSNR compared to state-of-the-art baselines in robot action prediction tasks. Unlike conventional video prediction models that require multiple input frames, heavy computation, and slow inference latency, our approach only needs a single image and a text prompt as input. This lightweight design enables faster inference, reduced GPU demands, and flexible multimodal control, particularly valuable for applications like robotics and sports motion trajectory analytics, where motion trajectory precision is prioritized over visual fidelity.

</details>


### [20] [PHATNet: A Physics-guided Haze Transfer Network for Domain-adaptive Real-world Image Dehazing](https://arxiv.org/abs/2507.14826)
*Fu-Jen Tsai,Yan-Tsung Peng,Yen-Yu Lin,Chia-Wen Lin*

Main category: cs.CV

TL;DR: 提出了一种基于物理引导的雾霾转移网络（PHATNet），通过将目标域的雾霾模式转移到源域的无雾图像上，生成特定领域的微调数据集，从而提升去雾模型的域适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有去雾模型在未见过的真实雾霾图像上性能下降明显，主要由于训练数据有限。

Method: 提出PHATNet，通过雾霾转移生成微调数据集，并引入雾霾转移一致性和内容泄漏损失以增强解耦能力。

Result: 实验结果表明，PHATNet显著提升了现有去雾模型在真实图像数据集上的性能。

Conclusion: PHATNet通过域适应方法有效解决了去雾模型在未见数据上的性能下降问题。

Abstract: Image dehazing aims to remove unwanted hazy artifacts in images. Although previous research has collected paired real-world hazy and haze-free images to improve dehazing models' performance in real-world scenarios, these models often experience significant performance drops when handling unseen real-world hazy images due to limited training data. This issue motivates us to develop a flexible domain adaptation method to enhance dehazing performance during testing. Observing that predicting haze patterns is generally easier than recovering clean content, we propose the Physics-guided Haze Transfer Network (PHATNet) which transfers haze patterns from unseen target domains to source-domain haze-free images, creating domain-specific fine-tuning sets to update dehazing models for effective domain adaptation. Additionally, we introduce a Haze-Transfer-Consistency loss and a Content-Leakage Loss to enhance PHATNet's disentanglement ability. Experimental results demonstrate that PHATNet significantly boosts state-of-the-art dehazing models on benchmark real-world image dehazing datasets.

</details>


### [21] [Paired Image Generation with Diffusion-Guided Diffusion Models](https://arxiv.org/abs/2507.14833)
*Haoxuan Zhang,Wenju Cui,Yuzhu Cao,Tao Tan,Jie Liu,Yunsong Peng,Jian Zheng*

Main category: cs.CV

TL;DR: 提出了一种无需外部条件的配对图像生成方法，用于解决乳腺断层扫描图像中病灶分割的数据标注不足问题，并提升生成质量和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 乳腺断层扫描图像中病灶的高隐蔽性导致标注困难，现有扩散模型生成质量低且无法生成标注，限制了监督训练的效果。

Method: 通过训练额外的扩散引导器，实现条件扩散模型的配对图像生成，生成乳腺断层扫描切片和病灶掩码。

Result: 实验表明，该方法无需外部条件即可提升生成质量，并缓解标注数据短缺问题，从而提升下游任务性能。

Conclusion: 提出的配对图像生成方法有效解决了数据标注不足和生成质量低的问题，对乳腺病灶分割任务有显著帮助。

Abstract: The segmentation of mass lesions in digital breast tomosynthesis (DBT) images is very significant for the early screening of breast cancer. However, the high-density breast tissue often leads to high concealment of the mass lesions, which makes manual annotation difficult and time-consuming. As a result, there is a lack of annotated data for model training. Diffusion models are commonly used for data augmentation, but the existing methods face two challenges. First, due to the high concealment of lesions, it is difficult for the model to learn the features of the lesion area. This leads to the low generation quality of the lesion areas, thus limiting the quality of the generated images. Second, existing methods can only generate images and cannot generate corresponding annotations, which restricts the usability of the generated images in supervised training. In this work, we propose a paired image generation method. The method does not require external conditions and can achieve the generation of paired images by training an extra diffusion guider for the conditional diffusion model. During the experimental phase, we generated paired DBT slices and mass lesion masks. Then, we incorporated them into the supervised training process of the mass lesion segmentation task. The experimental results show that our method can improve the generation quality without external conditions. Moreover, it contributes to alleviating the shortage of annotated data, thus enhancing the performance of downstream tasks.

</details>


### [22] [Grounding Degradations in Natural Language for All-In-One Video Restoration](https://arxiv.org/abs/2507.14851)
*Muhammad Kamran Janjua,Amirhosein Ghasemabadi,Kunlin Zhang,Mohammad Salameh,Chao Gao,Di Niu*

Main category: cs.CV

TL;DR: 提出了一种基于自然语言的多任务视频修复框架，无需先验退化知识，通过基础模型提供可解释的指导，并在多个基准测试中取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频修复方法通常需要退化知识，限制了灵活性和可解释性。本文旨在通过自然语言和基础模型，实现无需退化知识的视频修复。

Method: 利用基础模型将视频帧的退化感知语义上下文嵌入自然语言，学习近似知识以在推理时无需额外成本。提出标准化基准，包括多退化任务和时间变化复合退化数据集。

Result: 在多个基准测试中（包括新提出的雪强度变化数据集），方法性能优于现有技术。

Conclusion: 该框架为视频修复提供了一种灵活、可解释且无需退化知识的解决方案，同时呼吁标准化基准的建立。

Abstract: In this work, we propose an all-in-one video restoration framework that grounds degradation-aware semantic context of video frames in natural language via foundation models, offering interpretable and flexible guidance. Unlike prior art, our method assumes no degradation knowledge in train or test time and learns an approximation to the grounded knowledge such that the foundation model can be safely disentangled during inference adding no extra cost. Further, we call for standardization of benchmarks in all-in-one video restoration, and propose two benchmarks in multi-degradation setting, three-task (3D) and four-task (4D), and two time-varying composite degradation benchmarks; one of the latter being our proposed dataset with varying snow intensity, simulating how weather degradations affect videos naturally. We compare our method with prior works and report state-of-the-art performance on all benchmarks.

</details>


### [23] [Hybrid-supervised Hypergraph-enhanced Transformer for Micro-gesture Based Emotion Recognition](https://arxiv.org/abs/2507.14867)
*Zhaoqiang Xia,Hexiang Huang,Haoyu Chen,Xiaoyi Feng,Guoying Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种基于微手势的情绪识别方法，通过超图增强的Transformer在混合监督框架中重建行为模式，显著提升了情绪识别的性能。


<details>
  <summary>Details</summary>
Motivation: 微手势作为无意识的身体动作，能够传达人类情绪状态，但在情绪建模方面尚未充分探索。

Method: 采用超图增强的Transformer编码器和解码器，结合自监督和监督学习，设计了多尺度时间卷积模块和超图增强自注意力模块。

Result: 在两个公开数据集（iMiGUE和SMG）上表现最佳，优于现有方法。

Conclusion: 该方法通过混合监督框架和超图增强的Transformer，有效捕捉微手势的细微运动，提升了情绪识别的准确性。

Abstract: Micro-gestures are unconsciously performed body gestures that can convey the emotion states of humans and start to attract more research attention in the fields of human behavior understanding and affective computing as an emerging topic. However, the modeling of human emotion based on micro-gestures has not been explored sufficiently. In this work, we propose to recognize the emotion states based on the micro-gestures by reconstructing the behavior patterns with a hypergraph-enhanced Transformer in a hybrid-supervised framework. In the framework, hypergraph Transformer based encoder and decoder are separately designed by stacking the hypergraph-enhanced self-attention and multiscale temporal convolution modules. Especially, to better capture the subtle motion of micro-gestures, we construct a decoder with additional upsampling operations for a reconstruction task in a self-supervised learning manner. We further propose a hypergraph-enhanced self-attention module where the hyperedges between skeleton joints are gradually updated to present the relationships of body joints for modeling the subtle local motion. Lastly, for exploiting the relationship between the emotion states and local motion of micro-gestures, an emotion recognition head from the output of encoder is designed with a shallow architecture and learned in a supervised way. The end-to-end framework is jointly trained in a one-stage way by comprehensively utilizing self-reconstruction and supervision information. The proposed method is evaluated on two publicly available datasets, namely iMiGUE and SMG, and achieves the best performance under multiple metrics, which is superior to the existing methods.

</details>


### [24] [Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian Splatting Reconstruction](https://arxiv.org/abs/2507.14921)
*Xiufeng Huang,Ka Chun Cheung,Runmin Cong,Simon See,Renjie Wan*

Main category: cs.CV

TL;DR: 提出了一种解耦框架\method，用于高效预测3D高斯，通过立体视觉骨干提取特征并融合，生成几何和外观的高斯特征，实现无姿态的高质量3D重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D高斯几何和外观预测上存在耦合，依赖数据驱动先验且回归速度慢，需解决计算资源和大数据集需求问题。

Method: 使用立体视觉骨干提取局部图像对特征，通过全局注意力块融合，生成多视点几何点图和外观高斯特征，结合为GS图表示3DGS对象，并通过细化网络提升重建质量。

Result: 实现了无姿态的3D重建，提高了鲁棒性和实用性，同时减少资源需求并保持高质量输出。

Conclusion: \method为现实世界的3D内容生成提供了高效、可扩展的解决方案。

Abstract: Generalizable 3D Gaussian Splatting reconstruction showcases advanced Image-to-3D content creation but requires substantial computational resources and large datasets, posing challenges to training models from scratch. Current methods usually entangle the prediction of 3D Gaussian geometry and appearance, which rely heavily on data-driven priors and result in slow regression speeds. To address this, we propose \method, a disentangled framework for efficient 3D Gaussian prediction. Our method extracts features from local image pairs using a stereo vision backbone and fuses them via global attention blocks. Dedicated point and Gaussian prediction heads generate multi-view point-maps for geometry and Gaussian features for appearance, combined as GS-maps to represent the 3DGS object. A refinement network enhances these GS-maps for high-quality reconstruction. Unlike existing methods that depend on camera parameters, our approach achieves pose-free 3D reconstruction, improving robustness and practicality. By reducing resource demands while maintaining high-quality outputs, \method provides an efficient, scalable solution for real-world 3D content generation.

</details>


### [25] [Polymorph: Energy-Efficient Multi-Label Classification for Video Streams on Embedded Devices](https://arxiv.org/abs/2507.14959)
*Saeid Ghafouri,Mohsen Fayyaz,Xiangchen Li,Deepu John,Bo Ji,Dimitrios Nikolopoulos,Hans Vandierendonck*

Main category: cs.CV

TL;DR: Polymorph是一种上下文感知框架，通过动态激活轻量级低秩适配器（LoRA）实现嵌入式设备上的实时多标签视频分类，显著降低能耗并提升性能。


<details>
  <summary>Details</summary>
Motivation: 嵌入式设备在实时多标签视频分类中受限于计算和能源预算，但视频流具有标签稀疏性、时间连续性和标签共现性等结构特性，可被利用以提高效率。

Method: Polymorph框架动态选择和组合轻量级LoRA适配器，每个适配器专注于基于共现模式的子类，避免全模型切换和权重合并。

Result: 在TAO数据集上，Polymorph能耗降低40%，mAP提升9个百分点。

Conclusion: Polymorph通过模块化策略显著提升了嵌入式设备上视频分类的效率和性能，且已开源。

Abstract: Real-time multi-label video classification on embedded devices is constrained by limited compute and energy budgets. Yet, video streams exhibit structural properties such as label sparsity, temporal continuity, and label co-occurrence that can be leveraged for more efficient inference. We introduce Polymorph, a context-aware framework that activates a minimal set of lightweight Low Rank Adapters (LoRA) per frame. Each adapter specializes in a subset of classes derived from co-occurrence patterns and is implemented as a LoRA weight over a shared backbone. At runtime, Polymorph dynamically selects and composes only the adapters needed to cover the active labels, avoiding full-model switching and weight merging. This modular strategy improves scalability while reducing latency and energy overhead. Polymorph achieves 40% lower energy consumption and improves mAP by 9 points over strong baselines on the TAO dataset. Polymorph is open source at https://github.com/inference-serving/polymorph/.

</details>


### [26] [EBA-AI: Ethics-Guided Bias-Aware AI for Efficient Underwater Image Enhancement and Coral Reef Monitoring](https://arxiv.org/abs/2507.15036)
*Lyes Saad Saoud,Irfan Hussain*

Main category: cs.CV

TL;DR: EBA-AI是一个基于伦理和偏见的AI框架，用于水下图像增强，解决数据集偏见、高计算成本和透明度问题。


<details>
  <summary>Details</summary>
Motivation: 水下图像增强对海洋保护（如珊瑚礁监测）至关重要，但现有AI模型存在数据集偏见、高计算成本和缺乏透明度的问题。

Method: EBA-AI利用CLIP嵌入检测和减轻数据集偏见，结合自适应处理优化能效，减少GPU使用。

Result: 实验显示PSNR下降1.0 dB，但计算效率提升，支持实时大规模监测。不确定性估计和可解释性技术增强信任。

Conclusion: EBA-AI在效率、公平性和可解释性上优于现有方法，为可持续海洋保护提供支持。

Abstract: Underwater image enhancement is vital for marine conservation, particularly coral reef monitoring. However, AI-based enhancement models often face dataset bias, high computational costs, and lack of transparency, leading to potential misinterpretations. This paper introduces EBA-AI, an ethics-guided bias-aware AI framework to address these challenges. EBA-AI leverages CLIP embeddings to detect and mitigate dataset bias, ensuring balanced representation across varied underwater environments. It also integrates adaptive processing to optimize energy efficiency, significantly reducing GPU usage while maintaining competitive enhancement quality. Experiments on LSUI400, Oceanex, and UIEB100 show that while PSNR drops by a controlled 1.0 dB, computational savings enable real-time feasibility for large-scale marine monitoring. Additionally, uncertainty estimation and explainability techniques enhance trust in AI-driven environmental decisions. Comparisons with CycleGAN, FunIEGAN, RAUNENet, WaterNet, UGAN, PUGAN, and UTUIE validate EBA-AI's effectiveness in balancing efficiency, fairness, and interpretability in underwater image processing. By addressing key limitations of AI-driven enhancement, this work contributes to sustainable, bias-aware, and computationally efficient marine conservation efforts. For interactive visualizations, animations, source code, and access to the preprint, visit: https://lyessaadsaoud.github.io/EBA-AI/

</details>


### [27] [StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation](https://arxiv.org/abs/2507.15064)
*Shuyuan Tu,Zhen Xing,Xintong Han,Zhi-Qi Cheng,Qi Dai,Chong Luo,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: StableAnimator++是一种基于视频扩散模型的身份保持框架，通过可学习的姿态对齐和分布感知ID适配器，解决了现有方法在身份一致性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的人类图像动画扩散模型在参考图像和驱动视频差异较大时难以保持身份一致性。

Method: StableAnimator++通过可学习层预测相似变换矩阵，结合SVD引导对齐姿态，并使用图像和面部嵌入增强身份一致性。此外，引入分布感知ID适配器和HJB面部优化。

Result: 实验证明StableAnimator++在定性和定量上均表现出色。

Conclusion: StableAnimator++通过创新的对齐和优化方法，显著提升了身份一致性，适用于高质量视频生成。

Abstract: Current diffusion models for human image animation often struggle to maintain identity (ID) consistency, especially when the reference image and driving video differ significantly in body size or position. We introduce StableAnimator++, the first ID-preserving video diffusion framework with learnable pose alignment, capable of generating high-quality videos conditioned on a reference image and a pose sequence without any post-processing. Building upon a video diffusion model, StableAnimator++ contains carefully designed modules for both training and inference, striving for identity consistency. In particular, StableAnimator++ first uses learnable layers to predict the similarity transformation matrices between the reference image and the driven poses via injecting guidance from Singular Value Decomposition (SVD). These matrices align the driven poses with the reference image, mitigating misalignment to a great extent. StableAnimator++ then computes image and face embeddings using off-the-shelf encoders, refining the face embeddings via a global content-aware Face Encoder. To further maintain ID, we introduce a distribution-aware ID Adapter that counteracts interference caused by temporal layers while preserving ID via distribution alignment. During the inference stage, we propose a novel Hamilton-Jacobi-Bellman (HJB) based face optimization integrated into the denoising process, guiding the diffusion trajectory for enhanced facial fidelity. Experiments on benchmarks show the effectiveness of StableAnimator++ both qualitatively and quantitatively.

</details>


### [28] [Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR](https://arxiv.org/abs/2507.15085)
*Peirong Zhang,Haowei Xu,Jiaxin Zhang,Guitao Xu,Xuhan Zheng,Zhenhua Yang,Junle Liu,Yuyi Zhang,Lianwen Jin*

Main category: cs.CV

TL;DR: 评估当前最先进的生成模型在文本图像生成和编辑方面的能力，提出将逼真文本图像生成和编辑作为通用生成模型的基础技能。


<details>
  <summary>Details</summary>
Motivation: 由于文本图像的复杂性和重要性，研究现有生成模型是否能掌握其生成和编辑的细节。

Method: 选择33个代表性任务，分为五类，评估六种模型在封闭和开源领域的表现。

Result: 识别当前生成模型在OCR任务中的弱点，提出改进方向。

Conclusion: 逼真文本图像生成和编辑应作为通用生成模型的基础技能，而非依赖专用解决方案。

Abstract: Text image is a unique and crucial information medium that integrates visual aesthetics and linguistic semantics in modern e-society. Due to their subtlety and complexity, the generation of text images represents a challenging and evolving frontier in the image generation field. The recent surge of specialized image generators (\emph{e.g.}, Flux-series) and unified generative models (\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a natural question: can they master the intricacies of text image generation and editing? Motivated by this, we assess current state-of-the-art generative models' capabilities in terms of text image generation and editing. We incorporate various typical optical character recognition (OCR) tasks into our evaluation and broaden the concept of text-based generation tasks into OCR generative tasks. We select 33 representative tasks and categorize them into five categories: document, handwritten text, scene text, artistic text, and complex \& layout-rich text. For comprehensive evaluation, we examine six models across both closed-source and open-source domains, using tailored, high-quality image inputs and prompts. Through this evaluation, we draw crucial observations and identify the weaknesses of current generative models for OCR tasks. We argue that photorealistic text image generation and editing should be internalized as foundational skills into general-domain generative models, rather than being delegated to specialized solutions, and we hope this empirical analysis can provide valuable insights for the community to achieve this goal. This evaluation is online and will be continuously updated at our GitHub repository.

</details>


### [29] [MeshMamba: State Space Models for Articulated 3D Mesh Generation and Reconstruction](https://arxiv.org/abs/2507.15212)
*Yusuke Yoshiyasu,Leyuan Sun,Ryusuke Sagawa*

Main category: cs.CV

TL;DR: MeshMamba利用Mamba-SSMs高效处理大规模3D网格数据，生成和重建包含衣物和手部细节的密集人体网格。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在处理大规模3D网格数据时的效率问题，并扩展人体网格生成和重建的能力。

Method: 通过顶点序列化技术将网格顶点排序，利用Mamba-SSMs处理，设计了MambaDiff3D和Mamba-HMR两个模型。

Result: MambaDiff3D在生成3D人体网格任务中表现优异，Mamba-HMR实现了全身网格重建并接近实时性能。

Conclusion: MeshMamba为3D人体网格生成和重建提供了高效且可扩展的解决方案。

Abstract: In this paper, we introduce MeshMamba, a neural network model for learning 3D articulated mesh models by employing the recently proposed Mamba State Space Models (Mamba-SSMs). MeshMamba is efficient and scalable in handling a large number of input tokens, enabling the generation and reconstruction of body mesh models with more than 10,000 vertices, capturing clothing and hand geometries. The key to effectively learning MeshMamba is the serialization technique of mesh vertices into orderings that are easily processed by Mamba. This is achieved by sorting the vertices based on body part annotations or the 3D vertex locations of a template mesh, such that the ordering respects the structure of articulated shapes. Based on MeshMamba, we design 1) MambaDiff3D, a denoising diffusion model for generating 3D articulated meshes and 2) Mamba-HMR, a 3D human mesh recovery model that reconstructs a human body shape and pose from a single image. Experimental results showed that MambaDiff3D can generate dense 3D human meshes in clothes, with grasping hands, etc., and outperforms previous approaches in the 3D human shape generation task. Additionally, Mamba-HMR extends the capabilities of previous non-parametric human mesh recovery approaches, which were limited to handling body-only poses using around 500 vertex tokens, to the whole-body setting with face and hands, while achieving competitive performance in (near) real-time.

</details>


### [30] [Improving Joint Embedding Predictive Architecture with Diffusion Noise](https://arxiv.org/abs/2507.15216)
*Yuping Qiu,Rui Zhu,Ying-cong Chen*

Main category: cs.CV

TL;DR: 论文提出N-JEPA方法，将扩散噪声引入掩码图像建模（MIM），以增强自监督学习（SSL）的表征能力，并在下游分类任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 自监督学习（SSL）在特征学习上表现优异，但生成模型在图像生成和细节增强上更优。论文旨在结合SSL与生成模型的核心思想（如扩散噪声），以提升SSL的表征能力。

Method: 提出N-JEPA方法，通过掩码标记的位置嵌入将扩散噪声引入MIM，并采用多级噪声调度作为特征增强手段。

Result: 实验证实N-JEPA在下游分类任务中表现优异。

Conclusion: 结合扩散噪声与SSL能有效提升模型表征能力，N-JEPA为这一方向提供了可行方案。

Abstract: Self-supervised learning has become an incredibly successful method for feature learning, widely applied to many downstream tasks. It has proven especially effective for discriminative tasks, surpassing the trending generative models. However, generative models perform better in image generation and detail enhancement. Thus, it is natural for us to find a connection between SSL and generative models to further enhance the representation capacity of SSL. As generative models can create new samples by approximating the data distribution, such modeling should also lead to a semantic understanding of the raw visual data, which is necessary for recognition tasks. This enlightens us to combine the core principle of the diffusion model: diffusion noise, with SSL to learn a competitive recognition model. Specifically, diffusion noise can be viewed as a particular state of mask that reveals a close relationship between masked image modeling (MIM) and diffusion models. In this paper, we propose N-JEPA (Noise-based JEPA) to incorporate diffusion noise into MIM by the position embedding of masked tokens. The multi-level noise schedule is a series of feature augmentations to further enhance the robustness of our model. We perform a comprehensive study to confirm its effectiveness in the classification of downstream tasks. Codes will be released soon in public.

</details>


### [31] [FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers](https://arxiv.org/abs/2507.15249)
*Yanbing Zhang,Zhe Wang,Qin Zhou,Mengping Yang*

Main category: cs.CV

TL;DR: FreeCus是一个无需训练的框架，通过创新的注意力共享机制和改进的特征提取方法，激活扩散变换器（DiT）的零样本能力，实现高质量的主题驱动合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖训练过程，限制了实际应用，且未能充分利用现代扩散变换器的零样本潜力。

Method: 提出FreeCus框架，包括注意力共享机制、改进的特征提取方法，并整合多模态大语言模型（MLLMs）。

Result: 实验表明，FreeCus成功解锁了DiT的零样本能力，在多样场景中实现一致的主题合成，性能达到或超越需训练的方法。

Conclusion: FreeCus展示了与现有修复流程和控制模块的无缝兼容性，为设计工作流和娱乐应用提供了更灵活的解决方案。

Abstract: In light of recent breakthroughs in text-to-image (T2I) generation, particularly with diffusion transformers (DiT), subject-driven technologies are increasingly being employed for high-fidelity customized production that preserves subject identity from reference inputs, enabling thrilling design workflows and engaging entertainment. Existing alternatives typically require either per-subject optimization via trainable text embeddings or training specialized encoders for subject feature extraction on large-scale datasets. Such dependencies on training procedures fundamentally constrain their practical applications. More importantly, current methodologies fail to fully leverage the inherent zero-shot potential of modern diffusion transformers (e.g., the Flux series) for authentic subject-driven synthesis. To bridge this gap, we propose FreeCus, a genuinely training-free framework that activates DiT's capabilities through three key innovations: 1) We introduce a pivotal attention sharing mechanism that captures the subject's layout integrity while preserving crucial editing flexibility. 2) Through a straightforward analysis of DiT's dynamic shifting, we propose an upgraded variant that significantly improves fine-grained feature extraction. 3) We further integrate advanced Multimodal Large Language Models (MLLMs) to enrich cross-modal semantic representations. Extensive experiments reflect that our method successfully unlocks DiT's zero-shot ability for consistent subject synthesis across diverse contexts, achieving state-of-the-art or comparable results compared to approaches that require additional training. Notably, our framework demonstrates seamless compatibility with existing inpainting pipelines and control modules, facilitating more compelling experiences. Our code is available at: https://github.com/Monalissaa/FreeCus.

</details>


### [32] [Conditional Video Generation for High-Efficiency Video Compression](https://arxiv.org/abs/2507.15269)
*Fangqiu Yi,Jingyu Xu,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出了一种基于条件扩散模型的视频压缩框架，通过生成模型从稀疏信号中重建视频，显著提升了感知质量。


<details>
  <summary>Details</summary>
Motivation: 利用条件扩散模型在人类视觉感知对齐方面的优势，优化视频压缩的感知质量。

Method: 将视频压缩重新定义为条件生成任务，引入多粒度条件、紧凑表示和多条件训练模块。

Result: 在FVD和LPIPS等感知质量指标上显著优于传统和神经编解码器，尤其在高压缩比下表现突出。

Conclusion: 该方法通过生成模型实现了感知优化的视频压缩，为未来视频编码技术提供了新思路。

Abstract: Perceptual studies demonstrate that conditional diffusion models excel at reconstructing video content aligned with human visual perception. Building on this insight, we propose a video compression framework that leverages conditional diffusion models for perceptually optimized reconstruction. Specifically, we reframe video compression as a conditional generation task, where a generative model synthesizes video from sparse, yet informative signals. Our approach introduces three key modules: (1) Multi-granular conditioning that captures both static scene structure and dynamic spatio-temporal cues; (2) Compact representations designed for efficient transmission without sacrificing semantic richness; (3) Multi-condition training with modality dropout and role-aware embeddings, which prevent over-reliance on any single modality and enhance robustness. Extensive experiments show that our method significantly outperforms both traditional and neural codecs on perceptual quality metrics such as Fr\'echet Video Distance (FVD) and LPIPS, especially under high compression ratios.

</details>


### [33] [RoadFusion: Latent Diffusion Model for Pavement Defect Detection](https://arxiv.org/abs/2507.15346)
*Muhammad Aqeel,Kidus Dagnaw Bellete,Francesco Setti*

Main category: cs.CV

TL;DR: RoadFusion通过合成异常生成和双路径特征适应解决路面缺陷检测中的数据稀缺、领域偏移和缺陷多样性问题。


<details>
  <summary>Details</summary>
Motivation: 路面缺陷检测面临标注数据稀缺、训练与部署环境间的领域偏移以及不同道路条件下缺陷外观的高变异性等挑战。

Method: 提出RoadFusion框架，包括潜在扩散模型生成合成缺陷、双路径特征适应器分别处理正常和异常输入，以及轻量级判别器学习细粒度缺陷模式。

Result: 在六个基准数据集上评估，RoadFusion在分类和定位任务中表现优异，多项指标达到新最优。

Conclusion: RoadFusion通过合成数据和特征适应有效提升了路面缺陷检测的鲁棒性和性能。

Abstract: Pavement defect detection faces critical challenges including limited annotated data, domain shift between training and deployment environments, and high variability in defect appearances across different road conditions. We propose RoadFusion, a framework that addresses these limitations through synthetic anomaly generation with dual-path feature adaptation. A latent diffusion model synthesizes diverse, realistic defects using text prompts and spatial masks, enabling effective training under data scarcity. Two separate feature adaptors specialize representations for normal and anomalous inputs, improving robustness to domain shift and defect variability. A lightweight discriminator learns to distinguish fine-grained defect patterns at the patch level. Evaluated on six benchmark datasets, RoadFusion achieves consistently strong performance across both classification and localization tasks, setting new state-of-the-art in multiple metrics relevant to real-world road inspection.

</details>


### [34] [SAIGFormer: A Spatially-Adaptive Illumination-Guided Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.15520)
*Hanting Li,Fei Zhou,Xin Sun,Yang Hua,Jungong Han,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: SAIGFormer是一种基于Transformer的低光增强方法，通过动态积分图像表示和光照引导的多头自注意力机制，有效解决了非均匀光照场景下的问题。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer方法在非均匀光照场景（如背光和阴影）中表现不佳，导致过曝或亮度恢复不足。

Method: 提出动态积分图像表示建模空间变化光照，构建SAI2E估计器，并引入光照引导的多头自注意力机制（IG-MSA）。

Result: 在五个标准低光数据集和跨域基准（LOL-Blur）上表现优异，定量和定性指标均超越现有方法。

Conclusion: SAIGFormer在非均匀光照增强中表现卓越，并具有强大的跨数据集泛化能力。

Abstract: Recent Transformer-based low-light enhancement methods have made promising progress in recovering global illumination. However, they still struggle with non-uniform lighting scenarios, such as backlit and shadow, appearing as over-exposure or inadequate brightness restoration. To address this challenge, we present a Spatially-Adaptive Illumination-Guided Transformer (SAIGFormer) framework that enables accurate illumination restoration. Specifically, we propose a dynamic integral image representation to model the spatially-varying illumination, and further construct a novel Spatially-Adaptive Integral Illumination Estimator ($\text{SAI}^2\text{E}$). Moreover, we introduce an Illumination-Guided Multi-head Self-Attention (IG-MSA) mechanism, which leverages the illumination to calibrate the lightness-relevant features toward visual-pleased illumination enhancement. Extensive experiments on five standard low-light datasets and a cross-domain benchmark (LOL-Blur) demonstrate that our SAIGFormer significantly outperforms state-of-the-art methods in both quantitative and qualitative metrics. In particular, our method achieves superior performance in non-uniform illumination enhancement while exhibiting strong generalization capabilities across multiple datasets. Code is available at https://github.com/LHTcode/SAIGFormer.git.

</details>


### [35] [GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation](https://arxiv.org/abs/2507.15577)
*Hugo Carlesso,Maria Eliza Patulea,Moncef Garouani,Radu Tudor Ionescu,Josiane Mothe*

Main category: cs.CV

TL;DR: GeMix是一种基于条件GAN的两阶段图像增强框架，通过软标签合成视觉连贯的图像，优于传统mixup方法。


<details>
  <summary>Details</summary>
Motivation: 传统mixup的像素级插值生成不真实图像，可能影响学习效果，尤其在医学等高风险应用中。

Method: 使用StyleGAN2-ADA生成器，通过Dirichlet和Beta分布采样标签向量，生成连续类流形上的图像。

Result: 在COVIDx-CT-3数据集上，GeMix结合真实数据提升了所有骨干网络的macro-F1，降低了COVID-19检测的假阴性率。

Conclusion: GeMix是传统mixup的直接替代方案，提供更强的正则化和语义保真度，且不破坏现有训练流程。

Abstract: Mixup has become a popular augmentation strategy for image classification, yet its naive pixel-wise interpolation often produces unrealistic images that can hinder learning, particularly in high-stakes medical applications. We propose GeMix, a two-stage framework that replaces heuristic blending with a learned, label-aware interpolation powered by class-conditional GANs. First, a StyleGAN2-ADA generator is trained on the target dataset. During augmentation, we sample two label vectors from Dirichlet priors biased toward different classes and blend them via a Beta-distributed coefficient. Then, we condition the generator on this soft label to synthesize visually coherent images that lie along a continuous class manifold. We benchmark GeMix on the large-scale COVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101, EfficientNet-B0). When combined with real data, our method increases macro-F1 over traditional mixup for all backbones, reducing the false negative rate for COVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup, delivering stronger regularization and greater semantic fidelity, without disrupting existing training pipelines. We publicly release our code at https://github.com/hugocarlesso/GeMix to foster reproducibility and further research.

</details>


### [36] [SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting](https://arxiv.org/abs/2507.15602)
*Zihui Gao,Jia-Wang Bian,Guosheng Lin,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 提出了一种结合SDF和3DGS的混合方法，用于稀疏视图图像中的表面重建和新视角渲染，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图图像中的表面重建和新视角渲染存在挑战，SDF方法细节不足，3DGS方法缺乏全局几何一致性。

Method: 结合SDF的粗几何捕获和3DGS的细节渲染，互相优化。

Result: 在DTU和MobileBrick数据集上，表面重建和新视角合成表现优于现有方法。

Conclusion: 混合方法有效结合了SDF和3DGS的优势，提升了性能。

Abstract: Surface reconstruction and novel view rendering from sparse-view images are challenging. Signed Distance Function (SDF)-based methods struggle with fine details, while 3D Gaussian Splatting (3DGS)-based approaches lack global geometry coherence. We propose a novel hybrid method that combines the strengths of both approaches: SDF captures coarse geometry to enhance 3DGS-based rendering, while newly rendered images from 3DGS refine the details of SDF for accurate surface reconstruction. As a result, our method surpasses state-of-the-art approaches in surface reconstruction and novel view synthesis on the DTU and MobileBrick datasets. Code will be released at https://github.com/Gaozihui/SurfaceSplat.

</details>


### [37] [CylinderPlane: Nested Cylinder Representation for 3D-aware Image Generation](https://arxiv.org/abs/2507.15606)
*Ru Jia,Xiaozhuang Ma,Jianji Wang,Nanning Zheng*

Main category: cs.CV

TL;DR: 论文提出了一种基于圆柱坐标系的隐式表示方法CylinderPlane，解决了Tri-plane表示中的多视角一致性问题，实现了高质量、无伪影的360°图像合成。


<details>
  <summary>Details</summary>
Motivation: Tri-plane表示在3D感知图像生成中存在对称区域特征共享导致的多视角伪影问题，限制了360°视图图像的生成能力。

Method: 提出CylinderPlane，基于圆柱坐标系分离不同角度的特征，并引入嵌套圆柱表示以处理复杂几何和多分辨率需求。

Result: 实验表明，该方法在合成数据集和真实图像上均优于现有方法，实现了高质量的多视角一致图像生成。

Conclusion: CylinderPlane通过圆柱坐标系有效解决了特征模糊问题，提升了360°图像合成的质量和鲁棒性。

Abstract: While the proposal of the Tri-plane representation has advanced the development of the 3D-aware image generative models, problems rooted in its inherent structure, such as multi-face artifacts caused by sharing the same features in symmetric regions, limit its ability to generate 360$^\circ$ view images. In this paper, we propose CylinderPlane, a novel implicit representation based on Cylindrical Coordinate System, to eliminate the feature ambiguity issue and ensure multi-view consistency in 360$^\circ$. Different from the inevitable feature entanglement in Cartesian coordinate-based Tri-plane representation, the cylindrical coordinate system explicitly separates features at different angles, allowing our cylindrical representation possible to achieve high-quality, artifacts-free 360$^\circ$ image synthesis. We further introduce the nested cylinder representation that composites multiple cylinders at different scales, thereby enabling the model more adaptable to complex geometry and varying resolutions. The combination of cylinders with different resolutions can effectively capture more critical locations and multi-scale features, greatly facilitates fine detail learning and robustness to different resolutions. Moreover, our representation is agnostic to implicit rendering methods and can be easily integrated into any neural rendering pipeline. Extensive experiments on both synthetic dataset and unstructured in-the-wild images demonstrate that our proposed representation achieves superior performance over previous methods.

</details>


### [38] [Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization for Remote Sensing](https://arxiv.org/abs/2507.15683)
*Boni Hu,Zhenyu Xia,Lin Chen,Pengcheng Han,Shuhui Bu*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Visual relocalization, which estimates the 6-degree-of-freedom (6-DoF) camera pose from query images, is fundamental to remote sensing and UAV applications. Existing methods face inherent trade-offs: image-based retrieval and pose regression approaches lack precision, while structure-based methods that register queries to Structure-from-Motion (SfM) models suffer from computational complexity and limited scalability. These challenges are particularly pronounced in remote sensing scenarios due to large-scale scenes, high altitude variations, and domain gaps of existing visual priors. To overcome these limitations, we leverage 3D Gaussian Splatting (3DGS) as a novel scene representation that compactly encodes both 3D geometry and appearance. We introduce $\mathrm{Hi}^2$-GSLoc, a dual-hierarchical relocalization framework that follows a sparse-to-dense and coarse-to-fine paradigm, fully exploiting the rich semantic information and geometric constraints inherent in Gaussian primitives. To handle large-scale remote sensing scenarios, we incorporate partitioned Gaussian training, GPU-accelerated parallel matching, and dynamic memory management strategies. Our approach consists of two stages: (1) a sparse stage featuring a Gaussian-specific consistent render-aware sampling strategy and landmark-guided detector for robust and accurate initial pose estimation, and (2) a dense stage that iteratively refines poses through coarse-to-fine dense rasterization matching while incorporating reliability verification. Through comprehensive evaluation on simulation data, public datasets, and real flight experiments, we demonstrate that our method delivers competitive localization accuracy, recall rate, and computational efficiency while effectively filtering unreliable pose estimates. The results confirm the effectiveness of our approach for practical remote sensing applications.

</details>


### [39] [DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting](https://arxiv.org/abs/2507.15690)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: DWTGS提出了一种基于小波变换的稀疏视图3D高斯重建方法，通过低频监督和高频稀疏化提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图3D高斯重建容易过拟合高频细节，传统傅里叶变换方法参数调优困难且偏向有害高频学习。

Method: 利用小波空间损失提供额外空间监督，仅监督低频LL子带，同时对高频HH子带进行自监督稀疏化。

Result: 实验表明DWTGS在多个基准测试中优于傅里叶变换方法，低频策略提高了泛化能力并减少高频幻觉。

Conclusion: DWTGS通过小波变换的低频监督策略有效解决了稀疏视图重建中的高频过拟合问题。

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in reconstructing high-quality novel views, as it often overfits to the widely-varying high-frequency (HF) details of the sparse training views. While frequency regularization can be a promising approach, its typical reliance on Fourier transforms causes difficult parameter tuning and biases towards detrimental HF learning. We propose DWTGS, a framework that rethinks frequency regularization by leveraging wavelet-space losses that provide additional spatial supervision. Specifically, we supervise only the low-frequency (LF) LL subbands at multiple DWT levels, while enforcing sparsity on the HF HH subband in a self-supervised manner. Experiments across benchmarks show that DWTGS consistently outperforms Fourier-based counterparts, as this LF-centric strategy improves generalization and reduces HF hallucinations.

</details>


### [40] [A Practical Investigation of Spatially-Controlled Image Generation with Transformers](https://arxiv.org/abs/2507.15724)
*Guoxuan Xia,Harleen Hanspal,Petru-Daniel Tudosiu,Shifeng Zhang,Sarah Parisot*

Main category: cs.CV

TL;DR: 论文探讨了空间控制图像生成模型的改进方法，通过对比实验澄清了不同生成范式的优缺点，并提出了一些性能提升的技术。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决现有空间控制图像生成模型在科学比较上的不足，澄清文献中的知识空白，为实践者提供清晰的指导。

Method: 在ImageNet上进行了扩散模型、流模型和自回归模型的对比实验，提出了控制标记预填充基线方法，并探索了采样时间增强技术。

Result: 控制标记预填充表现优异，分类器自由引导和softmax截断显著提升控制与生成的一致性，适配器方法在有限数据下保持生成质量但一致性较差。

Conclusion: 论文为基于Transformer的空间控制生成系统提供了实用见解，澄清了技术细节并填补了知识空白。

Abstract: Enabling image generation models to be spatially controlled is an important area of research, empowering users to better generate images according to their own fine-grained specifications via e.g. edge maps, poses. Although this task has seen impressive improvements in recent times, a focus on rapidly producing stronger models has come at the cost of detailed and fair scientific comparison. Differing training data, model architectures and generation paradigms make it difficult to disentangle the factors contributing to performance. Meanwhile, the motivations and nuances of certain approaches become lost in the literature. In this work, we aim to provide clear takeaways across generation paradigms for practitioners wishing to develop transformer-based systems for spatially-controlled generation, clarifying the literature and addressing knowledge gaps. We perform controlled experiments on ImageNet across diffusion-based/flow-based and autoregressive (AR) models. First, we establish control token prefilling as a simple, general and performant baseline approach for transformers. We then investigate previously underexplored sampling time enhancements, showing that extending classifier-free guidance to control, as well as softmax truncation, have a strong impact on control-generation consistency. Finally, we re-clarify the motivation of adapter-based approaches, demonstrating that they mitigate "forgetting" and maintain generation quality when trained on limited downstream data, but underperform full training in terms of generation-control consistency. Code will be released upon publication.

</details>


### [41] [TokensGen: Harnessing Condensed Tokens for Long Video Generation](https://arxiv.org/abs/2507.15728)
*Wenqi Ouyang,Zeqi Xiao,Danni Yang,Yifan Zhou,Shuai Yang,Lei Yang,Jianlou Si,Xingang Pan*

Main category: cs.CV

TL;DR: TokensGen是一个两阶段框架，通过使用压缩的token解决长视频生成中的内存和一致性挑战。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成短视频时表现良好，但扩展到长视频时面临内存瓶颈和长期不一致性问题。

Method: 方法包括训练To2V（Token-to-Video）模型和T2To（Text-to-Token）模型，以及使用自适应FIFO-Diffusion策略平滑连接片段。

Result: 实验结果表明，该方法显著提升了长视频的时序和内容一致性，且计算开销可控。

Conclusion: TokensGen通过模块化设计为长视频生成提供了可扩展的解决方案，适用于故事叙述、电影制作和沉浸式模拟。

Abstract: Generating consistent long videos is a complex challenge: while diffusion-based generative models generate visually impressive short clips, extending them to longer durations often leads to memory bottlenecks and long-term inconsistency. In this paper, we propose TokensGen, a novel two-stage framework that leverages condensed tokens to address these issues. Our method decomposes long video generation into three core tasks: (1) inner-clip semantic control, (2) long-term consistency control, and (3) inter-clip smooth transition. First, we train To2V (Token-to-Video), a short video diffusion model guided by text and video tokens, with a Video Tokenizer that condenses short clips into semantically rich tokens. Second, we introduce T2To (Text-to-Token), a video token diffusion transformer that generates all tokens at once, ensuring global consistency across clips. Finally, during inference, an adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips, reducing boundary artifacts and enhancing smooth transitions. Experimental results demonstrate that our approach significantly enhances long-term temporal and content coherence without incurring prohibitive computational overhead. By leveraging condensed tokens and pre-trained short video models, our method provides a scalable, modular solution for long video generation, opening new possibilities for storytelling, cinematic production, and immersive simulations. Please see our project page at https://vicky0522.github.io/tokensgen-webpage/ .

</details>


### [42] [Appearance Harmonization via Bilateral Grid Prediction with Transformers for 3DGS](https://arxiv.org/abs/2507.15748)
*Jisu Shin,Richard Shaw,Seunghyun Shin,Anton Pelykh,Zhensong Zhang,Hae-Gon Jeon,Eduardo Perez-Pellitero*

Main category: cs.CV

TL;DR: 提出一种基于Transformer的方法，通过预测空间自适应双边网格来校正多视角间的光度变化，提升3D高斯溅射管线的重建质量。


<details>
  <summary>Details</summary>
Motivation: 现代相机处理流程导致多视角间光度不一致，影响新视角合成质量。现有方法通过联合优化场景表示和每张图像的外观嵌入来解决，但增加了计算复杂度和训练时间。

Method: 使用Transformer预测空间自适应双边网格，校正光度变化，并将其集成到3D高斯溅射管线中。

Result: 实验表明，该方法在重建质量和收敛速度上优于或匹配现有场景特定优化方法。

Conclusion: 该方法无需场景特定重训练，实现了跨场景的鲁棒泛化，同时保持了高训练效率。

Abstract: Modern camera pipelines apply extensive on-device processing, such as exposure adjustment, white balance, and color correction, which, while beneficial individually, often introduce photometric inconsistencies across views. These appearance variations violate multi-view consistency and degrade the quality of novel view synthesis. Joint optimization of scene representations and per-image appearance embeddings has been proposed to address this issue, but at the cost of increased computational complexity and slower training. In this work, we propose a transformer-based method that predicts spatially adaptive bilateral grids to correct photometric variations in a multi-view consistent manner, enabling robust cross-scene generalization without the need for scene-specific retraining. By incorporating the learned grids into the 3D Gaussian Splatting pipeline, we improve reconstruction quality while maintaining high training efficiency. Extensive experiments show that our approach outperforms or matches existing scene-specific optimization methods in reconstruction fidelity and convergence speed.

</details>


### [43] [Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization](https://arxiv.org/abs/2507.15765)
*Feng-Qi Cui,Anyang Tong,Jinyang Huang,Jie Zhang,Dan Guo,Zhi Liu,Meng Wang*

Main category: cs.CV

TL;DR: 提出了一种名为HDF的新框架，通过时间-频率分布注意力模块和自适应优化模块，解决了动态面部表情识别中的样本异质性和优化不平衡问题，显著提升了识别准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多源数据和个体表达变异性导致的样本异质性下性能下降，需要一种新框架来解决这些问题。

Method: 设计了时间-频率分布注意力模块（DAM）和分布感知缩放模块（DSM），分别增强时间-频率建模和优化不平衡问题。

Result: 在DFEW和FERV39k数据集上，HDF显著提高了识别准确性和鲁棒性，取得了优异的WAR和UAR。

Conclusion: HDF框架有效解决了样本异质性和优化不平衡问题，具有广泛的泛化能力。

Abstract: Dynamic Facial Expression Recognition (DFER) plays a critical role in affective computing and human-computer interaction. Although existing methods achieve comparable performance, they inevitably suffer from performance degradation under sample heterogeneity caused by multi-source data and individual expression variability. To address these challenges, we propose a novel framework, called Heterogeneity-aware Distributional Framework (HDF), and design two plug-and-play modules to enhance time-frequency modeling and mitigate optimization imbalance caused by hard samples. Specifically, the Time-Frequency Distributional Attention Module (DAM) captures both temporal consistency and frequency robustness through a dual-branch attention design, improving tolerance to sequence inconsistency and visual style shifts. Then, based on gradient sensitivity and information bottleneck principles, an adaptive optimization module Distribution-aware Scaling Module (DSM) is introduced to dynamically balance classification and contrastive losses, enabling more stable and discriminative representation learning. Extensive experiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF significantly improves both recognition accuracy and robustness. Our method achieves superior weighted average recall (WAR) and unweighted average recall (UAR) while maintaining strong generalization across diverse and imbalanced scenarios. Codes are released at https://github.com/QIcita/HDF_DFER.

</details>


### [44] [Regularized Low-Rank Adaptation for Few-Shot Organ Segmentation](https://arxiv.org/abs/2507.15793)
*Ghassen Baklouti,Julio Silva-Rodríguez,Jose Dolz,Houda Bahig,Ismail Ben Ayed*

Main category: cs.CV

TL;DR: 提出了一种动态调整低秩适应（LoRA）秩的PEFT方法，用于医学图像分割，通过l1正则化自动优化秩选择。


<details>
  <summary>Details</summary>
Motivation: 解决LoRA方法中固定秩选择对医学图像任务复杂性的挑战。

Method: 引入l1稀疏正则化到损失函数，使用近端优化器动态调整秩。

Result: 在少量样本微调任务中表现优于标准LoRA和其他PEFT方法。

Conclusion: 该方法高效、鲁棒，能自动适应任务需求，显著提升性能。

Abstract: Parameter-efficient fine-tuning (PEFT) of pre-trained foundation models is increasingly attracting interest in medical imaging due to its effectiveness and computational efficiency. Among these methods, Low-Rank Adaptation (LoRA) is a notable approach based on the assumption that the adaptation inherently occurs in a low-dimensional subspace. While it has shown good performance, its implementation requires a fixed and unalterable rank, which might be challenging to select given the unique complexities and requirements of each medical imaging downstream task. Inspired by advancements in natural image processing, we introduce a novel approach for medical image segmentation that dynamically adjusts the intrinsic rank during adaptation. Viewing the low-rank representation of the trainable weight matrices as a singular value decomposition, we introduce an l_1 sparsity regularizer to the loss function, and tackle it with a proximal optimizer. The regularizer could be viewed as a penalty on the decomposition rank. Hence, its minimization enables to find task-adapted ranks automatically. Our method is evaluated in a realistic few-shot fine-tuning setting, where we compare it first to the standard LoRA and then to several other PEFT methods across two distinguishable tasks: base organs and novel organs. Our extensive experiments demonstrate the significant performance improvements driven by our method, highlighting its efficiency and robustness against suboptimal rank initialization. Our code is publicly available: https://github.com/ghassenbaklouti/ARENA

</details>


### [45] [Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models](https://arxiv.org/abs/2507.15824)
*Enes Sanli,Baris Sarper Tezcan,Aykut Erdem,Erkut Erdem*

Main category: cs.CV

TL;DR: PhysVidBench是一个评估文本到视频（T2V）生成模型物理常识的基准，包含383个提示，通过三阶段评估流程间接测试模型的物理推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前T2V模型在物理常识方面表现不足，如违反因果关系和对象行为，因此需要一种评估其物理推理能力的方法。

Method: 提出PhysVidBench基准，包含383个提示，采用三阶段评估流程：生成物理问题、视频描述、语言模型回答问题。

Result: 通过间接评估策略，避免了直接视频评估中的幻觉问题，为T2V模型的物理常识评估提供了结构化框架。

Conclusion: PhysVidBench填补了当前T2V评估的空白，为生成视频模型的物理常识提供了可解释的评估方法。

Abstract: Recent progress in text-to-video (T2V) generation has enabled the synthesis of visually compelling and temporally coherent videos from natural language. However, these models often fall short in basic physical commonsense, producing outputs that violate intuitive expectations around causality, object behavior, and tool use. Addressing this gap, we present PhysVidBench, a benchmark designed to evaluate the physical reasoning capabilities of T2V systems. The benchmark includes 383 carefully curated prompts, emphasizing tool use, material properties, and procedural interactions, and domains where physical plausibility is crucial. For each prompt, we generate videos using diverse state-of-the-art models and adopt a three-stage evaluation pipeline: (1) formulate grounded physics questions from the prompt, (2) caption the generated video with a vision-language model, and (3) task a language model to answer several physics-involved questions using only the caption. This indirect strategy circumvents common hallucination issues in direct video-based evaluation. By highlighting affordances and tool-mediated actions, areas overlooked in current T2V evaluations, PhysVidBench provides a structured, interpretable framework for assessing physical commonsense in generative video models.

</details>


### [46] [Latent Denoising Makes Good Visual Tokenizers](https://arxiv.org/abs/2507.15856)
*Jiawei Yang,Tianhong Li,Lijie Fan,Yonglong Tian,Yue Wang*

Main category: cs.CV

TL;DR: 论文提出了一种新的视觉分词器（l-DeTok），通过直接与去噪目标对齐，提升生成模型的效果。


<details>
  <summary>Details</summary>
Motivation: 现代生成模型的训练目标（如去噪）与分词器的潜在嵌入未对齐，限制了生成效果。

Method: 提出Latent Denoising Tokenizer（l-DeTok），通过重构被噪声和掩码污染的潜在嵌入来训练分词器。

Result: 在ImageNet 256x256上，l-DeTok在六种代表性生成模型中均优于标准分词器。

Conclusion: 去噪应作为分词器设计的基本原则，为未来分词器开发提供新视角。

Abstract: Despite their fundamental role, it remains unclear what properties could make visual tokenizers more effective for generative modeling. We observe that modern generative models share a conceptually similar training objective -- reconstructing clean signals from corrupted inputs such as Gaussian noise or masking -- a process we term denoising. Motivated by this insight, we propose aligning tokenizer embeddings directly with the downstream denoising objective, encouraging latent embeddings to be more easily reconstructed even when heavily corrupted. To achieve this, we introduce the Latent Denoising Tokenizer (l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images from latent embeddings corrupted by interpolative noise and random masking. Extensive experiments on ImageNet 256x256 demonstrate that our tokenizer consistently outperforms standard tokenizers across six representative generative models. Our findings highlight denoising as a fundamental design principle for tokenizer development, and we hope it could motivate new perspectives for future tokenizer design.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [47] [Self-Supervised Joint Reconstruction and Denoising of T2-Weighted PROPELLER MRI of the Lungs at 0.55T](https://arxiv.org/abs/2507.14308)
*Jingjia Chen,Haoyang Pei,Christoph Maier,Mary Bruno,Qiuting Wen,Seon-Hi Shin,William Moore,Hersh Chandarana,Li Feng*

Main category: eess.IV

TL;DR: 提出了一种自监督联合重建和去噪模型，用于改进0.55T T2加权PROPELLER肺部MRI成像。


<details>
  <summary>Details</summary>
Motivation: 旨在通过自监督学习框架提升低场强肺部MRI的图像清晰度和结构完整性，同时减少扫描时间。

Method: 使用44例COVID-19康复患者的0.55T T2加权肺部MRI数据，将PROPELLER采集的每个叶片沿读出方向分为两部分，一部分用于训练重建网络，另一部分用于损失计算，实现无清洁目标的自监督训练。

Result: 模型显著提升了图像清晰度和结构完整性，与CT图像对齐良好，且仅需一半叶片数量即可完成扫描。放射科医师评估显示该方法优于传统MPPCA去噪（p<0.001）。

Conclusion: 通过利用k空间子集的内在结构冗余，自监督学习模型有效实现了0.55T T2加权PROPELLER肺部MRI的图像重建和去噪。

Abstract: Purpose: This study aims to improve 0.55T T2-weighted PROPELLER lung MRI through a self-supervised joint reconstruction and denoising model.   Methods: T2-weighted 0.55T lung MRI dataset including 44 patients with previous covid infection were used. A self-supervised learning framework was developed, where each blade of the PROPELLER acquisition was split along the readout direction into two partitions. One subset trains the unrolled reconstruction network, while the other subset is used for loss calculation, enabling self-supervised training without clean targets and leveraging matched noise statistics for denoising. For comparison, Marchenko-Pastur Principal Component Analysis (MPPCA) was performed along the coil dimension, followed by conventional parallel imaging reconstruction. The quality of the reconstructed lung MRI was assessed visually by two experienced radiologists independently.   Results: The proposed self-supervised model improved the clarity and structural integrity of the lung images. For cases with available CT scans, the reconstructed images demonstrated strong alignment with corresponding CT images. Additionally, the proposed model enables further scan time reduction by requiring only half the number of blades. Reader evaluations confirmed that the proposed method outperformed MPPCA-denoised images across all categories (Wilcoxon signed-rank test, p<0.001), with moderate inter-reader agreement (weighted Cohen's kappa=0.55; percentage of exact and within +/-1 point agreement=91%).   Conclusion: By leveraging intrinsic structural redundancies between two disjoint splits of k-space subsets, the proposed self-supervised learning model effectively reconstructs the image while suppressing the noise for 0.55T T2-weighted lung MRI with PROPELLER sampling.

</details>


### [48] [QUTCC: Quantile Uncertainty Training and Conformal Calibration for Imaging Inverse Problems](https://arxiv.org/abs/2507.14760)
*Cassandra Tong Ye,Shamus Li,Tyler King,Kristina Monakhova*

Main category: eess.IV

TL;DR: 提出QUTCC方法，通过非线性校准量化预测，提供更紧的不确定性区间，提高深度学习在医学成像任务中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学成像任务中可能产生幻觉，影响准确性，现有方法的不确定性边界过大且不精确。

Method: QUTCC结合U-Net架构和量化嵌入，预测条件分布，通过迭代校准生成更紧的不确定性区间。

Result: QUTCC在去噪和MRI重建任务中成功识别幻觉，并提供比现有方法更紧的不确定性区间。

Conclusion: QUTCC显著提升了深度学习在医学成像中的可靠性，为不确定性量化提供了更优解。

Abstract: Deep learning models often hallucinate, producing realistic artifacts that are not truly present in the sample. This can have dire consequences for scientific and medical inverse problems, such as MRI and microscopy denoising, where accuracy is more important than perceptual quality. Uncertainty quantification techniques, such as conformal prediction, can pinpoint outliers and provide guarantees for image regression tasks, improving reliability. However, existing methods utilize a linear constant scaling factor to calibrate uncertainty bounds, resulting in larger, less informative bounds. We propose QUTCC, a quantile uncertainty training and calibration technique that enables nonlinear, non-uniform scaling of quantile predictions to enable tighter uncertainty estimates. Using a U-Net architecture with a quantile embedding, QUTCC enables the prediction of the full conditional distribution of quantiles for the imaging task. During calibration, QUTCC generates uncertainty bounds by iteratively querying the network for upper and lower quantiles, progressively refining the bounds to obtain a tighter interval that captures the desired coverage. We evaluate our method on several denoising tasks as well as compressive MRI reconstruction. Our method successfully pinpoints hallucinations in image estimates and consistently achieves tighter uncertainty intervals than prior methods while maintaining the same statistical coverage.

</details>


### [49] [PET Image Reconstruction Using Deep Diffusion Image Prior](https://arxiv.org/abs/2507.15078)
*Fumio Hashimoto,Kuang Gong*

Main category: eess.IV

TL;DR: 提出了一种基于扩散模型的PET图像重建方法，结合解剖学先验和半二次分裂算法，提高了计算效率和跨示踪剂泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决PET成像中示踪剂特异性对比度变化和高计算需求的问题。

Method: 采用扩散采样和模型微调交替进行，结合HQS算法优化计算效率。

Result: 在模拟和临床数据上验证了方法的鲁棒性和高效性，适用于低剂量PET成像。

Conclusion: 该方法为低剂量PET成像提供了高效且通用的重建框架。

Abstract: Diffusion models have shown great promise in medical image denoising and reconstruction, but their application to Positron Emission Tomography (PET) imaging remains limited by tracer-specific contrast variability and high computational demands. In this work, we proposed an anatomical prior-guided PET image reconstruction method based on diffusion models, inspired by the deep diffusion image prior (DDIP) framework. The proposed method alternated between diffusion sampling and model fine-tuning guided by the PET sinogram, enabling the reconstruction of high-quality images from various PET tracers using a score function pretrained on a dataset of another tracer. To improve computational efficiency, the half-quadratic splitting (HQS) algorithm was adopted to decouple network optimization from iterative PET reconstruction. The proposed method was evaluated using one simulation and two clinical datasets. For the simulation study, a model pretrained on [$^{18}$F]FDG data was tested on amyloid-negative PET data to assess out-of-distribution (OOD) performance. For the clinical-data validation, ten low-dose [$^{18}$F]FDG datasets and one [$^{18}$F]Florbetapir dataset were tested on a model pretrained on data from another tracer. Experiment results show that the proposed PET reconstruction method can generalize robustly across tracer distributions and scanner types, providing an efficient and versatile reconstruction framework for low-dose PET imaging.

</details>


### [50] [MedSR-Impact: Transformer-Based Super-Resolution for Lung CT Segmentation, Radiomics, Classification, and Prognosis](https://arxiv.org/abs/2507.15340)
*Marc Boubnovski Martell,Kristofer Linton-Reid,Mitchell Chen,Sumeet Hindocha,Benjamin Hunter,Marco A. Calzado,Richard Lee,Joram M. Posma,Eric O. Aboagye*

Main category: eess.IV

TL;DR: TVSRN-V2是一种基于Transformer的超分辨率框架，用于提升低剂量CT图像的分辨率，显著改善肺癌诊断任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 高分辨率CT对胸部疾病诊断至关重要，但受限于辐射剂量和硬件成本。TVSRN-V2旨在提供一种临床可行的解决方案。

Method: 采用Through-Plane Attention Blocks和Swin Transformer V2构建模型，并通过伪低分辨率增强提高鲁棒性。

Result: 在肺叶分割、放射组学和预后任务中，TVSRN-V2显著提升了分割准确性（+4% Dice）、特征可重复性和预测性能（+0.06 C-index和AUC）。

Conclusion: TVSRN-V2是一种高效、临床适用的系统，能够通过超分辨率技术提升CT图像质量，支持临床决策。

Abstract: High-resolution volumetric computed tomography (CT) is essential for accurate diagnosis and treatment planning in thoracic diseases; however, it is limited by radiation dose and hardware costs. We present the Transformer Volumetric Super-Resolution Network (\textbf{TVSRN-V2}), a transformer-based super-resolution (SR) framework designed for practical deployment in clinical lung CT analysis. Built from scalable components, including Through-Plane Attention Blocks (TAB) and Swin Transformer V2 -- our model effectively reconstructs fine anatomical details in low-dose CT volumes and integrates seamlessly with downstream analysis pipelines. We evaluate its effectiveness on three critical lung cancer tasks -- lobe segmentation, radiomics, and prognosis -- across multiple clinical cohorts. To enhance robustness across variable acquisition protocols, we introduce pseudo-low-resolution augmentation, simulating scanner diversity without requiring private data. TVSRN-V2 demonstrates a significant improvement in segmentation accuracy (+4\% Dice), higher radiomic feature reproducibility, and enhanced predictive performance (+0.06 C-index and AUC). These results indicate that SR-driven recovery of structural detail significantly enhances clinical decision support, positioning TVSRN-V2 as a well-engineered, clinically viable system for dose-efficient imaging and quantitative analysis in real-world CT workflows.

</details>


### [51] [Latent Space Synergy: Text-Guided Data Augmentation for Direct Diffusion Biomedical Segmentation](https://arxiv.org/abs/2507.15361)
*Muhammad Aqeel,Maham Nazir,Zanxi Ruan,Francesco Setti*

Main category: eess.IV

TL;DR: SynDiff结合文本引导的合成数据生成和高效扩散分割，解决了医学图像分割中数据稀缺问题，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割（如息肉检测）因标注需专业知识而数据稀缺，限制了深度学习模型的性能。

Method: 使用潜在扩散模型通过文本条件修复生成临床真实的合成息肉，并引入直接潜在估计实现单步推理，提升计算效率。

Result: 在CVC-ClinicDB上达到96.0% Dice和92.9% IoU，同时保持实时性能。

Conclusion: SynDiff通过可控合成增强提升了分割鲁棒性，为资源有限的医疗场景提供了高效解决方案。

Abstract: Medical image segmentation suffers from data scarcity, particularly in polyp detection where annotation requires specialized expertise. We present SynDiff, a framework combining text-guided synthetic data generation with efficient diffusion-based segmentation. Our approach employs latent diffusion models to generate clinically realistic synthetic polyps through text-conditioned inpainting, augmenting limited training data with semantically diverse samples. Unlike traditional diffusion methods requiring iterative denoising, we introduce direct latent estimation enabling single-step inference with T x computational speedup. On CVC-ClinicDB, SynDiff achieves 96.0% Dice and 92.9% IoU while maintaining real-time capability suitable for clinical deployment. The framework demonstrates that controlled synthetic augmentation improves segmentation robustness without distribution shift. SynDiff bridges the gap between data-hungry deep learning models and clinical constraints, offering an efficient solution for deployment in resourcelimited medical settings.

</details>


### [52] [A Steel Surface Defect Detection Method Based on Lightweight Convolution Optimization](https://arxiv.org/abs/2507.15476)
*Cong Chen,Ming Chen,Hoileong Lee,Yan Li,Jiyang Yu*

Main category: eess.IV

TL;DR: 提出了一种基于YOLOv9s的深度学习框架，结合C3Ghost模块、SCConv模块和CARAFE上采样算子，用于提高钢表面多尺度缺陷检测的精度和模型性能。


<details>
  <summary>Details</summary>
Motivation: 钢表面缺陷检测在工业制造中具有挑战性，传统方法在复杂环境中对小目标缺陷检测精度不足且漏检率高。

Method: 采用SCConv模块优化特征表示，C3Ghost模块增强特征提取能力，CARAFE上采样算子精细化重组特征图。

Result: 实验表明，该方法在钢表面缺陷检测任务中具有更高的准确性和鲁棒性。

Conclusion: 提出的框架有效解决了钢表面缺陷检测问题，优于其他方法。

Abstract: Surface defect detection of steel, especially the recognition of multi-scale defects, has always been a major challenge in industrial manufacturing. Steel surfaces not only have defects of various sizes and shapes, which limit the accuracy of traditional image processing and detection methods in complex environments. However, traditional defect detection methods face issues of insufficient accuracy and high miss-detection rates when dealing with small target defects. To address this issue, this study proposes a detection framework based on deep learning, specifically YOLOv9s, combined with the C3Ghost module, SCConv module, and CARAFE upsampling operator, to improve detection accuracy and model performance. First, the SCConv module is used to reduce feature redundancy and optimize feature representation by reconstructing the spatial and channel dimensions. Second, the C3Ghost module is introduced to enhance the model's feature extraction ability by reducing redundant computations and parameter volume, thereby improving model efficiency. Finally, the CARAFE upsampling operator, which can more finely reorganize feature maps in a content-aware manner, optimizes the upsampling process and ensures detailed restoration of high-resolution defect regions. Experimental results demonstrate that the proposed model achieves higher accuracy and robustness in steel surface defect detection tasks compared to other methods, effectively addressing defect detection problems.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [53] [Self-Supervised Distillation of Legacy Rule-Based Methods for Enhanced EEG-Based Decision-Making](https://arxiv.org/abs/2507.14542)
*Yipeng Zhang,Yuanyi Ding,Chenda Duan,Atsuro Daida,Hiroki Nariai,Vwani Roychowdhury*

Main category: cs.CE

TL;DR: 提出了一种自监督标签发现框架（SS2LD），用于从传统检测器生成的大量候选事件中精确识别病理高频振荡（HFOs）。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的HFO检测器精度不足，且监督学习方法依赖难以获取的标注数据。SS2LD旨在解决这些问题。

Method: 利用变分自编码器（VAE）进行形态预训练，学习事件的潜在表示，并通过聚类生成弱监督信号，训练分类器优化检测边界。

Result: 在多机构间期iEEG数据集上，SS2LD优于现有方法，提供了一种可扩展且高效的病理HFO识别策略。

Conclusion: SS2LD为利用传统检测器识别病理HFO提供了一种无需大量标注数据的有效解决方案。

Abstract: High-frequency oscillations (HFOs) in intracranial Electroencephalography (iEEG) are critical biomarkers for localizing the epileptogenic zone in epilepsy treatment. However, traditional rule-based detectors for HFOs suffer from unsatisfactory precision, producing false positives that require time-consuming manual review. Supervised machine learning approaches have been used to classify the detection results, yet they typically depend on labeled datasets, which are difficult to acquire due to the need for specialized expertise. Moreover, accurate labeling of HFOs is challenging due to low inter-rater reliability and inconsistent annotation practices across institutions. The lack of a clear consensus on what constitutes a pathological HFO further challenges supervised refinement approaches. To address this, we leverage the insight that legacy detectors reliably capture clinically relevant signals despite their relatively high false positive rates. We thus propose the Self-Supervised to Label Discovery (SS2LD) framework to refine the large set of candidate events generated by legacy detectors into a precise set of pathological HFOs. SS2LD employs a variational autoencoder (VAE) for morphological pre-training to learn meaningful latent representation of the detected events. These representations are clustered to derive weak supervision for pathological events. A classifier then uses this supervision to refine detection boundaries, trained on real and VAE-augmented data. Evaluated on large multi-institutional interictal iEEG datasets, SS2LD outperforms state-of-the-art methods. SS2LD offers a scalable, label-efficient, and clinically effective strategy to identify pathological HFOs using legacy detectors.

</details>
