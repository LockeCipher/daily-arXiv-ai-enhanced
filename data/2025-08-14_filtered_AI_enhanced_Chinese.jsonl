{"id": "2508.09610", "pdf": "https://arxiv.org/pdf/2508.09610", "abs": "https://arxiv.org/abs/2508.09610", "authors": ["Jiachen Li", "Guangzhi Han", "Jin Wan", "Yuan Gao", "Delong Han"], "title": "DualPhys-GS: Dual Physically-Guided 3D Gaussian Splatting for Underwater Scene Reconstruction", "categories": ["cs.GR"], "comment": "12 pages, 4 figures", "summary": "In 3D reconstruction of underwater scenes, traditional methods based on atmospheric optical models cannot effectively deal with the selective attenuation of light wavelengths and the effect of suspended particle scattering, which are unique to the water medium, and lead to color distortion, geometric artifacts, and collapsing phenomena at long distances. We propose the DualPhys-GS framework to achieve high-quality underwater reconstruction through a dual-path optimization mechanism. Our approach further develops a dual feature-guided attenuation-scattering modeling mechanism, the RGB-guided attenuation optimization model combines RGB features and depth information and can handle edge and structural details. In contrast, the multi-scale depth-aware scattering model captures scattering effects at different scales using a feature pyramid network and an attention mechanism. Meanwhile, we design several special loss functions. The attenuation scattering consistency loss ensures physical consistency. The water body type adaptive loss dynamically adjusts the weighting coefficients. The edge-aware scattering loss is used to maintain the sharpness of structural edges. The multi-scale feature loss helps to capture global and local structural information. In addition, we design a scene adaptive mechanism that can automatically identify the water-body-type characteristics (e.g., clear coral reef waters or turbid coastal waters) and dynamically adjust the scattering and attenuation parameters and optimization strategies. Experimental results show that our method outperforms existing methods in several metrics, especially in suspended matter-dense regions and long-distance scenes, and the reconstruction quality is significantly improved.", "AI": {"tldr": "\u63d0\u51faDualPhys-GS\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u4f18\u5316\u673a\u5236\u89e3\u51b3\u6c34\u4e0b\u573a\u666f3D\u91cd\u5efa\u4e2d\u7684\u989c\u8272\u5931\u771f\u548c\u51e0\u4f55\u4f2a\u5f71\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5927\u6c14\u5149\u5b66\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u5904\u7406\u6c34\u4e0b\u5149\u6ce2\u957f\u9009\u62e9\u6027\u8870\u51cf\u548c\u60ac\u6d6e\u9897\u7c92\u6563\u5c04\uff0c\u5bfc\u81f4\u91cd\u5efa\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u91c7\u7528\u53cc\u7279\u5f81\u5f15\u5bfc\u7684\u8870\u51cf-\u6563\u5c04\u5efa\u6a21\u673a\u5236\uff0c\u7ed3\u5408RGB\u7279\u5f81\u548c\u6df1\u5ea6\u4fe1\u606f\u4f18\u5316\u8870\u51cf\uff0c\u591a\u5c3a\u5ea6\u6df1\u5ea6\u611f\u77e5\u6563\u5c04\u6a21\u578b\u6355\u6349\u6563\u5c04\u6548\u5e94\uff0c\u5e76\u8bbe\u8ba1\u591a\u79cd\u7279\u6b8a\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u60ac\u6d6e\u7269\u5bc6\u96c6\u533a\u57df\u548c\u8fdc\u8ddd\u79bb\u573a\u666f\u4e2d\uff0c\u91cd\u5efa\u8d28\u91cf\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DualPhys-GS\u6846\u67b6\u901a\u8fc7\u7269\u7406\u4e00\u81f4\u6027\u548c\u81ea\u9002\u5e94\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u4e0b3D\u91cd\u5efa\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.09188", "pdf": "https://arxiv.org/pdf/2508.09188", "abs": "https://arxiv.org/abs/2508.09188", "authors": ["Seyed Muhammad Hossein Mousavi", "S. Younes Mirinezhad"], "title": "Synthetic Data Generation for Emotional Depth Faces: Optimizing Conditional DCGANs via Genetic Algorithms in the Latent Space and Stabilizing Training with Knowledge Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Affective computing faces a major challenge: the lack of high-quality, diverse depth facial datasets for recognizing subtle emotional expressions. We propose a framework for synthetic depth face generation using an optimized GAN with Knowledge Distillation (EMA teacher models) to stabilize training, improve quality, and prevent mode collapse. We also apply Genetic Algorithms to evolve GAN latent vectors based on image statistics, boosting diversity and visual quality for target emotions. The approach outperforms GAN, VAE, GMM, and KDE in both diversity and quality. For classification, we extract and concatenate LBP, HOG, Sobel edge, and intensity histogram features, achieving 94% and 96% accuracy with XGBoost. Evaluation using FID, IS, SSIM, and PSNR shows consistent improvement over state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316GAN\u548c\u77e5\u8bc6\u84b8\u998f\u7684\u5408\u6210\u6df1\u5ea6\u4eba\u8138\u751f\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\u63d0\u5347\u591a\u6837\u6027\u548c\u8d28\u91cf\uff0c\u5728\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u60c5\u611f\u8ba1\u7b97\u4e2d\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u6df1\u5ea6\u9762\u90e8\u6570\u636e\u96c6\u7684\u7f3a\u4e4f\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4f18\u5316\u7684GAN\u548c\u77e5\u8bc6\u84b8\u998f\uff08EMA\u6559\u5e08\u6a21\u578b\uff09\u7a33\u5b9a\u8bad\u7ec3\uff0c\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u6f5c\u5728\u5411\u91cf\uff0c\u63d0\u53d6\u591a\u79cd\u7279\u5f81\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728\u591a\u6837\u6027\u548c\u8d28\u91cf\u4e0a\u4f18\u4e8eGAN\u3001VAE\u3001GMM\u548cKDE\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u8fbe94%\u548c96%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u8d28\u91cf\u548c\u5206\u7c7b\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2508.09202", "pdf": "https://arxiv.org/pdf/2508.09202", "abs": "https://arxiv.org/abs/2508.09202", "authors": ["Masoumeh Sharafi", "Soufiane Belharbi", "Houssem Ben Salem", "Ali Etemad", "Alessandro Lameiras Koerich", "Marco Pedersoli", "Simon Bacon", "Eric Granger"], "title": "Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Facial expression recognition (FER) models are employed in many video-based affective computing applications, such as human-computer interaction and healthcare monitoring. However, deep FER models often struggle with subtle expressions and high inter-subject variability, limiting their performance in real-world applications. To improve their performance, source-free domain adaptation (SFDA) methods have been proposed to personalize a pretrained source model using only unlabeled target domain data, thereby avoiding data privacy, storage, and transmission constraints. This paper addresses a challenging scenario where source data is unavailable for adaptation, and only unlabeled target data consisting solely of neutral expressions is available. SFDA methods are not typically designed to adapt using target data from only a single class. Further, using models to generate facial images with non-neutral expressions can be unstable and computationally intensive. In this paper, personalized feature translation (PFT) is proposed for SFDA. Unlike current image translation methods for SFDA, our lightweight method operates in the latent space. We first pre-train the translator on the source domain data to transform the subject-specific style features from one source subject into another. Expression information is preserved by optimizing a combination of expression consistency and style-aware objectives. Then, the translator is adapted on neutral target data, without using source data or image synthesis. By translating in the latent space, PFT avoids the complexity and noise of face expression generation, producing discriminative embeddings optimized for classification. Using PFT eliminates the need for image synthesis, reduces computational overhead (using a lightweight translator), and only adapts part of the model, making the method efficient compared to image-based translation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u4e2a\u6027\u5316\u7279\u5f81\u7ffb\u8bd1\uff08PFT\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u6e90\u6570\u636e\u4e0d\u53ef\u7528\u7684\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff08SFDA\uff09\uff0c\u4ec5\u5229\u7528\u672a\u6807\u8bb0\u7684\u4e2d\u6027\u8868\u60c5\u76ee\u6807\u6570\u636e\uff0c\u907f\u514d\u4e86\u56fe\u50cf\u5408\u6210\u7684\u590d\u6742\u6027\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684SFDA\u65b9\u6cd5\u901a\u5e38\u65e0\u6cd5\u9002\u5e94\u4ec5\u5305\u542b\u5355\u4e00\u7c7b\u522b\uff08\u5982\u4e2d\u6027\u8868\u60c5\uff09\u7684\u76ee\u6807\u6570\u636e\uff0c\u4e14\u57fa\u4e8e\u56fe\u50cf\u751f\u6210\u7684\u65b9\u6cd5\u4e0d\u7a33\u5b9a\u4e14\u8ba1\u7b97\u91cf\u5927\u3002", "method": "\u63d0\u51faPFT\u65b9\u6cd5\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u7279\u5f81\u7ffb\u8bd1\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7ffb\u8bd1\u5668\u5728\u6e90\u6570\u636e\u4e0a\u8f6c\u6362\u4e3b\u4f53\u98ce\u683c\u7279\u5f81\uff0c\u5e76\u5728\u76ee\u6807\u6570\u636e\u4e0a\u9002\u5e94\uff0c\u65e0\u9700\u6e90\u6570\u636e\u6216\u56fe\u50cf\u5408\u6210\u3002", "result": "PFT\u907f\u514d\u4e86\u8868\u60c5\u751f\u6210\u7684\u566a\u58f0\u548c\u590d\u6742\u6027\uff0c\u751f\u6210\u4f18\u5316\u7684\u5206\u7c7b\u5d4c\u5165\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\u3002", "conclusion": "PFT\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u8f7b\u91cf\u7ea7\u7684SFDA\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4ec5\u542b\u4e2d\u6027\u8868\u60c5\u7684\u76ee\u6807\u6570\u636e\u573a\u666f\u3002"}}
{"id": "2508.09207", "pdf": "https://arxiv.org/pdf/2508.09207", "abs": "https://arxiv.org/abs/2508.09207", "authors": ["Tai Vu", "Robert Yang"], "title": "GANime: Generating Anime and Manga Character Drawings from Sketches with Deep Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The process of generating fully colorized drawings from sketches is a large, usually costly bottleneck in the manga and anime industry. In this study, we examine multiple models for image-to-image translation between anime characters and their sketches, including Neural Style Transfer, C-GAN, and CycleGAN. By assessing them qualitatively and quantitatively, we find that C-GAN is the most effective model that is able to produce high-quality and high-resolution images close to those created by humans.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u79cd\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u6a21\u578b\uff0c\u53d1\u73b0C-GAN\u5728\u52a8\u6f2b\u89d2\u8272\u8349\u56fe\u7740\u8272\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u89e3\u51b3\u52a8\u6f2b\u884c\u4e1a\u4e2d\u8349\u56fe\u7740\u8272\u8fc7\u7a0b\u7684\u9ad8\u6210\u672c\u548c\u4f4e\u6548\u7387\u95ee\u9898\u3002", "method": "\u8bc4\u4f30\u4e86Neural Style Transfer\u3001C-GAN\u548cCycleGAN\u7b49\u6a21\u578b\u3002", "result": "C-GAN\u80fd\u751f\u6210\u63a5\u8fd1\u4eba\u7c7b\u521b\u4f5c\u7684\u9ad8\u8d28\u91cf\u3001\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u3002", "conclusion": "C-GAN\u662f\u52a8\u6f2b\u8349\u56fe\u7740\u8272\u4efb\u52a1\u4e2d\u6700\u6709\u6548\u7684\u6a21\u578b\u3002"}}
{"id": "2508.09239", "pdf": "https://arxiv.org/pdf/2508.09239", "abs": "https://arxiv.org/abs/2508.09239", "authors": ["Zheng Zhou", "Yu-Jie Xiong", "Chun-Ming Xia", "Jia-Chen Zhang", "Hong-Jian Zhan"], "title": "Gradient-Direction-Aware Density Control for 3D Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The emergence of 3D Gaussian Splatting (3DGS) has significantly advanced novel view synthesis through explicit scene representation, enabling real-time photorealistic rendering. However, existing approaches manifest two critical limitations in complex scenarios: (1) Over-reconstruction occurs when persistent large Gaussians cannot meet adaptive splitting thresholds during density control. This is exacerbated by conflicting gradient directions that prevent effective splitting of these Gaussians; (2) Over-densification of Gaussians occurs in regions with aligned gradient aggregation, leading to redundant component proliferation. This redundancy significantly increases memory overhead due to unnecessary data retention. We present Gradient-Direction-Aware Gaussian Splatting (GDAGS), a gradient-direction-aware adaptive density control framework to address these challenges. Our key innovations: the gradient coherence ratio (GCR), computed through normalized gradient vector norms, which explicitly discriminates Gaussians with concordant versus conflicting gradient directions; and a nonlinear dynamic weighting mechanism leverages the GCR to enable gradient-direction-aware density control. Specifically, GDAGS prioritizes conflicting-gradient Gaussians during splitting operations to enhance geometric details while suppressing redundant concordant-direction Gaussians. Conversely, in cloning processes, GDAGS promotes concordant-direction Gaussian densification for structural completion while preventing conflicting-direction Gaussian overpopulation. Comprehensive evaluations across diverse real-world benchmarks demonstrate that GDAGS achieves superior rendering quality while effectively mitigating over-reconstruction, suppressing over-densification, and constructing compact scene representations with 50\\% reduced memory consumption through optimized Gaussians utilization.", "AI": {"tldr": "GDAGS\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u65b9\u5411\u611f\u77e5\u7684\u81ea\u9002\u5e94\u5bc6\u5ea6\u63a7\u5236\u6846\u67b6\uff0c\u89e3\u51b3\u4e863D\u9ad8\u65af\u6e85\u5c04\u4e2d\u7684\u8fc7\u91cd\u5efa\u548c\u8fc7\u5bc6\u96c6\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u8d28\u91cf\u5e76\u51cf\u5c11\u4e86\u5185\u5b58\u6d88\u8017\u3002", "motivation": "\u73b0\u67093D\u9ad8\u65af\u6e85\u5c04\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e2d\u5b58\u5728\u8fc7\u91cd\u5efa\u548c\u8fc7\u5bc6\u96c6\u95ee\u9898\uff0c\u5bfc\u81f4\u5185\u5b58\u5f00\u9500\u589e\u52a0\u548c\u6e32\u67d3\u8d28\u91cf\u4e0b\u964d\u3002", "method": "GDAGS\u901a\u8fc7\u68af\u5ea6\u4e00\u81f4\u6027\u6bd4\u7387\uff08GCR\uff09\u548c\u975e\u7ebf\u6027\u52a8\u6001\u52a0\u6743\u673a\u5236\uff0c\u5b9e\u73b0\u68af\u5ea6\u65b9\u5411\u611f\u77e5\u7684\u5bc6\u5ea6\u63a7\u5236\uff0c\u4f18\u5316\u9ad8\u65af\u5206\u5e03\u3002", "result": "GDAGS\u5728\u591a\u79cd\u771f\u5b9e\u573a\u666f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6e32\u67d3\u8d28\u91cf\u63d0\u5347\uff0c\u5185\u5b58\u6d88\u8017\u51cf\u5c1150%\u3002", "conclusion": "GDAGS\u6709\u6548\u89e3\u51b3\u4e863D\u9ad8\u65af\u6e85\u5c04\u7684\u5173\u952e\u95ee\u9898\uff0c\u4e3a\u5b9e\u65f6\u903c\u771f\u6e32\u67d3\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09327", "pdf": "https://arxiv.org/pdf/2508.09327", "abs": "https://arxiv.org/abs/2508.09327", "authors": ["Yifan Jiang", "Ahmad Shariftabrizi", "Venkata SK. Manem"], "title": "Lung-DDPM+: Efficient Thoracic CT Image Synthesis using Diffusion Probabilistic Model", "categories": ["cs.CV"], "comment": null, "summary": "Generative artificial intelligence (AI) has been playing an important role in various domains. Leveraging its high capability to generate high-fidelity and diverse synthetic data, generative AI is widely applied in diagnostic tasks, such as lung cancer diagnosis using computed tomography (CT). However, existing generative models for lung cancer diagnosis suffer from low efficiency and anatomical imprecision, which limit their clinical applicability. To address these drawbacks, we propose Lung-DDPM+, an improved version of our previous model, Lung-DDPM. This novel approach is a denoising diffusion probabilistic model (DDPM) guided by nodule semantic layouts and accelerated by a pulmonary DPM-solver, enabling the method to focus on lesion areas while achieving a better trade-off between sampling efficiency and quality. Evaluation results on the public LIDC-IDRI dataset suggest that the proposed method achieves 8$\\times$ fewer FLOPs (floating point operations per second), 6.8$\\times$ lower GPU memory consumption, and 14$\\times$ faster sampling compared to Lung-DDPM. Moreover, it maintains comparable sample quality to both Lung-DDPM and other state-of-the-art (SOTA) generative models in two downstream segmentation tasks. We also conducted a Visual Turing Test by an experienced radiologist, showing the advanced quality and fidelity of synthetic samples generated by the proposed method. These experimental results demonstrate that Lung-DDPM+ can effectively generate high-quality thoracic CT images with lung nodules, highlighting its potential for broader applications, such as general tumor synthesis and lesion generation in medical imaging. The code and pretrained models are available at https://github.com/Manem-Lab/Lung-DDPM-PLUS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u751f\u6210\u6a21\u578bLung-DDPM+\uff0c\u7528\u4e8e\u9ad8\u6548\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u80ba\u90e8CT\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u6548\u7387\u4f4e\u548c\u89e3\u5256\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u80ba\u90e8\u764c\u75c7\u8bca\u65ad\u4e2d\u6548\u7387\u4f4e\u4e14\u89e3\u5256\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7ed3\u8282\u8bed\u4e49\u5e03\u5c40\u7684\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DDPM\uff09\uff0c\u5e76\u901a\u8fc7\u80ba\u90e8DPM-solver\u52a0\u901f\uff0c\u4f18\u5316\u91c7\u6837\u6548\u7387\u4e0e\u8d28\u91cf\u3002", "result": "\u5728LIDC-IDRI\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u51cf\u5c11\u4e868\u500dFLOPs\u30016.8\u500dGPU\u5185\u5b58\u6d88\u8017\uff0c\u91c7\u6837\u901f\u5ea6\u63d0\u534714\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u6837\u672c\u751f\u6210\u80fd\u529b\u3002", "conclusion": "Lung-DDPM+\u80fd\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u80ba\u90e8CT\u56fe\u50cf\uff0c\u5177\u6709\u5e7f\u6cdb\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.09392", "pdf": "https://arxiv.org/pdf/2508.09392", "abs": "https://arxiv.org/abs/2508.09392", "authors": ["Kang Ni", "Minrui Zou", "Yuxuan Li", "Xiang Li", "Kehua Guo", "Ming-Ming Cheng", "Yimian Dai"], "title": "DenoDet V2: Phase-Amplitude Cross Denoising for SAR Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "One of the primary challenges in Synthetic Aperture Radar (SAR) object detection lies in the pervasive influence of coherent noise. As a common practice, most existing methods, whether handcrafted approaches or deep learning-based methods, employ the analysis or enhancement of object spatial-domain characteristics to achieve implicit denoising. In this paper, we propose DenoDet V2, which explores a completely novel and different perspective to deconstruct and modulate the features in the transform domain via a carefully designed attention architecture. Compared to DenoDet V1, DenoDet V2 is a major advancement that exploits the complementary nature of amplitude and phase information through a band-wise mutual modulation mechanism, which enables a reciprocal enhancement between phase and amplitude spectra. Extensive experiments on various SAR datasets demonstrate the state-of-the-art performance of DenoDet V2. Notably, DenoDet V2 achieves a significant 0.8\\% improvement on SARDet-100K dataset compared to DenoDet V1, while reducing the model complexity by half. The code is available at https://github.com/GrokCV/GrokSAR.", "AI": {"tldr": "DenoDet V2\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u6362\u57df\u7279\u5f81\u89e3\u8c03\u548c\u8c03\u5236\u7684\u6ce8\u610f\u529b\u67b6\u6784\uff0c\u901a\u8fc7\u632f\u5e45\u548c\u76f8\u4f4d\u4fe1\u606f\u7684\u4e92\u8865\u6027\u5b9e\u73b0\u53bb\u566a\uff0c\u6027\u80fd\u4f18\u4e8eV1\u4e14\u6a21\u578b\u590d\u6742\u5ea6\u51cf\u534a\u3002", "motivation": "\u89e3\u51b3\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08SAR\uff09\u76ee\u6807\u68c0\u6d4b\u4e2d\u76f8\u5e72\u566a\u58f0\u7684\u666e\u904d\u5f71\u54cd\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u57fa\u4e8e\u7a7a\u95f4\u57df\u7279\u5f81\u5206\u6790\u6216\u589e\u5f3a\uff0c\u800cDenoDet V2\u63a2\u7d22\u4e86\u53d8\u6362\u57df\u7684\u65b0\u89c6\u89d2\u3002", "method": "\u8bbe\u8ba1\u4e86\u6ce8\u610f\u529b\u67b6\u6784\uff0c\u901a\u8fc7\u6ce2\u6bb5\u4e92\u8c03\u5236\u673a\u5236\u5229\u7528\u632f\u5e45\u548c\u76f8\u4f4d\u4fe1\u606f\u7684\u4e92\u8865\u6027\uff0c\u5b9e\u73b0\u76f8\u4e92\u589e\u5f3a\u3002", "result": "\u5728\u591a\u4e2aSAR\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cSARDet-100K\u4e0a\u6bd4V1\u63d0\u53470.8%\uff0c\u6a21\u578b\u590d\u6742\u5ea6\u51cf\u534a\u3002", "conclusion": "DenoDet V2\u901a\u8fc7\u53d8\u6362\u57df\u7279\u5f81\u8c03\u5236\u5b9e\u73b0\u4e86\u9ad8\u6548\u53bb\u566a\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u4e14\u6a21\u578b\u66f4\u8f7b\u91cf\u3002"}}
{"id": "2508.09449", "pdf": "https://arxiv.org/pdf/2508.09449", "abs": "https://arxiv.org/abs/2508.09449", "authors": ["Jiaqi Yan", "Shuning Xu", "Xiangyu Chen", "Dell Zhang", "Jie Tang", "Gangshan Wu", "Jie Liu"], "title": "RASR: Retrieval-Augmented Super Resolution for Practical Reference-based Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Reference-based Super Resolution (RefSR) improves upon Single Image Super Resolution (SISR) by leveraging high-quality reference images to enhance texture fidelity and visual realism. However, a critical limitation of existing RefSR approaches is their reliance on manually curated target-reference image pairs, which severely constrains their practicality in real-world scenarios. To overcome this, we introduce Retrieval-Augmented Super Resolution (RASR), a new and practical RefSR paradigm that automatically retrieves semantically relevant high-resolution images from a reference database given only a low-quality input. This enables scalable and flexible RefSR in realistic use cases, such as enhancing mobile photos taken in environments like zoos or museums, where category-specific reference data (e.g., animals, artworks) can be readily collected or pre-curated. To facilitate research in this direction, we construct RASR-Flickr30, the first benchmark dataset designed for RASR. Unlike prior datasets with fixed target-reference pairs, RASR-Flickr30 provides per-category reference databases to support open-world retrieval. We further propose RASRNet, a strong baseline that combines a semantic reference retriever with a diffusion-based RefSR generator. It retrieves relevant references based on semantic similarity and employs a diffusion-based generator enhanced with semantic conditioning. Experiments on RASR-Flickr30 demonstrate that RASRNet consistently improves over SISR baselines, achieving +0.38 dB PSNR and -0.0131 LPIPS, while generating more realistic textures. These findings highlight retrieval augmentation as a promising direction to bridge the gap between academic RefSR research and real-world applicability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53c2\u8003\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5RASR\uff0c\u901a\u8fc7\u81ea\u52a8\u68c0\u7d22\u76f8\u5173\u9ad8\u5206\u8fa8\u7387\u53c2\u8003\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRefSR\u4f9d\u8d56\u624b\u52a8\u914d\u5bf9\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709RefSR\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u914d\u5bf9\u7684\u53c2\u8003\u56fe\u50cf\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002RASR\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u68c0\u7d22\u53c2\u8003\u56fe\u50cf\uff0c\u63d0\u5347\u5b9e\u7528\u6027\u548c\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51faRASR\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u4e49\u68c0\u7d22\u5668\u548c\u6269\u6563\u751f\u6210\u5668\uff0c\u81ea\u52a8\u68c0\u7d22\u5e76\u5229\u7528\u76f8\u5173\u53c2\u8003\u56fe\u50cf\u8fdb\u884c\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u3002", "result": "\u5728RASR-Flickr30\u6570\u636e\u96c6\u4e0a\uff0cRASRNet\u6bd4SISR\u57fa\u7ebf\u63d0\u53470.38 dB PSNR\u548c-0.0131 LPIPS\uff0c\u751f\u6210\u66f4\u771f\u5b9e\u7684\u7eb9\u7406\u3002", "conclusion": "RASR\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\uff0c\u4e3aRefSR\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.09456", "pdf": "https://arxiv.org/pdf/2508.09456", "abs": "https://arxiv.org/abs/2508.09456", "authors": ["Junxian Li", "Beining Xu", "Di Zhang"], "title": "IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding", "categories": ["cs.CV", "cs.CL", "cs.CR"], "comment": "13 pages, 13 Figures", "summary": "Vision-language models (VLMs) have shown significant advancements in tasks such as visual grounding, where they localize specific objects in images based on natural language queries and images. However, security issues in visual grounding tasks for VLMs remain underexplored, especially in the context of backdoor attacks. In this paper, we introduce a novel input-aware backdoor attack method, IAG, designed to manipulate the grounding behavior of VLMs. This attack forces the model to ground a specific target object in the input image, regardless of the user's query. We propose an adaptive trigger generator that embeds the semantic information of the attack target's description into the original image using a text-conditional U-Net, thereby overcoming the open-vocabulary attack challenge. To ensure the attack's stealthiness, we utilize a reconstruction loss to minimize visual discrepancies between poisoned and clean images. Additionally, we introduce a unified method for generating attack data. IAG is evaluated theoretically and empirically, demonstrating its feasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches over 65\\% on various testing sets. IAG also shows promising potential on manipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on clean samples. Extensive specific experiments, such as ablation study and potential defense, also indicate the robustness and transferability of our attack.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8f93\u5165\u611f\u77e5\u540e\u95e8\u653b\u51fb\u65b9\u6cd5IAG\uff0c\u9488\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u89e6\u53d1\u5668\u751f\u6210\u5668\u5c06\u653b\u51fb\u76ee\u6807\u7684\u8bed\u4e49\u4fe1\u606f\u5d4c\u5165\u56fe\u50cf\uff0c\u5b9e\u73b0\u653b\u51fb\u76ee\u6807\u7684\u65e0\u89c6\u7528\u6237\u67e5\u8be2\u7684\u5b9a\u4f4d\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5728\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u5b89\u5168\u95ee\u9898\uff0c\u5c24\u5176\u662f\u540e\u95e8\u653b\u51fb\uff0c\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51faIAG\u65b9\u6cd5\uff0c\u5229\u7528\u6587\u672c\u6761\u4ef6U-Net\u751f\u6210\u81ea\u9002\u5e94\u89e6\u53d1\u5668\uff0c\u5c06\u653b\u51fb\u76ee\u6807\u7684\u8bed\u4e49\u4fe1\u606f\u5d4c\u5165\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u91cd\u6784\u635f\u5931\u786e\u4fdd\u653b\u51fb\u7684\u9690\u853d\u6027\u3002", "result": "IAG\u5728InternVL-2.5-8B\u4e0a\u7684ASR@0.5\u8d85\u8fc765%\uff0c\u4e14\u5728Ferret-7B\u548cLlaVA-1.5-7B\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5bf9\u5e72\u51c0\u6837\u672c\u7684\u51c6\u786e\u6027\u5f71\u54cd\u6781\u5c0f\u3002", "conclusion": "IAG\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u653b\u51fb\u6548\u679c\u548c\u9690\u853d\u6027\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2508.09476", "pdf": "https://arxiv.org/pdf/2508.09476", "abs": "https://arxiv.org/abs/2508.09476", "authors": ["Yuji Wang", "Moran Li", "Xiaobin Hu", "Ran Yi", "Jiangning Zhang", "Chengming Xu", "Weijian Cao", "Yabiao Wang", "Chengjie Wang", "Lizhuang Ma"], "title": "From Large Angles to Consistent Faces: Identity-Preserving Video Generation via Mixture of Facial Experts", "categories": ["cs.CV"], "comment": null, "summary": "Current video generation models struggle with identity preservation under large facial angles, primarily facing two challenges: the difficulty in exploring an effective mechanism to integrate identity features into DiT structure, and the lack of targeted coverage of large facial angles in existing open-source video datasets. To address these, we present two key innovations. First, we introduce a Mixture of Facial Experts (MoFE) that dynamically combines complementary cues from three specialized experts, each designed to capture distinct but mutually reinforcing aspects of facial attributes. The identity expert captures cross-pose identity-sensitive features, the semantic expert extracts high-level visual semantxics, and the detail expert preserves pixel-level features (e.g., skin texture, color gradients). Furthermore, to mitigate dataset limitations, we have tailored a data processing pipeline centered on two key aspects: Face Constraints and Identity Consistency. Face Constraints ensure facial angle diversity and a high proportion of facial regions, while Identity Consistency preserves coherent person-specific features across temporal sequences, collectively addressing the scarcity of large facial angles and identity-stable training data in existing datasets. Leveraging this pipeline, we have curated and refined a Large Face Angles (LFA) Dataset from existing open-source human video datasets, comprising 460K video clips with annotated facial angles. Experimental results on the LFA benchmark demonstrate that our method, empowered by the LFA dataset, significantly outperforms prior SOTA methods in face similarity, face FID, and CLIP semantic alignment. The code and dataset will be made publicly available at https://github.com/rain152/LFA-Video-Generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6df7\u5408\u9762\u90e8\u4e13\u5bb6\uff08MoFE\uff09\u65b9\u6cd5\u548c\u5b9a\u5236\u6570\u636e\u5904\u7406\u6d41\u7a0b\uff0c\u89e3\u51b3\u4e86\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\u5927\u89d2\u5ea6\u9762\u90e8\u8eab\u4efd\u4fdd\u6301\u7684\u96be\u9898\uff0c\u5e76\u521b\u5efa\u4e86LFA\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u5927\u89d2\u5ea6\u9762\u90e8\u65f6\u96be\u4ee5\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u4e3b\u8981\u56e0\u7f3a\u4e4f\u6709\u6548\u673a\u5236\u5c06\u8eab\u4efd\u7279\u5f81\u878d\u5165DiT\u7ed3\u6784\uff0c\u4ee5\u53ca\u5f00\u6e90\u6570\u636e\u96c6\u4e2d\u5927\u89d2\u5ea6\u9762\u90e8\u8986\u76d6\u4e0d\u8db3\u3002", "method": "\u5f15\u5165MoFE\uff0c\u7ed3\u5408\u4e09\u79cd\u4e13\u5bb6\uff08\u8eab\u4efd\u3001\u8bed\u4e49\u3001\u7ec6\u8282\uff09\u52a8\u6001\u6355\u6349\u9762\u90e8\u7279\u5f81\uff1b\u8bbe\u8ba1\u6570\u636e\u5904\u7406\u6d41\u7a0b\uff08\u9762\u90e8\u7ea6\u675f\u548c\u8eab\u4efd\u4e00\u81f4\u6027\uff09\u4ee5\u4f18\u5316\u6570\u636e\u96c6\u3002", "result": "\u5728LFA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b9\u6cd5\u5728\u9762\u90e8\u76f8\u4f3c\u5ea6\u3001FID\u548cCLIP\u8bed\u4e49\u5bf9\u9f50\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "MoFE\u548cLFA\u6570\u636e\u96c6\u6709\u6548\u63d0\u5347\u4e86\u5927\u89d2\u5ea6\u9762\u90e8\u89c6\u9891\u751f\u6210\u7684\u8eab\u4efd\u4fdd\u6301\u80fd\u529b\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u5f00\u6e90\u3002"}}
{"id": "2508.09479", "pdf": "https://arxiv.org/pdf/2508.09479", "abs": "https://arxiv.org/abs/2508.09479", "authors": ["Xuejun Huang", "Xinyi Liu", "Yi Wan", "Zhi Zheng", "Bin Zhang", "Mingtao Xiong", "Yingying Pei", "Yongjun Zhang"], "title": "SkySplat: Generalizable 3D Gaussian Splatting from Multi-Temporal Sparse Satellite Images", "categories": ["cs.CV"], "comment": null, "summary": "Three-dimensional scene reconstruction from sparse-view satellite images is a long-standing and challenging task. While 3D Gaussian Splatting (3DGS) and its variants have recently attracted attention for its high efficiency, existing methods remain unsuitable for satellite images due to incompatibility with rational polynomial coefficient (RPC) models and limited generalization capability. Recent advances in generalizable 3DGS approaches show potential, but they perform poorly on multi-temporal sparse satellite images due to limited geometric constraints, transient objects, and radiometric inconsistencies. To address these limitations, we propose SkySplat, a novel self-supervised framework that integrates the RPC model into the generalizable 3DGS pipeline, enabling more effective use of sparse geometric cues for improved reconstruction. SkySplat relies only on RGB images and radiometric-robust relative height supervision, thereby eliminating the need for ground-truth height maps. Key components include a Cross-Self Consistency Module (CSCM), which mitigates transient object interference via consistency-based masking, and a multi-view consistency aggregation strategy that refines reconstruction results. Compared to per-scene optimization methods, SkySplat achieves an 86 times speedup over EOGS with higher accuracy. It also outperforms generalizable 3DGS baselines, reducing MAE from 13.18 m to 1.80 m on the DFC19 dataset significantly, and demonstrates strong cross-dataset generalization on the MVS3D benchmark.", "AI": {"tldr": "SkySplat\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408RPC\u6a21\u578b\u52303DGS\u6d41\u7a0b\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u536b\u661f\u56fe\u50cf\u7684\u4e09\u7ef4\u91cd\u5efa\u6548\u679c\u3002", "motivation": "\u73b0\u67093DGS\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u536b\u661f\u56fe\u50cf\uff0c\u4e14\u591a\u65f6\u76f8\u7a00\u758f\u536b\u661f\u56fe\u50cf\u7684\u91cd\u5efa\u9762\u4e34\u51e0\u4f55\u7ea6\u675f\u4e0d\u8db3\u3001\u77ac\u6001\u7269\u4f53\u548c\u8f90\u5c04\u4e0d\u4e00\u81f4\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faSkySplat\u6846\u67b6\uff0c\u7ed3\u5408RPC\u6a21\u578b\u548c3DGS\uff0c\u5f15\u5165CSCM\u6a21\u5757\u548c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u805a\u5408\u7b56\u7565\uff0c\u4ec5\u4f9d\u8d56RGB\u56fe\u50cf\u548c\u76f8\u5bf9\u9ad8\u5ea6\u76d1\u7763\u3002", "result": "SkySplat\u6bd4EOGS\u5feb86\u500d\u4e14\u66f4\u51c6\u786e\uff0c\u5728DFC19\u6570\u636e\u96c6\u4e0aMAE\u4ece13.18\u7c73\u964d\u81f31.80\u7c73\uff0c\u5e76\u5728MVS3D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "SkySplat\u901a\u8fc7\u81ea\u76d1\u7763\u548c\u51e0\u4f55\u7ea6\u675f\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u536b\u661f\u56fe\u50cf\u7684\u4e09\u7ef4\u91cd\u5efa\u6548\u7387\u548c\u7cbe\u5ea6\u3002"}}
{"id": "2508.09522", "pdf": "https://arxiv.org/pdf/2508.09522", "abs": "https://arxiv.org/abs/2508.09522", "authors": ["Ajeet Kumar Yadav", "Nishant Kumar", "Rathna G N"], "title": "Generation of Indian Sign Language Letters, Numbers, and Words", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "6 pages, 5 figures, 2024 International Conference on Intelligent   Algorithms for Computational Intelligence Systems (IACIS)", "summary": "Sign language, which contains hand movements, facial expressions and bodily gestures, is a significant medium for communicating with hard-of-hearing people. A well-trained sign language community communicates easily, but those who don't know sign language face significant challenges. Recognition and generation are basic communication methods between hearing and hard-of-hearing individuals. Despite progress in recognition, sign language generation still needs to be explored. The Progressive Growing of Generative Adversarial Network (ProGAN) excels at producing high-quality images, while the Self-Attention Generative Adversarial Network (SAGAN) generates feature-rich images at medium resolutions. Balancing resolution and detail is crucial for sign language image generation. We are developing a Generative Adversarial Network (GAN) variant that combines both models to generate feature-rich, high-resolution, and class-conditional sign language images. Our modified Attention-based model generates high-quality images of Indian Sign Language letters, numbers, and words, outperforming the traditional ProGAN in Inception Score (IS) and Fr\\'echet Inception Distance (FID), with improvements of 3.2 and 30.12, respectively. Additionally, we are publishing a large dataset incorporating high-quality images of Indian Sign Language alphabets, numbers, and 129 words.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408ProGAN\u548cSAGAN\u7684GAN\u53d8\u4f53\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u9ad8\u5206\u8fa8\u7387\u7684\u5370\u5ea6\u624b\u8bed\u56fe\u50cf\uff0c\u5e76\u5728IS\u548cFID\u6307\u6807\u4e0a\u4f18\u4e8e\u4f20\u7edfProGAN\u3002", "motivation": "\u624b\u8bed\u662f\u542c\u529b\u969c\u788d\u4eba\u58eb\u7684\u91cd\u8981\u4ea4\u6d41\u5de5\u5177\uff0c\u4f46\u76ee\u524d\u624b\u8bed\u751f\u6210\u6280\u672f\u4ecd\u6709\u5f85\u63a2\u7d22\u3002", "method": "\u7ed3\u5408ProGAN\u548cSAGAN\u7684\u4f18\u70b9\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u5370\u5ea6\u624b\u8bed\u7684\u5b57\u6bcd\u3001\u6570\u5b57\u548c\u5355\u8bcd\u56fe\u50cf\u3002", "result": "\u65b0\u6a21\u578b\u5728IS\u548cFID\u6307\u6807\u4e0a\u5206\u522b\u63d0\u5347\u4e863.2\u548c30.12\uff0c\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b\u9ad8\u8d28\u91cf\u5370\u5ea6\u624b\u8bed\u56fe\u50cf\u7684\u5927\u578b\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u624b\u8bed\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u5e76\u8d21\u732e\u4e86\u4e00\u4e2a\u5b9d\u8d35\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2508.09565", "pdf": "https://arxiv.org/pdf/2508.09565", "abs": "https://arxiv.org/abs/2508.09565", "authors": ["Ming Zhao", "Pingping Liu", "Tongshun Zhang", "Zhe Zhang"], "title": "WEC-DG: Multi-Exposure Wavelet Correction Method Guided by Degradation Description", "categories": ["cs.CV"], "comment": null, "summary": "Multi-exposure correction technology is essential for restoring images affected by insufficient or excessive lighting, enhancing the visual experience by improving brightness, contrast, and detail richness. However, current multi-exposure correction methods often encounter challenges in addressing intra-class variability caused by diverse lighting conditions, shooting environments, and weather factors, particularly when processing images captured at a single exposure level. To enhance the adaptability of these models under complex imaging conditions, this paper proposes a Wavelet-based Exposure Correction method with Degradation Guidance (WEC-DG). Specifically, we introduce a degradation descriptor within the Exposure Consistency Alignment Module (ECAM) at both ends of the processing pipeline to ensure exposure consistency and achieve final alignment. This mechanism effectively addresses miscorrected exposure anomalies caused by existing methods' failure to recognize 'blurred' exposure degradation. Additionally, we investigate the light-detail decoupling properties of the wavelet transform to design the Exposure Restoration and Detail Reconstruction Module (EDRM), which processes low-frequency information related to exposure enhancement before utilizing high-frequency information as a prior guide for reconstructing spatial domain details. This serial processing strategy guarantees precise light correction and enhances detail recovery. Extensive experiments conducted on multiple public datasets demonstrate that the proposed method outperforms existing algorithms, achieving significant performance improvements and validating its effectiveness and practical applicability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u53d8\u6362\u7684\u591a\u66dd\u5149\u6821\u6b63\u65b9\u6cd5\uff08WEC-DG\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u9000\u5316\u63cf\u8ff0\u7b26\u548c\u5c0f\u6ce2\u53d8\u6362\u7684\u5149-\u7ec6\u8282\u89e3\u8026\u7279\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u6210\u50cf\u6761\u4ef6\u4e0b\u7684\u6821\u6b63\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u591a\u66dd\u5149\u6821\u6b63\u65b9\u6cd5\u5728\u5904\u7406\u5355\u66dd\u5149\u56fe\u50cf\u65f6\uff0c\u96be\u4ee5\u5e94\u5bf9\u7531\u591a\u6837\u5149\u7167\u6761\u4ef6\u548c\u73af\u5883\u56e0\u7d20\u5f15\u8d77\u7684\u7c7b\u5185\u53d8\u5f02\u6027\uff0c\u5bfc\u81f4\u66dd\u5149\u5f02\u5e38\u6821\u6b63\u4e0d\u51c6\u786e\u3002", "method": "\u63d0\u51faWEC-DG\u65b9\u6cd5\uff0c\u5305\u62ec\u66dd\u5149\u4e00\u81f4\u6027\u5bf9\u9f50\u6a21\u5757\uff08ECAM\uff09\u548c\u66dd\u5149\u6062\u590d\u4e0e\u7ec6\u8282\u91cd\u5efa\u6a21\u5757\uff08EDRM\uff09\uff0c\u5229\u7528\u5c0f\u6ce2\u53d8\u6362\u5904\u7406\u4f4e\u9ad8\u9891\u4fe1\u606f\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6821\u6b63\u6548\u679c\u3002", "conclusion": "WEC-DG\u65b9\u6cd5\u5728\u590d\u6742\u6210\u50cf\u6761\u4ef6\u4e0b\u5177\u6709\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u591a\u66dd\u5149\u6821\u6b63\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.09597", "pdf": "https://arxiv.org/pdf/2508.09597", "abs": "https://arxiv.org/abs/2508.09597", "authors": ["Heyi Sun", "Cong Wang", "Tian-Xing Xu", "Jingwei Huang", "Di Kang", "Chunchao Guo", "Song-Hai Zhang"], "title": "SVG-Head: Hybrid Surface-Volumetric Gaussians for High-Fidelity Head Reconstruction and Real-Time Editing", "categories": ["cs.CV"], "comment": null, "summary": "Creating high-fidelity and editable head avatars is a pivotal challenge in computer vision and graphics, boosting many AR/VR applications. While recent advancements have achieved photorealistic renderings and plausible animation, head editing, especially real-time appearance editing, remains challenging due to the implicit representation and entangled modeling of the geometry and global appearance. To address this, we propose Surface-Volumetric Gaussian Head Avatar (SVG-Head), a novel hybrid representation that explicitly models the geometry with 3D Gaussians bound on a FLAME mesh and leverages disentangled texture images to capture the global appearance. Technically, it contains two types of Gaussians, in which surface Gaussians explicitly model the appearance of head avatars using learnable texture images, facilitating real-time texture editing, while volumetric Gaussians enhance the reconstruction quality of non-Lambertian regions (e.g., lips and hair). To model the correspondence between 3D world and texture space, we provide a mesh-aware Gaussian UV mapping method, which leverages UV coordinates given by the FLAME mesh to obtain sharp texture images and real-time rendering speed. A hierarchical optimization strategy is further designed to pursue the optimal performance in both reconstruction quality and editing flexibility. Experiments on the NeRSemble dataset show that SVG-Head not only generates high-fidelity rendering results, but also is the first method to obtain explicit texture images for Gaussian head avatars and support real-time appearance editing.", "AI": {"tldr": "SVG-Head\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u8868\u793a\u65b9\u6cd5\uff0c\u7ed3\u54083D\u9ad8\u65af\u548cFLAME\u7f51\u683c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u3001\u53ef\u5b9e\u65f6\u7f16\u8f91\u7684\u5934\u50cf\u5efa\u6a21\u3002", "motivation": "\u89e3\u51b3\u5934\u50cf\u5efa\u6a21\u4e2d\u51e0\u4f55\u4e0e\u5168\u5c40\u5916\u89c2\u7684\u9690\u5f0f\u8868\u793a\u548c\u7ea0\u7f20\u95ee\u9898\uff0c\u63d0\u5347AR/VR\u5e94\u7528\u4e2d\u7684\u5b9e\u65f6\u7f16\u8f91\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u8868\u9762\u9ad8\u65af\u548c\u4f53\u79ef\u9ad8\u65af\u5206\u522b\u5efa\u6a21\u5916\u89c2\u548c\u975e\u6717\u4f2f\u533a\u57df\uff0c\u7ed3\u5408\u7f51\u683c\u611f\u77e5\u7684UV\u6620\u5c04\u548c\u5206\u5c42\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728NeRSemble\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u6e32\u67d3\uff0c\u5e76\u9996\u6b21\u652f\u6301\u5b9e\u65f6\u5916\u89c2\u7f16\u8f91\u3002", "conclusion": "SVG-Head\u4e3a\u5934\u50cf\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u4fdd\u771f\u548c\u5b9e\u65f6\u7f16\u8f91\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09598", "pdf": "https://arxiv.org/pdf/2508.09598", "abs": "https://arxiv.org/abs/2508.09598", "authors": ["Jie Shao", "Ke Zhu", "Minghao Fu", "Guo-hua Wang", "Jianxin Wu"], "title": "Images Speak Louder Than Scores: Failure Mode Escape for Enhancing Generative Quality", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have achieved remarkable progress in class-to-image generation. However, we observe that despite impressive FID scores, state-of-the-art models often generate distorted or low-quality images, especially in certain classes. This gap arises because FID evaluates global distribution alignment, while ignoring the perceptual quality of individual samples. We further examine the role of CFG, a common technique used to enhance generation quality. While effective in improving metrics and suppressing outliers, CFG can introduce distribution shift and visual artifacts due to its misalignment with both training objectives and user expectations. In this work, we propose FaME, a training-free and inference-efficient method for improving perceptual quality. FaME uses an image quality assessment model to identify low-quality generations and stores their sampling trajectories. These failure modes are then used as negative guidance to steer future sampling away from poor-quality regions. Experiments on ImageNet demonstrate that FaME brings consistent improvements in visual quality without compromising FID. FaME also shows the potential to be extended to improve text-to-image generation.", "AI": {"tldr": "FaME\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u9ad8\u6548\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\u8bc6\u522b\u4f4e\u8d28\u91cf\u751f\u6210\u6837\u672c\uff0c\u5e76\u5229\u7528\u5176\u91c7\u6837\u8f68\u8ff9\u4f5c\u4e3a\u8d1f\u5411\u5f15\u5bfc\uff0c\u63d0\u5347\u751f\u6210\u56fe\u50cf\u7684\u611f\u77e5\u8d28\u91cf\u3002", "motivation": "\u5c3d\u7ba1\u6269\u6563\u6a21\u578b\u5728\u7c7b\u5230\u56fe\u50cf\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5728\u67d0\u4e9b\u7c7b\u522b\u4e2d\u4ecd\u4f1a\u751f\u6210\u5931\u771f\u6216\u4f4e\u8d28\u91cf\u7684\u56fe\u50cf\uff0c\u800cFID\u7b49\u6307\u6807\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u5355\u4e2a\u6837\u672c\u7684\u611f\u77e5\u8d28\u91cf\u3002", "method": "\u63d0\u51faFaME\u65b9\u6cd5\uff0c\u5229\u7528\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\u8bc6\u522b\u4f4e\u8d28\u91cf\u751f\u6210\u6837\u672c\uff0c\u5e76\u5b58\u50a8\u5176\u91c7\u6837\u8f68\u8ff9\u4f5c\u4e3a\u8d1f\u5411\u5f15\u5bfc\uff0c\u907f\u514d\u672a\u6765\u91c7\u6837\u8fdb\u5165\u4f4e\u8d28\u91cf\u533a\u57df\u3002", "result": "\u5728ImageNet\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFaME\u5728\u4e0d\u5f71\u54cdFID\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u7684\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "FaME\u662f\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u65e0\u5173\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u5347\u6269\u6563\u6a21\u578b\u7684\u611f\u77e5\u8d28\u91cf\uff0c\u5e76\u6709\u671b\u6269\u5c55\u5230\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u3002"}}
{"id": "2508.09616", "pdf": "https://arxiv.org/pdf/2508.09616", "abs": "https://arxiv.org/abs/2508.09616", "authors": ["Daniel Barco", "Marc Stadelmann", "Martin Oswald", "Ivo Herzig", "Lukas Lichtensteiger", "Pascal Paysan", "Igor Peterlik", "Michal Walczak", "Bjoern Menze", "Frank-Peter Schilling"], "title": "MInDI-3D: Iterative Deep Learning in 3D for Sparse-view Cone Beam Computed Tomography", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present MInDI-3D (Medical Inversion by Direct Iteration in 3D), the first 3D conditional diffusion-based model for real-world sparse-view Cone Beam Computed Tomography (CBCT) artefact removal, aiming to reduce imaging radiation exposure. A key contribution is extending the \"InDI\" concept from 2D to a full 3D volumetric approach for medical images, implementing an iterative denoising process that refines the CBCT volume directly from sparse-view input. A further contribution is the generation of a large pseudo-CBCT dataset (16,182) from chest CT volumes of the CT-RATE public dataset to robustly train MInDI-3D. We performed a comprehensive evaluation, including quantitative metrics, scalability analysis, generalisation tests, and a clinical assessment by 11 clinicians. Our results show MInDI-3D's effectiveness, achieving a 12.96 (6.10) dB PSNR gain over uncorrected scans with only 50 projections on the CT-RATE pseudo-CBCT (independent real-world) test set and enabling an 8x reduction in imaging radiation exposure. We demonstrate its scalability by showing that performance improves with more training data. Importantly, MInDI-3D matches the performance of a 3D U-Net on real-world scans from 16 cancer patients across distortion and task-based metrics. It also generalises to new CBCT scanner geometries. Clinicians rated our model as sufficient for patient positioning across all anatomical sites and found it preserved lung tumour boundaries well.", "AI": {"tldr": "MInDI-3D\u662f\u4e00\u79cd\u57fa\u4e8e3D\u6761\u4ef6\u6269\u6563\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u7a00\u758f\u89c6\u56feCBCT\u4f2a\u5f71\u53bb\u9664\uff0c\u663e\u8457\u964d\u4f4e\u8f90\u5c04\u66b4\u9732\u3002", "motivation": "\u51cf\u5c11\u533b\u5b66\u6210\u50cf\u4e2d\u7684\u8f90\u5c04\u66b4\u9732\uff0c\u540c\u65f6\u63d0\u5347\u7a00\u758f\u89c6\u56feCBCT\u56fe\u50cf\u8d28\u91cf\u3002", "method": "\u5c062D InDI\u6269\u5c55\u52303D\uff0c\u901a\u8fc7\u8fed\u4ee3\u53bb\u566a\u76f4\u63a5\u4ece\u7a00\u758f\u89c6\u56fe\u8f93\u5165\u4f18\u5316CBCT\u4f53\u79ef\uff0c\u5e76\u4f7f\u7528\u5927\u578b\u4f2aCBCT\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728CT-RATE\u6d4b\u8bd5\u96c6\u4e0a\uff0cPSNR\u63d0\u534712.96 dB\uff0c\u8f90\u5c04\u66b4\u9732\u51cf\u5c118\u500d\uff0c\u4e14\u4e0e3D U-Net\u5728\u771f\u5b9e\u626b\u63cf\u4e2d\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "MInDI-3D\u5728\u4e34\u5e8a\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u89e3\u5256\u90e8\u4f4d\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u65b0\u7684CBCT\u626b\u63cf\u4eea\u51e0\u4f55\u7ed3\u6784\u3002"}}
{"id": "2508.09632", "pdf": "https://arxiv.org/pdf/2508.09632", "abs": "https://arxiv.org/abs/2508.09632", "authors": ["Jingwei Liu", "Ling Yang", "Hao Luo", "Fan Wang Hongyan Li", "Mengdi Wang"], "title": "Preacher: Paper-to-Video Agentic System", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The paper-to-video task converts a research paper into a structured video abstract, distilling key concepts, methods, and conclusions into an accessible, well-organized format. While state-of-the-art video generation models demonstrate potential, they are constrained by limited context windows, rigid video duration constraints, limited stylistic diversity, and an inability to represent domain-specific knowledge. To address these limitations, we introduce Preacher, the first paper-to-video agentic system. Preacher employs a top-down approach to decompose, summarize, and reformulate the paper, followed by bottom-up video generation, synthesizing diverse video segments into a coherent abstract. To align cross-modal representations, we define key scenes and introduce a Progressive Chain of Thought (P-CoT) for granular, iterative planning. Preacher successfully generates high-quality video abstracts across five research fields, demonstrating expertise beyond current video generation models. Code will be released at: https://github.com/GenVerse/Paper2Video", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPreacher\u7cfb\u7edf\uff0c\u5c06\u7814\u7a76\u8bba\u6587\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u89c6\u9891\u6458\u8981\uff0c\u89e3\u51b3\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u53d7\u9650\u4e8e\u4e0a\u4e0b\u6587\u7a97\u53e3\u3001\u89c6\u9891\u65f6\u957f\u3001\u98ce\u683c\u591a\u6837\u6027\u548c\u9886\u57df\u77e5\u8bc6\u8868\u793a\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u81ea\u4e0a\u800c\u4e0b\u5206\u89e3\u3001\u603b\u7ed3\u548c\u91cd\u6784\u8bba\u6587\uff0c\u518d\u81ea\u4e0b\u800c\u4e0a\u751f\u6210\u89c6\u9891\uff0c\u7ed3\u5408P-CoT\u8fdb\u884c\u8fed\u4ee3\u89c4\u5212\u3002", "result": "\u5728\u4e94\u4e2a\u7814\u7a76\u9886\u57df\u6210\u529f\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u6458\u8981\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "Preacher\u7cfb\u7edf\u5c55\u793a\u4e86\u8de8\u6a21\u6001\u8868\u793a\u5bf9\u9f50\u7684\u6f5c\u529b\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2508.09667", "pdf": "https://arxiv.org/pdf/2508.09667", "abs": "https://arxiv.org/abs/2508.09667", "authors": ["Xingyilang Yin", "Qi Zhang", "Jiahao Chang", "Ying Feng", "Qingnan Fan", "Xi Yang", "Chi-Man Pun", "Huaqi Zhang", "Xiaodong Cun"], "title": "GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing 3D scenes using 3D Gaussian Splatting (3DGS) from sparse views is an ill-posed problem due to insufficient information, often resulting in noticeable artifacts. While recent approaches have sought to leverage generative priors to complete information for under-constrained regions, they struggle to generate content that remains consistent with input observations. To address this challenge, we propose GSFixer, a novel framework designed to improve the quality of 3DGS representations reconstructed from sparse inputs. The core of our approach is the reference-guided video restoration model, built upon a DiT-based video diffusion model trained on paired artifact 3DGS renders and clean frames with additional reference-based conditions. Considering the input sparse views as references, our model integrates both 2D semantic features and 3D geometric features of reference views extracted from the visual geometry foundation model, enhancing the semantic coherence and 3D consistency when fixing artifact novel views. Furthermore, considering the lack of suitable benchmarks for 3DGS artifact restoration evaluation, we present DL3DV-Res which contains artifact frames rendered using low-quality 3DGS. Extensive experiments demonstrate our GSFixer outperforms current state-of-the-art methods in 3DGS artifact restoration and sparse-view 3D reconstruction. Project page: https://github.com/GVCLab/GSFixer.", "AI": {"tldr": "GSFixer\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u8003\u5f15\u5bfc\u7684\u89c6\u9891\u4fee\u590d\u6a21\u578b\u63d0\u5347\u7a00\u758f\u89c6\u56fe\u4e0b3D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u91cd\u5efa\u7684\u8d28\u91cf\u3002", "motivation": "\u7a00\u758f\u89c6\u56fe\u4e0b3DGS\u91cd\u5efa\u56e0\u4fe1\u606f\u4e0d\u8db3\u5bfc\u81f4\u660e\u663e\u4f2a\u5f71\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u751f\u6210\u4e0e\u8f93\u5165\u4e00\u81f4\u7684\u8865\u5168\u5185\u5bb9\u3002", "method": "\u57fa\u4e8eDiT\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u54082D\u8bed\u4e49\u548c3D\u51e0\u4f55\u7279\u5f81\uff0c\u589e\u5f3a\u4fee\u590d\u89c6\u56fe\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u548c3D\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGSFixer\u57283DGS\u4f2a\u5f71\u4fee\u590d\u548c\u7a00\u758f\u89c6\u56fe3D\u91cd\u5efa\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GSFixer\u901a\u8fc7\u591a\u7279\u5f81\u6574\u5408\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u89c6\u56fe3DGS\u91cd\u5efa\u7684\u8d28\u91cf\u3002"}}
{"id": "2508.09681", "pdf": "https://arxiv.org/pdf/2508.09681", "abs": "https://arxiv.org/abs/2508.09681", "authors": ["Gerardo Loza", "Junlei Hu", "Dominic Jones", "Sharib Ali", "Pietro Valdastri"], "title": "Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in surgical vision", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "10 pages", "summary": "We proposed a novel test-time optimisation (TTO) approach framed by a NeRF-based architecture for long-term 3D point tracking. Most current methods in point tracking struggle to obtain consistent motion or are limited to 2D motion. TTO approaches frame the solution for long-term tracking as optimising a function that aggregates correspondences from other specialised state-of-the-art methods. Unlike the state-of-the-art on TTO, we propose parametrising such a function with our new invertible Neural Radiance Field (InvNeRF) architecture to perform both 2D and 3D tracking in surgical scenarios. Our approach allows us to exploit the advantages of a rendering-based approach by supervising the reprojection of pixel correspondences. It adapts strategies from recent rendering-based methods to obtain a bidirectional deformable-canonical mapping, to efficiently handle a defined workspace, and to guide the rays' density. It also presents our multi-scale HexPlanes for fast inference and a new algorithm for efficient pixel sampling and convergence criteria. We present results in the STIR and SCARE datasets, for evaluating point tracking and testing the integration of kinematic data in our pipeline, respectively. In 2D point tracking, our approach surpasses the precision and accuracy of the TTO state-of-the-art methods by nearly 50% on average precision, while competing with other approaches. In 3D point tracking, this is the first TTO approach, surpassing feed-forward methods while incorporating the benefits of a deformable NeRF-based reconstruction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eNeRF\u67b6\u6784\u7684\u65b0\u578b\u6d4b\u8bd5\u65f6\u4f18\u5316\uff08TTO\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u957f\u671f3D\u70b9\u8ddf\u8e2a\uff0c\u663e\u8457\u63d0\u5347\u4e862D\u548c3D\u8ddf\u8e2a\u7684\u7cbe\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u70b9\u8ddf\u8e2a\u65b9\u6cd5\u5728\u957f\u671f\u4e00\u81f4\u6027\u62163D\u8fd0\u52a8\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7ed3\u5408\u591a\u65b9\u6cd5\u4f18\u52bf\u5e76\u9002\u5e94\u624b\u672f\u573a\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u53ef\u9006\u795e\u7ecf\u8f90\u5c04\u573a\uff08InvNeRF\uff09\u67b6\u6784\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6HexPlanes\u548c\u9ad8\u6548\u50cf\u7d20\u91c7\u6837\u7b97\u6cd5\uff0c\u901a\u8fc7\u6e32\u67d3\u76d1\u7763\u5b9e\u73b0\u53cc\u5411\u53ef\u53d8\u5f62-\u89c4\u8303\u6620\u5c04\u3002", "result": "\u5728STIR\u548cSCARE\u6570\u636e\u96c6\u4e0a\uff0c2D\u70b9\u8ddf\u8e2a\u7684\u5e73\u5747\u7cbe\u5ea6\u63d0\u5347\u8fd150%\uff0c3D\u8ddf\u8e2a\u9996\u6b21\u8d85\u8d8a\u524d\u9988\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u57282D\u548c3D\u70b9\u8ddf\u8e2a\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u624b\u672f\u573a\u666f\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09746", "pdf": "https://arxiv.org/pdf/2508.09746", "abs": "https://arxiv.org/abs/2508.09746", "authors": ["Zhiqiu Zhang", "Dongqi Fan", "Mingjie Wang", "Qiang Tang", "Jian Yang", "Zili Yi"], "title": "Region-to-Region: Enhancing Generative Image Harmonization with Adaptive Regional Injection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The goal of image harmonization is to adjust the foreground in a composite image to achieve visual consistency with the background. Recently, latent diffusion model (LDM) are applied for harmonization, achieving remarkable results. However, LDM-based harmonization faces challenges in detail preservation and limited harmonization ability. Additionally, current synthetic datasets rely on color transfer, which lacks local variations and fails to capture complex real-world lighting conditions. To enhance harmonization capabilities, we propose the Region-to-Region transformation. By injecting information from appropriate regions into the foreground, this approach preserves original details while achieving image harmonization or, conversely, generating new composite data. From this perspective, We propose a novel model R2R. Specifically, we design Clear-VAE to preserve high-frequency details in the foreground using Adaptive Filter while eliminating disharmonious elements. To further enhance harmonization, we introduce the Harmony Controller with Mask-aware Adaptive Channel Attention (MACA), which dynamically adjusts the foreground based on the channel importance of both foreground and background regions. To address the limitation of existing datasets, we propose Random Poisson Blending, which transfers color and lighting information from a suitable region to the foreground, thereby generating more diverse and challenging synthetic images. Using this method, we construct a new synthetic dataset, RPHarmony. Experiments demonstrate the superiority of our method over other methods in both quantitative metrics and visual harmony. Moreover, our dataset helps the model generate more realistic images in real examples. Our code, dataset, and model weights have all been released for open access.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u533a\u57df\u5230\u533a\u57df\u53d8\u6362\u7684\u56fe\u50cf\u548c\u8c10\u5316\u65b9\u6cd5R2R\uff0c\u901a\u8fc7Clear-VAE\u548cHarmony Controller\u63d0\u5347\u7ec6\u8282\u4fdd\u7559\u4e0e\u548c\u8c10\u5316\u80fd\u529b\uff0c\u5e76\u6784\u5efa\u4e86\u65b0\u7684\u5408\u6210\u6570\u636e\u96c6RPHarmony\u3002", "motivation": "\u73b0\u6709LDM\u548c\u8c10\u5316\u65b9\u6cd5\u5728\u7ec6\u8282\u4fdd\u7559\u548c\u590d\u6742\u5149\u7167\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u8db3\uff0c\u4e14\u5408\u6210\u6570\u636e\u96c6\u7f3a\u4e4f\u591a\u6837\u6027\u3002", "method": "\u63d0\u51faRegion-to-Region\u53d8\u6362\uff0c\u7ed3\u5408Clear-VAE\u548cMACA\u63a7\u5236\u5668\uff0c\u5e76\u91c7\u7528Random Poisson Blending\u751f\u6210\u591a\u6837\u5316\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eR2R\u5728\u5b9a\u91cf\u6307\u6807\u548c\u89c6\u89c9\u548c\u8c10\u6027\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u4e14\u65b0\u6570\u636e\u96c6\u63d0\u5347\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "R2R\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u548c\u8c10\u5316\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u5e76\u516c\u5f00\u4e86\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548c\u6a21\u578b\u6743\u91cd\u3002"}}
{"id": "2508.09802", "pdf": "https://arxiv.org/pdf/2508.09802", "abs": "https://arxiv.org/abs/2508.09802", "authors": ["Xin Du", "Maoyuan Xu", "Zhi Ying"], "title": "MUJICA: Reforming SISR Models for PBR Material Super-Resolution via Cross-Map Attention", "categories": ["cs.CV"], "comment": null, "summary": "Physically Based Rendering (PBR) materials are typically characterized by multiple 2D texture maps such as basecolor, normal, metallic, and roughness which encode spatially-varying bi-directional reflectance distribution function (SVBRDF) parameters to model surface reflectance properties and microfacet interactions. Upscaling SVBRDF material is valuable for modern 3D graphics applications. However, existing Single Image Super-Resolution (SISR) methods struggle with cross-map inconsistency, inadequate modeling of modality-specific features, and limited generalization due to data distribution shifts. In this work, we propose Multi-modal Upscaling Joint Inference via Cross-map Attention (MUJICA), a flexible adapter that reforms pre-trained Swin-transformer-based SISR models for PBR material super-resolution. MUJICA is seamlessly attached after the pre-trained and frozen SISR backbone. It leverages cross-map attention to fuse features while preserving remarkable reconstruction ability of the pre-trained SISR model. Applied to SISR models such as SwinIR, DRCT, and HMANet, MUJICA improves PSNR, SSIM, and LPIPS scores while preserving cross-map consistency. Experiments demonstrate that MUJICA enables efficient training even with limited resources and delivers state-of-the-art performance on PBR material datasets.", "AI": {"tldr": "MUJICA\u662f\u4e00\u79cd\u57fa\u4e8e\u8de8\u56fe\u6ce8\u610f\u529b\u7684\u591a\u6a21\u6001\u4e0a\u91c7\u6837\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347PBR\u6750\u6599\u7684\u8d85\u5206\u8fa8\u7387\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709SISR\u65b9\u6cd5\u7684\u8de8\u56fe\u4e0d\u4e00\u81f4\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709SISR\u65b9\u6cd5\u5728PBR\u6750\u6599\u8d85\u5206\u8fa8\u7387\u4e2d\u5b58\u5728\u8de8\u56fe\u4e0d\u4e00\u81f4\u3001\u6a21\u6001\u7279\u5f81\u5efa\u6a21\u4e0d\u8db3\u548c\u6570\u636e\u5206\u5e03\u504f\u79fb\u5bfc\u81f4\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\u95ee\u9898\u3002", "method": "\u63d0\u51faMUJICA\uff0c\u4e00\u79cd\u7075\u6d3b\u7684\u9002\u914d\u5668\uff0c\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684Swin-transformer SISR\u6a21\u578b\uff0c\u901a\u8fc7\u8de8\u56fe\u6ce8\u610f\u529b\u878d\u5408\u7279\u5f81\uff0c\u4fdd\u6301\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4f18\u79c0\u91cd\u5efa\u80fd\u529b\u3002", "result": "MUJICA\u5728SwinIR\u3001DRCT\u548cHMANet\u7b49SISR\u6a21\u578b\u4e0a\u63d0\u5347\u4e86PSNR\u3001SSIM\u548cLPIPS\u5206\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8de8\u56fe\u4e00\u81f4\u6027\u3002", "conclusion": "MUJICA\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u9ad8\u6548\u8bad\u7ec3\uff0c\u5e76\u5728PBR\u6750\u6599\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.09822", "pdf": "https://arxiv.org/pdf/2508.09822", "abs": "https://arxiv.org/abs/2508.09822", "authors": ["Zijian Song", "Sihan Qin", "Tianshui Chen", "Liang Lin", "Guangrun Wang"], "title": "Physical Autoregressive Model for Robotic Manipulation without Action Pretraining", "categories": ["cs.CV"], "comment": "16 pages, 6 figures", "summary": "The scarcity of manipulation data has motivated the use of pretrained large models from other modalities in robotics. In this work, we build upon autoregressive video generation models to propose a Physical Autoregressive Model (PAR), where physical tokens combine frames and actions to represent the joint evolution of the robot and its environment. PAR leverages the world knowledge embedded in video pretraining to understand physical dynamics without requiring action pretraining, enabling accurate video prediction and consistent action trajectories. It also adopts a DiT-based de-tokenizer to model frames and actions as continuous tokens, mitigating quantization errors and facilitating mutual enhancement. Furthermore, we incorporate a causal mask with inverse kinematics, parallel training, and the KV-cache mechanism to further improve performance and efficiency. Experiments on the ManiSkill benchmark show that PAR achieves a 100\\% success rate on the PushCube task, matches the performance of action-pretrained baselines on other tasks, and accurately predicts future videos with tightly aligned action trajectories. These findings underscore a promising direction for robotic manipulation by transferring world knowledge from autoregressive video pretraining.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u81ea\u56de\u5f52\u6a21\u578b\uff08PAR\uff09\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u6a21\u578b\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u5e27\u548c\u52a8\u4f5c\u7684\u7269\u7406\u4ee4\u724c\u8868\u793a\u673a\u5668\u4eba\u4e0e\u73af\u5883\u7684\u8054\u5408\u6f14\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u89c6\u9891\u9884\u6d4b\u548c\u4e00\u81f4\u7684\u52a8\u4f5c\u8f68\u8ff9\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u4f5c\u6570\u636e\u7a00\u7f3a\uff0c\u4fc3\u4f7f\u7814\u7a76\u8005\u5229\u7528\u5176\u4ed6\u6a21\u6001\u7684\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u89c6\u9891\u9884\u8bad\u7ec3\u4e2d\u7684\u4e16\u754c\u77e5\u8bc6\u7406\u89e3\u7269\u7406\u52a8\u529b\u5b66\uff0c\u65e0\u9700\u52a8\u4f5c\u9884\u8bad\u7ec3\u3002", "method": "\u63d0\u51faPAR\u6a21\u578b\uff0c\u7ed3\u5408\u5e27\u548c\u52a8\u4f5c\u7684\u7269\u7406\u4ee4\u724c\uff0c\u91c7\u7528DiT\u57fa\u7684\u53bb\u4ee4\u724c\u5668\u5efa\u6a21\u8fde\u7eed\u4ee4\u724c\uff0c\u51cf\u5c11\u91cf\u5316\u8bef\u5dee\u3002\u5f15\u5165\u56e0\u679c\u63a9\u7801\u3001\u9006\u8fd0\u52a8\u5b66\u3001\u5e76\u884c\u8bad\u7ec3\u548cKV\u7f13\u5b58\u673a\u5236\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728ManiSkill\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPAR\u5728PushCube\u4efb\u52a1\u4e0a\u8fbe\u5230100%\u6210\u529f\u7387\uff0c\u4e0e\u5176\u4ed6\u4efb\u52a1\u7684\u52a8\u4f5c\u9884\u8bad\u7ec3\u57fa\u7ebf\u6027\u80fd\u76f8\u5f53\uff0c\u5e76\u80fd\u51c6\u786e\u9884\u6d4b\u672a\u6765\u89c6\u9891\u548c\u52a8\u4f5c\u8f68\u8ff9\u3002", "conclusion": "PAR\u5c55\u793a\u4e86\u901a\u8fc7\u89c6\u9891\u9884\u8bad\u7ec3\u8fc1\u79fb\u4e16\u754c\u77e5\u8bc6\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.09823", "pdf": "https://arxiv.org/pdf/2508.09823", "abs": "https://arxiv.org/abs/2508.09823", "authors": ["Valentin Boussot", "Jean-Louis Dillenseger"], "title": "KonfAI: A Modular and Fully Configurable Framework for Deep Learning in Medical Imaging", "categories": ["cs.CV"], "comment": "https://github.com/vboussot/KonfAI", "summary": "KonfAI is a modular, extensible, and fully configurable deep learning framework specifically designed for medical imaging tasks. It enables users to define complete training, inference, and evaluation workflows through structured YAML configuration files, without modifying the underlying code. This declarative approach enhances reproducibility, transparency, and experimental traceability while reducing development time. Beyond the capabilities of standard pipelines, KonfAI provides native abstractions for advanced strategies including patch-based learning, test-time augmentation, model ensembling, and direct access to intermediate feature representations for deep supervision. It also supports complex multi-model training setups such as generative adversarial architectures. Thanks to its modular and extensible architecture, KonfAI can easily accommodate custom models, loss functions, and data processing components. The framework has been successfully applied to segmentation, registration, and image synthesis tasks, and has contributed to top-ranking results in several international medical imaging challenges. KonfAI is open source and available at \\href{https://github.com/vboussot/KonfAI}{https://github.com/vboussot/KonfAI}.", "AI": {"tldr": "KonfAI\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u4e14\u5b8c\u5168\u53ef\u914d\u7f6e\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u4e13\u4e3a\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\u8bbe\u8ba1\uff0c\u901a\u8fc7YAML\u914d\u7f6e\u6587\u4ef6\u5b9e\u73b0\u5de5\u4f5c\u6d41\u5b9a\u4e49\uff0c\u65e0\u9700\u4fee\u6539\u4ee3\u7801\u3002", "motivation": "\u63d0\u9ad8\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\u7684\u590d\u73b0\u6027\u3001\u900f\u660e\u6027\u548c\u5b9e\u9a8c\u53ef\u8ffd\u6eaf\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u5f00\u53d1\u65f6\u95f4\u3002", "method": "\u4f7f\u7528YAML\u914d\u7f6e\u6587\u4ef6\u5b9a\u4e49\u8bad\u7ec3\u3001\u63a8\u7406\u548c\u8bc4\u4f30\u5de5\u4f5c\u6d41\uff0c\u652f\u6301\u9ad8\u7ea7\u7b56\u7565\u5982\u57fa\u4e8e\u8865\u4e01\u7684\u5b66\u4e60\u3001\u6d4b\u8bd5\u65f6\u589e\u5f3a\u3001\u6a21\u578b\u96c6\u6210\u548c\u6df1\u5ea6\u76d1\u7763\u3002", "result": "\u6210\u529f\u5e94\u7528\u4e8e\u5206\u5272\u3001\u914d\u51c6\u548c\u56fe\u50cf\u5408\u6210\u4efb\u52a1\uff0c\u5e76\u5728\u591a\u4e2a\u56fd\u9645\u533b\u5b66\u5f71\u50cf\u6311\u6218\u4e2d\u53d6\u5f97\u9886\u5148\u6210\u7ee9\u3002", "conclusion": "KonfAI\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7684\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\uff0c\u4e14\u5f00\u6e90\u53ef\u7528\u3002"}}
{"id": "2508.09824", "pdf": "https://arxiv.org/pdf/2508.09824", "abs": "https://arxiv.org/abs/2508.09824", "authors": ["Xuhong Huang", "Shiqi Liu", "Kai Zhang", "Ying Tai", "Jian Yang", "Hui Zeng", "Lei Zhang"], "title": "Reverse Convolution and Its Applications to Image Restoration", "categories": ["cs.CV"], "comment": "ICCV 2025; https://github.com/cszn/ConverseNet", "summary": "Convolution and transposed convolution are fundamental operators widely used in neural networks. However, transposed convolution (a.k.a. deconvolution) does not serve as a true inverse of convolution due to inherent differences in their mathematical formulations. To date, no reverse convolution operator has been established as a standard component in neural architectures. In this paper, we propose a novel depthwise reverse convolution operator as an initial attempt to effectively reverse depthwise convolution by formulating and solving a regularized least-squares optimization problem. We thoroughly investigate its kernel initialization, padding strategies, and other critical aspects to ensure its effective implementation. Building upon this operator, we further construct a reverse convolution block by combining it with layer normalization, 1$\\times$1 convolution, and GELU activation, forming a Transformer-like structure. The proposed operator and block can directly replace conventional convolution and transposed convolution layers in existing architectures, leading to the development of ConverseNet. Corresponding to typical image restoration models such as DnCNN, SRResNet and USRNet, we train three variants of ConverseNet for Gaussian denoising, super-resolution and deblurring, respectively. Extensive experiments demonstrate the effectiveness of the proposed reverse convolution operator as a basic building module. We hope this work could pave the way for developing new operators in deep model design and applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df1\u5ea6\u53cd\u5411\u5377\u79ef\u7b97\u5b50\uff0c\u7528\u4e8e\u6709\u6548\u53cd\u8f6c\u6df1\u5ea6\u5377\u79ef\uff0c\u5e76\u901a\u8fc7\u6784\u5efa\u53cd\u5411\u5377\u79ef\u5757\u6539\u8fdb\u73b0\u6709\u67b6\u6784\u3002", "motivation": "\u7531\u4e8e\u8f6c\u7f6e\u5377\u79ef\u5e76\u975e\u5377\u79ef\u7684\u771f\u6b63\u9006\u8fd0\u7b97\uff0c\u76ee\u524d\u795e\u7ecf\u7f51\u7edc\u4e2d\u7f3a\u4e4f\u6807\u51c6\u7684\u53cd\u5411\u5377\u79ef\u7b97\u5b50\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u6709\u6548\u7684\u53cd\u5411\u5377\u79ef\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6b63\u5219\u5316\u6700\u5c0f\u4e8c\u4e58\u4f18\u5316\u95ee\u9898\u8bbe\u8ba1\u548c\u5b9e\u73b0\u6df1\u5ea6\u53cd\u5411\u5377\u79ef\u7b97\u5b50\uff0c\u5e76\u7ed3\u5408\u5c42\u5f52\u4e00\u5316\u30011\u00d71\u5377\u79ef\u548cGELU\u6fc0\u6d3b\u6784\u5efa\u53cd\u5411\u5377\u79ef\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u53cd\u5411\u5377\u79ef\u7b97\u5b50\u4f5c\u4e3a\u57fa\u7840\u6a21\u5757\u5728\u56fe\u50cf\u53bb\u566a\u3001\u8d85\u5206\u8fa8\u7387\u548c\u53bb\u6a21\u7cca\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u6548\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bbe\u8ba1\u4e2d\u7684\u65b0\u7b97\u5b50\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.09847", "pdf": "https://arxiv.org/pdf/2508.09847", "abs": "https://arxiv.org/abs/2508.09847", "authors": ["Dhruvraj Singh Rawat", "Enggen Sherpa", "Rishikesan Kirupanantha", "Tin Hoang"], "title": "Enhancing Diffusion Face Generation with Contrastive Embeddings and SegFormer Guidance", "categories": ["cs.CV"], "comment": "10 pages, preprint", "summary": "We present a benchmark of diffusion models for human face generation on a small-scale CelebAMask-HQ dataset, evaluating both unconditional and conditional pipelines. Our study compares UNet and DiT architectures for unconditional generation and explores LoRA-based fine-tuning of pretrained Stable Diffusion models as a separate experiment. Building on the multi-conditioning approach of Giambi and Lisanti, which uses both attribute vectors and segmentation masks, our main contribution is the integration of an InfoNCE loss for attribute embedding and the adoption of a SegFormer-based segmentation encoder. These enhancements improve the semantic alignment and controllability of attribute-guided synthesis. Our results highlight the effectiveness of contrastive embedding learning and advanced segmentation encoding for controlled face generation in limited data settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5c0f\u89c4\u6a21CelebAMask-HQ\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6269\u6563\u6a21\u578b\u751f\u6210\u4eba\u8138\u7684\u57fa\u51c6\uff0c\u6bd4\u8f83\u4e86UNet\u548cDiT\u67b6\u6784\u7684\u65e0\u6761\u4ef6\u751f\u6210\uff0c\u5e76\u63a2\u7d22\u4e86\u57fa\u4e8eLoRA\u7684\u9884\u8bad\u7ec3Stable Diffusion\u6a21\u578b\u5fae\u8c03\u3002\u901a\u8fc7\u6539\u8fdb\u5c5e\u6027\u5d4c\u5165\u548c\u5206\u5272\u7f16\u7801\uff0c\u63d0\u5347\u4e86\u8bed\u4e49\u5bf9\u9f50\u548c\u53ef\u63a7\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u6269\u6563\u6a21\u578b\u5728\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u6539\u8fdb\u5c5e\u6027\u5d4c\u5165\u548c\u5206\u5272\u7f16\u7801\u63d0\u5347\u751f\u6210\u4eba\u8138\u7684\u53ef\u63a7\u6027\u548c\u8bed\u4e49\u5bf9\u9f50\u3002", "method": "\u6bd4\u8f83\u4e86UNet\u548cDiT\u67b6\u6784\u7684\u65e0\u6761\u4ef6\u751f\u6210\uff0c\u91c7\u7528LoRA\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u5f15\u5165InfoNCE\u635f\u5931\u548cSegFormer\u5206\u5272\u7f16\u7801\u5668\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u6bd4\u5d4c\u5165\u5b66\u4e60\u548c\u5148\u8fdb\u5206\u5272\u7f16\u7801\u5728\u5c0f\u89c4\u6a21\u6570\u636e\u4e0b\u80fd\u6709\u6548\u63d0\u5347\u53ef\u63a7\u4eba\u8138\u751f\u6210\u7684\u6548\u679c\u3002", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u5c5e\u6027\u5d4c\u5165\u548c\u5206\u5272\u7f16\u7801\uff0c\u8bba\u6587\u5728\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u53ef\u63a7\u6027\uff0c\u4e3a\u53d7\u9650\u6570\u636e\u73af\u5883\u4e0b\u7684\u53ef\u63a7\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2508.09857", "pdf": "https://arxiv.org/pdf/2508.09857", "abs": "https://arxiv.org/abs/2508.09857", "authors": ["Yupeng Zhou", "Zhen Li", "Ziheng Ouyang", "Yuming Chen", "Ruoyi Du", "Daquan Zhou", "Bin Fu", "Yihao Liu", "Peng Gao", "Ming-Ming Cheng", "Qibin Hou"], "title": "OneVAE: Joint Discrete and Continuous Optimization Helps Discrete Video VAE Train Better", "categories": ["cs.CV"], "comment": null, "summary": "Encoding videos into discrete tokens could align with text tokens to facilitate concise and unified multi-modal LLMs, yet introducing significant spatiotemporal compression compared to continuous video representation. Previous discrete video VAEs experienced unstable training, long training time, and degraded reconstruction quality. Given the easier training and superior performance of continuous VAEs, an intuitive idea is to enhance discrete video VAEs by leveraging continuous VAEs. After rethinking the intrinsic link between discrete and continuous representations, we found that FSQ could effectively preserve pre-trained continuous VAE priors compared to other quantization methods. By leveraging continuous VAE priors, it converges several times faster than training from scratch and achieves superior performance at convergence. Meanwhile, two structural improvements are proposed. First, inspired by how continuous VAEs enhance reconstruction via enlarged latent dimensions, we introduce a multi-token quantization mechanism, which achieves nearly a 1 dB improvement in PSNR without compromising the token compression ratio. Second, to tackle reconstruction challenges in high-compression video VAEs, we strengthen first-frame reconstruction, enabling the causal VAE to leverage this information in subsequent frames and markedly improving the performance of 4 x 16 x 16 discrete VAEs. Furthermore, we propose a joint discrete-continuous optimization scheme that unifies the two paradigms and, for the first time, achieves competitive performance on both continuous and discrete representations within a single network. We name our method OneVAE to reflect this connection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOneVAE\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u8fde\u7eed\u548c\u79bb\u6563\u89c6\u9891VAE\u7684\u4f18\u52bf\uff0c\u63d0\u5347\u4e86\u79bb\u6563\u89c6\u9891VAE\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u79bb\u6563\u89c6\u9891VAE\u5728\u8bad\u7ec3\u7a33\u5b9a\u6027\u3001\u65f6\u957f\u548c\u91cd\u5efa\u8d28\u91cf\u4e0a\u5b58\u5728\u95ee\u9898\uff0c\u800c\u8fde\u7eedVAE\u8868\u73b0\u66f4\u4f18\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u6539\u8fdb\u79bb\u6563VAE\u3002", "method": "\u5229\u7528FSQ\u4fdd\u7559\u8fde\u7eedVAE\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u63d0\u51fa\u591a\u4ee4\u724c\u91cf\u5316\u673a\u5236\u548c\u5f3a\u5316\u9996\u5e27\u91cd\u5efa\uff0c\u5e76\u8bbe\u8ba1\u8054\u5408\u79bb\u6563-\u8fde\u7eed\u4f18\u5316\u65b9\u6848\u3002", "result": "\u65b9\u6cd5\u6536\u655b\u901f\u5ea6\u66f4\u5feb\uff0c\u6027\u80fd\u66f4\u4f18\uff0cPSNR\u63d0\u5347\u8fd11 dB\uff0c\u4e14\u5728\u5355\u7f51\u7edc\u4e2d\u540c\u65f6\u652f\u6301\u8fde\u7eed\u548c\u79bb\u6563\u8868\u793a\u3002", "conclusion": "OneVAE\u6210\u529f\u7ed3\u5408\u4e86\u8fde\u7eed\u548c\u79bb\u6563VAE\u7684\u4f18\u52bf\uff0c\u4e3a\u591a\u6a21\u6001LLM\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89c6\u9891\u8868\u793a\u65b9\u6cd5\u3002"}}
{"id": "2508.09858", "pdf": "https://arxiv.org/pdf/2508.09858", "abs": "https://arxiv.org/abs/2508.09858", "authors": ["Weiqi Li", "Zehao Zhang", "Liang Lin", "Guangrun Wang"], "title": "HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics", "categories": ["cs.CV"], "comment": null, "summary": "\\textbf{Synthetic human dynamics} aims to generate photorealistic videos of human subjects performing expressive, intention-driven motions. However, current approaches face two core challenges: (1) \\emph{geometric inconsistency} and \\emph{coarse reconstruction}, due to limited 3D modeling and detail preservation; and (2) \\emph{motion generalization limitations} and \\emph{scene inharmonization}, stemming from weak generative capabilities. To address these, we present \\textbf{HumanGenesis}, a framework that integrates geometric and generative modeling through four collaborative agents: (1) \\textbf{Reconstructor} builds 3D-consistent human-scene representations from monocular video using 3D Gaussian Splatting and deformation decomposition. (2) \\textbf{Critique Agent} enhances reconstruction fidelity by identifying and refining poor regions via multi-round MLLM-based reflection. (3) \\textbf{Pose Guider} enables motion generalization by generating expressive pose sequences using time-aware parametric encoders. (4) \\textbf{Video Harmonizer} synthesizes photorealistic, coherent video via a hybrid rendering pipeline with diffusion, refining the Reconstructor through a Back-to-4D feedback loop. HumanGenesis achieves state-of-the-art performance on tasks including text-guided synthesis, video reenactment, and novel-pose generalization, significantly improving expressiveness, geometric fidelity, and scene integration.", "AI": {"tldr": "HumanGenesis\u6846\u67b6\u901a\u8fc7\u56db\u4e2a\u534f\u4f5c\u4ee3\u7406\u89e3\u51b3\u5408\u6210\u4eba\u7c7b\u52a8\u6001\u4e2d\u7684\u51e0\u4f55\u4e0d\u4e00\u81f4\u548c\u8fd0\u52a8\u6cdb\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u76843D\u91cd\u5efa\u548c\u89c6\u9891\u5408\u6210\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u57283D\u5efa\u6a21\u548c\u7ec6\u8282\u4fdd\u7559\u4e0a\u5b58\u5728\u51e0\u4f55\u4e0d\u4e00\u81f4\u548c\u7c97\u7cd9\u91cd\u5efa\u95ee\u9898\uff0c\u4e14\u751f\u6210\u80fd\u529b\u6709\u9650\u5bfc\u81f4\u8fd0\u52a8\u6cdb\u5316\u548c\u573a\u666f\u4e0d\u534f\u8c03\u3002", "method": "HumanGenesis\u6574\u5408\u51e0\u4f55\u548c\u751f\u6210\u5efa\u6a21\uff0c\u5305\u62ecReconstructor\uff083D\u9ad8\u65af\u6e85\u5c04\uff09\u3001Critique Agent\uff08\u591a\u8f6eMLLM\u53cd\u5c04\uff09\u3001Pose Guider\uff08\u65f6\u95f4\u611f\u77e5\u53c2\u6570\u7f16\u7801\u5668\uff09\u548cVideo Harmonizer\uff08\u6df7\u5408\u6e32\u67d3\uff09\u3002", "result": "\u5728\u6587\u672c\u5f15\u5bfc\u5408\u6210\u3001\u89c6\u9891\u91cd\u6f14\u548c\u65b0\u59ff\u52bf\u6cdb\u5316\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u8868\u73b0\u529b\u3001\u51e0\u4f55\u4fdd\u771f\u5ea6\u548c\u573a\u666f\u6574\u5408\u3002", "conclusion": "HumanGenesis\u901a\u8fc7\u591a\u4ee3\u7406\u534f\u4f5c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6311\u6218\uff0c\u4e3a\u5408\u6210\u4eba\u7c7b\u52a8\u6001\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09912", "pdf": "https://arxiv.org/pdf/2508.09912", "abs": "https://arxiv.org/abs/2508.09912", "authors": ["Chaoran Feng", "Zhenyu Tang", "Wangbo Yu", "Yatian Pang", "Yian Zhao", "Jianbin Zhao", "Li Yuan", "Yonghong Tian"], "title": "E-4DGS: High-Fidelity Dynamic Reconstruction from the Multi-view Event Cameras", "categories": ["cs.CV"], "comment": "16 pages, 10 figures, 5 Tables, accepted by ACMMM 2025", "summary": "Novel view synthesis and 4D reconstruction techniques predominantly rely on RGB cameras, thereby inheriting inherent limitations such as the dependence on adequate lighting, susceptibility to motion blur, and a limited dynamic range. Event cameras, offering advantages of low power, high temporal resolution and high dynamic range, have brought a new perspective to addressing the scene reconstruction challenges in high-speed motion and low-light scenes. To this end, we propose E-4DGS, the first event-driven dynamic Gaussian Splatting approach, for novel view synthesis from multi-view event streams with fast-moving cameras. Specifically, we introduce an event-based initialization scheme to ensure stable training and propose event-adaptive slicing splatting for time-aware reconstruction. Additionally, we employ intensity importance pruning to eliminate floating artifacts and enhance 3D consistency, while incorporating an adaptive contrast threshold for more precise optimization. We design a synthetic multi-view camera setup with six moving event cameras surrounding the object in a 360-degree configuration and provide a benchmark multi-view event stream dataset that captures challenging motion scenarios. Our approach outperforms both event-only and event-RGB fusion baselines and paves the way for the exploration of multi-view event-based reconstruction as a novel approach for rapid scene capture.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faE-4DGS\uff0c\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u52a8\u6001\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u89c6\u89d2\u4e8b\u4ef6\u6d41\u7684\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u89e3\u51b3\u4e86\u9ad8\u901f\u8fd0\u52a8\u548c\u4f4e\u5149\u573a\u666f\u4e0b\u7684\u91cd\u5efa\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfRGB\u76f8\u673a\u5728\u5149\u7167\u4e0d\u8db3\u3001\u8fd0\u52a8\u6a21\u7cca\u548c\u52a8\u6001\u8303\u56f4\u6709\u9650\u7b49\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e8b\u4ef6\u76f8\u673a\u56e0\u5176\u4f4e\u529f\u8017\u3001\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u9ad8\u52a8\u6001\u8303\u56f4\u7279\u6027\uff0c\u4e3a\u9ad8\u901f\u8fd0\u52a8\u548c\u4f4e\u5149\u573a\u666f\u7684\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u63d0\u51fa\u4e8b\u4ef6\u9a71\u52a8\u7684\u521d\u59cb\u5316\u65b9\u6848\u548c\u4e8b\u4ef6\u81ea\u9002\u5e94\u5207\u7247\u6cfc\u6e85\u6280\u672f\uff0c\u7ed3\u5408\u5f3a\u5ea6\u91cd\u8981\u6027\u526a\u679d\u548c\u81ea\u9002\u5e94\u5bf9\u6bd4\u5ea6\u9608\u503c\u4f18\u5316\uff0c\u5b9e\u73b0\u65f6\u95f4\u611f\u77e5\u91cd\u5efa\u3002", "result": "E-4DGS\u5728\u5408\u6210\u591a\u89c6\u89d2\u4e8b\u4ef6\u6d41\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u7eaf\u4e8b\u4ef6\u548c\u4e8b\u4ef6-RGB\u878d\u5408\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u89c6\u89d2\u4e8b\u4ef6\u6d41\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u63a8\u52a8\u4e86\u5feb\u901f\u573a\u666f\u6355\u6349\u7684\u63a2\u7d22\u3002"}}
{"id": "2508.09943", "pdf": "https://arxiv.org/pdf/2508.09943", "abs": "https://arxiv.org/abs/2508.09943", "authors": ["Tom\u00e1s de la Sotta", "Jos\u00e9 M. Saavedra", "H\u00e9ctor Henr\u00edquez", "Violeta Chang", "Aline Xavier"], "title": "AST-n: A Fast Sampling Approach for Low-Dose CT Reconstruction using Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Low-dose CT (LDCT) protocols reduce radiation exposure but increase image noise, compromising diagnostic confidence. Diffusion-based generative models have shown promise for LDCT denoising by learning image priors and performing iterative refinement. In this work, we introduce AST-n, an accelerated inference framework that initiates reverse diffusion from intermediate noise levels, and integrate high-order ODE solvers within conditioned models to further reduce sampling steps. We evaluate two acceleration paradigms--AST-n sampling and standard scheduling with high-order solvers -- on the Low Dose CT Grand Challenge dataset, covering head, abdominal, and chest scans at 10-25 % of standard dose. Conditioned models using only 25 steps (AST-25) achieve peak signal-to-noise ratio (PSNR) above 38 dB and structural similarity index (SSIM) above 0.95, closely matching standard baselines while cutting inference time from ~16 seg to under 1 seg per slice. Unconditional sampling suffers substantial quality loss, underscoring the necessity of conditioning. We also assess DDIM inversion, which yields marginal PSNR gains at the cost of doubling inference time, limiting its clinical practicality. Our results demonstrate that AST-n with high-order samplers enables rapid LDCT reconstruction without significant loss of image fidelity, advancing the feasibility of diffusion-based methods in clinical workflows.", "AI": {"tldr": "AST-n\u6846\u67b6\u901a\u8fc7\u4ece\u4e2d\u95f4\u566a\u58f0\u6c34\u5e73\u542f\u52a8\u53cd\u5411\u6269\u6563\u5e76\u7ed3\u5408\u9ad8\u9636ODE\u6c42\u89e3\u5668\uff0c\u663e\u8457\u52a0\u901f\u4f4e\u5242\u91cfCT\uff08LDCT\uff09\u53bb\u566a\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u4f4e\u5242\u91cfCT\uff08LDCT\uff09\u534f\u8bae\u51cf\u5c11\u8f90\u5c04\u4f46\u589e\u52a0\u56fe\u50cf\u566a\u58f0\uff0c\u5f71\u54cd\u8bca\u65ad\u4fe1\u5fc3\u3002\u6269\u6563\u751f\u6210\u6a21\u578b\u901a\u8fc7\u5b66\u4e60\u56fe\u50cf\u5148\u9a8c\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u4e3aLDCT\u53bb\u566a\u63d0\u4f9b\u6f5c\u529b\u3002", "method": "\u63d0\u51faAST-n\u6846\u67b6\uff0c\u4ece\u4e2d\u95f4\u566a\u58f0\u6c34\u5e73\u542f\u52a8\u53cd\u5411\u6269\u6563\uff0c\u5e76\u96c6\u6210\u9ad8\u9636ODE\u6c42\u89e3\u5668\u4ee5\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\u3002\u8bc4\u4f30\u4e24\u79cd\u52a0\u901f\u8303\u5f0f\uff1aAST-n\u91c7\u6837\u548c\u6807\u51c6\u8c03\u5ea6\u7ed3\u5408\u9ad8\u9636\u6c42\u89e3\u5668\u3002", "result": "AST-25\uff08\u4ec525\u6b65\uff09\u5728\u5cf0\u503c\u4fe1\u566a\u6bd4\uff08PSNR\uff09\u548c\u7ed3\u6784\u76f8\u4f3c\u6027\u6307\u6570\uff08SSIM\uff09\u4e0a\u63a5\u8fd1\u6807\u51c6\u57fa\u7ebf\uff0c\u540c\u65f6\u5c06\u63a8\u7406\u65f6\u95f4\u4ece16\u79d2\u7f29\u77ed\u81f31\u79d2\u4ee5\u4e0b\u3002\u65e0\u6761\u4ef6\u91c7\u6837\u8d28\u91cf\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "AST-n\u7ed3\u5408\u9ad8\u9636\u91c7\u6837\u5668\u53ef\u5b9e\u73b0\u5feb\u901fLDCT\u91cd\u5efa\uff0c\u65e0\u660e\u663e\u56fe\u50cf\u8d28\u91cf\u635f\u5931\uff0c\u63a8\u52a8\u6269\u6563\u65b9\u6cd5\u5728\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2508.09949", "pdf": "https://arxiv.org/pdf/2508.09949", "abs": "https://arxiv.org/abs/2508.09949", "authors": ["Trevine Oorloff", "Vishwanath Sindagi", "Wele Gedara Chaminda Bandara", "Ali Shafahi", "Amin Ghiasi", "Charan Prakash", "Reza Ardekani"], "title": "Stable Diffusion Models are Secretly Good at Visual In-Context Learning", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to ICCV 2025", "summary": "Large language models (LLM) in natural language processing (NLP) have demonstrated great potential for in-context learning (ICL) -- the ability to leverage a few sets of example prompts to adapt to various tasks without having to explicitly update the model weights. ICL has recently been explored for computer vision tasks with promising early outcomes. These approaches involve specialized training and/or additional data that complicate the process and limit its generalizability. In this work, we show that off-the-shelf Stable Diffusion models can be repurposed for visual in-context learning (V-ICL). Specifically, we formulate an in-place attention re-computation within the self-attention layers of the Stable Diffusion architecture that explicitly incorporates context between the query and example prompts. Without any additional fine-tuning, we show that this repurposed Stable Diffusion model is able to adapt to six different tasks: foreground segmentation, single object detection, semantic segmentation, keypoint detection, edge detection, and colorization. For example, the proposed approach improves the mean intersection over union (mIoU) for the foreground segmentation task on Pascal-5i dataset by 8.9% and 3.2% over recent methods such as Visual Prompting and IMProv, respectively. Additionally, we show that the proposed method is able to effectively leverage multiple prompts through ensembling to infer the task better and further improve the performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u73b0\u6210\u7684Stable Diffusion\u6a21\u578b\u8fdb\u884c\u89c6\u89c9\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08V-ICL\uff09\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\u5373\u53ef\u9002\u5e94\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u7b80\u5316\u89c6\u89c9\u4e0a\u4e0b\u6587\u5b66\u4e60\u8fc7\u7a0b\uff0c\u907f\u514d\u590d\u6742\u7684\u8bad\u7ec3\u548c\u989d\u5916\u6570\u636e\u9700\u6c42\uff0c\u63d0\u9ad8\u901a\u7528\u6027\u3002", "method": "\u5728Stable Diffusion\u7684\u81ea\u6ce8\u610f\u529b\u5c42\u4e2d\u91cd\u65b0\u8ba1\u7b97\u6ce8\u610f\u529b\uff0c\u663e\u5f0f\u7ed3\u5408\u67e5\u8be2\u548c\u793a\u4f8b\u63d0\u793a\u7684\u4e0a\u4e0b\u6587\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u516d\u4e2a\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f8b\u5982\u5728Pascal-5i\u6570\u636e\u96c6\u4e0a\uff0c\u524d\u666f\u5206\u5272\u4efb\u52a1\u7684mIoU\u5206\u522b\u6bd4Visual Prompting\u548cIMProv\u63d0\u9ad8\u4e868.9%\u548c3.2%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u73b0\u6210\u7684Stable Diffusion\u6a21\u578b\u53ef\u901a\u8fc7\u7b80\u5355\u7684\u6ce8\u610f\u529b\u91cd\u8ba1\u7b97\u5b9e\u73b0\u9ad8\u6548\u7684\u89c6\u89c9\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u4e14\u80fd\u901a\u8fc7\u96c6\u6210\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2508.09973", "pdf": "https://arxiv.org/pdf/2508.09973", "abs": "https://arxiv.org/abs/2508.09973", "authors": ["Geonhee Sim", "Gyeongsik Moon"], "title": "PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations from a Single Image", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025. https://mks0601.github.io/PERSONA/", "summary": "Two major approaches exist for creating animatable human avatars. The first, a 3D-based approach, optimizes a NeRF- or 3DGS-based avatar from videos of a single person, achieving personalization through a disentangled identity representation. However, modeling pose-driven deformations, such as non-rigid cloth deformations, requires numerous pose-rich videos, which are costly and impractical to capture in daily life. The second, a diffusion-based approach, learns pose-driven deformations from large-scale in-the-wild videos but struggles with identity preservation and pose-dependent identity entanglement. We present PERSONA, a framework that combines the strengths of both approaches to obtain a personalized 3D human avatar with pose-driven deformations from a single image. PERSONA leverages a diffusion-based approach to generate pose-rich videos from the input image and optimizes a 3D avatar based on them. To ensure high authenticity and sharp renderings across diverse poses, we introduce balanced sampling and geometry-weighted optimization. Balanced sampling oversamples the input image to mitigate identity shifts in diffusion-generated training videos. Geometry-weighted optimization prioritizes geometry constraints over image loss, preserving rendering quality in diverse poses.", "AI": {"tldr": "PERSONA\u6846\u67b6\u7ed3\u54083D\u548c\u6269\u6563\u65b9\u6cd5\uff0c\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u4e2a\u6027\u53163D\u4eba\u4f53\u5316\u8eab\uff0c\u89e3\u51b3\u4e86\u8eab\u4efd\u4fdd\u6301\u548c\u59ff\u6001\u53d8\u5f62\u95ee\u9898\u3002", "motivation": "\u73b0\u67093D\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u59ff\u6001\u4e30\u5bcc\u7684\u89c6\u9891\uff0c\u800c\u6269\u6563\u65b9\u6cd5\u96be\u4ee5\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u3002PERSONA\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "method": "\u5229\u7528\u6269\u6563\u65b9\u6cd5\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u59ff\u6001\u4e30\u5bcc\u7684\u89c6\u9891\uff0c\u5e76\u901a\u8fc7\u5e73\u8861\u91c7\u6837\u548c\u51e0\u4f55\u52a0\u6743\u4f18\u5316\u4f18\u53163D\u5316\u8eab\u3002", "result": "\u5b9e\u73b0\u4e86\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u8eab\u4efd\u4e00\u81f4\u76843D\u4eba\u4f53\u5316\u8eab\uff0c\u652f\u6301\u59ff\u6001\u9a71\u52a8\u53d8\u5f62\u3002", "conclusion": "PERSONA\u6210\u529f\u7ed3\u5408\u4e24\u79cd\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u8eab\u4efd\u4fdd\u6301\u548c\u59ff\u6001\u53d8\u5f62\u7684\u6311\u6218\u3002"}}
{"id": "2508.09977", "pdf": "https://arxiv.org/pdf/2508.09977", "abs": "https://arxiv.org/abs/2508.09977", "authors": ["Shuting He", "Peilin Ji", "Yitong Yang", "Changshuo Wang", "Jiayi Ji", "Yinglin Wang", "Henghui Ding"], "title": "A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation", "categories": ["cs.CV"], "comment": "GitHub Repo:   https://github.com/heshuting555/Awesome-3DGS-Applications", "summary": "3D Gaussian Splatting (3DGS) has recently emerged as a powerful alternative to Neural Radiance Fields (NeRF) for 3D scene representation, offering high-fidelity photorealistic rendering with real-time performance. Beyond novel view synthesis, the explicit and compact nature of 3DGS enables a wide range of downstream applications that require geometric and semantic understanding. This survey provides a comprehensive overview of recent progress in 3DGS applications. It first introduces 2D foundation models that support semantic understanding and control in 3DGS applications, followed by a review of NeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGS applications into segmentation, editing, generation, and other functional tasks. For each, we summarize representative methods, supervision strategies, and learning paradigms, highlighting shared design principles and emerging trends. Commonly used datasets and evaluation protocols are also summarized, along with comparative analyses of recent methods across public benchmarks. To support ongoing research and development, a continually updated repository of papers, code, and resources is maintained at https://github.com/heshuting555/Awesome-3DGS-Applications.", "AI": {"tldr": "3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u4f5c\u4e3aNeRF\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u63d0\u4f9b\u9ad8\u4fdd\u771f\u5b9e\u65f6\u6e32\u67d3\uff0c\u5e76\u652f\u6301\u591a\u79cd\u4e0b\u6e38\u5e94\u7528\u3002\u672c\u6587\u7efc\u8ff0\u4e863DGS\u5728\u5206\u5272\u3001\u7f16\u8f91\u3001\u751f\u6210\u7b49\u4efb\u52a1\u4e2d\u7684\u8fdb\u5c55\uff0c\u603b\u7ed3\u4e86\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\u3002", "motivation": "\u63a2\u7d223DGS\u57283D\u573a\u666f\u8868\u793a\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u51e0\u4f55\u548c\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u7684\u5e94\u7528\u3002", "method": "\u7efc\u8ff0\u4e863DGS\u7684\u5e94\u7528\uff0c\u5305\u62ec2D\u57fa\u7840\u6a21\u578b\u3001NeRF\u65b9\u6cd5\u7684\u501f\u9274\uff0c\u4ee5\u53ca\u5206\u7c7b\u8ba8\u8bba\u5206\u5272\u3001\u7f16\u8f91\u3001\u751f\u6210\u7b49\u529f\u80fd\u4efb\u52a1\u3002", "result": "\u603b\u7ed3\u4e86\u4ee3\u8868\u6027\u65b9\u6cd5\u3001\u76d1\u7763\u7b56\u7565\u548c\u5b66\u4e60\u8303\u5f0f\uff0c\u5e76\u6bd4\u8f83\u4e86\u516c\u5f00\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "3DGS\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u672a\u6765\u7814\u7a76\u53ef\u901a\u8fc7\u6301\u7eed\u66f4\u65b0\u7684\u8d44\u6e90\u5e93\u8fdb\u4e00\u6b65\u63a8\u52a8\u53d1\u5c55\u3002"}}
{"id": "2508.09987", "pdf": "https://arxiv.org/pdf/2508.09987", "abs": "https://arxiv.org/abs/2508.09987", "authors": ["Junyan Ye", "Dongzhi Jiang", "Zihao Wang", "Leqi Zhu", "Zhenghao Hu", "Zilong Huang", "Jun He", "Zhiyuan Yan", "Jinghua Yu", "Hongsheng Li", "Conghui He", "Weijia Li"], "title": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "19 pages, 8 figures", "summary": "Recently, GPT-4o has garnered significant attention for its strong performance in image generation, yet open-source models still lag behind. Several studies have explored distilling image data from GPT-4o to enhance open-source models, achieving notable progress. However, a key question remains: given that real-world image datasets already constitute a natural source of high-quality data, why should we use GPT-4o-generated synthetic data? In this work, we identify two key advantages of synthetic images. First, they can complement rare scenarios in real-world datasets, such as surreal fantasy or multi-reference image generation, which frequently occur in user queries. Second, they provide clean and controllable supervision. Real-world data often contains complex background noise and inherent misalignment between text descriptions and image content, whereas synthetic images offer pure backgrounds and long-tailed supervision signals, facilitating more accurate text-to-image alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale synthetic dataset generated by GPT-4o, harnessing the power of synthetic image data to address blind spots in real-world coverage. Using this dataset, we fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o. In addition, we propose two new evaluation benchmarks for a more accurate and challenging assessment of image generation capabilities: GenEval++, which increases instruction complexity to mitigate score saturation, and Imagine-Bench, which focuses on evaluating both the understanding and generation of imaginative content. Echo-4o demonstrates strong performance across standard benchmarks. Moreover, applying Echo-4o-Image to other foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains across multiple metrics, highlighting the datasets strong transferability.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86GPT-4o\u751f\u6210\u7684\u5408\u6210\u56fe\u50cf\u6570\u636e\u5728\u8865\u5145\u73b0\u5b9e\u6570\u636e\u96c6\u4e0d\u8db3\u548c\u63d0\u4f9b\u5e72\u51c0\u76d1\u7763\u4fe1\u53f7\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u5e76\u63d0\u51fa\u4e86Echo-4o-Image\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u65b0\u8bc4\u4f30\u57fa\u51c6\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u56fe\u50cf\u6570\u636e\u96c6\u5b58\u5728\u7a00\u6709\u573a\u666f\u8986\u76d6\u4e0d\u8db3\u548c\u566a\u58f0\u95ee\u9898\uff0c\u800cGPT-4o\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u53ef\u4ee5\u5f25\u8865\u8fd9\u4e9b\u7f3a\u9677\u3002", "method": "\u5229\u7528GPT-4o\u751f\u6210180K\u89c4\u6a21\u7684\u5408\u6210\u6570\u636e\u96c6Echo-4o-Image\uff0c\u5e76\u57fa\u4e8e\u6b64\u5fae\u8c03Bagel\u6a21\u578b\u5f97\u5230Echo-4o\u3002\u540c\u65f6\u63d0\u51faGenEval++\u548cImagine-Bench\u4e24\u4e2a\u65b0\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "Echo-4o\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14Echo-4o-Image\u6570\u636e\u96c6\u5bf9\u5176\u4ed6\u57fa\u7840\u6a21\u578b\u4e5f\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u5408\u6210\u56fe\u50cf\u6570\u636e\u5728\u8865\u5145\u73b0\u5b9e\u6570\u636e\u96c6\u548c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u6f5c\u529b\u3002"}}
{"id": "2508.09145", "pdf": "https://arxiv.org/pdf/2508.09145", "abs": "https://arxiv.org/abs/2508.09145", "authors": ["Xingle Xu", "Yongkang Liu", "Dexian Cai", "Shi Feng", "Xiaocui Yang", "Daling Wang", "Yifei Zhang"], "title": "MoLAN: A Unified Modality-Aware Noise Dynamic Editing Framework for Multimodal Sentiment Analysis", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Multimodal Sentiment Analysis aims to integrate information from various modalities, such as audio, visual, and text, to make complementary predictions. However, it often struggles with irrelevant or misleading visual and auditory information. Most existing approaches typically treat the entire modality information (e.g., a whole image, audio segment, or text paragraph) as an independent unit for feature enhancement or denoising. They often suppress the redundant and noise information at the risk of losing critical information. To address this challenge, we propose MoLAN, a unified ModaLity-aware noise dynAmic editiNg framework. Specifically, MoLAN performs modality-aware blocking by dividing the features of each modality into multiple blocks. Each block is then dynamically assigned a distinct denoising strength based on its noise level and semantic relevance, enabling fine-grained noise suppression while preserving essential multimodal information. Notably, MoLAN is a unified and flexible framework that can be seamlessly integrated into a wide range of multimodal models. Building upon this framework, we further introduce MoLAN+, a new multimodal sentiment analysis approach. Experiments across five models and four datasets demonstrate the broad effectiveness of the MoLAN framework. Extensive evaluations show that MoLAN+ achieves the state-of-the-art performance. The code is publicly available at https://github.com/betterfly123/MoLAN-Framework.", "AI": {"tldr": "MoLAN\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u611f\u77e5\u7684\u52a8\u6001\u566a\u58f0\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5757\u5904\u7406\u548c\u52a8\u6001\u53bb\u566a\u5f3a\u5ea6\u5206\u914d\uff0c\u6709\u6548\u6291\u5236\u566a\u58f0\u5e76\u4fdd\u7559\u5173\u952e\u4fe1\u606f\u3002MoLAN+\u5728\u6b64\u57fa\u7840\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u56e0\u65e0\u5173\u6216\u8bef\u5bfc\u6027\u89c6\u89c9\u548c\u542c\u89c9\u4fe1\u606f\u5bfc\u81f4\u7684\u95ee\u9898\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u56e0\u6574\u4f53\u5904\u7406\u800c\u4e22\u5931\u5173\u952e\u4fe1\u606f\u3002", "method": "\u63d0\u51faMoLAN\u6846\u67b6\uff0c\u5c06\u6bcf\u4e2a\u6a21\u6001\u7279\u5f81\u5206\u5757\uff0c\u6839\u636e\u566a\u58f0\u6c34\u5e73\u548c\u8bed\u4e49\u76f8\u5173\u6027\u52a8\u6001\u5206\u914d\u53bb\u566a\u5f3a\u5ea6\u3002MoLAN+\u662f\u57fa\u4e8e\u6b64\u6846\u67b6\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u5728\u4e94\u4e2a\u6a21\u578b\u548c\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86MoLAN\u7684\u5e7f\u6cdb\u6709\u6548\u6027\uff0cMoLAN+\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MoLAN\u6846\u67b6\u7075\u6d3b\u7edf\u4e00\uff0c\u53ef\u96c6\u6210\u5230\u591a\u79cd\u591a\u6a21\u6001\u6a21\u578b\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u7684\u6548\u679c\u3002"}}
{"id": "2508.09177", "pdf": "https://arxiv.org/pdf/2508.09177", "abs": "https://arxiv.org/abs/2508.09177", "authors": ["Xuanru Zhou", "Cheng Li", "Shuqiang Wang", "Ye Li", "Tao Tan", "Hairong Zheng", "Shanshan Wang"], "title": "Generative Artificial Intelligence in Medical Imaging: Foundations, Progress, and Clinical Translation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Generative artificial intelligence (AI) is rapidly transforming medical imaging by enabling capabilities such as data synthesis, image enhancement, modality translation, and spatiotemporal modeling. This review presents a comprehensive and forward-looking synthesis of recent advances in generative modeling including generative adversarial networks (GANs), variational autoencoders (VAEs), diffusion models, and emerging multimodal foundation architectures and evaluates their expanding roles across the clinical imaging continuum. We systematically examine how generative AI contributes to key stages of the imaging workflow, from acquisition and reconstruction to cross-modality synthesis, diagnostic support, and treatment planning. Emphasis is placed on both retrospective and prospective clinical scenarios, where generative models help address longstanding challenges such as data scarcity, standardization, and integration across modalities. To promote rigorous benchmarking and translational readiness, we propose a three-tiered evaluation framework encompassing pixel-level fidelity, feature-level realism, and task-level clinical relevance. We also identify critical obstacles to real-world deployment, including generalization under domain shift, hallucination risk, data privacy concerns, and regulatory hurdles. Finally, we explore the convergence of generative AI with large-scale foundation models, highlighting how this synergy may enable the next generation of scalable, reliable, and clinically integrated imaging systems. By charting technical progress and translational pathways, this review aims to guide future research and foster interdisciplinary collaboration at the intersection of AI, medicine, and biomedical engineering.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u751f\u6210\u5f0fAI\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u8fdb\u5c55\uff0c\u5305\u62ecGANs\u3001VAEs\u3001\u6269\u6563\u6a21\u578b\u7b49\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u4e34\u5e8a\u5de5\u4f5c\u6d41\u4e2d\u7684\u5e94\u7528\u3001\u6311\u6218\u53ca\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u6f5c\u529b\u5de8\u5927\uff0c\u53ef\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u3001\u6807\u51c6\u5316\u548c\u591a\u6a21\u6001\u6574\u5408\u7b49\u957f\u671f\u6311\u6218\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u4e86\u751f\u6210\u5f0fAI\u5728\u5f71\u50cf\u5de5\u4f5c\u6d41\u5404\u9636\u6bb5\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e09\u5c42\u6b21\u8bc4\u4f30\u6846\u67b6\uff08\u50cf\u7d20\u7ea7\u3001\u7279\u5f81\u7ea7\u3001\u4efb\u52a1\u7ea7\uff09\u3002", "result": "\u751f\u6210\u5f0fAI\u5728\u6570\u636e\u5408\u6210\u3001\u56fe\u50cf\u589e\u5f3a\u3001\u6a21\u6001\u8f6c\u6362\u7b49\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4ecd\u9762\u4e34\u6cdb\u5316\u6027\u3001\u5e7b\u89c9\u98ce\u9669\u3001\u9690\u79c1\u548c\u76d1\u7ba1\u7b49\u6311\u6218\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u4e0e\u5927\u578b\u57fa\u7840\u6a21\u578b\u7684\u7ed3\u5408\u6709\u671b\u63a8\u52a8\u4e0b\u4e00\u4ee3\u53ef\u6269\u5c55\u3001\u53ef\u9760\u4e14\u4e34\u5e8a\u96c6\u6210\u7684\u5f71\u50cf\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2508.09225", "pdf": "https://arxiv.org/pdf/2508.09225", "abs": "https://arxiv.org/abs/2508.09225", "authors": ["Nak-Jun Sung", "Donghyun Lee", "Bo Hwa Choi", "Chae Jung Park"], "title": "AMRG: Extend Vision Language Models for Automatic Mammography Report Generation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Mammography report generation is a critical yet underexplored task in medical AI, characterized by challenges such as multiview image reasoning, high-resolution visual cues, and unstructured radiologic language. In this work, we introduce AMRG (Automatic Mammography Report Generation), the first end-to-end framework for generating narrative mammography reports using large vision-language models (VLMs). Building upon MedGemma-4B-it-a domain-specialized, instruction-tuned VLM-we employ a parameter-efficient fine-tuning (PEFT) strategy via Low-Rank Adaptation (LoRA), enabling lightweight adaptation with minimal computational overhead. We train and evaluate AMRG on DMID, a publicly available dataset of paired high-resolution mammograms and diagnostic reports. This work establishes the first reproducible benchmark for mammography report generation, addressing a longstanding gap in multimodal clinical AI. We systematically explore LoRA hyperparameter configurations and conduct comparative experiments across multiple VLM backbones, including both domain-specific and general-purpose models under a unified tuning protocol. Our framework demonstrates strong performance across both language generation and clinical metrics, achieving a ROUGE-L score of 0.5691, METEOR of 0.6152, CIDEr of 0.5818, and BI-RADS accuracy of 0.5582. Qualitative analysis further highlights improved diagnostic consistency and reduced hallucinations. AMRG offers a scalable and adaptable foundation for radiology report generation and paves the way for future research in multimodal medical AI.", "AI": {"tldr": "AMRG\u662f\u9996\u4e2a\u57fa\u4e8e\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u4e73\u817aX\u5149\u68c0\u67e5\u62a5\u544a\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u548cLoRA\u6280\u672f\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u9002\u5e94\uff0c\u5e76\u5728\u516c\u5f00\u6570\u636e\u96c6DMID\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u4e73\u817aX\u5149\u68c0\u67e5\u62a5\u544a\u751f\u6210\u662f\u533b\u5b66AI\u4e2d\u91cd\u8981\u4f46\u672a\u5145\u5206\u63a2\u7d22\u7684\u4efb\u52a1\uff0c\u9762\u4e34\u591a\u89c6\u56fe\u56fe\u50cf\u63a8\u7406\u3001\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u7ebf\u7d22\u548c\u975e\u7ed3\u6784\u5316\u653e\u5c04\u5b66\u8bed\u8a00\u7b49\u6311\u6218\u3002", "method": "\u57fa\u4e8eMedGemma-4B-it\u6a21\u578b\uff0c\u91c7\u7528LoRA\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u8bad\u7ec3\u548c\u8bc4\u4f30\u5728DMID\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\uff0c\u63a2\u7d22\u4e86\u591a\u79cdVLM\u4e3b\u5e72\u548cLoRA\u8d85\u53c2\u6570\u914d\u7f6e\u3002", "result": "\u5728\u8bed\u8a00\u751f\u6210\u548c\u4e34\u5e8a\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cROUGE-L\u4e3a0.5691\uff0cMETEOR\u4e3a0.6152\uff0cCIDEr\u4e3a0.5818\uff0cBI-RADS\u51c6\u786e\u7387\u4e3a0.5582\uff0c\u8bca\u65ad\u4e00\u81f4\u6027\u548c\u5e7b\u89c9\u51cf\u5c11\u3002", "conclusion": "AMRG\u4e3a\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u9002\u5e94\u6027\u5f3a\u7684\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u533b\u5b66AI\u7684\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2508.09855", "pdf": "https://arxiv.org/pdf/2508.09855", "abs": "https://arxiv.org/abs/2508.09855", "authors": ["Yuekun Wu", "Yik Lung Pang", "Andrea Cavallaro", "Changjae Oh"], "title": "Toward Human-Robot Teaming: Learning Handover Behaviors from 3D Scenes", "categories": ["cs.RO", "cs.CV", "cs.HC"], "comment": "3 pages, 3 figures", "summary": "Human-robot teaming (HRT) systems often rely on large-scale datasets of human and robot interactions, especially for close-proximity collaboration tasks such as human-robot handovers. Learning robot manipulation policies from raw, real-world image data requires a large number of robot-action trials in the physical environment. Although simulation training offers a cost-effective alternative, the visual domain gap between simulation and robot workspace remains a major limitation. We introduce a method for training HRT policies, focusing on human-to-robot handovers, solely from RGB images without the need for real-robot training or real-robot data collection. The goal is to enable the robot to reliably receive objects from a human with stable grasping while avoiding collisions with the human hand. The proposed policy learner leverages sparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes to generate robot demonstrations containing image-action pairs captured with a camera mounted on the robot gripper. As a result, the simulated camera pose changes in the reconstructed scene can be directly translated into gripper pose changes. Experiments in both Gaussian Splatting reconstructed scene and real-world human-to-robot handover experiments demonstrate that our method serves as a new and effective representation for the human-to-robot handover task, contributing to more seamless and robust HRT.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ec5\u4eceRGB\u56fe\u50cf\u8bad\u7ec3\u4eba\u673a\u534f\u4f5c\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\uff0c\u5229\u7528\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u751f\u6210\u6f14\u793a\uff0c\u5b9e\u73b0\u7a33\u5b9a\u6293\u53d6\u548c\u907f\u78b0\u3002", "motivation": "\u89e3\u51b3\u4eba\u673a\u534f\u4f5c\u4e2d\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u53ca\u4eff\u771f\u4e0e\u771f\u5b9e\u89c6\u89c9\u57df\u5dee\u8ddd\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u7a00\u758f\u89c6\u56fe\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u4eba\u673a\u4ea4\u63a5\u573a\u666f\uff0c\u751f\u6210\u56fe\u50cf-\u52a8\u4f5c\u5bf9\u6f14\u793a\uff0c\u6a21\u62df\u76f8\u673a\u59ff\u6001\u53d8\u5316\u8f6c\u6362\u4e3a\u5939\u722a\u59ff\u6001\u53d8\u5316\u3002", "result": "\u5728\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u573a\u666f\u548c\u771f\u5b9e\u4eba\u673a\u4ea4\u63a5\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4eba\u673a\u4ea4\u63a5\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u8868\u793a\uff0c\u4fc3\u8fdb\u4e86\u66f4\u65e0\u7f1d\u548c\u9c81\u68d2\u7684\u4eba\u673a\u534f\u4f5c\u3002"}}
{"id": "2508.09919", "pdf": "https://arxiv.org/pdf/2508.09919", "abs": "https://arxiv.org/abs/2508.09919", "authors": ["Xiaojiao Xiao", "Jianfeng Zhao", "Qinmin Vivian Hu", "Guanghui Wang"], "title": "T-CACE: A Time-Conditioned Autoregressive Contrast Enhancement Multi-Task Framework for Contrast-Free Liver MRI Synthesis, Segmentation, and Diagnosis", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "IEEE Journal of Biomedical and Health Informatics, 2025", "summary": "Magnetic resonance imaging (MRI) is a leading modality for the diagnosis of liver cancer, significantly improving the classification of the lesion and patient outcomes. However, traditional MRI faces challenges including risks from contrast agent (CA) administration, time-consuming manual assessment, and limited annotated datasets. To address these limitations, we propose a Time-Conditioned Autoregressive Contrast Enhancement (T-CACE) framework for synthesizing multi-phase contrast-enhanced MRI (CEMRI) directly from non-contrast MRI (NCMRI). T-CACE introduces three core innovations: a conditional token encoding (CTE) mechanism that unifies anatomical priors and temporal phase information into latent representations; and a dynamic time-aware attention mask (DTAM) that adaptively modulates inter-phase information flow using a Gaussian-decayed attention mechanism, ensuring smooth and physiologically plausible transitions across phases. Furthermore, a constraint for temporal classification consistency (TCC) aligns the lesion classification output with the evolution of the physiological signal, further enhancing diagnostic reliability. Extensive experiments on two independent liver MRI datasets demonstrate that T-CACE outperforms state-of-the-art methods in image synthesis, segmentation, and lesion classification. This framework offers a clinically relevant and efficient alternative to traditional contrast-enhanced imaging, improving safety, diagnostic efficiency, and reliability for the assessment of liver lesion. The implementation of T-CACE is publicly available at: https://github.com/xiaojiao929/T-CACE.", "AI": {"tldr": "T-CACE\u6846\u67b6\u901a\u8fc7\u5408\u6210\u591a\u671f\u5bf9\u6bd4\u589e\u5f3aMRI\uff08CEMRI\uff09\u6765\u66ff\u4ee3\u4f20\u7edf\u5bf9\u6bd4\u5242\u589e\u5f3aMRI\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfMRI\u7684\u5c40\u9650\u6027\uff0c\u5305\u62ec\u5bf9\u6bd4\u5242\u98ce\u9669\u3001\u624b\u52a8\u8bc4\u4f30\u8017\u65f6\u548c\u6807\u6ce8\u6570\u636e\u6709\u9650\u3002", "motivation": "\u4f20\u7edfMRI\u5728\u809d\u764c\u8bca\u65ad\u4e2d\u5b58\u5728\u5bf9\u6bd4\u5242\u98ce\u9669\u3001\u8017\u65f6\u548c\u6570\u636e\u96c6\u6709\u9650\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5b89\u5168\u3001\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "T-CACE\u6846\u67b6\u7ed3\u5408\u6761\u4ef6\u4ee4\u724c\u7f16\u7801\uff08CTE\uff09\u3001\u52a8\u6001\u65f6\u95f4\u611f\u77e5\u6ce8\u610f\u529b\u63a9\u7801\uff08DTAM\uff09\u548c\u65f6\u95f4\u5206\u7c7b\u4e00\u81f4\u6027\u7ea6\u675f\uff08TCC\uff09\uff0c\u76f4\u63a5\u4ece\u975e\u5bf9\u6bd4MRI\u5408\u6210\u591a\u671fCEMRI\u3002", "result": "\u5728\u4e24\u4e2a\u72ec\u7acb\u809d\u810fMRI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cT-CACE\u5728\u56fe\u50cf\u5408\u6210\u3001\u5206\u5272\u548c\u75c5\u7076\u5206\u7c7b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "T-CACE\u4e3a\u809d\u810f\u75c5\u53d8\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e34\u5e8a\u76f8\u5173\u4e14\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u3001\u8bca\u65ad\u6548\u7387\u548c\u53ef\u9760\u6027\u3002"}}
