<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 4]
- [cs.CV](#cs.CV) [Total: 26]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [MoReFlow: Motion Retargeting Learning through Unsupervised Flow Matching](https://arxiv.org/abs/2509.25600)
*Wontaek Kim,Tianyu Li,Sehoon Ha*

Main category: cs.GR

TL;DR: MoReFlow是一个基于流匹配的无监督运动重定向框架，通过VQ-VAE学习运动嵌入，然后使用条件耦合流匹配对齐不同角色的潜在空间，实现灵活可逆的重定向。


<details>
  <summary>Details</summary>
Motivation: 现有运动重定向方法依赖手工约束或配对数据集，仅适用于人形角色或狭窄行为，且缺乏对领域特定目标（如动画风格保持或机器人任务空间对齐）的考虑。

Method: 两阶段方法：1）使用VQ-VAE训练每个角色的标记化运动嵌入；2）使用条件耦合流匹配对齐角色间的潜在空间，同时学习条件和非条件匹配以实现鲁棒灵活的重定向。

Result: 实验表明MoReFlow能在多样化角色和任务上生成高质量运动，相比基线方法具有更好的可控性、泛化能力和运动真实感。

Conclusion: MoReFlow无需配对数据即可实现灵活可逆的运动重定向，为不同形态的角色和机器人提供了更广泛适用的解决方案。

Abstract: Motion retargeting holds a premise of offering a larger set of motion data for characters and robots with different morphologies. Many prior works have approached this problem via either handcrafted constraints or paired motion datasets, limiting their applicability to humanoid characters or narrow behaviors such as locomotion. Moreover, they often assume a fixed notion of retargeting, overlooking domain-specific objectives like style preservation in animation or task-space alignment in robotics. In this work, we propose MoReFlow, Motion Retargeting via Flow Matching, an unsupervised framework that learns correspondences between characters' motion embedding spaces. Our method consists of two stages. First, we train tokenized motion embeddings for each character using a VQ-VAE, yielding compact latent representations. Then, we employ flow matching with conditional coupling to align the latent spaces across characters, which simultaneously learns conditioned and unconditioned matching to achieve robust but flexible retargeting. Once trained, MoReFlow enables flexible and reversible retargeting without requiring paired data. Experiments demonstrate that MoReFlow produces high-quality motions across diverse characters and tasks, offering improved controllability, generalization, and motion realism compared to the baselines.

</details>


### [2] [GaussEdit: Adaptive 3D Scene Editing with Text and Image Prompts](https://arxiv.org/abs/2509.26055)
*Zhenyu Shu,Junlong Yu,Kai Chao,Shiqing Xin,Ligang Liu*

Main category: cs.GR

TL;DR: GaussEdit是一个基于3D高斯泼溅的自适应3D场景编辑框架，通过文本和图像提示指导编辑过程，采用三阶段优化策略实现高质量的场景编辑。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统3D场景编辑方法在编辑精度、视觉保真度和处理速度方面的不足，开发一个能够有效嵌入用户指定概念到3D场景中的强大工具。

Method: 使用3D高斯泼溅作为场景表示骨干，采用三阶段流程：1) 初始化3D高斯确保高质量编辑；2) 自适应全局-局部优化策略平衡场景一致性和局部细节；3) 使用图像到图像合成技术增强编辑对象纹理。

Result: 实验结果表明GaussEdit在编辑精度、视觉保真度和处理速度方面均优于现有方法。

Conclusion: GaussEdit是一个功能强大的3D场景编辑工具，通过成功嵌入用户指定概念到3D场景中，相比传统方法有显著改进。

Abstract: This paper presents GaussEdit, a framework for adaptive 3D scene editing guided by text and image prompts. GaussEdit leverages 3D Gaussian Splatting as its backbone for scene representation, enabling convenient Region of Interest selection and efficient editing through a three-stage process. The first stage involves initializing the 3D Gaussians to ensure high-quality edits. The second stage employs an Adaptive Global-Local Optimization strategy to balance global scene coherence and detailed local edits and a category-guided regularization technique to alleviate the Janus problem. The final stage enhances the texture of the edited objects using a sophisticated image-to-image synthesis technique, ensuring that the results are visually realistic and align closely with the given prompts. Our experimental results demonstrate that GaussEdit surpasses existing methods in editing accuracy, visual fidelity, and processing speed. By successfully embedding user-specified concepts into 3D scenes, GaussEdit is a powerful tool for detailed and user-driven 3D scene editing, offering significant improvements over traditional methods.

</details>


### [3] [Palace: A Library for Interactive GPU-Accelerated Large Tensor Processing and Visualization](https://arxiv.org/abs/2509.26213)
*Dominik Drees,Benjamin Risse*

Main category: cs.GR

TL;DR: Palace是一个开源的跨平台通用库，用于交互式和加速的核外张量处理和可视化，通过高性能异步并发架构和简单计算图接口实现工作站硬件上的交互式核外管道开发。


<details>
  <summary>Details</summary>
Motivation: 随着成像和仿真技术的发展，张量数据集规模不断增大，需要核外处理和可视化技术，但现有解决方案多为专用方案。

Method: 采用高性能异步并发架构和简单计算图接口，支持交互式核外管道开发。

Result: 在体积渲染和分层随机游走分割基准测试中优于或匹配最先进系统，适用于从2D图像到4D时间序列数据集的张量处理。

Conclusion: Palace提供了一个通用的核外张量处理和可视化解决方案，在多个应用场景中表现出色。

Abstract: Tensor datasets (two-, three-, or higher-dimensional) are fundamental to many scientific fields utilizing imaging or simulation technologies. Advances in these methods have led to ever-increasing data sizes and, consequently, interest and development of out-of-core processing and visualization techniques, although mostly as specialized solutions. Here we present Palace, an open-source, cross-platform, general-purpose library for interactive and accelerated out-of-core tensor processing and visualization. Through a high-performance asynchronous concurrent architecture and a simple compute-graph interface, Palace enables the interactive development of out-of-core pipelines on workstation hardware. We demonstrate on benchmarks that Palace outperforms or matches state-of-the-art systems for volume rendering and hierarchical random-walker segmentation and demonstrate applicability in use cases involving tensors from 2D images up to 4D time series datasets.

</details>


### [4] [3DiFACE: Synthesizing and Editing Holistic 3D Facial Animation](https://arxiv.org/abs/2509.26233)
*Balamurugan Thambiraja,Malte Prinzler,Sadegh Aliakbarian,Darren Cosker,Justus Thies*

Main category: cs.GR

TL;DR: 3DiFACE是一个用于整体语音驱动3D面部动画的新方法，能够为单一音频输入生成多样化的唇部和头部运动，并支持通过关键帧和插值进行编辑。


<details>
  <summary>Details</summary>
Motivation: 当前语音驱动3D面部动画方法在创建个性化动画时面临精确控制和真实头部运动不足的挑战，编辑复杂耗时，且现有方法无法编辑/重新生成部分输入动画，忽略了同一音频输入可能对应多种合理的唇部和头部运动。

Method: 提出完全卷积的扩散模型，利用训练语料库中的音素级多样性；采用说话风格个性化和新颖的稀疏引导运动扩散来实现精确控制和编辑。

Result: 通过定量和定性评估证明，该方法能够在单一音频输入下生成和编辑多样化的整体3D面部动画，并在高保真度和多样性之间实现控制。

Conclusion: 3DiFACE解决了现有方法的局限性，提供了更灵活和可控的3D面部动画生成和编辑能力。

Abstract: Creating personalized 3D animations with precise control and realistic head motions remains challenging for current speech-driven 3D facial animation methods. Editing these animations is especially complex and time consuming, requires precise control and typically handled by highly skilled animators. Most existing works focus on controlling style or emotion of the synthesized animation and cannot edit/regenerate parts of an input animation. They also overlook the fact that multiple plausible lip and head movements can match the same audio input. To address these challenges, we present 3DiFACE, a novel method for holistic speech-driven 3D facial animation. Our approach produces diverse plausible lip and head motions for a single audio input and allows for editing via keyframing and interpolation. Specifically, we propose a fully-convolutional diffusion model that can leverage the viseme-level diversity in our training corpus. Additionally, we employ a speaking-style personalization and a novel sparsely-guided motion diffusion to enable precise control and editing. Through quantitative and qualitative evaluations, we demonstrate that our method is capable of generating and editing diverse holistic 3D facial animations given a single audio input, with control between high fidelity and diversity. Code and models are available here: https://balamuruganthambiraja.github.io/3DiFACE

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [5] [LUMA: Low-Dimension Unified Motion Alignment with Dual-Path Anchoring for Text-to-Motion Diffusion Model](https://arxiv.org/abs/2509.25304)
*Haozhe Jia,Wenshuo Chen,Yuqi Lin,Yang Yang,Lei Wang,Mang Ning,Bowen Tian,Songning Lai,Nanqian Jia,Yifan Chen,Yutao Yue*

Main category: cs.CV

TL;DR: LUMA是一个基于扩散模型的文本到动作生成方法，通过双路径锚定机制解决语义对齐和运动伪影问题，在HumanML3D和KIT-ML数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于U-Net架构的扩散模型在文本到动作生成中存在语义错位和运动伪影问题，分析发现深层网络梯度衰减是主要瓶颈，导致高层特征学习不足。

Method: 提出LUMA模型，包含双路径锚定：1）轻量级MoCLIP模型提供时间域语义监督；2）从低频DCT分量提取频率域对齐信号。通过时间调制机制自适应融合两个锚点，在去噪过程中实现从粗到细的语义对齐。

Result: 在HumanML3D和KIT-ML数据集上分别达到FID 0.035和0.123的SOTA性能，收敛速度比基线快1.4倍。

Conclusion: LUMA通过双路径锚定和时间调制机制有效解决了文本到动作生成中的语义对齐问题，是一个高效可扩展的高保真解决方案。

Abstract: While current diffusion-based models, typically built on U-Net architectures, have shown promising results on the text-to-motion generation task, they still suffer from semantic misalignment and kinematic artifacts. Through analysis, we identify severe gradient attenuation in the deep layers of the network as a key bottleneck, leading to insufficient learning of high-level features. To address this issue, we propose \textbf{LUMA} (\textit{\textbf{L}ow-dimension \textbf{U}nified \textbf{M}otion \textbf{A}lignment}), a text-to-motion diffusion model that incorporates dual-path anchoring to enhance semantic alignment. The first path incorporates a lightweight MoCLIP model trained via contrastive learning without relying on external data, offering semantic supervision in the temporal domain. The second path introduces complementary alignment signals in the frequency domain, extracted from low-frequency DCT components known for their rich semantic content. These two anchors are adaptively fused through a temporal modulation mechanism, allowing the model to progressively transition from coarse alignment to fine-grained semantic refinement throughout the denoising process. Experimental results on HumanML3D and KIT-ML demonstrate that LUMA achieves state-of-the-art performance, with FID scores of 0.035 and 0.123, respectively. Furthermore, LUMA accelerates convergence by 1.4$\times$ compared to the baseline, making it an efficient and scalable solution for high-fidelity text-to-motion generation.

</details>


### [6] [Editing Physiological Signals in Videos Using Latent Representations](https://arxiv.org/abs/2509.25348)
*Tianwen Zhou,Akshay Paruchuri,Josef Spjut,Kaan Akşit*

Main category: cs.CV

TL;DR: 提出了一种学习框架，通过3D变分自编码器和文本提示融合来编辑视频中的生理信号，在保持视觉质量的同时保护隐私。


<details>
  <summary>Details</summary>
Motivation: 基于摄像头的生理信号估计存在隐私泄露风险，因为面部视频中的生命信号可能揭示个人的健康和情绪状态等敏感信息。

Method: 使用预训练的3D变分自编码器将输入视频编码到潜在空间，通过冻结文本编码器嵌入目标心率提示，使用可训练的时空层和自适应层归一化进行融合，在解码器中应用特征级线性调制来避免生理信号退化。

Result: 在选定数据集上平均PSNR为38.96 dB，SSIM为0.98，使用最先进的rPPG估计器实现平均心率调制误差为10.00 bpm MAE和10.09% MAPE。

Conclusion: 该方法实现了可控的心率编辑，可用于匿名化真实视频中的生物特征信号或合成具有所需生命体征的真实视频。

Abstract: Camera-based physiological signal estimation provides a non-contact and convenient means to monitor Heart Rate (HR). However, the presence of vital signals in facial videos raises significant privacy concerns, as they can reveal sensitive personal information related to the health and emotional states of an individual. To address this, we propose a learned framework that edits physiological signals in videos while preserving visual fidelity. First, we encode an input video into a latent space via a pretrained 3D Variational Autoencoder (3D VAE), while a target HR prompt is embedded through a frozen text encoder. We fuse them using a set of trainable spatio-temporal layers with Adaptive Layer Normalizations (AdaLN) to capture the strong temporal coherence of remote Photoplethysmography (rPPG) signals. We apply Feature-wise Linear Modulation (FiLM) in the decoder with a fine-tuned output layer to avoid the degradation of physiological signals during reconstruction, enabling accurate physiological modulation in the reconstructed video. Empirical results show that our method preserves visual quality with an average PSNR of 38.96 dB and SSIM of 0.98 on selected datasets, while achieving an average HR modulation error of 10.00 bpm MAE and 10.09% MAPE using a state-of-the-art rPPG estimator. Our design's controllable HR editing is useful for applications such as anonymizing biometric signals in real videos or synthesizing realistic videos with desired vital signs.

</details>


### [7] [GaussianLens: Localized High-Resolution Reconstruction via On-Demand Gaussian Densification](https://arxiv.org/abs/2509.25603)
*Yijia Weng,Zhicheng Wang,Songyou Peng,Saining Xie,Howard Zhou,Leonidas J. Guibas*

Main category: cs.CV

TL;DR: 提出了GaussianLens方法，通过按需高斯致密化实现局部高分辨率重建，在用户指定的感兴趣区域基于稀疏高分辨率观测来增强初始低分辨率3DGS重建，避免统一高分辨率重建的高成本。


<details>
  <summary>Details</summary>
Motivation: 人类感知具有空间变化的细节关注特性，需要为关键区域提供可检查的精细细节。现有3DGS方法要么输出均匀分辨率导致高计算成本，要么需要密集观测和长时间离线优化。

Method: GaussianLens前向致密化框架，融合初始3DGS和多视图图像的多模态信息，设计像素引导的致密化机制来有效捕捉大分辨率增加下的细节。

Result: 实验表明该方法在局部精细细节重建方面表现优越，对高达1024×1024分辨率的图像具有强可扩展性。

Conclusion: 通过按需局部高分辨率重建，成功解决了高分辨率整体重建的高成本与用户对局部精细细节需求之间的矛盾。

Abstract: We perceive our surroundings with an active focus, paying more attention to regions of interest, such as the shelf labels in a grocery store. When it comes to scene reconstruction, this human perception trait calls for spatially varying degrees of detail ready for closer inspection in critical regions, preferably reconstructed on demand. While recent works in 3D Gaussian Splatting (3DGS) achieve fast, generalizable reconstruction from sparse views, their uniform resolution output leads to high computational costs unscalable to high-resolution training. As a result, they cannot leverage available images at their original high resolution to reconstruct details. Per-scene optimization methods reconstruct finer details with adaptive density control, yet require dense observations and lengthy offline optimization. To bridge the gap between the prohibitive cost of high-resolution holistic reconstructions and the user needs for localized fine details, we propose the problem of localized high-resolution reconstruction via on-demand Gaussian densification. Given a low-resolution 3DGS reconstruction, the goal is to learn a generalizable network that densifies the initial 3DGS to capture fine details in a user-specified local region of interest (RoI), based on sparse high-resolution observations of the RoI. This formulation avoids the high cost and redundancy of uniformly high-resolution reconstructions and fully leverages high-resolution captures in critical regions. We propose GaussianLens, a feed-forward densification framework that fuses multi-modal information from the initial 3DGS and multi-view images. We further design a pixel-guided densification mechanism that effectively captures details under large resolution increases. Experiments demonstrate our method's superior performance in local fine detail reconstruction and strong scalability to images of up to $1024\times1024$ resolution.

</details>


### [8] [How Diffusion Models Memorize](https://arxiv.org/abs/2509.25705)
*Juyeop Kim,Songkuk Kim,Jong-Seok Lee*

Main category: cs.CV

TL;DR: 扩散模型会记忆训练数据，本文通过分析去噪过程中的潜在空间动态，发现记忆主要由早期去噪阶段对训练样本的高估驱动，这降低了多样性、使去噪轨迹坍缩并加速向记忆图像的收敛。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成方面取得成功，但会记忆训练数据，引发隐私和版权问题。现有工作主要关注表征、检测和缓解记忆，但对记忆发生的基本原因和机制仍不清楚。

Method: 重新审视扩散和去噪过程，分析潜在空间动态，研究：(i)记忆不能仅用过拟合解释；(ii)记忆提示如何将训练图像注入噪声预测；(iii)分解中间潜在变量揭示随机性如何被抑制并被记忆内容替代。

Result: 发现早期高估是扩散模型记忆的核心机制：训练损失在记忆时更大；记忆提示强制潜在轨迹收敛；偏离理论去噪计划与记忆严重程度高度相关。

Conclusion: 早期高估是扩散模型中记忆现象的根本驱动机制，这为理解和解决扩散模型的记忆问题提供了新的理论视角。

Abstract: Despite their success in image generation, diffusion models can memorize training data, raising serious privacy and copyright concerns. Although prior work has sought to characterize, detect, and mitigate memorization, the fundamental question of why and how it occurs remains unresolved. In this paper, we revisit the diffusion and denoising process and analyze latent space dynamics to address the question: "How do diffusion models memorize?" We show that memorization is driven by the overestimation of training samples during early denoising, which reduces diversity, collapses denoising trajectories, and accelerates convergence toward the memorized image. Specifically: (i) memorization cannot be explained by overfitting alone, as training loss is larger under memorization due to classifier-free guidance amplifying predictions and inducing overestimation; (ii) memorized prompts inject training images into noise predictions, forcing latent trajectories to converge and steering denoising toward their paired samples; and (iii) a decomposition of intermediate latents reveals how initial randomness is quickly suppressed and replaced by memorized content, with deviations from the theoretical denoising schedule correlating almost perfectly with memorization severity. Together, these results identify early overestimation as the central underlying mechanism of memorization in diffusion models.

</details>


### [9] [LieHMR: Autoregressive Human Mesh Recovery with $SO(3)$ Diffusion](https://arxiv.org/abs/2509.25739)
*Donghwan Kim,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: 提出基于SO(3)扩散模型的人体网格恢复方法，通过条件dropout生成符合2D观测的3D人体姿态分布，解决单目RGB图像中3D人体姿态恢复的模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 解决单目RGB图像中3D人体姿态恢复的固有模糊性问题，现有概率方法在准确性和样本多样性之间存在权衡，且单预测结果不如最先进的确定性模型。

Method: 使用SO(3)扩散模型生成以3D旋转表示的姿态参数分布，通过条件dropout实现无条件和图像条件生成，利用transformer学习人体关节的层次结构，MLP去噪模型学习基于潜在向量的关节分布。

Result: 实验证明该方法能有效预测准确的姿态概率分布。

Conclusion: 提出的SO(3)扩散模型方法能够有效建模与2D观测对齐的3D人体姿态分布，解决了现有概率方法在准确性和多样性之间的权衡问题。

Abstract: We tackle the problem of Human Mesh Recovery (HMR) from a single RGB image, formulating it as an image-conditioned human pose and shape generation. While recovering 3D human pose from 2D observations is inherently ambiguous, most existing approaches have regressed a single deterministic output. Probabilistic methods attempt to address this by generating multiple plausible outputs to model the ambiguity. However, these methods often exhibit a trade-off between accuracy and sample diversity, and their single predictions are not competitive with state-of-the-art deterministic models. To overcome these limitations, we propose a novel approach that models well-aligned distribution to 2D observations. In particular, we introduce $SO(3)$ diffusion model, which generates the distribution of pose parameters represented as 3D rotations unconditional and conditional to image observations via conditioning dropout. Our model learns the hierarchical structure of human body joints using the transformer. Instead of using transformer as a denoising model, the time-independent transformer extracts latent vectors for the joints and a small MLP-based denoising model learns the per-joint distribution conditioned on the latent vector. We experimentally demonstrate and analyze that our model predicts accurate pose probability distribution effectively.

</details>


### [10] [ART-VITON: Measurement-Guided Latent Diffusion for Artifact-Free Virtual Try-On](https://arxiv.org/abs/2509.25749)
*Junseo Park,Hyeryung Jang*

Main category: cs.CV

TL;DR: ART-VITON是一个基于测量引导的扩散框架，用于虚拟试穿任务，通过渐进式测量一致性确保非试穿区域的保真度，消除边界伪影。


<details>
  <summary>Details</summary>
Motivation: 现有的虚拟试穿方法在使用潜在扩散模型时，难以有效保持非试穿区域的身份和背景信息，直接替换策略会导致边界伪影问题。

Method: 将虚拟试穿重新表述为线性逆问题，采用轨迹对齐求解器，结合残差先验初始化和无伪影测量引导采样，包括数据一致性、频率级校正和周期性标准去噪。

Result: 在VITON-HD、DressCode和SHHQ-1.0数据集上的实验表明，ART-VITON能有效保持身份和背景，消除边界伪影，在视觉保真度和鲁棒性上优于现有方法。

Conclusion: ART-VITON通过测量引导的扩散框架成功解决了虚拟试穿中的非试穿区域保真问题，提供了更高质量的试穿效果。

Abstract: Virtual try-on (VITON) aims to generate realistic images of a person wearing a target garment, requiring precise garment alignment in try-on regions and faithful preservation of identity and background in non-try-on regions. While latent diffusion models (LDMs) have advanced alignment and detail synthesis, preserving non-try-on regions remains challenging. A common post-hoc strategy directly replaces these regions with original content, but abrupt transitions often produce boundary artifacts. To overcome this, we reformulate VITON as a linear inverse problem and adopt trajectory-aligned solvers that progressively enforce measurement consistency, reducing abrupt changes in non-try-on regions. However, existing solvers still suffer from semantic drift during generation, leading to artifacts. We propose ART-VITON, a measurement-guided diffusion framework that ensures measurement adherence while maintaining artifact-free synthesis. Our method integrates residual prior-based initialization to mitigate training-inference mismatch and artifact-free measurement-guided sampling that combines data consistency, frequency-level correction, and periodic standard denoising. Experiments on VITON-HD, DressCode, and SHHQ-1.0 demonstrate that ART-VITON effectively preserves identity and background, eliminates boundary artifacts, and consistently improves visual fidelity and robustness over state-of-the-art baselines.

</details>


### [11] [PCPO: Proportionate Credit Policy Optimization for Aligning Image Generation Models](https://arxiv.org/abs/2509.25774)
*Jeongjae Lee,Jong Chul Ye*

Main category: cs.CV

TL;DR: 提出了PCPO框架，通过比例信用分配解决文本到图像模型强化学习训练中的不稳定性和高方差问题，显著提升收敛速度和图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的策略梯度方法在文本到图像模型对齐中受到训练不稳定性和高方差的困扰，这阻碍了收敛速度并损害了图像质量。分析发现主要原因是生成采样器的数学结构导致跨时间步的信用分配不成比例。

Method: 引入比例信用策略优化(PCPO)框架，通过稳定的目标重构和原则性的时间步重新加权来强制执行比例信用分配。

Result: PCPO显著稳定了训练过程，大大加速了收敛并实现了更优的图像质量。通过减轻模型崩溃这一递归训练中的常见故障模式来直接提升质量。

Conclusion: PCPO在所有方面都大幅优于现有的策略梯度基线方法，包括最先进的DanceGRPO。

Abstract: While reinforcement learning has advanced the alignment of text-to-image (T2I) models, state-of-the-art policy gradient methods are still hampered by training instability and high variance, hindering convergence speed and compromising image quality. Our analysis identifies a key cause of this instability: disproportionate credit assignment, in which the mathematical structure of the generative sampler produces volatile and non-proportional feedback across timesteps. To address this, we introduce Proportionate Credit Policy Optimization (PCPO), a framework that enforces proportional credit assignment through a stable objective reformulation and a principled reweighting of timesteps. This correction stabilizes the training process, leading to significantly accelerated convergence and superior image quality. The improvement in quality is a direct result of mitigating model collapse, a common failure mode in recursive training. PCPO substantially outperforms existing policy gradient baselines on all fronts, including the state-of-the-art DanceGRPO.

</details>


### [12] [Editable Noise Map Inversion: Encoding Target-image into Noise For High-Fidelity Image Manipulation](https://arxiv.org/abs/2509.25776)
*Mingyu Kang,Yong Suk Choi*

Main category: cs.CV

TL;DR: 提出ENM Inversion方法，通过搜索最优噪声图来平衡内容保持和编辑能力，解决了现有扩散模型图像编辑方法在遵循目标文本提示方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型图像编辑方法在将源图像反转为可编辑噪声图时，难以紧密遵循目标文本提示，因为反转的噪声图虽然能忠实重建源图像，但限制了编辑灵活性。

Method: 分析噪声图特性以增强可编辑性，提出可编辑噪声优化方法，通过最小化重建和编辑噪声图之间的差异来与期望编辑对齐。

Result: 在广泛的图像编辑任务中，ENM Inversion在内容保持和编辑保真度方面优于现有方法，并能轻松应用于视频编辑，实现跨帧的时间一致性和内容操作。

Conclusion: ENM Inversion是一种有效的噪声图反转技术，能够同时确保内容保持和编辑能力，在文本引导的图像和视频编辑中表现出色。

Abstract: Text-to-image diffusion models have achieved remarkable success in generating high-quality and diverse images. Building on these advancements, diffusion models have also demonstrated exceptional performance in text-guided image editing. A key strategy for effective image editing involves inverting the source image into editable noise maps associated with the target image. However, previous inversion methods face challenges in adhering closely to the target text prompt. The limitation arises because inverted noise maps, while enabling faithful reconstruction of the source image, restrict the flexibility needed for desired edits. To overcome this issue, we propose Editable Noise Map Inversion (ENM Inversion), a novel inversion technique that searches for optimal noise maps to ensure both content preservation and editability. We analyze the properties of noise maps for enhanced editability. Based on this analysis, our method introduces an editable noise refinement that aligns with the desired edits by minimizing the difference between the reconstructed and edited noise maps. Extensive experiments demonstrate that ENM Inversion outperforms existing approaches across a wide range of image editing tasks in both preservation and edit fidelity with target prompts. Our approach can also be easily applied to video editing, enabling temporal consistency and content manipulation across frames.

</details>


### [13] [Adapting SAM with Dynamic Similarity Graphs for Few-Shot Parameter-Efficient Small Dense Object Detection: A Case Study of Chickpea Pods in Field Conditions](https://arxiv.org/abs/2509.25805)
*Xintong Jiang,Yixue Liu,Mohamed Debbagh,Yu Tian,Valerio Hoyos-Villegas,Viacheslav Adamchuk,Shangpeng Sun*

Main category: cs.CV

TL;DR: 提出DSGA模块结合LoRA，在极少量数据下优化SAM模型，实现农业图像中密集小物体的精确分割，仅需4.00M可训练参数，性能显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 农业计算机视觉任务中，基础模型的参数高效微调面临训练数据有限和复杂田间条件的挑战，需要开发在极端数据约束下仍能有效工作的适应方法。

Method: 引入动态相似性图适应(DSGA)模块，通过可学习多项式衰减初始化权重排序机制构建动态相似性图，结合自适应局部特征聚合，并与LoRA集成形成互补优化框架。

Result: 在鹰嘴豆荚数据集上，DSGA+LoRA在2、4、8、10样本设置下均表现优异，结构度量提升17.31%，自适应F度量提升62.36%，在10-120个豆荚的图像中实现准确计数(R²=0.8987)。

Conclusion: 该方法为自动化农业监测提供了实用解决方案，在保持参数效率的同时显著提升了复杂农业环境下小密集物体的分割性能。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) of foundation models for agricultural computer vision tasks remains challenging due to limited training data and complex field conditions. This study introduces a Dynamic Similarity-based Graph Adaptation (DSGA) module to adapt the Segment Anything Model (SAM) under extreme data constraints for precise foreground and instance segmentation of small dense objects in complex agricultural environments. Through dynamic similarity graph construction with a learnable polynomial decay-initialized weight ranking mechanism and adaptive local feature aggregation, DSGA establishes robust spatial and dynamic similarity representation with only 4.00M trainable parameters, which is 4.26% of the original SAM. Integrating this graph-based feature adaptation with Low-Rank Adaptation (LoRA) creates a complementary optimization framework that effectively captures both local and global dependencies in image embeddings while preserving model stability and parameter efficiency. Experimental results on a challenging chickpea pod dataset demonstrated that DSGA with LoRA achieved superior performance across multiple metrics evaluated under 2, 4, 8 and 10 shots, with progressive performance gains as shot count increased. Quantitative metrics showed a 17.31% improvement in Structure-measure and a 62.36% gain in adaptive F-measure compared to the baseline SAM fine-tuning. Comprehensive ablation studies and visualization analyses through Grad-CAM and t-SNE validated the framework's effectiveness in feature discrimination. The proposed adaptation demonstrated practical utility for automated agricultural monitoring applications, achieving accurate pod-counting with an adjusted R-squared of 0.8987 for images with 10 to 120 pods under challenging field conditions.

</details>


### [14] [Training-Free Reward-Guided Image Editing via Trajectory Optimal Control](https://arxiv.org/abs/2509.25845)
*Jinho Chang,Jaemin Kim,Jong Chul Ye*

Main category: cs.CV

TL;DR: 提出了一种无需训练、基于奖励引导的图像编辑框架，将编辑过程建模为轨迹最优控制问题，在保持源图像语义内容的同时增强目标奖励。


<details>
  <summary>Details</summary>
Motivation: 现有基于奖励引导的方法主要关注图像生成，而在图像编辑任务中如何保持源图像内容同时增强目标奖励的研究较少。

Method: 将扩散模型的反向过程视为从源图像出发的可控轨迹，通过迭代更新伴随状态来引导编辑过程，形成轨迹最优控制问题。

Result: 在多种编辑任务上的实验表明，该方法显著优于现有的基于反转的无训练引导基线，在奖励最大化和源图像保真度之间达到更好的平衡。

Conclusion: 该框架为奖励引导的图像编辑提供了一种有效的无需训练解决方案，避免了奖励黑客问题，实现了高质量的图像编辑效果。

Abstract: Recent advancements in diffusion and flow-matching models have demonstrated remarkable capabilities in high-fidelity image synthesis. A prominent line of research involves reward-guided guidance, which steers the generation process during inference to align with specific objectives. However, leveraging this reward-guided approach to the task of image editing, which requires preserving the semantic content of the source image while enhancing a target reward, is largely unexplored. In this work, we introduce a novel framework for training-free, reward-guided image editing. We formulate the editing process as a trajectory optimal control problem where the reverse process of a diffusion model is treated as a controllable trajectory originating from the source image, and the adjoint states are iteratively updated to steer the editing process. Through extensive experiments across distinct editing tasks, we demonstrate that our approach significantly outperforms existing inversion-based training-free guidance baselines, achieving a superior balance between reward maximization and fidelity to the source image without reward hacking.

</details>


### [15] [LiDAR Point Cloud Colourisation Using Multi-Camera Fusion and Low-Light Image Enhancement](https://arxiv.org/abs/2509.25859)
*Pasindu Ranasinghe,Dibyayan Patra,Bikram Banerjee,Simit Raval*

Main category: cs.CV

TL;DR: 提出了一种硬件无关的方法，使用多个摄像头输入为机械LiDAR生成彩色点云，实现360度覆盖，并在低光照条件下通过集成低光图像增强模块保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 融合摄像头和LiDAR数据以增强空间理解，特别是在低光照条件下保持可靠性能，解决传统方法在弱光环境下的局限性。

Method: 采用硬件无关方法，通过初始校准确定相机内参，自动计算LiDAR与相机间的几何变换，无需专用校准目标；集成低光图像增强模块，使用色彩校正确保相机数据一致性。

Result: 使用Velodyne Puck Hi-Res LiDAR和四摄像头配置测试，优化软件实现实时性能，在极低光照下仍能可靠着色，成功恢复原本无法检测的场景细节。

Conclusion: 该方法提供了一种高效、鲁棒的LiDAR点云着色解决方案，特别适用于低光照环境，简化了设置流程并实现了实时处理。

Abstract: In recent years, the fusion of camera data with LiDAR measurements has emerged as a powerful approach to enhance spatial understanding. This study introduces a novel, hardware-agnostic methodology that generates colourised point clouds from mechanical LiDAR using multiple camera inputs, providing complete 360-degree coverage. The primary innovation lies in its robustness under low-light conditions, achieved through the integration of a low-light image enhancement module within the fusion pipeline. The system requires initial calibration to determine intrinsic camera parameters, followed by automatic computation of the geometric transformation between the LiDAR and cameras, removing the need for specialised calibration targets and streamlining the setup. The data processing framework uses colour correction to ensure uniformity across camera feeds before fusion. The algorithm was tested using a Velodyne Puck Hi-Res LiDAR and a four-camera configuration. The optimised software achieved real-time performance and reliable colourisation even under very low illumination, successfully recovering scene details that would otherwise remain undetectable.

</details>


### [16] [The Impact of Scaling Training Data on Adversarial Robustness](https://arxiv.org/abs/2509.25927)
*Marco Zimmerli,Andreas Plesner,Till Aczel,Roger Wattenhofer*

Main category: cs.CV

TL;DR: 论文研究了训练数据特征对对抗鲁棒性的影响，发现鲁棒性遵循数据量和模型规模的对数缩放定律，但数据质量、架构和训练目标比原始规模在实现广谱对抗鲁棒性方面更具决定性作用。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络架构和训练范式有所进步，但仍容易受到对抗样本的攻击。本研究旨在探究训练数据特征如何影响对抗鲁棒性。

Method: 评估了36种最先进的视觉模型，涵盖监督、自监督和对比学习方法，在从120万到220亿图像的数据集上训练。模型在六种黑盒攻击类别下进行评估：随机扰动、两种几何掩码、COCO对象操作、ImageNet-C损坏和ImageNet-R风格偏移。

Result: 鲁棒性遵循数据量和模型规模的对数缩放定律：数据量增加十倍，攻击成功率平均降低约3.2%；模型规模增加十倍，攻击成功率平均降低约13.4%。一些在精选数据集上训练的自监督模型（如DINOv2）表现优于在更大但较少精选数据集上训练的模型。对抗性微调改善了跨结构变化的泛化能力，但不能改善跨颜色分布的泛化能力。

Conclusion: 虽然扩展规模能提高鲁棒性，但数据质量、架构和训练目标在实现广谱对抗鲁棒性方面比原始规模更具决定性作用。人类评估揭示了人类和机器视觉之间存在持续差距。

Abstract: Deep neural networks remain vulnerable to adversarial examples despite advances in architectures and training paradigms. We investigate how training data characteristics affect adversarial robustness across 36 state-of-the-art vision models spanning supervised, self-supervised, and contrastive learning approaches, trained on datasets from 1.2M to 22B images. Models were evaluated under six black-box attack categories: random perturbations, two types of geometric masks, COCO object manipulations, ImageNet-C corruptions, and ImageNet-R style shifts. Robustness follows a logarithmic scaling law with both data volume and model size: a tenfold increase in data reduces attack success rate (ASR) on average by ~3.2%, whereas a tenfold increase in model size reduces ASR on average by ~13.4%. Notably, some self-supervised models trained on curated datasets, such as DINOv2, outperform others trained on much larger but less curated datasets, challenging the assumption that scale alone drives robustness. Adversarial fine-tuning of ResNet50s improves generalization across structural variations but not across color distributions. Human evaluation reveals persistent gaps between human and machine vision. These results show that while scaling improves robustness, data quality, architecture, and training objectives play a more decisive role than raw scale in achieving broad-spectrum adversarial resilience.

</details>


### [17] [CO3: Contrasting Concepts Compose Better](https://arxiv.org/abs/2509.25940)
*Debottam Dutta,Jianchong Chen,Rajalaxmi Rajagopalan,Yu-Lin Wei,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 提出CO3方法，通过校正采样策略改善多概念提示在文本到图像扩散模型中的保真度，避免概念缺失或碰撞问题


<details>
  <summary>Details</summary>
Motivation: 解决多概念提示（如“猫和狗”）在扩散模型中常出现的概念缺失、模糊或碰撞问题，这是由于模型在采样时偏向训练中学习较强的单一概念

Method: 引入校正采样策略，引导模型远离联合提示行为与任何单一概念重叠过强的区域，转向所有概念能平衡共存的“纯”联合模式

Result: 在多样化多概念提示上的实验显示，相比标准基线和现有组合方法，概念覆盖度、平衡性和鲁棒性均有提升，概念丢失或扭曲更少

Conclusion: 轻量级校正引导能显著缓解现代扩散系统中的脆弱语义对齐行为

Abstract: We propose to improve multi-concept prompt fidelity in text-to-image diffusion models. We begin with common failure cases-prompts like "a cat and a dog" that sometimes yields images where one concept is missing, faint, or colliding awkwardly with another. We hypothesize that this happens when the diffusion model drifts into mixed modes that over-emphasize a single concept it learned strongly during training. Instead of re-training, we introduce a corrective sampling strategy that steers away from regions where the joint prompt behavior overlaps too strongly with any single concept in the prompt. The goal is to steer towards "pure" joint modes where all concepts can coexist with balanced visual presence. We further show that existing multi-concept guidance schemes can operate in unstable weight regimes that amplify imbalance; we characterize favorable regions and adapt sampling to remain within them. Our approach, CO3, is plug-and-play, requires no model tuning, and complements standard classifier-free guidance. Experiments on diverse multi-concept prompts indicate improvements in concept coverage, balance and robustness, with fewer dropped or distorted concepts compared to standard baselines and prior compositional methods. Results suggest that lightweight corrective guidance can substantially mitigate brittle semantic alignment behavior in modern diffusion systems.

</details>


### [18] [New Fourth-Order Grayscale Indicator-Based Telegraph Diffusion Model for Image Despeckling](https://arxiv.org/abs/2509.26010)
*Rajendra K. Ray,Manish Kumar*

Main category: cs.CV

TL;DR: 提出了一种结合扩散和波特性的四阶非线性PDE模型，用于抑制乘性噪声，相比传统的二阶PDE模型能更好地减少块状伪影并保留细节纹理。


<details>
  <summary>Details</summary>
Motivation: 传统的二阶PDE模型在抑制乘性噪声时会在去噪早期引入块状伪影，需要开发更有效的模型来解决这个问题。

Method: 使用四阶非线性PDE模型，结合拉普拉斯算子和强度值指导的扩散过程来降噪，同时利用波部分保持细节和纹理。对于彩色图像，将去噪过程独立应用于每个通道。

Result: 在PSNR、MSSIM和SI等指标上均优于现有的二阶各向异性扩散方法，对灰度图像和彩色图像都取得了更好的去噪效果。

Conclusion: 提出的四阶PDE模型在抑制乘性噪声方面比现有模型表现更优，能有效减少块状伪影并保持图像细节和颜色一致性。

Abstract: Second-order PDE models have been widely used for suppressing multiplicative noise, but they often introduce blocky artifacts in the early stages of denoising. To resolve this, we propose a fourth-order nonlinear PDE model that integrates diffusion and wave properties. The diffusion process, guided by both the Laplacian and intensity values, reduces noise better than gradient-based methods, while the wave part keeps fine details and textures. The effectiveness of the proposed model is evaluated against two second-order anisotropic diffusion approaches using the Peak Signal-to-Noise Ratio (PSNR) and Mean Structural Similarity Index (MSSIM) for images with available ground truth. For SAR images, where a noise-free reference is unavailable, the Speckle Index (SI) is used to measure noise reduction. Additionally, we extend the proposed model to study color images by applying the denoising process independently to each channel, preserving both structure and color consistency. The same quantitative metrics PSNR and MSSIM are used for performance evaluation, ensuring a fair comparison across grayscale and color images. In all the cases, our computed results produce better results compared to existing models in this genre.

</details>


### [19] [PatchVSR: Breaking Video Diffusion Resolution Limits with Patch-wise Video Super-Resolution](https://arxiv.org/abs/2509.26025)
*Shian Du,Menghan Xia,Chang Liu,Xintao Wang,Jing Wang,Pengfei Wan,Di Zhang,Xiangyang Ji*

Main category: cs.CV

TL;DR: 提出PatchVSR方法，利用视频扩散先验进行分块视频超分辨率，通过双流适配器整合局部和全局信息，实现高效高保真的4K视频超分辨率。


<details>
  <summary>Details</summary>
Motivation: 现有方法将预训练视频生成模型直接用于全尺寸视频超分辨率存在计算密集和输出分辨率固定的问题，需要探索更高效的分块处理方式。

Method: 提出PatchVSR方法，包含双流适配器：局部分支提取输入块特征保持内容保真度，全局分支提取全视频上下文特征弥补块语义不完整问题，并注入位置信息以更好地在全局帧中定位块合成。

Result: 实验表明该方法能在块级别合成高保真、高分辨率细节，通过定制的多块联合调制确保增强块间的视觉一致性，基于512x512基础模型实现高效竞争性4K视频超分辨率。

Conclusion: 基于分块的范式具有灵活性，能够以极高效率实现高质量4K视频超分辨率，突破了传统全尺寸方法的计算限制。

Abstract: Pre-trained video generation models hold great potential for generative video super-resolution (VSR). However, adapting them for full-size VSR, as most existing methods do, suffers from unnecessary intensive full-attention computation and fixed output resolution. To overcome these limitations, we make the first exploration into utilizing video diffusion priors for patch-wise VSR. This is non-trivial because pre-trained video diffusion models are not native for patch-level detail generation. To mitigate this challenge, we propose an innovative approach, called PatchVSR, which integrates a dual-stream adapter for conditional guidance. The patch branch extracts features from input patches to maintain content fidelity while the global branch extracts context features from the resized full video to bridge the generation gap caused by incomplete semantics of patches. Particularly, we also inject the patch's location information into the model to better contextualize patch synthesis within the global video frame. Experiments demonstrate that our method can synthesize high-fidelity, high-resolution details at the patch level. A tailor-made multi-patch joint modulation is proposed to ensure visual consistency across individually enhanced patches. Due to the flexibility of our patch-based paradigm, we can achieve highly competitive 4K VSR based on a 512x512 resolution base model, with extremely high efficiency.

</details>


### [20] [EVODiff: Entropy-aware Variance Optimized Diffusion Inference](https://arxiv.org/abs/2509.26096)
*Shigui Li,Wei Chen,Delu Zeng*

Main category: cs.CV

TL;DR: 提出EVODiff方法，从信息论角度优化扩散模型的推理过程，通过优化条件熵来减少不确定性，显著提升生成质量和效率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然图像生成效果好，但存在推理速度慢和训练-推理不一致的问题。现有梯度求解器缺乏信息传输效率的理论基础。

Method: 从信息论角度分析扩散模型推理过程，提出基于条件熵优化的EVODiff方法，通过优化条件方差来最小化转换和重建误差。

Result: 在CIFAR-10上，EVODiff在10次函数评估时重建误差降低45.5%（FID从5.10提升到2.78）；在ImageNet-256上，高质量样本的NFE成本减少25%（从20降到15）；文本到图像生成质量提升且伪影减少。

Conclusion: EVODiff通过信息论视角优化扩散模型推理，在生成质量和效率上显著优于现有最先进的梯度求解器，为扩散模型提供了更有效的推理方法。

Abstract: Diffusion models (DMs) excel in image generation, but suffer from slow inference and the training-inference discrepancies. Although gradient-based solvers like DPM-Solver accelerate the denoising inference, they lack theoretical foundations in information transmission efficiency. In this work, we introduce an information-theoretic perspective on the inference processes of DMs, revealing that successful denoising fundamentally reduces conditional entropy in reverse transitions. This principle leads to our key insights into the inference processes: (1) data prediction parameterization outperforms its noise counterpart, and (2) optimizing conditional variance offers a reference-free way to minimize both transition and reconstruction errors. Based on these insights, we propose an entropy-aware variance optimized method for the generative process of DMs, called EVODiff, which systematically reduces uncertainty by optimizing conditional entropy during denoising. Extensive experiments on DMs validate our insights and demonstrate that our method significantly and consistently outperforms state-of-the-art (SOTA) gradient-based solvers. For example, compared to the DPM-Solver++, EVODiff reduces the reconstruction error by up to 45.5\% (FID improves from 5.10 to 2.78) at 10 function evaluations (NFE) on CIFAR-10, cuts the NFE cost by 25\% (from 20 to 15 NFE) for high-quality samples on ImageNet-256, and improves text-to-image generation while reducing artifacts. Code is available at https://github.com/ShiguiLi/EVODiff.

</details>


### [21] [Continuous Space-Time Video Super-Resolution with 3D Fourier Fields](https://arxiv.org/abs/2509.26325)
*Alexander Becker,Julius Erbach,Dominik Narnhofer,Konrad Schindler*

Main category: cs.CV

TL;DR: 提出了一种连续时空视频超分辨率的新方法——3D视频傅里叶场(VFF)，通过神经编码器预测傅里叶系数，实现任意时空位置的灵活采样和抗锯齿重建。


<details>
  <summary>Details</summary>
Motivation: 传统方法将视频分解为空间和时间分量，依赖脆弱的显式帧扭曲进行运动补偿，存在局限性。需要一种能同时捕捉精细空间细节和平滑时间动态的连续表示方法。

Method: 将视频编码为连续的3D视频傅里叶场，使用具有大时空感受野的神经编码器预测傅里叶系数，可包含高斯点扩散函数进行抗锯齿重建。

Result: 在多个基准测试中达到新的最先进水平，在广泛的放大因子范围内提供更清晰、时间更一致的超分辨率重建，同时计算效率更高。

Conclusion: 联合建模方法显著改善了时空超分辨率性能，证明了连续傅里叶场表示在视频超分辨率任务中的有效性。

Abstract: We introduce a novel formulation for continuous space-time video super-resolution. Instead of decoupling the representation of a video sequence into separate spatial and temporal components and relying on brittle, explicit frame warping for motion compensation, we encode video as a continuous, spatio-temporally coherent 3D Video Fourier Field (VFF). That representation offers three key advantages: (1) it enables cheap, flexible sampling at arbitrary locations in space and time; (2) it is able to simultaneously capture fine spatial detail and smooth temporal dynamics; and (3) it offers the possibility to include an analytical, Gaussian point spread function in the sampling to ensure aliasing-free reconstruction at arbitrary scale. The coefficients of the proposed, Fourier-like sinusoidal basis are predicted with a neural encoder with a large spatio-temporal receptive field, conditioned on the low-resolution input video. Through extensive experiments, we show that our joint modeling substantially improves both spatial and temporal super-resolution and sets a new state of the art for multiple benchmarks: across a wide range of upscaling factors, it delivers sharper and temporally more consistent reconstructions than existing baselines, while being computationally more efficient. Project page: https://v3vsr.github.io.

</details>


### [22] [Go with Your Gut: Scaling Confidence for Autoregressive Image Generation](https://arxiv.org/abs/2509.26376)
*Harold Haodong Chen,Xianfeng Wu,Wen-Jie Shu,Rongjin Guo,Disen Lan,Harry Yang,Ying-Cong Chen*

Main category: cs.CV

TL;DR: ScalingAR是首个专为基于下一个token预测的自回归图像生成设计的测试时缩放框架，无需早期解码或辅助奖励，通过token熵作为视觉token生成的新信号，在配置和策略两个层面进行自适应优化。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时缩放方法依赖频繁的部分解码和外部奖励模型，不适用于基于下一个token预测的自回归图像生成，因为中间解码结果具有固有的不完整性。

Method: ScalingAR利用token熵作为视觉token生成的新信号，在配置层面通过融合内在和条件信号来流式传输校准的置信状态，在策略层面利用该状态自适应终止低置信度轨迹并动态调度指导强度。

Result: 在通用和组合基准测试中，ScalingAR将基础模型在GenEval上提升12.5%，在TIIF-Bench上提升15.2%，同时将视觉token消耗减少62.0%，并在挑战性场景中将性能下降减轻26.0%。

Conclusion: ScalingAR成功地将测试时缩放应用于基于下一个token预测的自回归图像生成，显著提升了生成质量和效率，同时增强了鲁棒性。

Abstract: Test-time scaling (TTS) has demonstrated remarkable success in enhancing large language models, yet its application to next-token prediction (NTP) autoregressive (AR) image generation remains largely uncharted. Existing TTS approaches for visual AR (VAR), which rely on frequent partial decoding and external reward models, are ill-suited for NTP-based image generation due to the inherent incompleteness of intermediate decoding results. To bridge this gap, we introduce ScalingAR, the first TTS framework specifically designed for NTP-based AR image generation that eliminates the need for early decoding or auxiliary rewards. ScalingAR leverages token entropy as a novel signal in visual token generation and operates at two complementary scaling levels: (i) Profile Level, which streams a calibrated confidence state by fusing intrinsic and conditional signals; and (ii) Policy Level, which utilizes this state to adaptively terminate low-confidence trajectories and dynamically schedule guidance for phase-appropriate conditioning strength. Experiments on both general and compositional benchmarks show that ScalingAR (1) improves base models by 12.5% on GenEval and 15.2% on TIIF-Bench, (2) efficiently reduces visual token consumption by 62.0% while outperforming baselines, and (3) successfully enhances robustness, mitigating performance drops by 26.0% in challenging scenarios.

</details>


### [23] [MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation](https://arxiv.org/abs/2509.26391)
*Chenhui Zhu,Yilu Wu,Shuai Wang,Gangshan Wu,Limin Wang*

Main category: cs.CV

TL;DR: MotionRAG是一个检索增强框架，通过从相关参考视频中提取运动先验来提升图像到视频生成的运动的真实性，采用上下文感知运动适应技术，在推理时计算开销可忽略不计。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的图像到视频生成在运动真实性方面仍面临挑战，因为准确建模运动需要捕捉物理约束、物体交互和特定领域动态，这些难以在不同场景中泛化。

Method: 提出检索增强框架MotionRAG，包括：(1)基于检索的管道，使用视频编码器和专用重采样器提取高级运动特征；(2)通过因果变换器架构实现上下文学习的运动适应；(3)基于注意力的运动注入适配器，将转移的运动特征无缝集成到预训练视频扩散模型中。

Result: 广泛实验表明，该方法在多个领域和各种基础模型上都取得了显著改进，推理时计算开销可忽略不计。模块化设计支持零样本泛化到新领域，只需更新检索数据库而无需重新训练任何组件。

Conclusion: 这项研究通过实现运动先验的有效检索和转移，增强了视频生成系统的核心能力，促进了真实运动动态的合成。

Abstract: Image-to-video generation has made remarkable progress with the advancements in diffusion models, yet generating videos with realistic motion remains highly challenging. This difficulty arises from the complexity of accurately modeling motion, which involves capturing physical constraints, object interactions, and domain-specific dynamics that are not easily generalized across diverse scenarios. To address this, we propose MotionRAG, a retrieval-augmented framework that enhances motion realism by adapting motion priors from relevant reference videos through Context-Aware Motion Adaptation (CAMA). The key technical innovations include: (i) a retrieval-based pipeline extracting high-level motion features using video encoder and specialized resamplers to distill semantic motion representations; (ii) an in-context learning approach for motion adaptation implemented through a causal transformer architecture; (iii) an attention-based motion injection adapter that seamlessly integrates transferred motion features into pretrained video diffusion models. Extensive experiments demonstrate that our method achieves significant improvements across multiple domains and various base models, all with negligible computational overhead during inference. Furthermore, our modular design enables zero-shot generalization to new domains by simply updating the retrieval database without retraining any components. This research enhances the core capability of video generation systems by enabling the effective retrieval and transfer of motion priors, facilitating the synthesis of realistic motion dynamics.

</details>


### [24] [Image-Difficulty-Aware Evaluation of Super-Resolution Models](https://arxiv.org/abs/2509.26398)
*Atakan Topaloglu,Ahmet Bilican,Cansu Korkmaz,A. Murat Tekalp*

Main category: cs.CV

TL;DR: 提出了一种基于图像难度感知的超分辨率模型评估方法，通过高频指数和旋转不变边缘指数来识别测试图像中模型性能差异，弥补传统平均分数评估的不足。


<details>
  <summary>Details</summary>
Motivation: 传统超分辨率模型评估使用平均分数，无法反映模型在不同难度图像上的性能差异，也无法捕捉某些模型在困难图像上产生的伪影问题。

Method: 提出了两种图像难度度量指标：高频指数和旋转不变边缘指数，用于预测模型在哪些测试图像上会产生显著视觉差异，并设计了相应的评估方法。

Result: 实验结果表明，所提出的图像难度度量和评估方法能够有效区分产生不同视觉结果的超分辨率模型。

Conclusion: 基于图像难度的评估方法能够更准确地反映超分辨率模型的真实性能，特别是在视觉质量方面。

Abstract: Image super-resolution models are commonly evaluated by average scores (over some benchmark test sets), which fail to reflect the performance of these models on images of varying difficulty and that some models generate artifacts on certain difficult images, which is not reflected by the average scores. We propose difficulty-aware performance evaluation procedures to better differentiate between SISR models that produce visually different results on some images but yield close average performance scores over the entire test set. In particular, we propose two image-difficulty measures, the high-frequency index and rotation-invariant edge index, to predict those test images, where a model would yield significantly better visual results over another model, and an evaluation method where these visual differences are reflected on objective measures. Experimental results demonstrate the effectiveness of the proposed image-difficulty measures and evaluation methodology.

</details>


### [25] [PRISM: Progressive Rain removal with Integrated State-space Modeling](https://arxiv.org/abs/2509.26413)
*Pengze Xue,Shanwen Wang,Fei Zhou,Yan Cui,Xin Sun*

Main category: cs.CV

TL;DR: 提出了PRISM框架，一个渐进式三阶段去雨方法，结合多尺度特征聚合和混合域状态空间建模，在多个数据集上取得竞争性结果


<details>
  <summary>Details</summary>
Motivation: 当前单尺度模型在细粒度恢复和全局一致性方面存在困难，需要更有效的去雨方法

Method: 三阶段框架：CENet粗提取网络、SFNet频率融合网络、RNet精炼网络，使用HA-UNet进行多尺度特征聚合，HDMamba进行混合域建模

Result: 在多个数据集上取得竞争性结果，能够学习高频雨特征同时保持结构细节和全局上下文

Conclusion: PRISM框架通过渐进式处理和混合域建模，有效提升了图像去雨的质量和一致性

Abstract: Image deraining is an essential vision technique that removes rain streaks and water droplets, enhancing clarity for critical vision tasks like autonomous driving. However, current single-scale models struggle with fine-grained recovery and global consistency. To address this challenge, we propose Progressive Rain removal with Integrated State-space Modeling (PRISM), a progressive three-stage framework: Coarse Extraction Network (CENet), Frequency Fusion Network (SFNet), and Refine Network (RNet). Specifically, CENet and SFNet utilize a novel Hybrid Attention UNet (HA-UNet) for multi-scale feature aggregation by combining channel attention with windowed spatial transformers. Moreover, we propose Hybrid Domain Mamba (HDMamba) for SFNet to jointly model spatial semantics and wavelet domain characteristics. Finally, RNet recovers the fine-grained structures via an original-resolution subnetwork. Our model learns high-frequency rain characteristics while preserving structural details and maintaining global context, leading to improved image quality. Our method achieves competitive results on multiple datasets against recent deraining methods.

</details>


### [26] [Post-Training Quantization via Residual Truncation and Zero Suppression for Diffusion Models](https://arxiv.org/abs/2509.26436)
*Donghoon Kim,Dongyoung Lee,Ik Joon Chang,Sung-Ho Bae*

Main category: cs.CV

TL;DR: 提出QuaRTZ方法，通过残差截断和零抑制实现扩散模型的4位量化，在保持纹理细节的同时减少舍入误差。


<details>
  <summary>Details</summary>
Motivation: 扩散模型计算需求高，部署困难。4位量化中较大的步长会放大低幅值激活的舍入误差，导致细粒度纹理丢失。

Method: QuaRTZ结合8位最小-最大量化处理异常值，通过前导零抑制压缩到4位以保留最低有效位，平衡异常值保留和LSB精度。

Result: 在FLUX.1-schnell上达到FID 6.98，优于需要FP16辅助分支的SVDQuant方法。

Conclusion: QuaRTZ通过理论推导和实证评估证明了对不同激活分布的通用性，是有效的4位PTQ方案。

Abstract: Diffusion models achieve high-quality image generation but face deployment challenges due to their high computational requirements. Although 8-bit outlier-aware post-training quantization (PTQ) matches full-precision performance, extending PTQ to 4 bits remains challenging. Larger step sizes in 4-bit quantization amplify rounding errors in dense, low-magnitude activations, leading to the loss of fine-grained textures. We hypothesize that not only outliers but also small activations are critical for texture fidelity. To this end, we propose Quantization via Residual Truncation and Zero Suppression (QuaRTZ), a 4-bit PTQ scheme for diffusion models. QuaRTZ applies 8-bit min-max quantization for outlier handling and compresses to 4 bits via leading-zero suppression to retain LSBs, thereby preserving texture details. Our approach reduces rounding errors and improves quantization efficiency by balancing outlier preservation and LSB precision. Both theoretical derivations and empirical evaluations demonstrate the generalizability of QuaRTZ across diverse activation distributions. Notably, 4-bit QuaRTZ achieves an FID of 6.98 on FLUX.1-schnell, outperforming SVDQuant that requires auxiliary FP16 branches.

</details>


### [27] [Stylos: Multi-View 3D Stylization with Single-Forward Gaussian Splatting](https://arxiv.org/abs/2509.26455)
*Hanzhou Liu,Jia Huang,Mi Lu,Srikanth Saripalli,Peng Jiang*

Main category: cs.CV

TL;DR: Stylos是一个单前向3D高斯框架，用于3D风格迁移，支持从单张图像到多视图集合的无姿态内容，通过参考风格图像进行条件化。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D风格迁移方法需要逐场景优化或预计算姿态的问题，实现无需优化即可生成几何感知、视角一致的风格化3D场景。

Method: 采用Transformer主干网络，包含两条路径：几何预测保留自注意力以保持几何保真度，风格通过全局交叉注意力注入以确保视角一致性。结合基于体素的3D风格损失来对齐场景特征与风格统计。

Result: 在多个数据集上的实验表明，Stylos能够实现高质量零样本风格迁移，验证了全局风格-内容耦合、3D风格损失的有效性以及从单视图到大规模多视图设置的可扩展性。

Conclusion: Stylos框架通过单前向传播实现了无需优化的3D风格迁移，在保持几何一致性的同时实现了视角一致的风格化效果，具有良好的泛化能力。

Abstract: We present Stylos, a single-forward 3D Gaussian framework for 3D style transfer that operates on unposed content, from a single image to a multi-view collection, conditioned on a separate reference style image. Stylos synthesizes a stylized 3D Gaussian scene without per-scene optimization or precomputed poses, achieving geometry-aware, view-consistent stylization that generalizes to unseen categories, scenes, and styles. At its core, Stylos adopts a Transformer backbone with two pathways: geometry predictions retain self-attention to preserve geometric fidelity, while style is injected via global cross-attention to enforce visual consistency across views. With the addition of a voxel-based 3D style loss that aligns aggregated scene features to style statistics, Stylos enforces view-consistent stylization while preserving geometry. Experiments across multiple datasets demonstrate that Stylos delivers high-quality zero-shot stylization, highlighting the effectiveness of global style-content coupling, the proposed 3D style loss, and the scalability of our framework from single view to large-scale multi-view settings.

</details>


### [28] [Contrastive Diffusion Guidance for Spatial Inverse Problems](https://arxiv.org/abs/2509.26489)
*Sattwik Basu,Chaitanya Amballa,Zhongweiyang Xu,Jorge Vančo Sampedro,Srihari Nelakuditi,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 提出CoGuide方法，通过用户移动轨迹重建空间布局（如房屋平面图），使用扩散模型和对比学习嵌入空间来解决传统方法的不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 从用户移动轨迹重建空间布局是病态逆问题，因为多个布局可以解释相同轨迹。现有生成式逆求解器面临前向算子不可逆、不可微导致的优化不稳定问题。

Method: 使用扩散后验采样器生成与测量一致的布局。通过对比学习训练嵌入空间，使兼容的平面图和轨迹靠近，不匹配的远离。在嵌入空间中定义替代似然分数来指导去噪过程。

Result: CoGuide模型在实验中比可微分规划器基线和引导扩散方法产生更一致的平面图，且更稳健。

Conclusion: 在嵌入空间中定义替代似然分数是真实似然分数的有效近似，能够成功引导去噪过程趋向后验分布。

Abstract: We consider the inverse problem of reconstructing the spatial layout of a place, a home floorplan for example, from a user`s movements inside that layout. Direct inversion is ill-posed since many floorplans can explain the same movement trajectories. We adopt a diffusion-based posterior sampler to generate layouts consistent with the measurements. While active research is in progress on generative inverse solvers, we find that the forward operator in our problem poses new challenges. The path-planning process inside a floorplan is a non-invertible, non-differentiable function, and causes instability while optimizing using the likelihood score. We break-away from existing approaches and reformulate the likelihood score in a smoother embedding space. The embedding space is trained with a contrastive loss which brings compatible floorplans and trajectories close to each other, while pushing mismatched pairs far apart. We show that a surrogate form of the likelihood score in this embedding space is a valid approximation of the true likelihood score, making it possible to steer the denoising process towards the posterior. Across extensive experiments, our model CoGuide produces more consistent floorplans from trajectories, and is more robust than differentiable-planner baselines and guided-diffusion methods.

</details>


### [29] [Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation](https://arxiv.org/abs/2509.26555)
*Agneet Chatterjee,Rahim Entezari,Maksym Zhuravinskyi,Maksim Lapin,Reshinth Adithyan,Amit Raj,Chitta Baral,Yezhou Yang,Varun Jampani*

Main category: cs.CV

TL;DR: 提出了Stable Cinemetrics评估框架，将电影制作控制分为四个层次化分类体系：Setup、Event、Lighting和Camera，包含76个细粒度控制节点，并构建了专业用例的基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型和基准测试无法捕捉专业视频生成的复杂性和需求，需要建立面向专业电影制作的结构化评估框架。

Method: 开发了四个解耦的层次化分类体系，构建专业用例基准测试，建立自动化提示分类和问题生成流程，进行大规模人类研究（10+模型，20K视频，80+电影专业人士标注）。

Result: 即使当前最强模型在Events和Camera相关控制方面仍存在显著差距，训练了优于现有零样本基线的自动评估器。

Conclusion: SCINE是首个将专业视频生成置于视频生成模型背景下的方法，引入了以电影控制为中心的分类体系，并通过结构化评估流程和详细分析指导未来研究。

Abstract: Recent advances in video generation have enabled high-fidelity video synthesis from user provided prompts. However, existing models and benchmarks fail to capture the complexity and requirements of professional video generation. Towards that goal, we introduce Stable Cinemetrics, a structured evaluation framework that formalizes filmmaking controls into four disentangled, hierarchical taxonomies: Setup, Event, Lighting, and Camera. Together, these taxonomies define 76 fine-grained control nodes grounded in industry practices. Using these taxonomies, we construct a benchmark of prompts aligned with professional use cases and develop an automated pipeline for prompt categorization and question generation, enabling independent evaluation of each control dimension. We conduct a large-scale human study spanning 10+ models and 20K videos, annotated by a pool of 80+ film professionals. Our analysis, both coarse and fine-grained reveal that even the strongest current models exhibit significant gaps, particularly in Events and Camera-related controls. To enable scalable evaluation, we train an automatic evaluator, a vision-language model aligned with expert annotations that outperforms existing zero-shot baselines. SCINE is the first approach to situate professional video generation within the landscape of video generative models, introducing taxonomies centered around cinematic controls and supporting them with structured evaluation pipelines and detailed analyses to guide future research.

</details>


### [30] [Query-Kontext: An Unified Multimodal Model for Image Generation and Editing](https://arxiv.org/abs/2509.26641)
*Yuxin Song,Wenkai Dong,Shizun Wang,Qi Zhang,Song Xue,Tao Yuan,Hu Yang,Haocheng Feng,Hang Zhou,Xinyan Xiao,Jingdong Wang*

Main category: cs.CV

TL;DR: Query-Kontext是一种新型统一多模态模型，通过多模态"kontext"连接视觉语言模型和扩散模型，将多模态生成推理能力委托给VLM，同时保留扩散模型的高质量视觉合成功能。


<details>
  <summary>Details</summary>
Motivation: 当前统一框架中，多模态生成推理能力（包括指令理解、定位和图像引用）与高保真合成能力内在纠缠，需要解耦这些功能。

Method: 提出三阶段渐进训练策略：1）通过多模态kontext令牌连接VLM和轻量扩散头；2）扩展到大型预训练扩散模型；3）引入低级图像编码器进行指令调优。构建综合数据管道覆盖多种多模态参考到图像场景。

Result: 实验表明，该方法与强大统一基线相当，在某些情况下甚至优于特定任务的最先进方法。

Conclusion: Query-Kontext成功解耦了多模态生成推理和视觉合成，通过委托复杂推理给VLM并保留扩散模型的合成能力，实现了卓越性能。

Abstract: Unified Multimodal Models (UMMs) have demonstrated remarkable performance in text-to-image generation (T2I) and editing (TI2I), whether instantiated as assembled unified frameworks which couple powerful vision-language model (VLM) with diffusion-based generator, or as naive Unified Multimodal Models with an early fusion of understanding and generation modalities. We contend that in current unified frameworks, the crucial capability of multimodal generative reasoning which encompasses instruction understanding, grounding, and image referring for identity preservation and faithful reconstruction, is intrinsically entangled with high-fidelity synthesis. In this work, we introduce Query-Kontext, a novel approach that bridges the VLM and diffusion model via a multimodal ``kontext'' composed of semantic cues and coarse-grained image conditions encoded from multimodal inputs. This design delegates the complex ability of multimodal generative reasoning to powerful VLM while reserving diffusion model's role for high-quality visual synthesis. To achieve this, we propose a three-stage progressive training strategy. First, we connect the VLM to a lightweight diffusion head via multimodal kontext tokens to unleash the VLM's generative reasoning ability. Second, we scale this head to a large, pre-trained diffusion model to enhance visual detail and realism. Finally, we introduce a low-level image encoder to improve image fidelity and perform instruction tuning on downstream tasks. Furthermore, we build a comprehensive data pipeline integrating real, synthetic, and open-source datasets, covering diverse multimodal reference-to-image scenarios, including image generation, instruction-driven editing, customized generation, and multi-subject composition. Experiments show that our approach matches strong unified baselines and even outperforms task-specific state-of-the-art methods in several cases.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [31] [LTA-L2S: Lexical Tone-Aware Lip-to-Speech Synthesis for Mandarin with Cross-Lingual Transfer Learning](https://arxiv.org/abs/2509.25670)
*Kang Yang,Yifan Liang,Fangkun Liu,Zhenping Xie,Chengshi Zheng*

Main category: cs.SD

TL;DR: 提出Lexical Tone-Aware Lip-to-Speech (LTA-L2S)模型，通过跨语言迁移学习和流匹配模型解决普通话唇语合成中的音素映射复杂性和声调建模问题。


<details>
  <summary>Details</summary>
Motivation: 普通话唇语合成面临两个主要挑战：复杂的视素-音素映射关系，以及声调对可理解性的关键作用。现有方法难以有效处理这些特性。

Method: 1. 使用跨语言迁移学习策略，将英语预训练的视听自监督学习模型适配到普通话领域；2. 采用流匹配模型生成F0轮廓，由ASR微调的SSL语音单元指导；3. 两阶段训练范式，通过流匹配后处理网络细化粗粒度频谱图。

Result: 大量实验表明，LTA-L2S在语音可理解性和声调准确性方面显著优于现有方法。

Conclusion: 该模型成功解决了普通话唇语合成的关键挑战，通过跨语言迁移和声调感知建模实现了优异的性能。

Abstract: Lip-to-speech (L2S) synthesis for Mandarin is a significant challenge, hindered by complex viseme-to-phoneme mappings and the critical role of lexical tones in intelligibility. To address this issue, we propose Lexical Tone-Aware Lip-to-Speech (LTA-L2S). To tackle viseme-to-phoneme complexity, our model adapts an English pre-trained audio-visual self-supervised learning (SSL) model via a cross-lingual transfer learning strategy. This strategy not only transfers universal knowledge learned from extensive English data to the Mandarin domain but also circumvents the prohibitive cost of training such a model from scratch. To specifically model lexical tones and enhance intelligibility, we further employ a flow-matching model to generate the F0 contour. This generation process is guided by ASR-fine-tuned SSL speech units, which contain crucial suprasegmental information. The overall speech quality is then elevated through a two-stage training paradigm, where a flow-matching postnet refines the coarse spectrogram from the first stage. Extensive experiments demonstrate that LTA-L2S significantly outperforms existing methods in both speech intelligibility and tonal accuracy.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [32] [IRIS: Intrinsic Reward Image Synthesis](https://arxiv.org/abs/2509.25562)
*Yihang Chen,Yuanhao Ban,Yunqi Hong,Cho-Jui Hsieh*

Main category: cs.AI

TL;DR: 本文提出了IRIS框架，首次使用内在奖励通过强化学习改进自回归文本到图像生成模型，无需依赖外部奖励或标记数据。


<details>
  <summary>Details</summary>
Motivation: 由于人类偏好数据有限，传统的基于人类反馈的强化学习在自回归文本到图像生成中应用受限，需要探索不依赖外部信号的学习方法。

Method: 提出IRIS框架，通过最大化模型的自不确定性而非自确定性作为内在奖励，使用强化学习训练自回归文本到图像模型。

Result: 实验结果表明，IRIS框架在自回归文本到图像模型上的表现与外部奖励方法相当甚至更优。

Conclusion: 最大化自不确定性可以有效改进自回归文本到图像生成，IRIS框架为无外部奖励的强化学习提供了可行方案。

Abstract: Despite the success of Reinforcement Learning from Human Feedback (RLHF) in language reasoning, its application to autoregressive Text-to-Image (T2I) generation is often constrained by the limited availability of human preference data. This paper explores how an autoregressive T2I model can learn from internal signals without relying on external rewards or labeled data. Contrary to recent findings in text generation, we show that maximizing self-uncertainty, rather than self-certainty, improves image generation. We observe that this is because autoregressive T2I models with low uncertainty tend to generate simple and uniform images, which are less aligned with human preferences. Based on these observations, we propose IRIS (Intrinsic Reward Image Synthesis), the first framework to improve autoregressive T2I models with reinforcement learning using only an intrinsic reward. Empirical results demonstrate that applying IRIS to autoregressive T2I models achieves performance that is competitive with or superior to external rewards.

</details>


### [33] [PUREVQ-GAN: Defending Data Poisoning Attacks through Vector-Quantized Bottlenecks](https://arxiv.org/abs/2509.25792)
*Alexander Branch,Omead Pooladzandi,Radin Khosraviani,Sunay Gajanan Bhat,Jeffrey Jiang,Gregory Pottie*

Main category: cs.AI

TL;DR: PureVQ-GAN是一种基于向量量化VAE和GAN鉴别器的数据投毒防御方法，通过离散瓶颈破坏后门触发器模式，同时保持语义内容完整性。


<details>
  <summary>Details</summary>
Motivation: 针对数据投毒攻击中后门触发器的威胁，需要一种既能有效消除触发器又能保持图像质量的高效防御方法。

Method: 使用向量量化VAE将投毒图像通过学习的码本进行量化，破坏细粒度触发器模式，同时用GAN鉴别器确保输出符合自然图像分布，防止重构分布外扰动。

Result: 在CIFAR-10数据集上，对Gradient Matching和Bullseye Polytope攻击实现0%投毒成功率，对Narcissus攻击为1.64%，同时保持91-95%的干净准确率，比基于扩散的方法快50倍以上。

Conclusion: PureVQ-GAN是一种高效实用的数据投毒防御方案，在保持高防御效果的同时大幅提升了处理速度。

Abstract: We introduce PureVQ-GAN, a defense against data poisoning that forces backdoor triggers through a discrete bottleneck using Vector-Quantized VAE with GAN discriminator. By quantizing poisoned images through a learned codebook, PureVQ-GAN destroys fine-grained trigger patterns while preserving semantic content. A GAN discriminator ensures outputs match the natural image distribution, preventing reconstruction of out-of-distribution perturbations. On CIFAR-10, PureVQ-GAN achieves 0% poison success rate (PSR) against Gradient Matching and Bullseye Polytope attacks, and 1.64% against Narcissus while maintaining 91-95% clean accuracy. Unlike diffusion-based defenses requiring hundreds of iterative refinement steps, PureVQ-GAN is over 50x faster, making it practical for real training pipelines.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [34] [Personalized Scientific Figure Caption Generation: An Empirical Study on Author-Specific Writing Style Transfer](https://arxiv.org/abs/2509.25817)
*Jaeyoung Kim,Jongho Lee,Hongjun Choi,Sion Jang*

Main category: cs.CL

TL;DR: 使用科学论文作者档案数据进行个性化图表标题生成的研究，揭示了作者风格匹配与标题质量之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用作者档案数据和相关元数据来提升多模态大语言模型在个性化图表标题生成方面的性能。

Method: 结合丰富的作者档案数据和相关元数据，在多模态大语言模型上进行个性化图表标题生成的实验。

Result: 作者档案数据能显著提升个性化性能，但存在作者风格匹配与标题质量之间的基本权衡关系。

Conclusion: 为开发平衡个性化风格和标题质量的实用标题自动化系统提供了有价值的见解和未来方向。

Abstract: We study personalized figure caption generation using author profile data from scientific papers. Our experiments demonstrate that rich author profile data, combined with relevant metadata, can significantly improve the personalization performance of multimodal large language models. However, we also reveal a fundamental trade-off between matching author style and maintaining caption quality. Our findings offer valuable insights and future directions for developing practical caption automation systems that balance both objectives. This work was conducted as part of the 3rd SciCap challenge.

</details>
