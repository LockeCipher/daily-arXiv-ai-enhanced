<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 33]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.HC](#cs.HC) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [UnGuide: Learning to Forget with LoRA-Guided Diffusion Models](https://arxiv.org/abs/2508.05755)
*Agnieszka Polowczyk,Alicja Polowczyk,Dawid Malarz,Artur Kasymov,Marcin Mazur,Jacek Tabor,Przemysław Spurek*

Main category: cs.CV

TL;DR: UnGuide是一种新方法，通过动态推理机制UnGuidance结合LoRA，实现对扩散模型中特定知识的精确去除，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像扩散模型可能被滥用生成有害内容，因此需要有效的机器遗忘方法，去除特定知识而不影响整体性能。

Method: UnGuide结合LoRA和Classifier-Free Guidance（CFG），通过动态调整引导尺度，选择性去除知识。LoRA模块主导目标概念去除，基础模型保持无关内容生成。

Result: UnGuide在对象擦除和显式内容去除任务中优于现有LoRA方法，实现了可控概念去除并保留模型表达能力。

Conclusion: UnGuide提供了一种高效且精确的机器遗忘方法，解决了LoRA在无关内容上的副作用，提升了扩散模型的安全性。

Abstract: Recent advances in large-scale text-to-image diffusion models have heightened concerns about their potential misuse, especially in generating harmful or misleading content. This underscores the urgent need for effective machine unlearning, i.e., removing specific knowledge or concepts from pretrained models without compromising overall performance. One possible approach is Low-Rank Adaptation (LoRA), which offers an efficient means to fine-tune models for targeted unlearning. However, LoRA often inadvertently alters unrelated content, leading to diminished image fidelity and realism. To address this limitation, we introduce UnGuide -- a novel approach which incorporates UnGuidance, a dynamic inference mechanism that leverages Classifier-Free Guidance (CFG) to exert precise control over the unlearning process. UnGuide modulates the guidance scale based on the stability of a few first steps of denoising processes, enabling selective unlearning by LoRA adapter. For prompts containing the erased concept, the LoRA module predominates and is counterbalanced by the base model; for unrelated prompts, the base model governs generation, preserving content fidelity. Empirical results demonstrate that UnGuide achieves controlled concept removal and retains the expressive power of diffusion models, outperforming existing LoRA-based methods in both object erasure and explicit content removal tasks.

</details>


### [2] [Improving Masked Style Transfer using Blended Partial Convolution](https://arxiv.org/abs/2508.05769)
*Seyed Hadi Seyed,Ayberk Cansever,David Hart*

Main category: cs.CV

TL;DR: 提出了一种基于部分卷积的风格迁移网络，专注于对图像特定区域进行风格迁移，避免了传统方法在区域选择上的缺陷。


<details>
  <summary>Details</summary>
Motivation: 传统方法对整个图像进行风格迁移后通过掩码处理，但会导致感兴趣区域的风格特征捕捉不准确。

Method: 采用部分卷积网络，结合内部混合技术，精确地将风格特征应用于感兴趣区域。

Result: 在SA-1B数据集上验证了视觉和量化上的改进效果。

Conclusion: 该方法显著提升了局部风格迁移的准确性和视觉效果。

Abstract: Artistic style transfer has long been possible with the advancements of convolution- and transformer-based neural networks. Most algorithms apply the artistic style transfer to the whole image, but individual users may only need to apply a style transfer to a specific region in the image. The standard practice is to simply mask the image after the stylization. This work shows that this approach tends to improperly capture the style features in the region of interest. We propose a partial-convolution-based style transfer network that accurately applies the style features exclusively to the region of interest. Additionally, we present network-internal blending techniques that account for imperfections in the region selection. We show that this visually and quantitatively improves stylization using examples from the SA-1B dataset. Code is publicly available at https://github.com/davidmhart/StyleTransferMasked.

</details>


### [3] [MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with Rectified Flow and Region-specific Contrastive Loss](https://arxiv.org/abs/2508.05772)
*Can Zhao,Pengfei Guo,Dong Yang,Yucheng Tang,Yufan He,Benjamin Simon,Mason Belue,Stephanie Harmon,Baris Turkbey,Daguang Xu*

Main category: cs.CV

TL;DR: MAISI-v2是一个加速的3D医学图像合成框架，通过引入校正流和区域特异性对比损失，解决了现有扩散模型在通用性、推理速度和条件一致性上的问题。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像合成方法在通用性、推理速度和输入条件对齐方面存在不足，MAISI-v2旨在解决这些问题。

Method: MAISI-v2整合了校正流以加速生成，并引入区域特异性对比损失增强条件一致性。

Result: 实验表明，MAISI-v2在图像质量上达到SOTA，推理速度提升33倍，且合成图像可用于数据增强。

Conclusion: MAISI-v2在医学图像合成中实现了高效、高质量的生成，并提供了开源资源以促进社区发展。

Abstract: Medical image synthesis is an important topic for both clinical and research applications. Recently, diffusion models have become a leading approach in this area. Despite their strengths, many existing methods struggle with (1) limited generalizability that only work for specific body regions or voxel spacings, (2) slow inference, which is a common issue for diffusion models, and (3) weak alignment with input conditions, which is a critical issue for medical imaging. MAISI, a previously proposed framework, addresses generalizability issues but still suffers from slow inference and limited condition consistency. In this work, we present MAISI-v2, the first accelerated 3D medical image synthesis framework that integrates rectified flow to enable fast and high quality generation. To further enhance condition fidelity, we introduce a novel region-specific contrastive loss to enhance the sensitivity to region of interest. Our experiments show that MAISI-v2 can achieve SOTA image quality with $33 \times$ acceleration for latent diffusion model. We also conducted a downstream segmentation experiment to show that the synthetic images can be used for data augmentation. We release our code, training details, model weights, and a GUI demo to facilitate reproducibility and promote further development within the community.

</details>


### [4] [Optimization-Free Style Transfer for 3D Gaussian Splats](https://arxiv.org/abs/2508.05813)
*Raphael Du Sablon,David Hart*

Main category: cs.CV

TL;DR: 提出了一种无需重建或优化的3D高斯样条风格迁移方法，通过生成图结构并插值实现快速风格化。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要重建或优化样条，效率低且限制多，因此提出一种无需额外训练或优化的快速风格化方法。

Method: 在样条隐式表面生成图结构，使用前馈表面风格化方法并插值回样条。

Result: 实现了快速风格化（2分钟内），支持任意风格图像和3D高斯样条，效果优于其他方法。

Conclusion: 该方法高效、灵活，适用于多种场景，代码已开源。

Abstract: The task of style transfer for 3D Gaussian splats has been explored in many previous works, but these require reconstructing or fine-tuning the splat while incorporating style information or optimizing a feature extraction network on the splat representation. We propose a reconstruction- and optimization-free approach to stylizing 3D Gaussian splats. This is done by generating a graph structure across the implicit surface of the splat representation. A feed-forward, surface-based stylization method is then used and interpolated back to the individual splats in the scene. This allows for any style image and 3D Gaussian splat to be used without any additional training or optimization. This also allows for fast stylization of splats, achieving speeds under 2 minutes even on consumer-grade hardware. We demonstrate the quality results this approach achieves and compare to other 3D Gaussian splat style transfer methods. Code is publicly available at https://github.com/davidmhart/FastSplatStyler.

</details>


### [5] [MZEN: Multi-Zoom Enhanced NeRF for 3-D Reconstruction with Unknown Camera Poses](https://arxiv.org/abs/2508.05819)
*Jong-Ik Park,Carlee Joe-Wong,Gary K. Fedder*

Main category: cs.CV

TL;DR: MZEN是一种改进的NeRF框架，能够处理多缩放图像集，显著提升工业检测中的细节重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF方法在工业检测中无法捕捉微米级细节，且多缩放图像会破坏多视角一致性。

Method: MZEN通过引入可学习的缩放标量改进相机模型，并提出一种新的姿态策略，先解决广角图像再处理缩放图像。

Result: 在多种场景下，MZEN显著提升了PSNR、SSIM和LPIPS指标。

Conclusion: MZEN成功将NeRF应用于工业检测，兼顾全局精度和微米级细节。

Abstract: Neural Radiance Fields (NeRF) methods excel at 3D reconstruction from multiple 2D images, even those taken with unknown camera poses. However, they still miss the fine-detailed structures that matter in industrial inspection, e.g., detecting sub-micron defects on a production line or analyzing chips with Scanning Electron Microscopy (SEM). In these scenarios, the sensor resolution is fixed and compute budgets are tight, so the only way to expose fine structure is to add zoom-in images; yet, this breaks the multi-view consistency that pose-free NeRF training relies on. We propose Multi-Zoom Enhanced NeRF (MZEN), the first NeRF framework that natively handles multi-zoom image sets. MZEN (i) augments the pin-hole camera model with an explicit, learnable zoom scalar that scales the focal length, and (ii) introduces a novel pose strategy: wide-field images are solved first to establish a global metric frame, and zoom-in images are then pose-primed to the nearest wide-field counterpart via a zoom-consistent crop-and-match procedure before joint refinement. Across eight forward-facing scenes$\unicode{x2013}$synthetic TCAD models, real SEM of micro-structures, and BLEFF objects$\unicode{x2013}$MZEN consistently outperforms pose-free baselines and even high-resolution variants, boosting PSNR by up to $28 \%$, SSIM by $10 \%$, and reducing LPIPS by up to $222 \%$. MZEN, therefore, extends NeRF to real-world factory settings, preserving global accuracy while capturing the micron-level details essential for industrial inspection.

</details>


### [6] [A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image](https://arxiv.org/abs/2508.05950)
*Yanxing Liang,Yinghui Wang,Jinlong Yang,Wei Li*

Main category: cs.CV

TL;DR: SINGAD提出了一种自监督框架，通过3D高斯扩散估计单张图像的法线，解决了多视角几何不一致性和数据依赖性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖数据驱动的统计先验，缺乏对光-表面交互的显式建模，导致多视角法线方向冲突，且无法通过梯度传播优化几何误差。

Method: 结合物理驱动的光交互建模和可微分渲染重投影策略，构建3D高斯重参数化模型和跨域特征融合模块，实现自监督优化。

Result: 在Google Scanned Objects数据集上，SINGAD在多个指标上优于现有方法。

Conclusion: SINGAD通过几何一致性约束和自监督学习，显著提升了单图像法线估计的性能。

Abstract: The lack of spatial dimensional information remains a challenge in normal estimation from a single image. Recent diffusion-based methods have demonstrated significant potential in 2D-to-3D implicit mapping, they rely on data-driven statistical priors and miss the explicit modeling of light-surface interaction, leading to multi-view normal direction conflicts. Moreover, the discrete sampling mechanism of diffusion models causes gradient discontinuity in differentiable rendering reconstruction modules, preventing 3D geometric errors from being backpropagated to the normal generation network, thereby forcing existing methods to depend on dense normal annotations. This paper proposes SINGAD, a novel Self-supervised framework from a single Image for Normal estimation via 3D GAussian splatting guided Diffusion. By integrating physics-driven light-interaction modeling and a differentiable rendering-based reprojection strategy, our framework directly converts 3D geometric errors into normal optimization signals, solving the challenges of multi-view geometric inconsistency and data dependency. Specifically, the framework constructs a light-interaction-driven 3DGS reparameterization model to generate multi-scale geometric features consistent with light transport principles, ensuring multi-view normal consistency. A cross-domain feature fusion module is designed within a conditional diffusion model, embedding geometric priors to constrain normal generation while maintaining accurate geometric error propagation. Furthermore, a differentiable 3D reprojection loss strategy is introduced for self-supervised optimization that minimizes geometric error between the reconstructed and input image, eliminating dependence on annotated normal datasets. Quantitative evaluations on the Google Scanned Objects dataset demonstrate that our method outperforms state-of-the-art approaches across multiple metrics.

</details>


### [7] [Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents](https://arxiv.org/abs/2508.05954)
*Han Lin,Jaemin Cho,Amir Zadeh,Chuan Li,Mohit Bansal*

Main category: cs.CV

TL;DR: Bifrost-1是一个统一框架，通过将预训练的多模态大语言模型（MLLMs）与扩散模型结合，利用CLIP图像嵌入作为潜在变量，实现高效的高保真可控图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在训练成本高且未充分利用预训练模型的视觉表示能力，Bifrost-1旨在解决这一问题。

Method: 使用patch级CLIP图像嵌入作为潜在变量，轻量级ControlNet适配扩散模型，并初始化MLLM的视觉生成分支。

Result: Bifrost-1在视觉保真度和多模态理解上表现优异，且训练计算成本显著降低。

Conclusion: Bifrost-1通过高效整合预训练模型，实现了高质量图像生成，同时保留了多模态推理能力。

Abstract: There is growing interest in integrating high-fidelity visual synthesis capabilities into large language models (LLMs) without compromising their strong reasoning capabilities. Existing methods that directly train LLMs or bridge LLMs and diffusion models usually suffer from costly training since the backbone LLMs have not seen image representations during pretraining. We present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs (MLLMs) and diffusion models using patch-level CLIP image embeddings as latent variables, which are natively aligned with the MLLM's CLIP visual encoder. These patch-level image embeddings are integrated into the diffusion model with a lightweight adaptation of its ControlNet. To retain the original multimodal reasoning capabilities of MLLMs, we equip the MLLM with a visual generation branch initialized from the original MLLM parameters when predicting the patch-level image embeddings. By seamlessly integrating pretrained MLLMs and diffusion models with patch-level CLIP latents, our framework enables high-fidelity controllable image generation with significant training efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or better performance than previous methods in terms of visual fidelity and multimodal understanding, with substantially lower compute during training. We also provide comprehensive ablation studies showing the effectiveness of our design choices.

</details>


### [8] [ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera Samplings and Diffusion Priors](https://arxiv.org/abs/2508.06014)
*Minsu Kim,Subin Jeon,In Cho,Mijin Yoo,Seon Joo Kim*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯泼溅（3DGS）的管道，通过生成额外训练视图和虚拟相机放置策略，提升新视角合成（NVS）的质量，减少伪影和缺失区域。


<details>
  <summary>Details</summary>
Motivation: 现有方法在偏离训练轨迹的视角下渲染时存在伪影和缺失区域，限制了无缝场景探索。

Method: 采用信息增益驱动的虚拟相机放置策略最大化场景覆盖，结合视频扩散先验优化渲染结果，并通过增强视图微调3D高斯。

Result: 实验表明，该方法优于现有3DGS方法，支持高质量、无伪影的任意视角渲染。

Conclusion: 提出的方法显著提升了3DGS的重建质量，适用于挑战性场景探索。

Abstract: Recent advances in novel view synthesis (NVS) have enabled real-time rendering with 3D Gaussian Splatting (3DGS). However, existing methods struggle with artifacts and missing regions when rendering from viewpoints that deviate from the training trajectory, limiting seamless scene exploration. To address this, we propose a 3DGS-based pipeline that generates additional training views to enhance reconstruction. We introduce an information-gain-driven virtual camera placement strategy to maximize scene coverage, followed by video diffusion priors to refine rendered results. Fine-tuning 3D Gaussians with these enhanced views significantly improves reconstruction quality. To evaluate our method, we present Wild-Explore, a benchmark designed for challenging scene exploration. Experiments demonstrate that our approach outperforms existing 3DGS-based methods, enabling high-quality, artifact-free rendering from arbitrary viewpoints.   https://exploregs.github.io

</details>


### [9] [Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis](https://arxiv.org/abs/2508.06021)
*Utku Ozbulak,Michaela Cohrs,Hristo L. Svilenov,Joris Vankerschaver,Wesley De Neve*

Main category: cs.CV

TL;DR: 利用扩散模型生成高质量图像以解决数据不平衡问题，提升多类分类器的性能。


<details>
  <summary>Details</summary>
Motivation: 数据稀缺和类别不平衡是多类分类器在亚可见颗粒分析中的主要障碍，尤其是对于罕见颗粒类型（如硅油和气泡）。

Method: 开发了一种先进的扩散模型，生成高保真图像以扩充训练数据集，并通过大规模实验验证其效果。

Result: 生成的样本在视觉质量和结构上与真实颗粒图像相似，显著提升了分类性能。

Conclusion: 公开了扩散模型和多类分类器，促进开放研究和可重复性。

Abstract: Sub-visible particle analysis using flow imaging microscopy combined with deep learning has proven effective in identifying particle types, enabling the distinction of harmless components such as silicone oil from protein particles. However, the scarcity of available data and severe imbalance between particle types within datasets remain substantial hurdles when applying multi-class classifiers to such problems, often forcing researchers to rely on less effective methods. The aforementioned issue is particularly challenging for particle types that appear unintentionally and in lower numbers, such as silicone oil and air bubbles, as opposed to protein particles, where obtaining large numbers of images through controlled settings is comparatively straightforward. In this work, we develop a state-of-the-art diffusion model to address data imbalance by generating high-fidelity images that can augment training datasets, enabling the effective training of multi-class deep neural networks. We validate this approach by demonstrating that the generated samples closely resemble real particle images in terms of visual quality and structure. To assess the effectiveness of using diffusion-generated images in training datasets, we conduct large-scale experiments on a validation dataset comprising 500,000 protein particle images and demonstrate that this approach improves classification performance with no negligible downside. Finally, to promote open research and reproducibility, we publicly release both our diffusion models and the trained multi-class deep neural network classifiers, along with a straightforward interface for easy integration into future studies, at https://github.com/utkuozbulak/svp-generative-ai.

</details>


### [10] [Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts](https://arxiv.org/abs/2508.06032)
*Kiran Chhatre,Christopher Peters,Srikrishna Karanam*

Main category: cs.CV

TL;DR: Spectrum是一个统一网络，用于细粒度的人体部位和衣物解析，通过改进的I2Tx扩散模型实现更好的语义对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法在人体解析中常使用宽泛的标签，无法区分细粒度的衣物类型或详细的身体部位。扩散模型虽能泛化，但其内部表示不适合详细解析。

Method: Spectrum利用经过微调的I2Tx扩散模型提取特征，并通过提示引导生成语义有效的掩码，实现细粒度解析。

Result: Spectrum在跨数据集实验中表现优于基线方法，尤其在提示引导的分割任务中。

Conclusion: Spectrum通过改进的扩散模型和提示引导，实现了更精确的人体部位和衣物解析。

Abstract: Existing methods for human parsing into body parts and clothing often use fixed mask categories with broad labels that obscure fine-grained clothing types. Recent open-vocabulary segmentation approaches leverage pretrained text-to-image (T2I) diffusion model features for strong zero-shot transfer, but typically group entire humans into a single person category, failing to distinguish diverse clothing or detailed body parts. To address this, we propose Spectrum, a unified network for part-level pixel parsing (body parts and clothing) and instance-level grouping. While diffusion-based open-vocabulary models generalize well across tasks, their internal representations are not specialized for detailed human parsing. We observe that, unlike diffusion models with broad representations, image-driven 3D texture generators maintain faithful correspondence to input images, enabling stronger representations for parsing diverse clothing and body parts. Spectrum introduces a novel repurposing of an Image-to-Texture (I2Tx) diffusion model -- obtained by fine-tuning a T2I model on 3D human texture maps -- for improved alignment with body parts and clothing. From an input image, we extract human-part internal features via the I2Tx diffusion model and generate semantically valid masks aligned to diverse clothing categories through prompt-guided grounding. Once trained, Spectrum produces semantic segmentation maps for every visible body part and clothing category, ignoring standalone garments or irrelevant objects, for any number of humans in the scene. We conduct extensive cross-dataset experiments -- separately assessing body parts, clothing parts, unseen clothing categories, and full-body masks -- and demonstrate that Spectrum consistently outperforms baseline methods in prompt-based segmentation.

</details>


### [11] [InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow](https://arxiv.org/abs/2508.06033)
*Yiming Gong,Zhen Zhu,Minjia Zhang*

Main category: cs.CV

TL;DR: InstantEdit是一种基于RectifiedFlow框架的快速文本引导图像编辑方法，通过PerRFI反转策略和Inversion Latent Injection技术，实现了高效且一致的编辑效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本引导图像编辑方法在快速性和编辑一致性上的不足，同时保留关键内容并紧密遵循文本指令。

Method: 结合RectifiedFlow的直线采样轨迹，提出PerRFI反转策略和Inversion Latent Injection技术，并引入Disentangled Prompt Guidance和Canny-conditioned ControlNet。

Result: 在PIE数据集上，InstantEdit在速度和编辑质量上均优于现有方法。

Conclusion: InstantEdit通过创新技术实现了快速、一致且高质量的文本引导图像编辑。

Abstract: We propose a fast text-guided image editing method called InstantEdit based on the RectifiedFlow framework, which is structured as a few-step editing process that preserves critical content while following closely to textual instructions. Our approach leverages the straight sampling trajectories of RectifiedFlow by introducing a specialized inversion strategy called PerRFI. To maintain consistent while editable results for RectifiedFlow model, we further propose a novel regeneration method, Inversion Latent Injection, which effectively reuses latent information obtained during inversion to facilitate more coherent and detailed regeneration. Additionally, we propose a Disentangled Prompt Guidance technique to balance editability with detail preservation, and integrate a Canny-conditioned ControlNet to incorporate structural cues and suppress artifacts. Evaluation on the PIE image editing dataset demonstrates that InstantEdit is not only fast but also achieves better qualitative and quantitative results compared to state-of-the-art few-step editing methods.

</details>


### [12] [NEP: Autoregressive Image Editing via Next Editing Token Prediction](https://arxiv.org/abs/2508.06044)
*Huimin Wu,Xiaojian Ma,Haozhe Zhao,Yanpeng Zhao,Qing Li*

Main category: cs.CV

TL;DR: 提出了一种基于自回归图像生成的Next Editing-token Prediction (NEP)方法，仅编辑需要修改的区域，避免不必要的计算和非编辑区域的偏差。


<details>
  <summary>Details</summary>
Motivation: 现有方法生成整个目标图像，导致计算成本高且非编辑区域重建偏差影响编辑质量。

Method: 通过预训练任意顺序自回归文本到图像（T2I）模型，实现零样本图像编辑，并适应NEP方法。

Result: 在广泛使用的图像编辑基准测试中达到新最优性能，并支持零样本迭代优化（TTS）。

Conclusion: NEP方法通过选择性编辑区域解决了现有问题，实现了高效高质量的图像编辑。

Abstract: Text-guided image editing involves modifying a source image based on a language instruction and, typically, requires changes to only small local regions. However, existing approaches generate the entire target image rather than selectively regenerate only the intended editing areas. This results in (1) unnecessary computational costs and (2) a bias toward reconstructing non-editing regions, which compromises the quality of the intended edits. To resolve these limitations, we propose to formulate image editing as Next Editing-token Prediction (NEP) based on autoregressive image generation, where only regions that need to be edited are regenerated, thus avoiding unintended modification to the non-editing areas. To enable any-region editing, we propose to pre-train an any-order autoregressive text-to-image (T2I) model. Once trained, it is capable of zero-shot image editing and can be easily adapted to NEP for image editing, which achieves a new state-of-the-art on widely used image editing benchmarks. Moreover, our model naturally supports test-time scaling (TTS) through iteratively refining its generation in a zero-shot manner. The project page is: https://nep-bigai.github.io/

</details>


### [13] [VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning](https://arxiv.org/abs/2508.06051)
*Linhan Cao,Wei Sun,Weixia Zhang,Xiangyang Zhu,Jun Jia,Kaiwei Zhang,Dandan Zhu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: VQAThinker是一个基于推理的视频质量评估框架，利用多模态模型和强化学习解决现有模型的泛化性和可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频质量评估（VQA）模型在泛化性和可解释性方面存在不足，限制了其实际应用。

Method: 采用组相对策略优化（GRPO）和三种VQA特定奖励（回归、排序和时间一致性）来模拟人类感知决策。

Result: VQAThinker在域内和域外VQA基准测试中表现最佳，并在质量理解和描述任务中优于现有模型。

Conclusion: 强化学习为仅依赖分数级监督构建泛化性和可解释性强的VQA模型提供了有效途径。

Abstract: Video quality assessment (VQA) aims to objectively quantify perceptual quality degradation in alignment with human visual perception. Despite recent advances, existing VQA models still suffer from two critical limitations: \textit{poor generalization to out-of-distribution (OOD) videos} and \textit{limited explainability}, which restrict their applicability in real-world scenarios. To address these challenges, we propose \textbf{VQAThinker}, a reasoning-based VQA framework that leverages large multimodal models (LMMs) with reinforcement learning to jointly model video quality understanding and scoring, emulating human perceptual decision-making. Specifically, we adopt group relative policy optimization (GRPO), a rule-guided reinforcement learning algorithm that enables reasoning over video quality under score-level supervision, and introduce three VQA-specific rewards: (1) a \textbf{bell-shaped regression reward} that increases rapidly as the prediction error decreases and becomes progressively less sensitive near the ground truth; (2) a \textbf{pairwise ranking reward} that guides the model to correctly determine the relative quality between video pairs; and (3) a \textbf{temporal consistency reward} that encourages the model to prefer temporally coherent videos over their perturbed counterparts. Extensive experiments demonstrate that VQAThinker achieves state-of-the-art performance on both in-domain and OOD VQA benchmarks, showing strong generalization for video quality scoring. Furthermore, evaluations on video quality understanding tasks validate its superiority in distortion attribution and quality description compared to existing explainable VQA models and LMMs. These findings demonstrate that reinforcement learning offers an effective pathway toward building generalizable and explainable VQA models solely with score-level supervision.

</details>


### [14] [Lightweight Quad Bayer HybridEVS Demosaicing via State Space Augmented Cross-Attention](https://arxiv.org/abs/2508.06058)
*Shiyang Zhou,Haijin Zeng,Yunfan Lu,Yongyong Chen,Jie Liu,Jingyong Su*

Main category: cs.CV

TL;DR: TSANet是一个轻量级的两阶段网络，通过状态空间增强的交叉注意力，分别处理事件像素修复和去马赛克任务，显著提升了HybridEVS相机的图像质量。


<details>
  <summary>Details</summary>
Motivation: HybridEVS相机结合Quad Bayer CFA传感器和事件像素时，缺乏颜色信息导致去马赛克过程中出现伪影和混叠问题，现有方法在移动设备上效果不佳。

Method: 提出TSANet，采用两阶段网络设计，分别处理事件像素修复和去马赛克；引入轻量级的Cross-Swin State Block，利用位置先验和状态空间模型增强全局依赖。

Result: 在模拟和真实HybridEVS数据上表现优异，PSNR和SSIM优于现有方法DemosaicFormer，参数和计算成本分别降低1.86倍和3.29倍。

Conclusion: TSANet为移动设备上的高效图像去马赛克提供了新思路，模型轻量且性能优越。

Abstract: Event cameras like the Hybrid Event-based Vision Sensor (HybridEVS) camera capture brightness changes as asynchronous "events" instead of frames, offering advanced application on mobile photography. However, challenges arise from combining a Quad Bayer Color Filter Array (CFA) sensor with event pixels lacking color information, resulting in aliasing and artifacts on the demosaicing process before downstream application. Current methods struggle to address these issues, especially on resource-limited mobile devices. In response, we introduce \textbf{TSANet}, a lightweight \textbf{T}wo-stage network via \textbf{S}tate space augmented cross-\textbf{A}ttention, which can handle event pixels inpainting and demosaicing separately, leveraging the benefits of dividing complex tasks into manageable subtasks. Furthermore, we introduce a lightweight Cross-Swin State Block that uniquely utilizes positional prior for demosaicing and enhances global dependencies through the state space model with linear complexity. In summary, TSANet demonstrates excellent demosaicing performance on both simulated and real data of HybridEVS while maintaining a lightweight model, averaging better results than the previous state-of-the-art method DemosaicFormer across seven diverse datasets in both PSNR and SSIM, while respectively reducing parameter and computation costs by $1.86\times$ and $3.29\times$. Our approach presents new possibilities for efficient image demosaicing on mobile devices. Code is available in the supplementary materials.

</details>


### [15] [Towards MR-Based Trochleoplasty Planning](https://arxiv.org/abs/2508.06076)
*Michael Wehrli,Alicia Durrer,Paul Friedrich,Sidaty El Hadramy,Edwin Li,Luana Brahaj,Carol C. Hasler,Philippe C. Cattin*

Main category: cs.CV

TL;DR: 提出了一种基于临床MR扫描生成高分辨率3D伪健康目标形态的流程，用于治疗滑车发育不良（TD），无需CT扫描，减少辐射。


<details>
  <summary>Details</summary>
Motivation: 当前治疗TD的方法依赖低分辨率MR扫描和外科医生经验，导致结果不一致且微创技术应用有限。

Method: 使用隐式神经表示（INR）生成各向同性超分辨率MR体积，多标签网络分割骨骼，小波扩散模型（WDM）生成伪健康目标形态。

Result: 在25名TD患者中验证，显著改善了滑车角度（SA）和滑车沟深度（TGD）。

Conclusion: 该方法为术前规划提供了高分辨率3D形态，减少辐射，效果显著。

Abstract: To treat Trochlear Dysplasia (TD), current approaches rely mainly on low-resolution clinical Magnetic Resonance (MR) scans and surgical intuition. The surgeries are planned based on surgeons experience, have limited adoption of minimally invasive techniques, and lead to inconsistent outcomes. We propose a pipeline that generates super-resolved, patient-specific 3D pseudo-healthy target morphologies from conventional clinical MR scans. First, we compute an isotropic super-resolved MR volume using an Implicit Neural Representation (INR). Next, we segment femur, tibia, patella, and fibula with a multi-label custom-trained network. Finally, we train a Wavelet Diffusion Model (WDM) to generate pseudo-healthy target morphologies of the trochlear region. In contrast to prior work producing pseudo-healthy low-resolution 3D MR images, our approach enables the generation of sub-millimeter resolved 3D shapes compatible for pre- and intraoperative use. These can serve as preoperative blueprints for reshaping the femoral groove while preserving the native patella articulation. Furthermore, and in contrast to other work, we do not require a CT for our pipeline - reducing the amount of radiation. We evaluated our approach on 25 TD patients and could show that our target morphologies significantly improve the sulcus angle (SA) and trochlear groove depth (TGD). The code and interactive visualization are available at https://wehrlimi.github.io/sr-3d-planning/.

</details>


### [16] [DreamVE: Unified Instruction-based Image and Video Editing](https://arxiv.org/abs/2508.06080)
*Bin Xia,Jiyang Liu,Yuechen Zhang,Bohao Peng,Ruihang Chu,Yitong Wang,Xinglong Wu,Bei Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: DreamVE是一个基于指令的图像和视频编辑统一模型，通过两阶段训练策略（先图像后视频）和综合数据合成方法（拼贴和生成模型）提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决基于指令的视频编辑因训练数据不足而受限的问题，推动其实际应用。

Method: 采用两阶段训练策略（图像编辑后视频编辑），结合拼贴和生成模型的数据合成方法，并设计高效的编辑框架。

Result: DreamVE在关键编辑类型上表现优异，具有强泛化和迁移能力，但属性编辑性能稍弱。

Conclusion: DreamVE通过统一框架和多样化数据合成方法，显著提升了基于指令的编辑性能，未来将开源代码和模型。

Abstract: Instruction-based editing holds vast potential due to its simple and efficient interactive editing format. However, instruction-based editing, particularly for video, has been constrained by limited training data, hindering its practical application. To this end, we introduce DreamVE, a unified model for instruction-based image and video editing. Specifically, We propose a two-stage training strategy: first image editing, then video editing. This offers two main benefits: (1) Image data scales more easily, and models are more efficient to train, providing useful priors for faster and better video editing training. (2) Unifying image and video generation is natural and aligns with current trends. Moreover, we present comprehensive training data synthesis pipelines, including collage-based and generative model-based data synthesis. The collage-based data synthesis combines foreground objects and backgrounds to generate diverse editing data, such as object manipulation, background changes, and text modifications. It can easily generate billions of accurate, consistent, realistic, and diverse editing pairs. We pretrain DreamVE on extensive collage-based data to achieve strong performance in key editing types and enhance generalization and transfer capabilities. However, collage-based data lacks some attribute editing cases, leading to a relative drop in performance. In contrast, the generative model-based pipeline, despite being hard to scale up, offers flexibility in handling attribute editing cases. Therefore, we use generative model-based data to further fine-tune DreamVE. Besides, we design an efficient and powerful editing framework for DreamVE. We build on the SOTA T2V model and use a token concatenation with early drop approach to inject source image guidance, ensuring strong consistency and editability. The codes and models will be released.

</details>


### [17] [SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment](https://arxiv.org/abs/2508.06082)
*Yanxiao Sun,Jiafu Wu,Yun Cao,Chengming Xu,Yabiao Wang,Weijian Cao,Donghao Luo,Chengjie Wang,Yanwei Fu*

Main category: cs.CV

TL;DR: SwiftVideo是一种结合轨迹保持和分布匹配策略的统一稳定蒸馏框架，用于加速视频生成模型，减少推理步骤的同时保持高质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于轨迹保持或分布匹配的蒸馏方法在少步设置下性能下降或产生更多伪影，需要一种更稳定的解决方案。

Method: 提出连续时间一致性蒸馏确保ODE轨迹精确保持，并引入双视角对齐（分布对齐和轨迹对齐）以优化性能。

Result: 在OpenVid-1M基准测试中，SwiftVideo在少步视频生成中显著优于现有方法。

Conclusion: SwiftVideo通过结合两种策略，实现了高效且高质量的视频生成，为少步推理提供了稳定解决方案。

Abstract: Diffusion-based or flow-based models have achieved significant progress in video synthesis but require multiple iterative sampling steps, which incurs substantial computational overhead. While many distillation methods that are solely based on trajectory-preserving or distribution-matching have been developed to accelerate video generation models, these approaches often suffer from performance breakdown or increased artifacts under few-step settings. To address these limitations, we propose \textbf{\emph{SwiftVideo}}, a unified and stable distillation framework that combines the advantages of trajectory-preserving and distribution-matching strategies. Our approach introduces continuous-time consistency distillation to ensure precise preservation of ODE trajectories. Subsequently, we propose a dual-perspective alignment that includes distribution alignment between synthetic and real data along with trajectory alignment across different inference steps. Our method maintains high-quality video generation while substantially reducing the number of inference steps. Quantitative evaluations on the OpenVid-1M benchmark demonstrate that our method significantly outperforms existing approaches in few-step video generation.

</details>


### [18] [E-React: Towards Emotionally Controlled Synthesis of Human Reactions](https://arxiv.org/abs/2508.06093)
*Chen Zhu,Buzhen Huang,Zijing Wu,Binghui Zuo,Yangang Wang*

Main category: cs.CV

TL;DR: 论文提出了一种基于情感驱动的人体反应动作生成方法，通过半监督情感先验和扩散模型提升生成的自然性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的人体动作生成框架未考虑情感影响，导致生成动作不够自然，限制了在交互任务中的应用。

Method: 采用半监督学习训练情感先验，并结合扩散模型生成考虑空间交互和情感响应的反应动作。

Result: 实验表明，该方法在生成多样情感驱动的反应动作上优于现有方法。

Conclusion: 提出的方法有效解决了情感驱动动作生成的挑战，提升了生成动作的自然性和多样性。

Abstract: Emotion serves as an essential component in daily human interactions. Existing human motion generation frameworks do not consider the impact of emotions, which reduces naturalness and limits their application in interactive tasks, such as human reaction synthesis. In this work, we introduce a novel task: generating diverse reaction motions in response to different emotional cues. However, learning emotion representation from limited motion data and incorporating it into a motion generation framework remains a challenging problem. To address the above obstacles, we introduce a semi-supervised emotion prior in an actor-reactor diffusion model to facilitate emotion-driven reaction synthesis. Specifically, based on the observation that motion clips within a short sequence tend to share the same emotion, we first devise a semi-supervised learning framework to train an emotion prior. With this prior, we further train an actor-reactor diffusion model to generate reactions by considering both spatial interaction and emotional response. Finally, given a motion sequence of an actor, our approach can generate realistic reactions under various emotional conditions. Experimental results demonstrate that our model outperforms existing reaction generation methods. The code and data will be made publicly available at https://ereact.github.io/

</details>


### [19] [Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation](https://arxiv.org/abs/2508.06136)
*YoungChan Choi,HengFei Wang,YiHua Cheng,Boeun Kim,Hyung Jin Chang,YoungGeun Choi,Sang-Il Choi*

Main category: cs.CV

TL;DR: 提出了一种基于显式3D眼球结构的新型3D视线重定向框架，优于现有基于NeRF的隐式方法。


<details>
  <summary>Details</summary>
Motivation: 现有视线重定向方法多基于神经辐射场（NeRF），其隐式表示难以显式建模眼球旋转和平移。

Method: 采用3D高斯泼溅（3DGS）显式表示眼球结构，并引入自适应变形模块模拟眼部肌肉运动。

Result: 在ETH-XGaze数据集上验证，生成图像质量高且视线估计准确，优于现有方法。

Conclusion: 显式3D眼球结构结合自适应变形模块，显著提升了视线重定向的逼真度和准确性。

Abstract: We propose a novel 3D gaze redirection framework that leverages an explicit 3D eyeball structure. Existing gaze redirection methods are typically based on neural radiance fields, which employ implicit neural representations via volume rendering. Unlike these NeRF-based approaches, where the rotation and translation of 3D representations are not explicitly modeled, we introduce a dedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian Splatting (3DGS). Our method generates photorealistic images that faithfully reproduce the desired gaze direction by explicitly rotating and translating the 3D eyeball structure. In addition, we propose an adaptive deformation module that enables the replication of subtle muscle movements around the eyes. Through experiments conducted on the ETH-XGaze dataset, we demonstrate that our framework is capable of generating diverse novel gaze images, achieving superior image quality and gaze estimation accuracy compared to previous state-of-the-art methods.

</details>


### [20] [DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera](https://arxiv.org/abs/2508.06139)
*Shaohua Pan,Xinyu Yi,Yan Zhou,Weihua Jian,Yuan Zhang,Pengfei Wan,Feng Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种基于扩散模型的方法，结合稀疏IMU和单目摄像头进行实时人体运动捕捉，通过统一框架融合两种信号，并在实验中展示了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 结合稀疏IMU和单目摄像头进行实时运动捕捉是一个有前景的方向，但需要解决视觉信息偶尔失效和IMU信号稳定性的问题。

Method: 将视觉信息整体转化为条件嵌入，而IMU测量值逐帧与噪声姿态拼接作为扩散模型的输入，以充分利用两种信号的特点。

Result: 实验表明，该方法在姿态估计任务中表现优异，优于现有方法。

Conclusion: 通过统一框架融合视觉和IMU信号，该方法在运动捕捉中实现了鲁棒性和高性能。

Abstract: Combining sparse IMUs and a monocular camera is a new promising setting to perform real-time human motion capture. This paper proposes a diffusion-based solution to learn human motion priors and fuse the two modalities of signals together seamlessly in a unified framework. By delicately considering the characteristics of the two signals, the sequential visual information is considered as a whole and transformed into a condition embedding, while the inertial measurement is concatenated with the noisy body pose frame by frame to construct a sequential input for the diffusion model. Firstly, we observe that the visual information may be unavailable in some frames due to occlusions or subjects moving out of the camera view. Thus incorporating the sequential visual features as a whole to get a single feature embedding is robust to the occasional degenerations of visual information in those frames. On the other hand, the IMU measurements are robust to occlusions and always stable when signal transmission has no problem. So incorporating them frame-wisely could better explore the temporal information for the system. Experiments have demonstrated the effectiveness of the system design and its state-of-the-art performance in pose estimation compared with the previous works. Our codes are available for research at https://shaohua-pan.github.io/diffcap-page.

</details>


### [21] [Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment](https://arxiv.org/abs/2508.06160)
*Zhenbang Du,Yonggan Fu,Lifu Wang,Jiayi Qian,Xiao Luo,Yingyan,Lin*

Main category: cs.CV

TL;DR: PostDiff框架通过混合分辨率去噪和模块级缓存策略，在无需微调的情况下优化预训练扩散模型的效率与保真度权衡。


<details>
  <summary>Details</summary>
Motivation: 研究在资源有限平台上部署扩散模型时，减少去噪步骤或降低每步计算成本哪种方式更有效。

Method: 提出PostDiff框架，包括混合分辨率去噪方案和模块级缓存策略，以减少冗余计算。

Result: 实验表明，PostDiff显著提升了效率与保真度的平衡，且降低每步计算成本比减少去噪步骤更有效。

Conclusion: PostDiff为资源受限环境下的扩散模型部署提供了高效且无需微调的解决方案。

Abstract: Diffusion models have shown remarkable success across generative tasks, yet their high computational demands challenge deployment on resource-limited platforms. This paper investigates a critical question for compute-optimal diffusion model deployment: Under a post-training setting without fine-tuning, is it more effective to reduce the number of denoising steps or to use a cheaper per-step inference? Intuitively, reducing the number of denoising steps increases the variability of the distributions across steps, making the model more sensitive to compression. In contrast, keeping more denoising steps makes the differences smaller, preserving redundancy, and making post-training compression more feasible. To systematically examine this, we propose PostDiff, a training-free framework for accelerating pre-trained diffusion models by reducing redundancy at both the input level and module level in a post-training manner. At the input level, we propose a mixed-resolution denoising scheme based on the insight that reducing generation resolution in early denoising steps can enhance low-frequency components and improve final generation fidelity. At the module level, we employ a hybrid module caching strategy to reuse computations across denoising steps. Extensive experiments and ablation studies demonstrate that (1) PostDiff can significantly improve the fidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to boost efficiency while maintaining decent generation fidelity, reducing per-step inference cost is often more effective than reducing the number of denoising steps. Our code is available at https://github.com/GATECH-EIC/PostDiff.

</details>


### [22] [UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting](https://arxiv.org/abs/2508.06169)
*Wenpeng Xing,Jie Chen,Zaifeng Yang,Changting Lin,Jianfeng Dong,Chaochao Chen,Xun Zhou,Meng Han*

Main category: cs.CV

TL;DR: UW-3DGS是一种基于3D高斯泼溅（3DGS）的水下3D场景重建框架，通过可学习的物理模块和自适应噪声修剪技术，显著提升了水下重建的几何和颜色保真度。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如NeRF及其扩展）在水下环境中因光线吸收、散射和浑浊问题导致重建效果不佳，且效率较低。

Method: UW-3DGS结合了可学习的水下图像形成模块和物理感知不确定性修剪（PAUP）分支，通过端到端训练优化高斯分布并去除噪声。

Result: 在SeaThru-NeRF和UWBundle数据集上表现优异，PSNR达27.604，SSIM为0.868，LPIPS为0.104，浮动物体减少约65%。

Conclusion: UW-3DGS通过物理建模和自适应修剪，显著提升了水下3D重建的质量和效率。

Abstract: Underwater 3D scene reconstruction faces severe challenges from light absorption, scattering, and turbidity, which degrade geometry and color fidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF extensions such as SeaThru-NeRF incorporate physics-based models, their MLP reliance limits efficiency and spatial resolution in hazy environments. We introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for robust underwater reconstruction. Key innovations include: (1) a plug-and-play learnable underwater image formation module using voxel-based regression for spatially varying attenuation and backscatter; and (2) a Physics-Aware Uncertainty Pruning (PAUP) branch that adaptively removes noisy floating Gaussians via uncertainty scoring, ensuring artifact-free geometry. The pipeline operates in training and rendering stages. During training, noisy Gaussians are optimized end-to-end with underwater parameters, guided by PAUP pruning and scattering modeling. In rendering, refined Gaussians produce clean Unattenuated Radiance Images (URIs) free from media effects, while learned physics enable realistic Underwater Images (UWIs) with accurate light transport. Experiments on SeaThru-NeRF and UWBundle datasets show superior performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on SeaThru-NeRF, with ~65% reduction in floating artifacts.

</details>


### [23] [Synthetic Data-Driven Multi-Architecture Framework for Automated Polyp Segmentation Through Integrated Detection and Mask Generation](https://arxiv.org/abs/2508.06170)
*Ojonugwa Oluwafemi Ejiga Peter,Akingbola Oluwapemiisin,Amalahu Chetachi,Adeniran Opeyemi,Fahmi Khalifa,Md Mahmudur Rahman*

Main category: cs.CV

TL;DR: 研究提出了一种多方向架构框架，用于自动化结肠镜图像中的息肉检测，结合合成数据生成和检测分割算法，显著提升了检测和分割性能。


<details>
  <summary>Details</summary>
Motivation: 结肠镜检查是结直肠癌早期诊断的关键工具，但医疗数据集有限且标注复杂，因此需要自动化解决方案。

Method: 结合Faster R-CNN进行初始目标定位，Segment Anything Model（SAM）优化分割掩码，并评估了五种分割模型（U-Net、PSPNet、FPN、LinkNet、MANet）。

Result: Faster R-CNN的召回率为93.08%，精确率为88.97%，F1分数为90.98%；FPN在PSNR和SSIM上表现最佳，UNet在召回率上领先，LinkNet在IoU和Dice分数上表现均衡。

Conclusion: 该框架有效解决了数据集和标注问题，显著提升了息肉检测和分割的自动化性能。

Abstract: Colonoscopy is a vital tool for the early diagnosis of colorectal cancer, which is one of the main causes of cancer-related mortality globally; hence, it is deemed an essential technique for the prevention and early detection of colorectal cancer. The research introduces a unique multidirectional architectural framework to automate polyp detection within colonoscopy images while helping resolve limited healthcare dataset sizes and annotation complexities. The research implements a comprehensive system that delivers synthetic data generation through Stable Diffusion enhancements together with detection and segmentation algorithms. This detection approach combines Faster R-CNN for initial object localization while the Segment Anything Model (SAM) refines the segmentation masks. The faster R-CNN detection algorithm achieved a recall of 93.08% combined with a precision of 88.97% and an F1 score of 90.98%.SAM is then used to generate the image mask. The research evaluated five state-of-the-art segmentation models that included U-Net, PSPNet, FPN, LinkNet, and MANet using ResNet34 as a base model. The results demonstrate the superior performance of FPN with the highest scores of PSNR (7.205893) and SSIM (0.492381), while UNet excels in recall (84.85%) and LinkNet shows balanced performance in IoU (64.20%) and Dice score (77.53%).

</details>


### [24] [LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning](https://arxiv.org/abs/2508.06202)
*Chang Che,Ziqi Wang,Pengwan Yang,Qi Wang,Hui Ma,Zenglin Shi*

Main category: cs.CV

TL;DR: LiLoRA是一种高效架构扩展方法，用于解决多模态大语言模型（MLLMs）在持续视觉指令调整（CVIT）中的灾难性遗忘问题，通过共享LoRA矩阵和低秩分解减少参数冗余，同时引入余弦正则化损失保持表示一致性。


<details>
  <summary>Details</summary>
Motivation: 持续视觉指令调整（CVIT）中，灾难性遗忘和参数效率低是主要挑战，现有方法因全层扩展导致参数冗余和可扩展性差。

Method: 提出LiLoRA方法，共享LoRA矩阵A，对矩阵B进行低秩分解以减少任务特定参数，并引入余弦正则化稳定性损失。

Result: 在多样化CVIT基准测试中，LiLoRA在顺序任务学习中表现优异，参数效率显著优于现有方法。

Conclusion: LiLoRA为CVIT提供了一种高效且可扩展的解决方案，有效平衡了任务学习和参数效率。

Abstract: Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language Models (MLLMs) to incrementally learn new tasks over time. However, this process is challenged by catastrophic forgetting, where performance on previously learned tasks deteriorates as the model adapts to new ones. A common approach to mitigate forgetting is architecture expansion, which introduces task-specific modules to prevent interference. Yet, existing methods often expand entire layers for each task, leading to significant parameter overhead and poor scalability. To overcome these issues, we introduce LoRA in LoRA (LiLoRA), a highly efficient architecture expansion method tailored for CVIT in MLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy, applies an additional low-rank decomposition to matrix B to minimize task-specific parameters, and incorporates a cosine-regularized stability loss to preserve consistency in shared representations over time. Extensive experiments on a diverse CVIT benchmark show that LiLoRA consistently achieves superior performance in sequential task learning while significantly improving parameter efficiency compared to existing approaches.

</details>


### [25] [Towards Unified Image Deblurring using a Mixture-of-Experts Decoder](https://arxiv.org/abs/2508.06228)
*Daniel Feijoo,Paula Garrido-Mellado,Jaesung Rim,Alvaro Garcia,Marcos V. Conde*

Main category: cs.CV

TL;DR: 本文提出了一种通用的图像去模糊方法，能够处理多种模糊类型，包括全局运动、局部运动、低光模糊和散焦模糊。


<details>
  <summary>Details</summary>
Motivation: 现有方法针对特定模糊类型设计，缺乏泛化能力，需要多个模型覆盖不同模糊类型，实际应用中不实用。

Method: 提出了一种混合专家（MoE）解码模块，根据识别的模糊类型动态路由图像特征，实现端到端的精确恢复。

Result: 该方法在性能上媲美专用模型，并在未见过的模糊场景中表现出卓越的鲁棒性和泛化能力。

Conclusion: 提出的统一方法为图像去模糊提供了一种高效且通用的解决方案。

Abstract: Image deblurring, removing blurring artifacts from images, is a fundamental task in computational photography and low-level computer vision. Existing approaches focus on specialized solutions tailored to particular blur types, thus, these solutions lack generalization. This limitation in current methods implies requiring multiple models to cover several blur types, which is not practical in many real scenarios. In this paper, we introduce the first all-in-one deblurring method capable of efficiently restoring images affected by diverse blur degradations, including global motion, local motion, blur in low-light conditions, and defocus blur. We propose a mixture-of-experts (MoE) decoding module, which dynamically routes image features based on the recognized blur degradation, enabling precise and efficient restoration in an end-to-end manner. Our unified approach not only achieves performance comparable to dedicated task-specific models, but also demonstrates remarkable robustness and generalization capabilities on unseen blur degradation scenarios.

</details>


### [26] [Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2508.06318)
*Giacomo D'Amicantonio,Snehashis Majhi,Quan Kong,Lorenzo Garattoni,Gianpiero Francesca,François Bremond,Egor Bondarev*

Main category: cs.CV

TL;DR: 论文提出了一种基于高斯溅射引导的专家混合模型（GS-MoE），用于解决弱监督视频异常检测中的多样性和弱监督信号问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前弱监督视频异常检测模型在处理复杂异常事件时表现不佳，主要因为模型无法区分异常类型的多样性，且弱监督信号缺乏精确的时间信息。

Method: GS-MoE框架采用一组专家模型，每个模型专注于特定异常类型，并通过时间高斯溅射损失增强弱监督信号。

Result: 在UCF-Crime数据集上达到91.58%的AUC，在XD-Violence和MSAD数据集上也表现优异。

Conclusion: GS-MoE通过类别特定专家和时间引导，为弱监督视频异常检测设立了新基准。

Abstract: Video Anomaly Detection (VAD) is a challenging task due to the variability of anomalous events and the limited availability of labeled data. Under the Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided during training, while predictions are made at the frame level. Although state-of-the-art models perform well on simple anomalies (e.g., explosions), they struggle with complex real-world events (e.g., shoplifting). This difficulty stems from two key issues: (1) the inability of current models to address the diversity of anomaly types, as they process all categories with a shared model, overlooking category-specific features; and (2) the weak supervision signal, which lacks precise temporal information, limiting the ability to capture nuanced anomalous patterns blended with normal events. To address these challenges, we propose Gaussian Splatting-guided Mixture of Experts (GS-MoE), a novel framework that employs a set of expert models, each specialized in capturing specific anomaly types. These experts are guided by a temporal Gaussian splatting loss, enabling the model to leverage temporal consistency and enhance weak supervision. The Gaussian splatting approach encourages a more precise and comprehensive representation of anomalies by focusing on temporal segments most likely to contain abnormal events. The predictions from these specialized experts are integrated through a mixture-of-experts mechanism to model complex relationships across diverse anomaly patterns. Our approach achieves state-of-the-art performance, with a 91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on XD-Violence and MSAD datasets. By leveraging category-specific expertise and temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.

</details>


### [27] [Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?](https://arxiv.org/abs/2508.06327)
*Xin Ci Wong,Duygu Sarikaya,Kieran Zucker,Marc De Kamps,Nishant Ravikumar*

Main category: cs.CV

TL;DR: 提出一种基于扩散模型的方法，生成合成心脏MR图像以解决域偏移问题，显著提升多中心分割性能。


<details>
  <summary>Details</summary>
Motivation: 心脏MR成像因设备和协议差异导致域偏移，限制了AI模型在实际场景中的应用。传统方法如数据增强或迁移学习存在局限性。

Method: 使用扩散模型生成合成图像，保持结构和空间一致性，并评估其在多中心分割中的效果。

Result: 合成数据显著提升了分割性能（p < 0.01），优于仅使用真实数据的方法。

Conclusion: 该方法有效解决了域偏移问题，尤其适用于数据稀缺场景，无需依赖迁移学习或在线训练。

Abstract: Magnetic resonance (MR) imaging, including cardiac MR, is prone to domain shift due to variations in imaging devices and acquisition protocols. This challenge limits the deployment of trained AI models in real-world scenarios, where performance degrades on unseen domains. Traditional solutions involve increasing the size of the dataset through ad-hoc image augmentation or additional online training/transfer learning, which have several limitations. Synthetic data offers a promising alternative, but anatomical/structural consistency constraints limit the effectiveness of generative models in creating image-label pairs. To address this, we propose a diffusion model (DM) trained on a source domain that generates synthetic cardiac MR images that resemble a given reference. The synthetic data maintains spatial and structural fidelity, ensuring similarity to the source domain and compatibility with the segmentation mask. We assess the utility of our generative approach in multi-centre cardiac MR segmentation, using the 2D nnU-Net, 3D nnU-Net and vanilla U-Net segmentation networks. We explore domain generalisation, where, domain-invariant segmentation models are trained on synthetic source domain data, and domain adaptation, where, we shift target domain data towards the source domain using the DM. Both strategies significantly improved segmentation performance on data from an unseen target domain, in terms of surface-based metrics (Welch's t-test, p < 0.01), compared to training segmentation models on real data alone. The proposed method ameliorates the need for transfer learning or online training to address domain shift challenges in cardiac MR image analysis, especially useful in data-scarce settings.

</details>


### [28] [ViPro-2: Unsupervised State Estimation via Integrated Dynamics for Guiding Video Prediction](https://arxiv.org/abs/2508.06335)
*Patrick Takenaka,Johannes Maucher,Marco F. Huber*

Main category: cs.CV

TL;DR: 改进ViPro模型，使其能从未知初始状态正确推断视频帧，并在无监督下完成，同时扩展数据集以适应真实场景。


<details>
  <summary>Details</summary>
Motivation: 解决ViPro模型因依赖初始真实状态而无法处理噪声观测的问题，提升其在实际场景中的应用能力。

Method: 在ViPro基础上添加改进，使其能从未知初始状态推断状态，并以无监督方式实现，同时扩展3D数据集。

Result: 模型能够从未知初始状态正确推断状态，且在噪声观测下表现更稳健。

Conclusion: 改进后的ViPro模型在无监督下能更准确推断状态，适用于更复杂的真实场景。

Abstract: Predicting future video frames is a challenging task with many downstream applications. Previous work has shown that procedural knowledge enables deep models for complex dynamical settings, however their model ViPro assumed a given ground truth initial symbolic state. We show that this approach led to the model learning a shortcut that does not actually connect the observed environment with the predicted symbolic state, resulting in the inability to estimate states given an observation if previous states are noisy. In this work, we add several improvements to ViPro that enables the model to correctly infer states from observations without providing a full ground truth state in the beginning. We show that this is possible in an unsupervised manner, and extend the original Orbits dataset with a 3D variant to close the gap to real world scenarios.

</details>


### [29] [FVGen: Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation](https://arxiv.org/abs/2508.06392)
*Wenbin Teng,Gonglin Chen,Haiwei Chen,Yajie Zhao*

Main category: cs.CV

TL;DR: FVGen提出了一种快速新视角合成框架，通过视频扩散模型蒸馏技术，在仅需四步采样的情况下生成高质量3D视图，显著提升了时间效率。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角下的3D重建存在未观察区域的伪影问题，现有方法依赖视频扩散模型（VDMs）生成密集观测，但采样速度慢。

Method: 提出FVGen框架，利用生成对抗网络（GANs）和软化反向KL散度最小化，将多步去噪教师模型蒸馏为少步去噪学生模型。

Result: 实验表明，FVGen在生成相同数量新视角时，视觉质量相当或更好，采样时间减少90%以上。

Conclusion: FVGen显著提升了稀疏输入视角下3D重建任务的时间效率。

Abstract: Recent progress in 3D reconstruction has enabled realistic 3D models from dense image captures, yet challenges persist with sparse views, often leading to artifacts in unseen areas. Recent works leverage Video Diffusion Models (VDMs) to generate dense observations, filling the gaps when only sparse views are available for 3D reconstruction tasks. A significant limitation of these methods is their slow sampling speed when using VDMs. In this paper, we present FVGen, a novel framework that addresses this challenge by enabling fast novel view synthesis using VDMs in as few as four sampling steps. We propose a novel video diffusion model distillation method that distills a multi-step denoising teacher model into a few-step denoising student model using Generative Adversarial Networks (GANs) and softened reverse KL-divergence minimization. Extensive experiments on real-world datasets show that, compared to previous works, our framework generates the same number of novel views with similar (or even better) visual quality while reducing sampling time by more than 90%. FVGen significantly improves time efficiency for downstream reconstruction tasks, particularly when working with sparse input views (more than 2) where pre-trained VDMs need to be run multiple times to achieve better spatial coverage.

</details>


### [30] [A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery](https://arxiv.org/abs/2508.06407)
*Ch Muhammad Awais,Marco Reggiannini,Davide Moroni,Oktay Karakus*

Main category: cs.CV

TL;DR: 论文探讨了将分类目标直接融入超分辨率过程是否能提升分类准确性，并提出了一种新方法，通过优化损失函数同时提升图像质量和分类性能。


<details>
  <summary>Details</summary>
Motivation: 低分辨率图像限制了自动化分析的准确性，传统超分辨率方法仅关注像素级指标，未充分探索超分辨率图像保真度与下游分类性能的关系。

Method: 提出了一种新颖的方法，通过优化同时考虑图像质量和分类性能的损失函数，提升合成孔径雷达图像的分辨率。

Result: 该方法不仅提高了图像质量（通过科学验证的图像质量指标），还增强了分类准确性。

Conclusion: 研究表明，将分类目标融入超分辨率过程可以同时提升图像质量和分类性能。

Abstract: High-resolution imagery plays a critical role in improving the performance of visual recognition tasks such as classification, detection, and segmentation. In many domains, including remote sensing and surveillance, low-resolution images can limit the accuracy of automated analysis. To address this, super-resolution (SR) techniques have been widely adopted to attempt to reconstruct high-resolution images from low-resolution inputs. Related traditional approaches focus solely on enhancing image quality based on pixel-level metrics, leaving the relationship between super-resolved image fidelity and downstream classification performance largely underexplored. This raises a key question: can integrating classification objectives directly into the super-resolution process further improve classification accuracy? In this paper, we try to respond to this question by investigating the relationship between super-resolution and classification through the deployment of a specialised algorithmic strategy. We propose a novel methodology that increases the resolution of synthetic aperture radar imagery by optimising loss functions that account for both image quality and classification performance. Our approach improves image quality, as measured by scientifically ascertained image quality indicators, while also enhancing classification accuracy.

</details>


### [31] [SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation](https://arxiv.org/abs/2508.06429)
*Guido Manni,Clemente Lauretti,Loredana Zollo,Paolo Soda*

Main category: cs.CV

TL;DR: 提出一种基于GAN的半监督学习框架，针对医学影像中标记数据稀缺的问题，通过集成生成器、判别器和分类器，结合伪标签技术，显著提升了小样本场景下的分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像中因标记数据不足而导致的深度学习模型性能受限问题。

Method: 采用三阶段训练框架，结合生成对抗网络（GAN）和伪标签技术，利用少量标记数据和大量未标记数据进行半监督学习。

Result: 在11个MedMNIST数据集上显著优于六种最先进的GAN半监督方法，尤其在5-shot极端场景下表现突出。

Conclusion: 该框架为标记成本高的医学影像应用提供了实用解决方案，即使在极少量标记数据下也能实现稳健分类。

Abstract: Deep learning has revolutionized medical imaging, but its effectiveness is severely limited by insufficient labeled training data. This paper introduces a novel GAN-based semi-supervised learning framework specifically designed for low labeled-data regimes, evaluated across settings with 5 to 50 labeled samples per class. Our approach integrates three specialized neural networks -- a generator for class-conditioned image translation, a discriminator for authenticity assessment and classification, and a dedicated classifier -- within a three-phase training framework. The method alternates between supervised training on limited labeled data and unsupervised learning that leverages abundant unlabeled images through image-to-image translation rather than generation from noise. We employ ensemble-based pseudo-labeling that combines confidence-weighted predictions from the discriminator and classifier with temporal consistency through exponential moving averaging, enabling reliable label estimation for unlabeled data. Comprehensive evaluation across eleven MedMNIST datasets demonstrates that our approach achieves statistically significant improvements over six state-of-the-art GAN-based semi-supervised methods, with particularly strong performance in the extreme 5-shot setting where the scarcity of labeled data is most challenging. The framework maintains its superiority across all evaluated settings (5, 10, 20, and 50 shots per class). Our approach offers a practical solution for medical imaging applications where annotation costs are prohibitive, enabling robust classification performance even with minimal labeled data. Code is available at https://github.com/GuidoManni/SPARSE.

</details>


### [32] [MotionSwap](https://arxiv.org/abs/2508.06430)
*Om Patil,Jinesh Modi,Suryabha Mukhopadhyay,Meghaditya Giri,Chhavi Malhotra*

Main category: cs.CV

TL;DR: 本文介绍了对SimSwap框架的改进，通过引入自注意力与交叉注意力机制、动态损失加权和余弦退火学习率调度，显著提升了换脸效果。


<details>
  <summary>Details</summary>
Motivation: 提升换脸技术在身份保留、属性一致性和视觉质量方面的表现。

Method: 在生成器中集成自注意力与交叉注意力机制，采用动态损失加权和余弦退火学习率调度。

Result: 改进后的模型在身份相似性、FID分数和视觉质量上优于基线，并通过消融实验验证了各改进的重要性。

Conclusion: 未来方向包括整合StyleGAN3、改进唇同步、引入3D面部建模和视频应用中的时间一致性。

Abstract: Face swapping technology has gained significant attention in both academic research and commercial applications. This paper presents our implementation and enhancement of SimSwap, an efficient framework for high fidelity face swapping. We introduce several improvements to the original model, including the integration of self and cross-attention mechanisms in the generator architecture, dynamic loss weighting, and cosine annealing learning rate scheduling. These enhancements lead to significant improvements in identity preservation, attribute consistency, and overall visual quality.   Our experimental results, spanning 400,000 training iterations, demonstrate progressive improvements in generator and discriminator performance. The enhanced model achieves better identity similarity, lower FID scores, and visibly superior qualitative results compared to the baseline. Ablation studies confirm the importance of each architectural and training improvement. We conclude by identifying key future directions, such as integrating StyleGAN3, improving lip synchronization, incorporating 3D facial modeling, and introducing temporal consistency for video-based applications.

</details>


### [33] [LightSwitch: Multi-view Relighting with Material-guided Diffusion](https://arxiv.org/abs/2508.06494)
*Yehonathan Litman,Fernando De la Torre,Shubham Tulsiani*

Main category: cs.CV

TL;DR: Lightswitch是一种基于扩散框架的3D重光照方法，通过利用多视角和材质信息，高效地将输入图像重光照到目标光照条件。


<details>
  <summary>Details</summary>
Motivation: 现有2D重光照生成先验未充分利用物体固有属性或大规模多视角数据，导致重光照效果不佳。

Method: 提出Lightswitch，一种微调的材料重光照扩散框架，结合多视角和材质信息，采用可扩展的去噪方案。

Result: Lightswitch在2D重光照预测质量上超越现有方法，并在合成和真实物体重光照任务中表现优异。

Conclusion: Lightswitch通过结合固有属性和多视角数据，实现了高效且一致的重光照效果。

Abstract: Recent approaches for 3D relighting have shown promise in integrating 2D image relighting generative priors to alter the appearance of a 3D representation while preserving the underlying structure. Nevertheless, generative priors used for 2D relighting that directly relight from an input image do not take advantage of intrinsic properties of the subject that can be inferred or cannot consider multi-view data at scale, leading to subpar relighting. In this paper, we propose Lightswitch, a novel finetuned material-relighting diffusion framework that efficiently relights an arbitrary number of input images to a target lighting condition while incorporating cues from inferred intrinsic properties. By using multi-view and material information cues together with a scalable denoising scheme, our method consistently and efficiently relights dense multi-view data of objects with diverse material compositions. We show that our 2D relighting prediction quality exceeds previous state-of-the-art relighting priors that directly relight from images. We further demonstrate that LightSwitch matches or outperforms state-of-the-art diffusion inverse rendering methods in relighting synthetic and real objects in as little as 2 minutes.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [34] [Universally Unfiltered and Unseen:Input-Agnostic Multimodal Jailbreaks against Text-to-Image Model Safeguards](https://arxiv.org/abs/2508.05658)
*Song Yan,Hui Wei,Jinlong Fei,Guoliang Yang,Zhengyu Zhao,Zheng Wamg*

Main category: cs.CR

TL;DR: 论文提出了一种名为U3-Attack的多模态越狱攻击方法，用于绕过文本到图像（T2I）模型的安全检查器和提示过滤器，解决了现有方法扩展性差和优化耗时的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态越狱攻击方法局限于特定提示和图像的扰动，扩展性差且优化耗时，因此需要一种更高效且通用的攻击方法。

Method: U3-Attack通过优化图像背景的对抗性补丁来绕过安全检查器，并优化敏感词的安全释义集来绕过提示过滤器，同时减少冗余计算。

Result: 实验表明，U3-Attack在开源和商业T2I模型上表现优异，例如在Runway-inpainting模型上，其成功率比现有最佳方法高4倍。

Conclusion: U3-Attack是一种高效且通用的多模态越狱攻击方法，显著提升了绕过T2I模型安全措施的成功率。

Abstract: Various (text) prompt filters and (image) safety checkers have been implemented to mitigate the misuse of Text-to-Image (T2I) models in creating Not-Safe-For-Work (NSFW) content.In order to expose potential security vulnerabilities of such safeguards, multimodal jailbreaks have been studied.However, existing jailbreaks are limited to prompt-specific and image-specific perturbations, which suffer from poor scalability and time-consuming optimization.To address these limitations, we propose Universally Unfiltered and Unseen (U3)-Attack, a multimodal jailbreak attack method against T2I safeguards.Specifically, U3-Attack optimizes an adversarial patch on the image background to universally bypass safety checkers and optimizes a safe paraphrase set from a sensitive word to universally bypass prompt filters while eliminating redundant computations.Extensive experimental results demonstrate the superiority of our U3-Attack on both open-source and commercial T2I models.For example, on the commercial Runway-inpainting model with both prompt filter and safety checker, our U3-Attack achieves $~4\times$ higher success rates than the state-of-the-art multimodal jailbreak attack, MMA-Diffusion.Content Warning: This paper includes examples of NSFW content.

</details>


### [35] [Anti-Tamper Protection for Unauthorized Individual Image Generation](https://arxiv.org/abs/2508.06325)
*Zelin Li,Ruohan Zong,Yifan Liu,Ruichen Yao,Yaokun Liu,Yang Zhang,Dong Wang*

Main category: cs.CR

TL;DR: 论文提出了一种新型的抗篡改扰动（ATP）方法，通过结合保护扰动和授权扰动，有效防御伪造攻击并检测净化篡改。


<details>
  <summary>Details</summary>
Motivation: 随着个性化图像生成技术的发展，伪造攻击侵犯肖像权和隐私的问题日益严重，现有保护扰动算法易被净化技术绕过。

Method: ATP在频域中引入保护扰动和授权扰动，通过掩模指导确保两者互不干扰，授权扰动分布在全图像素以保持对净化篡改的敏感性。

Result: 实验表明，ATP在各种攻击场景下均能有效防御伪造攻击，为肖像权和隐私保护提供了鲁棒解决方案。

Conclusion: ATP通过双重扰动设计，解决了现有保护算法易被绕过的问题，为图像安全提供了新思路。

Abstract: With the advancement of personalized image generation technologies, concerns about forgery attacks that infringe on portrait rights and privacy are growing. To address these concerns, protection perturbation algorithms have been developed to disrupt forgery generation. However, the protection algorithms would become ineffective when forgery attackers apply purification techniques to bypass the protection. To address this issue, we present a novel approach, Anti-Tamper Perturbation (ATP). ATP introduces a tamper-proof mechanism within the perturbation. It consists of protection and authorization perturbations, where the protection perturbation defends against forgery attacks, while the authorization perturbation detects purification-based tampering. Both protection and authorization perturbations are applied in the frequency domain under the guidance of a mask, ensuring that the protection perturbation does not disrupt the authorization perturbation. This design also enables the authorization perturbation to be distributed across all image pixels, preserving its sensitivity to purification-based tampering. ATP demonstrates its effectiveness in defending forgery attacks across various attack settings through extensive experiments, providing a robust solution for protecting individuals' portrait rights and privacy. Our code is available at: https://github.com/Seeyn/Anti-Tamper-Perturbation .

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [36] [ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation](https://arxiv.org/abs/2508.06065)
*Daniel Lee,Nikhil Sharma,Donghoon Shin,DaEun Choi,Harsh Sharma,Jeonghwan Kim,Heng Ji*

Main category: cs.HC

TL;DR: ThematicPlane系统通过交互式主题设计平面帮助用户探索和操作高级语义概念，弥合创意意图与系统控制之间的差距。


<details>
  <summary>Details</summary>
Motivation: 生成式AI使图像创作更易获取，但非专家用户难以精确表达创意意图，现有工具限制了流畅探索。

Method: 引入ThematicPlane系统，支持用户通过交互式主题设计平面操作语义概念（如情绪、风格或叙事基调）。

Result: 探索性研究（N=6）显示，用户能在发散和收敛创意模式中工作，但需更可解释的控制。

Conclusion: ThematicPlane支持迭代式创意流程，为生成设计工具提供了语义驱动交互的新方向。

Abstract: Generative AI has made image creation more accessible, yet aligning outputs with nuanced creative intent remains challenging, particularly for non-experts. Existing tools often require users to externalize ideas through prompts or references, limiting fluid exploration. We introduce ThematicPlane, a system that enables users to navigate and manipulate high-level semantic concepts (e.g., mood, style, or narrative tone) within an interactive thematic design plane. This interface bridges the gap between tacit creative intent and system control. In our exploratory study (N=6), participants engaged in divergent and convergent creative modes, often embracing unexpected results as inspiration or iteration cues. While they grounded their exploration in familiar themes, differing expectations of how themes mapped to outputs revealed a need for more explainable controls. Overall, ThematicPlane fosters expressive, iterative workflows and highlights new directions for intuitive, semantics-driven interaction in generative design tools.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [37] [Clinically-guided Data Synthesis for Laryngeal Lesion Detection](https://arxiv.org/abs/2508.06182)
*Chiara Baldini,Kaisar Kushibar,Richard Osuala,Simone Balocco,Oliver Diaz,Karim Lekadir,Leonardo S. Mattos*

Main category: eess.IV

TL;DR: 该论文提出了一种利用潜在扩散模型（LDM）和ControlNet适配器生成喉镜图像-标注对的方法，以解决喉镜CADx/e系统数据稀缺问题。实验表明，仅添加10%合成数据即可显著提升病变检测率。


<details>
  <summary>Details</summary>
Motivation: 喉镜CADx/e系统因数据稀缺和病变异质性高而发展受限，现有方法依赖专家经验和活检，成本高且风险大。

Method: 结合LDM和ControlNet适配器，生成高质量、临床相关的喉镜图像-标注对，扩展训练数据集。

Result: 添加10%合成数据后，病变检测率在内部测试中提升9%，外部数据中提升22.1%；生成图像的真实性得到专家认可。

Conclusion: 该方法有效缓解数据稀缺问题，展示了合成数据在喉镜CADx/e系统中的实际应用潜力。

Abstract: Although computer-aided diagnosis (CADx) and detection (CADe) systems have made significant progress in various medical domains, their application is still limited in specialized fields such as otorhinolaryngology. In the latter, current assessment methods heavily depend on operator expertise, and the high heterogeneity of lesions complicates diagnosis, with biopsy persisting as the gold standard despite its substantial costs and risks. A critical bottleneck for specialized endoscopic CADx/e systems is the lack of well-annotated datasets with sufficient variability for real-world generalization. This study introduces a novel approach that exploits a Latent Diffusion Model (LDM) coupled with a ControlNet adapter to generate laryngeal endoscopic image-annotation pairs, guided by clinical observations. The method addresses data scarcity by conditioning the diffusion process to produce realistic, high-quality, and clinically relevant image features that capture diverse anatomical conditions. The proposed approach can be leveraged to expand training datasets for CADx/e models, empowering the assessment process in laryngology. Indeed, during a downstream task of detection, the addition of only 10% synthetic data improved the detection rate of laryngeal lesions by 9% when the model was internally tested and 22.1% on out-of-domain external data. Additionally, the realism of the generated images was evaluated by asking 5 expert otorhinolaryngologists with varying expertise to rate their confidence in distinguishing synthetic from real images. This work has the potential to accelerate the development of automated tools for laryngeal disease diagnosis, offering a solution to data scarcity and demonstrating the applicability of synthetic data in real-world scenarios.

</details>


### [38] [Multivariate Fields of Experts](https://arxiv.org/abs/2508.06490)
*Stanislas Ducotterd,Michael Unser*

Main category: eess.IV

TL;DR: 提出了一种基于多元专家场的新框架，用于学习图像先验，通过引入多元势函数改进现有方法，在多个逆问题上表现优异。


<details>
  <summary>Details</summary>
Motivation: 改进现有专家场方法，提升图像先验学习的效果，同时保持高效性和可解释性。

Method: 利用Moreau包络构造多元势函数，扩展了专家场模型，应用于图像去噪、去模糊、压缩感知MRI和CT等逆问题。

Result: 性能优于单变量模型，接近基于深度学习的正则化方法，但速度更快、参数更少、训练数据需求更低，且保持较高可解释性。

Conclusion: 多元专家场框架在逆问题中表现优异，兼具高效性和可解释性，为图像先验学习提供了新思路。

Abstract: We introduce the multivariate fields of experts, a new framework for the learning of image priors. Our model generalizes existing fields of experts methods by incorporating multivariate potential functions constructed via Moreau envelopes of the $\ell_\infty$-norm. We demonstrate the effectiveness of our proposal across a range of inverse problems that include image denoising, deblurring, compressed-sensing magnetic-resonance imaging, and computed tomography. The proposed approach outperforms comparable univariate models and achieves performance close to that of deep-learning-based regularizers while being significantly faster, requiring fewer parameters, and being trained on substantially fewer data. In addition, our model retains a relatively high level of interpretability due to its structured design.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [39] [Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports](https://arxiv.org/abs/2508.05669)
*Jin Khye Tan,En Jun Choong,Ethan Jeremiah Chitty,Yan Pheng Choo,John Hsin Yang Wong,Chern Eu Cheah*

Main category: cs.IR

TL;DR: 该研究提出了一种基于Qwen2.5-VL-7B的视觉语言模型，用于将马来西亚审计财务报表中的表格转换为Markdown格式，性能显著优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 解决从财务文档中准确提取和表示表格结构的挑战，尤其是针对监管和分析用例的需求。

Method: 使用Qwen2.5-VL-7B微调的视觉语言模型，结合2,152个图像-文本对的数据集和LoRA监督微调策略。

Result: 模型在标准评估中达到92.20%的准确率和96.53%的Markdown TEDS分数，性能优于其他模型。

Conclusion: 领域特定微调是连接非结构化财务文档与下游自动化的高效方法，性能优于大型通用模型。

Abstract: Accurately extracting and representing the structure of tabular data from financial documents remains a critical challenge in document understanding, particularly for regulatory and analytical use cases. This study addresses the complexity of converting financial tables from Malaysian audited financial reports into Markdown format, a task complicated by rotated layouts, multi-level headers, and implicit structural cues. We propose a fine-tuned vision-language model (VLM), based on Qwen2.5-VL-7B, optimized for high-fidelity Markdown generation from document images. Our approach includes a curated dataset of 2,152 image-text pairs with augmentations and a supervised fine-tuning strategy using LoRA. To assess performance, we evaluated our model on 100 out-of-sample tables using a dual framework: a criteria-based LLM-as-a-judge for fine-grained accuracy and our novel Markdown Tree-Edit-Distance-based Similarity (TEDS) metric for holistic structural fidelity. Our model achieves a 92.20% overall accuracy on the criteria-based assessment and a 96.53% Markdown TEDS score. This performance significantly surpasses its Qwen2.5-VL-7B base model, larger-scale VLMs, and specialized reasoning-enabled models. Compared to these self-hosted alternatives, it also significantly reduces inference time. Furthermore, its accuracy exceeds that of widely used proprietary models such as OpenAI's GPT-4o and Gemini 2.5 Flash. These results demonstrate that domain-specific fine-tuning provides an effective and efficient method to bridge the gap between unstructured financial documents and downstream automation, rivalling much larger and more general models without their computational overhead.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [40] [Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models](https://arxiv.org/abs/2508.06151)
*Yong Oh Lee,JeeEun Kim,Jung Woo Lee*

Main category: cs.LG

TL;DR: 研究提出了一种基于扩散模型的图像修复技术，用于生成高保真度的口腔癌病变合成图像，显著提升了诊断模型的性能。


<details>
  <summary>Details</summary>
Motivation: 口腔癌诊断中，标注数据集的稀缺性和多样性不足限制了诊断模型的性能，因此需要一种方法增强数据。

Method: 使用微调的扩散模型和图像修复技术生成逼真的口腔癌病变合成图像，并结合多源数据集训练诊断模型。

Result: 分类模型在区分癌变和非癌变组织时准确率达0.97，检测模型定位病变的准确率为0.85。

Conclusion: 合成图像生成在医学诊断中具有潜力，未来可扩展至其他癌症诊断领域。

Abstract: In oral cancer diagnostics, the limited availability of annotated datasets frequently constrains the performance of diagnostic models, particularly due to the variability and insufficiency of training data. To address these challenges, this study proposed a novel approach to enhance diagnostic accuracy by synthesizing realistic oral cancer lesions using an inpainting technique with a fine-tuned diffusion model. We compiled a comprehensive dataset from multiple sources, featuring a variety of oral cancer images. Our method generated synthetic lesions that exhibit a high degree of visual fidelity to actual lesions, thereby significantly enhancing the performance of diagnostic algorithms. The results show that our classification model achieved a diagnostic accuracy of 0.97 in differentiating between cancerous and non-cancerous tissues, while our detection model accurately identified lesion locations with 0.85 accuracy. This method validates the potential for synthetic image generation in medical diagnostics and paves the way for further research into extending these methods to other types of cancer diagnostics.

</details>
