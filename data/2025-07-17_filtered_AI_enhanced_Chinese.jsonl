{"id": "2507.11971", "pdf": "https://arxiv.org/pdf/2507.11971", "abs": "https://arxiv.org/abs/2507.11971", "authors": ["Tielong Wang", "Yuxuan Xiong", "Jinfan Liu", "Zhifan Zhang", "Ye Chen", "Yue Shi", "Bingbing Ni"], "title": "HPR3D: Hierarchical Proxy Representation for High-Fidelity 3D Reconstruction and Controllable Editing", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Current 3D representations like meshes, voxels, point clouds, and NeRF-based neural implicit fields exhibit significant limitations: they are often task-specific, lacking universal applicability across reconstruction, generation, editing, and driving. While meshes offer high precision, their dense vertex data complicates editing; NeRFs deliver excellent rendering but suffer from structural ambiguity, hindering animation and manipulation; all representations inherently struggle with the trade-off between data complexity and fidelity. To overcome these issues, we introduce a novel 3D Hierarchical Proxy Node representation. Its core innovation lies in representing an object's shape and texture via a sparse set of hierarchically organized (tree-structured) proxy nodes distributed on its surface and interior. Each node stores local shape and texture information (implicitly encoded by a small MLP) within its neighborhood. Querying any 3D coordinate's properties involves efficient neural interpolation and lightweight decoding from relevant nearby and parent nodes. This framework yields a highly compact representation where nodes align with local semantics, enabling direct drag-and-edit manipulation, and offers scalable quality-complexity control. Extensive experiments across 3D reconstruction and editing demonstrate our method's expressive efficiency, high-fidelity rendering quality, and superior editability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u76843D\u5c42\u6b21\u4ee3\u7406\u8282\u70b9\u8868\u793a\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u67093D\u8868\u793a\u5728\u901a\u7528\u6027\u3001\u7f16\u8f91\u6027\u548c\u590d\u6742\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u67093D\u8868\u793a\uff08\u5982\u7f51\u683c\u3001\u4f53\u7d20\u3001\u70b9\u4e91\u548cNeRF\uff09\u5b58\u5728\u4efb\u52a1\u7279\u5b9a\u6027\u3001\u7f16\u8f91\u56f0\u96be\u548c\u7ed3\u6784\u6a21\u7cca\u7b49\u95ee\u9898\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002", "method": "\u901a\u8fc7\u7a00\u758f\u7684\u5c42\u6b21\u5316\u4ee3\u7406\u8282\u70b9\u8868\u793a\u7269\u4f53\u7684\u5f62\u72b6\u548c\u7eb9\u7406\uff0c\u6bcf\u4e2a\u8282\u70b9\u5b58\u50a8\u5c40\u90e8\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u89e3\u7801\u548c\u795e\u7ecf\u63d2\u503c\u5b9e\u73b0\u9ad8\u6548\u67e5\u8be2\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u57283D\u91cd\u5efa\u548c\u7f16\u8f91\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u8868\u8fbe\u3001\u9ad8\u4fdd\u771f\u6e32\u67d3\u548c\u4f18\u8d8a\u7684\u7f16\u8f91\u6027\u3002", "conclusion": "\u63d0\u51fa\u76843D\u5c42\u6b21\u4ee3\u7406\u8282\u70b9\u8868\u793a\u65b9\u6cd5\u5728\u901a\u7528\u6027\u3001\u7f16\u8f91\u6027\u548c\u590d\u6742\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2507.11554", "pdf": "https://arxiv.org/pdf/2507.11554", "abs": "https://arxiv.org/abs/2507.11554", "authors": ["Zejian Li", "Yize Li", "Chenye Meng", "Zhongni Liu", "Yang Ling", "Shengyuan Zhang", "Guang Yang", "Changyuan Yang", "Zhiyuan Yang", "Lingyun Sun"], "title": "Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in diffusion models (DMs) have been propelled by alignment methods that post-train models to better conform to human preferences. However, these approaches typically require computation-intensive training of a base model and a reward model, which not only incurs substantial computational overhead but may also compromise model accuracy and training efficiency. To address these limitations, we propose Inversion-DPO, a novel alignment framework that circumvents reward modeling by reformulating Direct Preference Optimization (DPO) with DDIM inversion for DMs. Our method conducts intractable posterior sampling in Diffusion-DPO with the deterministic inversion from winning and losing samples to noise and thus derive a new post-training paradigm. This paradigm eliminates the need for auxiliary reward models or inaccurate appromixation, significantly enhancing both precision and efficiency of training. We apply Inversion-DPO to a basic task of text-to-image generation and a challenging task of compositional image generation. Extensive experiments show substantial performance improvements achieved by Inversion-DPO compared to existing post-training methods and highlight the ability of the trained generative models to generate high-fidelity compositionally coherent images. For the post-training of compostitional image geneation, we curate a paired dataset consisting of 11,140 images with complex structural annotations and comprehensive scores, designed to enhance the compositional capabilities of generative models. Inversion-DPO explores a new avenue for efficient, high-precision alignment in diffusion models, advancing their applicability to complex realistic generation tasks. Our code is available at https://github.com/MIGHTYEZ/Inversion-DPO", "AI": {"tldr": "Inversion-DPO\u662f\u4e00\u79cd\u65b0\u7684\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7DDIM\u53cd\u6f14\u6539\u8fdbDPO\uff0c\u907f\u514d\u4e86\u5956\u52b1\u5efa\u6a21\uff0c\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u8bad\u7ec3\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u9700\u8981\u8ba1\u7b97\u5bc6\u96c6\u578b\u8bad\u7ec3\uff0c\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0b\u964d\uff0cInversion-DPO\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5229\u7528DDIM\u53cd\u6f14\u5c06DPO\u91cd\u65b0\u8868\u8ff0\uff0c\u901a\u8fc7\u4ece\u6837\u672c\u5230\u566a\u58f0\u7684\u786e\u5b9a\u6027\u53cd\u6f14\u8fdb\u884c\u540e\u9a8c\u91c7\u6837\uff0c\u65e0\u9700\u8f85\u52a9\u5956\u52b1\u6a21\u578b\u3002", "result": "\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u548c\u7ec4\u5408\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0cInversion-DPO\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u9ad8\u4fdd\u771f\u4e14\u7ec4\u5408\u4e00\u81f4\u7684\u56fe\u50cf\u3002", "conclusion": "Inversion-DPO\u4e3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9ad8\u7cbe\u5ea6\u7684\u5bf9\u9f50\u65b0\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73b0\u5b9e\u751f\u6210\u4efb\u52a1\u3002"}}
{"id": "2507.11562", "pdf": "https://arxiv.org/pdf/2507.11562", "abs": "https://arxiv.org/abs/2507.11562", "authors": ["Ozer Can Devecioglu", "Serkan Kiranyaz", "Mehmet Yamac", "Moncef Gabbouj"], "title": "Expert Operational GANS: Towards Real-Color Underwater Image Restoration", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "6 pages", "summary": "The wide range of deformation artifacts that arise from complex light propagation, scattering, and depth-dependent attenuation makes the underwater image restoration to remain a challenging problem. Like other single deep regressor networks, conventional GAN-based restoration methods struggle to perform well across this heterogeneous domain, since a single generator network is typically insufficient to capture the full range of visual degradations. In order to overcome this limitation, we propose xOp-GAN, a novel GAN model with several expert generator networks, each trained solely on a particular subset with a certain image quality. Thus, each generator can learn to maximize its restoration performance for a particular quality range. Once a xOp-GAN is trained, each generator can restore the input image and the best restored image can then be selected by the discriminator based on its perceptual confidence score. As a result, xOP-GAN is the first GAN model with multiple generators where the discriminator is being used during the inference of the regression task. Experimental results on benchmark Large Scale Underwater Image (LSUI) dataset demonstrates that xOp-GAN achieves PSNR levels up to 25.16 dB, surpassing all single-regressor models by a large margin even, with reduced complexity.", "AI": {"tldr": "xOp-GAN\u662f\u4e00\u79cd\u65b0\u578bGAN\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u4e2a\u4e13\u5bb6\u751f\u6210\u5668\u7f51\u7edc\u5206\u522b\u5904\u7406\u4e0d\u540c\u8d28\u91cf\u8303\u56f4\u7684\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u6c34\u4e0b\u56fe\u50cf\u6062\u590d\u4e2d\u7684\u5f02\u8d28\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5355\u751f\u6210\u5668GAN\u96be\u4ee5\u5e94\u5bf9\u6c34\u4e0b\u56fe\u50cf\u6062\u590d\u4e2d\u7684\u590d\u6742\u9000\u5316\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5904\u7406\u5f02\u8d28\u6027\u57df\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faxOp-GAN\uff0c\u5305\u542b\u591a\u4e2a\u4e13\u5bb6\u751f\u6210\u5668\uff0c\u6bcf\u4e2a\u751f\u6210\u5668\u4e13\u6ce8\u4e8e\u7279\u5b9a\u8d28\u91cf\u8303\u56f4\u7684\u56fe\u50cf\u6062\u590d\uff0c\u5e76\u901a\u8fc7\u5224\u522b\u5668\u9009\u62e9\u6700\u4f73\u7ed3\u679c\u3002", "result": "\u5728LSUI\u6570\u636e\u96c6\u4e0a\uff0cxOp-GAN\u7684PSNR\u8fbe\u523025.16 dB\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u56de\u5f52\u5668\u6a21\u578b\u3002", "conclusion": "xOp-GAN\u901a\u8fc7\u591a\u751f\u6210\u5668\u548c\u5224\u522b\u5668\u534f\u540c\u5de5\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u4e0b\u56fe\u50cf\u6062\u590d\u7684\u6027\u80fd\u3002"}}
{"id": "2507.11638", "pdf": "https://arxiv.org/pdf/2507.11638", "abs": "https://arxiv.org/abs/2507.11638", "authors": ["Benjamin Keel", "Aaron Quyn", "David Jayne", "Maryam Mohsin", "Samuel D. Relton"], "title": "Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Published in Medical Image Understanding and Analysis (MIUA) 2025", "summary": "Effective treatment for rectal cancer relies on accurate lymph node metastasis (LNM) staging. However, radiological criteria based on lymph node (LN) size, shape and texture morphology have limited diagnostic accuracy. In this work, we investigate applying a Variational Autoencoder (VAE) as a feature encoder model to replace the large pre-trained Convolutional Neural Network (CNN) used in existing approaches. The motivation for using a VAE is that the generative model aims to reconstruct the images, so it directly encodes visual features and meaningful patterns across the data. This leads to a disentangled and structured latent space which can be more interpretable than a CNN. Models are deployed on an in-house MRI dataset with 168 patients who did not undergo neo-adjuvant treatment. The post-operative pathological N stage was used as the ground truth to evaluate model predictions. Our proposed model 'VAE-MLP' achieved state-of-the-art performance on the MRI dataset, with cross-validated metrics of AUC 0.86 +/- 0.05, Sensitivity 0.79 +/- 0.06, and Specificity 0.85 +/- 0.05. Code is available at: https://github.com/benkeel/Lymph_Node_Classification_MIUA.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u7684\u7279\u5f81\u7f16\u7801\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u9ad8\u76f4\u80a0\u764c\u6dcb\u5df4\u7ed3\u8f6c\u79fb\uff08LNM\uff09\u5206\u671f\u7684\u51c6\u786e\u6027\uff0c\u66ff\u4ee3\u4e86\u4f20\u7edf\u7684\u5927\u578b\u9884\u8bad\u7ec3\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6dcb\u5df4\u7ed3\u5927\u5c0f\u3001\u5f62\u72b6\u548c\u7eb9\u7406\u5f62\u6001\u7684\u653e\u5c04\u5b66\u6807\u51c6\u8bca\u65ad\u51c6\u786e\u6027\u6709\u9650\uff0c\u800cVAE\u901a\u8fc7\u91cd\u5efa\u56fe\u50cf\u76f4\u63a5\u7f16\u7801\u89c6\u89c9\u7279\u5f81\u548c\u6709\u610f\u4e49\u7684\u6570\u636e\u6a21\u5f0f\uff0c\u751f\u6210\u89e3\u8026\u4e14\u7ed3\u6784\u5316\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4f7f\u7528VAE\u4f5c\u4e3a\u7279\u5f81\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\uff0c\u5728168\u540d\u672a\u63a5\u53d7\u65b0\u8f85\u52a9\u6cbb\u7597\u7684\u60a3\u8005\u7684MRI\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u672f\u540e\u75c5\u7406N\u5206\u671f\u4f5c\u4e3a\u771f\u5b9e\u6807\u7b7e\u3002", "result": "\u63d0\u51fa\u7684'VAE-MLP'\u6a21\u578b\u5728MRI\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4ea4\u53c9\u9a8c\u8bc1\u6307\u6807\u4e3aAUC 0.86 +/- 0.05\uff0c\u654f\u611f\u60270.79 +/- 0.06\uff0c\u7279\u5f02\u60270.85 +/- 0.05\u3002", "conclusion": "VAE-MLP\u6a21\u578b\u5728\u76f4\u80a0\u764c\u6dcb\u5df4\u7ed3\u8f6c\u79fb\u5206\u671f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.11834", "pdf": "https://arxiv.org/pdf/2507.11834", "abs": "https://arxiv.org/abs/2507.11834", "authors": ["Peiwen Xia", "Tangfei Liao", "Wei Zhu", "Danhuai Zhao", "Jianjun Ke", "Kaihao Zhang", "Tong Lu", "Tao Wang"], "title": "CorrMoE: Mixture of Experts with De-stylization Learning for Cross-Scene and Cross-Domain Correspondence Pruning", "categories": ["cs.CV"], "comment": "Accepted by ECAI 2025", "summary": "Establishing reliable correspondences between image pairs is a fundamental task in computer vision, underpinning applications such as 3D reconstruction and visual localization. Although recent methods have made progress in pruning outliers from dense correspondence sets, they often hypothesize consistent visual domains and overlook the challenges posed by diverse scene structures. In this paper, we propose CorrMoE, a novel correspondence pruning framework that enhances robustness under cross-domain and cross-scene variations. To address domain shift, we introduce a De-stylization Dual Branch, performing style mixing on both implicit and explicit graph features to mitigate the adverse influence of domain-specific representations. For scene diversity, we design a Bi-Fusion Mixture of Experts module that adaptively integrates multi-perspective features through linear-complexity attention and dynamic expert routing. Extensive experiments on benchmark datasets demonstrate that CorrMoE achieves superior accuracy and generalization compared to state-of-the-art methods. The code and pre-trained models are available at https://github.com/peiwenxia/CorrMoE.", "AI": {"tldr": "CorrMoE\u662f\u4e00\u79cd\u65b0\u7684\u5bf9\u5e94\u5173\u7cfb\u4fee\u526a\u6846\u67b6\uff0c\u901a\u8fc7\u53bb\u98ce\u683c\u5316\u53cc\u5206\u652f\u548c\u53cc\u878d\u5408\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\u63d0\u5347\u8de8\u57df\u548c\u8de8\u573a\u666f\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5bc6\u96c6\u5bf9\u5e94\u5173\u7cfb\u65f6\u5047\u8bbe\u89c6\u89c9\u57df\u4e00\u81f4\uff0c\u5ffd\u89c6\u4e86\u591a\u6837\u573a\u666f\u7ed3\u6784\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faDe-stylization Dual Branch\u548cBi-Fusion Mixture of Experts\u6a21\u5757\uff0c\u5206\u522b\u89e3\u51b3\u57df\u504f\u79fb\u548c\u573a\u666f\u591a\u6837\u6027\u95ee\u9898\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CorrMoE\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df\u548c\u8de8\u573a\u666f\u7684\u5bf9\u5e94\u5173\u7cfb\u4fee\u526a\u6027\u80fd\u3002"}}
{"id": "2507.11893", "pdf": "https://arxiv.org/pdf/2507.11893", "abs": "https://arxiv.org/abs/2507.11893", "authors": ["Linwei Chen", "Ying Fu", "Lin Gu", "Dezhi Zheng", "Jifeng Dai"], "title": "Spatial Frequency Modulation for Semantic Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "Accept by TPAMI 2025", "summary": "High spatial frequency information, including fine details like textures, significantly contributes to the accuracy of semantic segmentation. However, according to the Nyquist-Shannon Sampling Theorem, high-frequency components are vulnerable to aliasing or distortion when propagating through downsampling layers such as strided-convolution. Here, we propose a novel Spatial Frequency Modulation (SFM) that modulates high-frequency features to a lower frequency before downsampling and then demodulates them back during upsampling. Specifically, we implement modulation through adaptive resampling (ARS) and design a lightweight add-on that can densely sample the high-frequency areas to scale up the signal, thereby lowering its frequency in accordance with the Frequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling (MSAU) to demodulate the modulated feature and recover high-frequency information through non-uniform upsampling This module further improves segmentation by explicitly exploiting information interaction between densely and sparsely resampled areas at multiple scales. Both modules can seamlessly integrate with various architectures, extending from convolutional neural networks to transformers. Feature visualization and analysis confirm that our method effectively alleviates aliasing while successfully retaining details after demodulation. Finally, we validate the broad applicability and effectiveness of SFM by extending it to image classification, adversarial robustness, instance segmentation, and panoptic segmentation tasks. The code is available at \\href{https://github.com/Linwei-Chen/SFM}{https://github.com/Linwei-Chen/SFM}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7a7a\u95f4\u9891\u7387\u8c03\u5236\uff08SFM\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u5236\u9ad8\u9891\u7279\u5f81\u4ee5\u964d\u4f4e\u9891\u7387\uff0c\u518d\u901a\u8fc7\u89e3\u8c03\u6062\u590d\u9ad8\u9891\u4fe1\u606f\uff0c\u6709\u6548\u51cf\u8f7b\u6df7\u53e0\u5e76\u4fdd\u7559\u7ec6\u8282\u3002", "motivation": "\u9ad8\u9891\u4fe1\u606f\u5bf9\u8bed\u4e49\u5206\u5272\u7cbe\u5ea6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u4e0b\u91c7\u6837\u8fc7\u7a0b\u4e2d\u6613\u53d7\u6df7\u53e0\u6216\u5931\u771f\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u91cd\u91c7\u6837\uff08ARS\uff09\u8c03\u5236\u9ad8\u9891\u7279\u5f81\uff0c\u8bbe\u8ba1\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u4e0a\u91c7\u6837\uff08MSAU\uff09\u89e3\u8c03\uff0c\u5e76\u7ed3\u5408\u591a\u5c3a\u5ea6\u4fe1\u606f\u4ea4\u4e92\u3002", "result": "SFM\u6709\u6548\u51cf\u8f7b\u6df7\u53e0\u5e76\u4fdd\u7559\u7ec6\u8282\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u548c\u67b6\u6784\u3002", "conclusion": "SFM\u662f\u4e00\u79cd\u901a\u7528\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u3002"}}
{"id": "2507.11931", "pdf": "https://arxiv.org/pdf/2507.11931", "abs": "https://arxiv.org/abs/2507.11931", "authors": ["Jingqian Wu", "Peiqi Duan", "Zongqiang Wang", "Changwei Wang", "Boxin Shi", "Edmund Y. Lam"], "title": "Dark-EvGS: Event Camera as an Eye for Radiance Field in the Dark", "categories": ["cs.CV"], "comment": null, "summary": "In low-light environments, conventional cameras often struggle to capture clear multi-view images of objects due to dynamic range limitations and motion blur caused by long exposure. Event cameras, with their high-dynamic range and high-speed properties, have the potential to mitigate these issues. Additionally, 3D Gaussian Splatting (GS) enables radiance field reconstruction, facilitating bright frame synthesis from multiple viewpoints in low-light conditions. However, naively using an event-assisted 3D GS approach still faced challenges because, in low light, events are noisy, frames lack quality, and the color tone may be inconsistent. To address these issues, we propose Dark-EvGS, the first event-assisted 3D GS framework that enables the reconstruction of bright frames from arbitrary viewpoints along the camera trajectory. Triplet-level supervision is proposed to gain holistic knowledge, granular details, and sharp scene rendering. The color tone matching block is proposed to guarantee the color consistency of the rendered frames. Furthermore, we introduce the first real-captured dataset for the event-guided bright frame synthesis task via 3D GS-based radiance field reconstruction. Experiments demonstrate that our method achieves better results than existing methods, conquering radiance field reconstruction under challenging low-light conditions. The code and sample data are included in the supplementary material.", "AI": {"tldr": "Dark-EvGS\u6846\u67b6\u901a\u8fc7\u4e8b\u4ef6\u76f8\u673a\u548c3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u5728\u4f4e\u5149\u73af\u5883\u4e0b\u91cd\u5efa\u660e\u4eae\u7684\u591a\u89c6\u89d2\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u566a\u58f0\u3001\u4f4e\u8d28\u91cf\u5e27\u548c\u8272\u8c03\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u4f4e\u5149\u73af\u5883\u4e0b\u4f20\u7edf\u76f8\u673a\u96be\u4ee5\u6355\u6349\u6e05\u6670\u591a\u89c6\u89d2\u56fe\u50cf\uff0c\u4e8b\u4ef6\u76f8\u673a\u548c\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u6709\u6f5c\u529b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ecd\u9762\u4e34\u566a\u58f0\u548c\u8272\u8c03\u95ee\u9898\u3002", "method": "\u63d0\u51faDark-EvGS\u6846\u67b6\uff0c\u91c7\u7528\u4e09\u91cd\u76d1\u7763\u5b66\u4e60\u3001\u8272\u8c03\u5339\u914d\u6a21\u5757\uff0c\u5e76\u6784\u5efa\u9996\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u5149\u6761\u4ef6\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6210\u529f\u91cd\u5efa\u8f90\u5c04\u573a\u3002", "conclusion": "Dark-EvGS\u4e3a\u4f4e\u5149\u73af\u5883\u4e0b\u7684\u591a\u89c6\u89d2\u56fe\u50cf\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11947", "pdf": "https://arxiv.org/pdf/2507.11947", "abs": "https://arxiv.org/abs/2507.11947", "authors": ["Geon Park", "Seon Bin Kim", "Gunho Jung", "Seong-Whan Lee"], "title": "RaDL: Relation-aware Disentangled Learning for Multi-Instance Text-to-Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": "6 Pages", "summary": "With recent advancements in text-to-image (T2I) models, effectively generating multiple instances within a single image prompt has become a crucial challenge. Existing methods, while successful in generating positions of individual instances, often struggle to account for relationship discrepancy and multiple attributes leakage. To address these limitations, this paper proposes the relation-aware disentangled learning (RaDL) framework. RaDL enhances instance-specific attributes through learnable parameters and generates relation-aware image features via Relation Attention, utilizing action verbs extracted from the global prompt. Through extensive evaluations on benchmarks such as COCO-Position, COCO-MIG, and DrawBench, we demonstrate that RaDL outperforms existing methods, showing significant improvements in positional accuracy, multiple attributes consideration, and the relationships between instances. Our results present RaDL as the solution for generating images that consider both the relationships and multiple attributes of each instance within the multi-instance image.", "AI": {"tldr": "RaDL\u6846\u67b6\u901a\u8fc7\u5173\u7cfb\u611f\u77e5\u89e3\u8026\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u591a\u5b9e\u4f8b\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5173\u7cfb\u5dee\u5f02\u548c\u5c5e\u6027\u6cc4\u6f0f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4d\u7f6e\u51c6\u786e\u6027\u548c\u5b9e\u4f8b\u95f4\u5173\u7cfb\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u5b9e\u4f8b\u56fe\u50cf\u751f\u6210\u4e2d\u96be\u4ee5\u5904\u7406\u5173\u7cfb\u5dee\u5f02\u548c\u591a\u91cd\u5c5e\u6027\u6cc4\u6f0f\uff0cRaDL\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "RaDL\u5229\u7528\u53ef\u5b66\u4e60\u53c2\u6570\u589e\u5f3a\u5b9e\u4f8b\u7279\u5b9a\u5c5e\u6027\uff0c\u5e76\u901a\u8fc7\u5173\u7cfb\u6ce8\u610f\u529b\u751f\u6210\u5173\u7cfb\u611f\u77e5\u56fe\u50cf\u7279\u5f81\uff0c\u7ed3\u5408\u5168\u5c40\u63d0\u793a\u4e2d\u7684\u52a8\u4f5c\u52a8\u8bcd\u3002", "result": "\u5728COCO-Position\u3001COCO-MIG\u548cDrawBench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRaDL\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4d\u7f6e\u51c6\u786e\u6027\u548c\u5b9e\u4f8b\u95f4\u5173\u7cfb\u3002", "conclusion": "RaDL\u662f\u591a\u5b9e\u4f8b\u56fe\u50cf\u751f\u6210\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u540c\u65f6\u8003\u8651\u5b9e\u4f8b\u95f4\u5173\u7cfb\u548c\u591a\u91cd\u5c5e\u6027\u3002"}}
{"id": "2507.11968", "pdf": "https://arxiv.org/pdf/2507.11968", "abs": "https://arxiv.org/abs/2507.11968", "authors": ["Sahid Hossain Mustakim", "S M Jishanul Islam", "Ummay Maria Muna", "Montasir Chowdhury", "Mohammed Jawwadul Islam", "Sadia Ahmmed", "Tashfia Sikder", "Syed Tasdid Azam Dhrubo", "Swakkhar Shatabda"], "title": "Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on Short Videos for Content Appropriateness Evaluation", "categories": ["cs.CV"], "comment": "Accepted as long paper, SVU Workshop at ICCV 2025", "summary": "Multimodal Large Language Models (MLLMs) are increasingly used for content moderation, yet their robustness in short-form video contexts remains underexplored. Current safety evaluations often rely on unimodal attacks, failing to address combined attack vulnerabilities. In this paper, we introduce a comprehensive framework for evaluating the tri-modal safety of MLLMs. First, we present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising diverse short-form videos with human-guided synthetic adversarial attacks. Second, we propose ChimeraBreak, a novel tri-modal attack strategy that simultaneously challenges visual, auditory, and semantic reasoning pathways. Extensive experiments on state-of-the-art MLLMs reveal significant vulnerabilities with high Attack Success Rates (ASR). Our findings uncover distinct failure modes, showing model biases toward misclassifying benign or policy-violating content. We assess results using LLM-as-a-judge, demonstrating attack reasoning efficacy. Our dataset and findings provide crucial insights for developing more robust and safe MLLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u77ed\u89c6\u9891\u5185\u5bb9\u5ba1\u6838\u4e2d\u5b89\u5168\u6027\u7684\u6846\u67b6\uff0c\u5305\u62ecSVMA\u6570\u636e\u96c6\u548cChimeraBreak\u653b\u51fb\u7b56\u7565\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u89c6\u89c9\u3001\u542c\u89c9\u548c\u8bed\u4e49\u63a8\u7406\u8def\u5f84\u4e0a\u7684\u663e\u8457\u6f0f\u6d1e\u3002", "motivation": "\u5f53\u524d\u7684\u5b89\u5168\u8bc4\u4f30\u4e3b\u8981\u9488\u5bf9\u5355\u6a21\u6001\u653b\u51fb\uff0c\u672a\u80fd\u5168\u9762\u8bc4\u4f30\u591a\u6a21\u6001\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u5c24\u5176\u662f\u5728\u77ed\u89c6\u9891\u5185\u5bb9\u5ba1\u6838\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86SVMA\u6570\u636e\u96c6\u548cChimeraBreak\u4e09\u6a21\u6001\u653b\u51fb\u7b56\u7565\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30MLLMs\u7684\u5b89\u5168\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793aMLLMs\u5728\u89c6\u89c9\u3001\u542c\u89c9\u548c\u8bed\u4e49\u63a8\u7406\u8def\u5f84\u4e0a\u5b58\u5728\u663e\u8457\u6f0f\u6d1e\uff0c\u653b\u51fb\u6210\u529f\u7387\uff08ASR\uff09\u8f83\u9ad8\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u5b89\u5168\u7684MLLMs\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2507.11980", "pdf": "https://arxiv.org/pdf/2507.11980", "abs": "https://arxiv.org/abs/2507.11980", "authors": ["Jiajian Xie", "Shengyu Zhang", "Zhou Zhao", "Fan Wu", "Fei Wu"], "title": "EC-Diff: Fast and High-Quality Edge-Cloud Collaborative Inference for Diffusion Models", "categories": ["cs.CV"], "comment": "21 pages, 8 figures", "summary": "Diffusion Models have shown remarkable proficiency in image and video synthesis. As model size and latency increase limit user experience, hybrid edge-cloud collaborative framework was recently proposed to realize fast inference and high-quality generation, where the cloud model initiates high-quality semantic planning and the edge model expedites later-stage refinement. However, excessive cloud denoising prolongs inference time, while insufficient steps cause semantic ambiguity, leading to inconsistency in edge model output. To address these challenges, we propose EC-Diff that accelerates cloud inference through gradient-based noise estimation while identifying the optimal point for cloud-edge handoff to maintain generation quality. Specifically, we design a K-step noise approximation strategy to reduce cloud inference frequency by using noise gradients between steps and applying cloud inference periodically to adjust errors. Then we design a two-stage greedy search algorithm to efficiently find the optimal parameters for noise approximation and edge model switching. Extensive experiments demonstrate that our method significantly enhances generation quality compared to edge inference, while achieving up to an average $2\\times$ speedup in inference compared to cloud inference. Video samples and source code are available at https://ec-diff.github.io/.", "AI": {"tldr": "EC-Diff\u662f\u4e00\u79cd\u6df7\u5408\u8fb9\u7f18-\u4e91\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u68af\u5ea6\u566a\u58f0\u4f30\u8ba1\u52a0\u901f\u4e91\u63a8\u7406\uff0c\u5e76\u901a\u8fc7K\u6b65\u566a\u58f0\u8fd1\u4f3c\u7b56\u7565\u548c\u4e24\u9636\u6bb5\u8d2a\u5fc3\u641c\u7d22\u7b97\u6cd5\u4f18\u5316\u4e91-\u8fb9\u7f18\u5207\u6362\u70b9\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf\u548c\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u6df7\u5408\u8fb9\u7f18-\u4e91\u534f\u4f5c\u6846\u67b6\u4e2d\uff0c\u8fc7\u591a\u7684\u4e91\u53bb\u566a\u6b65\u9aa4\u5ef6\u957f\u63a8\u7406\u65f6\u95f4\uff0c\u800c\u6b65\u9aa4\u4e0d\u8db3\u5219\u5bfc\u81f4\u8bed\u4e49\u6a21\u7cca\u548c\u8fb9\u7f18\u6a21\u578b\u8f93\u51fa\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51faK\u6b65\u566a\u58f0\u8fd1\u4f3c\u7b56\u7565\u51cf\u5c11\u4e91\u63a8\u7406\u9891\u7387\uff0c\u8bbe\u8ba1\u4e24\u9636\u6bb5\u8d2a\u5fc3\u641c\u7d22\u7b97\u6cd5\u4f18\u5316\u566a\u58f0\u8fd1\u4f3c\u548c\u8fb9\u7f18\u6a21\u578b\u5207\u6362\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEC-Diff\u5728\u751f\u6210\u8d28\u91cf\u4e0a\u4f18\u4e8e\u8fb9\u7f18\u63a8\u7406\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u4e91\u63a8\u7406\u5e73\u5747\u63d0\u53472\u500d\u3002", "conclusion": "EC-Diff\u901a\u8fc7\u4f18\u5316\u4e91-\u8fb9\u7f18\u534f\u4f5c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2507.11985", "pdf": "https://arxiv.org/pdf/2507.11985", "abs": "https://arxiv.org/abs/2507.11985", "authors": ["Jiahao Xia", "Yike Wu", "Wenjian Huang", "Jianguo Zhang", "Jian Zhang"], "title": "Unsupervised Part Discovery via Descriptor-Based Masked Image Restoration with Optimized Constraints", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "Part-level features are crucial for image understanding, but few studies focus on them because of the lack of fine-grained labels. Although unsupervised part discovery can eliminate the reliance on labels, most of them cannot maintain robustness across various categories and scenarios, which restricts their application range. To overcome this limitation, we present a more effective paradigm for unsupervised part discovery, named Masked Part Autoencoder (MPAE). It first learns part descriptors as well as a feature map from the inputs and produces patch features from a masked version of the original images. Then, the masked regions are filled with the learned part descriptors based on the similarity between the local features and descriptors. By restoring these masked patches using the part descriptors, they become better aligned with their part shapes, guided by appearance features from unmasked patches. Finally, MPAE robustly discovers meaningful parts that closely match the actual object shapes, even in complex scenarios. Moreover, several looser yet more effective constraints are proposed to enable MPAE to identify the presence of parts across various scenarios and categories in an unsupervised manner. This provides the foundation for addressing challenges posed by occlusion and for exploring part similarity across multiple categories. Extensive experiments demonstrate that our method robustly discovers meaningful parts across various categories and scenarios. The code is available at the project https://github.com/Jiahao-UTS/MPAE.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMasked Part Autoencoder (MPAE)\u7684\u65e0\u76d1\u7763\u90e8\u4ef6\u53d1\u73b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u90e8\u4ef6\u63cf\u8ff0\u7b26\u548c\u7279\u5f81\u56fe\uff0c\u5229\u7528\u63a9\u7801\u548c\u6062\u590d\u673a\u5236\uff0c\u5b9e\u73b0\u8de8\u7c7b\u522b\u548c\u573a\u666f\u7684\u9c81\u68d2\u6027\u90e8\u4ef6\u53d1\u73b0\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u6807\u7b7e\uff0c\u90e8\u4ef6\u7ea7\u7279\u5f81\u7684\u7814\u7a76\u8f83\u5c11\uff0c\u73b0\u6709\u65e0\u76d1\u7763\u65b9\u6cd5\u5728\u8de8\u7c7b\u522b\u548c\u573a\u666f\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5e94\u7528\u8303\u56f4\u3002", "method": "MPAE\u901a\u8fc7\u5b66\u4e60\u90e8\u4ef6\u63cf\u8ff0\u7b26\u548c\u7279\u5f81\u56fe\uff0c\u5229\u7528\u63a9\u7801\u56fe\u50cf\u751f\u6210\u5c40\u90e8\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u76f8\u4f3c\u6027\u6062\u590d\u63a9\u7801\u533a\u57df\uff0c\u4f7f\u90e8\u4ef6\u63cf\u8ff0\u7b26\u4e0e\u5f62\u72b6\u5bf9\u9f50\u3002", "result": "MPAE\u5728\u590d\u6742\u573a\u666f\u4e2d\u9c81\u68d2\u5730\u53d1\u73b0\u4e0e\u5b9e\u9645\u7269\u4f53\u5f62\u72b6\u5339\u914d\u7684\u90e8\u4ef6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u8de8\u7c7b\u522b\u548c\u573a\u666f\u7684\u6709\u6548\u6027\u3002", "conclusion": "MPAE\u4e3a\u65e0\u76d1\u7763\u90e8\u4ef6\u53d1\u73b0\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u906e\u6321\u95ee\u9898\uff0c\u5e76\u652f\u6301\u8de8\u7c7b\u522b\u90e8\u4ef6\u76f8\u4f3c\u6027\u7814\u7a76\u3002"}}
{"id": "2507.11986", "pdf": "https://arxiv.org/pdf/2507.11986", "abs": "https://arxiv.org/abs/2507.11986", "authors": ["Jaehyun Lee", "Wonhark Park", "Wonsik Shin", "Hyunho Lee", "Hyoung Min Na", "Nojun Kwak"], "title": "Style Composition within Distinct LoRA modules for Traditional Art", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion-based text-to-image models have achieved remarkable results in synthesizing diverse images from text prompts and can capture specific artistic styles via style personalization. However, their entangled latent space and lack of smooth interpolation make it difficult to apply distinct painting techniques in a controlled, regional manner, often causing one style to dominate. To overcome this, we propose a zero-shot diffusion pipeline that naturally blends multiple styles by performing style composition on the denoised latents predicted during the flow-matching denoising process of separately trained, style-specialized models. We leverage the fact that lower-noise latents carry stronger stylistic information and fuse them across heterogeneous diffusion pipelines using spatial masks, enabling precise, region-specific style control. This mechanism preserves the fidelity of each individual style while allowing user-guided mixing. Furthermore, to ensure structural coherence across different models, we incorporate depth-map conditioning via ControlNet into the diffusion framework. Qualitative and quantitative experiments demonstrate that our method successfully achieves region-specific style mixing according to the given masks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u6269\u6563\u7ba1\u9053\uff0c\u901a\u8fc7\u878d\u5408\u4e0d\u540c\u98ce\u683c\u6a21\u578b\u7684\u53bb\u566a\u6f5c\u5728\u7a7a\u95f4\uff0c\u5b9e\u73b0\u533a\u57df\u7279\u5b9a\u7684\u98ce\u683c\u6df7\u5408\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e2d\u98ce\u683c\u7ea0\u7f20\u548c\u63d2\u503c\u4e0d\u5e73\u6ed1\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u53ef\u63a7\u7684\u533a\u57df\u98ce\u683c\u6df7\u5408\u3002", "method": "\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u878d\u5408\u4e0d\u540c\u98ce\u683c\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5229\u7528\u7a7a\u95f4\u63a9\u7801\u548c\u6df1\u5ea6\u56fe\u6761\u4ef6\u63a7\u5236\u98ce\u683c\u6df7\u5408\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u6839\u636e\u63a9\u7801\u8fdb\u884c\u533a\u57df\u7279\u5b9a\u98ce\u683c\u6df7\u5408\uff0c\u4fdd\u6301\u4e86\u5404\u98ce\u683c\u7684\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u98ce\u683c\u6df7\u5408\u63d0\u4f9b\u4e86\u53ef\u63a7\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11990", "pdf": "https://arxiv.org/pdf/2507.11990", "abs": "https://arxiv.org/abs/2507.11990", "authors": ["Hyun-Jun Jin", "Young-Eun Kim", "Seong-Whan Lee"], "title": "ID-EA: Identity-driven Text Enhancement and Adaptation with Textual Inversion for Personalized Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recently, personalized portrait generation with a text-to-image diffusion model has significantly advanced with Textual Inversion, emerging as a promising approach for creating high-fidelity personalized images. Despite its potential, current Textual Inversion methods struggle to maintain consistent facial identity due to semantic misalignments between textual and visual embedding spaces regarding identity. We introduce ID-EA, a novel framework that guides text embeddings to align with visual identity embeddings, thereby improving identity preservation in a personalized generation. ID-EA comprises two key components: the ID-driven Enhancer (ID-Enhancer) and the ID-conditioned Adapter (ID-Adapter). First, the ID-Enhancer integrates identity embeddings with a textual ID anchor, refining visual identity embeddings derived from a face recognition model using representative text embeddings. Then, the ID-Adapter leverages the identity-enhanced embedding to adapt the text condition, ensuring identity preservation by adjusting the cross-attention module in the pre-trained UNet model. This process encourages the text features to find the most related visual clues across the foreground snippets. Extensive quantitative and qualitative evaluations demonstrate that ID-EA substantially outperforms state-of-the-art methods in identity preservation metrics while achieving remarkable computational efficiency, generating personalized portraits approximately 15 times faster than existing approaches.", "AI": {"tldr": "ID-EA\u6846\u67b6\u901a\u8fc7ID-Enhancer\u548cID-Adapter\u6539\u8fdb\u6587\u672c\u5d4c\u5165\u4e0e\u89c6\u89c9\u8eab\u4efd\u5d4c\u5165\u7684\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e2a\u6027\u5316\u8096\u50cf\u751f\u6210\u4e2d\u7684\u8eab\u4efd\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524dTextual Inversion\u65b9\u6cd5\u5728\u4e2a\u6027\u5316\u8096\u50cf\u751f\u6210\u4e2d\u96be\u4ee5\u4fdd\u6301\u9762\u90e8\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u4e3b\u8981\u7531\u4e8e\u6587\u672c\u4e0e\u89c6\u89c9\u5d4c\u5165\u7a7a\u95f4\u5728\u8eab\u4efd\u8bed\u4e49\u4e0a\u7684\u4e0d\u5339\u914d\u3002", "method": "ID-EA\u5305\u542bID-Enhancer\u548cID-Adapter\uff1a\u524d\u8005\u901a\u8fc7\u6587\u672cID\u951a\u70b9\u4f18\u5316\u89c6\u89c9\u8eab\u4efd\u5d4c\u5165\uff0c\u540e\u8005\u8c03\u6574\u9884\u8bad\u7ec3UNet\u6a21\u578b\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u4ee5\u786e\u4fdd\u8eab\u4efd\u4fdd\u7559\u3002", "result": "ID-EA\u5728\u8eab\u4efd\u4fdd\u7559\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u751f\u6210\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb\u7ea615\u500d\u3002", "conclusion": "ID-EA\u901a\u8fc7\u6539\u8fdb\u6587\u672c\u4e0e\u89c6\u89c9\u5d4c\u5165\u7684\u5bf9\u9f50\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8eab\u4efd\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u751f\u6210\u6548\u7387\u3002"}}
{"id": "2507.12027", "pdf": "https://arxiv.org/pdf/2507.12027", "abs": "https://arxiv.org/abs/2507.12027", "authors": ["Beining Xu", "Siting Zhu", "Hesheng Wang"], "title": "SGLoc: Semantic Localization System for Camera Pose Estimation from 3D Gaussian Splatting Representation", "categories": ["cs.CV", "cs.RO", "I.4.8; I.2.9"], "comment": "8 pages, 2 figures, IROS 2025", "summary": "We propose SGLoc, a novel localization system that directly regresses camera poses from 3D Gaussian Splatting (3DGS) representation by leveraging semantic information. Our method utilizes the semantic relationship between 2D image and 3D scene representation to estimate the 6DoF pose without prior pose information. In this system, we introduce a multi-level pose regression strategy that progressively estimates and refines the pose of query image from the global 3DGS map, without requiring initial pose priors. Moreover, we introduce a semantic-based global retrieval algorithm that establishes correspondences between 2D (image) and 3D (3DGS map). By matching the extracted scene semantic descriptors of 2D query image and 3DGS semantic representation, we align the image with the local region of the global 3DGS map, thereby obtaining a coarse pose estimation. Subsequently, we refine the coarse pose by iteratively optimizing the difference between the query image and the rendered image from 3DGS. Our SGLoc demonstrates superior performance over baselines on 12scenes and 7scenes datasets, showing excellent capabilities in global localization without initial pose prior. Code will be available at https://github.com/IRMVLab/SGLoc.", "AI": {"tldr": "SGLoc\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u901a\u8fc7\u5229\u7528\u8bed\u4e49\u4fe1\u606f\u76f4\u63a5\u4ece3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u8868\u793a\u4e2d\u56de\u5f52\u76f8\u673a\u59ff\u6001\u3002", "motivation": "\u89e3\u51b3\u65e0\u9700\u521d\u59cb\u59ff\u6001\u5148\u9a8c\u76846\u81ea\u7531\u5ea6\uff086DoF\uff09\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u95ee\u9898\uff0c\u5229\u7528\u8bed\u4e49\u4fe1\u606f\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "method": "\u91c7\u7528\u591a\u7ea7\u59ff\u6001\u56de\u5f52\u7b56\u7565\uff0c\u7ed3\u5408\u8bed\u4e49\u5168\u5c40\u68c0\u7d22\u7b97\u6cd5\uff0c\u9010\u6b65\u4f30\u8ba1\u548c\u4f18\u5316\u67e5\u8be2\u56fe\u50cf\u7684\u59ff\u6001\u3002", "result": "\u572812scenes\u548c7scenes\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u65e0\u9700\u521d\u59cb\u59ff\u6001\u5148\u9a8c\u7684\u5168\u5c40\u5b9a\u4f4d\u80fd\u529b\u3002", "conclusion": "SGLoc\u901a\u8fc7\u8bed\u4e49\u4fe1\u606f\u548c3DGS\u8868\u793a\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\uff0c\u9002\u7528\u4e8e\u65e0\u5148\u9a8c\u59ff\u6001\u7684\u573a\u666f\u5b9a\u4f4d\u3002"}}
{"id": "2507.12062", "pdf": "https://arxiv.org/pdf/2507.12062", "abs": "https://arxiv.org/abs/2507.12062", "authors": ["Hongxu Ma", "Guanshuo Wang", "Fufu Yu", "Qiong Jia", "Shouhong Ding"], "title": "MS-DETR: Towards Effective Video Moment Retrieval and Highlight Detection by Joint Motion-Semantic Learning", "categories": ["cs.CV"], "comment": "Accepted by ACM MM'25", "summary": "Video Moment Retrieval (MR) and Highlight Detection (HD) aim to pinpoint specific moments and assess clip-wise relevance based on the text query. While DETR-based joint frameworks have made significant strides, there remains untapped potential in harnessing the intricate relationships between temporal motion and spatial semantics within video content. In this paper, we propose the Motion-Semantics DETR (MS-DETR), a framework that captures rich motion-semantics features through unified learning for MR/HD tasks. The encoder first explicitly models disentangled intra-modal correlations within motion and semantics dimensions, guided by the given text queries. Subsequently, the decoder utilizes the task-wise correlation across temporal motion and spatial semantics dimensions to enable precise query-guided localization for MR and refined highlight boundary delineation for HD. Furthermore, we observe the inherent sparsity dilemma within the motion and semantics dimensions of MR/HD datasets. To address this issue, we enrich the corpus from both dimensions by generation strategies and propose contrastive denoising learning to ensure the above components learn robustly and effectively. Extensive experiments on four MR/HD benchmarks demonstrate that our method outperforms existing state-of-the-art models by a margin. Our code is available at https://github.com/snailma0229/MS-DETR.git.", "AI": {"tldr": "MS-DETR\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u5b66\u4e60\u6355\u83b7\u8fd0\u52a8-\u8bed\u4e49\u7279\u5f81\uff0c\u63d0\u5347\u89c6\u9891\u65f6\u523b\u68c0\u7d22\u548c\u9ad8\u5149\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u7b56\u7565\u548c\u5bf9\u6bd4\u53bb\u566a\u5b66\u4e60\u89e3\u51b3\u6570\u636e\u7a00\u758f\u95ee\u9898\u3002", "motivation": "\u73b0\u6709DETR\u6846\u67b6\u672a\u5145\u5206\u5229\u7528\u89c6\u9891\u4e2d\u65f6\u95f4\u8fd0\u52a8\u548c\u7a7a\u95f4\u8bed\u4e49\u7684\u590d\u6742\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51faMS-DETR\uff0c\u7f16\u7801\u5668\u663e\u5f0f\u5efa\u6a21\u8fd0\u52a8\u4e0e\u8bed\u4e49\u7684\u6a21\u6001\u5185\u76f8\u5173\u6027\uff0c\u89e3\u7801\u5668\u5229\u7528\u4efb\u52a1\u95f4\u76f8\u5173\u6027\u8fdb\u884c\u7cbe\u786e\u5b9a\u4f4d\uff1b\u901a\u8fc7\u751f\u6210\u7b56\u7565\u548c\u5bf9\u6bd4\u53bb\u566a\u5b66\u4e60\u89e3\u51b3\u6570\u636e\u7a00\u758f\u95ee\u9898\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMS-DETR\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u6a21\u578b\u3002", "conclusion": "MS-DETR\u901a\u8fc7\u7edf\u4e00\u5b66\u4e60\u8fd0\u52a8-\u8bed\u4e49\u7279\u5f81\u548c\u89e3\u51b3\u6570\u636e\u7a00\u758f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u65f6\u523b\u68c0\u7d22\u548c\u9ad8\u5149\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.12095", "pdf": "https://arxiv.org/pdf/2507.12095", "abs": "https://arxiv.org/abs/2507.12095", "authors": ["Davide Di Nucci", "Matteo Tomei", "Guido Borghi", "Luca Ciuffreda", "Roberto Vezzani", "Rita Cucchiara"], "title": "BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images", "categories": ["cs.CV"], "comment": null, "summary": "Accurate 3D reconstruction of vehicles is vital for applications such as vehicle inspection, predictive maintenance, and urban planning. Existing methods like Neural Radiance Fields and Gaussian Splatting have shown impressive results but remain limited by their reliance on dense input views, which hinders real-world applicability. This paper addresses the challenge of reconstructing vehicles from sparse-view inputs, leveraging depth maps and a robust pose estimation architecture to synthesize novel views and augment training data. Specifically, we enhance Gaussian Splatting by integrating a selective photometric loss, applied only to high-confidence pixels, and replacing standard Structure-from-Motion pipelines with the DUSt3R architecture to improve camera pose estimation. Furthermore, we present a novel dataset featuring both synthetic and real-world public transportation vehicles, enabling extensive evaluation of our approach. Experimental results demonstrate state-of-the-art performance across multiple benchmarks, showcasing the method's ability to achieve high-quality reconstructions even under constrained input conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u89c6\u56fe\u8f93\u5165\u7684\u8f66\u8f863D\u91cd\u5efa\u65b9\u6cd5\uff0c\u7ed3\u5408\u6df1\u5ea6\u56fe\u548c\u9c81\u68d2\u7684\u59ff\u6001\u4f30\u8ba1\u67b6\u6784\uff0c\u6539\u8fdb\u4e86\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982NeRF\u548c\u9ad8\u65af\u6cfc\u6e85\u4f9d\u8d56\u5bc6\u96c6\u8f93\u5165\u89c6\u56fe\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u89e3\u51b3\u7a00\u758f\u89c6\u56fe\u4e0b\u7684\u9ad8\u8d28\u91cf\u91cd\u5efa\u95ee\u9898\u3002", "method": "\u6574\u5408\u9009\u62e9\u6027\u5149\u5ea6\u635f\u5931\uff08\u4ec5\u7528\u4e8e\u9ad8\u7f6e\u4fe1\u50cf\u7d20\uff09\u548cDUSt3R\u67b6\u6784\u6539\u8fdb\u59ff\u6001\u4f30\u8ba1\uff0c\u589e\u5f3a\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u5e76\u6784\u5efa\u65b0\u6570\u636e\u96c6\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u9886\u5148\u6c34\u5e73\uff0c\u5373\u4f7f\u5728\u8f93\u5165\u53d7\u9650\u6761\u4ef6\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u89c6\u56fe\u4e0b\u7684\u8f66\u8f863D\u91cd\u5efa\u6548\u679c\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.12103", "pdf": "https://arxiv.org/pdf/2507.12103", "abs": "https://arxiv.org/abs/2507.12103", "authors": ["Longchao Da", "Xiangrui Liu", "Mithun Shivakoti", "Thirulogasankar Pranav Kutralingam", "Yezhou Yang", "Hua Wei"], "title": "DeepShade: Enable Shade Simulation by Text-conditioned Image Generation", "categories": ["cs.CV", "cs.CY", "68T45, 68U10, 62H35", "I.2.10; I.4.8; I.5.1"], "comment": "7pages, 4 figures. Accepted to IJCAI 2025", "summary": "Heatwaves pose a significant threat to public health, especially as global warming intensifies. However, current routing systems (e.g., online maps) fail to incorporate shade information due to the difficulty of estimating shades directly from noisy satellite imagery and the limited availability of training data for generative models. In this paper, we address these challenges through two main contributions. First, we build an extensive dataset covering diverse longitude-latitude regions, varying levels of building density, and different urban layouts. Leveraging Blender-based 3D simulations alongside building outlines, we capture building shadows under various solar zenith angles throughout the year and at different times of day. These simulated shadows are aligned with satellite images, providing a rich resource for learning shade patterns. Second, we propose the DeepShade, a diffusion-based model designed to learn and synthesize shade variations over time. It emphasizes the nuance of edge features by jointly considering RGB with the Canny edge layer, and incorporates contrastive learning to capture the temporal change rules of shade. Then, by conditioning on textual descriptions of known conditions (e.g., time of day, solar angles), our framework provides improved performance in generating shade images. We demonstrate the utility of our approach by using our shade predictions to calculate shade ratios for real-world route planning in Tempe, Arizona. We believe this work will benefit society by providing a reference for urban planning in extreme heat weather and its potential practical applications in the environment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeepShade\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u536b\u661f\u56fe\u50cf\u4e2d\u5b66\u4e60\u5e76\u5408\u6210\u9634\u5f71\u53d8\u5316\uff0c\u4ee5\u6539\u8fdb\u70ed\u6d6a\u5929\u6c14\u4e0b\u7684\u8def\u5f84\u89c4\u5212\u3002", "motivation": "\u70ed\u6d6a\u5bf9\u516c\u5171\u5065\u5eb7\u6784\u6210\u5a01\u80c1\uff0c\u4f46\u73b0\u6709\u8def\u5f84\u89c4\u5212\u7cfb\u7edf\u7f3a\u4e4f\u9634\u5f71\u4fe1\u606f\uff0c\u4e3b\u8981\u56e0\u4e3a\u536b\u661f\u56fe\u50cf\u566a\u58f0\u5927\u4e14\u8bad\u7ec3\u6570\u636e\u6709\u9650\u3002", "method": "1. \u6784\u5efa\u5305\u542b\u591a\u6837\u57ce\u5e02\u5e03\u5c40\u7684\u9634\u5f71\u6570\u636e\u96c6\uff1b2. \u63d0\u51faDeepShade\u6a21\u578b\uff0c\u7ed3\u5408RGB\u548c\u8fb9\u7f18\u7279\u5f81\uff0c\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u6355\u6349\u9634\u5f71\u65f6\u95f4\u53d8\u5316\u3002", "result": "\u6a21\u578b\u80fd\u6839\u636e\u5df2\u77e5\u6761\u4ef6\uff08\u5982\u65f6\u95f4\u3001\u592a\u9633\u89d2\u5ea6\uff09\u751f\u6210\u66f4\u51c6\u786e\u7684\u9634\u5f71\u56fe\u50cf\uff0c\u5e76\u5728\u4e9a\u5229\u6851\u90a3\u5ddeTempe\u7684\u5b9e\u9645\u8def\u5f84\u89c4\u5212\u4e2d\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6781\u7aef\u70ed\u6d6a\u5929\u6c14\u4e0b\u7684\u57ce\u5e02\u89c4\u5212\u63d0\u4f9b\u4e86\u53c2\u8003\uff0c\u5177\u6709\u6f5c\u5728\u7684\u73af\u5883\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.12114", "pdf": "https://arxiv.org/pdf/2507.12114", "abs": "https://arxiv.org/abs/2507.12114", "authors": ["Yuzhou Ji", "Ke Ma", "Hong Cai", "Anchun Zhang", "Lizhuang Ma", "Xin Tan"], "title": "LidarPainter: One-Step Away From Any Lidar View To Novel Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Dynamic driving scene reconstruction is of great importance in fields like digital twin system and autonomous driving simulation. However, unacceptable degradation occurs when the view deviates from the input trajectory, leading to corrupted background and vehicle models. To improve reconstruction quality on novel trajectory, existing methods are subject to various limitations including inconsistency, deformation, and time consumption. This paper proposes LidarPainter, a one-step diffusion model that recovers consistent driving views from sparse LiDAR condition and artifact-corrupted renderings in real-time, enabling high-fidelity lane shifts in driving scene reconstruction. Extensive experiments show that LidarPainter outperforms state-of-the-art methods in speed, quality and resource efficiency, specifically 7 x faster than StreetCrafter with only one fifth of GPU memory required. LidarPainter also supports stylized generation using text prompts such as \"foggy\" and \"night\", allowing for a diverse expansion of the existing asset library.", "AI": {"tldr": "LidarPainter\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5b9e\u65f6\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u7a00\u758fLiDAR\u6570\u636e\u548c\u635f\u574f\u7684\u6e32\u67d3\u4e2d\u6062\u590d\u4e00\u81f4\u7684\u9a7e\u9a76\u573a\u666f\u89c6\u56fe\uff0c\u652f\u6301\u9ad8\u4fdd\u771f\u8f66\u9053\u53d8\u6362\u548c\u98ce\u683c\u5316\u751f\u6210\u3002", "motivation": "\u52a8\u6001\u9a7e\u9a76\u573a\u666f\u91cd\u5efa\u5728\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u548c\u81ea\u52a8\u9a7e\u9a76\u6a21\u62df\u4e2d\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u504f\u79bb\u8f93\u5165\u8f68\u8ff9\u65f6\u4f1a\u51fa\u73b0\u8d28\u91cf\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u63d0\u51faLidarPainter\uff0c\u4e00\u79cd\u4e00\u6b65\u6269\u6563\u6a21\u578b\uff0c\u4ece\u7a00\u758fLiDAR\u6761\u4ef6\u548c\u635f\u574f\u7684\u6e32\u67d3\u4e2d\u5b9e\u65f6\u6062\u590d\u4e00\u81f4\u7684\u9a7e\u9a76\u89c6\u56fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLidarPainter\u5728\u901f\u5ea6\u3001\u8d28\u91cf\u548c\u8d44\u6e90\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u901f\u5ea6\u5feb7\u500d\u4e14\u4ec5\u9700\u4e94\u5206\u4e4b\u4e00\u7684GPU\u5185\u5b58\u3002", "conclusion": "LidarPainter\u4e0d\u4ec5\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\uff0c\u8fd8\u652f\u6301\u901a\u8fc7\u6587\u672c\u63d0\u793a\u751f\u6210\u591a\u6837\u5316\u98ce\u683c\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u8d44\u6e90\u5e93\u3002"}}
{"id": "2507.12135", "pdf": "https://arxiv.org/pdf/2507.12135", "abs": "https://arxiv.org/abs/2507.12135", "authors": ["Junyu Lou", "Xiaorui Zhao", "Kexuan Shi", "Shuhang Gu"], "title": "Learning Pixel-adaptive Multi-layer Perceptrons for Real-time Image Enhancement", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "Deep learning-based bilateral grid processing has emerged as a promising solution for image enhancement, inherently encoding spatial and intensity information while enabling efficient full-resolution processing through slicing operations. However, existing approaches are limited to linear affine transformations, hindering their ability to model complex color relationships. Meanwhile, while multi-layer perceptrons (MLPs) excel at non-linear mappings, traditional MLP-based methods employ globally shared parameters, which is hard to deal with localized variations. To overcome these dual challenges, we propose a Bilateral Grid-based Pixel-Adaptive Multi-layer Perceptron (BPAM) framework. Our approach synergizes the spatial modeling of bilateral grids with the non-linear capabilities of MLPs. Specifically, we generate bilateral grids containing MLP parameters, where each pixel dynamically retrieves its unique transformation parameters and obtain a distinct MLP for color mapping based on spatial coordinates and intensity values. In addition, we propose a novel grid decomposition strategy that categorizes MLP parameters into distinct types stored in separate subgrids. Multi-channel guidance maps are used to extract category-specific parameters from corresponding subgrids, ensuring effective utilization of color information during slicing while guiding precise parameter generation. Extensive experiments on public datasets demonstrate that our method outperforms state-of-the-art methods in performance while maintaining real-time processing capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53cc\u8fb9\u7f51\u683c\u548cMLP\u7684BPAM\u6846\u67b6\uff0c\u7528\u4e8e\u56fe\u50cf\u589e\u5f3a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u7ebf\u6027\u548c\u5168\u5c40\u53c2\u6570\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u53cc\u8fb9\u7f51\u683c\u65b9\u6cd5\u4ec5\u652f\u6301\u7ebf\u6027\u53d8\u6362\uff0c\u800c\u4f20\u7edfMLP\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5c40\u90e8\u53d8\u5316\uff0c\u9700\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "method": "\u901a\u8fc7\u53cc\u8fb9\u7f51\u683c\u5b58\u50a8MLP\u53c2\u6570\uff0c\u52a8\u6001\u751f\u6210\u50cf\u7d20\u7ea7MLP\uff0c\u5e76\u5f15\u5165\u7f51\u683c\u5206\u89e3\u7b56\u7565\u548c\u591a\u901a\u9053\u5f15\u5bfc\u56fe\u4f18\u5316\u53c2\u6570\u63d0\u53d6\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u4fdd\u6301\u5b9e\u65f6\u5904\u7406\u80fd\u529b\u3002", "conclusion": "BPAM\u6846\u67b6\u6210\u529f\u7ed3\u5408\u4e86\u53cc\u8fb9\u7f51\u683c\u7684\u7a7a\u95f4\u5efa\u6a21\u548cMLP\u7684\u975e\u7ebf\u6027\u80fd\u529b\uff0c\u63d0\u5347\u4e86\u56fe\u50cf\u589e\u5f3a\u6548\u679c\u3002"}}
{"id": "2507.12137", "pdf": "https://arxiv.org/pdf/2507.12137", "abs": "https://arxiv.org/abs/2507.12137", "authors": ["Jiawei Xu", "Kai Deng", "Zexin Fan", "Shenlong Wang", "Jin Xie", "Jian Yang"], "title": "AD-GS: Object-Aware B-Spline Gaussian Splatting for Self-Supervised Autonomous Driving", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Modeling and rendering dynamic urban driving scenes is crucial for self-driving simulation. Current high-quality methods typically rely on costly manual object tracklet annotations, while self-supervised approaches fail to capture dynamic object motions accurately and decompose scenes properly, resulting in rendering artifacts. We introduce AD-GS, a novel self-supervised framework for high-quality free-viewpoint rendering of driving scenes from a single log. At its core is a novel learnable motion model that integrates locality-aware B-spline curves with global-aware trigonometric functions, enabling flexible yet precise dynamic object modeling. Rather than requiring comprehensive semantic labeling, AD-GS automatically segments scenes into objects and background with the simplified pseudo 2D segmentation, representing objects using dynamic Gaussians and bidirectional temporal visibility masks. Further, our model incorporates visibility reasoning and physically rigid regularization to enhance robustness. Extensive evaluations demonstrate that our annotation-free model significantly outperforms current state-of-the-art annotation-free methods and is competitive with annotation-dependent approaches.", "AI": {"tldr": "AD-GS\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5355\u4e00\u65e5\u5fd7\u4e2d\u9ad8\u8d28\u91cf\u6e32\u67d3\u9a7e\u9a76\u573a\u666f\uff0c\u65e0\u9700\u624b\u52a8\u6807\u6ce8\uff0c\u901a\u8fc7\u52a8\u6001\u9ad8\u65af\u548c\u53cc\u5411\u65f6\u95f4\u53ef\u89c1\u6027\u63a9\u6a21\u5b9e\u73b0\u573a\u666f\u5206\u89e3\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u81ea\u76d1\u7763\u65b9\u6cd5\u5728\u52a8\u6001\u7269\u4f53\u8fd0\u52a8\u548c\u573a\u666f\u5206\u89e3\u4e0a\u7684\u4e0d\u8db3\uff0c\u51cf\u5c11\u5bf9\u6602\u8d35\u624b\u52a8\u6807\u6ce8\u7684\u4f9d\u8d56\u3002", "method": "\u7ed3\u5408\u5c40\u90e8\u611f\u77e5B\u6837\u6761\u66f2\u7ebf\u548c\u5168\u5c40\u611f\u77e5\u4e09\u89d2\u51fd\u6570\u7684\u5b66\u4e60\u8fd0\u52a8\u6a21\u578b\uff0c\u52a8\u6001\u9ad8\u65af\u8868\u793a\u7269\u4f53\uff0c\u53cc\u5411\u65f6\u95f4\u53ef\u89c1\u6027\u63a9\u6a21\u548c\u7269\u7406\u521a\u6027\u6b63\u5219\u5316\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5728\u65e0\u6807\u6ce8\u65b9\u6cd5\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e0e\u4f9d\u8d56\u6807\u6ce8\u7684\u65b9\u6cd5\u7ade\u4e89\u3002", "conclusion": "AD-GS\u4e3a\u9ad8\u8d28\u91cf\u9a7e\u9a76\u573a\u666f\u6e32\u67d3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u6807\u6ce8\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12138", "pdf": "https://arxiv.org/pdf/2507.12138", "abs": "https://arxiv.org/abs/2507.12138", "authors": ["Michal Heker", "Sefy Kararlitsky", "David Tolpin"], "title": "Neural Human Pose Prior", "categories": ["cs.CV", "cs.LG"], "comment": "Work in progress", "summary": "We introduce a principled, data-driven approach for modeling a neural prior over human body poses using normalizing flows. Unlike heuristic or low-expressivity alternatives, our method leverages RealNVP to learn a flexible density over poses represented in the 6D rotation format. We address the challenge of modeling distributions on the manifold of valid 6D rotations by inverting the Gram-Schmidt process during training, enabling stable learning while preserving downstream compatibility with rotation-based frameworks. Our architecture and training pipeline are framework-agnostic and easily reproducible. We demonstrate the effectiveness of the learned prior through both qualitative and quantitative evaluations, and we analyze its impact via ablation studies. This work provides a sound probabilistic foundation for integrating pose priors into human motion capture and reconstruction pipelines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f52\u4e00\u5316\u6d41\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u4eba\u4f53\u59ff\u6001\u7684\u795e\u7ecf\u5148\u9a8c\uff0c\u901a\u8fc7RealNVP\u5b66\u4e606D\u65cb\u8f6c\u59ff\u6001\u7684\u7075\u6d3b\u5bc6\u5ea6\u5206\u5e03\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5efa\u6a216D\u65cb\u8f6c\u59ff\u6001\u5206\u5e03\u65f6\u7684\u542f\u53d1\u5f0f\u6216\u4f4e\u8868\u8fbe\u80fd\u529b\u95ee\u9898\uff0c\u63d0\u4f9b\u7a33\u5b9a\u7684\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u5229\u7528RealNVP\u5b66\u4e606D\u65cb\u8f6c\u59ff\u6001\u7684\u5bc6\u5ea6\u5206\u5e03\uff0c\u901a\u8fc7\u53cd\u8f6cGram-Schmidt\u8fc7\u7a0b\u89e3\u51b3\u6d41\u5f62\u5206\u5e03\u5efa\u6a21\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u5206\u6790\u4e86\u5176\u5f71\u54cd\u3002", "conclusion": "\u4e3a\u4eba\u4f53\u8fd0\u52a8\u6355\u6349\u548c\u91cd\u5efa\u4e2d\u7684\u59ff\u6001\u5148\u9a8c\u63d0\u4f9b\u4e86\u6982\u7387\u57fa\u7840\uff0c\u5177\u6709\u6846\u67b6\u65e0\u5173\u6027\u548c\u6613\u590d\u73b0\u6027\u3002"}}
{"id": "2507.12188", "pdf": "https://arxiv.org/pdf/2507.12188", "abs": "https://arxiv.org/abs/2507.12188", "authors": ["Shuangli Du", "Siming Yan", "Zhenghao Shi", "Zhenzhen You", "Lu Sun"], "title": "Wavelet-based Decoupling Framework for low-light Stereo Image Enhancement", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Low-light images suffer from complex degradation, and existing enhancement methods often encode all degradation factors within a single latent space. This leads to highly entangled features and strong black-box characteristics, making the model prone to shortcut learning. To mitigate the above issues, this paper proposes a wavelet-based low-light stereo image enhancement method with feature space decoupling. Our insight comes from the following findings: (1) Wavelet transform enables the independent processing of low-frequency and high-frequency information. (2) Illumination adjustment can be achieved by adjusting the low-frequency component of a low-light image, extracted through multi-level wavelet decomposition. Thus, by using wavelet transform the feature space is decomposed into a low-frequency branch for illumination adjustment and multiple high-frequency branches for texture enhancement. Additionally, stereo low-light image enhancement can extract useful cues from another view to improve enhancement. To this end, we propose a novel high-frequency guided cross-view interaction module (HF-CIM) that operates within high-frequency branches rather than across the entire feature space, effectively extracting valuable image details from the other view. Furthermore, to enhance the high-frequency information, a detail and texture enhancement module (DTEM) is proposed based on cross-attention mechanism. The model is trained on a dataset consisting of images with uniform illumination and images with non-uniform illumination. Experimental results on both real and synthetic images indicate that our algorithm offers significant advantages in light adjustment while effectively recovering high-frequency information. The code and dataset are publicly available at: https://github.com/Cherisherr/WDCI-Net.git.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u53d8\u6362\u7684\u4f4e\u5149\u7acb\u4f53\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u7279\u5f81\u7a7a\u95f4\u89e3\u8026\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u7279\u5f81\u7ea0\u7f20\u548c\u9ed1\u76d2\u7279\u6027\u95ee\u9898\u3002", "motivation": "\u4f4e\u5149\u56fe\u50cf\u5b58\u5728\u590d\u6742\u7684\u9000\u5316\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5c06\u6240\u6709\u9000\u5316\u56e0\u7d20\u7f16\u7801\u5728\u5355\u4e00\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u5bfc\u81f4\u7279\u5f81\u9ad8\u5ea6\u7ea0\u7f20\u548c\u6a21\u578b\u6613\u53d7\u6377\u5f84\u5b66\u4e60\u5f71\u54cd\u3002", "method": "\u5229\u7528\u5c0f\u6ce2\u53d8\u6362\u5c06\u7279\u5f81\u7a7a\u95f4\u5206\u89e3\u4e3a\u4f4e\u9891\u5206\u652f\uff08\u7528\u4e8e\u5149\u7167\u8c03\u6574\uff09\u548c\u591a\u4e2a\u9ad8\u9891\u5206\u652f\uff08\u7528\u4e8e\u7eb9\u7406\u589e\u5f3a\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u9ad8\u9891\u5f15\u5bfc\u7684\u8de8\u89c6\u56fe\u4ea4\u4e92\u6a21\u5757\uff08HF-CIM\uff09\u548c\u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u7ec6\u8282\u7eb9\u7406\u589e\u5f3a\u6a21\u5757\uff08DTEM\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5149\u7167\u8c03\u6574\u548c\u9ad8\u9891\u4fe1\u606f\u6062\u590d\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u901a\u8fc7\u5c0f\u6ce2\u53d8\u6362\u548c\u7279\u5f81\u7a7a\u95f4\u89e3\u8026\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u4f4e\u5149\u7acb\u4f53\u56fe\u50cf\u7684\u589e\u5f3a\u6548\u679c\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002"}}
{"id": "2507.12195", "pdf": "https://arxiv.org/pdf/2507.12195", "abs": "https://arxiv.org/abs/2507.12195", "authors": ["Arkaprabha Basu"], "title": "Revealing the Ancient Beauty: Digital Reconstruction of Temple Tiles using Computer Vision", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Modern digitised approaches have dramatically changed the preservation and restoration of cultural treasures, integrating computer scientists into multidisciplinary projects with ease. Machine learning, deep learning, and computer vision techniques have revolutionised developing sectors like 3D reconstruction, picture inpainting,IoT-based methods, genetic algorithms, and image processing with the integration of computer scientists into multidisciplinary initiatives. We suggest three cutting-edge techniques in recognition of the special qualities of Indian monuments, which are famous for their architectural skill and aesthetic appeal. First is the Fractal Convolution methodology, a segmentation method based on image processing that successfully reveals subtle architectural patterns within these irreplaceable cultural buildings. The second is a revolutionary Self-Sensitive Tile Filling (SSTF) method created especially for West Bengal's mesmerising Bankura Terracotta Temples with a brand-new data augmentation method called MosaicSlice on the third. Furthermore, we delve deeper into the Super Resolution strategy to upscale the images without losing significant amount of quality. Our methods allow for the development of seamless region-filling and highly detailed tiles while maintaining authenticity using a novel data augmentation strategy within affordable costs introducing automation. By providing effective solutions that preserve the delicate balance between tradition and innovation, this study improves the subject and eventually ensures unrivalled efficiency and aesthetic excellence in cultural heritage protection. The suggested approaches advance the field into an era of unmatched efficiency and aesthetic quality while carefully upholding the delicate equilibrium between tradition and innovation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u5148\u8fdb\u6280\u672f\uff08Fractal Convolution\u3001SSTF\u548cSuper Resolution\uff09\u7528\u4e8e\u5370\u5ea6\u6587\u5316\u9057\u4ea7\u7684\u4fdd\u62a4\u4e0e\u4fee\u590d\uff0c\u7ed3\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u7684\u81ea\u52a8\u5316\u5904\u7406\u3002", "motivation": "\u73b0\u4ee3\u6570\u5b57\u5316\u65b9\u6cd5\u4e3a\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u5e26\u6765\u4e86\u9769\u547d\u6027\u53d8\u5316\uff0c\u4f46\u5370\u5ea6\u53e4\u8ff9\u7684\u7279\u6b8a\u6027\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u6280\u672f\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5e73\u8861\u4f20\u7edf\u4e0e\u521b\u65b0\u3002", "method": "1. Fractal Convolution\u7528\u4e8e\u56fe\u50cf\u5206\u5272\uff1b2. SSTF\u7ed3\u5408MosaicSlice\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff1b3. Super Resolution\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u4fee\u590d\u548c\u533a\u57df\u586b\u5145\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6587\u5316\u9057\u4ea7\u7684\u771f\u5b9e\u6027\uff0c\u4e14\u6210\u672c\u53ef\u63a7\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7f8e\u89c2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u6b65\u3002"}}
{"id": "2507.12232", "pdf": "https://arxiv.org/pdf/2507.12232", "abs": "https://arxiv.org/abs/2507.12232", "authors": ["Tao Chen", "Jingyi Zhang", "Decheng Liu", "Chunlei Peng"], "title": "MGFFD-VLM: Multi-Granularity Prompt Learning for Face Forgery Detection with VLM", "categories": ["cs.CV"], "comment": null, "summary": "Recent studies have utilized visual large language models (VLMs) to answer not only \"Is this face a forgery?\" but also \"Why is the face a forgery?\" These studies introduced forgery-related attributes, such as forgery location and type, to construct deepfake VQA datasets and train VLMs, achieving high accuracy while providing human-understandable explanatory text descriptions. However, these methods still have limitations. For example, they do not fully leverage face quality-related attributes, which are often abnormal in forged faces, and they lack effective training strategies for forgery-aware VLMs. In this paper, we extend the VQA dataset to create DD-VQA+, which features a richer set of attributes and a more diverse range of samples. Furthermore, we introduce a novel forgery detection framework, MGFFD-VLM, which integrates an Attribute-Driven Hybrid LoRA Strategy to enhance the capabilities of Visual Large Language Models (VLMs). Additionally, our framework incorporates Multi-Granularity Prompt Learning and a Forgery-Aware Training Strategy. By transforming classification and forgery segmentation results into prompts, our method not only improves forgery classification but also enhances interpretability. To further boost detection performance, we design multiple forgery-related auxiliary losses. Experimental results demonstrate that our approach surpasses existing methods in both text-based forgery judgment and analysis, achieving superior accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6MGFFD-VLM\uff0c\u901a\u8fc7\u6269\u5c55\u6570\u636e\u96c6\u548c\u5f15\u5165\u591a\u7c92\u5ea6\u63d0\u793a\u5b66\u4e60\u7b49\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u4eba\u8138\u8d28\u91cf\u76f8\u5173\u5c5e\u6027\uff0c\u4e14\u7f3a\u4e4f\u6709\u6548\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u5bfc\u81f4\u68c0\u6d4b\u6548\u679c\u53d7\u9650\u3002", "method": "\u6269\u5c55\u4e86VQA\u6570\u636e\u96c6\u4e3aDD-VQA+\uff0c\u5f15\u5165Attribute-Driven Hybrid LoRA Strategy\u3001\u591a\u7c92\u5ea6\u63d0\u793a\u5b66\u4e60\u548c\u4f2a\u9020\u611f\u77e5\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6587\u672c\u4f2a\u9020\u5224\u65ad\u548c\u5206\u6790\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u66f4\u9ad8\u3002", "conclusion": "MGFFD-VLM\u6846\u67b6\u901a\u8fc7\u591a\u7b56\u7565\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.12283", "pdf": "https://arxiv.org/pdf/2507.12283", "abs": "https://arxiv.org/abs/2507.12283", "authors": ["Zixuan Fu", "Yan Ren", "Finn Carter", "Chenyue Wang", "Ze Niu", "Dacheng Yu", "Emily Davis", "Bo Zhang"], "title": "FADE: Adversarial Concept Erasure in Flow Models", "categories": ["cs.CV"], "comment": "Camera Ready", "summary": "Diffusion models have demonstrated remarkable image generation capabilities, but also pose risks in privacy and fairness by memorizing sensitive concepts or perpetuating biases. We propose a novel \\textbf{concept erasure} method for text-to-image diffusion models, designed to remove specified concepts (e.g., a private individual or a harmful stereotype) from the model's generative repertoire. Our method, termed \\textbf{FADE} (Fair Adversarial Diffusion Erasure), combines a trajectory-aware fine-tuning strategy with an adversarial objective to ensure the concept is reliably removed while preserving overall model fidelity. Theoretically, we prove a formal guarantee that our approach minimizes the mutual information between the erased concept and the model's outputs, ensuring privacy and fairness. Empirically, we evaluate FADE on Stable Diffusion and FLUX, using benchmarks from prior work (e.g., object, celebrity, explicit content, and style erasure tasks from MACE). FADE achieves state-of-the-art concept removal performance, surpassing recent baselines like ESD, UCE, MACE, and ANT in terms of removal efficacy and image quality. Notably, FADE improves the harmonic mean of concept removal and fidelity by 5--10\\% over the best prior method. We also conduct an ablation study to validate each component of FADE, confirming that our adversarial and trajectory-preserving objectives each contribute to its superior performance. Our work sets a new standard for safe and fair generative modeling by unlearning specified concepts without retraining from scratch.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFADE\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u5220\u9664\u6307\u5b9a\u6982\u5ff5\uff0c\u786e\u4fdd\u9690\u79c1\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4e5f\u5b58\u5728\u9690\u79c1\u6cc4\u9732\u548c\u504f\u89c1\u4f20\u64ad\u7684\u98ce\u9669\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u53ef\u9760\u5730\u5220\u9664\u654f\u611f\u6216\u6709\u5bb3\u6982\u5ff5\u3002", "method": "FADE\u7ed3\u5408\u8f68\u8ff9\u611f\u77e5\u5fae\u8c03\u7b56\u7565\u548c\u5bf9\u6297\u76ee\u6807\uff0c\u6700\u5c0f\u5316\u88ab\u5220\u9664\u6982\u5ff5\u4e0e\u6a21\u578b\u8f93\u51fa\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u3002", "result": "FADE\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u6982\u5ff5\u5220\u9664\u6548\u679c\u548c\u56fe\u50cf\u8d28\u91cf\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FADE\u4e3a\u5b89\u5168\u516c\u5e73\u7684\u751f\u6210\u5efa\u6a21\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u65e0\u9700\u4ece\u5934\u8bad\u7ec3\u5373\u53ef\u5220\u9664\u6307\u5b9a\u6982\u5ff5\u3002"}}
{"id": "2507.12318", "pdf": "https://arxiv.org/pdf/2507.12318", "abs": "https://arxiv.org/abs/2507.12318", "authors": ["Samuel Lavoie", "Michael Noukhovitch", "Aaron Courville"], "title": "Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "In submission, 22 pages, 7 tables, 12 figures", "summary": "We argue that diffusion models' success in modeling complex distributions is, for the most part, coming from their input conditioning. This paper investigates the representation used to condition diffusion models from the perspective that ideal representations should improve sample fidelity, be easy to generate, and be compositional to allow out-of-training samples generation. We introduce Discrete Latent Code (DLC), an image representation derived from Simplicial Embeddings trained with a self-supervised learning objective. DLCs are sequences of discrete tokens, as opposed to the standard continuous image embeddings. They are easy to generate and their compositionality enables sampling of novel images beyond the training distribution. Diffusion models trained with DLCs have improved generation fidelity, establishing a new state-of-the-art for unconditional image generation on ImageNet. Additionally, we show that composing DLCs allows the image generator to produce out-of-distribution samples that coherently combine the semantics of images in diverse ways. Finally, we showcase how DLCs can enable text-to-image generation by leveraging large-scale pretrained language models. We efficiently finetune a text diffusion language model to generate DLCs that produce novel samples outside of the image generator training distribution.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u79bb\u6563\u6f5c\u5728\u7801\uff08DLC\uff09\u4f5c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u8f93\u5165\u6761\u4ef6\u8868\u793a\uff0c\u63d0\u5347\u751f\u6210\u8d28\u91cf\u548c\u7ec4\u5408\u6027\uff0c\u5b9e\u73b0\u8d85\u51fa\u8bad\u7ec3\u5206\u5e03\u7684\u6837\u672c\u751f\u6210\u3002", "motivation": "\u7814\u7a76\u6269\u6563\u6a21\u578b\u6210\u529f\u7684\u5173\u952e\u5728\u4e8e\u8f93\u5165\u6761\u4ef6\u8868\u793a\uff0c\u7406\u60f3\u8868\u793a\u5e94\u63d0\u5347\u6837\u672c\u4fdd\u771f\u5ea6\u3001\u6613\u4e8e\u751f\u6210\u4e14\u5177\u6709\u7ec4\u5408\u6027\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u79bb\u6563\u6f5c\u5728\u7801\uff08DLC\uff09\uff0c\u4f5c\u4e3a\u79bb\u6563\u4ee4\u724c\u5e8f\u5217\u66ff\u4ee3\u8fde\u7eed\u5d4c\u5165\uff0c\u7528\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u3002", "result": "DLC\u663e\u8457\u63d0\u5347\u65e0\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u8d28\u91cf\uff08ImageNet\u4e0aSOTA\uff09\uff0c\u5e76\u652f\u6301\u7ec4\u5408\u751f\u6210\u8d85\u51fa\u8bad\u7ec3\u5206\u5e03\u7684\u65b0\u6837\u672c\u3002", "conclusion": "DLC\u4e3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u9ad8\u6548\u3001\u7ec4\u5408\u6027\u5f3a\u7684\u6761\u4ef6\u8868\u793a\uff0c\u652f\u6301\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7b49\u5e94\u7528\u3002"}}
{"id": "2507.12336", "pdf": "https://arxiv.org/pdf/2507.12336", "abs": "https://arxiv.org/abs/2507.12336", "authors": ["Subin Jeon", "In Cho", "Junyoung Hong", "Seon Joo Kim"], "title": "Unsupervised Monocular 3D Keypoint Discovery from Multi-View Diffusion Priors", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces KeyDiff3D, a framework for unsupervised monocular 3D keypoints estimation that accurately predicts 3D keypoints from a single image. While previous methods rely on manual annotations or calibrated multi-view images, both of which are expensive to collect, our method enables monocular 3D keypoints estimation using only a collection of single-view images. To achieve this, we leverage powerful geometric priors embedded in a pretrained multi-view diffusion model. In our framework, this model generates multi-view images from a single image, serving as a supervision signal to provide 3D geometric cues to our model. We also use the diffusion model as a powerful 2D multi-view feature extractor and construct 3D feature volumes from its intermediate representations. This transforms implicit 3D priors learned by the diffusion model into explicit 3D features. Beyond accurate keypoints estimation, we further introduce a pipeline that enables manipulation of 3D objects generated by the diffusion model. Experimental results on diverse aspects and datasets, including Human3.6M, Stanford Dogs, and several in-the-wild and out-of-domain datasets, highlight the effectiveness of our method in terms of accuracy, generalization, and its ability to enable manipulation of 3D objects generated by the diffusion model from a single image.", "AI": {"tldr": "KeyDiff3D\u662f\u4e00\u4e2a\u65e0\u76d1\u7763\u7684\u5355\u76ee3D\u5173\u952e\u70b9\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u89c6\u89d2\u56fe\u50cf\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u5b9e\u73b0\u4ece\u5355\u5f20\u56fe\u50cf\u9884\u6d4b3D\u5173\u952e\u70b9\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u624b\u52a8\u6807\u6ce8\u6216\u591a\u89c6\u89d2\u56fe\u50cf\uff0c\u800cKeyDiff3D\u4ec5\u9700\u5355\u89c6\u89d2\u56fe\u50cf\u5373\u53ef\u5b9e\u73b03D\u5173\u952e\u70b9\u4f30\u8ba1\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u89c6\u89d2\u56fe\u50cf\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u5e76\u4ece\u4e2d\u63d0\u53d62D\u591a\u89c6\u89d2\u7279\u5f81\u6784\u5efa3D\u7279\u5f81\u4f53\u79ef\u3002", "result": "\u5728Human3.6M\u3001Stanford Dogs\u7b49\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u3001\u6cdb\u5316\u80fd\u529b\u53ca\u5bf93D\u5bf9\u8c61\u751f\u6210\u4e0e\u64cd\u7eb5\u7684\u80fd\u529b\u3002", "conclusion": "KeyDiff3D\u65e0\u9700\u6602\u8d35\u6807\u6ce8\uff0c\u6709\u6548\u5b9e\u73b0\u4e86\u5355\u76ee3D\u5173\u952e\u70b9\u4f30\u8ba1\uff0c\u5e76\u652f\u6301\u5bf9\u6269\u6563\u6a21\u578b\u751f\u6210\u76843D\u5bf9\u8c61\u8fdb\u884c\u64cd\u7eb5\u3002"}}
{"id": "2507.11557", "pdf": "https://arxiv.org/pdf/2507.11557", "abs": "https://arxiv.org/abs/2507.11557", "authors": ["Jiaxu Zheng", "Meiman He", "Xuhui Tang", "Xiong Wang", "Tuoyu Cao", "Tianyi Zeng", "Lichi Zhang", "Chenyu You"], "title": "3D Wavelet Latent Diffusion Model for Whole-Body MR-to-CT Modality Translation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Magnetic Resonance (MR) imaging plays an essential role in contemporary clinical diagnostics. It is increasingly integrated into advanced therapeutic workflows, such as hybrid Positron Emission Tomography/Magnetic Resonance (PET/MR) imaging and MR-only radiation therapy. These integrated approaches are critically dependent on accurate estimation of radiation attenuation, which is typically facilitated by synthesizing Computed Tomography (CT) images from MR scans to generate attenuation maps. However, existing MR-to-CT synthesis methods for whole-body imaging often suffer from poor spatial alignment between the generated CT and input MR images, and insufficient image quality for reliable use in downstream clinical tasks. In this paper, we present a novel 3D Wavelet Latent Diffusion Model (3D-WLDM) that addresses these limitations by performing modality translation in a learned latent space. By incorporating a Wavelet Residual Module into the encoder-decoder architecture, we enhance the capture and reconstruction of fine-scale features across image and latent spaces. To preserve anatomical integrity during the diffusion process, we disentangle structural and modality-specific characteristics and anchor the structural component to prevent warping. We also introduce a Dual Skip Connection Attention mechanism within the diffusion model, enabling the generation of high-resolution CT images with improved representation of bony structures and soft-tissue contrast.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b3D\u5c0f\u6ce2\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff083D-WLDM\uff09\uff0c\u7528\u4e8e\u89e3\u51b3MR\u5230CT\u56fe\u50cf\u5408\u6210\u4e2d\u7684\u7a7a\u95f4\u5bf9\u9f50\u548c\u56fe\u50cf\u8d28\u91cf\u95ee\u9898\u3002", "motivation": "MR\u6210\u50cf\u5728\u4e34\u5e8a\u8bca\u65ad\u548c\u6cbb\u7597\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684MR\u5230CT\u5408\u6210\u65b9\u6cd5\u5b58\u5728\u7a7a\u95f4\u5bf9\u9f50\u5dee\u548c\u56fe\u50cf\u8d28\u91cf\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e0b\u6e38\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5728\u5c0f\u6ce2\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u6a21\u6001\u8f6c\u6362\uff0c\u7ed3\u5408\u5c0f\u6ce2\u6b8b\u5dee\u6a21\u5757\u589e\u5f3a\u7279\u5f81\u6355\u83b7\uff0c\u5e76\u5229\u7528\u53cc\u8df3\u8dc3\u8fde\u63a5\u6ce8\u610f\u529b\u673a\u5236\u751f\u6210\u9ad8\u5206\u8fa8\u7387CT\u56fe\u50cf\u3002", "result": "\u6a21\u578b\u80fd\u591f\u751f\u6210\u5177\u6709\u66f4\u597d\u9aa8\u9abc\u7ed3\u6784\u548c\u8f6f\u7ec4\u7ec7\u5bf9\u6bd4\u7684\u9ad8\u5206\u8fa8\u7387CT\u56fe\u50cf\u3002", "conclusion": "3D-WLDM\u663e\u8457\u63d0\u5347\u4e86MR\u5230CT\u5408\u6210\u7684\u51c6\u786e\u6027\u548c\u56fe\u50cf\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u4efb\u52a1\u3002"}}
{"id": "2507.11561", "pdf": "https://arxiv.org/pdf/2507.11561", "abs": "https://arxiv.org/abs/2507.11561", "authors": ["Lucas Erlacher", "Samuel Ruip\u00e9rez-Campillo", "Holger Michel", "Sven Wellmann", "Thomas M. Sutter", "Ece Ozkan", "Julia E. Vogt"], "title": "Predicting Pulmonary Hypertension in Newborns: A Multi-view VAE Approach", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Pulmonary hypertension (PH) in newborns is a critical condition characterized by elevated pressure in the pulmonary arteries, leading to right ventricular strain and heart failure. While right heart catheterization (RHC) is the diagnostic gold standard, echocardiography is preferred due to its non-invasive nature, safety, and accessibility. However, its accuracy highly depends on the operator, making PH assessment subjective. While automated detection methods have been explored, most models focus on adults and rely on single-view echocardiographic frames, limiting their performance in diagnosing PH in newborns. While multi-view echocardiography has shown promise in improving PH assessment, existing models struggle with generalizability. In this work, we employ a multi-view variational autoencoder (VAE) for PH prediction using echocardiographic videos. By leveraging the VAE framework, our model captures complex latent representations, improving feature extraction and robustness. We compare its performance against single-view and supervised learning approaches. Our results show improved generalization and classification accuracy, highlighting the effectiveness of multi-view learning for robust PH assessment in newborns.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u89c6\u89d2\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u65b0\u751f\u513f\u80ba\u52a8\u8109\u9ad8\u538b\uff08PH\uff09\u7684\u9884\u6d4b\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u8d85\u58f0\u5fc3\u52a8\u56fe\u89c6\u9891\u63d0\u9ad8\u7279\u5f81\u63d0\u53d6\u548c\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u5355\u89c6\u89d2\u548c\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u65b0\u751f\u513f\u80ba\u52a8\u8109\u9ad8\u538b\uff08PH\uff09\u7684\u8bca\u65ad\u4f9d\u8d56\u8d85\u58f0\u5fc3\u52a8\u56fe\uff0c\u4f46\u5176\u51c6\u786e\u6027\u53d7\u64cd\u4f5c\u8005\u4e3b\u89c2\u5f71\u54cd\uff0c\u4e14\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u591a\u9488\u5bf9\u6210\u4eba\uff0c\u5355\u89c6\u89d2\u6a21\u578b\u6027\u80fd\u6709\u9650\u3002\u591a\u89c6\u89d2\u65b9\u6cd5\u867d\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u591a\u89c6\u89d2\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u6846\u67b6\uff0c\u5229\u7528\u8d85\u58f0\u5fc3\u52a8\u56fe\u89c6\u9891\u63d0\u53d6\u590d\u6742\u6f5c\u5728\u8868\u5f81\uff0c\u63d0\u5347\u7279\u5f81\u63d0\u53d6\u548c\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u5e76\u4e0e\u5355\u89c6\u89d2\u548c\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u591a\u89c6\u89d2\u5b66\u4e60\u65b9\u6cd5\u5728\u6cdb\u5316\u80fd\u529b\u548c\u5206\u7c7b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u5355\u89c6\u89d2\u548c\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u591a\u89c6\u89d2\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u65b0\u751f\u513fPH\u8bc4\u4f30\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2507.11853", "pdf": "https://arxiv.org/pdf/2507.11853", "abs": "https://arxiv.org/abs/2507.11853", "authors": ["J. Senthilnath", "Jayasanker Jayabalan", "Zhuoyi Lin", "Aye Phyu Phyu Aung", "Chen Hao", "Kaixin Xu", "Yeow Kheng Lim", "F. C. Wellstood"], "title": "A Spatial-Physics Informed Model for 3D Spiral Sample Scanned by SQUID Microscopy", "categories": ["physics.ins-det", "cs.CV"], "comment": "copyright 2025 IEEE. Personal use of this material is permitted.   Permission from IEEE must be obtained for all other uses, in any current or   future media, including reprinting/republishing this material for advertising   or promotional purposes, creating new collective works, for resale or   redistribution to servers or lists, or reuse of any copyrighted component of   this work in other works", "summary": "The development of advanced packaging is essential in the semiconductor manufacturing industry. However, non-destructive testing (NDT) of advanced packaging becomes increasingly challenging due to the depth and complexity of the layers involved. In such a scenario, Magnetic field imaging (MFI) enables the imaging of magnetic fields generated by currents. For MFI to be effective in NDT, the magnetic fields must be converted into current density. This conversion has typically relied solely on a Fast Fourier Transform (FFT) for magnetic field inversion; however, the existing approach does not consider eddy current effects or image misalignment in the test setup. In this paper, we present a spatial-physics informed model (SPIM) designed for a 3D spiral sample scanned using Superconducting QUantum Interference Device (SQUID) microscopy. The SPIM encompasses three key components: i) magnetic image enhancement by aligning all the \"sharp\" wire field signals to mitigate the eddy current effect using both in-phase (I-channel) and quadrature-phase (Q-channel) images; (ii) magnetic image alignment that addresses skew effects caused by any misalignment of the scanning SQUID microscope relative to the wire segments; and (iii) an inversion method for converting magnetic fields to magnetic currents by integrating the Biot-Savart Law with FFT. The results show that the SPIM improves I-channel sharpness by 0.3% and reduces Q-channel sharpness by 25%. Also, we were able to remove rotational and skew misalignments of 0.30 in a real image. Overall, SPIM highlights the potential of combining spatial analysis with physics-driven models in practical applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7a7a\u95f4\u7269\u7406\u4fe1\u606f\u6a21\u578b\uff08SPIM\uff09\uff0c\u7528\u4e8e\u6539\u8fdb\u534a\u5bfc\u4f53\u5148\u8fdb\u5c01\u88c5\u4e2d\u7684\u975e\u7834\u574f\u6027\u6d4b\u8bd5\uff08NDT\uff09\uff0c\u901a\u8fc7\u78c1\u6210\u50cf\uff08MFI\uff09\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7531\u4e8e\u5148\u8fdb\u5c01\u88c5\u5c42\u6df1\u5ea6\u548c\u590d\u6742\u6027\u589e\u52a0\uff0c\u73b0\u6709\u975e\u7834\u574f\u6027\u6d4b\u8bd5\u65b9\u6cd5\uff08\u5982FFT\uff09\u672a\u8003\u8651\u6da1\u6d41\u6548\u5e94\u548c\u56fe\u50cf\u9519\u4f4d\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "SPIM\u5305\u542b\u4e09\u4e2a\u5173\u952e\u90e8\u5206\uff1a\u78c1\u56fe\u50cf\u589e\u5f3a\uff08\u5bf9\u9f50\u4fe1\u53f7\u4ee5\u51cf\u5c11\u6da1\u6d41\u6548\u5e94\uff09\u3001\u78c1\u56fe\u50cf\u5bf9\u9f50\uff08\u89e3\u51b3\u626b\u63cf\u663e\u5fae\u955c\u7684\u9519\u4f4d\u95ee\u9898\uff09\u3001\u4ee5\u53ca\u7ed3\u5408\u6bd5\u5965-\u8428\u4f10\u5c14\u5b9a\u5f8b\u548cFFT\u7684\u78c1\u573a\u53cd\u6f14\u65b9\u6cd5\u3002", "result": "SPIM\u63d0\u9ad8\u4e86I\u901a\u9053\u9510\u5ea60.3%\uff0c\u51cf\u5c11Q\u901a\u9053\u9510\u5ea625%\uff0c\u5e76\u6210\u529f\u6821\u6b63\u4e860.30\u7684\u65cb\u8f6c\u548c\u9519\u4f4d\u504f\u5dee\u3002", "conclusion": "SPIM\u5c55\u793a\u4e86\u7a7a\u95f4\u5206\u6790\u4e0e\u7269\u7406\u9a71\u52a8\u6a21\u578b\u7ed3\u5408\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u534a\u5bfc\u4f53\u5c01\u88c5\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u5de5\u5177\u3002"}}
{"id": "2507.12132", "pdf": "https://arxiv.org/pdf/2507.12132", "abs": "https://arxiv.org/abs/2507.12132", "authors": ["Navid Hasanzadeh", "Shahrokh Valaee"], "title": "DoRF: Doppler Radiance Fields for Robust Human Activity Recognition Using Wi-Fi", "categories": ["eess.SP", "cs.CV"], "comment": null, "summary": "Wi-Fi Channel State Information (CSI) has gained increasing interest for remote sensing applications. Recent studies show that Doppler velocity projections extracted from CSI can enable human activity recognition (HAR) that is robust to environmental changes and generalizes to new users. However, despite these advances, generalizability still remains insufficient for practical deployment. Inspired by neural radiance fields (NeRF), which learn a volumetric representation of a 3D scene from 2D images, this work proposes a novel approach to reconstruct an informative 3D latent motion representation from one-dimensional Doppler velocity projections extracted from Wi-Fi CSI. The resulting latent representation is then used to construct a uniform Doppler radiance field (DoRF) of the motion, providing a comprehensive view of the performed activity and improving the robustness to environmental variability. The results show that the proposed approach noticeably enhances the generalization accuracy of Wi-Fi-based HAR, highlighting the strong potential of DoRFs for practical sensing applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWi-Fi CSI\u76843D\u6f5c\u5728\u8fd0\u52a8\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u7edf\u4e00\u7684Doppler\u8f90\u5c04\u573a\uff08DoRF\uff09\u63d0\u5347\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1Wi-Fi CSI\u7684\u591a\u666e\u52d2\u901f\u5ea6\u6295\u5f71\u5728\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff08HAR\uff09\u4e2d\u8868\u73b0\u51fa\u4e00\u5b9a\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u5176\u6cdb\u5316\u80fd\u529b\u4ecd\u4e0d\u8db3\u4ee5\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u53d7\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u542f\u53d1\uff0c\u4eceWi-Fi CSI\u7684\u4e00\u7ef4\u591a\u666e\u52d2\u901f\u5ea6\u6295\u5f71\u4e2d\u91cd\u5efa3D\u6f5c\u5728\u8fd0\u52a8\u8868\u793a\uff0c\u5e76\u6784\u5efa\u7edf\u4e00\u7684Doppler\u8f90\u5c04\u573a\uff08DoRF\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86Wi-Fi HAR\u7684\u6cdb\u5316\u51c6\u786e\u6027\u3002", "conclusion": "DoRF\u5728\u5b9e\u7528\u4f20\u611f\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.12297", "pdf": "https://arxiv.org/pdf/2507.12297", "abs": "https://arxiv.org/abs/2507.12297", "authors": ["Yuan-Chen Shu", "Zhiwei Lin", "Yongtao Wang"], "title": "RegCL: Continual Adaptation of Segment Anything Model via Model Merging", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "To address the performance limitations of the Segment Anything Model (SAM) in specific domains, existing works primarily adopt adapter-based one-step adaptation paradigms. However, some of these methods are specific developed for specific domains. If used on other domains may lead to performance degradation. This issue of catastrophic forgetting severely limits the model's scalability. To address this issue, this paper proposes RegCL, a novel non-replay continual learning (CL) framework designed for efficient multi-domain knowledge integration through model merging. Specifically, RegCL incorporates the model merging algorithm into the continual learning paradigm by merging the parameters of SAM's adaptation modules (e.g., LoRA modules) trained on different domains. The merging process is guided by weight optimization, which minimizes prediction discrepancies between the merged model and each of the domain-specific models. RegCL effectively consolidates multi-domain knowledge while maintaining parameter efficiency, i.e., the model size remains constant regardless of the number of tasks, and no historical data storage is required. Experimental results demonstrate that RegCL achieves favorable continual learning performance across multiple downstream datasets, validating its effectiveness in dynamic scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRegCL\uff0c\u4e00\u79cd\u65b0\u578b\u975e\u56de\u653e\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u5408\u5e76\u5b9e\u73b0\u591a\u9886\u57df\u77e5\u8bc6\u9ad8\u6548\u6574\u5408\uff0c\u89e3\u51b3SAM\u5728\u7279\u5b9a\u9886\u57df\u6027\u80fd\u53d7\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u7279\u5b9a\u9886\u57df\u8bbe\u8ba1\uff0c\u8de8\u9886\u57df\u4f7f\u7528\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "RegCL\u5c06\u6a21\u578b\u5408\u5e76\u7b97\u6cd5\u5f15\u5165\u6301\u7eed\u5b66\u4e60\uff0c\u901a\u8fc7\u4f18\u5316\u6743\u91cd\u5408\u5e76\u4e0d\u540c\u9886\u57df\u7684\u9002\u914d\u6a21\u5757\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRegCL\u5728\u591a\u9886\u57df\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u5176\u52a8\u6001\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "RegCL\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5408\u5e76\uff0c\u5b9e\u73b0\u4e86\u591a\u9886\u57df\u77e5\u8bc6\u7684\u6574\u5408\uff0c\u89e3\u51b3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002"}}
