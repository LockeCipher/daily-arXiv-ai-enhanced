{"id": "2507.17963", "pdf": "https://arxiv.org/pdf/2507.17963", "abs": "https://arxiv.org/abs/2507.17963", "authors": ["Rameen Abdal", "Or Patashnik", "Ekaterina Deyneka", "Hao Chen", "Aliaksandr Siarohin", "Sergey Tulyakov", "Daniel Cohen-Or", "Kfir Aberman"], "title": "Zero-Shot Dynamic Concept Personalization with Grid-Based LoRA", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Project Page and Video :   https://snap-research.github.io/zero-shot-dynamic-concepts/", "summary": "Recent advances in text-to-video generation have enabled high-quality synthesis from text and image prompts. While the personalization of dynamic concepts, which capture subject-specific appearance and motion from a single video, is now feasible, most existing methods require per-instance fine-tuning, limiting scalability. We introduce a fully zero-shot framework for dynamic concept personalization in text-to-video models. Our method leverages structured 2x2 video grids that spatially organize input and output pairs, enabling the training of lightweight Grid-LoRA adapters for editing and composition within these grids. At inference, a dedicated Grid Fill module completes partially observed layouts, producing temporally coherent and identity preserving outputs. Once trained, the entire system operates in a single forward pass, generalizing to previously unseen dynamic concepts without any test-time optimization. Extensive experiments demonstrate high-quality and consistent results across a wide range of subjects beyond trained concepts and editing scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u6846\u67b6\uff0c\u7528\u4e8e\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u4e2d\u7684\u52a8\u6001\u6982\u5ff5\u4e2a\u6027\u5316\uff0c\u65e0\u9700\u5b9e\u4f8b\u5fae\u8c03\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u89c6\u9891\u7f51\u683c\u548c\u8f7b\u91cf\u7ea7Grid-LoRA\u9002\u914d\u5668\u5b9e\u73b0\u9ad8\u6548\u7f16\u8f91\u548c\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u5b9e\u4f8b\u8fdb\u884c\u5fae\u8c03\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u52a8\u6001\u6982\u5ff5\u4e2a\u6027\u5316\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u7ed3\u6784\u53162x2\u89c6\u9891\u7f51\u683c\u7ec4\u7ec7\u8f93\u5165\u8f93\u51fa\u5bf9\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7Grid-LoRA\u9002\u914d\u5668\uff0c\u5e76\u901a\u8fc7Grid Fill\u6a21\u5757\u5728\u63a8\u7406\u65f6\u5b8c\u6210\u90e8\u5206\u89c2\u5bdf\u7684\u5e03\u5c40\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u672a\u89c1\u8fc7\u7684\u52a8\u6001\u6982\u5ff5\u548c\u7f16\u8f91\u573a\u666f\u4e2d\u5747\u80fd\u4ea7\u751f\u9ad8\u8d28\u91cf\u4e14\u4e00\u81f4\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u5b9e\u73b0\u52a8\u6001\u6982\u5ff5\u4e2a\u6027\u5316\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.18155", "pdf": "https://arxiv.org/pdf/2507.18155", "abs": "https://arxiv.org/abs/2507.18155", "authors": ["SeungJun Moon", "Hah Min Lew", "Seungeun Lee", "Ji-Su Kang", "Gyeong-Moon Park"], "title": "GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "ICCV 2025, Project page: https://hahminlew.github.io/geoavatar/", "summary": "Despite recent progress in 3D head avatar generation, balancing identity preservation, i.e., reconstruction, with novel poses and expressions, i.e., animation, remains a challenge. Existing methods struggle to adapt Gaussians to varying geometrical deviations across facial regions, resulting in suboptimal quality. To address this, we propose GeoAvatar, a framework for adaptive geometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation Stage (APS), an unsupervised method that segments Gaussians into rigid and flexible sets for adaptive offset regularization. Then, based on mouth anatomy and dynamics, we introduce a novel mouth structure and the part-wise deformation strategy to enhance the animation fidelity of the mouth. Finally, we propose a regularization loss for precise rigging between Gaussians and 3DMM faces. Moreover, we release DynamicFace, a video dataset with highly expressive facial motions. Extensive experiments show the superiority of GeoAvatar compared to state-of-the-art methods in reconstruction and novel animation scenarios.", "AI": {"tldr": "GeoAvatar\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u51e0\u4f55\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u901a\u8fc7APS\u5206\u5272\u9ad8\u65af\u4e3a\u521a\u6027\u548c\u67d4\u6027\u96c6\uff0c\u7ed3\u5408\u5634\u90e8\u7ed3\u6784\u548c\u90e8\u5206\u53d8\u5f62\u7b56\u7565\uff0c\u63d0\u5347\u52a8\u753b\u8d28\u91cf\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b33D\u5934\u90e8\u5316\u8eab\u751f\u6210\u4e2d\u8eab\u4efd\u4fdd\u6301\u4e0e\u65b0\u9896\u59ff\u52bf\u548c\u8868\u60c5\u52a8\u753b\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u81ea\u9002\u5e94\u9884\u5206\u914d\u9636\u6bb5\uff08APS\uff09\u5206\u5272\u9ad8\u65af\uff0c\u5f15\u5165\u5634\u90e8\u7ed3\u6784\u548c\u90e8\u5206\u53d8\u5f62\u7b56\u7565\uff0c\u63d0\u51fa\u6b63\u5219\u5316\u635f\u5931\u3002", "result": "\u5728\u91cd\u5efa\u548c\u65b0\u52a8\u753b\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GeoAvatar\u901a\u8fc7\u81ea\u9002\u5e94\u51e0\u4f55\u9ad8\u65af\u6cfc\u6e85\u548c\u5634\u90e8\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u5934\u90e8\u5316\u8eab\u7684\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2507.18231", "pdf": "https://arxiv.org/pdf/2507.18231", "abs": "https://arxiv.org/abs/2507.18231", "authors": ["Yixiao Chen", "Bin Liang", "Hanzhi Guo", "Yongqing Cheng", "Jiayi Zhao", "Dongdong Weng"], "title": "PS-GS: Gaussian Splatting for Multi-View Photometric Stereo", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Integrating inverse rendering with multi-view photometric stereo (MVPS) yields more accurate 3D reconstructions than the inverse rendering approaches that rely on fixed environment illumination. However, efficient inverse rendering with MVPS remains challenging. To fill this gap, we introduce the Gaussian Splatting for Multi-view Photometric Stereo (PS-GS), which efficiently and jointly estimates the geometry, materials, and lighting of the object that is illuminated by diverse directional lights (multi-light). Our method first reconstructs a standard 2D Gaussian splatting model as the initial geometry. Based on the initialization model, it then proceeds with the deferred inverse rendering by the full rendering equation containing a lighting-computing multi-layer perceptron. During the whole optimization, we regularize the rendered normal maps by the uncalibrated photometric stereo estimated normals. We also propose the 2D Gaussian ray-tracing for single directional light to refine the incident lighting. The regularizations and the use of multi-view and multi-light images mitigate the ill-posed problem of inverse rendering. After optimization, the reconstructed object can be used for novel-view synthesis, relighting, and material and shape editing. Experiments on both synthetic and real datasets demonstrate that our method outperforms prior works in terms of reconstruction accuracy and computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9ad8\u65af\u70b9\u4e91\u4e0e\u591a\u89c6\u89d2\u5149\u5ea6\u7acb\u4f53\uff08MVPS\uff09\u7684\u9ad8\u6548\u9006\u6e32\u67d3\u65b9\u6cd5PS-GS\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u91cd\u5efa\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u9006\u6e32\u67d3\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u5149\u7167\u73af\u5883\uff0c\u800c\u7ed3\u5408MVPS\u867d\u80fd\u63d0\u5347\u7cbe\u5ea6\uff0c\u4f46\u6548\u7387\u4ecd\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8e2D\u9ad8\u65af\u70b9\u4e91\u521d\u59cb\u5316\u51e0\u4f55\u6a21\u578b\uff0c\u901a\u8fc7\u5ef6\u8fdf\u9006\u6e32\u67d3\u548c\u5149\u7167\u8ba1\u7b97\u591a\u5c42\u611f\u77e5\u673a\u8054\u5408\u4f18\u5316\u51e0\u4f55\u3001\u6750\u8d28\u548c\u5149\u7167\uff0c\u5e76\u5229\u7528\u672a\u6807\u5b9a\u5149\u5ea6\u7acb\u4f53\u6cd5\u4f30\u8ba1\u7684\u6cd5\u7ebf\u56fe\u8fdb\u884c\u6b63\u5219\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u91cd\u5efa\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "PS-GS\u4e3a\u9006\u6e32\u67d3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u65b0\u89c6\u89d2\u5408\u6210\u3001\u91cd\u5149\u7167\u53ca\u6750\u8d28\u5f62\u72b6\u7f16\u8f91\u3002"}}
