<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 6]
- [cs.CV](#cs.CV) [Total: 23]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Generative Panoramic Image Stitching](https://arxiv.org/abs/2507.07133)
*Mathieu Tuli,Kaveh Kamali,David B. Lindell*

Main category: cs.GR

TL;DR: 提出了一种基于扩散模型的生成式全景图像拼接方法，解决了传统方法和现有生成模型在视差、光照和风格差异下的拼接问题。


<details>
  <summary>Details</summary>
Motivation: 传统图像拼接方法在视差和光照变化下会产生伪影，而现有生成模型难以合成大范围连贯的全景图。

Method: 通过微调扩散式修复模型，基于多张参考图像保留场景内容和布局，生成无缝全景图。

Result: 在图像质量和场景布局一致性上显著优于基线方法。

Conclusion: 该方法能生成无缝且视觉连贯的全景图，忠实整合多张参考图像内容。

Abstract: We introduce the task of generative panoramic image stitching, which aims to synthesize seamless panoramas that are faithful to the content of multiple reference images containing parallax effects and strong variations in lighting, camera capture settings, or style. In this challenging setting, traditional image stitching pipelines fail, producing outputs with ghosting and other artifacts. While recent generative models are capable of outpainting content consistent with multiple reference images, they fail when tasked with synthesizing large, coherent regions of a panorama. To address these limitations, we propose a method that fine-tunes a diffusion-based inpainting model to preserve a scene's content and layout based on multiple reference images. Once fine-tuned, the model outpaints a full panorama from a single reference image, producing a seamless and visually coherent result that faithfully integrates content from all reference images. Our approach significantly outperforms baselines for this task in terms of image quality and the consistency of image structure and scene layout when evaluated on captured datasets.

</details>


### [2] [LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+ FPS](https://arxiv.org/abs/2507.07136)
*Wanhua Li,Yujie Zhao,Minghan Qin,Yang Liu,Yuanhao Cai,Chuang Gan,Hanspeter Pfister*

Main category: cs.GR

TL;DR: LangSplatV2通过稀疏编码和高斯溅射技术，显著提升了3D语言特征的处理速度和查询准确性，实现了实时性能。


<details>
  <summary>Details</summary>
Motivation: LangSplat在实时推理性能上表现不佳（8.2 FPS），限制了其广泛应用。LangSplatV2旨在解决这一问题。

Method: 采用稀疏编码假设，学习3D稀疏系数场，消除重型解码器需求，并结合CUDA优化的稀疏系数溅射方法。

Result: LangSplatV2达到476.2 FPS的高维特征溅射和384.6 FPS的3D开放词汇查询，速度和准确性均显著提升。

Conclusion: LangSplatV2在保持或提升查询准确性的同时，实现了显著的性能优化，为复杂场景中的语言交互应用提供了高效解决方案。

Abstract: In this paper, we introduce LangSplatV2, which achieves high-dimensional feature splatting at 476.2 FPS and 3D open-vocabulary text querying at 384.6 FPS for high-resolution images, providing a 42 $\times$ speedup and a 47 $\times$ boost over LangSplat respectively, along with improved query accuracy. LangSplat employs Gaussian Splatting to embed 2D CLIP language features into 3D, significantly enhancing speed and learning a precise 3D language field with SAM semantics. Such advancements in 3D language fields are crucial for applications that require language interaction within complex scenes. However, LangSplat does not yet achieve real-time inference performance (8.2 FPS), even with advanced A100 GPUs, severely limiting its broader application. In this paper, we first conduct a detailed time analysis of LangSplat, identifying the heavyweight decoder as the primary speed bottleneck. Our solution, LangSplatV2 assumes that each Gaussian acts as a sparse code within a global dictionary, leading to the learning of a 3D sparse coefficient field that entirely eliminates the need for a heavyweight decoder. By leveraging this sparsity, we further propose an efficient sparse coefficient splatting method with CUDA optimization, rendering high-dimensional feature maps at high quality while incurring only the time cost of splatting an ultra-low-dimensional feature. Our experimental results demonstrate that LangSplatV2 not only achieves better or competitive query accuracy but is also significantly faster. Codes and demos are available at our project page: https://langsplat-v2.github.io.

</details>


### [3] [Digital Salon: An AI and Physics-Driven Tool for 3D Hair Grooming and Simulation](https://arxiv.org/abs/2507.07387)
*Chengan He,Jorge Alejandro Amador Herrera,Zhixin Shu,Xin Sun,Yao Feng,Sören Pirk,Dominik L. Michels,Meng Zhang,Tuanfeng Y. Wang,Julie Dorsey,Holly Rushmeier,Yi Zhou*

Main category: cs.GR

TL;DR: Digital Salon是一个支持实时3D头发生成、模拟和渲染的综合系统，通过自然语言交互降低技术门槛。


<details>
  <summary>Details</summary>
Motivation: 现有方法专注于3D头发建模的孤立部分，计算量大或需要网络训练，Digital Salon旨在提供一种全面且交互式的解决方案。

Method: 系统包括四个关键阶段：文本引导的头发检索、实时头发模拟、交互式头发精修和头发条件图像生成。

Result: 用户研究表明，该系统在快速原型设计方面优于传统工作流程。

Conclusion: Digital Salon为不同技能水平的用户提供了直观、多功能且高效的头发建模解决方案，并展示了在真实沙龙环境中部署的潜力。

Abstract: We introduce Digital Salon, a comprehensive hair authoring system that supports real-time 3D hair generation, simulation, and rendering. Unlike existing methods that focus on isolated parts of 3D hair modeling and involve a heavy computation process or network training, Digital Salon offers a holistic and interactive system that lowers the technical barriers of 3D hair modeling through natural language-based interaction. The system guides users through four key stages: text-guided hair retrieval, real-time hair simulation, interactive hair refinement, and hair-conditioned image generation. This cohesive workflow makes advanced hair design accessible to users of varying skill levels and dramatically streamlines the creative process in digital media with an intuitive, versatile, and efficient solution for hair modeling. User studies show that our system can outperform traditional hair modeling workflows for rapid prototyping. Furthermore, we provide insights into the benefits of our system with future potential of deploying our system in real salon environments. More details can be found on our project page: https://digital-salon.github.io/.

</details>


### [4] [SD-GS: Structured Deformable 3D Gaussians for Efficient Dynamic Scene Reconstruction](https://arxiv.org/abs/2507.07465)
*Wei Yao,Shuzhao Xie,Letian Li,Weixiang Zhang,Zhixin Lai,Shiqi Dai,Ke Zhang,Zhi Wang*

Main category: cs.GR

TL;DR: SD-GS是一种紧凑高效的动态高斯泼溅框架，通过可变形锚点网格和变形感知的密集化策略，显著降低了存储成本并提升了渲染速度。


<details>
  <summary>Details</summary>
Motivation: 当前4D高斯框架在动态场景重建中存在存储成本与复杂运动建模能力的权衡问题，限制了实际应用。

Method: 提出可变形锚点网格作为层次化、内存高效的场景表示，并结合变形感知密集化策略，动态调整锚点分布。

Result: 实验显示，SD-GS模型大小平均减少60%，FPS提升100%，同时保持或超越视觉质量。

Conclusion: SD-GS在存储效率和计算性能上显著优于现有方法，适用于复杂动态场景重建。

Abstract: Current 4D Gaussian frameworks for dynamic scene reconstruction deliver impressive visual fidelity and rendering speed, however, the inherent trade-off between storage costs and the ability to characterize complex physical motions significantly limits the practical application of these methods. To tackle these problems, we propose SD-GS, a compact and efficient dynamic Gaussian splatting framework for complex dynamic scene reconstruction, featuring two key contributions. First, we introduce a deformable anchor grid, a hierarchical and memory-efficient scene representation where each anchor point derives multiple 3D Gaussians in its local spatiotemporal region and serves as the geometric backbone of the 3D scene. Second, to enhance modeling capability for complex motions, we present a deformation-aware densification strategy that adaptively grows anchors in under-reconstructed high-dynamic regions while reducing redundancy in static areas, achieving superior visual quality with fewer anchors. Experimental results demonstrate that, compared to state-of-the-art methods, SD-GS achieves an average of 60\% reduction in model size and an average of 100\% improvement in FPS, significantly enhancing computational efficiency while maintaining or even surpassing visual quality.

</details>


### [5] [Capture Stage Environments: A Guide to Better Matting](https://arxiv.org/abs/2507.07623)
*Hannah Dröge,Janelle Pfeifer,Saskia Rabich,Markus Plack,Reinhard Klein,Matthias B. Hullin*

Main category: cs.GR

TL;DR: 论文探讨了高端捕捉场景中图像抠图的挑战，提出了改进工作流程的指南，并展示了一种无需大量标注的高效管道。


<details>
  <summary>Details</summary>
Motivation: 现有抠图算法在捕捉场景内容中表现不佳，需解决其特殊性带来的挑战。

Method: 提出改进工作流程指南，并展示离线与实时的高效管道，无需大量标注。

Result: 通过基于扩散模型的验证方法，证明了方法的优势。

Conclusion: 论文为实践者提供了解决捕捉场景抠图挑战的实用指南和高效方法。

Abstract: Capture stages are high-end sources of state-of-the-art recordings for downstream applications in movies, games, and other media. One crucial step in almost all pipelines is the matting of images to isolate the captured performances from the background. While common matting algorithms deliver remarkable performance in other applications like teleconferencing and mobile entertainment, we found that they struggle significantly with the peculiarities of capture stage content. The goal of our work is to share insights into those challenges as a curated list of those characteristics along with a constructive discussion for proactive intervention and present a guideline to practitioners for an improved workflow to mitigate unresolved challenges. To this end, we also demonstrate an efficient pipeline to adapt state-of-the-art approaches to such custom setups without the need of extensive annotations, both offline and real-time. For an objective evaluation, we propose a validation methodology based on a leading diffusion model that highlights the benefits of our approach.

</details>


### [6] [RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance Transfer and Reflection](https://arxiv.org/abs/2507.07733)
*Yongyang Zhou,Fang-Lue Zhang,Zichen Wang,Lei Zhang*

Main category: cs.GR

TL;DR: RTR-GS是一种新的逆渲染框架，能够处理任意反射属性的物体，分解BRDF和光照，并提供可信的重光照结果。


<details>
  <summary>Details</summary>
Motivation: 尽管3D高斯溅射（3DGS）在新视角合成中表现出色，但渲染反射物体仍是一个挑战，尤其是在逆渲染和重光照方面。

Method: 通过结合前向渲染和延迟渲染的混合模型，RTR-GS有效恢复几何结构，并分离高频和低频外观。进一步使用基于物理的延迟渲染分支优化BRDF和光照分解。

Result: 实验表明，该方法提升了新视角合成、法线估计、分解和重光照的效果，同时保持了高效的训练和推理过程。

Conclusion: RTR-GS在逆渲染和重光照任务中表现出色，解决了高频率细节处理中的问题。

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated impressive capabilities in novel view synthesis. However, rendering reflective objects remains a significant challenge, particularly in inverse rendering and relighting. We introduce RTR-GS, a novel inverse rendering framework capable of robustly rendering objects with arbitrary reflectance properties, decomposing BRDF and lighting, and delivering credible relighting results. Given a collection of multi-view images, our method effectively recovers geometric structure through a hybrid rendering model that combines forward rendering for radiance transfer with deferred rendering for reflections. This approach successfully separates high-frequency and low-frequency appearances, mitigating floating artifacts caused by spherical harmonic overfitting when handling high-frequency details. We further refine BRDF and lighting decomposition using an additional physically-based deferred rendering branch. Experimental results show that our method enhances novel view synthesis, normal estimation, decomposition, and relighting while maintaining efficient training inference process.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [7] [Image Can Bring Your Memory Back: A Novel Multi-Modal Guided Attack against Image Generation Model Unlearning](https://arxiv.org/abs/2507.07139)
*Renyang Liu,Guanlin Li,Tianwei Zhang,See-Kiong Ng*

Main category: cs.CV

TL;DR: 论文提出了一种名为Recall的新型对抗框架，旨在测试图像生成模型（如Stable Diffusion）在去学习技术中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管去学习技术（MU）被提出以解决图像生成模型（IGMs）可能产生的有害或侵权内容，但其在多模态对抗输入下的有效性尚未充分验证。

Method: Recall通过优化对抗性图像提示，利用扩散模型的多模态条件能力，测试现有去学习方法的鲁棒性。

Result: 实验表明，Recall在对抗效果、计算效率和语义保真度上均优于现有基线，揭示了当前去学习机制的脆弱性。

Conclusion: 研究强调了开发更鲁棒的去学习解决方案的必要性，以确保生成模型的安全性和可靠性。

Abstract: Recent advances in image generation models (IGMs), particularly diffusion-based architectures such as Stable Diffusion (SD), have markedly enhanced the quality and diversity of AI-generated visual content. However, their generative capability has also raised significant ethical, legal, and societal concerns, including the potential to produce harmful, misleading, or copyright-infringing content. To mitigate these concerns, machine unlearning (MU) emerges as a promising solution by selectively removing undesirable concepts from pretrained models. Nevertheless, the robustness and effectiveness of existing unlearning techniques remain largely unexplored, particularly in the presence of multi-modal adversarial inputs.   To bridge this gap, we propose Recall, a novel adversarial framework explicitly designed to compromise the robustness of unlearned IGMs. Unlike existing approaches that predominantly rely on adversarial text prompts, Recall exploits the intrinsic multi-modal conditioning capabilities of diffusion models by efficiently optimizing adversarial image prompts with guidance from a single semantically relevant reference image. Extensive experiments across ten state-of-the-art unlearning methods and diverse tasks show that Recall consistently outperforms existing baselines in terms of adversarial effectiveness, computational efficiency, and semantic fidelity with the original textual prompt. These findings reveal critical vulnerabilities in current unlearning mechanisms and underscore the need for more robust solutions to ensure the safety and reliability of generative models. Code and data are publicly available at \textcolor{blue}{https://github.com/ryliu68/RECALL}.

</details>


### [8] [Robust Multimodal Large Language Models Against Modality Conflict](https://arxiv.org/abs/2507.07151)
*Zongmeng Zhang,Wengang Zhou,Jie Zhao,Houqiang Li*

Main category: cs.CV

TL;DR: 论文研究了多模态大语言模型（MLLMs）在视觉语言任务中的幻觉现象，提出模态冲突是导致幻觉的原因，并构建了MMMC数据集和三种缓解方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注模型响应与输入的冲突，而忽略了多模态输入间的固有冲突，本研究旨在填补这一空白。

Method: 通过提示工程、监督微调和强化学习三种方法缓解模态冲突引发的幻觉，并在MMMC数据集上进行实验。

Result: 强化学习方法在缓解模态冲突导致的幻觉中表现最佳，监督微调方法表现稳定且有潜力。

Conclusion: 研究揭示了模态冲突对幻觉的影响，为提升MLLMs的鲁棒性提供了新视角。

Abstract: Despite the impressive capabilities of multimodal large language models (MLLMs) in vision-language tasks, they are prone to hallucinations in real-world scenarios. This paper investigates the hallucination phenomenon in MLLMs from the perspective of modality conflict. Unlike existing works focusing on the conflicts between model responses and inputs, we study the inherent conflicts in inputs from different modalities that place MLLMs in a dilemma and directly lead to hallucinations. We formally define the modality conflict and construct a dataset named Multimodal Modality Conflict (MMMC) to simulate this phenomenon in vision-language tasks. Three methods based on prompt engineering, supervised fine-tuning, and reinforcement learning are proposed to alleviate the hallucination caused by modality conflict. Extensive experiments are conducted on the MMMC dataset to analyze the merits and demerits of these methods. Our results show that the reinforcement learning method achieves the best performance in mitigating the hallucination under modality conflict, while the supervised fine-tuning method shows promising and stable performance. Our work sheds light on the unnoticed modality conflict that leads to hallucinations and provides more insights into the robustness of MLLMs.

</details>


### [9] [Interpretable EEG-to-Image Generation with Semantic Prompts](https://arxiv.org/abs/2507.07157)
*Arshak Rezvani,Ali Akbari,Kosar Sanjar Arani,Maryam Mirian,Emad Arasteh,Martin J. McKeown*

Main category: cs.CV

TL;DR: 该论文提出了一种通过语义对齐EEG信号与多级文本描述，间接生成图像的方法，显著提升了EEG视觉解码的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: EEG在视觉解码中空间分辨率有限，直接生成图像效果不佳。通过语义中介（多级文本描述）可以更好地解码大脑活动。

Method: 使用基于Transformer的EEG编码器，通过对比学习将EEG信号对齐到多级语义描述（由大型语言模型生成）。推理时，利用投影头提取的文本嵌入条件预训练的潜在扩散模型生成图像。

Result: 在EEGCVPR数据集上实现了最先进的视觉解码性能，EEG与语义描述的关联揭示了神经认知路径的层次结构。

Conclusion: 通过语义中介的框架，实现了与认知对齐的EEG视觉解码，为神经科学和可解释AI提供了新思路。

Abstract: Decoding visual experience from brain signals offers exciting possibilities for neuroscience and interpretable AI. While EEG is accessible and temporally precise, its limitations in spatial detail hinder image reconstruction. Our model bypasses direct EEG-to-image generation by aligning EEG signals with multilevel semantic captions -- ranging from object-level to abstract themes -- generated by a large language model. A transformer-based EEG encoder maps brain activity to these captions through contrastive learning. During inference, caption embeddings retrieved via projection heads condition a pretrained latent diffusion model for image generation. This text-mediated framework yields state-of-the-art visual decoding on the EEGCVPR dataset, with interpretable alignment to known neurocognitive pathways. Dominant EEG-caption associations reflected the importance of different semantic levels extracted from perceived images. Saliency maps and t-SNE projections reveal semantic topography across the scalp. Our model demonstrates how structured semantic mediation enables cognitively aligned visual decoding from EEG.

</details>


### [10] [A Survey on Long-Video Storytelling Generation: Architectures, Consistency, and Cinematic Quality](https://arxiv.org/abs/2507.07202)
*Mohamed Elmoghany,Ryan Rossi,Seunghyun Yoon,Subhojyoti Mukherjee,Eslam Bakr,Puneet Mathur,Gang Wu,Viet Dac Lai,Nedim Lipka,Ruiyi Zhang,Varun Manjunatha,Chien Nguyen,Daksh Dangi,Abel Salinas,Mohammad Taesiri,Hongjie Chen,Xiaolei Huang,Joe Barrow,Nesreen Ahmed,Hoda Eldardiry,Namyong Park,Yu Wang,Jaemin Cho,Anh Totti Nguyen,Zhengzhong Tu,Thien Nguyen,Dinesh Manocha,Mohamed Elhoseiny,Franck Dernoncourt*

Main category: cs.CV

TL;DR: 论文分析了现有视频生成模型的局限性，提出了改进长视频生成的方法，并构建了新的分类法。


<details>
  <summary>Details</summary>
Motivation: 现有方法生成的长视频（超过16秒）难以保持角色一致性和场景布局，且多主题长视频表现更差。

Method: 研究了32篇视频生成论文，识别关键架构和训练策略，并构建了新的分类法。

Result: 提出了改进长视频生成的方法，并提供了分类表和性能比较。

Conclusion: 论文为长视频生成提供了新的研究方向和分类框架。

Abstract: Despite the significant progress that has been made in video generative models, existing state-of-the-art methods can only produce videos lasting 5-16 seconds, often labeled "long-form videos". Furthermore, videos exceeding 16 seconds struggle to maintain consistent character appearances and scene layouts throughout the narrative. In particular, multi-subject long videos still fail to preserve character consistency and motion coherence. While some methods can generate videos up to 150 seconds long, they often suffer from frame redundancy and low temporal diversity. Recent work has attempted to produce long-form videos featuring multiple characters, narrative coherence, and high-fidelity detail. We comprehensively studied 32 papers on video generation to identify key architectural components and training strategies that consistently yield these qualities. We also construct a comprehensive novel taxonomy of existing methods and present comparative tables that categorize papers by their architectural designs and performance characteristics.

</details>


### [11] [Scalable and Realistic Virtual Try-on Application for Foundation Makeup with Kubelka-Munk Theory](https://arxiv.org/abs/2507.07333)
*Hui Pang,Sunil Hadap,Violetta Shevchenko,Rahul Suresh,Amin Banitalebi-Dehkordi*

Main category: cs.CV

TL;DR: 提出了一种基于Kubelka-Munk理论的快速图像合成方法，用于虚拟试妆（VTO），并构建了一个可扩展的端到端框架。


<details>
  <summary>Details</summary>
Motivation: 解决虚拟试妆中粉底与肤色准确融合的技术挑战，同时保持方法的可扩展性。

Method: 近似Kubelka-Munk理论以实现快速图像合成，并构建依赖电商产品信息的端到端框架。

Result: 在真实化妆图像上验证，优于其他技术。

Conclusion: 该方法在保持真实感的同时提高了效率和可扩展性。

Abstract: Augmented reality is revolutionizing beauty industry with virtual try-on (VTO) applications, which empowers users to try a wide variety of products using their phones without the hassle of physically putting on real products. A critical technical challenge in foundation VTO applications is the accurate synthesis of foundation-skin tone color blending while maintaining the scalability of the method across diverse product ranges. In this work, we propose a novel method to approximate well-established Kubelka-Munk (KM) theory for faster image synthesis while preserving foundation-skin tone color blending realism. Additionally, we build a scalable end-to-end framework for realistic foundation makeup VTO solely depending on the product information available on e-commerce sites. We validate our method using real-world makeup images, demonstrating that our framework outperforms other techniques.

</details>


### [12] [Behave Your Motion: Habit-preserved Cross-category Animal Motion Transfer](https://arxiv.org/abs/2507.07394)
*Zhimin Zhang,Bi'an Du,Caoyuan Ma,Zheng Wang,Wei Hu*

Main category: cs.CV

TL;DR: 提出了一种保留动物习惯的跨类别运动转移框架，结合生成模型和大型语言模型，有效解决了现有方法忽视动物习惯行为的问题。


<details>
  <summary>Details</summary>
Motivation: 现有运动转移方法主要关注人类运动，忽略了动物特有的习惯行为，导致跨类别动物运动转移效果不佳。

Method: 基于生成框架，引入习惯保留模块和类别特定习惯编码器，结合大型语言模型（LLM）处理未观察到的物种。

Result: 在DeformingThings4D-skl数据集上的实验验证了模型的有效性，展示了其优越性。

Conclusion: 该框架成功保留了动物习惯行为，为跨类别动物运动转移提供了新思路。

Abstract: Animal motion embodies species-specific behavioral habits, making the transfer of motion across categories a critical yet complex task for applications in animation and virtual reality. Existing motion transfer methods, primarily focused on human motion, emphasize skeletal alignment (motion retargeting) or stylistic consistency (motion style transfer), often neglecting the preservation of distinct habitual behaviors in animals. To bridge this gap, we propose a novel habit-preserved motion transfer framework for cross-category animal motion. Built upon a generative framework, our model introduces a habit-preservation module with category-specific habit encoder, allowing it to learn motion priors that capture distinctive habitual characteristics. Furthermore, we integrate a large language model (LLM) to facilitate the motion transfer to previously unobserved species. To evaluate the effectiveness of our approach, we introduce the DeformingThings4D-skl dataset, a quadruped dataset with skeletal bindings, and conduct extensive experiments and quantitative analyses, which validate the superiority of our proposed model.

</details>


### [13] [Seg-Wild: Interactive Segmentation based on 3D Gaussian Splatting for Unconstrained Image Collections](https://arxiv.org/abs/2507.07395)
*Yongtang Bao,Chengjie Tang,Yuze Wang,Haojie Li*

Main category: cs.CV

TL;DR: Seg-Wild是一种基于3D高斯泼溅的交互式分割方法，适用于无约束图像集合，解决了光照不一致和瞬态遮挡问题。


<details>
  <summary>Details</summary>
Motivation: 无约束照片集合易于获取但存在光照不一致和瞬态遮挡问题，传统方法无法有效处理。

Method: 结合多维特征嵌入和3D高斯相似性计算，引入Spiky 3D Gaussian Cutter平滑异常高斯，并通过SAM掩模计算切割比例。

Result: 实验表明Seg-Wild在分割和重建质量上优于先前方法。

Conclusion: Seg-Wild为无约束场景提供了高效的分割和重建解决方案。

Abstract: Reconstructing and segmenting scenes from unconstrained photo collections obtained from the Internet is a novel but challenging task. Unconstrained photo collections are easier to get than well-captured photo collections. These unconstrained images suffer from inconsistent lighting and transient occlusions, which makes segmentation challenging. Previous segmentation methods cannot address transient occlusions or accurately restore the scene's lighting conditions. Therefore, we propose Seg-Wild, an interactive segmentation method based on 3D Gaussian Splatting for unconstrained image collections, suitable for in-the-wild scenes. We integrate multi-dimensional feature embeddings for each 3D Gaussian and calculate the feature similarity between the feature embeddings and the segmentation target to achieve interactive segmentation in the 3D scene. Additionally, we introduce the Spiky 3D Gaussian Cutter (SGC) to smooth abnormal 3D Gaussians. We project the 3D Gaussians onto a 2D plane and calculate the ratio of 3D Gaussians that need to be cut using the SAM mask. We also designed a benchmark to evaluate segmentation quality in in-the-wild scenes. Experimental results demonstrate that compared to previous methods, Seg-Wild achieves better segmentation results and reconstruction quality. Our code will be available at https://github.com/Sugar0725/Seg-Wild.

</details>


### [14] [EscherNet++: Simultaneous Amodal Completion and Scalable View Synthesis through Masked Fine-Tuning and Enhanced Feed-Forward 3D Reconstruction](https://arxiv.org/abs/2507.07410)
*Xinan Zhang,Muhammad Zubair Irshad,Anthony Yezzi,Yi-Chang Tsai,Zsolt Kira*

Main category: cs.CV

TL;DR: EscherNet++是一种掩码微调扩散模型，能够零样本合成物体的新视角并具备非模态补全能力。相比多阶段复杂流程的方法，它通过端到端模型实现高效新视角合成和非模态补全。


<details>
  <summary>Details</summary>
Motivation: 现有方法采用多阶段流程，忽略了跨视角依赖且计算冗余。EscherNet++旨在通过掩码微调实现高效、端到端的新视角合成和非模态补全。

Method: 采用输入级和特征级掩码微调，构建端到端模型，并与其他前馈图像到网格模型结合，无需额外训练。

Result: 在较小数据集和批量下，PSNR提升3.9，Volume IoU提升0.28，重建时间减少95%，且适用于真实世界遮挡重建。

Conclusion: EscherNet++通过掩码微调和端到端设计，实现了高效、高质量的新视角合成和非模态补全，性能优于现有方法。

Abstract: We propose EscherNet++, a masked fine-tuned diffusion model that can synthesize novel views of objects in a zero-shot manner with amodal completion ability. Existing approaches utilize multiple stages and complex pipelines to first hallucinate missing parts of the image and then perform novel view synthesis, which fail to consider cross-view dependencies and require redundant storage and computing for separate stages. Instead, we apply masked fine-tuning including input-level and feature-level masking to enable an end-to-end model with the improved ability to synthesize novel views and conduct amodal completion. In addition, we empirically integrate our model with other feed-forward image-to-mesh models without extra training and achieve competitive results with reconstruction time decreased by 95%, thanks to its ability to synthesize arbitrary query views. Our method's scalable nature further enhances fast 3D reconstruction. Despite fine-tuning on a smaller dataset and batch size, our method achieves state-of-the-art results, improving PSNR by 3.9 and Volume IoU by 0.28 on occluded tasks in 10-input settings, while also generalizing to real-world occluded reconstruction.

</details>


### [15] [Degradation-Agnostic Statistical Facial Feature Transformation for Blind Face Restoration in Adverse Weather Conditions](https://arxiv.org/abs/2507.07464)
*Chang-Hwan Son*

Main category: cs.CV

TL;DR: 提出了一种基于GAN的盲人脸图像恢复框架，结合局部统计特征变换和退化无关特征嵌入，显著提升了恶劣天气下的人脸识别性能。


<details>
  <summary>Details</summary>
Motivation: 恶劣天气条件下的人脸图像质量下降，导致识别精度降低，现有方法因缺乏针对性模块而表现有限。

Method: 提出局部统计特征变换（SFFT）和退化无关特征嵌入（DAFE）模块，分别增强局部统计分布对齐和特征提取鲁棒性。

Result: 实验表明，该框架在抑制纹理失真和重建面部结构方面优于现有GAN和扩散模型方法。

Conclusion: SFFT和DAFE模块有效提升了恶劣天气下的人脸恢复效果，具有显著的结构保真度和感知质量提升。

Abstract: With the increasing deployment of intelligent CCTV systems in outdoor environments, there is a growing demand for face recognition systems optimized for challenging weather conditions. Adverse weather significantly degrades image quality, which in turn reduces recognition accuracy. Although recent face image restoration (FIR) models based on generative adversarial networks (GANs) and diffusion models have shown progress, their performance remains limited due to the lack of dedicated modules that explicitly address weather-induced degradations. This leads to distorted facial textures and structures. To address these limitations, we propose a novel GAN-based blind FIR framework that integrates two key components: local Statistical Facial Feature Transformation (SFFT) and Degradation-Agnostic Feature Embedding (DAFE). The local SFFT module enhances facial structure and color fidelity by aligning the local statistical distributions of low-quality (LQ) facial regions with those of high-quality (HQ) counterparts. Complementarily, the DAFE module enables robust statistical facial feature extraction under adverse weather conditions by aligning LQ and HQ encoder representations, thereby making the restoration process adaptive to severe weather-induced degradations. Experimental results demonstrate that the proposed degradation-agnostic SFFT model outperforms existing state-of-the-art FIR methods based on GAN and diffusion models, particularly in suppressing texture distortions and accurately reconstructing facial structures. Furthermore, both the SFFT and DAFE modules are empirically validated in enhancing structural fidelity and perceptual quality in face restoration under challenging weather scenarios.

</details>


### [16] [Divergence Minimization Preference Optimization for Diffusion Model Alignment](https://arxiv.org/abs/2507.07510)
*Binxu Li,Minkai Xu,Meihua Dang,Stefano Ermon*

Main category: cs.CV

TL;DR: DMPO是一种通过最小化反向KL散度来对齐扩散模型的新方法，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法存在次优均值寻求问题，需要更有效的对齐方法。

Method: 提出DMPO方法，通过最小化反向KL散度对齐扩散模型。

Result: DMPO在人类评估和自动指标上均优于现有方法，PickScore提升64.6%。

Conclusion: DMPO为扩散模型提供了一种理论严谨且性能优越的偏好对齐途径。

Abstract: Diffusion models have achieved remarkable success in generating realistic and versatile images from text prompts. Inspired by the recent advancements of language models, there is an increasing interest in further improving the models by aligning with human preferences. However, we investigate alignment from a divergence minimization perspective and reveal that existing preference optimization methods are typically trapped in suboptimal mean-seeking optimization. In this paper, we introduce Divergence Minimization Preference Optimization (DMPO), a novel and principled method for aligning diffusion models by minimizing reverse KL divergence, which asymptotically enjoys the same optimization direction as original RL. We provide rigorous analysis to justify the effectiveness of DMPO and conduct comprehensive experiments to validate its empirical strength across both human evaluations and automatic metrics. Our extensive results show that diffusion models fine-tuned with DMPO can consistently outperform or match existing techniques, specifically outperforming all existing diffusion alignment baselines by at least 64.6% in PickScore across all evaluation datasets, demonstrating the method's superiority in aligning generative behavior with desired outputs. Overall, DMPO unlocks a robust and elegant pathway for preference alignment, bridging principled theory with practical performance in diffusion models.

</details>


### [17] [MUVOD: A Novel Multi-view Video Object Segmentation Dataset and A Benchmark for 3D Segmentation](https://arxiv.org/abs/2507.07519)
*Bangning Wei,Joshua Maraval,Meriem Outtas,Kidiyo Kpalma,Nicolas Ramin,Lu Zhang*

Main category: cs.CV

TL;DR: 论文提出了MUVOD数据集，用于动态场景的4D对象分割，填补了多视角视频数据集的空白，并提供了评估指标和基线方法。


<details>
  <summary>Details</summary>
Motivation: 动态场景的4D对象分割研究不足，主要原因是缺乏广泛且准确标注的多视角视频数据集。

Method: 收集了17个场景的多视角视频数据，提供了7830张RGB图像及其分割掩码，并提出评估指标和基线方法。

Result: MUVOD数据集包含459个实例和73个类别，支持跨时间和视角的对象跟踪，并提供了3D对象分割的基准子集。

Conclusion: MUVOD数据集为动态场景分割提供了基础基准，推动了该领域的研究。

Abstract: The application of methods based on Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3D GS) have steadily gained popularity in the field of 3D object segmentation in static scenes. These approaches demonstrate efficacy in a range of 3D scene understanding and editing tasks. Nevertheless, the 4D object segmentation of dynamic scenes remains an underexplored field due to the absence of a sufficiently extensive and accurately labelled multi-view video dataset. In this paper, we present MUVOD, a new multi-view video dataset for training and evaluating object segmentation in reconstructed real-world scenarios. The 17 selected scenes, describing various indoor or outdoor activities, are collected from different sources of datasets originating from various types of camera rigs. Each scene contains a minimum of 9 views and a maximum of 46 views. We provide 7830 RGB images (30 frames per video) with their corresponding segmentation mask in 4D motion, meaning that any object of interest in the scene could be tracked across temporal frames of a given view or across different views belonging to the same camera rig. This dataset, which contains 459 instances of 73 categories, is intended as a basic benchmark for the evaluation of multi-view video segmentation methods. We also present an evaluation metric and a baseline segmentation approach to encourage and evaluate progress in this evolving field. Additionally, we propose a new benchmark for 3D object segmentation task with a subset of annotated multi-view images selected from our MUVOD dataset. This subset contains 50 objects of different conditions in different scenarios, providing a more comprehensive analysis of state-of-the-art 3D object segmentation methods. Our proposed MUVOD dataset is available at https://volumetric-repository.labs.b-com.com/#/muvod.

</details>


### [18] [Diffusion-Guided Knowledge Distillation for Weakly-Supervised Low-Light Semantic Segmentation](https://arxiv.org/abs/2507.07578)
*Chunyan Wang,Dong Zhang,Jinhui Tang*

Main category: cs.CV

TL;DR: 论文提出了一种结合扩散引导知识蒸馏（DGKD）和深度引导特征融合（DGF2）的新框架DGKD-WLSS，用于解决弱监督低光语义分割问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在低光环境下性能显著下降，主要由于图像质量退化（如低对比度、噪声和颜色失真）和弱监督的固有限制，导致不可靠的类激活图和语义模糊的伪标签。

Method: DGKD-WLSS通过扩散引导知识蒸馏（DGKD）对齐正常光和低光特征，同时利用深度引导特征融合（DGF2）整合深度图作为光照不变的几何先验。

Result: 实验表明，DGKD-WLSS在低光条件下的弱监督语义分割任务中达到了最先进的性能。

Conclusion: DGKD-WLSS有效解决了低光环境下弱监督语义分割的挑战，并显著提升了模型性能。

Abstract: Weakly-supervised semantic segmentation aims to assign category labels to each pixel using weak annotations, significantly reducing manual annotation costs. Although existing methods have achieved remarkable progress in well-lit scenarios, their performance significantly degrades in low-light environments due to two fundamental limitations: severe image quality degradation (e.g., low contrast, noise, and color distortion) and the inherent constraints of weak supervision. These factors collectively lead to unreliable class activation maps and semantically ambiguous pseudo-labels, ultimately compromising the model's ability to learn discriminative feature representations. To address these problems, we propose Diffusion-Guided Knowledge Distillation for Weakly-Supervised Low-light Semantic Segmentation (DGKD-WLSS), a novel framework that synergistically combines Diffusion-Guided Knowledge Distillation (DGKD) with Depth-Guided Feature Fusion (DGF2). DGKD aligns normal-light and low-light features via diffusion-based denoising and knowledge distillation, while DGF2 integrates depth maps as illumination-invariant geometric priors to enhance structural feature learning. Extensive experiments demonstrate the effectiveness of DGKD-WLSS, which achieves state-of-the-art performance in weakly supervised semantic segmentation tasks under low-light conditions. The source codes have been released at:https://github.com/ChunyanWang1/DGKD-WLSS.

</details>


### [19] [Stable-Hair v2: Real-World Hair Transfer via Multiple-View Diffusion Model](https://arxiv.org/abs/2507.07591)
*Kuiyuan Sun,Yuxuan Zhang,Jichao Zhang,Jiaming Liu,Wei Wang,Niculae Sebe,Yao Zhao*

Main category: cs.CV

TL;DR: Stable-Hair v2是一种基于扩散的多视角头发转移框架，首次利用多视角扩散模型实现高质量、视角一致的头发转移。


<details>
  <summary>Details</summary>
Motivation: 扩散方法在捕捉多样复杂发型方面表现出色，但在生成一致高质量多视角输出方面仍有不足，这对数字人类和虚拟化身等应用至关重要。

Method: 提出多视角训练数据生成流程，包括扩散基的Bald Converter、数据增强修复模型和面部微调多视角扩散模型。模型整合极坐标嵌入和时序注意力层，采用多阶段训练策略。

Result: 实验表明，该方法能准确转移细节丰富的发型，并在多视角间实现无缝一致效果，显著优于现有方法。

Conclusion: Stable-Hair v2在多视角头发转移领域设定了新基准，代码已开源。

Abstract: While diffusion-based methods have shown impressive capabilities in capturing diverse and complex hairstyles, their ability to generate consistent and high-quality multi-view outputs -- crucial for real-world applications such as digital humans and virtual avatars -- remains underexplored. In this paper, we propose Stable-Hair v2, a novel diffusion-based multi-view hair transfer framework. To the best of our knowledge, this is the first work to leverage multi-view diffusion models for robust, high-fidelity, and view-consistent hair transfer across multiple perspectives. We introduce a comprehensive multi-view training data generation pipeline comprising a diffusion-based Bald Converter, a data-augment inpainting model, and a face-finetuned multi-view diffusion model to generate high-quality triplet data, including bald images, reference hairstyles, and view-aligned source-bald pairs. Our multi-view hair transfer model integrates polar-azimuth embeddings for pose conditioning and temporal attention layers to ensure smooth transitions between views. To optimize this model, we design a novel multi-stage training strategy consisting of pose-controllable latent IdentityNet training, hair extractor training, and temporal attention training. Extensive experiments demonstrate that our method accurately transfers detailed and realistic hairstyles to source subjects while achieving seamless and consistent results across views, significantly outperforming existing methods and establishing a new benchmark in multi-view hair transfer. Code is publicly available at https://github.com/sunkymepro/StableHairV2.

</details>


### [20] [LOSC: LiDAR Open-voc Segmentation Consolidator](https://arxiv.org/abs/2507.07605)
*Nermin Samet,Gilles Puy,Renaud Marlet*

Main category: cs.CV

TL;DR: LOSC方法通过整合图像语义标签并训练3D网络，显著提升了零样本开放词汇语义和全景分割的性能。


<details>
  <summary>Details</summary>
Motivation: 解决激光雷达扫描中图像语义标签噪声大且稀疏的问题。

Method: 整合标签以确保时空一致性，并对图像级增强具有鲁棒性，随后训练3D网络。

Result: 在nuScenes和SemanticKITTI数据集上显著超越现有零样本开放词汇分割方法。

Conclusion: LOSC方法简单有效，显著提升了激光雷达扫描的分割性能。

Abstract: We study the use of image-based Vision-Language Models (VLMs) for open-vocabulary segmentation of lidar scans in driving settings. Classically, image semantics can be back-projected onto 3D point clouds. Yet, resulting point labels are noisy and sparse. We consolidate these labels to enforce both spatio-temporal consistency and robustness to image-level augmentations. We then train a 3D network based on these refined labels. This simple method, called LOSC, outperforms the SOTA of zero-shot open-vocabulary semantic and panoptic segmentation on both nuScenes and SemanticKITTI, with significant margins.

</details>


### [21] [T-GVC: Trajectory-Guided Generative Video Coding at Ultra-Low Bitrates](https://arxiv.org/abs/2507.07633)
*Zhitao Wang,Hengyu Man,Wenrui Li,Xingtao Wang,Xiaopeng Fan,Debin Zhao*

Main category: cs.CV

TL;DR: T-GVC框架通过轨迹引导生成视频编码，解决了现有方法在超低比特率下语义重建的局限性，结合语义感知稀疏运动采样和扩散过程约束，实现了更精确的运动控制和逼真的重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法在超低比特率场景下受限于领域特异性或过度依赖文本引导，导致运动细节丢失和重建不真实。

Method: 提出T-GVC框架，采用语义感知稀疏运动采样管道，结合轨迹对齐损失约束和扩散过程，实现低比特率下的高效视频编码。

Result: 实验表明，T-GVC在超低比特率下优于传统编解码器和端到端视频压缩方法，且运动控制更精确。

Conclusion: T-GVC为基于几何运动建模的生成视频编码开辟了新方向。

Abstract: Recent advances in video generation techniques have given rise to an emerging paradigm of generative video coding, aiming to achieve semantically accurate reconstructions in Ultra-Low Bitrate (ULB) scenarios by leveraging strong generative priors. However, most existing methods are limited by domain specificity (e.g., facial or human videos) or an excessive dependence on high-level text guidance, which often fails to capture motion details and results in unrealistic reconstructions. To address these challenges, we propose a Trajectory-Guided Generative Video Coding framework (dubbed T-GVC). T-GVC employs a semantic-aware sparse motion sampling pipeline to effectively bridge low-level motion tracking with high-level semantic understanding by extracting pixel-wise motion as sparse trajectory points based on their semantic importance, not only significantly reducing the bitrate but also preserving critical temporal semantic information. In addition, by incorporating trajectory-aligned loss constraints into diffusion processes, we introduce a training-free latent space guidance mechanism to ensure physically plausible motion patterns without sacrificing the inherent capabilities of generative models. Experimental results demonstrate that our framework outperforms both traditional codecs and state-of-the-art end-to-end video compression methods under ULB conditions. Furthermore, additional experiments confirm that our approach achieves more precise motion control than existing text-guided methods, paving the way for a novel direction of generative video coding guided by geometric motion modeling.

</details>


### [22] [Motion-Aware Adaptive Pixel Pruning for Efficient Local Motion Deblurring](https://arxiv.org/abs/2507.07708)
*Wei Shang,Dongwei Ren,Wanying Zhang,Pengfei Zhu,Qinghua Hu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 提出了一种新的局部运动模糊去除方法，通过可训练掩码预测器和帧内运动分析器优化计算资源分配和模糊模式处理。


<details>
  <summary>Details</summary>
Motivation: 现有去模糊方法在计算资源分配和空间变化模糊模式处理上效率不足，需改进。

Method: 使用可训练掩码预测器识别模糊区域，并通过结构重参数化优化计算；开发帧内运动分析器生成运动轨迹指导模糊恢复。

Result: 在局部和全局模糊数据集上表现优于现有方法，计算量减少49%。

Conclusion: 该方法高效且性能优越，为局部运动模糊去除提供了新思路。

Abstract: Local motion blur in digital images originates from the relative motion between dynamic objects and static imaging systems during exposure. Existing deblurring methods face significant challenges in addressing this problem due to their inefficient allocation of computational resources and inadequate handling of spatially varying blur patterns. To overcome these limitations, we first propose a trainable mask predictor that identifies blurred regions in the image. During training, we employ blur masks to exclude sharp regions. For inference optimization, we implement structural reparameterization by converting $3\times 3$ convolutions to computationally efficient $1\times 1$ convolutions, enabling pixel-level pruning of sharp areas to reduce computation. Second, we develop an intra-frame motion analyzer that translates relative pixel displacements into motion trajectories, establishing adaptive guidance for region-specific blur restoration. Our method is trained end-to-end using a combination of reconstruction loss, reblur loss, and mask loss guided by annotated blur masks. Extensive experiments demonstrate superior performance over state-of-the-art methods on both local and global blur datasets while reducing FLOPs by 49\% compared to SOTA models (e.g., LMD-ViT). The source code is available at https://github.com/shangwei5/M2AENet.

</details>


### [23] [Benchmarking Content-Based Puzzle Solvers on Corrupted Jigsaw Puzzles](https://arxiv.org/abs/2507.07828)
*Richard Dirauf,Florian Wolz,Dario Zanca,Björn Eskofier*

Main category: cs.CV

TL;DR: 论文研究了内容拼图求解器在缺失、边缘腐蚀和内容腐蚀三种损坏情况下的鲁棒性，发现标准求解器性能下降明显，但深度学习模型通过数据增强可显著提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有内容拼图求解器的评估缺乏对现实挑战（如文物或文件碎片重组）的考量，因此研究其鲁棒性。

Method: 引入三种拼图损坏类型，评估启发式和深度学习求解器的表现，分析其局限性。

Result: 标准求解器性能随损坏增加快速下降，但深度学习模型（如Positional Diffusion）通过微调显著提升鲁棒性。

Conclusion: 研究为提升现实文物自动重建提供了方向，深度学习模型表现优异。

Abstract: Content-based puzzle solvers have been extensively studied, demonstrating significant progress in computational techniques. However, their evaluation often lacks realistic challenges crucial for real-world applications, such as the reassembly of fragmented artefacts or shredded documents. In this work, we investigate the robustness of State-Of-The-Art content-based puzzle solvers introducing three types of jigsaw puzzle corruptions: missing pieces, eroded edges, and eroded contents. Evaluating both heuristic and deep learning-based solvers, we analyse their ability to handle these corruptions and identify key limitations. Our results show that solvers developed for standard puzzles have a rapid decline in performance if more pieces are corrupted. However, deep learning models can significantly improve their robustness through fine-tuning with augmented data. Notably, the advanced Positional Diffusion model adapts particularly well, outperforming its competitors in most experiments. Based on our findings, we highlight promising research directions for enhancing the automated reconstruction of real-world artefacts.

</details>


### [24] [Single-Step Latent Diffusion for Underwater Image Restoration](https://arxiv.org/abs/2507.07878)
*Jiayi Wu,Tianfu Wang,Md Abu Bakr Siddique,Md Jahidul Islam,Cornelia Fermuller,Yiannis Aloimonos,Christopher A. Metzler*

Main category: cs.CV

TL;DR: SLURPP是一种新型水下图像恢复方法，结合预训练潜在扩散模型和显式场景分解，显著提升了复杂场景下的恢复效果和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的水下图像恢复方法在复杂几何和深度变化场景中计算量大且易产生不真实伪影，SLURPP旨在解决这些问题。

Method: SLURPP结合预训练潜在扩散模型和显式场景分解，并利用物理合成数据生成管道训练模型。

Result: SLURPP在合成和真实数据上表现优异，比现有方法快200倍，PSNR提升约3 dB。

Conclusion: SLURPP通过创新架构和合成数据生成，实现了高效且高质量的水下图像恢复。

Abstract: Underwater image restoration algorithms seek to restore the color, contrast, and appearance of a scene that is imaged underwater. They are a critical tool in applications ranging from marine ecology and aquaculture to underwater construction and archaeology. While existing pixel-domain diffusion-based image restoration approaches are effective at restoring simple scenes with limited depth variation, they are computationally intensive and often generate unrealistic artifacts when applied to scenes with complex geometry and significant depth variation. In this work we overcome these limitations by combining a novel network architecture (SLURPP) with an accurate synthetic data generation pipeline. SLURPP combines pretrained latent diffusion models -- which encode strong priors on the geometry and depth of scenes -- with an explicit scene decomposition -- which allows one to model and account for the effects of light attenuation and backscattering. To train SLURPP we design a physics-based underwater image synthesis pipeline that applies varied and realistic underwater degradation effects to existing terrestrial image datasets. This approach enables the generation of diverse training data with dense medium/degradation annotations. We evaluate our method extensively on both synthetic and real-world benchmarks and demonstrate state-of-the-art performance. Notably, SLURPP is over 200X faster than existing diffusion-based methods while offering ~ 3 dB improvement in PSNR on synthetic benchmarks. It also offers compelling qualitative improvements on real-world data. Project website https://tianfwang.github.io/slurpp/.

</details>


### [25] [Not Only Consistency: Enhance Test-Time Adaptation with Spatio-temporal Inconsistency for Remote Physiological Measurement](https://arxiv.org/abs/2507.07908)
*Xiao Yang,Yuxuan Fan,Can Liu,Houcheng Su,Weichen Guo,Jiyao Wang,Dengbo He*

Main category: cs.CV

TL;DR: 提出了一种名为CiCi的新型测试时间自适应（TTA）策略，用于远程光电容积描记（rPPG）任务，结合生理学先验知识，利用频率域的一致性和时间域的不一致性，提升模型在推理时的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有深度rPPG模型在未见过部署环境中的适应性受限，隐私问题和实时适应性需求限制了其实际应用。

Method: 基于生理学先验知识，提出CiCi框架，结合频率域一致性和时间域不一致性，并引入梯度动态控制机制以稳定适应。

Result: 在五个数据集上的实验表明，该方法在实时自监督适应中优于现有技术，无需访问源数据。

Conclusion: CiCi框架在rPPG任务中表现出色，为实时自监督适应提供了新思路。

Abstract: Remote photoplethysmography (rPPG) has emerged as a promising non-invasive method for monitoring physiological signals using the camera. Although various domain adaptation and generalization methods were proposed to promote the adaptability of deep-based rPPG models in unseen deployment environments, considerations in aspects like privacy concerns and real-time adaptation restrict their application in real-world deployment. Thus, we aim to propose a novel fully Test-Time Adaptation (TTA) strategy tailored for rPPG tasks in this work. Specifically, based on prior knowledge in physiology and our observations, we noticed not only there is spatio-temporal consistency in the frequency domain of rPPG signals, but also that inconsistency in the time domain was significant. Given this, by leveraging both consistency and inconsistency priors, we introduce an innovative expert knowledge-based self-supervised \textbf{C}onsistency-\textbf{i}n\textbf{C}onsistency-\textbf{i}ntegration (\textbf{CiCi}) framework to enhances model adaptation during inference. Besides, our approach further incorporates a gradient dynamic control mechanism to mitigate potential conflicts between priors, ensuring stable adaptation across instances. Through extensive experiments on five diverse datasets under the TTA protocol, our method consistently outperforms existing techniques, presenting state-of-the-art performance in real-time self-supervised adaptation without accessing source data. The code will be released later.

</details>


### [26] [Scaling RL to Long Videos](https://arxiv.org/abs/2507.07966)
*Yukang Chen,Wei Huang,Baifeng Shi,Qinghao Hu,Hanrong Ye,Ligeng Zhu,Zhijian Liu,Pavlo Molchanov,Jan Kautz,Xiaojuan Qi,Sifei Liu,Hongxu Yin,Yao Lu,Song Han*

Main category: cs.CV

TL;DR: 提出一个全栈框架，通过强化学习扩展视觉语言模型（VLM）在长视频中的推理能力，包含大规模数据集、两阶段训练管道和高效训练基础设施。


<details>
  <summary>Details</summary>
Motivation: 解决长视频推理的独特挑战，提升视觉语言模型在长视频中的表现。

Method: 整合大规模数据集LongVideo-Reason、两阶段训练管道（CoT-SFT和强化学习）以及高效训练基础设施MR-SP。

Result: LongVILA-R1-7B在长视频QA基准测试中表现优异，性能超越Video-R1-7B，匹配Gemini-1.5-Pro，且MR-SP系统训练速度提升2.1倍。

Conclusion: LongVILA-R1为长视频推理迈出重要一步，并公开支持多模态和多模型训练的系统。

Abstract: We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 52K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves strong performance on long video QA benchmarks such as VideoMME. It also outperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal reasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on our LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent performance gains as the number of input video frames scales. LongVILA-R1 marks a firm step towards long video reasoning in VLMs. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames / around 256k tokens).

</details>


### [27] [Martian World Models: Controllable Video Synthesis with Physically Accurate 3D Reconstructions](https://arxiv.org/abs/2507.07978)
*Longfei Li,Zhiwen Fan,Wenyan Cong,Xinhang Liu,Yuyang Yin,Matt Foutter,Panwang Pan,Chenyu You,Yue Wang,Zhangyang Wang,Yao Zhao,Marco Pavone,Yunchao Wei*

Main category: cs.CV

TL;DR: 提出了一种合成火星景观视频的方法，包括数据重建和视频生成两部分，解决了火星数据稀缺和领域差异问题。


<details>
  <summary>Details</summary>
Motivation: 为任务演练和机器人模拟提供逼真的火星景观视频，但火星数据稀缺且与地球图像差异大。

Method: 1) M3arsSynth：从NASA立体图像重建3D火星环境并渲染视频；2) MarsGen：基于3D结构生成逼真视频。

Result: 方法在视觉逼真度和3D结构一致性上优于基于地球数据训练的模型。

Conclusion: 提出的方法能有效生成高质量火星视频，适用于任务模拟和机器人训练。

Abstract: Synthesizing realistic Martian landscape videos is crucial for mission rehearsal and robotic simulation. However, this task poses unique challenges due to the scarcity of high-quality Martian data and the significant domain gap between Martian and terrestrial imagery. To address these challenges, we propose a holistic solution composed of two key components: 1) A data curation pipeline Multimodal Mars Synthesis (M3arsSynth), which reconstructs 3D Martian environments from real stereo navigation images, sourced from NASA's Planetary Data System (PDS), and renders high-fidelity multiview 3D video sequences. 2) A Martian terrain video generator, MarsGen, which synthesizes novel videos visually realistic and geometrically consistent with the 3D structure encoded in the data. Our M3arsSynth engine spans a wide range of Martian terrains and acquisition dates, enabling the generation of physically accurate 3D surface models at metric-scale resolution. MarsGen, fine-tuned on M3arsSynth data, synthesizes videos conditioned on an initial image frame and, optionally, camera trajectories or textual prompts, allowing for video generation in novel environments. Experimental results show that our approach outperforms video synthesis models trained on terrestrial datasets, achieving superior visual fidelity and 3D structural consistency.

</details>


### [28] [Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling](https://arxiv.org/abs/2507.07982)
*Haoyu Wu,Diankun Wu,Tianyu He,Junliang Guo,Yang Ye,Yueqi Duan,Jiang Bian*

Main category: cs.CV

TL;DR: 论文提出了一种名为Geometry Forcing的方法，通过几何对齐目标提升视频扩散模型对3D结构的捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型仅基于原始视频数据训练时，往往无法捕捉有意义的几何感知结构。本文旨在弥合视频扩散模型与物理世界3D本质之间的差距。

Method: 提出Geometry Forcing方法，通过Angular Alignment和Scale Alignment两个几何对齐目标，引导模型学习几何感知的潜在表示。

Result: 实验表明，该方法在相机视角和动作条件视频生成任务中显著提升了视觉质量和3D一致性。

Conclusion: Geometry Forcing是一种简单有效的方法，能够显著提升视频扩散模型的几何感知能力。

Abstract: Videos inherently represent 2D projections of a dynamic 3D world. However, our analysis suggests that video diffusion models trained solely on raw video data often fail to capture meaningful geometric-aware structure in their learned representations. To bridge this gap between video diffusion models and the underlying 3D nature of the physical world, we propose Geometry Forcing, a simple yet effective method that encourages video diffusion models to internalize latent 3D representations. Our key insight is to guide the model's intermediate representations toward geometry-aware structure by aligning them with features from a pretrained geometric foundation model. To this end, we introduce two complementary alignment objectives: Angular Alignment, which enforces directional consistency via cosine similarity, and Scale Alignment, which preserves scale-related information by regressing unnormalized geometric features from normalized diffusion representation. We evaluate Geometry Forcing on both camera view-conditioned and action-conditioned video generation tasks. Experimental results demonstrate that our method substantially improves visual quality and 3D consistency over the baseline methods. Project page: https://GeometryForcing.github.io.

</details>


### [29] [MGVQ: Could VQ-VAE Beat VAE? A Generalizable Tokenizer with Multi-group Quantization](https://arxiv.org/abs/2507.07997)
*Mingkai Jia,Wei Yin,Xiaotao Hu,Jiaxin Guo,Xiaoyang Guo,Qian Zhang,Xiao-Xiao Long,Ping Tan*

Main category: cs.CV

TL;DR: 论文提出了一种名为\NickName的新方法，通过增强离散码本的表示能力，优化量化策略，显著提升了VQ-VAEs的重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有VQ-VAEs与VAEs在重建质量上存在较大差距，需改进量化策略以缩小差距。

Method: 保留潜在维度以保留编码特征，并引入一组子码本进行量化。

Result: \NickName在ImageNet和8个零样本基准测试中均达到最优性能，显著优于SD-VAE。

Conclusion: \NickName在重建任务中表现卓越，为高清图像处理任务提供了新思路。

Abstract: Vector Quantized Variational Autoencoders (VQ-VAEs) are fundamental models that compress continuous visual data into discrete tokens. Existing methods have tried to improve the quantization strategy for better reconstruction quality, however, there still exists a large gap between VQ-VAEs and VAEs. To narrow this gap, we propose \NickName, a novel method to augment the representation capability of discrete codebooks, facilitating easier optimization for codebooks and minimizing information loss, thereby enhancing reconstruction quality. Specifically, we propose to retain the latent dimension to preserve encoded features and incorporate a set of sub-codebooks for quantization. Furthermore, we construct comprehensive zero-shot benchmarks featuring resolutions of 512p and 2k to evaluate the reconstruction performance of existing methods rigorously. \NickName~achieves the \textbf{state-of-the-art performance on both ImageNet and $8$ zero-shot benchmarks} across all VQ-VAEs. Notably, compared with SD-VAE, we outperform them on ImageNet significantly, with rFID $\textbf{0.49}$ v.s. $\textbf{0.91}$, and achieve superior PSNR on all zero-shot benchmarks. These results highlight the superiority of \NickName~in reconstruction and pave the way for preserving fidelity in HD image processing tasks. Code will be publicly available at https://github.com/MKJia/MGVQ.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [30] [D-CNN and VQ-VAE Autoencoders for Compression and Denoising of Industrial X-ray Computed Tomography Images](https://arxiv.org/abs/2507.07704)
*Bardia Hejazi,Keerthana Chand,Tobias Fritsch,Giovanni Bruno*

Main category: eess.IV

TL;DR: 研究探讨了使用深度学习自动编码器压缩工业X射线计算机断层扫描（XCT）数据，并分析了不同压缩算法对数据恢复质量的影响。


<details>
  <summary>Details</summary>
Motivation: 随着成像技术的进步，成像科学中的数据量急剧增长，需要高效可靠的存储解决方案。

Method: 采用两种网络架构（D-CNN和VQ-VAE）以不同压缩率对XCT数据进行压缩，并引入对边缘保护敏感的指标来评估解码图像质量。

Result: 结果表明，不同架构和压缩率适用于需要保留特定特征的数据分析。

Conclusion: 研究结果为科学家确定数据存储和分析需求提供了策略指导。

Abstract: The ever-growing volume of data in imaging sciences stemming from the advancements in imaging technologies, necessitates efficient and reliable storage solutions for such large datasets. This study investigates the compression of industrial X-ray computed tomography (XCT) data using deep learning autoencoders and examines how these compression algorithms affect the quality of the recovered data. Two network architectures with different compression rates were used, a deep convolution neural network (D-CNN) and a vector quantized variational autoencoder (VQ-VAE). The XCT data used was from a sandstone sample with a complex internal pore network. The quality of the decoded images obtained from the two different deep learning architectures with different compression rates were quantified and compared to the original input data. In addition, to improve image decoding quality metrics, we introduced a metric sensitive to edge preservation, which is crucial for three-dimensional data analysis. We showed that different architectures and compression rates are required depending on the specific characteristics needed to be preserved for later analysis. The findings presented here can aid scientists to determine the requirements and strategies for their data storage and analysis needs.

</details>
