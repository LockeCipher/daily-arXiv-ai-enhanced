<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 19]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Inference Time Debiasing Concepts in Diffusion Models](https://arxiv.org/abs/2508.14933)
*Lucas S. Kupssinskü,Marco N. Bochernitsan,Jordan Kopper,Otávio Parraga,Rodrigo C. Barros*

Main category: cs.GR

TL;DR: DeCoDi是一种针对文本到图像扩散模型的去偏方法，通过改变推理过程来避免潜在维度中的偏见概念区域，无需复杂干预或显著计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习去偏方法通常需要复杂或计算密集的干预，而DeCoDi旨在仅改变推理过程，使其更易于广泛实践者使用。

Method: 通过修改扩散过程来避免潜在维度中的偏见概念区域，该方法不改变图像质量，计算开销可忽略，适用于任何基于扩散的图像生成模型。

Result: 在护士、消防员和CEO等概念上成功去除了性别、种族和年龄偏见。人工评估1200张图像显示方法有效，GPT4o自动评估与人工评估结果无显著统计差异。

Conclusion: DeCoDi方法能显著提高扩散基文本到图像生成模型生成图像的多样性，评估结果显示评估者间一致性可靠，对受保护属性的覆盖更全面。

Abstract: We propose DeCoDi, a debiasing procedure for text-to-image diffusion-based models that changes the inference procedure, does not significantly change image quality, has negligible compute overhead, and can be applied in any diffusion-based image generation model. DeCoDi changes the diffusion process to avoid latent dimension regions of biased concepts. While most deep learning debiasing methods require complex or compute-intensive interventions, our method is designed to change only the inference procedure. Therefore, it is more accessible to a wide range of practitioners. We show the effectiveness of the method by debiasing for gender, ethnicity, and age for the concepts of nurse, firefighter, and CEO. Two distinct human evaluators manually inspect 1,200 generated images. Their evaluation results provide evidence that our method is effective in mitigating biases based on gender, ethnicity, and age. We also show that an automatic bias evaluation performed by the GPT4o is not significantly statistically distinct from a human evaluation. Our evaluation shows promising results, with reliable levels of agreement between evaluators and more coverage of protected attributes. Our method has the potential to significantly improve the diversity of images it generates by diffusion-based text-to-image generative models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [Image-Conditioned 3D Gaussian Splat Quantization](https://arxiv.org/abs/2508.15372)
*Xinshuang Liu,Runfa Blark Li,Keito Suzuki,Truong Nguyen*

Main category: cs.CV

TL;DR: 提出ICGS-Quantizer方法，通过联合利用高斯间和属性间相关性，使用跨场景共享码本，将3DGS压缩到千字节级别，并支持基于图像的场景更新。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS压缩方法只能将中等规模场景压缩到兆字节级别，不适用于大规模场景或场景集合，且缺乏对存档后场景变化的适应机制。

Method: 使用图像条件化高斯溅射量化器，联合利用高斯间和属性间相关性，采用跨训练场景的共享码本，编码、量化和解码过程联合训练，支持基于捕获图像的场景解码更新。

Result: 在3D场景压缩和场景更新任务上始终优于最先进方法，将3DGS存储需求降低到千字节范围同时保持视觉保真度。

Conclusion: ICGS-Quantizer显著提升了压缩效率，提供了对存档后场景变化的适应能力，解决了现有方法的两个主要限制。

Abstract: 3D Gaussian Splatting (3DGS) has attracted considerable attention for enabling high-quality real-time rendering. Although 3DGS compression methods have been proposed for deployment on storage-constrained devices, two limitations hinder archival use: (1) they compress medium-scale scenes only to the megabyte range, which remains impractical for large-scale scenes or extensive scene collections; and (2) they lack mechanisms to accommodate scene changes after long-term archival. To address these limitations, we propose an Image-Conditioned Gaussian Splat Quantizer (ICGS-Quantizer) that substantially enhances compression efficiency and provides adaptability to scene changes after archiving. ICGS-Quantizer improves quantization efficiency by jointly exploiting inter-Gaussian and inter-attribute correlations and by using shared codebooks across all training scenes, which are then fixed and applied to previously unseen test scenes, eliminating the overhead of per-scene codebooks. This approach effectively reduces the storage requirements for 3DGS to the kilobyte range while preserving visual fidelity. To enable adaptability to post-archival scene changes, ICGS-Quantizer conditions scene decoding on images captured at decoding time. The encoding, quantization, and decoding processes are trained jointly, ensuring that the codes, which are quantized representations of the scene, are effective for conditional decoding. We evaluate ICGS-Quantizer on 3D scene compression and 3D scene updating. Experimental results show that ICGS-Quantizer consistently outperforms state-of-the-art methods in compression efficiency and adaptability to scene changes. Our code, model, and data will be publicly available on GitHub.

</details>


### [3] [Scaling Group Inference for Diverse and High-Quality Generation](https://arxiv.org/abs/2508.15773)
*Gaurav Parmar,Or Patashnik,Daniil Ostashev,Kuan-Chieh Wang,Kfir Aberman,Srinivasa Narasimhan,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: 提出了一种可扩展的组推理方法，通过将组推理建模为二次整数分配问题，在提升样本质量的同时最大化组多样性，解决了独立采样导致的冗余问题。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，用户通常需要获得一组多样化的输出样本（如4-8张图像），但传统的独立采样方法往往产生冗余结果，限制了用户选择和创意探索。

Method: 将组推理建模为二次整数分配问题：候选输出作为图节点，通过优化样本质量（一元项）和最大化组多样性（二元项）来选择最优子集。采用渐进式剪枝策略提高运行效率，可扩展到大规模候选集。

Result: 大量实验表明，该方法相比独立采样基线和最新推理算法，显著提升了组多样性和质量。框架可泛化到文本到图像、图像到图像、图像提示和视频生成等多种任务。

Conclusion: 该方法使生成模型能够将多个输出视为有凝聚力的组而非独立样本，为实际应用提供了更优质的多样化输出选择。

Abstract: Generative models typically sample outputs independently, and recent inference-time guidance and scaling algorithms focus on improving the quality of individual samples. However, in real-world applications, users are often presented with a set of multiple images (e.g., 4-8) for each prompt, where independent sampling tends to lead to redundant results, limiting user choices and hindering idea exploration. In this work, we introduce a scalable group inference method that improves both the diversity and quality of a group of samples. We formulate group inference as a quadratic integer assignment problem: candidate outputs are modeled as graph nodes, and a subset is selected to optimize sample quality (unary term) while maximizing group diversity (binary term). To substantially improve runtime efficiency, we progressively prune the candidate set using intermediate predictions, allowing our method to scale up to large candidate sets. Extensive experiments show that our method significantly improves group diversity and quality compared to independent sampling baselines and recent inference algorithms. Our framework generalizes across a wide range of tasks, including text-to-image, image-to-image, image prompting, and video generation, enabling generative models to treat multiple outputs as cohesive groups rather than independent samples.

</details>


### [4] [TAIGen: Training-Free Adversarial Image Generation via Diffusion Models](https://arxiv.org/abs/2508.15020)
*Susim Roy,Anubhooti Jain,Mayank Vatsa,Richa Singh*

Main category: cs.CV

TL;DR: TAIGen是一种无需训练的黑盒对抗图像生成方法，通过仅需3-20个采样步骤从无条件扩散模型中高效生成高质量对抗样本，比现有方法快10倍


<details>
  <summary>Details</summary>
Motivation: 现有生成模型的对抗攻击通常产生低质量图像且计算资源需求大，扩散模型虽然能生成高质量图像但需要数百个采样步骤，因此需要开发更高效的对抗生成方法

Method: 在混合步骤区间注入扰动，采用选择性RGB通道策略：对红色通道应用注意力图，对绿色和蓝色通道使用GradCAM引导的扰动，保持图像结构的同时最大化目标模型的误分类

Result: 在ImageNet数据集上，TAIGen对VGGNet作为源模型时，对ResNet成功率为70.6%，对MNASNet为80.8%，对ShuffleNet为97.8%。PSNR超过30dB，生成速度快10倍

Conclusion: TAIGen是最具影响力的攻击方法，防御机制对其生成的图像净化效果最差，实现了最低的鲁棒准确率

Abstract: Adversarial attacks from generative models often produce low-quality images and require substantial computational resources. Diffusion models, though capable of high-quality generation, typically need hundreds of sampling steps for adversarial generation. This paper introduces TAIGen, a training-free black-box method for efficient adversarial image generation. TAIGen produces adversarial examples using only 3-20 sampling steps from unconditional diffusion models. Our key finding is that perturbations injected during the mixing step interval achieve comparable attack effectiveness without processing all timesteps. We develop a selective RGB channel strategy that applies attention maps to the red channel while using GradCAM-guided perturbations on green and blue channels. This design preserves image structure while maximizing misclassification in target models. TAIGen maintains visual quality with PSNR above 30 dB across all tested datasets. On ImageNet with VGGNet as source, TAIGen achieves 70.6% success against ResNet, 80.8% against MNASNet, and 97.8% against ShuffleNet. The method generates adversarial examples 10x faster than existing diffusion-based attacks. Our method achieves the lowest robust accuracy, indicating it is the most impactful attack as the defense mechanism is least successful in purifying the images generated by TAIGen.

</details>


### [5] [Reversible Unfolding Network for Concealed Visual Perception with Generative Refinement](https://arxiv.org/abs/2508.15027)
*Chunming He,Fengyang Xiao,Rihan Zhang,Chengyu Fang,Deng-Ping Fan,Sina Farsiu*

Main category: cs.CV

TL;DR: RUN++是一个可逆展开网络，通过生成式精炼解决隐蔽视觉感知问题，在mask和RGB双域应用可逆建模，并引入针对性扩散模型来优化不确定区域


<details>
  <summary>Details</summary>
Motivation: 现有CVP方法主要局限于mask域，未能充分利用RGB域的潜力，且在处理不确定性方面存在局限

Method: 将CVP任务建模为数学优化问题并展开为多阶段深度网络，包含CORE（mask域可逆建模）、CARE（RGB域可逆建模）和FINE（针对性扩散精炼）三个模块

Result: 提出的方法能够有效减少误报和漏报，在真实世界退化条件下保持鲁棒性

Conclusion: RUN++通过可逆展开网络与生成式精炼的独特协同，为CVP任务提供了新的解决方案，并建立了更鲁棒的双层优化框架

Abstract: Existing methods for concealed visual perception (CVP) often leverage reversible strategies to decrease uncertainty, yet these are typically confined to the mask domain, leaving the potential of the RGB domain underexplored. To address this, we propose a reversible unfolding network with generative refinement, termed RUN++. Specifically, RUN++ first formulates the CVP task as a mathematical optimization problem and unfolds the iterative solution into a multi-stage deep network. This approach provides a principled way to apply reversible modeling across both mask and RGB domains while leveraging a diffusion model to resolve the resulting uncertainty. Each stage of the network integrates three purpose-driven modules: a Concealed Object Region Extraction (CORE) module applies reversible modeling to the mask domain to identify core object regions; a Context-Aware Region Enhancement (CARE) module extends this principle to the RGB domain to foster better foreground-background separation; and a Finetuning Iteration via Noise-based Enhancement (FINE) module provides a final refinement. The FINE module introduces a targeted Bernoulli diffusion model that refines only the uncertain regions of the segmentation mask, harnessing the generative power of diffusion for fine-detail restoration without the prohibitive computational cost of a full-image process. This unique synergy, where the unfolding network provides a strong uncertainty prior for the diffusion model, allows RUN++ to efficiently direct its focus toward ambiguous areas, significantly mitigating false positives and negatives. Furthermore, we introduce a new paradigm for building robust CVP systems that remain effective under real-world degradations and extend this concept into a broader bi-level optimization framework.

</details>


### [6] [CurveFlow: Curvature-Guided Flow Matching for Image Generation](https://arxiv.org/abs/2508.15093)
*Yan Luo,Drake Du,Hao Huang,Yi Fang,Mengyu Wang*

Main category: cs.CV

TL;DR: CurveFlow通过引入曲率引导的非线性轨迹来改进文本到图像生成，显著提升语义一致性和图像质量


<details>
  <summary>Details</summary>
Motivation: 现有整流流模型使用线性轨迹，可能迫使生成过程经过数据流形的低概率区域，影响语义对齐和指令遵循能力

Method: 提出CurveFlow框架，通过曲率正则化技术学习平滑非线性轨迹，惩罚轨迹内在动力学的突变

Result: 在MS COCO数据集上达到SOTA性能，在BLEU、METEOR、ROUGE和CLAIR等语义一致性指标上显著优于标准整流流变体和其他非线性基线

Conclusion: 曲率感知建模能显著增强模型忠实遵循复杂指令的能力，同时保持高图像质量

Abstract: Existing rectified flow models are based on linear trajectories between data and noise distributions. This linearity enforces zero curvature, which can inadvertently force the image generation process through low-probability regions of the data manifold. A key question remains underexplored: how does the curvature of these trajectories correlate with the semantic alignment between generated images and their corresponding captions, i.e., instructional compliance? To address this, we introduce CurveFlow, a novel flow matching framework designed to learn smooth, non-linear trajectories by directly incorporating curvature guidance into the flow path. Our method features a robust curvature regularization technique that penalizes abrupt changes in the trajectory's intrinsic dynamics.Extensive experiments on MS COCO 2014 and 2017 demonstrate that CurveFlow achieves state-of-the-art performance in text-to-image generation, significantly outperforming both standard rectified flow variants and other non-linear baselines like Rectified Diffusion. The improvements are especially evident in semantic consistency metrics such as BLEU, METEOR, ROUGE, and CLAIR. This confirms that our curvature-aware modeling substantially enhances the model's ability to faithfully follow complex instructions while simultaneously maintaining high image quality. The code is made publicly available at https://github.com/Harvard-AI-and-Robotics-Lab/CurveFlow.

</details>


### [7] [XDR-LVLM: An Explainable Vision-Language Large Model for Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2508.15168)
*Masato Ito,Kaito Tanaka,Keisuke Matsuda,Aya Nakayama*

Main category: cs.CV

TL;DR: XDR-LVLM是一个基于视觉语言大模型的可解释性糖尿病视网膜病变诊断框架，通过自然语言解释实现高精度诊断和透明化报告生成


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在糖尿病视网膜病变诊断中的黑盒问题，缺乏透明度和可解释性阻碍了临床采用

Method: 整合专用医学视觉编码器和LVLM核心，采用多任务提示工程和多阶段微调，从眼底图像理解病理特征并生成包含DR分级、关键病理概念识别和详细解释的诊断报告

Result: 在DDR数据集上达到SOTA性能：疾病诊断平衡准确率84.55%、F1分数79.92%；概念检测平衡准确率77.95%、F1分数66.88%；人类评估确认生成解释的高流畅性、准确性和临床实用性

Conclusion: XDR-LVLM通过提供强大且可解释的见解，成功弥合了自动化诊断与临床需求之间的差距

Abstract: Diabetic Retinopathy (DR) is a major cause of global blindness, necessitating early and accurate diagnosis. While deep learning models have shown promise in DR detection, their black-box nature often hinders clinical adoption due to a lack of transparency and interpretability. To address this, we propose XDR-LVLM (eXplainable Diabetic Retinopathy Diagnosis with LVLM), a novel framework that leverages Vision-Language Large Models (LVLMs) for high-precision DR diagnosis coupled with natural language-based explanations. XDR-LVLM integrates a specialized Medical Vision Encoder, an LVLM Core, and employs Multi-task Prompt Engineering and Multi-stage Fine-tuning to deeply understand pathological features within fundus images and generate comprehensive diagnostic reports. These reports explicitly include DR severity grading, identification of key pathological concepts (e.g., hemorrhages, exudates, microaneurysms), and detailed explanations linking observed features to the diagnosis. Extensive experiments on the Diabetic Retinopathy (DDR) dataset demonstrate that XDR-LVLM achieves state-of-the-art performance, with a Balanced Accuracy of 84.55% and an F1 Score of 79.92% for disease diagnosis, and superior results for concept detection (77.95% BACC, 66.88% F1). Furthermore, human evaluations confirm the high fluency, accuracy, and clinical utility of the generated explanations, showcasing XDR-LVLM's ability to bridge the gap between automated diagnosis and clinical needs by providing robust and interpretable insights.

</details>


### [8] [MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View Consistent Diffusion](https://arxiv.org/abs/2508.15169)
*Xuyang Chen,Zhijun Zhai,Kaixuan Zhou,Zengmao Wang,Jianan He,Dong Wang,Yanfeng Zhang,mingwei Sun,Rüdiger Westermann,Konrad Schindler,Liqiu Meng*

Main category: cs.CV

TL;DR: 提出了MeSS方法，使用城市网格模型作为几何先验，通过改进图像扩散模型来生成高质量、风格一致的室外3D场景，解决了现有方法在几何对齐和跨视图一致性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 城市网格模型缺乏真实纹理，限制了在虚拟城市导航和自动驾驶中的应用。现有图像和视频扩散模型在3D场景生成方面存在局限性：视频模型难以遵循预定相机路径，图像模型缺乏跨视图一致性。

Method: 三阶段流水线：1) 使用级联外绘ControlNet生成几何一致的稀疏视图；2) 通过AGInpaint组件传播更密集的中间视图；3) 使用GCAlign模块全局消除视觉不一致性。同时基于网格表面初始化3D高斯溅射场景重建。

Result: 方法在几何对齐和生成质量方面优于现有方法，生成的场景可通过重照明和风格转换技术以不同风格渲染。

Conclusion: MeSS成功解决了城市网格模型的纹理生成问题，为虚拟城市导航和自动驾驶提供了高质量的3D场景合成解决方案。

Abstract: Mesh models have become increasingly accessible for numerous cities; however, the lack of realistic textures restricts their application in virtual urban navigation and autonomous driving. To address this, this paper proposes MeSS (Meshbased Scene Synthesis) for generating high-quality, styleconsistent outdoor scenes with city mesh models serving as the geometric prior. While image and video diffusion models can leverage spatial layouts (such as depth maps or HD maps) as control conditions to generate street-level perspective views, they are not directly applicable to 3D scene generation. Video diffusion models excel at synthesizing consistent view sequences that depict scenes but often struggle to adhere to predefined camera paths or align accurately with rendered control videos. In contrast, image diffusion models, though unable to guarantee cross-view visual consistency, can produce more geometry-aligned results when combined with ControlNet. Building on this insight, our approach enhances image diffusion models by improving cross-view consistency. The pipeline comprises three key stages: first, we generate geometrically consistent sparse views using Cascaded Outpainting ControlNets; second, we propagate denser intermediate views via a component dubbed AGInpaint; and third, we globally eliminate visual inconsistencies (e.g., varying exposure) using the GCAlign module. Concurrently with generation, a 3D Gaussian Splatting (3DGS) scene is reconstructed by initializing Gaussian balls on the mesh surface. Our method outperforms existing approaches in both geometric alignment and generation quality. Once synthesized, the scene can be rendered in diverse styles through relighting and style transfer techniques.

</details>


### [9] [Collaborative Multi-Modal Coding for High-Quality 3D Generation](https://arxiv.org/abs/2508.15228)
*Ziang Cao,Zhaoxi Chen,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: TriMM是首个前馈式3D原生生成模型，通过多模态（RGB、RGBD、点云）协同编码和triplane潜在扩散模型，有效利用多模态数据进行高质量3D资产生成。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成模型大多局限于单一模态或3D结构，未能充分利用多模态数据的互补优势，限制了训练数据集的范围和模型性能。

Method: 1）引入协作多模态编码整合模态特定特征；2）采用辅助2D和3D监督提升鲁棒性；3）基于triplane潜在扩散模型生成高质量3D资产

Result: 在多个知名数据集上的实验表明，TriMM通过有效利用多模态，在使用少量训练数据的情况下实现了与大规模数据集训练模型相竞争的性能

Conclusion: TriMM证明了多模态数据在3D生成中的有效性，并为整合其他多模态数据集提供了可行性

Abstract: 3D content inherently encompasses multi-modal characteristics and can be projected into different modalities (e.g., RGB images, RGBD, and point clouds). Each modality exhibits distinct advantages in 3D asset modeling: RGB images contain vivid 3D textures, whereas point clouds define fine-grained 3D geometries. However, most existing 3D-native generative architectures either operate predominantly within single-modality paradigms-thus overlooking the complementary benefits of multi-modality data-or restrict themselves to 3D structures, thereby limiting the scope of available training datasets. To holistically harness multi-modalities for 3D modeling, we present TriMM, the first feed-forward 3D-native generative model that learns from basic multi-modalities (e.g., RGB, RGBD, and point cloud). Specifically, 1) TriMM first introduces collaborative multi-modal coding, which integrates modality-specific features while preserving their unique representational strengths. 2) Furthermore, auxiliary 2D and 3D supervision are introduced to raise the robustness and performance of multi-modal coding. 3) Based on the embedded multi-modal code, TriMM employs a triplane latent diffusion model to generate 3D assets of superior quality, enhancing both the texture and the geometric detail. Extensive experiments on multiple well-known datasets demonstrate that TriMM, by effectively leveraging multi-modality, achieves competitive performance with models trained on large-scale datasets, despite utilizing a small amount of training data. Furthermore, we conduct additional experiments on recent RGB-D datasets, verifying the feasibility of incorporating other multi-modal datasets into 3D generation.

</details>


### [10] [Pretrained Diffusion Models Are Inherently Skipped-Step Samplers](https://arxiv.org/abs/2508.15233)
*Wenju Xu*

Main category: cs.CV

TL;DR: 本文提出了一种跳过步采样机制，可以直接绕过多个中间去噪步骤，证明马尔可夫扩散过程本身具有加速采样的内在属性，无需依赖非马尔可夫过程。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然在各生成任务中表现优异，但其顺序生成过程需要多步迭代，效率较低。现有方法如DDIM通过非马尔可夫过程减少采样步数，但缺乏对原始马尔可夫扩散过程是否具有相同效率的理解。

Method: 提出跳过步采样机制，直接从噪声输入预测多个步骤后的结果，绕过传统逐步细化过程。该方法与标准扩散模型具有相同的训练目标，证明加速采样是预训练扩散模型的内在属性。

Result: 在OpenAI ADM、Stable Diffusion和Open Sora等流行预训练扩散模型上的大量实验表明，该方法能够以显著减少的采样步骤实现高质量生成。

Conclusion: 跳过步采样是扩散模型的固有特性，可以与DDIM等现有方法结合进一步加速生成，为扩散模型的高效采样提供了新的理论依据和实践方法。

Abstract: Diffusion models have been achieving state-of-the-art results across various generation tasks. However, a notable drawback is their sequential generation process, requiring long-sequence step-by-step generation. Existing methods, such as DDIM, attempt to reduce sampling steps by constructing a class of non-Markovian diffusion processes that maintain the same training objective. However, there remains a gap in understanding whether the original diffusion process can achieve the same efficiency without resorting to non-Markovian processes. In this paper, we provide a confirmative answer and introduce skipped-step sampling, a mechanism that bypasses multiple intermediate denoising steps in the iterative generation process, in contrast with the traditional step-by-step refinement of standard diffusion inference. Crucially, we demonstrate that this skipped-step sampling mechanism is derived from the same training objective as the standard diffusion model, indicating that accelerated sampling via skipped-step sampling via a Markovian way is an intrinsic property of pretrained diffusion models. Additionally, we propose an enhanced generation method by integrating our accelerated sampling technique with DDIM. Extensive experiments on popular pretrained diffusion models, including the OpenAI ADM, Stable Diffusion, and Open Sora models, show that our method achieves high-quality generation with significantly reduced sampling steps.

</details>


### [11] [TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect Classification](https://arxiv.org/abs/2508.15298)
*Darya Taratynova,Alya Almsouti,Beknur Kalmakhanbet,Numan Saeed,Mohammad Yaqub*

Main category: cs.CV

TL;DR: TPA是一个结合时序建模、提示感知对比学习和不确定性量化的胎儿先天性心脏病超声视频分类框架，在CHD检测和心脏功能评估任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前超声视频CHD检测方法存在图像噪声、探头位置变异、忽略时序信息、仅限于二分类且缺乏预测校准等问题，需要开发更可靠的自动化诊断方法。

Method: 提出时序提示对齐(TPA)方法：使用图像编码器提取视频帧特征，可训练时序提取器聚合心脏运动信息，通过边界铰链对比损失对齐视频表示与类别特定文本提示；引入条件变分自编码器风格调制模块学习潜在风格向量来调制嵌入并量化分类不确定性。

Result: 在CHD检测上达到85.40%的宏F1分数，预期校准误差降低5.38%，自适应ECE降低6.8%；在EchoNet-Dynamic三分类任务上宏F1提升4.73%(从53.89%到58.62%)。

Conclusion: TPA框架通过整合时序建模、提示感知对比学习和不确定性量化，显著提升了胎儿先天性心脏病超声视频分类的准确性和临床可靠性。

Abstract: Congenital heart defect (CHD) detection in ultrasound videos is hindered by image noise and probe positioning variability. While automated methods can reduce operator dependence, current machine learning approaches often neglect temporal information, limit themselves to binary classification, and do not account for prediction calibration. We propose Temporal Prompt Alignment (TPA), a method leveraging foundation image-text model and prompt-aware contrastive learning to classify fetal CHD on cardiac ultrasound videos. TPA extracts features from each frame of video subclips using an image encoder, aggregates them with a trainable temporal extractor to capture heart motion, and aligns the video representation with class-specific text prompts via a margin-hinge contrastive loss. To enhance calibration for clinical reliability, we introduce a Conditional Variational Autoencoder Style Modulation (CVAESM) module, which learns a latent style vector to modulate embeddings and quantifies classification uncertainty. Evaluated on a private dataset for CHD detection and on a large public dataset, EchoNet-Dynamic, for systolic dysfunction, TPA achieves state-of-the-art macro F1 scores of 85.40% for CHD diagnosis, while also reducing expected calibration error by 5.38% and adaptive ECE by 6.8%. On EchoNet-Dynamic's three-class task, it boosts macro F1 by 4.73% (from 53.89% to 58.62%). Temporal Prompt Alignment (TPA) is a framework for fetal congenital heart defect (CHD) classification in ultrasound videos that integrates temporal modeling, prompt-aware contrastive learning, and uncertainty quantification.

</details>


### [12] [Transfer learning optimization based on evolutionary selective fine tuning](https://arxiv.org/abs/2508.15367)
*Jacinto Colan,Ana Davila,Yasuhisa Hasegawa*

Main category: cs.CV

TL;DR: BioTune是一种进化自适应微调技术，通过进化算法选择性地微调层来提高迁移学习效率，在多个图像分类数据集上实现了竞争性或改进的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法需要更新所有模型参数，可能导致过拟合和计算成本高，需要一种更高效的迁移学习策略。

Method: 使用进化算法识别需要微调的层子集，选择性微调相关层来优化目标任务的性能。

Result: 在9个不同领域的图像分类数据集上评估，BioTune相比AutoRGN和LoRA等方法实现了竞争性或改进的准确性和效率。

Conclusion: 通过集中微调相关层子集，BioTune减少了可训练参数数量，降低了计算成本，促进了跨不同数据特征和分布的更高效迁移学习。

Abstract: Deep learning has shown substantial progress in image analysis. However, the computational demands of large, fully trained models remain a consideration. Transfer learning offers a strategy for adapting pre-trained models to new tasks. Traditional fine-tuning often involves updating all model parameters, which can potentially lead to overfitting and higher computational costs. This paper introduces BioTune, an evolutionary adaptive fine-tuning technique that selectively fine-tunes layers to enhance transfer learning efficiency. BioTune employs an evolutionary algorithm to identify a focused set of layers for fine-tuning, aiming to optimize model performance on a given target task. Evaluation across nine image classification datasets from various domains indicates that BioTune achieves competitive or improved accuracy and efficiency compared to existing fine-tuning methods such as AutoRGN and LoRA. By concentrating the fine-tuning process on a subset of relevant layers, BioTune reduces the number of trainable parameters, potentially leading to decreased computational cost and facilitating more efficient transfer learning across diverse data characteristics and distributions.

</details>


### [13] [DriveSplat: Decoupled Driving Scene Reconstruction with Geometry-enhanced Partitioned Neural Gaussians](https://arxiv.org/abs/2508.15376)
*Cong Wang,Xianda Guo,Wenbo Xu,Wei Tian,Ruiqi Song,Chenming Zhang,Lingxi Li,Long Chen*

Main category: cs.CV

TL;DR: DriveSplat是一种基于神经高斯表示的动态-静态解耦方法，专门针对驾驶场景的高质量3D重建，通过区域化体素初始化、可变形神经高斯和深度法线先验监督，在Waymo和KITTI数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯泼溅的方法在解耦动态静态组件时忽视了背景优化的几何关系，仅通过添加高斯来拟合训练视图，导致新视角渲染鲁棒性有限且几何表示不准确。

Method: 1. 区域化体素初始化方案，将场景分为近、中、远区域以增强近景细节表示；2. 引入可变形神经高斯建模非刚性动态物体，通过可学习变形网络进行时间参数调整；3. 使用预训练模型的深度和法线先验监督整个框架。

Result: 在Waymo和KITTI数据集上的严格评估表明，该方法在驾驶场景的新视角合成方面达到了最先进的性能。

Conclusion: DriveSplat通过创新的动态-静态解耦策略和几何先验监督，有效解决了驾驶场景中快速移动车辆、行人和大规模静态背景带来的3D重建挑战，实现了高质量的场景重建和新视角渲染。

Abstract: In the realm of driving scenarios, the presence of rapidly moving vehicles, pedestrians in motion, and large-scale static backgrounds poses significant challenges for 3D scene reconstruction. Recent methods based on 3D Gaussian Splatting address the motion blur problem by decoupling dynamic and static components within the scene. However, these decoupling strategies overlook background optimization with adequate geometry relationships and rely solely on fitting each training view by adding Gaussians. Therefore, these models exhibit limited robustness in rendering novel views and lack an accurate geometric representation. To address the above issues, we introduce DriveSplat, a high-quality reconstruction method for driving scenarios based on neural Gaussian representations with dynamic-static decoupling. To better accommodate the predominantly linear motion patterns of driving viewpoints, a region-wise voxel initialization scheme is employed, which partitions the scene into near, middle, and far regions to enhance close-range detail representation. Deformable neural Gaussians are introduced to model non-rigid dynamic actors, whose parameters are temporally adjusted by a learnable deformation network. The entire framework is further supervised by depth and normal priors from pre-trained models, improving the accuracy of geometric structures. Our method has been rigorously evaluated on the Waymo and KITTI datasets, demonstrating state-of-the-art performance in novel-view synthesis for driving scenarios.

</details>


### [14] [Enhancing Novel View Synthesis from extremely sparse views with SfM-free 3D Gaussian Splatting Framework](https://arxiv.org/abs/2508.15457)
*Zongqi He,Hanmin Li,Kin-Chung Chan,Yushen Zuo,Hao Xie,Zhe Xiao,Jun Xiao,Kin-Man Lam*

Main category: cs.CV

TL;DR: 提出了一种无需SfM的3D高斯泼溅方法，能够在极稀疏视角输入下联合估计相机位姿和重建3D场景，显著提升渲染质量


<details>
  <summary>Details</summary>
Motivation: 传统3DGS方法严重依赖密集多视角输入和精确相机位姿，在极稀疏视角下SfM初始化失败导致渲染质量下降

Method: 使用密集立体模块替代SfM进行相机位姿估计和全局稠密点云重建，提出一致性视角插值模块生成额外监督信号，引入多尺度拉普拉斯一致性正则化和自适应空间感知多尺度几何正则化

Result: 在极稀疏视角条件下（仅2个训练视角）PSNR显著提升2.75dB，合成图像失真最小且保留丰富高频细节

Conclusion: 该方法有效解决了极稀疏视角下的3D场景重建问题，在视觉质量和几何结构方面均优于现有技术

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated remarkable real-time performance in novel view synthesis, yet its effectiveness relies heavily on dense multi-view inputs with precisely known camera poses, which are rarely available in real-world scenarios. When input views become extremely sparse, the Structure-from-Motion (SfM) method that 3DGS depends on for initialization fails to accurately reconstruct the 3D geometric structures of scenes, resulting in degraded rendering quality. In this paper, we propose a novel SfM-free 3DGS-based method that jointly estimates camera poses and reconstructs 3D scenes from extremely sparse-view inputs. Specifically, instead of SfM, we propose a dense stereo module to progressively estimates camera pose information and reconstructs a global dense point cloud for initialization. To address the inherent problem of information scarcity in extremely sparse-view settings, we propose a coherent view interpolation module that interpolates camera poses based on training view pairs and generates viewpoint-consistent content as additional supervision signals for training. Furthermore, we introduce multi-scale Laplacian consistent regularization and adaptive spatial-aware multi-scale geometry regularization to enhance the quality of geometrical structures and rendered content. Experiments show that our method significantly outperforms other state-of-the-art 3DGS-based approaches, achieving a remarkable 2.75dB improvement in PSNR under extremely sparse-view conditions (using only 2 training views). The images synthesized by our method exhibit minimal distortion while preserving rich high-frequency details, resulting in superior visual quality compared to existing techniques.

</details>


### [15] [Multi-Object Sketch Animation with Grouping and Motion Trajectory Priors](https://arxiv.org/abs/2508.15535)
*Guotao Liang,Juncheng Hu,Ximing Xing,Jing Zhang,Qian Yu*

Main category: cs.CV

TL;DR: GroupSketch是一种新颖的矢量草图动画方法，通过两阶段流程（运动初始化和运动细化）有效处理多对象交互和复杂运动，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多对象交互和复杂运动时存在局限，要么仅限于单对象情况，要么存在时间不一致性和泛化能力差的问题。

Method: 采用两阶段流程：1）运动初始化阶段交互式分割语义组并定义关键帧，通过插值生成粗略动画；2）运动细化阶段使用基于组的位移网络（GDN）预测组特定位移场，并整合上下文条件特征增强（CCFE）等模块提升时间一致性。

Result: 大量实验证明，该方法在生成复杂多对象草图的高质量、时间一致动画方面显著优于现有方法。

Conclusion: GroupSketch扩展了草图动画的实际应用范围，为处理复杂多对象交互提供了有效的解决方案。

Abstract: We introduce GroupSketch, a novel method for vector sketch animation that effectively handles multi-object interactions and complex motions. Existing approaches struggle with these scenarios, either being limited to single-object cases or suffering from temporal inconsistency and poor generalization. To address these limitations, our method adopts a two-stage pipeline comprising Motion Initialization and Motion Refinement. In the first stage, the input sketch is interactively divided into semantic groups and key frames are defined, enabling the generation of a coarse animation via interpolation. In the second stage, we propose a Group-based Displacement Network (GDN), which refines the coarse animation by predicting group-specific displacement fields, leveraging priors from a text-to-video model. GDN further incorporates specialized modules, such as Context-conditioned Feature Enhancement (CCFE), to improve temporal consistency. Extensive experiments demonstrate that our approach significantly outperforms existing methods in generating high-quality, temporally consistent animations for complex, multi-object sketches, thus expanding the practical applications of sketch animation.

</details>


### [16] [When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding](https://arxiv.org/abs/2508.15641)
*Pengcheng Fang,Yuxia Chen,Rui Guo*

Main category: cs.CV

TL;DR: Grounded VideoDiT是一个视频大语言模型，通过扩散时间潜在编码器、对象接地表示和混合令牌方案三大创新，解决了现有视频LLM在时间感知精度上的不足，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频LLM在整体推理方面取得显著进展，但在时间感知上仍显粗糙：时间戳仅隐式编码、帧级特征难以捕捉连续性、语言视觉对齐容易偏离关注实体。需要解决这些限制以实现更精确的视频理解。

Method: 1. 扩散时间潜在(DTL)编码器增强边界敏感性和保持时间一致性；2. 对象接地表示将查询实体显式绑定到局部视觉证据，加强对齐；3. 混合令牌方案使用离散时间令牌提供显式时间戳建模，支持细粒度时间推理。

Result: 在Charades STA、NExT GQA和多个VideoQA基准测试中实现了最先进的结果，验证了模型具有强大的接地能力。

Conclusion: Grounded VideoDiT通过三大技术创新成功克服了现有视频LLM的时间感知局限性，为视频理解提供了更精确的时间推理和实体定位能力。

Abstract: Understanding videos requires more than answering open ended questions, it demands the ability to pinpoint when events occur and how entities interact across time. While recent Video LLMs have achieved remarkable progress in holistic reasoning, they remain coarse in temporal perception: timestamps are encoded only implicitly, frame level features are weak in capturing continuity, and language vision alignment often drifts from the entities of interest. In this paper, we present Grounded VideoDiT, a Video LLM designed to overcome these limitations by introducing three key innovations. First, a Diffusion Temporal Latent (DTL) encoder enhances boundary sensitivity and maintains temporal consistency. Second, object grounded representations explicitly bind query entities to localized visual evidence, strengthening alignment. Third, a mixed token scheme with discrete temporal tokens provides explicit timestamp modeling, enabling fine grained temporal reasoning. Together, these designs equip Grounded VideoDiT with robust grounding capabilities, as validated by state of the art results on Charades STA, NExT GQA, and multiple VideoQA benchmarks.

</details>


### [17] [WorldWeaver: Generating Long-Horizon Video Worlds via Rich Perception](https://arxiv.org/abs/2508.15720)
*Zhiheng Liu,Xueqing Deng,Shoufa Chen,Angtian Wang,Qiushan Guo,Mingfei Han,Zeyue Xue,Mengzhao Chen,Ping Luo,Linjie Yang*

Main category: cs.CV

TL;DR: WorldWeaver是一个用于生成长视频的框架，通过联合建模RGB帧和感知条件来增强时间一致性和运动动态，利用深度线索构建内存库，并采用分段噪声调度来减少漂移和计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前生成视频建模方法主要依赖RGB信号，导致在长序列中对象结构和运动出现累积错误，需要解决长视频生成中的结构性和时间一致性问题。

Method: 提出WorldWeaver框架，联合建模RGB帧和感知条件，利用深度线索构建内存库保存上下文信息，并采用分段噪声调度训练预测组。

Result: 在基于扩散和整流流的模型上进行广泛实验，证明WorldWeaver能有效减少时间漂移并提高生成视频的保真度。

Conclusion: WorldWeaver通过统一的长期建模方案，显著提升了长视频生成的时间一致性和运动动态，减少了累积错误。

Abstract: Generative video modeling has made significant strides, yet ensuring structural and temporal consistency over long sequences remains a challenge. Current methods predominantly rely on RGB signals, leading to accumulated errors in object structure and motion over extended durations. To address these issues, we introduce WorldWeaver, a robust framework for long video generation that jointly models RGB frames and perceptual conditions within a unified long-horizon modeling scheme. Our training framework offers three key advantages. First, by jointly predicting perceptual conditions and color information from a unified representation, it significantly enhances temporal consistency and motion dynamics. Second, by leveraging depth cues, which we observe to be more resistant to drift than RGB, we construct a memory bank that preserves clearer contextual information, improving quality in long-horizon video generation. Third, we employ segmented noise scheduling for training prediction groups, which further mitigates drift and reduces computational cost. Extensive experiments on both diffusion- and rectified flow-based models demonstrate the effectiveness of WorldWeaver in reducing temporal drift and improving the fidelity of generated videos.

</details>


### [18] [Waver: Wave Your Way to Lifelike Video Generation](https://arxiv.org/abs/2508.15761)
*Yifu Zhang,Hao Yang,Yuqi Zhang,Yifei Hu,Fengda Zhu,Chuang Lin,Xiaofeng Mei,Yi Jiang,Zehuan Yuan,Bingyue Peng*

Main category: cs.CV

TL;DR: Waver是一个高性能的统一图像和视频生成基础模型，能够直接生成5-10秒720p原生分辨率视频，并支持上采样至1080p。该模型在单一框架内同时支持文本到视频(T2V)、图像到视频(I2V)和文本到图像(T2I)生成，在多个榜单排名前三。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有视频生成模型在模态对齐、训练效率和生成质量方面的挑战，开发一个统一的框架来同时处理图像和视频生成任务，并提供高质量的训练数据和详细的方法指导。

Method: 采用混合流DiT架构来增强模态对齐并加速训练收敛；建立全面的数据筛选流程，使用基于MLLM的视频质量模型手动标注和筛选高质量样本；提供详细的训练和推理方案。

Result: Waver在复杂运动捕捉方面表现优异，实现了卓越的运动幅度和时间一致性。在Artificial Analysis的T2V和I2V排行榜上均排名前三，持续超越现有开源模型，达到或超越了最先进的商业解决方案水平。

Conclusion: Waver作为一个统一的多模态生成模型，在视频生成质量和技术性能方面取得了显著突破，为社区提供了高效训练高质量视频生成模型的方法，有望加速视频生成技术的发展。

Abstract: We present Waver, a high-performance foundation model for unified image and video generation. Waver can directly generate videos with durations ranging from 5 to 10 seconds at a native resolution of 720p, which are subsequently upscaled to 1080p. The model simultaneously supports text-to-video (T2V), image-to-video (I2V), and text-to-image (T2I) generation within a single, integrated framework. We introduce a Hybrid Stream DiT architecture to enhance modality alignment and accelerate training convergence. To ensure training data quality, we establish a comprehensive data curation pipeline and manually annotate and train an MLLM-based video quality model to filter for the highest-quality samples. Furthermore, we provide detailed training and inference recipes to facilitate the generation of high-quality videos. Building on these contributions, Waver excels at capturing complex motion, achieving superior motion amplitude and temporal consistency in video synthesis. Notably, it ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial Analysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming existing open-source models and matching or surpassing state-of-the-art commercial solutions. We hope this technical report will help the community more efficiently train high-quality video generation models and accelerate progress in video generation technologies. Official page: https://github.com/FoundationVision/Waver.

</details>


### [19] [Visual Autoregressive Modeling for Instruction-Guided Image Editing](https://arxiv.org/abs/2508.15772)
*Qingyang Mao,Qi Cai,Yehao Li,Yingwei Pan,Mingyue Cheng,Ting Yao,Qi Liu,Tao Mei*

Main category: cs.CV

TL;DR: VAREdit是一个基于视觉自回归模型的图像编辑框架，通过多尺度特征预测和尺度对齐参考模块，解决了扩散模型在指令引导编辑中的全局纠缠问题，实现了更精确的编辑效果和更高的效率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在指令引导图像编辑中存在全局去噪过程导致编辑区域与整个图像上下文纠缠的问题，产生意外的伪修改和编辑指令遵循度不足。自回归模型因其因果和组合机制能自然避免这些问题。

Method: 提出VAREdit框架，将图像编辑重构为下一尺度预测问题。基于源图像特征和文本指令生成多尺度目标特征。引入尺度对齐参考(SAR)模块，在第一个自注意力层注入尺度匹配的条件信息。

Result: 在标准基准测试中，GPT-Balance分数比领先的基于扩散的方法高出30%+。完成512×512图像编辑仅需1.2秒，比同等规模的UltraEdit快2.2倍。

Conclusion: VAREdit通过自回归范式和多尺度特征预测，显著提升了图像编辑的精确度和效率，为指令引导的图像编辑提供了新的解决方案。

Abstract: Recent advances in diffusion models have brought remarkable visual fidelity to instruction-guided image editing. However, their global denoising process inherently entangles the edited region with the entire image context, leading to unintended spurious modifications and compromised adherence to editing instructions. In contrast, autoregressive models offer a distinct paradigm by formulating image synthesis as a sequential process over discrete visual tokens. Their causal and compositional mechanism naturally circumvents the adherence challenges of diffusion-based methods. In this paper, we present VAREdit, a visual autoregressive (VAR) framework that reframes image editing as a next-scale prediction problem. Conditioned on source image features and text instructions, VAREdit generates multi-scale target features to achieve precise edits. A core challenge in this paradigm is how to effectively condition the source image tokens. We observe that finest-scale source features cannot effectively guide the prediction of coarser target features. To bridge this gap, we introduce a Scale-Aligned Reference (SAR) module, which injects scale-matched conditioning information into the first self-attention layer. VAREdit demonstrates significant advancements in both editing adherence and efficiency. On standard benchmarks, it outperforms leading diffusion-based methods by 30\%+ higher GPT-Balance score. Moreover, it completes a $512\times512$ editing in 1.2 seconds, making it 2.2$\times$ faster than the similarly sized UltraEdit. The models are available at https://github.com/HiDream-ai/VAREdit.

</details>


### [20] [CineScale: Free Lunch in High-Resolution Cinematic Visual Generation](https://arxiv.org/abs/2508.15774)
*Haonan Qiu,Ning Yu,Ziqi Huang,Paul Debevec,Ziwei Liu*

Main category: cs.CV

TL;DR: CineScale是一种无需微调的推理范式，能够实现更高分辨率的视觉生成，支持8K图像生成和4K视频生成，解决了现有方法在高分辨率生成时出现的重复模式问题。


<details>
  <summary>Details</summary>
Motivation: 视觉扩散模型由于缺乏高分辨率数据和计算资源限制，通常只能在有限分辨率下训练，这限制了它们生成高保真高分辨率图像或视频的能力。现有方法在高分辨率生成时容易出现重复模式和质量问题。

Method: 提出了CineScale推理范式，针对两种视频生成架构设计了专门的变体。该方法基于开源视频生成框架，支持高分辨率T2I、T2V、I2V和V2V合成，无需微调即可实现8K图像生成，仅需最小LoRA微调即可实现4K视频生成。

Result: 大量实验验证了该范式在扩展图像和视频模型高分辨率生成能力方面的优越性。能够实现8K图像无微调生成和4K视频最小微调生成。

Conclusion: CineScale成功解决了高分辨率视觉生成中的关键障碍，为预训练模型的高分辨率生成潜力提供了有效的推理范式，显著提升了生成质量。

Abstract: Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained models. However, these methods are still prone to producing low-quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. In this work, we propose CineScale, a novel inference paradigm to enable higher-resolution visual generation. To tackle the various issues introduced by the two types of video generation architectures, we propose dedicated variants tailored to each. Unlike existing baseline methods that are confined to high-resolution T2I and T2V generation, CineScale broadens the scope by enabling high-resolution I2V and V2V synthesis, built atop state-of-the-art open-source video generation frameworks. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Remarkably, our approach enables 8k image generation without any fine-tuning, and achieves 4k video generation with only minimal LoRA fine-tuning. Generated video samples are available at our website: https://eyeline-labs.github.io/CineScale/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [21] [Probability Density from Latent Diffusion Models for Out-of-Distribution Detection](https://arxiv.org/abs/2508.15737)
*Joonas Järve,Karl Kaspar Haavel,Meelis Kull*

Main category: cs.LG

TL;DR: 本文探讨了在表示空间中使用数据似然进行OOD检测的有效性，挑战了传统像素空间中似然检测失败的观点


<details>
  <summary>Details</summary>
Motivation: 尽管数据似然在理论上是最优的OOD检测器，但在实践中经常失败。作者希望探究这是否是像素空间特有的问题，还是表示空间也存在类似问题

Method: 训练变分扩散模型在预训练ResNet-18的表示空间上，而不是在图像像素空间上，使用基于似然的检测器并与OpenOOD套件中的最先进方法进行比较

Result: 通过在表示空间而非像素空间进行实验，评估了基于似然的OOD检测器性能，并与现有方法进行了对比

Conclusion: 研究旨在验证表示空间中基于似然的OOD检测是否比像素空间更有效，为解决实际应用中似然检测失败问题提供新视角

Abstract: Despite rapid advances in AI, safety remains the main bottleneck to deploying machine-learning systems. A critical safety component is out-of-distribution detection: given an input, decide whether it comes from the same distribution as the training data. In generative models, the most natural OOD score is the data likelihood. Actually, under the assumption of uniformly distributed OOD data, the likelihood is even the optimal OOD detector, as we show in this work. However, earlier work reported that likelihood often fails in practice, raising doubts about its usefulness. We explore whether, in practice, the representation space also suffers from the inability to learn good density estimation for OOD detection, or if it is merely a problem of the pixel space typically used in generative models. To test this, we trained a Variational Diffusion Model not on images, but on the representation space of a pre-trained ResNet-18 to assess the performance of our likelihood-based detector in comparison to state-of-the-art methods from the OpenOOD suite.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [22] [Scalable FPGA Framework for Real-Time Denoising in High-Throughput Imaging: A DRAM-Optimized Pipeline using High-Level Synthesis](https://arxiv.org/abs/2508.14917)
*Weichien Liao*

Main category: cs.AR

TL;DR: 基于FPGA的实时图像预处理流水线，通过HLS实现高效去噪，满足高通量成像的实时处理需求


<details>
  <summary>Details</summary>
Motivation: 高通量成像工作流（如PRISM）的数据生成速度超过传统实时处理能力，需要低延迟的实时预处理解决方案

Method: 采用可扩展的FPGA架构，通过高级综合（HLS）实现，优化DRAM缓冲，使用突发模式AXI4接口执行帧减法和平均操作

Result: 内核操作时间低于帧间间隔，实现内联去噪并减少下游CPU/GPU分析的数据集大小，在PRISM规模采集下验证有效

Conclusion: 该模块化FPGA框架为光谱学和显微镜学中的延迟敏感成像工作流提供了实用解决方案

Abstract: High-throughput imaging workflows, such as Parallel Rapid Imaging with Spectroscopic Mapping (PRISM), generate data at rates that exceed conventional real-time processing capabilities. We present a scalable FPGA-based preprocessing pipeline for real-time denoising, implemented via High-Level Synthesis (HLS) and optimized for DRAM-backed buffering. Our architecture performs frame subtraction and averaging directly on streamed image data, minimizing latency through burst-mode AXI4 interfaces. The resulting kernel operates below the inter-frame interval, enabling inline denoising and reducing dataset size for downstream CPU/GPU analysis. Validated under PRISM-scale acquisition, this modular FPGA framework offers a practical solution for latency-sensitive imaging workflows in spectroscopy and microscopy.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [23] [Pixels Under Pressure: Exploring Fine-Tuning Paradigms for Foundation Models in High-Resolution Medical Imaging](https://arxiv.org/abs/2508.14931)
*Zahra TehraniNasab,Amar Kumar,Tal Arbel*

Main category: eess.IV

TL;DR: 本研究系统评估了不同微调方法对扩散模型在高分辨率（512x512）图像生成质量的影响，包括完整微调和参数高效微调方法，并分析了生成图像在下游分类任务中的实用性。


<details>
  <summary>Details</summary>
Motivation: 随着高分辨率图像合成在医疗影像等领域的重要性日益增加，需要研究如何通过微调技术将预训练扩散模型适配到特定任务和数据分布，以提升生成质量。

Method: 采用系统研究方法，对多种微调技术进行基准测试，包括完整微调策略和参数高效微调(PEFT)方法，评估FID、Vendi分数和提示-图像对齐等关键质量指标。

Result: 研究显示特定微调策略能够同时改善生成保真度和下游性能，在数据稀缺条件下使用合成图像进行分类器训练和真实图像评估时表现更佳。

Conclusion: 微调是适配预训练扩散模型到高分辨率图像生成任务的有效机制，特定微调方法能够显著提升生成质量和下游任务性能，为医疗影像等应用提供了重要参考。

Abstract: Advancements in diffusion-based foundation models have improved text-to-image generation, yet most efforts have been limited to low-resolution settings. As high-resolution image synthesis becomes increasingly essential for various applications, particularly in medical imaging domains, fine-tuning emerges as a crucial mechanism for adapting these powerful pre-trained models to task-specific requirements and data distributions. In this work, we present a systematic study, examining the impact of various fine-tuning techniques on image generation quality when scaling to high resolution 512x512 pixels. We benchmark a diverse set of fine-tuning methods, including full fine-tuning strategies and parameter-efficient fine-tuning (PEFT). We dissect how different fine-tuning methods influence key quality metrics, including Fr\'echet Inception Distance (FID), Vendi score, and prompt-image alignment. We also evaluate the utility of generated images in a downstream classification task under data-scarce conditions, demonstrating that specific fine-tuning strategies improve both generation fidelity and downstream performance when synthetic images are used for classifier training and evaluation on real images. Our code is accessible through the project website - https://tehraninasab.github.io/PixelUPressure/.

</details>


### [24] [Zero-shot Volumetric CT Super-Resolution using 3D Gaussian Splatting with Upsampled 2D X-ray Projection Priors](https://arxiv.org/abs/2508.15151)
*Jeonghyun Noh,Hyun-Jic Oh,Byungju Chae,Won-Ki Jeong*

Main category: eess.IV

TL;DR: 一种新的无监督3D CT超分辨量重建框架，利用激光扩散模型生成的高分辨率2D X光投影作为外部先验知识，通过3D高斯拷贝技术实现优秀的重建效果


<details>
  <summary>Details</summary>
Motivation: 解决传统监督方法需要大量成对LR-HR数据集且难以获得，而现有零检测方法又困于内部信息有限而无法恢复细微解剖结构

Method: 基于激光扩散模型生成高分辨率2D X光投影作为外部先验，采用每张投影适配采样策略，通过3D高斯拷贝技术进行3D CT重建，并提出支持负值的负值Alpha混合技术

Result: 在两个数据集上进行实验，该方法在数量和质量上都取得了优秀的3D CT超分辨重建结果

Conclusion: 该方法通过结合激光扩散模型生成的外部先验知识和3D重建技术，有效解决了无监督条件下3D CT超分辨的挑战，能够恢复细微的解剖结构细节

Abstract: Computed tomography (CT) is widely used in clinical diagnosis, but acquiring high-resolution (HR) CT is limited by radiation exposure risks. Deep learning-based super-resolution (SR) methods have been studied to reconstruct HR from low-resolution (LR) inputs. While supervised SR approaches have shown promising results, they require large-scale paired LR-HR volume datasets that are often unavailable. In contrast, zero-shot methods alleviate the need for paired data by using only a single LR input, but typically struggle to recover fine anatomical details due to limited internal information. To overcome these, we propose a novel zero-shot 3D CT SR framework that leverages upsampled 2D X-ray projection priors generated by a diffusion model. Exploiting the abundance of HR 2D X-ray data, we train a diffusion model on large-scale 2D X-ray projection and introduce a per-projection adaptive sampling strategy. It selects the generative process for each projection, thus providing HR projections as strong external priors for 3D CT reconstruction. These projections serve as inputs to 3D Gaussian splatting for reconstructing a 3D CT volume. Furthermore, we propose negative alpha blending (NAB-GS) that allows negative values in Gaussian density representation. NAB-GS enables residual learning between LR and diffusion-based projections, thereby enhancing high-frequency structure reconstruction. Experiments on two datasets show that our method achieves superior quantitative and qualitative results for 3D CT SR.

</details>


### [25] [Pathology-Informed Latent Diffusion Model for Anomaly Detection in Lymph Node Metastasis](https://arxiv.org/abs/2508.15236)
*Jiamu Wang,Keunho Byeon,Jinsol Song,Anh Nguyen,Sangjeong Ahn,Sung Hak Lee,Jin Tae Kwak*

Main category: eess.IV

TL;DR: 结合视觉语言模型和扩散模型的无监督异常检测方法，在数字病理学中利用组织病理学提示进行重建，无需大量标注即可区分正常和异常组织


<details>
  <summary>Details</summary>
Motivation: 数字病理学中监督学习方法需要大量标注数据，但数据稀缺问题严重。无监督异常检测能够识别正常组织分布的偏差，而无需详尽标注。扩散概率模型在无监督异常检测中表现出色

Method: 将视觉语言模型与扩散模型结合，在重建过程中使用与正常组织相关的病理学关键词提示来指导重建过程，从而区分正常和异常组织

Result: 在本地医院胃淋巴结数据集和公共乳腺淋巴结数据集上进行实验，验证了该方法在不同器官间的泛化能力，展示了在数字病理学无监督异常检测中的潜力

Conclusion: 该方法为数字病理学中跨器官的无监督异常检测提供了有效解决方案，通过结合视觉语言模型和扩散模型，利用病理学提示实现了良好的异常检测性能

Abstract: Anomaly detection is an emerging approach in digital pathology for its ability to efficiently and effectively utilize data for disease diagnosis. While supervised learning approaches deliver high accuracy, they rely on extensively annotated datasets, suffering from data scarcity in digital pathology. Unsupervised anomaly detection, however, offers a viable alternative by identifying deviations from normal tissue distributions without requiring exhaustive annotations. Recently, denoising diffusion probabilistic models have gained popularity in unsupervised anomaly detection, achieving promising performance in both natural and medical imaging datasets. Building on this, we incorporate a vision-language model with a diffusion model for unsupervised anomaly detection in digital pathology, utilizing histopathology prompts during reconstruction. Our approach employs a set of pathology-related keywords associated with normal tissues to guide the reconstruction process, facilitating the differentiation between normal and abnormal tissues. To evaluate the effectiveness of the proposed method, we conduct experiments on a gastric lymph node dataset from a local hospital and assess its generalization ability under domain shift using a public breast lymph node dataset. The experimental results highlight the potential of the proposed method for unsupervised anomaly detection across various organs in digital pathology. Code: https://github.com/QuIIL/AnoPILaD.

</details>


### [26] [Deep Equilibrium Convolutional Sparse Coding for Hyperspectral Image Denoising](https://arxiv.org/abs/2508.15553)
*Jin Ye,Jingran Wang,Fengchao Xiong,Jingzhou Chen,Yuntao Qian*

Main category: eess.IV

TL;DR: 提出基于深度均衡模型的卷积稀疏编码框架DECSC，用于高光谱图像去噪，结合局部空间-光谱相关性、非局部空间自相似性和全局空间一致性，在DEQ框架下实现无限深度网络优化。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像在遥感中至关重要但常受复杂噪声影响。现有深度展开方法缺乏收敛保证，而深度均衡模型天然符合优化问题的固定点求解，能够提供更好的理论保证和性能。

Method: 在卷积稀疏编码框架下，使用共享2D卷积稀疏表示确保波段间全局空间一致性，非共享3D卷积稀疏表示捕获局部空间-光谱细节。嵌入Transformer块利用非局部自相似性，集成细节增强模块。将CSC模型的近端梯度下降转化为固定点问题，在DEQ框架下构建可学习网络架构。

Result: 实验结果表明，DECSC方法相比最先进方法实现了更优越的去噪性能。

Conclusion: 提出的DECSC框架成功统一了多种先验信息，通过深度均衡模型实现了高效的高光谱图像去噪，为相关领域提供了新的解决方案。

Abstract: Hyperspectral images (HSIs) play a crucial role in remote sensing but are often degraded by complex noise patterns. Ensuring the physical property of the denoised HSIs is vital for robust HSI denoising, giving the rise of deep unfolding-based methods. However, these methods map the optimization of a physical model to a learnable network with a predefined depth, which lacks convergence guarantees. In contrast, Deep Equilibrium (DEQ) models treat the hidden layers of deep networks as the solution to a fixed-point problem and models them as infinite-depth networks, naturally consistent with the optimization. Under the framework of DEQ, we propose a Deep Equilibrium Convolutional Sparse Coding (DECSC) framework that unifies local spatial-spectral correlations, nonlocal spatial self-similarities, and global spatial consistency for robust HSI denoising. Within the convolutional sparse coding (CSC) framework, we enforce shared 2D convolutional sparse representation to ensure global spatial consistency across bands, while unshared 3D convolutional sparse representation captures local spatial-spectral details. To further exploit nonlocal self-similarities, a transformer block is embedded after the 2D CSC. Additionally, a detail enhancement module is integrated with the 3D CSC to promote image detail preservation. We formulate the proximal gradient descent of the CSC model as a fixed-point problem and transform the iterative updates into a learnable network architecture within the framework of DEQ. Experimental results demonstrate that our DECSC method achieves superior denoising performance compared to state-of-the-art methods.

</details>


### [27] [Are Virtual DES Images a Valid Alternative to the Real Ones?](https://arxiv.org/abs/2508.15594)
*Ana C. Perre,Luís A. Alexandre,Luís C. Freire*

Main category: eess.IV

TL;DR: 本研究探索使用三种深度学习模型从低能量乳腺图像生成虚拟双能量减影图像，评估其在乳腺病变分类中的效果，发现预训练U-Net模型表现最佳，F1分数达85.59%，接近真实图像的90.35%。


<details>
  <summary>Details</summary>
Motivation: 对比增强乳腺摄影(CESM)需要采集高能量图像，会增加患者辐射暴露。通过图像到图像转换技术从低能量图像生成虚拟双能量减影图像，可减少辐射风险。

Method: 使用三种模型进行虚拟DES图像生成：预训练U-Net、端到端训练U-Net和CycleGAN模型，并评估生成的虚拟图像在乳腺病变恶性/非恶性分类中的性能。

Result: 预训练U-Net模型表现最佳，虚拟DES图像分类F1分数为85.59%，真实DES图像为90.35%。虚拟图像虽略逊于真实图像，但显示出临床应用潜力。

Conclusion: 虚拟DES图像生成具有重要价值，未来技术进步有望缩小性能差距，使完全依赖虚拟图像在临床上成为可行选择。

Abstract: Contrast-enhanced spectral mammography (CESM) is an imaging modality that provides two types of images, commonly known as low-energy (LE) and dual-energy subtracted (DES) images. In many domains, particularly in medicine, the emergence of image-to-image translation techniques has enabled the artificial generation of images using other images as input. Within CESM, applying such techniques to generate DES images from LE images could be highly beneficial, potentially reducing patient exposure to radiation associated with high-energy image acquisition. In this study, we investigated three models for the artificial generation of DES images (virtual DES): a pre-trained U-Net model, a U-Net trained end-to-end model, and a CycleGAN model. We also performed a series of experiments to assess the impact of using virtual DES images on the classification of CESM examinations into malignant and non-malignant categories. To our knowledge, this is the first study to evaluate the impact of virtual DES images on CESM lesion classification. The results demonstrate that the best performance was achieved with the pre-trained U-Net model, yielding an F1 score of 85.59% when using the virtual DES images, compared to 90.35% with the real DES images. This discrepancy likely results from the additional diagnostic information in real DES images, which contributes to a higher classification accuracy. Nevertheless, the potential for virtual DES image generation is considerable and future advancements may narrow this performance gap to a level where exclusive reliance on virtual DES images becomes clinically viable.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [28] [See it. Say it. Sorted: Agentic System for Compositional Diagram Generation](https://arxiv.org/abs/2508.15222)
*Hantao Zhang,Jingyang Liu,Ed Li*

Main category: cs.AI

TL;DR: 提出See it. Say it. Sorted.系统，通过VLM和LLM的协作将手绘草图转换为精确的可编辑SVG图表，解决了扩散模型在空间精度和符号结构方面的不足。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在照片级真实感方面表现出色，但在流程图等需要空间精度、对齐和符号结构的图表生成任务中存在困难，需要一种能够产生精确可编辑矢量图的方法。

Method: 采用训练免费的代理系统，结合视觉语言模型(VLM)和大型语言模型(LLM)的迭代循环：Critic VLM提出定性关系编辑建议，多个候选LLM以不同策略合成SVG更新，Judge VLM选择最佳候选以确保稳定改进。

Result: 在10个源自已发表论文流程图的草图测试中，该方法比GPT-5和Gemini-2.5-Pro更忠实地重建布局和结构，准确组合图元（如多箭头）且不插入多余文本。

Conclusion: 该方法优先考虑定性推理而非脆弱的数值估计，保持全局约束（对齐、连接性），支持人工干预修正，输出为可编程SVG，易于通过API扩展到演示工具，代码已开源。

Abstract: We study sketch-to-diagram generation: converting rough hand sketches into precise, compositional diagrams. Diffusion models excel at photorealism but struggle with the spatial precision, alignment, and symbolic structure required for flowcharts. We introduce See it. Say it. Sorted., a training-free agentic system that couples a Vision-Language Model (VLM) with Large Language Models (LLMs) to produce editable Scalable Vector Graphics (SVG) programs. The system runs an iterative loop in which a Critic VLM proposes a small set of qualitative, relational edits; multiple candidate LLMs synthesize SVG updates with diverse strategies (conservative->aggressive, alternative, focused); and a Judge VLM selects the best candidate, ensuring stable improvement. This design prioritizes qualitative reasoning over brittle numerical estimates, preserves global constraints (e.g., alignment, connectivity), and naturally supports human-in-the-loop corrections. On 10 sketches derived from flowcharts in published papers, our method more faithfully reconstructs layout and structure than two frontier closed-source image generation LLMs (GPT-5 and Gemini-2.5-Pro), accurately composing primitives (e.g., multi-headed arrows) without inserting unwanted text. Because outputs are programmatic SVGs, the approach is readily extensible to presentation tools (e.g., PowerPoint) via APIs and can be specialized with improved prompts and task-specific tools. The codebase is open-sourced at https://github.com/hantaoZhangrichard/see_it_say_it_sorted.git.

</details>
