<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 4]
- [cs.CV](#cs.CV) [Total: 24]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Controllable Video Generation: A Survey](https://arxiv.org/abs/2507.16869)
*Yue Ma,Kunyu Feng,Zhongyuan Hu,Xinyu Wang,Yucheng Wang,Mingzhe Zheng,Xuanhua He,Chenyang Zhu,Hongyu Liu,Yingqing He,Zeyu Wang,Zhifeng Li,Xiu Li,Wei Liu,Dan Xu,Linfeng Zhang,Qifeng Chen*

Main category: cs.GR

TL;DR: 这是一篇关于可控视频生成的综述论文，系统回顾了在AI生成内容快速发展背景下，如何通过整合多种非文本条件（如相机运动、深度图、人体姿态等）来增强视频生成模型的可控性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成基础模型主要依赖文本到视频的生成方式，但仅凭文本提示往往无法充分表达复杂、多模态和细粒度的用户需求，这使得用户难以使用当前模型精确控制生成的视频内容。随着AI生成内容的快速发展，对能够更准确反映用户意图的可控视频生成方法需求日益增长。

Method: 本综述采用系统性回顾的方法，首先介绍关键概念和常用的开源视频生成模型，然后重点分析视频扩散模型中的控制机制，研究如何将不同类型的条件整合到去噪过程中来指导生成。最后根据控制信号的类型对现有方法进行分类，包括单条件生成、多条件生成和通用可控生成。

Result: 论文系统梳理了可控视频生成领域的理论基础和最新进展，建立了基于控制信号类型的分类框架，并提供了完整的文献库资源。研究表明，通过整合相机运动、深度图、人体姿态等额外的非文本条件，可以有效扩展预训练视频生成模型的能力，实现更加可控的视频合成。

Conclusion: 可控视频生成通过整合多种非文本控制条件，显著提升了视频生成系统的灵活性和实际应用价值。该领域的发展为AI驱动的视频生成系统提供了更强的可控性，使其能够更好地满足用户的复杂需求。未来的研究方向包括进一步提升多条件融合的效果和开发更加通用的可控生成方法。

Abstract: With the rapid development of AI-generated content (AIGC), video generation has emerged as one of its most dynamic and impactful subfields. In particular, the advancement of video generation foundation models has led to growing demand for controllable video generation methods that can more accurately reflect user intent. Most existing foundation models are designed for text-to-video generation, where text prompts alone are often insufficient to express complex, multi-modal, and fine-grained user requirements. This limitation makes it challenging for users to generate videos with precise control using current models. To address this issue, recent research has explored the integration of additional non-textual conditions, such as camera motion, depth maps, and human pose, to extend pretrained video generation models and enable more controllable video synthesis. These approaches aim to enhance the flexibility and practical applicability of AIGC-driven video generation systems. In this survey, we provide a systematic review of controllable video generation, covering both theoretical foundations and recent advances in the field. We begin by introducing the key concepts and commonly used open-source video generation models. We then focus on control mechanisms in video diffusion models, analyzing how different types of conditions can be incorporated into the denoising process to guide generation. Finally, we categorize existing methods based on the types of control signals they leverage, including single-condition generation, multi-condition generation, and universal controllable generation. For a complete list of the literature on controllable video generation reviewed, please visit our curated repository at https://github.com/mayuelala/Awesome-Controllable-Video-Generation.

</details>


### [2] [StreamME: Simplify 3D Gaussian Avatar within Live Stream](https://arxiv.org/abs/2507.17029)
*Luchuan Song,Yang Zhou,Zhan Xu,Yi Zhou,Deepali Aneja,Chenliang Xu*

Main category: cs.GR

TL;DR: StreamME是一种基于3D高斯喷射的快速3D头像重建方法，能够从实时视频流中同步录制和重建头像，无需预缓存数据，支持即时训练策略


<details>
  <summary>Details</summary>
Motivation: 现有的3D头像重建方法通常需要预缓存数据且训练速度慢，无法满足实时应用需求。需要一种能够从实时视频流中快速重建3D头像的方法，以保护面部隐私、减少通信带宽，并支持VR系统和在线会议等下游应用

Method: 基于3D高斯喷射(3DGS)构建方法，去除可变形3DGS中对MLP的依赖，仅依靠几何信息来显著提高对面部表情的适应速度。引入基于主要点的简化策略，在面部表面更稀疏地分布点云，在保持渲染质量的同时优化点的数量，实现即时训练(on-the-fly training)

Result: 实现了从实时视频流中同步录制和重建头像，无需任何预缓存数据。方法具有异常快速的训练策略，能够无缝集成到下游应用中。可直接应用于动画、卡通化和重光照等应用场景

Conclusion: StreamME成功实现了快速3D头像重建，通过即时训练策略解决了传统方法的速度瓶颈。该方法在保护面部隐私、减少通信带宽方面具有优势，并能直接应用于多种下游应用，为VR系统和在线会议等实时应用提供了有效解决方案

Abstract: We propose StreamME, a method focuses on fast 3D avatar reconstruction. The StreamME synchronously records and reconstructs a head avatar from live video streams without any pre-cached data, enabling seamless integration of the reconstructed appearance into downstream applications. This exceptionally fast training strategy, which we refer to as on-the-fly training, is central to our approach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating the reliance on MLPs in deformable 3DGS and relying solely on geometry, which significantly improves the adaptation speed to facial expression. To further ensure high efficiency in on-the-fly training, we introduced a simplification strategy based on primary points, which distributes the point clouds more sparsely across the facial surface, optimizing points number while maintaining rendering quality. Leveraging the on-the-fly training capabilities, our method protects the facial privacy and reduces communication bandwidth in VR system or online conference. Additionally, it can be directly applied to downstream application such as animation, toonify, and relighting. Please refer to our project page for more details: https://songluchuan.github.io/StreamME/.

</details>


### [3] [Temporal Smoothness-Aware Rate-Distortion Optimized 4D Gaussian Splatting](https://arxiv.org/abs/2507.17336)
*Hyeongmin Lee,Kyungjune Baek*

Main category: cs.GR

TL;DR: 本文提出了一种针对4D高斯点云(4DGS)的端到端压缩框架，通过小波变换优化运动轨迹存储，实现了高达91倍的压缩率，同时保持高质量的动态场景渲染效果。


<details>
  <summary>Details</summary>
Motivation: 动态4D高斯点云虽然能有效扩展3D高斯点云的高速渲染能力来表示体积视频，但由于高斯点数量庞大、时间冗余严重，特别是缺乏熵感知的压缩框架，导致存储需求巨大，在实际部署、边缘设备处理和数据传输方面面临重大挑战。

Method: 基于Ex4DGS作为基线，从现有3DGS压缩方法出发保证兼容性，同时有效解决时间轴引入的额外挑战。关键创新是采用小波变换来反映真实世界的平滑性先验，而不是独立存储每个点的运动轨迹，显著提升存储效率。

Result: 实验结果表明该方法相比原始Ex4DGS模型实现了高达91倍的压缩率，同时保持了高视觉保真度。该框架在从资源受限的边缘设备到高性能环境的各种场景中都能实现实时动态场景渲染。

Conclusion: 提出的RD优化压缩框架成功解决了4DGS的存储问题，通过小波变换优化运动轨迹存储，在保持高渲染质量的同时实现了显著的压缩效果，为4DGS在实际应用中的部署提供了有效解决方案。

Abstract: Dynamic 4D Gaussian Splatting (4DGS) effectively extends the high-speed rendering capabilities of 3D Gaussian Splatting (3DGS) to represent volumetric videos. However, the large number of Gaussians, substantial temporal redundancies, and especially the absence of an entropy-aware compression framework result in large storage requirements. Consequently, this poses significant challenges for practical deployment, efficient edge-device processing, and data transmission. In this paper, we introduce a novel end-to-end RD-optimized compression framework tailored for 4DGS, aiming to enable flexible, high-fidelity rendering across varied computational platforms. Leveraging Fully Explicit Dynamic Gaussian Splatting (Ex4DGS), one of the state-of-the-art 4DGS methods, as our baseline, we start from the existing 3DGS compression methods for compatibility while effectively addressing additional challenges introduced by the temporal axis. In particular, instead of storing motion trajectories independently per point, we employ a wavelet transform to reflect the real-world smoothness prior, significantly enhancing storage efficiency. This approach yields significantly improved compression ratios and provides a user-controlled balance between compression efficiency and rendering quality. Extensive experiments demonstrate the effectiveness of our method, achieving up to 91x compression compared to the original Ex4DGS model while maintaining high visual fidelity. These results highlight the applicability of our framework for real-time dynamic scene rendering in diverse scenarios, from resource-constrained edge devices to high-performance environments.

</details>


### [4] [Parametric Integration with Neural Integral Operators](https://arxiv.org/abs/2507.17440)
*Christoph Schied,Alexander Keller*

Main category: cs.GR

TL;DR: 本文提出了一种材质无关去噪（MAD）方法，通过在材质着色前而非着色后进行去噪来提升实时渲染的图像质量，使用神经网络近似光传输积分算子实现参数化积分。


<details>
  <summary>Details</summary>
Motivation: 实时渲染对光传输模拟的采样预算有严格限制，经常导致图像噪声。虽然去噪器可以通过滤波产生无噪声图像，但传统方法是对已着色的噪声图像进行滤波，存在局限性。

Method: 提出材质无关去噪（MAD）方法，在材质着色前进行去噪而非着色后滤波。使用神经网络近似光传输积分算子，通过神经算子执行参数化积分来实现去噪。

Result: 该方法能够实时运行，仅需单帧数据，可与现有去噪器和时间抗锯齿技术无缝集成，训练效率高，且易于与基于物理的渲染算法结合。

Conclusion: 通过在着色前进行材质无关去噪，该方法能够有效提升实时渲染的图像质量，同时保持实时性能和良好的兼容性，为实时渲染中的噪声问题提供了新的解决方案。

Abstract: Real-time rendering imposes strict limitations on the sampling budget for light transport simulation, often resulting in noisy images. However, denoisers have demonstrated that it is possible to produce noise-free images through filtering. We enhance image quality by removing noise before material shading, rather than filtering already shaded noisy images. This approach allows for material-agnostic denoising (MAD) and leverages machine learning by approximating the light transport integral operator with a neural network, effectively performing parametric integration with neural operators. Our method operates in real-time, requires data from only a single frame, seamlessly integrates with existing denoisers and temporal anti-aliasing techniques, and is efficient to train. Additionally, it is straightforward to incorporate with physically based rendering algorithms.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [5] [Coarse-to-fine crack cue for robust crack detection](https://arxiv.org/abs/2507.16851)
*Zelong Liu,Yuliang Gu,Zhichao Sun,Huachao Zhu,Xin Xiao,Bo Du,Laurent Najman,Yongchao Xu*

Main category: cs.CV

TL;DR: 提出了CrackCue方法，通过粗到细的裂缝线索生成来提升裂缝检测的泛化能力和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 深度学习裂缝检测方法在跨域泛化方面表现不佳，且以往方法忽略了裂缝的细长结构特性，需要一种能够应对复杂背景、阴影和光照变化的鲁棒裂缝检测方法

Method: 基于粗到细的裂缝线索生成：首先通过最大池化和上采样操作获得粗糙的无裂缝背景，然后通过重建网络得到精细的无裂缝背景，原图像与精细无裂缝背景的差异提供精细裂缝线索，该线索嵌入了不受复杂背景影响的鲁棒裂缝先验信息

Result: 作为即插即用方法，CrackCue被集成到三个先进的裂缝检测网络中，大量实验结果表明该方法显著提升了基线方法的泛化能力和鲁棒性

Conclusion: CrackCue方法通过利用裂缝的细长结构特性生成鲁棒的裂缝线索，有效解决了深度学习裂缝检测方法的跨域泛化问题，为裂缝检测任务提供了一种有效的解决方案

Abstract: Crack detection is an important task in computer vision. Despite impressive in-dataset performance, deep learning-based methods still struggle in generalizing to unseen domains. The thin structure property of cracks is usually overlooked by previous methods. In this work, we introduce CrackCue, a novel method for robust crack detection based on coarse-to-fine crack cue generation. The core concept lies on leveraging the thin structure property to generate a robust crack cue, guiding the crack detection. Specifically, we first employ a simple max-pooling and upsampling operation on the crack image. This results in a coarse crack-free background, based on which a fine crack-free background can be obtained via a reconstruction network. The difference between the original image and fine crack-free background provides a fine crack cue. This fine cue embeds robust crack prior information which is unaffected by complex backgrounds, shadow, and varied lighting. As a plug-and-play method, we incorporate the proposed CrackCue into three advanced crack detection networks. Extensive experimental results demonstrate that the proposed CrackCue significantly improves the generalization ability and robustness of the baseline methods. The source code will be publicly available.

</details>


### [6] [SIA: Enhancing Safety via Intent Awareness for Vision-Language Models](https://arxiv.org/abs/2507.16856)
*Youngjin Na,Sangheon Jeong,Youngwan Lee*

Main category: cs.CV

TL;DR: 本文提出SIA（Safety via Intent Awareness）框架，通过三阶段推理过程（视觉抽象、意图推理、响应优化）来检测和缓解视觉-语言模型中的潜在有害意图，在多个安全基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着视觉-语言模型在现实应用中的广泛部署，图像和文本的微妙交互产生了新的安全风险。看似无害的输入组合可能揭示有害意图，导致不安全的模型响应。现有的基于后置过滤或静态拒绝提示的方法难以检测这种潜在风险，特别是当有害性仅从输入组合中出现时。

Method: 提出SIA（Safety via Intent Awareness）框架，这是一个无需训练的提示工程框架，采用三阶段推理过程：(1)通过字幕生成进行视觉抽象；(2)通过少样本思维链提示进行意图推理；(3)基于意图的响应优化。SIA不依赖预定义规则或分类器，而是动态适应从图像-文本对中推断出的隐式意图。

Result: 在SIUO、MM-SafetyBench和HoliSafe等安全关键基准测试中进行了广泛实验，SIA实现了显著的安全性改进，超越了先前的方法。虽然SIA在MMStar通用推理准确性上略有下降，但相应的安全性提升突出了意图感知推理的价值。

Conclusion: SIA框架通过意图感知推理有效提升了视觉-语言模型的安全性，能够动态检测和缓解多模态输入中的有害意图。尽管在一般推理任务上有轻微性能下降，但安全性的显著提升证明了该方法在使VLMs与人类价值观保持一致方面的重要价值。

Abstract: As vision-language models (VLMs) are increasingly deployed in real-world applications, new safety risks arise from the subtle interplay between images and text. In particular, seemingly innocuous inputs can combine to reveal harmful intent, leading to unsafe model responses. Despite increasing attention to multimodal safety, previous approaches based on post hoc filtering or static refusal prompts struggle to detect such latent risks, especially when harmfulness emerges only from the combination of inputs. We propose SIA (Safety via Intent Awareness), a training-free prompt engineering framework that proactively detects and mitigates harmful intent in multimodal inputs. SIA employs a three-stage reasoning process: (1) visual abstraction via captioning, (2) intent inference through few-shot chain-of-thought prompting, and (3) intent-conditioned response refinement. Rather than relying on predefined rules or classifiers, SIA dynamically adapts to the implicit intent inferred from the image-text pair. Through extensive experiments on safety-critical benchmarks including SIUO, MM-SafetyBench, and HoliSafe, we demonstrate that SIA achieves substantial safety improvements, outperforming prior methods. Although SIA shows a minor reduction in general reasoning accuracy on MMStar, the corresponding safety gains highlight the value of intent-aware reasoning in aligning VLMs with human-centric values.

</details>


### [7] [Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less Local Than Assumed](https://arxiv.org/abs/2507.16880)
*Antoni Kowalczuk,Dominik Hintersdorf,Lukas Struppek,Kristian Kersting,Adam Dziedzic,Franziska Boenisch*

Main category: cs.CV

TL;DR: 该研究揭示了文本到图像扩散模型中基于权重剪枝的记忆化缓解方法存在脆弱性，提出了一种对抗性微调方法来增强模型的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型可能无意中记忆和复制训练数据，引发数据隐私和知识产权问题。现有的权重剪枝缓解方法基于记忆化可以局部化的假设，但其鲁棒性有待验证

Method: 评估基于剪枝方法的鲁棒性，通过微调文本嵌入来测试是否能重新触发数据复制；挑战记忆化局部性假设，展示复制可从文本嵌入空间的不同位置触发；提出一种新的对抗性微调方法，迭代搜索复制触发器并更新模型以增强鲁棒性

Result: 证明了即使在剪枝后，对输入提示的文本嵌入进行微小调整仍足以重新触发数据复制；发现复制可以从文本嵌入空间的不同位置触发，并在模型中遵循不同路径；现有缓解策略不足以解决记忆化问题

Conclusion: 现有的基于剪枝的缓解策略是脆弱和不充分的，需要真正移除记忆化内容而非仅仅抑制其检索的方法。提出的对抗性微调方法为构建更可信和合规的生成式AI提供了基础，为理解文本到图像扩散模型中记忆化的本质提供了新见解

Abstract: Text-to-image diffusion models (DMs) have achieved remarkable success in image generation. However, concerns about data privacy and intellectual property remain due to their potential to inadvertently memorize and replicate training data. Recent mitigation efforts have focused on identifying and pruning weights responsible for triggering replication, based on the assumption that memorization can be localized. Our research assesses the robustness of these pruning-based approaches. We demonstrate that even after pruning, minor adjustments to text embeddings of input prompts are sufficient to re-trigger data replication, highlighting the fragility of these defenses. Furthermore, we challenge the fundamental assumption of memorization locality, by showing that replication can be triggered from diverse locations within the text embedding space, and follows different paths in the model. Our findings indicate that existing mitigation strategies are insufficient and underscore the need for methods that truly remove memorized content, rather than attempting to suppress its retrieval. As a first step in this direction, we introduce a novel adversarial fine-tuning method that iteratively searches for replication triggers and updates the model to increase robustness. Through our research, we provide fresh insights into the nature of memorization in text-to-image DMs and a foundation for building more trustworthy and compliant generative AI.

</details>


### [8] [Sparser2Sparse: Single-shot Sparser-to-Sparse Learning for Spatial Transcriptomics Imputation with Natural Image Co-learning](https://arxiv.org/abs/2507.16886)
*Yaoyu Fang,Jiahe Qian,Xinkun Wang,Lee A. Cooper,Bo Zhou*

Main category: cs.CV

TL;DR: 提出了一种名为S2S-ST的空间转录组学数据插补框架，仅需要单个稀疏采样的ST数据集和自然图像进行协同训练，就能实现准确的高分辨率ST数据重建，显著降低了对昂贵高分辨率数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学技术虽然实现了组织内高分辨率基因表达谱分析的革命性突破，但高成本和高分辨率ST数据稀缺性仍是重大挑战，需要开发能从稀疏数据中准确重建高分辨率ST信息的方法。

Method: 提出S2S-ST框架，集成三个关键创新：(1)利用ST数据内在空间模式的稀疏到稀疏自监督学习策略；(2)与自然图像的跨域协同学习来增强特征表示；(3)级联数据一致性插补网络(CDCIN)，迭代优化预测结果同时保持采样基因数据的保真度。

Result: 在乳腺癌、肝脏和淋巴组织等多种组织类型上的广泛实验表明，该方法在插补准确性方面优于现有最先进的方法，能够从稀疏输入中实现鲁棒的ST重建。

Conclusion: 该框架通过实现从稀疏输入的鲁棒ST重建，显著减少了对昂贵高分辨率数据的依赖，有助于促进空间转录组学技术在生物医学研究和临床应用中的更广泛采用。

Abstract: Spatial transcriptomics (ST) has revolutionized biomedical research by enabling high resolution gene expression profiling within tissues. However, the high cost and scarcity of high resolution ST data remain significant challenges. We present Single-shot Sparser-to-Sparse (S2S-ST), a novel framework for accurate ST imputation that requires only a single and low-cost sparsely sampled ST dataset alongside widely available natural images for co-training. Our approach integrates three key innovations: (1) a sparser-to-sparse self-supervised learning strategy that leverages intrinsic spatial patterns in ST data, (2) cross-domain co-learning with natural images to enhance feature representation, and (3) a Cascaded Data Consistent Imputation Network (CDCIN) that iteratively refines predictions while preserving sampled gene data fidelity. Extensive experiments on diverse tissue types, including breast cancer, liver, and lymphoid tissue, demonstrate that our method outperforms state-of-the-art approaches in imputation accuracy. By enabling robust ST reconstruction from sparse inputs, our framework significantly reduces reliance on costly high resolution data, facilitating potential broader adoption in biomedical research and clinical applications.

</details>


### [9] [Bringing Balance to Hand Shape Classification: Mitigating Data Imbalance Through Generative Models](https://arxiv.org/abs/2507.17008)
*Gaston Gustavo Rios,Pedro Dal Bianco,Franco Ronchetti,Facundo Quiroga,Oscar Stanchi,Santiago Ponte Ahón,Waldo Hasperué*

Main category: cs.CV

TL;DR: 该研究通过生成对抗网络(GAN)生成合成手势数据来增强手语手势分类器的训练数据，在小规模且不平衡的RWTH德语手语数据集上实现了5%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 手语手势数据集通常规模小且严重不平衡，这给有效的模型训练带来了重大挑战。研究者希望通过生成合成数据来增强训练数据，以提高手势分类器的性能。

Method: 使用EfficientNet分类器在RWTH德语手语手势数据集上进行训练，并比较两种生成对抗网络架构：ReACGAN（通过辅助分类器使用标签信息条件化数据生成）和SPADE（利用空间自适应归一化基于姿态信息条件化生成）来生成合成图像数据。

Result: 提出的技术将RWTH数据集上的当前最先进准确率提高了5%。此外，该方法还展示了跨不同手语数据集的泛化能力，通过在大规模HaGRID数据集上训练的基于姿态的生成方法，在无需重新训练生成器的情况下实现了与单源训练分类器相当的性能。

Conclusion: 通过使用生成对抗网络生成合成手势数据，可以有效解决小规模和不平衡数据集的局限性，显著提升手语手势分类的准确性，并且具有良好的跨数据集泛化能力。

Abstract: Most sign language handshape datasets are severely limited and unbalanced, posing significant challenges to effective model training. In this paper, we explore the effectiveness of augmenting the training data of a handshape classifier by generating synthetic data. We use an EfficientNet classifier trained on the RWTH German sign language handshape dataset, which is small and heavily unbalanced, applying different strategies to combine generated and real images. We compare two Generative Adversarial Networks (GAN) architectures for data generation: ReACGAN, which uses label information to condition the data generation process through an auxiliary classifier, and SPADE, which utilizes spatially-adaptive normalization to condition the generation on pose information. ReACGAN allows for the generation of realistic images that align with specific handshape labels, while SPADE focuses on generating images with accurate spatial handshape configurations. Our proposed techniques improve the current state-of-the-art accuracy on the RWTH dataset by 5%, addressing the limitations of small and unbalanced datasets. Additionally, our method demonstrates the capability to generalize across different sign language datasets by leveraging pose-based generation trained on the extensive HaGRID dataset. We achieve comparable performance to single-source trained classifiers without the need for retraining the generator.

</details>


### [10] [FedVLM: Scalable Personalized Vision-Language Models through Federated Learning](https://arxiv.org/abs/2507.17088)
*Arkajyoti Mitra,Afia Anjum,Paul Agbaje,Mert Pesé,Habeeb Olufowobi*

Main category: cs.CV

TL;DR: 本文提出FedVLM框架，通过个性化LoRA(pLoRA)在联邦学习环境中高效微调视觉-语言模型，在非独立同分布数据下显著提升客户端特定性能24.5%


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型在联邦环境中的大规模微调面临挑战，特别是在数据分散且非独立同分布的情况下，现有的参数高效调优方法如LoRA在异构客户端数据上表现不佳，泛化能力欠佳

Method: 提出FedVLM联邦LoRA微调框架，并引入个性化LoRA(pLoRA)方法，该方法能够动态适应每个客户端独特的数据分布，在保持全局模型聚合的同时显著改善本地适应性

Result: 在RLAIF-V数据集上的实验表明，pLoRA相比标准LoRA在客户端特定性能上提升了24.5%，在非独立同分布设置下展现出优越的适应能力

Conclusion: FedVLM为联邦环境中微调视觉-语言模型提供了可扩展且高效的解决方案，推进了分布式学习场景中的个性化适应技术发展

Abstract: Vision-language models (VLMs) demonstrate impressive zero-shot and few-shot learning capabilities, making them essential for several downstream tasks. However, fine-tuning these models at scale remains challenging, particularly in federated environments where data is decentralized and non-iid across clients. Existing parameter-efficient tuning methods like LoRA (Low-Rank Adaptation) reduce computational overhead but struggle with heterogeneous client data, leading to suboptimal generalization. To address these challenges, we propose FedVLM, a federated LoRA fine-tuning framework that enables decentralized adaptation of VLMs while preserving model privacy and reducing reliance on centralized training. To further tackle data heterogeneity, we introduce personalized LoRA (pLoRA), which dynamically adapts LoRA parameters to each client's unique data distribution, significantly improving local adaptation while maintaining global model aggregation. Experiments on the RLAIF-V dataset show that pLoRA improves client-specific performance by 24.5% over standard LoRA, demonstrating superior adaptation in non-iid settings. FedVLM provides a scalable and efficient solution for fine-tuning VLMs in federated settings, advancing personalized adaptation in distributed learning scenarios.

</details>


### [11] [UNICE: Training A Universal Image Contrast Enhancer](https://arxiv.org/abs/2507.17157)
*Ruodai Cui,Lei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一个通用的图像对比度增强方法UNICE，通过收集HDR原始图像并渲染生成多曝光序列，训练网络从单张sRGB图像生成多曝光序列并融合成增强图像，在多种对比度增强任务中展现出优异的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图像对比度增强方法通常针对特定任务设计（如曝光不足/过度校正、低光和背光图像增强等），学习到的模型在不同任务间甚至同一任务的不同数据集间泛化性能较差，因此需要探索能够适用于各种对比度增强任务的通用泛化模型。

Method: 作者观察到这些任务的共同关键因素在于需要曝光和对比度调整，如果有高动态范围(HDR)输入则可以很好地解决。因此收集了46,928张HDR原始图像，渲染328,496张sRGB图像构建多曝光序列(MES)和相应的伪sRGB真值。训练一个网络从单张sRGB图像生成MES，然后训练另一个网络将生成的MES融合成增强图像。

Result: UNICE方法无需昂贵的人工标注，在不同任务间和任务内都展现出比现有图像对比度增强方法更强的泛化性能，在多个无参考图像质量指标上甚至优于人工创建的真值。

Conclusion: 通过利用HDR数据构建多曝光序列的方法，可以训练出具有强泛化能力的通用图像对比度增强模型，该模型在各种对比度增强任务中都能取得优异表现，为图像增强领域提供了新的解决思路。

Abstract: Existing image contrast enhancement methods are typically designed for specific tasks such as under-/over-exposure correction, low-light and backlit image enhancement, etc. The learned models, however, exhibit poor generalization performance across different tasks, even across different datasets of a specific task. It is important to explore whether we can learn a universal and generalized model for various contrast enhancement tasks. In this work, we observe that the common key factor of these tasks lies in the need of exposure and contrast adjustment, which can be well-addressed if high-dynamic range (HDR) inputs are available. We hence collect 46,928 HDR raw images from public sources, and render 328,496 sRGB images to build multi-exposure sequences (MES) and the corresponding pseudo sRGB ground-truths via multi-exposure fusion. Consequently, we train a network to generate an MES from a single sRGB image, followed by training another network to fuse the generated MES into an enhanced image. Our proposed method, namely UNiversal Image Contrast Enhancer (UNICE), is free of costly human labeling. However, it demonstrates significantly stronger generalization performance than existing image contrast enhancement methods across and within different tasks, even outperforming manually created ground-truths in multiple no-reference image quality metrics. The dataset, code and model are available at https://github.com/BeyondHeaven/UNICE.

</details>


### [12] [Vec2Face+ for Face Dataset Generation](https://arxiv.org/abs/2507.17192)
*Haiyu Wu,Jaskirat Singh,Sicong Tian,Liang Zheng,Kevin W. Bowyer*

Main category: cs.CV

TL;DR: 本文提出Vec2Face+生成模型，通过维持类内身份一致性的同时增加类间可分性和类内属性变化，生成高质量人脸识别训练数据集，在多个真实测试集上超越了真实数据集CASIA-WebFace的性能。


<details>
  <summary>Details</summary>
Motivation: 现有人脸识别数据合成方法在增加类内变化时忽略了维持类内身份一致性的必要性，导致合成数据质量不佳。需要一种能够在保证身份一致性的前提下，同时实现大的类间可分性和类内属性变化的生成模型。

Method: 提出Vec2Face+生成模型，直接从图像特征创建图像并允许连续且简单的人脸身份和属性控制。采用三种策略：1）采样充分不同的向量生成良好分离的身份；2）提出AttrOP算法增加一般属性变化；3）提出基于LoRA的姿态控制生成侧面头部姿态图像。

Result: 生成的VFace10K数据集（包含1万个身份）使人脸识别模型在7个真实测试集上达到最先进精度。扩展到400万和1200万图像的VFace100K和VFace300K数据集在5个真实测试集上的准确率超过了真实训练数据集CASIA-WebFace，这是首次合成数据集在平均准确率上击败CASIA-WebFace。

Conclusion: Vec2Face+成功生成了高质量的合成人脸数据集，在多个基准测试中超越真实数据集性能。然而，研究还发现合成数据集在双胞胎验证任务中表现较差，且用合成身份训练的模型比用真实身份训练的模型存在更大偏差，这些问题值得未来深入研究。

Abstract: When synthesizing identities as face recognition training data, it is generally believed that large inter-class separability and intra-class attribute variation are essential for synthesizing a quality dataset. % This belief is generally correct, and this is what we aim for. However, when increasing intra-class variation, existing methods overlook the necessity of maintaining intra-class identity consistency. % To address this and generate high-quality face training data, we propose Vec2Face+, a generative model that creates images directly from image features and allows for continuous and easy control of face identities and attributes. Using Vec2Face+, we obtain datasets with proper inter-class separability and intra-class variation and identity consistency using three strategies: 1) we sample vectors sufficiently different from others to generate well-separated identities; 2) we propose an AttrOP algorithm for increasing general attribute variations; 3) we propose LoRA-based pose control for generating images with profile head poses, which is more efficient and identity-preserving than AttrOP. % Our system generates VFace10K, a synthetic face dataset with 10K identities, which allows an FR model to achieve state-of-the-art accuracy on seven real-world test sets. Scaling the size to 4M and 12M images, the corresponding VFace100K and VFace300K datasets yield higher accuracy than the real-world training dataset, CASIA-WebFace, on five real-world test sets. This is the first time a synthetic dataset beats the CASIA-WebFace in average accuracy. In addition, we find that only 1 out of 11 synthetic datasets outperforms random guessing (\emph{i.e., 50\%}) in twin verification and that models trained with synthetic identities are more biased than those trained with real identities. Both are important aspects for future investigation.

</details>


### [13] [Unsupervised Exposure Correction](https://arxiv.org/abs/2507.17252)
*Ruodai Cui,Li Niu,Guosheng Hu*

Main category: cs.CV

TL;DR: 本文提出了一种无监督曝光校正(UEC)方法，无需手动标注数据，通过模拟ISP管道生成训练数据，在保持图像细节的同时超越了有监督方法的性能，且仅使用0.01%的参数量


<details>
  <summary>Details</summary>
Motivation: 现有曝光校正方法存在三个挑战：需要大量人工标注配对数据、泛化能力有限、在低级计算机视觉任务中性能下降。这些问题限制了曝光校正技术的实际应用和效果

Method: 提出无监督曝光校正(UEC)方法，利用模拟图像信号处理(ISP)管道生成的免费配对数据进行训练；构建大规模辐射校正数据集强调曝光变化以促进无监督学习；开发保持图像细节的变换函数

Result: 该方法在保持图像细节方面表现优异，超越了最先进的有监督方法，同时仅使用0.01%的参数量；在边缘检测等下游任务中证明了其有效性，能够减轻曝光不良对低级特征的不利影响

Conclusion: 无监督曝光校正方法成功解决了传统方法的三个主要挑战，无需手动标注即可实现优异性能，具有更好的泛化能力，并在低级计算机视觉任务中表现出色，为曝光校正领域提供了新的解决方案

Abstract: Current exposure correction methods have three challenges, labor-intensive paired data annotation, limited generalizability, and performance degradation in low-level computer vision tasks. In this work, we introduce an innovative Unsupervised Exposure Correction (UEC) method that eliminates the need for manual annotations, offers improved generalizability, and enhances performance in low-level downstream tasks. Our model is trained using freely available paired data from an emulated Image Signal Processing (ISP) pipeline. This approach does not need expensive manual annotations, thereby minimizing individual style biases from the annotation and consequently improving its generalizability. Furthermore, we present a large-scale Radiometry Correction Dataset, specifically designed to emphasize exposure variations, to facilitate unsupervised learning. In addition, we develop a transformation function that preserves image details and outperforms state-of-the-art supervised methods [12], while utilizing only 0.01% of their parameters. Our work further investigates the broader impact of exposure correction on downstream tasks, including edge detection, demonstrating its effectiveness in mitigating the adverse effects of poor exposure on low-level features. The source code and dataset are publicly available at https://github.com/BeyondHeaven/uec_code.

</details>


### [14] [PolarAnything: Diffusion-based Polarimetric Image Synthesis](https://arxiv.org/abs/2507.17268)
*Kailong Zhang,Youwei Lyu,Heng Guo,Si Li,Zhanyu Ma,Boxin Shi*

Main category: cs.CV

TL;DR: 提出了PolarAnything方法，能够从单张RGB图像生成具有真实感和物理准确性的偏振图像，解决了偏振相机获取困难和现有模拟器需要大量3D资产的问题


<details>
  <summary>Details</summary>
Motivation: 偏振图像在图像增强和3D重建任务中很有用，但偏振相机的获取困难限制了其广泛应用。现有的偏振模拟器Mitsuba依赖参数化偏振图像形成模型，需要大量包含形状和PBR材质的3D资产，无法生成大规模真实感图像

Method: 基于扩散模型的生成框架，利用预训练扩散模型的零样本性能，引入有效的表示策略来保持偏振属性的保真度，从单张RGB输入合成偏振图像

Result: 模型能够生成高质量的偏振图像，并支持偏振形状重建等下游任务，在真实感和物理准确性方面表现良好

Conclusion: PolarAnything成功解决了偏振图像合成的挑战，消除了对3D资产集合的依赖，为偏振成像技术的广泛应用提供了新的解决方案

Abstract: Polarization images facilitate image enhancement and 3D reconstruction tasks, but the limited accessibility of polarization cameras hinders their broader application. This gap drives the need for synthesizing photorealistic polarization images.The existing polarization simulator Mitsuba relies on a parametric polarization image formation model and requires extensive 3D assets covering shape and PBR materials, preventing it from generating large-scale photorealistic images. To address this problem, we propose PolarAnything, capable of synthesizing polarization images from a single RGB input with both photorealism and physical accuracy, eliminating the dependency on 3D asset collections. Drawing inspiration from the zero-shot performance of pretrained diffusion models, we introduce a diffusion-based generative framework with an effective representation strategy that preserves the fidelity of polarization properties. Experiments show that our model generates high-quality polarization images and supports downstream tasks like shape from polarization.

</details>


### [15] [PARTE: Part-Guided Texturing for 3D Human Reconstruction from a Single Image](https://arxiv.org/abs/2507.17332)
*Hyeongjin Nam,Donghwan Kim,Gyeongsik Moon,Kyoung Mu Lee*

Main category: cs.CV

TL;DR: PARTE是一个利用3D人体部位信息指导单张图像3D人体纹理重建的框架，通过部位分割模块和部位引导纹理模块解决现有方法中人体不同部位纹理错位的问题


<details>
  <summary>Details</summary>
Motivation: 现有3D人体重建方法存在人体不同部位纹理错位的主要限制，每个人体部位（如夹克或裤子）应保持独特纹理而不与其他部位混合，但大多数现有方法没有明确利用部位分割先验，导致重建中纹理错位

Method: 提出PARTE框架，包含两个核心组件：1）3D部位分割模块（PartSegmenter），从单张图像推断3D人体部位信息，首先重建无纹理人体表面并基于该表面预测人体部位标签；2）部位引导纹理模块（PartTexturer），将部位信息融入纹理重建，从预训练图像生成网络获取人体部位纹理对齐的先验知识

Result: 广泛实验表明该框架在3D人体重建质量方面达到了最先进水平

Conclusion: PARTE通过明确利用3D人体部位信息作为关键指导，成功解决了3D人体纹理重建中的部位纹理错位问题，实现了最先进的重建质量

Abstract: The misaligned human texture across different human parts is one of the main limitations of existing 3D human reconstruction methods. Each human part, such as a jacket or pants, should maintain a distinct texture without blending into others. The structural coherence of human parts serves as a crucial cue to infer human textures in the invisible regions of a single image. However, most existing 3D human reconstruction methods do not explicitly exploit such part segmentation priors, leading to misaligned textures in their reconstructions. In this regard, we present PARTE, which utilizes 3D human part information as a key guide to reconstruct 3D human textures. Our framework comprises two core components. First, to infer 3D human part information from a single image, we propose a 3D part segmentation module (PartSegmenter) that initially reconstructs a textureless human surface and predicts human part labels based on the textureless surface. Second, to incorporate part information into texture reconstruction, we introduce a part-guided texturing module (PartTexturer), which acquires prior knowledge from a pre-trained image generation network on texture alignment of human parts. Extensive experiments demonstrate that our framework achieves state-of-the-art quality in 3D human reconstruction. The project page is available at https://hygenie1228.github.io/PARTE/.

</details>


### [16] [Temporal Point-Supervised Signal Reconstruction: A Human-Annotation-Free Framework for Weak Moving Target Detection](https://arxiv.org/abs/2507.17334)
*Weihua Gao,Chunxu Ren,Wenlong Niu,Xiaodong Peng*

Main category: cs.CV

TL;DR: 提出了一种无需人工标注的时间点监督框架(TPS)，通过时间信号重建网络(TSRNet)检测低空监视系统中的弱运动目标，在保持高检测性能的同时实现了超过1000 FPS的实时处理速度


<details>
  <summary>Details</summary>
Motivation: 在低空监视和预警系统中，由于信号能量低、空间范围小和复杂背景杂波的影响，弱运动目标检测面临重大挑战。现有方法在提取鲁棒特征方面存在困难，且缺乏可靠的标注数据

Method: 提出时间点监督(TPS)框架，将任务重新定义为像素级时间信号建模问题，将弱目标表征为短时脉冲响应。开发了时间信号重建网络(TSRNet)，采用编码器-解码器架构并集成动态多尺度注意力(DMSAttention)模块。同时采用基于图的轨迹挖掘策略来抑制虚警并确保时间一致性

Result: 在专门构建的低信噪比数据集上的大量实验表明，该框架在不需要人工标注的情况下优于最先进的方法，实现了强检测性能并以超过1000 FPS的速度运行

Conclusion: 该框架成功解决了弱运动目标检测中的标注依赖问题，通过时间信号建模和动态注意力机制实现了高性能检测，同时保证了实时处理能力，具有在实际场景中部署的潜力

Abstract: In low-altitude surveillance and early warning systems, detecting weak moving targets remains a significant challenge due to low signal energy, small spatial extent, and complex background clutter. Existing methods struggle with extracting robust features and suffer from the lack of reliable annotations. To address these limitations, we propose a novel Temporal Point-Supervised (TPS) framework that enables high-performance detection of weak targets without any manual annotations.Instead of conventional frame-based detection, our framework reformulates the task as a pixel-wise temporal signal modeling problem, where weak targets manifest as short-duration pulse-like responses. A Temporal Signal Reconstruction Network (TSRNet) is developed under the TPS paradigm to reconstruct these transient signals.TSRNet adopts an encoder-decoder architecture and integrates a Dynamic Multi-Scale Attention (DMSAttention) module to enhance its sensitivity to diverse temporal patterns. Additionally, a graph-based trajectory mining strategy is employed to suppress false alarms and ensure temporal consistency.Extensive experiments on a purpose-built low-SNR dataset demonstrate that our framework outperforms state-of-the-art methods while requiring no human annotations. It achieves strong detection performance and operates at over 1000 FPS, underscoring its potential for real-time deployment in practical scenarios.

</details>


### [17] [Exploring Active Learning for Label-Efficient Training of Semantic Neural Radiance Field](https://arxiv.org/abs/2507.17351)
*Yuzhe Zhu,Lile Cai,Kangkang Lu,Fayao Liu,Xulei Yang*

Main category: cs.CV

TL;DR: 本文提出了一种主动学习方法来减少语义感知NeRF训练所需的像素级标注成本，通过考虑3D几何约束的新颖选择策略，相比随机采样实现了超过2倍的标注成本降低。


<details>
  <summary>Details</summary>
Motivation: 语义感知NeRF训练通常需要像素级类别标签，这种标注成本过于昂贵。为了降低标注负担，作者探索主动学习作为潜在解决方案。

Method: 研究了语义感知NeRF主动学习的各种设计选择，包括选择粒度和选择策略。提出了一种考虑3D几何约束的新颖主动学习策略进行样本选择。

Result: 实验表明主动学习能够有效减少训练语义感知NeRF的标注成本，相比随机采样实现了超过2倍的标注成本降低。

Conclusion: 主动学习是减少语义感知NeRF训练标注负担的有效方法，特别是结合3D几何约束的选择策略能够显著提高标注效率。

Abstract: Neural Radiance Field (NeRF) models are implicit neural scene representation methods that offer unprecedented capabilities in novel view synthesis. Semantically-aware NeRFs not only capture the shape and radiance of a scene, but also encode semantic information of the scene. The training of semantically-aware NeRFs typically requires pixel-level class labels, which can be prohibitively expensive to collect. In this work, we explore active learning as a potential solution to alleviate the annotation burden. We investigate various design choices for active learning of semantically-aware NeRF, including selection granularity and selection strategies. We further propose a novel active learning strategy that takes into account 3D geometric constraints in sample selection. Our experiments demonstrate that active learning can effectively reduce the annotation cost of training semantically-aware NeRF, achieving more than 2X reduction in annotation cost compared to random sampling.

</details>


### [18] [EndoGen: Conditional Autoregressive Endoscopic Video Generation](https://arxiv.org/abs/2507.17388)
*Xinyu Liu,Hengyu Liu,Cheng Wang,Tianming Liu,Yixuan Yuan*

Main category: cs.CV

TL;DR: 本文提出了首个条件内镜视频生成框架EndoGen，通过时空网格帧模式化策略和语义感知令牌掩码机制，实现高质量的条件引导内镜视频生成，并在息肉分割等下游任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有内镜视频生成方法要么局限于静态图像缺乏动态上下文，要么依赖无条件生成无法为临床医生提供有意义的参考，因此需要开发一个能够生成高质量条件引导内镜视频的框架来推进医学影像和增强诊断能力。

Method: 构建了一个自回归模型，采用定制的时空网格帧模式化(SGP)策略将多帧生成学习重新表述为基于网格的图像生成模式，充分利用自回归架构的全局依赖建模能力；提出语义感知令牌掩码(SAT)机制，通过在生成过程中选择性关注语义有意义的区域来增强模型产生丰富多样内容的能力。

Result: 通过广泛实验证明了该框架在生成高质量条件引导内镜内容方面的有效性，并提高了息肉分割等下游任务的性能表现。

Conclusion: EndoGen作为首个条件内镜视频生成框架，通过SGP策略和SAT机制成功实现了高质量的条件引导内镜视频生成，为医学影像领域的视频生成任务提供了新的解决方案，并在实际临床应用中展现出良好的潜力。

Abstract: Endoscopic video generation is crucial for advancing medical imaging and enhancing diagnostic capabilities. However, prior efforts in this field have either focused on static images, lacking the dynamic context required for practical applications, or have relied on unconditional generation that fails to provide meaningful references for clinicians. Therefore, in this paper, we propose the first conditional endoscopic video generation framework, namely EndoGen. Specifically, we build an autoregressive model with a tailored Spatiotemporal Grid-Frame Patterning (SGP) strategy. It reformulates the learning of generating multiple frames as a grid-based image generation pattern, which effectively capitalizes the inherent global dependency modeling capabilities of autoregressive architectures. Furthermore, we propose a Semantic-Aware Token Masking (SAT) mechanism, which enhances the model's ability to produce rich and diverse content by selectively focusing on semantically meaningful regions during the generation process. Through extensive experiments, we demonstrate the effectiveness of our framework in generating high-quality, conditionally guided endoscopic content, and improves the performance of downstream task of polyp segmentation. Code released at https://www.github.com/CUHK-AIM-Group/EndoGen.

</details>


### [19] [ERMV: Editing 4D Robotic Multi-view images to enhance embodied agents](https://arxiv.org/abs/2507.17462)
*Chang Nie,Guangming Wang,Zhe Lie,Hesheng Wang*

Main category: cs.CV

TL;DR: 提出了ERMV框架，用于编辑机器人4D多视角序列数据进行数据增强，通过极线运动感知注意力机制、稀疏时空模块和反馈干预机制解决时空一致性、计算效率和语义完整性问题，显著提升VLA模型的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 机器人模仿学习依赖4D多视角序列图像，但数据收集成本高、高质量数据稀缺严重制约了VLA等具身智能策略的泛化和应用。虽然数据增强是解决数据稀缺的有效策略，但目前缺乏针对操作任务的4D多视角序列图像编辑方法。

Method: 提出ERMV数据增强框架，包含三个核心创新：(1)极线运动感知注意力机制(EMA-Attn)，在应用几何约束前学习运动引起的像素偏移以确保时空一致性；(2)稀疏时空模块(STT)，解耦时间和空间视图，通过稀疏采样将问题重建为单帧多视角问题以降低计算需求；(3)反馈干预机制，使用多模态大语言模型检查编辑不一致性并在必要时请求专家指导。

Result: 大量实验表明，ERMV增强的数据显著提升了VLA模型在仿真和真实环境中的鲁棒性和泛化能力。该框架成功解决了动态视图和长时间跨度下的几何和外观一致性维护、低计算成本的工作窗口扩展，以及机器人臂等关键对象的语义完整性保证等核心挑战。

Conclusion: ERMV框架为机器人4D多视角序列数据编辑提供了有效解决方案，通过创新的注意力机制、时空解耦模块和智能反馈机制，成功解决了数据增强中的关键技术挑战，为提升具身智能模型的性能和应用范围提供了重要技术支撑。

Abstract: Robot imitation learning relies on 4D multi-view sequential images. However, the high cost of data collection and the scarcity of high-quality data severely constrain the generalization and application of embodied intelligence policies like Vision-Language-Action (VLA) models. Data augmentation is a powerful strategy to overcome data scarcity, but methods for editing 4D multi-view sequential images for manipulation tasks are currently lacking. Thus, we propose ERMV (Editing Robotic Multi-View 4D data), a novel data augmentation framework that efficiently edits an entire multi-view sequence based on single-frame editing and robot state conditions. This task presents three core challenges: (1) maintaining geometric and appearance consistency across dynamic views and long time horizons; (2) expanding the working window with low computational costs; and (3) ensuring the semantic integrity of critical objects like the robot arm. ERMV addresses these challenges through a series of innovations. First, to ensure spatio-temporal consistency in motion blur, we introduce a novel Epipolar Motion-Aware Attention (EMA-Attn) mechanism that learns pixel shift caused by movement before applying geometric constraints. Second, to maximize the editing working window, ERMV pioneers a Sparse Spatio-Temporal (STT) module, which decouples the temporal and spatial views and remodels a single-frame multi-view problem through sparse sampling of the views to reduce computational demands. Third, to alleviate error accumulation, we incorporate a feedback intervention Mechanism, which uses a Multimodal Large Language Model (MLLM) to check editing inconsistencies and request targeted expert guidance only when necessary. Extensive experiments demonstrate that ERMV-augmented data significantly boosts the robustness and generalization of VLA models in both simulated and real-world environments.

</details>


### [20] [SRMambaV2: Biomimetic Attention for Sparse Point Cloud Upsampling in Autonomous Driving](https://arxiv.org/abs/2507.17479)
*Chuang Chen,Xiaolin Qin,Jing Hu,Wenyi Ge*

Main category: cs.CV

TL;DR: 本文提出了SRMambaV2方法，通过仿生2D选择性扫描自注意力机制和双分支网络架构，解决自动驾驶场景中LiDAR点云上采样的稀疏性和复杂3D结构重建问题


<details>
  <summary>Details</summary>
Motivation: 自动驾驶场景中LiDAR点云数据固有的稀疏性和复杂3D结构使得上采样任务面临重大挑战。现有方法将3D空间场景转换为2D图像超分辨率任务，但由于距离图像的稀疏和模糊特征表示，准确重建详细复杂的空间拓扑结构仍然困难

Method: 提出SRMambaV2稀疏点云上采样方法，包含三个核心组件：1）受人类驾驶员视觉感知启发的仿生2D选择性扫描自注意力(2DSSA)机制，用于建模远距离稀疏区域的特征分布；2）双分支网络架构增强稀疏特征表示；3）渐进式自适应损失(PAL)函数优化上采样过程中细粒度细节的重建

Result: 实验结果表明SRMambaV2在定性和定量评估中都取得了优越的性能，在提升长距离稀疏区域上采样精度的同时保持了整体几何重建质量

Conclusion: SRMambaV2方法有效解决了汽车稀疏点云上采样任务中的关键问题，展现了其在实际应用中的有效性和实用价值

Abstract: Upsampling LiDAR point clouds in autonomous driving scenarios remains a significant challenge due to the inherent sparsity and complex 3D structures of the data. Recent studies have attempted to address this problem by converting the complex 3D spatial scenes into 2D image super-resolution tasks. However, due to the sparse and blurry feature representation of range images, accurately reconstructing detailed and complex spatial topologies remains a major difficulty. To tackle this, we propose a novel sparse point cloud upsampling method named SRMambaV2, which enhances the upsampling accuracy in long-range sparse regions while preserving the overall geometric reconstruction quality. Specifically, inspired by human driver visual perception, we design a biomimetic 2D selective scanning self-attention (2DSSA) mechanism to model the feature distribution in distant sparse areas. Meanwhile, we introduce a dual-branch network architecture to enhance the representation of sparse features. In addition, we introduce a progressive adaptive loss (PAL) function to further refine the reconstruction of fine-grained details during the upsampling process. Experimental results demonstrate that SRMambaV2 achieves superior performance in both qualitative and quantitative evaluations, highlighting its effectiveness and practical value in automotive sparse point cloud upsampling tasks.

</details>


### [21] [Unsupervised anomaly detection using Bayesian flow networks: application to brain FDG PET in the context of Alzheimer's disease](https://arxiv.org/abs/2507.17486)
*Hugues Roy,Reuben Dorent,Ninon Burgos*

Main category: cs.CV

TL;DR: 本文提出了AnoBFN，一种基于贝叶斯流网络的无监督异常检测方法，用于神经影像中的阿尔茨海默病相关异常检测，在FDG PET图像上表现优于现有的VAE、GAN和扩散模型方法。


<details>
  <summary>Details</summary>
Motivation: 无监督异常检测在神经影像学中对于识别健康受试者数据的偏差并促进神经系统疾病诊断具有重要作用。贝叶斯流网络作为一种新颖的生成模型类别，尚未应用于医学影像或异常检测领域，具有结合扩散框架和贝叶斯推理优势的潜力。

Method: 提出AnoBFN，这是贝叶斯流网络在无监督异常检测上的扩展。该方法设计用于：1）在高水平空间相关噪声条件下执行条件图像生成；2）通过在整个生成过程中融入来自输入图像的递归反馈来保持受试者特异性。

Result: 在FDG PET图像的阿尔茨海默病相关异常检测这一具有挑战性的任务上进行评估，AnoBFN方法优于基于VAE（beta-VAE）、GAN（f-AnoGAN）和扩散模型（AnoDDPM）的其他最先进方法。

Conclusion: AnoBFN在检测异常方面表现出有效性，同时降低了假阳性率，证明了贝叶斯流网络在医学影像异常检测领域的应用潜力和优越性能。

Abstract: Unsupervised anomaly detection (UAD) plays a crucial role in neuroimaging for identifying deviations from healthy subject data and thus facilitating the diagnosis of neurological disorders. In this work, we focus on Bayesian flow networks (BFNs), a novel class of generative models, which have not yet been applied to medical imaging or anomaly detection. BFNs combine the strength of diffusion frameworks and Bayesian inference. We introduce AnoBFN, an extension of BFNs for UAD, designed to: i) perform conditional image generation under high levels of spatially correlated noise, and ii) preserve subject specificity by incorporating a recursive feedback from the input image throughout the generative process. We evaluate AnoBFN on the challenging task of Alzheimer's disease-related anomaly detection in FDG PET images. Our approach outperforms other state-of-the-art methods based on VAEs (beta-VAE), GANs (f-AnoGAN), and diffusion models (AnoDDPM), demonstrating its effectiveness at detecting anomalies while reducing false positive rates.

</details>


### [22] [DFDNet: Dynamic Frequency-Guided De-Flare Network](https://arxiv.org/abs/2507.17489)
*Minglong Xue,Aoxiang Ning,Shivakumara Palaiahnakote,Mingliang Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种动态频域引导的去眩光网络(DFDNet)，通过在频域解耦内容信息和眩光伪影，有效去除夜间摄影中的大规模眩光伪影并修复光源附近的结构损伤。


<details>
  <summary>Details</summary>
Motivation: 夜间摄影中的强光源经常产生眩光，严重影响图像视觉质量和下游任务性能。现有方法在去除大规模眩光伪影和修复光源附近结构损伤方面仍存在困难。观察发现这些具有挑战性的眩光伪影在频域比空域表现出与参考图像更显著的差异。

Method: 提出动态频域引导去眩光网络(DFDNet)，主要包含全局动态频域引导(GDFG)模块和局部细节引导模块(LDGM)。GDFG模块通过动态优化全局频域特征引导网络感知眩光伪影的频率特性，有效分离眩光信息和内容信息。LDGM通过对比学习策略将光源局部特征与参考图像对齐，减少去眩光过程中的局部细节损伤。

Result: 实验结果表明，所提出的方法在性能上优于现有的最先进方法，能够有效去除大规模眩光伪影并改善细粒度图像恢复效果。

Conclusion: DFDNet通过在频域解耦内容和眩光信息，结合全局频域引导和局部细节优化，成功解决了夜间摄影中大规模眩光去除的难题，为图像去眩光领域提供了新的有效解决方案。

Abstract: Strong light sources in nighttime photography frequently produce flares in images, significantly degrading visual quality and impacting the performance of downstream tasks. While some progress has been made, existing methods continue to struggle with removing large-scale flare artifacts and repairing structural damage in regions near the light source. We observe that these challenging flare artifacts exhibit more significant discrepancies from the reference images in the frequency domain compared to the spatial domain. Therefore, this paper presents a novel dynamic frequency-guided deflare network (DFDNet) that decouples content information from flare artifacts in the frequency domain, effectively removing large-scale flare artifacts. Specifically, DFDNet consists mainly of a global dynamic frequency-domain guidance (GDFG) module and a local detail guidance module (LDGM). The GDFG module guides the network to perceive the frequency characteristics of flare artifacts by dynamically optimizing global frequency domain features, effectively separating flare information from content information. Additionally, we design an LDGM via a contrastive learning strategy that aligns the local features of the light source with the reference image, reduces local detail damage from flare removal, and improves fine-grained image restoration. The experimental results demonstrate that the proposed method outperforms existing state-of-the-art methods in terms of performance. The code is available at \href{https://github.com/AXNing/DFDNet}{https://github.com/AXNing/DFDNet}.

</details>


### [23] [Accelerating Parallel Diffusion Model Serving with Residual Compression](https://arxiv.org/abs/2507.17511)
*Jiajun Luo,Yicheng Xiao,Jianru Xu,Yangxiu You,Rongwei Lu,Chen Tang,Jingyan Jiang,Zhi Wang*

Main category: cs.CV

TL;DR: CompactFusion是一个针对扩散模型并行推理的压缩框架，通过残差压缩技术减少设备间通信开销，在保持生成质量的同时实现3.0-6.7倍的加速效果。


<details>
  <summary>Details</summary>
Motivation: 扩散模型需要大量计算资源并依赖多加速器并行来实现实时部署，但并行推理过程中设备间交换大型激活数据会产生巨大的通信开销，限制了效率和可扩展性。现有方法存在带宽饱和问题，传输大量冗余的近似重复数据。

Method: 提出CompactFusion压缩框架，核心是残差压缩（Residual Compression）技术，仅传输压缩后的残差（步骤间激活差异）而非完整激活数据。利用扩散激活的强时间冗余性，相邻步骤产生高度相似的激活。同时集成轻量级错误反馈机制防止误差累积。

Result: 在4xL20配置下实现3.0倍加速，同时显著提升生成保真度。在慢速网络上支持序列并行等通信密集策略，相比先前重叠方法实现6.7倍加速。框架广泛适用于不同扩散模型和并行设置，易于集成且无需重新设计流水线。

Conclusion: CompactFusion为并行扩散推理建立了新的范式，通过有效的残差压缩技术解决了通信瓶颈问题，在保持高生成质量的同时实现了显著的性能提升。该方法具有良好的通用性和实用性，已在xDiT上实现并开源。

Abstract: Diffusion models produce realistic images and videos but require substantial computational resources, necessitating multi-accelerator parallelism for real-time deployment. However, parallel inference introduces significant communication overhead from exchanging large activations between devices, limiting efficiency and scalability. We present CompactFusion, a compression framework that significantly reduces communication while preserving generation quality. Our key observation is that diffusion activations exhibit strong temporal redundancy-adjacent steps produce highly similar activations, saturating bandwidth with near-duplicate data carrying little new information. To address this inefficiency, we seek a more compact representation that encodes only the essential information. CompactFusion achieves this via Residual Compression that transmits only compressed residuals (step-wise activation differences). Based on empirical analysis and theoretical justification, we show that it effectively removes redundant data, enabling substantial data reduction while maintaining high fidelity. We also integrate lightweight error feedback to prevent error accumulation. CompactFusion establishes a new paradigm for parallel diffusion inference, delivering lower latency and significantly higher generation quality than prior methods. On 4xL20, it achieves 3.0x speedup while greatly improving fidelity. It also uniquely supports communication-heavy strategies like sequence parallelism on slow networks, achieving 6.7x speedup over prior overlap-based method. CompactFusion applies broadly across diffusion models and parallel settings, and integrates easily without requiring pipeline rework. Portable implementation demonstrated on xDiT is publicly available at https://github.com/Cobalt-27/CompactFusion

</details>


### [24] [An h-space Based Adversarial Attack for Protection Against Few-shot Personalization](https://arxiv.org/abs/2507.17554)
*Xide Xu,Sandesh Kamath,Muhammad Atif Butt,Bogdan Raducanu*

Main category: cs.CV

TL;DR: 本文提出了HAAD和HAAD-KV两种基于h空间的对抗攻击方法，通过在扩散模型的语义潜在空间中构造扰动来防止未经授权的图像定制，有效保护隐私内容免受恶意修改。


<details>
  <summary>Details</summary>
Motivation: 扩散模型能够从少量样本生成定制化图像，这引发了严重的隐私担忧，特别是对私人内容的未经授权修改。现有的基于对抗攻击的保护机制需要改进，因为扩散模型在其语义潜在空间（h空间）中表现出高度抽象性，该空间编码了生成连贯有意义内容的关键高级特征。

Method: 提出HAAD（基于h空间的扩散模型对抗攻击）方法，利用对抗攻击在h空间中构造扰动来有效降解图像生成过程。进一步提出更高效的变体HAAD-KV，仅基于h空间的KV参数构造扰动，提供更强的保护且计算成本更低。

Result: HAAD和HAAD-KV方法尽管简单，但在性能上超越了最先进的对抗攻击方法，证明了其有效性。HAAD-KV变体在提供更强保护的同时，计算开销更小。

Conclusion: 基于h空间的对抗攻击方法能够有效防止扩散模型的未经授权定制，为保护隐私内容提供了一种简单而有效的解决方案。HAAD-KV作为更高效的变体，在保护强度和计算效率之间取得了良好平衡。

Abstract: The versatility of diffusion models in generating customized images from few samples raises significant privacy concerns, particularly regarding unauthorized modifications of private content. This concerning issue has renewed the efforts in developing protection mechanisms based on adversarial attacks, which generate effective perturbations to poison diffusion models. Our work is motivated by the observation that these models exhibit a high degree of abstraction within their semantic latent space (`h-space'), which encodes critical high-level features for generating coherent and meaningful content. In this paper, we propose a novel anti-customization approach, called HAAD (h-space based Adversarial Attack for Diffusion models), that leverages adversarial attacks to craft perturbations based on the h-space that can efficiently degrade the image generation process. Building upon HAAD, we further introduce a more efficient variant, HAAD-KV, that constructs perturbations solely based on the KV parameters of the h-space. This strategy offers a stronger protection, that is computationally less expensive. Despite their simplicity, our methods outperform state-of-the-art adversarial attacks, highlighting their effectiveness.

</details>


### [25] [Dual-branch Prompting for Multimodal Machine Translation](https://arxiv.org/abs/2507.17588)
*Jie Wang,Zhendong Yang,Liansong Zong,Xiaobo Zhang,Dexian Wang,Ji Zhang*

Main category: cs.CV

TL;DR: 提出D2P-MMT，一个基于扩散模型的双分支提示框架，通过重构图像而非原始图像进行视觉引导翻译，在Multi30K数据集上取得了优于现有方法的性能


<details>
  <summary>Details</summary>
Motivation: 现有多模态机器翻译方法在推理时需要配对的图像-文本输入，且对无关视觉噪声敏感，限制了其鲁棒性和实际应用性

Method: 提出D2P-MMT框架：1）使用预训练扩散模型生成重构图像，自然过滤干扰视觉细节；2）采用双分支提示策略，同时从真实图像和重构图像学习；3）引入分布对齐损失，确保两个分支输出分布的一致性

Result: 在Multi30K数据集上的大量实验表明，D2P-MMT在翻译性能上优于现有的最先进方法

Conclusion: D2P-MMT通过使用扩散模型重构图像和双分支训练策略，有效解决了多模态机器翻译中的视觉噪声敏感性问题，提高了模型的鲁棒性和翻译质量

Abstract: Multimodal Machine Translation (MMT) typically enhances text-only translation by incorporating aligned visual features. Despite the remarkable progress, state-of-the-art MMT approaches often rely on paired image-text inputs at inference and are sensitive to irrelevant visual noise, which limits their robustness and practical applicability. To address these issues, we propose D2P-MMT, a diffusion-based dual-branch prompting framework for robust vision-guided translation. Specifically, D2P-MMT requires only the source text and a reconstructed image generated by a pre-trained diffusion model, which naturally filters out distracting visual details while preserving semantic cues. During training, the model jointly learns from both authentic and reconstructed images using a dual-branch prompting strategy, encouraging rich cross-modal interactions. To bridge the modality gap and mitigate training-inference discrepancies, we introduce a distributional alignment loss that enforces consistency between the output distributions of the two branches. Extensive experiments on the Multi30K dataset demonstrate that D2P-MMT achieves superior translation performance compared to existing state-of-the-art approaches.

</details>


### [26] [CNS-Bench: Benchmarking Image Classifier Robustness Under Continuous Nuisance Shifts](https://arxiv.org/abs/2507.17651)
*Olaf Dünkel,Artur Jesslen,Jiahao Xie,Christian Theobalt,Christian Rupprecht,Adam Kortylewski*

Main category: cs.CV

TL;DR: 本文提出CNS-Bench，一个连续扰动偏移基准测试，用于量化图像分类器在连续且真实的生成式扰动偏移下的分布外(OOD)鲁棒性，并通过大规模研究评估了40多个分类器的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的合成损坏测试往往无法捕捉真实世界中发生的扰动偏移，而基于扩散模型的方法又局限于二元扰动偏移，缺乏对连续扰动变化的评估能力，因此需要开发能够生成连续且真实扰动偏移的基准测试。

Method: 通过将LoRA适配器应用于扩散模型来生成连续严重程度的各种单独扰动偏移，提出过滤机制来解决生成失败案例，建立CNS-Bench连续扰动偏移基准测试平台。

Result: 对40多个分类器进行大规模研究发现，模型排名会随着不同的偏移类型和偏移尺度而变化，这是传统二元偏移无法捕捉的；连续尺度评估能够识别模型失效点，提供更细致的鲁棒性理解。

Conclusion: CNS-Bench为评估图像分类器的OOD鲁棒性提供了更全面和现实的基准测试方法，揭示了连续扰动评估相比二元偏移的优势，能够更准确地识别模型的失效边界和鲁棒性特征。

Abstract: An important challenge when using computer vision models in the real world is to evaluate their performance in potential out-of-distribution (OOD) scenarios. While simple synthetic corruptions are commonly applied to test OOD robustness, they often fail to capture nuisance shifts that occur in the real world. Recently, diffusion models have been applied to generate realistic images for benchmarking, but they are restricted to binary nuisance shifts. In this work, we introduce CNS-Bench, a Continuous Nuisance Shift Benchmark to quantify OOD robustness of image classifiers for continuous and realistic generative nuisance shifts. CNS-Bench allows generating a wide range of individual nuisance shifts in continuous severities by applying LoRA adapters to diffusion models. To address failure cases, we propose a filtering mechanism that outperforms previous methods, thereby enabling reliable benchmarking with generative models. With the proposed benchmark, we perform a large-scale study to evaluate the robustness of more than 40 classifiers under various nuisance shifts. Through carefully designed comparisons and analyses, we find that model rankings can change for varying shifts and shift scales, which cannot be captured when applying common binary shifts. Additionally, we show that evaluating the model performance on a continuous scale allows the identification of model failure points, providing a more nuanced understanding of model robustness. Project page including code and data: https://genintel.github.io/CNS.

</details>


### [27] [Attention (as Discrete-Time Markov) Chains](https://arxiv.org/abs/2507.17657)
*Yotam Erel,Olaf Dünkel,Rishabh Dabral,Vladislav Golyanik,Christian Theobalt,Amit H. Bermano*

Main category: cs.CV

TL;DR: 本文提出将注意力矩阵解释为离散时间马尔可夫链，通过分析元稳定状态实现零样本分割，并定义TokenRank衡量全局token重要性来改进图像生成


<details>
  <summary>Details</summary>
Motivation: 现有注意力机制研究只关注直接效应，缺乏统一框架来理解注意力分数的选择、求和和平均操作，需要新的视角来理解现代视觉transformer中token的注意力分布模式

Method: 将注意力矩阵建模为离散时间马尔可夫链，通过矩阵乘法和特征分析计算元稳定状态及其流行度，考虑通过马尔可夫链传播的间接注意力，定义TokenRank作为马尔可夫链的稳态向量

Result: 在零样本分割任务上达到最先进性能，在无条件图像生成任务中使用TokenRank带来性能提升，证明了语义相似区域对应的token形成元稳定状态而噪声注意力分数趋于分散

Conclusion: 马尔可夫链框架为理解现代视觉transformer中token注意力机制提供了新颖且有效的视角，通过轻量级工具实现了多个下游任务的性能提升

Abstract: We introduce a new interpretation of the attention matrix as a discrete-time Markov chain. Our interpretation sheds light on common operations involving attention scores such as selection, summation, and averaging in a unified framework. It further extends them by considering indirect attention, propagated through the Markov chain, as opposed to previous studies that only model immediate effects. Our main observation is that tokens corresponding to semantically similar regions form a set of metastable states, where the attention clusters, while noisy attention scores tend to disperse. Metastable states and their prevalence can be easily computed through simple matrix multiplication and eigenanalysis, respectively. Using these lightweight tools, we demonstrate state-of-the-art zero-shot segmentation. Lastly, we define TokenRank -- the steady state vector of the Markov chain, which measures global token importance. We demonstrate that using it brings improvements in unconditional image generation. We believe our framework offers a fresh view of how tokens are being attended in modern visual transformers.

</details>


### [28] [Yume: An Interactive World Generation Model](https://arxiv.org/abs/2507.17744)
*Xiaofeng Mao,Shaoheng Lin,Zhen Li,Chuanhao Li,Wenshuo Peng,Tong He,Jiangmiao Pang,Mingmin Chi,Yu Qiao,Kaipeng Zhang*

Main category: cs.CV

TL;DR: Yume是一个从单张图像创建可交互动态世界的系统，用户可以通过键盘操作探索生成的视频世界，采用了相机运动量化、视频生成架构、高级采样器和模型加速等技术组件。


<details>
  <summary>Details</summary>
Motivation: 现有技术无法很好地从静态图像创建可交互、真实且动态的世界，缺乏允许用户通过外设或神经信号进行探索和控制的能力，因此需要开发一个高保真度的交互式视频世界生成系统。

Method: 提出了包含四个主要组件的框架：1）相机运动量化，用于稳定训练和用户友好的键盘交互；2）带记忆模块的掩码视频扩散变换器(MVDT)，用于自回归方式的无限视频生成；3）无训练的反伪影机制(AAM)和基于随机微分方程的时间旅行采样(TTS-SDE)；4）通过对抗蒸馏和缓存机制的协同优化实现模型加速。

Result: 在高质量世界探索数据集Sekai上训练后，Yume在多样化场景和应用中取得了显著效果，能够从输入图像创建动态世界并允许用户通过键盘操作进行探索。

Conclusion: 成功开发了Yume系统的预览版本，实现了从静态图像生成可交互动态视频世界的目标，所有数据、代码库和模型权重已开源，系统将每月更新以实现最终目标。

Abstract: Yume aims to use images, text, or videos to create an interactive, realistic, and dynamic world, which allows exploration and control using peripheral devices or neural signals. In this report, we present a preview version of \method, which creates a dynamic world from an input image and allows exploration of the world using keyboard actions. To achieve this high-fidelity and interactive video world generation, we introduce a well-designed framework, which consists of four main components, including camera motion quantization, video generation architecture, advanced sampler, and model acceleration. First, we quantize camera motions for stable training and user-friendly interaction using keyboard inputs. Then, we introduce the Masked Video Diffusion Transformer~(MVDT) with a memory module for infinite video generation in an autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM) and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE) are introduced to the sampler for better visual quality and more precise control. Moreover, we investigate model acceleration by synergistic optimization of adversarial distillation and caching mechanisms. We use the high-quality world exploration dataset \sekai to train \method, and it achieves remarkable results in diverse scenes and applications. All data, codebase, and model weights are available on https://github.com/stdstu12/YUME. Yume will update monthly to achieve its original goal. Project page: https://stdstu12.github.io/YUME-Project/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [29] [SADA: Stability-guided Adaptive Diffusion Acceleration](https://arxiv.org/abs/2507.17135)
*Ting Jiang,Yixiao Wang,Hancheng Ye,Zishan Shao,Jingwei Sun,Jingyang Zhang,Zekai Chen,Jianyi Zhang,Yiran Chen,Hai Li*

Main category: cs.LG

TL;DR: 提出了SADA（稳定性引导的自适应扩散加速）方法，通过统一步骤和token级别的稀疏决策来加速ODE基础生成模型的采样，在保持高保真度的同时实现了至少1.8倍的加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型虽然在生成任务中表现出色，但由于迭代采样过程和二次注意力成本导致计算开销很高。现有的无训练加速策略虽然能减少采样时间，但与原始基线相比保真度较低。作者假设这种保真度差距是因为：(a)不同提示对应不同的去噪轨迹，(b)这些方法没有考虑底层ODE公式及其数值解。

Method: 提出SADA方法，通过单一稳定性准则统一步骤级和token级的稀疏决策来加速基于ODE的生成模型采样。针对问题(a)，SADA根据采样轨迹自适应分配稀疏性；针对问题(b)，SADA引入了利用数值ODE求解器精确梯度信息的原理性近似方案。

Result: 在SD-2、SDXL和Flux上使用EDM和DPM++求解器的综合评估显示，与未修改的基线相比，SADA实现了一致的≥1.8倍加速，同时保持最小的保真度下降（LPIPS≤0.10，FID≤4.5），显著优于先前方法。SADA还能无缝适应其他管道和模态：无需任何修改即可加速ControlNet，并以约0.01的谱图LPIPS将MusicLDM加速1.8倍。

Conclusion: SADA通过稳定性引导的自适应稀疏策略，成功解决了扩散模型加速中的保真度问题，实现了显著的计算效率提升，同时保持了生成质量，并展现出良好的跨模态适应性。

Abstract: Diffusion models have achieved remarkable success in generative tasks but suffer from high computational costs due to their iterative sampling process and quadratic attention costs. Existing training-free acceleration strategies that reduce per-step computation cost, while effectively reducing sampling time, demonstrate low faithfulness compared to the original baseline. We hypothesize that this fidelity gap arises because (a) different prompts correspond to varying denoising trajectory, and (b) such methods do not consider the underlying ODE formulation and its numerical solution. In this paper, we propose Stability-guided Adaptive Diffusion Acceleration (SADA), a novel paradigm that unifies step-wise and token-wise sparsity decisions via a single stability criterion to accelerate sampling of ODE-based generative models (Diffusion and Flow-matching). For (a), SADA adaptively allocates sparsity based on the sampling trajectory. For (b), SADA introduces principled approximation schemes that leverage the precise gradient information from the numerical ODE solver. Comprehensive evaluations on SD-2, SDXL, and Flux using both EDM and DPM++ solvers reveal consistent $\ge 1.8\times$ speedups with minimal fidelity degradation (LPIPS $\leq 0.10$ and FID $\leq 4.5$) compared to unmodified baselines, significantly outperforming prior methods. Moreover, SADA adapts seamlessly to other pipelines and modalities: It accelerates ControlNet without any modifications and speeds up MusicLDM by $1.8\times$ with $\sim 0.01$ spectrogram LPIPS.

</details>
