{"id": "2507.16869", "pdf": "https://arxiv.org/pdf/2507.16869", "abs": "https://arxiv.org/abs/2507.16869", "authors": ["Yue Ma", "Kunyu Feng", "Zhongyuan Hu", "Xinyu Wang", "Yucheng Wang", "Mingzhe Zheng", "Xuanhua He", "Chenyang Zhu", "Hongyu Liu", "Yingqing He", "Zeyu Wang", "Zhifeng Li", "Xiu Li", "Wei Liu", "Dan Xu", "Linfeng Zhang", "Qifeng Chen"], "title": "Controllable Video Generation: A Survey", "categories": ["cs.GR", "cs.CV"], "comment": "project page:   https://github.com/mayuelala/Awesome-Controllable-Video-Generation", "summary": "With the rapid development of AI-generated content (AIGC), video generation has emerged as one of its most dynamic and impactful subfields. In particular, the advancement of video generation foundation models has led to growing demand for controllable video generation methods that can more accurately reflect user intent. Most existing foundation models are designed for text-to-video generation, where text prompts alone are often insufficient to express complex, multi-modal, and fine-grained user requirements. This limitation makes it challenging for users to generate videos with precise control using current models. To address this issue, recent research has explored the integration of additional non-textual conditions, such as camera motion, depth maps, and human pose, to extend pretrained video generation models and enable more controllable video synthesis. These approaches aim to enhance the flexibility and practical applicability of AIGC-driven video generation systems. In this survey, we provide a systematic review of controllable video generation, covering both theoretical foundations and recent advances in the field. We begin by introducing the key concepts and commonly used open-source video generation models. We then focus on control mechanisms in video diffusion models, analyzing how different types of conditions can be incorporated into the denoising process to guide generation. Finally, we categorize existing methods based on the types of control signals they leverage, including single-condition generation, multi-condition generation, and universal controllable generation. For a complete list of the literature on controllable video generation reviewed, please visit our curated repository at https://github.com/mayuelala/Awesome-Controllable-Video-Generation."}
{"id": "2507.17029", "pdf": "https://arxiv.org/pdf/2507.17029", "abs": "https://arxiv.org/abs/2507.17029", "authors": ["Luchuan Song", "Yang Zhou", "Zhan Xu", "Yi Zhou", "Deepali Aneja", "Chenliang Xu"], "title": "StreamME: Simplify 3D Gaussian Avatar within Live Stream", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "12 pages, 15 Figures", "summary": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The StreamME synchronously records and reconstructs a head avatar from live video streams without any pre-cached data, enabling seamless integration of the reconstructed appearance into downstream applications. This exceptionally fast training strategy, which we refer to as on-the-fly training, is central to our approach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating the reliance on MLPs in deformable 3DGS and relying solely on geometry, which significantly improves the adaptation speed to facial expression. To further ensure high efficiency in on-the-fly training, we introduced a simplification strategy based on primary points, which distributes the point clouds more sparsely across the facial surface, optimizing points number while maintaining rendering quality. Leveraging the on-the-fly training capabilities, our method protects the facial privacy and reduces communication bandwidth in VR system or online conference. Additionally, it can be directly applied to downstream application such as animation, toonify, and relighting. Please refer to our project page for more details: https://songluchuan.github.io/StreamME/."}
{"id": "2507.17336", "pdf": "https://arxiv.org/pdf/2507.17336", "abs": "https://arxiv.org/abs/2507.17336", "authors": ["Hyeongmin Lee", "Kyungjune Baek"], "title": "Temporal Smoothness-Aware Rate-Distortion Optimized 4D Gaussian Splatting", "categories": ["cs.GR"], "comment": "21 pages, 10 figures", "summary": "Dynamic 4D Gaussian Splatting (4DGS) effectively extends the high-speed rendering capabilities of 3D Gaussian Splatting (3DGS) to represent volumetric videos. However, the large number of Gaussians, substantial temporal redundancies, and especially the absence of an entropy-aware compression framework result in large storage requirements. Consequently, this poses significant challenges for practical deployment, efficient edge-device processing, and data transmission. In this paper, we introduce a novel end-to-end RD-optimized compression framework tailored for 4DGS, aiming to enable flexible, high-fidelity rendering across varied computational platforms. Leveraging Fully Explicit Dynamic Gaussian Splatting (Ex4DGS), one of the state-of-the-art 4DGS methods, as our baseline, we start from the existing 3DGS compression methods for compatibility while effectively addressing additional challenges introduced by the temporal axis. In particular, instead of storing motion trajectories independently per point, we employ a wavelet transform to reflect the real-world smoothness prior, significantly enhancing storage efficiency. This approach yields significantly improved compression ratios and provides a user-controlled balance between compression efficiency and rendering quality. Extensive experiments demonstrate the effectiveness of our method, achieving up to 91x compression compared to the original Ex4DGS model while maintaining high visual fidelity. These results highlight the applicability of our framework for real-time dynamic scene rendering in diverse scenarios, from resource-constrained edge devices to high-performance environments."}
{"id": "2507.16851", "pdf": "https://arxiv.org/pdf/2507.16851", "abs": "https://arxiv.org/abs/2507.16851", "authors": ["Zelong Liu", "Yuliang Gu", "Zhichao Sun", "Huachao Zhu", "Xin Xiao", "Bo Du", "Laurent Najman", "Yongchao Xu"], "title": "Coarse-to-fine crack cue for robust crack detection", "categories": ["cs.CV", "cs.NE", "eess.IV"], "comment": null, "summary": "Crack detection is an important task in computer vision. Despite impressive in-dataset performance, deep learning-based methods still struggle in generalizing to unseen domains. The thin structure property of cracks is usually overlooked by previous methods. In this work, we introduce CrackCue, a novel method for robust crack detection based on coarse-to-fine crack cue generation. The core concept lies on leveraging the thin structure property to generate a robust crack cue, guiding the crack detection. Specifically, we first employ a simple max-pooling and upsampling operation on the crack image. This results in a coarse crack-free background, based on which a fine crack-free background can be obtained via a reconstruction network. The difference between the original image and fine crack-free background provides a fine crack cue. This fine cue embeds robust crack prior information which is unaffected by complex backgrounds, shadow, and varied lighting. As a plug-and-play method, we incorporate the proposed CrackCue into three advanced crack detection networks. Extensive experimental results demonstrate that the proposed CrackCue significantly improves the generalization ability and robustness of the baseline methods. The source code will be publicly available."}
{"id": "2507.17440", "pdf": "https://arxiv.org/pdf/2507.17440", "abs": "https://arxiv.org/abs/2507.17440", "authors": ["Christoph Schied", "Alexander Keller"], "title": "Parametric Integration with Neural Integral Operators", "categories": ["cs.GR"], "comment": null, "summary": "Real-time rendering imposes strict limitations on the sampling budget for light transport simulation, often resulting in noisy images. However, denoisers have demonstrated that it is possible to produce noise-free images through filtering. We enhance image quality by removing noise before material shading, rather than filtering already shaded noisy images. This approach allows for material-agnostic denoising (MAD) and leverages machine learning by approximating the light transport integral operator with a neural network, effectively performing parametric integration with neural operators. Our method operates in real-time, requires data from only a single frame, seamlessly integrates with existing denoisers and temporal anti-aliasing techniques, and is efficient to train. Additionally, it is straightforward to incorporate with physically based rendering algorithms."}
{"id": "2507.16856", "pdf": "https://arxiv.org/pdf/2507.16856", "abs": "https://arxiv.org/abs/2507.16856", "authors": ["Youngjin Na", "Sangheon Jeong", "Youngwan Lee"], "title": "SIA: Enhancing Safety via Intent Awareness for Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "5 pages, 6 figures", "summary": "As vision-language models (VLMs) are increasingly deployed in real-world applications, new safety risks arise from the subtle interplay between images and text. In particular, seemingly innocuous inputs can combine to reveal harmful intent, leading to unsafe model responses. Despite increasing attention to multimodal safety, previous approaches based on post hoc filtering or static refusal prompts struggle to detect such latent risks, especially when harmfulness emerges only from the combination of inputs. We propose SIA (Safety via Intent Awareness), a training-free prompt engineering framework that proactively detects and mitigates harmful intent in multimodal inputs. SIA employs a three-stage reasoning process: (1) visual abstraction via captioning, (2) intent inference through few-shot chain-of-thought prompting, and (3) intent-conditioned response refinement. Rather than relying on predefined rules or classifiers, SIA dynamically adapts to the implicit intent inferred from the image-text pair. Through extensive experiments on safety-critical benchmarks including SIUO, MM-SafetyBench, and HoliSafe, we demonstrate that SIA achieves substantial safety improvements, outperforming prior methods. Although SIA shows a minor reduction in general reasoning accuracy on MMStar, the corresponding safety gains highlight the value of intent-aware reasoning in aligning VLMs with human-centric values."}
{"id": "2507.16880", "pdf": "https://arxiv.org/pdf/2507.16880", "abs": "https://arxiv.org/abs/2507.16880", "authors": ["Antoni Kowalczuk", "Dominik Hintersdorf", "Lukas Struppek", "Kristian Kersting", "Adam Dziedzic", "Franziska Boenisch"], "title": "Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less Local Than Assumed", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Text-to-image diffusion models (DMs) have achieved remarkable success in image generation. However, concerns about data privacy and intellectual property remain due to their potential to inadvertently memorize and replicate training data. Recent mitigation efforts have focused on identifying and pruning weights responsible for triggering replication, based on the assumption that memorization can be localized. Our research assesses the robustness of these pruning-based approaches. We demonstrate that even after pruning, minor adjustments to text embeddings of input prompts are sufficient to re-trigger data replication, highlighting the fragility of these defenses. Furthermore, we challenge the fundamental assumption of memorization locality, by showing that replication can be triggered from diverse locations within the text embedding space, and follows different paths in the model. Our findings indicate that existing mitigation strategies are insufficient and underscore the need for methods that truly remove memorized content, rather than attempting to suppress its retrieval. As a first step in this direction, we introduce a novel adversarial fine-tuning method that iteratively searches for replication triggers and updates the model to increase robustness. Through our research, we provide fresh insights into the nature of memorization in text-to-image DMs and a foundation for building more trustworthy and compliant generative AI."}
{"id": "2507.16886", "pdf": "https://arxiv.org/pdf/2507.16886", "abs": "https://arxiv.org/abs/2507.16886", "authors": ["Yaoyu Fang", "Jiahe Qian", "Xinkun Wang", "Lee A. Cooper", "Bo Zhou"], "title": "Sparser2Sparse: Single-shot Sparser-to-Sparse Learning for Spatial Transcriptomics Imputation with Natural Image Co-learning", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages, 5 figure, under review", "summary": "Spatial transcriptomics (ST) has revolutionized biomedical research by enabling high resolution gene expression profiling within tissues. However, the high cost and scarcity of high resolution ST data remain significant challenges. We present Single-shot Sparser-to-Sparse (S2S-ST), a novel framework for accurate ST imputation that requires only a single and low-cost sparsely sampled ST dataset alongside widely available natural images for co-training. Our approach integrates three key innovations: (1) a sparser-to-sparse self-supervised learning strategy that leverages intrinsic spatial patterns in ST data, (2) cross-domain co-learning with natural images to enhance feature representation, and (3) a Cascaded Data Consistent Imputation Network (CDCIN) that iteratively refines predictions while preserving sampled gene data fidelity. Extensive experiments on diverse tissue types, including breast cancer, liver, and lymphoid tissue, demonstrate that our method outperforms state-of-the-art approaches in imputation accuracy. By enabling robust ST reconstruction from sparse inputs, our framework significantly reduces reliance on costly high resolution data, facilitating potential broader adoption in biomedical research and clinical applications."}
{"id": "2507.17008", "pdf": "https://arxiv.org/pdf/2507.17008", "abs": "https://arxiv.org/abs/2507.17008", "authors": ["Gaston Gustavo Rios", "Pedro Dal Bianco", "Franco Ronchetti", "Facundo Quiroga", "Oscar Stanchi", "Santiago Ponte Ah\u00f3n", "Waldo Hasperu\u00e9"], "title": "Bringing Balance to Hand Shape Classification: Mitigating Data Imbalance Through Generative Models", "categories": ["cs.CV", "cs.AI"], "comment": "23 pages, 8 figures, to be published in Applied Soft Computing", "summary": "Most sign language handshape datasets are severely limited and unbalanced, posing significant challenges to effective model training. In this paper, we explore the effectiveness of augmenting the training data of a handshape classifier by generating synthetic data. We use an EfficientNet classifier trained on the RWTH German sign language handshape dataset, which is small and heavily unbalanced, applying different strategies to combine generated and real images. We compare two Generative Adversarial Networks (GAN) architectures for data generation: ReACGAN, which uses label information to condition the data generation process through an auxiliary classifier, and SPADE, which utilizes spatially-adaptive normalization to condition the generation on pose information. ReACGAN allows for the generation of realistic images that align with specific handshape labels, while SPADE focuses on generating images with accurate spatial handshape configurations. Our proposed techniques improve the current state-of-the-art accuracy on the RWTH dataset by 5%, addressing the limitations of small and unbalanced datasets. Additionally, our method demonstrates the capability to generalize across different sign language datasets by leveraging pose-based generation trained on the extensive HaGRID dataset. We achieve comparable performance to single-source trained classifiers without the need for retraining the generator."}
{"id": "2507.17088", "pdf": "https://arxiv.org/pdf/2507.17088", "abs": "https://arxiv.org/abs/2507.17088", "authors": ["Arkajyoti Mitra", "Afia Anjum", "Paul Agbaje", "Mert Pes\u00e9", "Habeeb Olufowobi"], "title": "FedVLM: Scalable Personalized Vision-Language Models through Federated Learning", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) demonstrate impressive zero-shot and few-shot learning capabilities, making them essential for several downstream tasks. However, fine-tuning these models at scale remains challenging, particularly in federated environments where data is decentralized and non-iid across clients. Existing parameter-efficient tuning methods like LoRA (Low-Rank Adaptation) reduce computational overhead but struggle with heterogeneous client data, leading to suboptimal generalization. To address these challenges, we propose FedVLM, a federated LoRA fine-tuning framework that enables decentralized adaptation of VLMs while preserving model privacy and reducing reliance on centralized training. To further tackle data heterogeneity, we introduce personalized LoRA (pLoRA), which dynamically adapts LoRA parameters to each client's unique data distribution, significantly improving local adaptation while maintaining global model aggregation. Experiments on the RLAIF-V dataset show that pLoRA improves client-specific performance by 24.5% over standard LoRA, demonstrating superior adaptation in non-iid settings. FedVLM provides a scalable and efficient solution for fine-tuning VLMs in federated settings, advancing personalized adaptation in distributed learning scenarios."}
{"id": "2507.17157", "pdf": "https://arxiv.org/pdf/2507.17157", "abs": "https://arxiv.org/abs/2507.17157", "authors": ["Ruodai Cui", "Lei Zhang"], "title": "UNICE: Training A Universal Image Contrast Enhancer", "categories": ["cs.CV"], "comment": null, "summary": "Existing image contrast enhancement methods are typically designed for specific tasks such as under-/over-exposure correction, low-light and backlit image enhancement, etc. The learned models, however, exhibit poor generalization performance across different tasks, even across different datasets of a specific task. It is important to explore whether we can learn a universal and generalized model for various contrast enhancement tasks. In this work, we observe that the common key factor of these tasks lies in the need of exposure and contrast adjustment, which can be well-addressed if high-dynamic range (HDR) inputs are available. We hence collect 46,928 HDR raw images from public sources, and render 328,496 sRGB images to build multi-exposure sequences (MES) and the corresponding pseudo sRGB ground-truths via multi-exposure fusion. Consequently, we train a network to generate an MES from a single sRGB image, followed by training another network to fuse the generated MES into an enhanced image. Our proposed method, namely UNiversal Image Contrast Enhancer (UNICE), is free of costly human labeling. However, it demonstrates significantly stronger generalization performance than existing image contrast enhancement methods across and within different tasks, even outperforming manually created ground-truths in multiple no-reference image quality metrics. The dataset, code and model are available at https://github.com/BeyondHeaven/UNICE."}
{"id": "2507.17192", "pdf": "https://arxiv.org/pdf/2507.17192", "abs": "https://arxiv.org/abs/2507.17192", "authors": ["Haiyu Wu", "Jaskirat Singh", "Sicong Tian", "Liang Zheng", "Kevin W. Bowyer"], "title": "Vec2Face+ for Face Dataset Generation", "categories": ["cs.CV"], "comment": null, "summary": "When synthesizing identities as face recognition training data, it is generally believed that large inter-class separability and intra-class attribute variation are essential for synthesizing a quality dataset. % This belief is generally correct, and this is what we aim for. However, when increasing intra-class variation, existing methods overlook the necessity of maintaining intra-class identity consistency. % To address this and generate high-quality face training data, we propose Vec2Face+, a generative model that creates images directly from image features and allows for continuous and easy control of face identities and attributes. Using Vec2Face+, we obtain datasets with proper inter-class separability and intra-class variation and identity consistency using three strategies: 1) we sample vectors sufficiently different from others to generate well-separated identities; 2) we propose an AttrOP algorithm for increasing general attribute variations; 3) we propose LoRA-based pose control for generating images with profile head poses, which is more efficient and identity-preserving than AttrOP. % Our system generates VFace10K, a synthetic face dataset with 10K identities, which allows an FR model to achieve state-of-the-art accuracy on seven real-world test sets. Scaling the size to 4M and 12M images, the corresponding VFace100K and VFace300K datasets yield higher accuracy than the real-world training dataset, CASIA-WebFace, on five real-world test sets. This is the first time a synthetic dataset beats the CASIA-WebFace in average accuracy. In addition, we find that only 1 out of 11 synthetic datasets outperforms random guessing (\\emph{i.e., 50\\%}) in twin verification and that models trained with synthetic identities are more biased than those trained with real identities. Both are important aspects for future investigation."}
{"id": "2507.17252", "pdf": "https://arxiv.org/pdf/2507.17252", "abs": "https://arxiv.org/abs/2507.17252", "authors": ["Ruodai Cui", "Li Niu", "Guosheng Hu"], "title": "Unsupervised Exposure Correction", "categories": ["cs.CV"], "comment": null, "summary": "Current exposure correction methods have three challenges, labor-intensive paired data annotation, limited generalizability, and performance degradation in low-level computer vision tasks. In this work, we introduce an innovative Unsupervised Exposure Correction (UEC) method that eliminates the need for manual annotations, offers improved generalizability, and enhances performance in low-level downstream tasks. Our model is trained using freely available paired data from an emulated Image Signal Processing (ISP) pipeline. This approach does not need expensive manual annotations, thereby minimizing individual style biases from the annotation and consequently improving its generalizability. Furthermore, we present a large-scale Radiometry Correction Dataset, specifically designed to emphasize exposure variations, to facilitate unsupervised learning. In addition, we develop a transformation function that preserves image details and outperforms state-of-the-art supervised methods [12], while utilizing only 0.01% of their parameters. Our work further investigates the broader impact of exposure correction on downstream tasks, including edge detection, demonstrating its effectiveness in mitigating the adverse effects of poor exposure on low-level features. The source code and dataset are publicly available at https://github.com/BeyondHeaven/uec_code."}
{"id": "2507.17268", "pdf": "https://arxiv.org/pdf/2507.17268", "abs": "https://arxiv.org/abs/2507.17268", "authors": ["Kailong Zhang", "Youwei Lyu", "Heng Guo", "Si Li", "Zhanyu Ma", "Boxin Shi"], "title": "PolarAnything: Diffusion-based Polarimetric Image Synthesis", "categories": ["cs.CV"], "comment": "11 pages", "summary": "Polarization images facilitate image enhancement and 3D reconstruction tasks, but the limited accessibility of polarization cameras hinders their broader application. This gap drives the need for synthesizing photorealistic polarization images.The existing polarization simulator Mitsuba relies on a parametric polarization image formation model and requires extensive 3D assets covering shape and PBR materials, preventing it from generating large-scale photorealistic images. To address this problem, we propose PolarAnything, capable of synthesizing polarization images from a single RGB input with both photorealism and physical accuracy, eliminating the dependency on 3D asset collections. Drawing inspiration from the zero-shot performance of pretrained diffusion models, we introduce a diffusion-based generative framework with an effective representation strategy that preserves the fidelity of polarization properties. Experiments show that our model generates high-quality polarization images and supports downstream tasks like shape from polarization."}
{"id": "2507.17332", "pdf": "https://arxiv.org/pdf/2507.17332", "abs": "https://arxiv.org/abs/2507.17332", "authors": ["Hyeongjin Nam", "Donghwan Kim", "Gyeongsik Moon", "Kyoung Mu Lee"], "title": "PARTE: Part-Guided Texturing for 3D Human Reconstruction from a Single Image", "categories": ["cs.CV"], "comment": "Published at ICCV 2025, 22 pages including the supplementary material", "summary": "The misaligned human texture across different human parts is one of the main limitations of existing 3D human reconstruction methods. Each human part, such as a jacket or pants, should maintain a distinct texture without blending into others. The structural coherence of human parts serves as a crucial cue to infer human textures in the invisible regions of a single image. However, most existing 3D human reconstruction methods do not explicitly exploit such part segmentation priors, leading to misaligned textures in their reconstructions. In this regard, we present PARTE, which utilizes 3D human part information as a key guide to reconstruct 3D human textures. Our framework comprises two core components. First, to infer 3D human part information from a single image, we propose a 3D part segmentation module (PartSegmenter) that initially reconstructs a textureless human surface and predicts human part labels based on the textureless surface. Second, to incorporate part information into texture reconstruction, we introduce a part-guided texturing module (PartTexturer), which acquires prior knowledge from a pre-trained image generation network on texture alignment of human parts. Extensive experiments demonstrate that our framework achieves state-of-the-art quality in 3D human reconstruction. The project page is available at https://hygenie1228.github.io/PARTE/."}
{"id": "2507.17334", "pdf": "https://arxiv.org/pdf/2507.17334", "abs": "https://arxiv.org/abs/2507.17334", "authors": ["Weihua Gao", "Chunxu Ren", "Wenlong Niu", "Xiaodong Peng"], "title": "Temporal Point-Supervised Signal Reconstruction: A Human-Annotation-Free Framework for Weak Moving Target Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In low-altitude surveillance and early warning systems, detecting weak moving targets remains a significant challenge due to low signal energy, small spatial extent, and complex background clutter. Existing methods struggle with extracting robust features and suffer from the lack of reliable annotations. To address these limitations, we propose a novel Temporal Point-Supervised (TPS) framework that enables high-performance detection of weak targets without any manual annotations.Instead of conventional frame-based detection, our framework reformulates the task as a pixel-wise temporal signal modeling problem, where weak targets manifest as short-duration pulse-like responses. A Temporal Signal Reconstruction Network (TSRNet) is developed under the TPS paradigm to reconstruct these transient signals.TSRNet adopts an encoder-decoder architecture and integrates a Dynamic Multi-Scale Attention (DMSAttention) module to enhance its sensitivity to diverse temporal patterns. Additionally, a graph-based trajectory mining strategy is employed to suppress false alarms and ensure temporal consistency.Extensive experiments on a purpose-built low-SNR dataset demonstrate that our framework outperforms state-of-the-art methods while requiring no human annotations. It achieves strong detection performance and operates at over 1000 FPS, underscoring its potential for real-time deployment in practical scenarios."}
{"id": "2507.17351", "pdf": "https://arxiv.org/pdf/2507.17351", "abs": "https://arxiv.org/abs/2507.17351", "authors": ["Yuzhe Zhu", "Lile Cai", "Kangkang Lu", "Fayao Liu", "Xulei Yang"], "title": "Exploring Active Learning for Label-Efficient Training of Semantic Neural Radiance Field", "categories": ["cs.CV"], "comment": "Accepted to ICME 2025", "summary": "Neural Radiance Field (NeRF) models are implicit neural scene representation methods that offer unprecedented capabilities in novel view synthesis. Semantically-aware NeRFs not only capture the shape and radiance of a scene, but also encode semantic information of the scene. The training of semantically-aware NeRFs typically requires pixel-level class labels, which can be prohibitively expensive to collect. In this work, we explore active learning as a potential solution to alleviate the annotation burden. We investigate various design choices for active learning of semantically-aware NeRF, including selection granularity and selection strategies. We further propose a novel active learning strategy that takes into account 3D geometric constraints in sample selection. Our experiments demonstrate that active learning can effectively reduce the annotation cost of training semantically-aware NeRF, achieving more than 2X reduction in annotation cost compared to random sampling."}
{"id": "2507.17388", "pdf": "https://arxiv.org/pdf/2507.17388", "abs": "https://arxiv.org/abs/2507.17388", "authors": ["Xinyu Liu", "Hengyu Liu", "Cheng Wang", "Tianming Liu", "Yixuan Yuan"], "title": "EndoGen: Conditional Autoregressive Endoscopic Video Generation", "categories": ["cs.CV", "eess.IV"], "comment": "MICCAI 2025", "summary": "Endoscopic video generation is crucial for advancing medical imaging and enhancing diagnostic capabilities. However, prior efforts in this field have either focused on static images, lacking the dynamic context required for practical applications, or have relied on unconditional generation that fails to provide meaningful references for clinicians. Therefore, in this paper, we propose the first conditional endoscopic video generation framework, namely EndoGen. Specifically, we build an autoregressive model with a tailored Spatiotemporal Grid-Frame Patterning (SGP) strategy. It reformulates the learning of generating multiple frames as a grid-based image generation pattern, which effectively capitalizes the inherent global dependency modeling capabilities of autoregressive architectures. Furthermore, we propose a Semantic-Aware Token Masking (SAT) mechanism, which enhances the model's ability to produce rich and diverse content by selectively focusing on semantically meaningful regions during the generation process. Through extensive experiments, we demonstrate the effectiveness of our framework in generating high-quality, conditionally guided endoscopic content, and improves the performance of downstream task of polyp segmentation. Code released at https://www.github.com/CUHK-AIM-Group/EndoGen."}
{"id": "2507.17462", "pdf": "https://arxiv.org/pdf/2507.17462", "abs": "https://arxiv.org/abs/2507.17462", "authors": ["Chang Nie", "Guangming Wang", "Zhe Lie", "Hesheng Wang"], "title": "ERMV: Editing 4D Robotic Multi-view images to enhance embodied agents", "categories": ["cs.CV"], "comment": null, "summary": "Robot imitation learning relies on 4D multi-view sequential images. However, the high cost of data collection and the scarcity of high-quality data severely constrain the generalization and application of embodied intelligence policies like Vision-Language-Action (VLA) models. Data augmentation is a powerful strategy to overcome data scarcity, but methods for editing 4D multi-view sequential images for manipulation tasks are currently lacking. Thus, we propose ERMV (Editing Robotic Multi-View 4D data), a novel data augmentation framework that efficiently edits an entire multi-view sequence based on single-frame editing and robot state conditions. This task presents three core challenges: (1) maintaining geometric and appearance consistency across dynamic views and long time horizons; (2) expanding the working window with low computational costs; and (3) ensuring the semantic integrity of critical objects like the robot arm. ERMV addresses these challenges through a series of innovations. First, to ensure spatio-temporal consistency in motion blur, we introduce a novel Epipolar Motion-Aware Attention (EMA-Attn) mechanism that learns pixel shift caused by movement before applying geometric constraints. Second, to maximize the editing working window, ERMV pioneers a Sparse Spatio-Temporal (STT) module, which decouples the temporal and spatial views and remodels a single-frame multi-view problem through sparse sampling of the views to reduce computational demands. Third, to alleviate error accumulation, we incorporate a feedback intervention Mechanism, which uses a Multimodal Large Language Model (MLLM) to check editing inconsistencies and request targeted expert guidance only when necessary. Extensive experiments demonstrate that ERMV-augmented data significantly boosts the robustness and generalization of VLA models in both simulated and real-world environments."}
{"id": "2507.17479", "pdf": "https://arxiv.org/pdf/2507.17479", "abs": "https://arxiv.org/abs/2507.17479", "authors": ["Chuang Chen", "Xiaolin Qin", "Jing Hu", "Wenyi Ge"], "title": "SRMambaV2: Biomimetic Attention for Sparse Point Cloud Upsampling in Autonomous Driving", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Upsampling LiDAR point clouds in autonomous driving scenarios remains a significant challenge due to the inherent sparsity and complex 3D structures of the data. Recent studies have attempted to address this problem by converting the complex 3D spatial scenes into 2D image super-resolution tasks. However, due to the sparse and blurry feature representation of range images, accurately reconstructing detailed and complex spatial topologies remains a major difficulty. To tackle this, we propose a novel sparse point cloud upsampling method named SRMambaV2, which enhances the upsampling accuracy in long-range sparse regions while preserving the overall geometric reconstruction quality. Specifically, inspired by human driver visual perception, we design a biomimetic 2D selective scanning self-attention (2DSSA) mechanism to model the feature distribution in distant sparse areas. Meanwhile, we introduce a dual-branch network architecture to enhance the representation of sparse features. In addition, we introduce a progressive adaptive loss (PAL) function to further refine the reconstruction of fine-grained details during the upsampling process. Experimental results demonstrate that SRMambaV2 achieves superior performance in both qualitative and quantitative evaluations, highlighting its effectiveness and practical value in automotive sparse point cloud upsampling tasks."}
{"id": "2507.17486", "pdf": "https://arxiv.org/pdf/2507.17486", "abs": "https://arxiv.org/abs/2507.17486", "authors": ["Hugues Roy", "Reuben Dorent", "Ninon Burgos"], "title": "Unsupervised anomaly detection using Bayesian flow networks: application to brain FDG PET in the context of Alzheimer's disease", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Unsupervised anomaly detection (UAD) plays a crucial role in neuroimaging for identifying deviations from healthy subject data and thus facilitating the diagnosis of neurological disorders. In this work, we focus on Bayesian flow networks (BFNs), a novel class of generative models, which have not yet been applied to medical imaging or anomaly detection. BFNs combine the strength of diffusion frameworks and Bayesian inference. We introduce AnoBFN, an extension of BFNs for UAD, designed to: i) perform conditional image generation under high levels of spatially correlated noise, and ii) preserve subject specificity by incorporating a recursive feedback from the input image throughout the generative process. We evaluate AnoBFN on the challenging task of Alzheimer's disease-related anomaly detection in FDG PET images. Our approach outperforms other state-of-the-art methods based on VAEs (beta-VAE), GANs (f-AnoGAN), and diffusion models (AnoDDPM), demonstrating its effectiveness at detecting anomalies while reducing false positive rates."}
{"id": "2507.17489", "pdf": "https://arxiv.org/pdf/2507.17489", "abs": "https://arxiv.org/abs/2507.17489", "authors": ["Minglong Xue", "Aoxiang Ning", "Shivakumara Palaiahnakote", "Mingliang Zhou"], "title": "DFDNet: Dynamic Frequency-Guided De-Flare Network", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Strong light sources in nighttime photography frequently produce flares in images, significantly degrading visual quality and impacting the performance of downstream tasks. While some progress has been made, existing methods continue to struggle with removing large-scale flare artifacts and repairing structural damage in regions near the light source. We observe that these challenging flare artifacts exhibit more significant discrepancies from the reference images in the frequency domain compared to the spatial domain. Therefore, this paper presents a novel dynamic frequency-guided deflare network (DFDNet) that decouples content information from flare artifacts in the frequency domain, effectively removing large-scale flare artifacts. Specifically, DFDNet consists mainly of a global dynamic frequency-domain guidance (GDFG) module and a local detail guidance module (LDGM). The GDFG module guides the network to perceive the frequency characteristics of flare artifacts by dynamically optimizing global frequency domain features, effectively separating flare information from content information. Additionally, we design an LDGM via a contrastive learning strategy that aligns the local features of the light source with the reference image, reduces local detail damage from flare removal, and improves fine-grained image restoration. The experimental results demonstrate that the proposed method outperforms existing state-of-the-art methods in terms of performance. The code is available at \\href{https://github.com/AXNing/DFDNet}{https://github.com/AXNing/DFDNet}."}
{"id": "2507.17511", "pdf": "https://arxiv.org/pdf/2507.17511", "abs": "https://arxiv.org/abs/2507.17511", "authors": ["Jiajun Luo", "Yicheng Xiao", "Jianru Xu", "Yangxiu You", "Rongwei Lu", "Chen Tang", "Jingyan Jiang", "Zhi Wang"], "title": "Accelerating Parallel Diffusion Model Serving with Residual Compression", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models produce realistic images and videos but require substantial computational resources, necessitating multi-accelerator parallelism for real-time deployment. However, parallel inference introduces significant communication overhead from exchanging large activations between devices, limiting efficiency and scalability. We present CompactFusion, a compression framework that significantly reduces communication while preserving generation quality. Our key observation is that diffusion activations exhibit strong temporal redundancy-adjacent steps produce highly similar activations, saturating bandwidth with near-duplicate data carrying little new information. To address this inefficiency, we seek a more compact representation that encodes only the essential information. CompactFusion achieves this via Residual Compression that transmits only compressed residuals (step-wise activation differences). Based on empirical analysis and theoretical justification, we show that it effectively removes redundant data, enabling substantial data reduction while maintaining high fidelity. We also integrate lightweight error feedback to prevent error accumulation. CompactFusion establishes a new paradigm for parallel diffusion inference, delivering lower latency and significantly higher generation quality than prior methods. On 4xL20, it achieves 3.0x speedup while greatly improving fidelity. It also uniquely supports communication-heavy strategies like sequence parallelism on slow networks, achieving 6.7x speedup over prior overlap-based method. CompactFusion applies broadly across diffusion models and parallel settings, and integrates easily without requiring pipeline rework. Portable implementation demonstrated on xDiT is publicly available at https://github.com/Cobalt-27/CompactFusion"}
{"id": "2507.17554", "pdf": "https://arxiv.org/pdf/2507.17554", "abs": "https://arxiv.org/abs/2507.17554", "authors": ["Xide Xu", "Sandesh Kamath", "Muhammad Atif Butt", "Bogdan Raducanu"], "title": "An h-space Based Adversarial Attack for Protection Against Few-shot Personalization", "categories": ["cs.CV"], "comment": "32 pages, 15 figures. Accepted by ACM Multimedia 2025", "summary": "The versatility of diffusion models in generating customized images from few samples raises significant privacy concerns, particularly regarding unauthorized modifications of private content. This concerning issue has renewed the efforts in developing protection mechanisms based on adversarial attacks, which generate effective perturbations to poison diffusion models. Our work is motivated by the observation that these models exhibit a high degree of abstraction within their semantic latent space (`h-space'), which encodes critical high-level features for generating coherent and meaningful content. In this paper, we propose a novel anti-customization approach, called HAAD (h-space based Adversarial Attack for Diffusion models), that leverages adversarial attacks to craft perturbations based on the h-space that can efficiently degrade the image generation process. Building upon HAAD, we further introduce a more efficient variant, HAAD-KV, that constructs perturbations solely based on the KV parameters of the h-space. This strategy offers a stronger protection, that is computationally less expensive. Despite their simplicity, our methods outperform state-of-the-art adversarial attacks, highlighting their effectiveness."}
{"id": "2507.17588", "pdf": "https://arxiv.org/pdf/2507.17588", "abs": "https://arxiv.org/abs/2507.17588", "authors": ["Jie Wang", "Zhendong Yang", "Liansong Zong", "Xiaobo Zhang", "Dexian Wang", "Ji Zhang"], "title": "Dual-branch Prompting for Multimodal Machine Translation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Multimodal Machine Translation (MMT) typically enhances text-only translation by incorporating aligned visual features. Despite the remarkable progress, state-of-the-art MMT approaches often rely on paired image-text inputs at inference and are sensitive to irrelevant visual noise, which limits their robustness and practical applicability. To address these issues, we propose D2P-MMT, a diffusion-based dual-branch prompting framework for robust vision-guided translation. Specifically, D2P-MMT requires only the source text and a reconstructed image generated by a pre-trained diffusion model, which naturally filters out distracting visual details while preserving semantic cues. During training, the model jointly learns from both authentic and reconstructed images using a dual-branch prompting strategy, encouraging rich cross-modal interactions. To bridge the modality gap and mitigate training-inference discrepancies, we introduce a distributional alignment loss that enforces consistency between the output distributions of the two branches. Extensive experiments on the Multi30K dataset demonstrate that D2P-MMT achieves superior translation performance compared to existing state-of-the-art approaches."}
{"id": "2507.17651", "pdf": "https://arxiv.org/pdf/2507.17651", "abs": "https://arxiv.org/abs/2507.17651", "authors": ["Olaf D\u00fcnkel", "Artur Jesslen", "Jiahao Xie", "Christian Theobalt", "Christian Rupprecht", "Adam Kortylewski"], "title": "CNS-Bench: Benchmarking Image Classifier Robustness Under Continuous Nuisance Shifts", "categories": ["cs.CV"], "comment": "ICCV 2025. Project page: https://genintel.github.io/CNS", "summary": "An important challenge when using computer vision models in the real world is to evaluate their performance in potential out-of-distribution (OOD) scenarios. While simple synthetic corruptions are commonly applied to test OOD robustness, they often fail to capture nuisance shifts that occur in the real world. Recently, diffusion models have been applied to generate realistic images for benchmarking, but they are restricted to binary nuisance shifts. In this work, we introduce CNS-Bench, a Continuous Nuisance Shift Benchmark to quantify OOD robustness of image classifiers for continuous and realistic generative nuisance shifts. CNS-Bench allows generating a wide range of individual nuisance shifts in continuous severities by applying LoRA adapters to diffusion models. To address failure cases, we propose a filtering mechanism that outperforms previous methods, thereby enabling reliable benchmarking with generative models. With the proposed benchmark, we perform a large-scale study to evaluate the robustness of more than 40 classifiers under various nuisance shifts. Through carefully designed comparisons and analyses, we find that model rankings can change for varying shifts and shift scales, which cannot be captured when applying common binary shifts. Additionally, we show that evaluating the model performance on a continuous scale allows the identification of model failure points, providing a more nuanced understanding of model robustness. Project page including code and data: https://genintel.github.io/CNS."}
{"id": "2507.17657", "pdf": "https://arxiv.org/pdf/2507.17657", "abs": "https://arxiv.org/abs/2507.17657", "authors": ["Yotam Erel", "Olaf D\u00fcnkel", "Rishabh Dabral", "Vladislav Golyanik", "Christian Theobalt", "Amit H. Bermano"], "title": "Attention (as Discrete-Time Markov) Chains", "categories": ["cs.CV"], "comment": "Project page: https://yoterel.github.io/attention_chains/", "summary": "We introduce a new interpretation of the attention matrix as a discrete-time Markov chain. Our interpretation sheds light on common operations involving attention scores such as selection, summation, and averaging in a unified framework. It further extends them by considering indirect attention, propagated through the Markov chain, as opposed to previous studies that only model immediate effects. Our main observation is that tokens corresponding to semantically similar regions form a set of metastable states, where the attention clusters, while noisy attention scores tend to disperse. Metastable states and their prevalence can be easily computed through simple matrix multiplication and eigenanalysis, respectively. Using these lightweight tools, we demonstrate state-of-the-art zero-shot segmentation. Lastly, we define TokenRank -- the steady state vector of the Markov chain, which measures global token importance. We demonstrate that using it brings improvements in unconditional image generation. We believe our framework offers a fresh view of how tokens are being attended in modern visual transformers."}
{"id": "2507.17744", "pdf": "https://arxiv.org/pdf/2507.17744", "abs": "https://arxiv.org/abs/2507.17744", "authors": ["Xiaofeng Mao", "Shaoheng Lin", "Zhen Li", "Chuanhao Li", "Wenshuo Peng", "Tong He", "Jiangmiao Pang", "Mingmin Chi", "Yu Qiao", "Kaipeng Zhang"], "title": "Yume: An Interactive World Generation Model", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "Yume aims to use images, text, or videos to create an interactive, realistic, and dynamic world, which allows exploration and control using peripheral devices or neural signals. In this report, we present a preview version of \\method, which creates a dynamic world from an input image and allows exploration of the world using keyboard actions. To achieve this high-fidelity and interactive video world generation, we introduce a well-designed framework, which consists of four main components, including camera motion quantization, video generation architecture, advanced sampler, and model acceleration. First, we quantize camera motions for stable training and user-friendly interaction using keyboard inputs. Then, we introduce the Masked Video Diffusion Transformer~(MVDT) with a memory module for infinite video generation in an autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM) and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE) are introduced to the sampler for better visual quality and more precise control. Moreover, we investigate model acceleration by synergistic optimization of adversarial distillation and caching mechanisms. We use the high-quality world exploration dataset \\sekai to train \\method, and it achieves remarkable results in diverse scenes and applications. All data, codebase, and model weights are available on https://github.com/stdstu12/YUME. Yume will update monthly to achieve its original goal. Project page: https://stdstu12.github.io/YUME-Project/."}
{"id": "2507.16869", "pdf": "https://arxiv.org/pdf/2507.16869", "abs": "https://arxiv.org/abs/2507.16869", "authors": ["Yue Ma", "Kunyu Feng", "Zhongyuan Hu", "Xinyu Wang", "Yucheng Wang", "Mingzhe Zheng", "Xuanhua He", "Chenyang Zhu", "Hongyu Liu", "Yingqing He", "Zeyu Wang", "Zhifeng Li", "Xiu Li", "Wei Liu", "Dan Xu", "Linfeng Zhang", "Qifeng Chen"], "title": "Controllable Video Generation: A Survey", "categories": ["cs.GR", "cs.CV"], "comment": "project page:   https://github.com/mayuelala/Awesome-Controllable-Video-Generation", "summary": "With the rapid development of AI-generated content (AIGC), video generation has emerged as one of its most dynamic and impactful subfields. In particular, the advancement of video generation foundation models has led to growing demand for controllable video generation methods that can more accurately reflect user intent. Most existing foundation models are designed for text-to-video generation, where text prompts alone are often insufficient to express complex, multi-modal, and fine-grained user requirements. This limitation makes it challenging for users to generate videos with precise control using current models. To address this issue, recent research has explored the integration of additional non-textual conditions, such as camera motion, depth maps, and human pose, to extend pretrained video generation models and enable more controllable video synthesis. These approaches aim to enhance the flexibility and practical applicability of AIGC-driven video generation systems. In this survey, we provide a systematic review of controllable video generation, covering both theoretical foundations and recent advances in the field. We begin by introducing the key concepts and commonly used open-source video generation models. We then focus on control mechanisms in video diffusion models, analyzing how different types of conditions can be incorporated into the denoising process to guide generation. Finally, we categorize existing methods based on the types of control signals they leverage, including single-condition generation, multi-condition generation, and universal controllable generation. For a complete list of the literature on controllable video generation reviewed, please visit our curated repository at https://github.com/mayuelala/Awesome-Controllable-Video-Generation."}
{"id": "2507.17029", "pdf": "https://arxiv.org/pdf/2507.17029", "abs": "https://arxiv.org/abs/2507.17029", "authors": ["Luchuan Song", "Yang Zhou", "Zhan Xu", "Yi Zhou", "Deepali Aneja", "Chenliang Xu"], "title": "StreamME: Simplify 3D Gaussian Avatar within Live Stream", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "12 pages, 15 Figures", "summary": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The StreamME synchronously records and reconstructs a head avatar from live video streams without any pre-cached data, enabling seamless integration of the reconstructed appearance into downstream applications. This exceptionally fast training strategy, which we refer to as on-the-fly training, is central to our approach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating the reliance on MLPs in deformable 3DGS and relying solely on geometry, which significantly improves the adaptation speed to facial expression. To further ensure high efficiency in on-the-fly training, we introduced a simplification strategy based on primary points, which distributes the point clouds more sparsely across the facial surface, optimizing points number while maintaining rendering quality. Leveraging the on-the-fly training capabilities, our method protects the facial privacy and reduces communication bandwidth in VR system or online conference. Additionally, it can be directly applied to downstream application such as animation, toonify, and relighting. Please refer to our project page for more details: https://songluchuan.github.io/StreamME/."}
{"id": "2507.17135", "pdf": "https://arxiv.org/pdf/2507.17135", "abs": "https://arxiv.org/abs/2507.17135", "authors": ["Ting Jiang", "Yixiao Wang", "Hancheng Ye", "Zishan Shao", "Jingwei Sun", "Jingyang Zhang", "Zekai Chen", "Jianyi Zhang", "Yiran Chen", "Hai Li"], "title": "SADA: Stability-guided Adaptive Diffusion Acceleration", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Accepted and published by ICML 2025. Code is available at:   https://github.com/Ting-Justin-Jiang/sada-icml", "summary": "Diffusion models have achieved remarkable success in generative tasks but suffer from high computational costs due to their iterative sampling process and quadratic attention costs. Existing training-free acceleration strategies that reduce per-step computation cost, while effectively reducing sampling time, demonstrate low faithfulness compared to the original baseline. We hypothesize that this fidelity gap arises because (a) different prompts correspond to varying denoising trajectory, and (b) such methods do not consider the underlying ODE formulation and its numerical solution. In this paper, we propose Stability-guided Adaptive Diffusion Acceleration (SADA), a novel paradigm that unifies step-wise and token-wise sparsity decisions via a single stability criterion to accelerate sampling of ODE-based generative models (Diffusion and Flow-matching). For (a), SADA adaptively allocates sparsity based on the sampling trajectory. For (b), SADA introduces principled approximation schemes that leverage the precise gradient information from the numerical ODE solver. Comprehensive evaluations on SD-2, SDXL, and Flux using both EDM and DPM++ solvers reveal consistent $\\ge 1.8\\times$ speedups with minimal fidelity degradation (LPIPS $\\leq 0.10$ and FID $\\leq 4.5$) compared to unmodified baselines, significantly outperforming prior methods. Moreover, SADA adapts seamlessly to other pipelines and modalities: It accelerates ControlNet without any modifications and speeds up MusicLDM by $1.8\\times$ with $\\sim 0.01$ spectrogram LPIPS."}
